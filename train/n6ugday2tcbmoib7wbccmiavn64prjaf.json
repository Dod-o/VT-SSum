{
    "id": "n6ugday2tcbmoib7wbccmiavn64prjaf",
    "title": "AdaBoost is Universally Consistent",
    "info": {
        "author": [
            "Peter L. Bartlett, UC Berkeley"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Ensemble Methods"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_bartlett_auc/",
    "segmentation": [
        [
            "Thank you.",
            "OK, so this is joint work with Mikael Truscan, who's a PhD student in the Department of Statistics at UC Berkeley, and the talk is about Adaboost and."
        ],
        [
            "Some asymptotic properties of Adaboost.",
            "So let me start by reviewing the Adaboost algorithm.",
            "We have a sample we have X in XY pairs.",
            "And this is this is an input to our algorithm.",
            "We also have a bound on the number of iterations T. And we start off with a zero function, identically zero function, and then for T steps we add into our function.",
            "A real number Alpha Times H sub T where H comes from some class of functions capital F. They are chosen so as to minimize the sample average of E to the minus y * F of X.",
            "So this is this is the function that we had at the previous iteration and then we want to add in Alpha times.",
            "HXH comes from this class and Alpha is a real number.",
            "OK, so that's Adaboost, OK?",
            "I mean, you've seen it expressed in terms of re weightings of the data and so on, but it's doing precisely this, and of course you can write.",
            "You can write the value of Alpha explicitly in terms of the weighted misclassification probability of the age that you that you choose here, you know there are a lot of nice properties like that, but you know this is."
        ],
        [
            "Equivalent to two Adaboost.",
            "OK, so what's the title?",
            "Was this newspaper headline Adaboost is universally consistent, so what's the second part of it mean?",
            "Universal consistency.",
            "I talked about this a little earlier in the in the previous lecture.",
            "So if we have, we're assuming that we've got N IID XY pairs chosen from some product space, where X is the domain.",
            "And why is our our space of plus or minus one?",
            "Is the set plus 1 -- 1?",
            "And we come up with some function FN through some method or algorithm that takes as input the X1Y one through XNYN.",
            "Alright, so for instance, we might.",
            "We might use out of this to do that and we say that this method is universally consistent.",
            "If, as the sample size N gets large for all probability distributions, the risk of FN converges to the Bayes risk, the risk is the probability that our function misclassify's AY and the Bayes risk is the infima.",
            "Overall measurable functions of that of that quantity.",
            "OK, and.",
            "See, I've switched notation from the.",
            "From the lectures.",
            "Um?",
            "OK, so so the result is that Adaboost is universally consistent.",
            "It satisfies this property that its risk approaches the."
        ],
        [
            "The optimum under some.",
            "You know appropriate choice for the stopping time.",
            "OK, which is the one variable.",
            "I guess that we have freedom to adjust and the algorithm is still at a boost.",
            "So let me tell you first about some previous results for Adaboost.",
            "It's like algorithms and some of these you've seen mention of already.",
            "And then the key theorem is that if we stop Adaboost after number of iterations, that is less than linear in the sample size.",
            "Then it's.",
            "The risk of that function as the sample size goes to Infinity.",
            "The risk of that function approaches the the Bayes risk under some.",
            "Mild assumption, essential assumptions on the basis class.",
            "And then I want to give you a quick."
        ],
        [
            "Idea of the proof.",
            "And we'll see that there are a few a few open questions.",
            "Many open questions.",
            "OK, so previous results.",
            "Almost all of these are for other versions of algorithms.",
            "There are things that are related to Adaboost in the sense that they work somehow with an exponential loss function.",
            "But they're not.",
            "They're not Adaboost, so Adaboost works to greatly minimize the sample average of this exponential.",
            "This decreasing exponential of why times F of X.",
            "And it does that over the space of linear combinations of functions from some basis class.",
            "OK well.",
            "You know this optimization problem if we express it so Adaboost uses this particular greedy forward stagewise optimization.",
            "If we think of the optimization problem of minimizing this thing over a space of linear combinations of functions, well, for many interesting basis classes, the infamous zero, right?",
            "So we wouldn't expect great performance from the actual solution to this optimization problem.",
            "Right?",
            "So you know clearly we don't want.",
            "We don't want to actually solve this optimization problem, and So what do we want to do?",
            "Well, Adaboost works in this in this sequential way.",
            "We could consider, and this is what's done.",
            "In practice, we could consider stopping Adaboost at some point.",
            "That's what I'm going to talk about, but previously people have considered.",
            "You know modifying this criterion.",
            "So for instance, introducing a regularization term."
        ],
        [
            "Into this criterion.",
            "OK, so one example of this is minimizing.",
            "Well, not not an explicit introduction of regularization.",
            "I guess it's minimizing this.",
            "This kind of criterion, but over a restricted class rather than looking at the span of functions from F. Right, you look at the you look at some subset of that, for instance the scaled convex Hull of functions from F. OK, so this notation Co of F means the convex Hull of the set of convex combinations of functions from F. And we're scaling that by some constant gamma N. OK, or equivalently, we could think of these functions as being linear combinations of functions from F where the L1 norm is bounded by by gamma N. An alternative is to consider minimizing over the span of F. The sample average of the minus YF of X + a regularization term right where this is Lambda N times the appropriate norm up here in which we're considering a baller gamaran ball right?",
            "This is this is the smallest gamma for which F is in the scale.",
            "Convex Hull of capital F. OK, so these algorithms have been studied respectively by location by artisan, by, by Zhang, an for suitable choices of these parameters, gamma N and Lambda and the algorithms have been proven to have this this consistency property.",
            "That you know, if you allow your ball to increase suitably slowly.",
            "Right when you minimize this criterion, then everything works nicely.",
            "If you have the regularization term go to zero suitably slowly, then this works nicely also."
        ],
        [
            "Another alternative is to bound the step size.",
            "So you can think of his Adaboost with a.",
            "With a bounded stepsize, everything is the same, except instead of taking the step Alpha T that optimizes this thing, you ensure that your steps are always smaller than some epsilon.",
            "Right so.",
            "So we don't ever in particular.",
            "We don't take big steps through this through this function space, and so for suitable choices of the of the stopping time T and the step, the maximum step size epsilon allowing those things to vary.",
            "The.",
            "I guess both increase as a function of N. You could show this algorithm is universally consistent.",
            "And there are several several papers that do that."
        ],
        [
            "OK, so so those are results about algorithms that are.",
            "Related to to Adaboost, through this, this exponential cost function, there is a result about Adaboost itself.",
            "So what do I mean by that?",
            "I guess I've already already stated this, so Adaboost is greatly minimizing this thing.",
            "What do we mean by FN?",
            "What's what method are we really talking about?",
            "Of course we've got to have the function that's returned by Adaboost after some number of steps.",
            "In practice, perhaps this number of steps would be chosen by cross validation, or it could be some fixed fixed sequence.",
            "You know, and obviously you know this observation that for rich F this criterion has has a minimal value of 0 means.",
            "We don't want TN to be too large, you know we've seen that.",
            "It's known that that Adaboost cannot."
        ],
        [
            "Achieve.",
            "The optimum, so we want to.",
            "We want to ensure that we don't have too many steps.",
            "Well there is a result about Adaboost.",
            "It's a process consistency result due to due to Jang and it's the only previous result I know of for database itself.",
            "Of this kind, and it says that if the probability distribution satisfies certain smoothness assumptions, then there is a sequence of stopping times such that the risk of the function that you get if you stop after time TN converges to the base risk.",
            "OK, so.",
            "OK, there are two things that you could criticize about this result.",
            "The first is that you're only talking about distributions that are that satisfy some some smoothness assumptions.",
            "These are things that might not be natural, for instance, so it's only distributions on ardion Euclidean space.",
            "It's only distributions that are absolutely continuous with respect to Lebesgue measure on some compact set in Rd.",
            "OK, so that that's already something that.",
            "Is quite a natural and it's also there's another assumption, and that is that the the conditional probability.",
            "Satisfies a certain has has a smoothness property.",
            "And you know, again, this is something that.",
            "Isn't so pleasant and it's not something that you can you can check.",
            "Anet, for instance, rules out that the conditional probability could be 0 or or one.",
            "The second?",
            "That it raises is what's T sub N look like.",
            "In particular, how does it grow with the sample size in?",
            "And also doesn't need to depend on the probability distribution P. The result is for all distributions.",
            "There is a sequence.",
            "And it raises the question of what kind of rate we might expect in."
        ],
        [
            "In this convergence.",
            "OK, so let me."
        ],
        [
            "Let me tell you about the result, so we're taking a basis class F. We assume two things.",
            "1st that the VC dimension of this class is finite and this is essential from the results from the very first lecture.",
            "We saw, well.",
            "I guess the second later we saw that.",
            "You know, if we don't have this then add a bit.",
            "You can come up with situations where Adaboost is forced to stop after one step and it will fail, right?",
            "So there's no.",
            "There's nothing interesting to say in that case.",
            "The second assumption that we make is and something like.",
            "This is also essential is that the class of functions that we're using is rich enough to represent the two to have the optimal fire risk.",
            "I guess the notation using the previous talks was our sub five.",
            "For this I'm using R for this and L for risk.",
            "OK, so the that is as we allow the one norm of our parameters to go to Infinity.",
            "The smallest fire risk of a function in that scaled convex Hull of our basis class should go to the optimal fire risk.",
            "OK, and this is satisfied for all probability distributions.",
            "You know there are many natural families of basis functions for which this is satisfied for all probability distributions.",
            "For instance, if if we consider the class of linear threshold functions or binary trees with the usual axis, orthogonal decisions in Rd, where the number of leaves is at least D + 1.",
            "You know these are cases where you get this sort of a denseness property."
        ],
        [
            "There are many others, of course.",
            "OK, so under these two assumptions, this is the result.",
            "As long as the stopping time, the number of steps that we allow Adaboost to take.",
            "Goes to Infinity with the sample size N as long as it goes to Infinity, not too rapidly.",
            "So big oh event to the one minus Alpha for some positive Alpha.",
            "Then at a base with this, stopping time is universally consistent.",
            "That is, for all probability distributions, the risk of the function returned from a sample of size N goes to the Bayes risk as N goes to Infinity.",
            "Yep, how slow can you speak?",
            "Because I mean ideally when you implement it, you care about how many iterations they have to use.",
            "How slow can you pick it?",
            "So this says that as long as it's it goes slower than linear, you're OK.",
            "Slow rhythmically.",
            "You certainly yeah, any tiene that goes to Infinity no faster than linearly is fine, right?",
            "So as slow as you could pick it.",
            "You know log, log, log if you like, but you know you're not.",
            "You're not going to.",
            "Change the number of iterations very much.",
            "Remove very rapidly, right?",
            "So I'll say something about you know any what we conjecture about the converse.",
            "You know how fast you could.",
            "It's OK. Is it possible to increase it anymore quickly than this?"
        ],
        [
            "Right, and you know, it seems like it seems likely that that's not the case.",
            "So let me give you a brief idea of the of the proof.",
            "So we're working.",
            "We've seen this classification calibrated idea.",
            "We know that the exponential function is classification calibrated, and so all we need to show is that the fire risk right of our class approaches the optimal fire risk.",
            "OK, so we don't quite.",
            "We don't quite do that.",
            "We're working instead.",
            "With a clipped version of functions from our class.",
            "So actually before I go into those, those ideas of the proof, I guess.",
            "I guess I should say just at a high level.",
            "We've seen in these lectures that are different ways of measuring the complexity of a class we could measure.",
            "For instance, if we take the space of tea combinations, right?",
            "So we're combining T functions from our basis class.",
            "That's what Adaboost produces after T steps, then.",
            "You know the VC theory tells us that we've got some control of the complexity there because it's a combination of two things right?",
            "The VC dimension of a combination of T of these linear combination of T of these things that have a fixed VC dimension?",
            "Well, that's like vaguely T log T times the VC dimension of the basis class.",
            "Alright, you know there's you can get a result of that form, so you can say something when you measure complexity in terms of the number of functions that we combine.",
            "You could also say something for complexity in terms of the radius of the L1 ball in parameter space we saw that also, right?",
            "That's another way of measuring the complexity of the class of functions that Adaboost is producing, but that one is a little harder to control here because it's not.",
            "Adaboost is not keeping track of it step size, right?",
            "And this is the key problem in analyzing its behavior that way that in order to.",
            "To ensure that we have nice statistical properties, we want to make sure that that L1 norm isn't too big.",
            "So actually we use both of those measures of complexity, and it seems to be essential to do that in proving the universal consistency of Adaboost, right?",
            "Both the number of functions that we combine and the L1L1 ball.",
            "In order to deal with the number of functions you know to use that that sort of theory.",
            "We measure complexity according to the size of the of the combination that the number of functions in a combination we need to clip our function.",
            "So we worry about, you know the how large the larger values the functions have.",
            "So we define this clipping function.",
            "That takes the value of our function and always keeps it between minus, Lambda and Lambda, right?",
            "So we never we never go above Lambda.",
            "We never go below minus, Lambda and then we look at the fire risk of this thing and we ensure that it approaches the optimal fire risk.",
            "And that tells us that the risk right, the probability of misclassification of the clipped guy approaches the optimal well the risk of the clip guy is the same as the risk of the of the original.",
            "The unclipped right because we're just thresholding here.",
            "OK, we work with this so that we can get a handle on its its complexity using the VC sort of theory.",
            "OK and of course in order to have this true well we really do need to allow Lambda end to go to Infinity if the conditional probabilities are close to zero or one somewhere then we need very big values in magnitude of the function, right?",
            "So we're going to have to have this Lambda."
        ],
        [
            "Getting large, but you know we'll see.",
            "But that's not that's an innocuous requirement.",
            "OK, so so in looking at viewing these functions were working with as combinations of T functions from the basis class we could just use the standard results involving the VC dimension of these basis functions to say that the fire risk is not much bigger than the empirical fire risk.",
            "Right?",
            "Um?",
            "And yeah, where this is the notation I'm using here for empirical fire."
        ],
        [
            "The sample average of the minus wife of ex.",
            "The clipping isn't changing things very much OK, because we're clipping these functions.",
            "If we consider the exponential loss where clipping down at minus Lambda and Lambda up here clipping at Lambda is helping us, possibly a lot right on the on the negative side.",
            "On the positive side, clipping at Lambda is only hurting us by maybe either the minus Lambda, maybe as much as that."
        ],
        [
            "OK, so clipping doesn't hurt very much.",
            "And there's a whole lot of detail here that I'm skipping over.",
            "This is really.",
            "The the main bulk of the proof in fact is in this in this step, but let me just say that you know there is that you can use some nice properties that the exponential function has to apply without without.",
            "Without any additional assumptions, like stepsize assumptions to apply a numerical convergence result of vehicle and writ of to say that we after T steps, we do almost as well as some function in.",
            "I should define this in the Lambda scaled convex Hull of our class, right?",
            "So so we this result tells us that after T steps we're doing almost as well as a function in an L1 ball in parameter space, OK?"
        ],
        [
            "And then.",
            "We have to.",
            "I said VC theory is not really the case at this step.",
            "We use this the size of the 01 ball as the measure of complexity to tell us that.",
            "That once again.",
            "The empirical fire risk for this guy who's in who's in a small ball is close to the true risk, so we need to use both notions of complexity to get to get a result of this form and then by playing around with all the parameters, making things go to zero or Infinity appropriately, we have the result right, and the crucial point is back here.",
            "OK, we need this term to go to zero, which means that T needs to grow.",
            "We want T log T / N and there's something else here that's.",
            "That's going to 0 slow going to Infinity slowly, but basically T log T / N needs to go to zero.",
            "OK, that's where the sub linear thing comes in.",
            "So you could refine it from sub sub linear to this.",
            "Uh.",
            "Faster than in log in and over login."
        ],
        [
            "OK, so that's that's it.",
            "So let me let me finish with a few open problems.",
            "All of this is for the exponential cost function, right?",
            "So without without any modification, Adaboost has this nice asymptotic property.",
            "What about other cost functions?",
            "You know people have proposed the logic function and many others.",
            "You know the exponential function seems to seems to really be well suited, right?",
            "It's it's very easy to work with the difficulty with using something like this is we don't have as nice a handle on the second derivative of the function we want.",
            "We want the second derivative to be large whenever the risk is large, and that's not the case for such a function.",
            "We've given a result in terms of plus, minus one value basis functions.",
            "So.",
            "There's also been versions of Adaboost proposed that involve real value basis functions.",
            "We can't say our result does not apply.",
            "In that case, the same issue arises, right?",
            "The relationship between the second derivative in the direction of a basis function and the empirical risk virus criterion is just.",
            "Is just true because we have these plus or minus one value functions, so you know again this is an extension that we don't know how to do when you look at the actual rates, assume some approximation rate and then say now what's the optimal.",
            "What's the actual rate of convergence of the of the risk to the the Bayes risk?",
            "The rates that pop out of this method of proof, so we have an asymptotically result.",
            "It actually gives explicit rates.",
            "You know constants at all, but the rates that pop out are really pretty awful, and the bottleneck is in the numerical results, right?",
            "With the rate of decrease of the empirical fire risk to to some comparison for the empirical risk for some comparison function.",
            "So we're appealing there too.",
            "To a previous work of Bickleton ripped off and that gives a one over square root log T right right as the number of iterations T well, this is.",
            "This is really terrible.",
            "And that everything else seems well behaved.",
            "It's really just the numerical part that lets us down.",
            "This really seems pleasant."
        ],
        [
            "Stick, it seems likely that it could be improved, but we don't know how to do that.",
            "Alright, are there any questions?",
            "That's the result depend on where there is a base learner optimized desktop problem.",
            "Essentially, that's that's a good question.",
            "Yeah, I didn't mention that.",
            "So what?",
            "I'm assuming what I presented as the Adaboost algorithm was the base learner was producing the optimum right?",
            "The optimal function, the?",
            "The minimizer, it turns out that you can relax that that the result is also true if you if you just.",
            "If the base learner produces a function that is within some constant factor of the optimum.",
            "Right and then everything goes through.",
            "It goes through it away.",
            "That's curiously also dependent on the form of the exponential loss, right?",
            "So it's funny.",
            "You know that if we want to relax the algorithm to one that that.",
            "Involves just a weak learner that's really rather weak then.",
            "There would be another point in the proof where things breakdown if we if we move away from the exponential criterion so you know everything seems to be tide to the exponential criterion is sort of strange that the exponential loss function in a strange way.",
            "But yeah, that's a good point.",
            "It can be.",
            "Restated for the everything here is true when the weak learner gets within a constant factor of optimum.",
            "'cause I'm wondering with this and ultimate proofing proof technique to involve the tool to take this exponential loss actually should do a log in front of it and go to the dual.",
            "It becomes minimizing the relative entropy subject to linear constraint, which is been very well studied.",
            "And you know you may be able to prove convergence in that domain and then right?",
            "And the logical is hardly any different from the other.",
            "One is just right, so that might that might work to give us, you know something, some improvement in the numerical part, right?",
            "I mean, I guess what we?",
            "What we're doing here is not worrying about you know the convergence of the empirical criterion, that that's one step in the proof, right?",
            "The convergence of the empirical fire risk to its optimal value.",
            "Right, you know the the.",
            "The most of the work is in is, I guess in applying a result of that form.",
            "But you know this is one step in the whole process, but it's true.",
            "It may be that we could get that we could use alternative techniques to look at that kind of convergence to get a result or result of this form.",
            "By sensor and then when you minimize the diversion center to demonstrate some convergence properties.",
            "Yeah, I mean I don't know about that book.",
            "I would be interested to see it the the big and rich of result is the only one that I know of that actually gives you a suitable.",
            "Convergence property with respect to a comparison function, so all of the other techniques that I'm aware of.",
            "Involve.",
            "Constraints.",
            "Don't apply to this kind of.",
            "An algorithm, right?",
            "The greedy, the forward stepwise optimization that Adaboost uses.",
            "Yeah.",
            "But it may be that you know considering it as considering this dual optimization problem, you may be able to.",
            "Find some property of such a greedy scheme in the jewel.",
            "I don't know.",
            "Right?",
            "Thank you.",
            "Oh, I see.",
            "Be careful, his excellent post.",
            "Thank you.",
            "Beautiful.",
            "Tiffany thank you, thank you very much.",
            "Alright.",
            "So after you see one more piece that you have your summer, you can have your summer better.",
            "Wow.",
            "Thank you this is you know for me and I'm, you know, from solid National University and what I am going to present here is about the implementation of a condom machines with daily molecules and this is kind of computing which uses the molecules as competing materials and is also used.",
            "May tell Kenny Computing an the objective of my research is make a model of.",
            "Concrete fires in which can be implemented with DNA molecules, and here are my contents and easier.",
            "That process is of the.",
            "Is needed in gaining the corner case buyers and some pictures.",
            "Can explain my conversions and this is pretty much like the machines in software solutions that data in some space and we make contact from them and make some optimization.",
            "Optimizing process and finally despiteful is classified as blue an in game content machines.",
            "The data included a scaling molecules, an hybrid.",
            "We need to hybridize station to make corner an after some iterative selections we can make the distinction of single strength.",
            "An if we put white once because this fight once more to the Brewers.",
            "This is this is disconnected, you blue.",
            "And here is my contact again.",
            "The color element is intended to prevent the simulated between DNS change and this is defined as like this.",
            "The corner element ID is defined as the.",
            "The amount of double Saints who Jerry much are single stranded DNA index.",
            "I an complementary option English sent the next Friday and I took.",
            "I explained this again later and here is my hybrid hybrid model.",
            "When hyper hyper you know cause the energy changes and the amount of energy change is.",
            "Take created by summing each pairs energy change, which is a scientist minus 7.2, four, 80 and Cape Air and minus 9, four CJ&GC pair and my and minus 5.3 four orders and with some definition some pages of entropy and with some definition of.",
            "Change your favorite case.",
            "We finally get the amount of hybridization and amount of nature and so this we make the image the 1st order differential equation.",
            "Anne here here.",
            "The first I hit here is the first.",
            "How the corner is made and we first synthesized anywhere, gives operating today to like this and some of them may be labeled as plus one and some of them may be leveled.",
            "It's minus one an.",
            "This can be included like this, for example an another one can be encoded as like this and we make the complementary sequence for this and.",
            "And this should be included like this, cause the compliment to go even one is.",
            "Input it like this and they make perfect complementary and after we doing hybridization we finally get to corner and put into our definition the Continental 2 one is.",
            "Get an ace captain by the amount of double change of these Ann.",
            "This element is gotten by the amount of docs change, which is composed of these two elements.",
            "And definitely we have no guarantee that this corner is positive definite and the problem is that the 1st order differential equation is difficult to make.",
            "The General general condition for the positive definiteness of our corner.",
            "However, we have some hints that we can make put different corner though for City states if we in some high temperature where the little diagonal anyone produces we can make.",
            "Diamonite dominant men case, which is definitely positive definite.",
            "An surely this this is.",
            "This could be based pass and the Secondly second hint is that with some of submission and with some appropriate control temperature we can get pushed.",
            "It definitely corner and I will explain this here and the first ocean depth ocean.",
            "Very simple models.",
            "Which is constant speed hybrid hybridization an the hybridization of course with constant with.",
            "We will concentrate see before some.",
            "With constant speed, see below.",
            "Some temperature key, an above this temperature no hybridization of course, and this is our solution over contested hybrid daily station and the special temperature K is determined by the energy.",
            "Binding energy and if we control temperature like this, constantly decrease from key 12TB we guess.",
            "We can calculate each element on any query and here is the.",
            "Here is the amount of double stage and actually I put some other variables here which is that.",
            "Actually I inserted common SQL Server links M While the origonal consist length, is there an the corner can be changed so much differently and here we have another layer would be which represents the average binding energy of Commerce equals an.",
            "This can be disabled.",
            "Can help from minus 7.22.",
            "Nine point circle and from this we can get some pound fish guarantees the positive definiteness an we 2 pounds.",
            "We can get this.",
            "Fingers and hoping to pick values that are some.",
            "If some different figures which like this and like this, and this is somewhere in between an if we control the temperature from this region about some T one and two this region, we guarantee that we can get the definite corner.",
            "With this activities, this Dior hint and actually we must make the hybrid agent dynamics and.",
            "I think we make Edgware don't go through in detail because of the time limit and the.",
            "Patient can be efficient like this and we can serve the first or the differential equations and so this we finally we can get this kind of videos and these figures.",
            "The rag might increase fire with the corner previously.",
            "Unquote an this figure is the figure that we control the temperature constantly at very high temperature and this shows the dominant video and which is over 58 and we cannot determine in these points and this is the figure that the temperature is quote from 60 degrees to 30 TV and this shows so much better figure.",
            "So how?",
            "Match the video and this is the figure somewhere in between.",
            "And this is the picture that we control the temperature at.",
            "We kept the temperature at 3030 degree an we failed to make the positive definite corner and disappear.",
            "Picture we have got an actually.",
            "You know that in Gaussian corner we can get similar figures an according to corner parameters and then finally to stay small.",
            "We can get this type of video and appropriate.",
            "You can just type or figure an here we can know from this test.",
            "The temperature control can be used as an animator.",
            "Seemed encounter machines.",
            "Anne here, until now we I spend over the preceding text about Connors an.",
            "We'd like to optimize some parameters to operate this kernel to discrimination an.",
            "The ultimatum to conventional optimization method of large machines is additional very complex and difficult, so it is very difficult to implement it with to be implemented with the.",
            "Then America's because training workers has only simple operations like hybridization, anti nature, an.",
            "So I pushed I. I suggest another method, Java with meditation and this is for performance.",
            "Really sticks an.",
            "Where after we talked if Lee.",
            "Update this corner.",
            "According to this equation, and this is the case.",
            "Update The corner which is denoted as K subscript K and this is.",
            "This can be gotten from summing up or corner elements whose love is different from the rabbits and.",
            "Why we update this corner?",
            "We can get some parameters from the updated corner an with these parameters.",
            "We can do discrimination and I explain this again.",
            "This is Polly Reader that I operate the kernel text message an at 4C.",
            "At Dell's update it is donkey berries uniform and this video source.",
            "Plus an update goes on.",
            "The discrimination region finds the appropriate region an from.",
            "From this picture, the training is settled by the.",
            "Product key approaches to the parameters that is consumed from randomizing classifier optimized by randomizing classifier an.",
            "This kind of figure is the correlation of exactly 1.",
            "And this piece of text messages can be implemented in a market like this, and this is very simple.",
            "If we this is a corner made by hybridization, and if you choose the double standard molecules of different levels, for example.",
            "Much pressure, massive an which elements are minus impressive reselling photo storage trains an put on put on another test tube and we denature an eye hybridize again.",
            "Then we get the updated Connor.",
            "This is the simple process of an update and.",
            "Yeah so.",
            "An at the case of date you can give him for CIPK which can be which represents the population of single strange the ice English change and we can get we have the.",
            "Number of people meters Anne Anne with this parameters if we do.",
            "If we put a new data and count.",
            "Which library of science is?",
            "But it's more we.",
            "King get the discriminated level.",
            "And here is some agent provide a major data which is a very easy problem and this is a gene expression data.",
            "I included this as the high efficiency and low expression HA an.",
            "Brake disc including we got the running cover like this an update commensals got very.",
            "Capital P approach to the target performance an.",
            "Here this is the figure that I put.",
            "1st first we started with little with little amounts of data an.",
            "Continually we put some amount of data again and again and again.",
            "An actor shows that.",
            "It is very quick recovery to the performance and the overall performance increases as the training data increases an.",
            "This is the performance of a corner update and this is slightly lower than subject matter machine, which is large margin classifier and if we constructed this equation has no bias term.",
            "Compared with SVM with no bias, which is the performance of this one, this is not very.",
            "Basically, yeah, so this is the end of my presentation and please give some comments or questions.",
            "Thank you.",
            "So we have.",
            "Other speakers present OK so thanks for that.",
            "OK, yeah.",
            "Use.",
            "Turn off with the SVN some fairly.",
            "Design Colonel yeah I designed a corner.",
            "He kicked an eel.",
            "Yeah, we have been getting Neil Neil.",
            "You know this is a simulator and this is designed for in vitro experiment an.",
            "Cause in between extent we cannot use the conventional method of making corner an optimization method and the corner itself is designed OK and the optimization process is again.",
            "So we both knew here.",
            "So you just narrow down the regions for anneal temperature?",
            "Yeah please.",
            "Under some simplified condition and I put the consequences to every sequence to make.",
            "It makes the region never so qualify in the graph.",
            "I showed the axis is the length of.",
            ", ice, encourages the temperature region.",
            "'cause yeah.",
            "About"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is joint work with Mikael Truscan, who's a PhD student in the Department of Statistics at UC Berkeley, and the talk is about Adaboost and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some asymptotic properties of Adaboost.",
                    "label": 0
                },
                {
                    "sent": "So let me start by reviewing the Adaboost algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have a sample we have X in XY pairs.",
                    "label": 0
                },
                {
                    "sent": "And this is this is an input to our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We also have a bound on the number of iterations T. And we start off with a zero function, identically zero function, and then for T steps we add into our function.",
                    "label": 1
                },
                {
                    "sent": "A real number Alpha Times H sub T where H comes from some class of functions capital F. They are chosen so as to minimize the sample average of E to the minus y * F of X.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the function that we had at the previous iteration and then we want to add in Alpha times.",
                    "label": 0
                },
                {
                    "sent": "HXH comes from this class and Alpha is a real number.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's Adaboost, OK?",
                    "label": 0
                },
                {
                    "sent": "I mean, you've seen it expressed in terms of re weightings of the data and so on, but it's doing precisely this, and of course you can write.",
                    "label": 0
                },
                {
                    "sent": "You can write the value of Alpha explicitly in terms of the weighted misclassification probability of the age that you that you choose here, you know there are a lot of nice properties like that, but you know this is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Equivalent to two Adaboost.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the title?",
                    "label": 0
                },
                {
                    "sent": "Was this newspaper headline Adaboost is universally consistent, so what's the second part of it mean?",
                    "label": 0
                },
                {
                    "sent": "Universal consistency.",
                    "label": 0
                },
                {
                    "sent": "I talked about this a little earlier in the in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "So if we have, we're assuming that we've got N IID XY pairs chosen from some product space, where X is the domain.",
                    "label": 0
                },
                {
                    "sent": "And why is our our space of plus or minus one?",
                    "label": 0
                },
                {
                    "sent": "Is the set plus 1 -- 1?",
                    "label": 0
                },
                {
                    "sent": "And we come up with some function FN through some method or algorithm that takes as input the X1Y one through XNYN.",
                    "label": 0
                },
                {
                    "sent": "Alright, so for instance, we might.",
                    "label": 0
                },
                {
                    "sent": "We might use out of this to do that and we say that this method is universally consistent.",
                    "label": 1
                },
                {
                    "sent": "If, as the sample size N gets large for all probability distributions, the risk of FN converges to the Bayes risk, the risk is the probability that our function misclassify's AY and the Bayes risk is the infima.",
                    "label": 0
                },
                {
                    "sent": "Overall measurable functions of that of that quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "See, I've switched notation from the.",
                    "label": 0
                },
                {
                    "sent": "From the lectures.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so so the result is that Adaboost is universally consistent.",
                    "label": 0
                },
                {
                    "sent": "It satisfies this property that its risk approaches the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The optimum under some.",
                    "label": 0
                },
                {
                    "sent": "You know appropriate choice for the stopping time.",
                    "label": 0
                },
                {
                    "sent": "OK, which is the one variable.",
                    "label": 0
                },
                {
                    "sent": "I guess that we have freedom to adjust and the algorithm is still at a boost.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you first about some previous results for Adaboost.",
                    "label": 1
                },
                {
                    "sent": "It's like algorithms and some of these you've seen mention of already.",
                    "label": 0
                },
                {
                    "sent": "And then the key theorem is that if we stop Adaboost after number of iterations, that is less than linear in the sample size.",
                    "label": 1
                },
                {
                    "sent": "Then it's.",
                    "label": 0
                },
                {
                    "sent": "The risk of that function as the sample size goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The risk of that function approaches the the Bayes risk under some.",
                    "label": 0
                },
                {
                    "sent": "Mild assumption, essential assumptions on the basis class.",
                    "label": 0
                },
                {
                    "sent": "And then I want to give you a quick.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Idea of the proof.",
                    "label": 0
                },
                {
                    "sent": "And we'll see that there are a few a few open questions.",
                    "label": 0
                },
                {
                    "sent": "Many open questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so previous results.",
                    "label": 0
                },
                {
                    "sent": "Almost all of these are for other versions of algorithms.",
                    "label": 0
                },
                {
                    "sent": "There are things that are related to Adaboost in the sense that they work somehow with an exponential loss function.",
                    "label": 0
                },
                {
                    "sent": "But they're not.",
                    "label": 0
                },
                {
                    "sent": "They're not Adaboost, so Adaboost works to greatly minimize the sample average of this exponential.",
                    "label": 0
                },
                {
                    "sent": "This decreasing exponential of why times F of X.",
                    "label": 0
                },
                {
                    "sent": "And it does that over the space of linear combinations of functions from some basis class.",
                    "label": 0
                },
                {
                    "sent": "OK well.",
                    "label": 0
                },
                {
                    "sent": "You know this optimization problem if we express it so Adaboost uses this particular greedy forward stagewise optimization.",
                    "label": 0
                },
                {
                    "sent": "If we think of the optimization problem of minimizing this thing over a space of linear combinations of functions, well, for many interesting basis classes, the infamous zero, right?",
                    "label": 1
                },
                {
                    "sent": "So we wouldn't expect great performance from the actual solution to this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So you know clearly we don't want.",
                    "label": 0
                },
                {
                    "sent": "We don't want to actually solve this optimization problem, and So what do we want to do?",
                    "label": 0
                },
                {
                    "sent": "Well, Adaboost works in this in this sequential way.",
                    "label": 0
                },
                {
                    "sent": "We could consider, and this is what's done.",
                    "label": 0
                },
                {
                    "sent": "In practice, we could consider stopping Adaboost at some point.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm going to talk about, but previously people have considered.",
                    "label": 0
                },
                {
                    "sent": "You know modifying this criterion.",
                    "label": 0
                },
                {
                    "sent": "So for instance, introducing a regularization term.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into this criterion.",
                    "label": 0
                },
                {
                    "sent": "OK, so one example of this is minimizing.",
                    "label": 0
                },
                {
                    "sent": "Well, not not an explicit introduction of regularization.",
                    "label": 0
                },
                {
                    "sent": "I guess it's minimizing this.",
                    "label": 0
                },
                {
                    "sent": "This kind of criterion, but over a restricted class rather than looking at the span of functions from F. Right, you look at the you look at some subset of that, for instance the scaled convex Hull of functions from F. OK, so this notation Co of F means the convex Hull of the set of convex combinations of functions from F. And we're scaling that by some constant gamma N. OK, or equivalently, we could think of these functions as being linear combinations of functions from F where the L1 norm is bounded by by gamma N. An alternative is to consider minimizing over the span of F. The sample average of the minus YF of X + a regularization term right where this is Lambda N times the appropriate norm up here in which we're considering a baller gamaran ball right?",
                    "label": 0
                },
                {
                    "sent": "This is this is the smallest gamma for which F is in the scale.",
                    "label": 0
                },
                {
                    "sent": "Convex Hull of capital F. OK, so these algorithms have been studied respectively by location by artisan, by, by Zhang, an for suitable choices of these parameters, gamma N and Lambda and the algorithms have been proven to have this this consistency property.",
                    "label": 1
                },
                {
                    "sent": "That you know, if you allow your ball to increase suitably slowly.",
                    "label": 0
                },
                {
                    "sent": "Right when you minimize this criterion, then everything works nicely.",
                    "label": 0
                },
                {
                    "sent": "If you have the regularization term go to zero suitably slowly, then this works nicely also.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another alternative is to bound the step size.",
                    "label": 0
                },
                {
                    "sent": "So you can think of his Adaboost with a.",
                    "label": 0
                },
                {
                    "sent": "With a bounded stepsize, everything is the same, except instead of taking the step Alpha T that optimizes this thing, you ensure that your steps are always smaller than some epsilon.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So we don't ever in particular.",
                    "label": 0
                },
                {
                    "sent": "We don't take big steps through this through this function space, and so for suitable choices of the of the stopping time T and the step, the maximum step size epsilon allowing those things to vary.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "I guess both increase as a function of N. You could show this algorithm is universally consistent.",
                    "label": 1
                },
                {
                    "sent": "And there are several several papers that do that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so those are results about algorithms that are.",
                    "label": 0
                },
                {
                    "sent": "Related to to Adaboost, through this, this exponential cost function, there is a result about Adaboost itself.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "I guess I've already already stated this, so Adaboost is greatly minimizing this thing.",
                    "label": 0
                },
                {
                    "sent": "What do we mean by FN?",
                    "label": 0
                },
                {
                    "sent": "What's what method are we really talking about?",
                    "label": 0
                },
                {
                    "sent": "Of course we've got to have the function that's returned by Adaboost after some number of steps.",
                    "label": 1
                },
                {
                    "sent": "In practice, perhaps this number of steps would be chosen by cross validation, or it could be some fixed fixed sequence.",
                    "label": 0
                },
                {
                    "sent": "You know, and obviously you know this observation that for rich F this criterion has has a minimal value of 0 means.",
                    "label": 1
                },
                {
                    "sent": "We don't want TN to be too large, you know we've seen that.",
                    "label": 0
                },
                {
                    "sent": "It's known that that Adaboost cannot.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Achieve.",
                    "label": 0
                },
                {
                    "sent": "The optimum, so we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to ensure that we don't have too many steps.",
                    "label": 0
                },
                {
                    "sent": "Well there is a result about Adaboost.",
                    "label": 1
                },
                {
                    "sent": "It's a process consistency result due to due to Jang and it's the only previous result I know of for database itself.",
                    "label": 1
                },
                {
                    "sent": "Of this kind, and it says that if the probability distribution satisfies certain smoothness assumptions, then there is a sequence of stopping times such that the risk of the function that you get if you stop after time TN converges to the base risk.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, there are two things that you could criticize about this result.",
                    "label": 0
                },
                {
                    "sent": "The first is that you're only talking about distributions that are that satisfy some some smoothness assumptions.",
                    "label": 0
                },
                {
                    "sent": "These are things that might not be natural, for instance, so it's only distributions on ardion Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "It's only distributions that are absolutely continuous with respect to Lebesgue measure on some compact set in Rd.",
                    "label": 0
                },
                {
                    "sent": "OK, so that that's already something that.",
                    "label": 0
                },
                {
                    "sent": "Is quite a natural and it's also there's another assumption, and that is that the the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Satisfies a certain has has a smoothness property.",
                    "label": 0
                },
                {
                    "sent": "And you know, again, this is something that.",
                    "label": 0
                },
                {
                    "sent": "Isn't so pleasant and it's not something that you can you can check.",
                    "label": 0
                },
                {
                    "sent": "Anet, for instance, rules out that the conditional probability could be 0 or or one.",
                    "label": 0
                },
                {
                    "sent": "The second?",
                    "label": 0
                },
                {
                    "sent": "That it raises is what's T sub N look like.",
                    "label": 1
                },
                {
                    "sent": "In particular, how does it grow with the sample size in?",
                    "label": 0
                },
                {
                    "sent": "And also doesn't need to depend on the probability distribution P. The result is for all distributions.",
                    "label": 1
                },
                {
                    "sent": "There is a sequence.",
                    "label": 0
                },
                {
                    "sent": "And it raises the question of what kind of rate we might expect in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this convergence.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me tell you about the result, so we're taking a basis class F. We assume two things.",
                    "label": 0
                },
                {
                    "sent": "1st that the VC dimension of this class is finite and this is essential from the results from the very first lecture.",
                    "label": 0
                },
                {
                    "sent": "We saw, well.",
                    "label": 0
                },
                {
                    "sent": "I guess the second later we saw that.",
                    "label": 0
                },
                {
                    "sent": "You know, if we don't have this then add a bit.",
                    "label": 0
                },
                {
                    "sent": "You can come up with situations where Adaboost is forced to stop after one step and it will fail, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no.",
                    "label": 0
                },
                {
                    "sent": "There's nothing interesting to say in that case.",
                    "label": 0
                },
                {
                    "sent": "The second assumption that we make is and something like.",
                    "label": 0
                },
                {
                    "sent": "This is also essential is that the class of functions that we're using is rich enough to represent the two to have the optimal fire risk.",
                    "label": 0
                },
                {
                    "sent": "I guess the notation using the previous talks was our sub five.",
                    "label": 0
                },
                {
                    "sent": "For this I'm using R for this and L for risk.",
                    "label": 0
                },
                {
                    "sent": "OK, so the that is as we allow the one norm of our parameters to go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The smallest fire risk of a function in that scaled convex Hull of our basis class should go to the optimal fire risk.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is satisfied for all probability distributions.",
                    "label": 0
                },
                {
                    "sent": "You know there are many natural families of basis functions for which this is satisfied for all probability distributions.",
                    "label": 0
                },
                {
                    "sent": "For instance, if if we consider the class of linear threshold functions or binary trees with the usual axis, orthogonal decisions in Rd, where the number of leaves is at least D + 1.",
                    "label": 1
                },
                {
                    "sent": "You know these are cases where you get this sort of a denseness property.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many others, of course.",
                    "label": 0
                },
                {
                    "sent": "OK, so under these two assumptions, this is the result.",
                    "label": 0
                },
                {
                    "sent": "As long as the stopping time, the number of steps that we allow Adaboost to take.",
                    "label": 0
                },
                {
                    "sent": "Goes to Infinity with the sample size N as long as it goes to Infinity, not too rapidly.",
                    "label": 0
                },
                {
                    "sent": "So big oh event to the one minus Alpha for some positive Alpha.",
                    "label": 1
                },
                {
                    "sent": "Then at a base with this, stopping time is universally consistent.",
                    "label": 1
                },
                {
                    "sent": "That is, for all probability distributions, the risk of the function returned from a sample of size N goes to the Bayes risk as N goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Yep, how slow can you speak?",
                    "label": 0
                },
                {
                    "sent": "Because I mean ideally when you implement it, you care about how many iterations they have to use.",
                    "label": 0
                },
                {
                    "sent": "How slow can you pick it?",
                    "label": 0
                },
                {
                    "sent": "So this says that as long as it's it goes slower than linear, you're OK.",
                    "label": 0
                },
                {
                    "sent": "Slow rhythmically.",
                    "label": 0
                },
                {
                    "sent": "You certainly yeah, any tiene that goes to Infinity no faster than linearly is fine, right?",
                    "label": 0
                },
                {
                    "sent": "So as slow as you could pick it.",
                    "label": 0
                },
                {
                    "sent": "You know log, log, log if you like, but you know you're not.",
                    "label": 0
                },
                {
                    "sent": "You're not going to.",
                    "label": 0
                },
                {
                    "sent": "Change the number of iterations very much.",
                    "label": 0
                },
                {
                    "sent": "Remove very rapidly, right?",
                    "label": 0
                },
                {
                    "sent": "So I'll say something about you know any what we conjecture about the converse.",
                    "label": 0
                },
                {
                    "sent": "You know how fast you could.",
                    "label": 0
                },
                {
                    "sent": "It's OK. Is it possible to increase it anymore quickly than this?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and you know, it seems like it seems likely that that's not the case.",
                    "label": 0
                },
                {
                    "sent": "So let me give you a brief idea of the of the proof.",
                    "label": 1
                },
                {
                    "sent": "So we're working.",
                    "label": 0
                },
                {
                    "sent": "We've seen this classification calibrated idea.",
                    "label": 0
                },
                {
                    "sent": "We know that the exponential function is classification calibrated, and so all we need to show is that the fire risk right of our class approaches the optimal fire risk.",
                    "label": 1
                },
                {
                    "sent": "OK, so we don't quite.",
                    "label": 0
                },
                {
                    "sent": "We don't quite do that.",
                    "label": 0
                },
                {
                    "sent": "We're working instead.",
                    "label": 0
                },
                {
                    "sent": "With a clipped version of functions from our class.",
                    "label": 0
                },
                {
                    "sent": "So actually before I go into those, those ideas of the proof, I guess.",
                    "label": 0
                },
                {
                    "sent": "I guess I should say just at a high level.",
                    "label": 0
                },
                {
                    "sent": "We've seen in these lectures that are different ways of measuring the complexity of a class we could measure.",
                    "label": 1
                },
                {
                    "sent": "For instance, if we take the space of tea combinations, right?",
                    "label": 0
                },
                {
                    "sent": "So we're combining T functions from our basis class.",
                    "label": 0
                },
                {
                    "sent": "That's what Adaboost produces after T steps, then.",
                    "label": 0
                },
                {
                    "sent": "You know the VC theory tells us that we've got some control of the complexity there because it's a combination of two things right?",
                    "label": 0
                },
                {
                    "sent": "The VC dimension of a combination of T of these linear combination of T of these things that have a fixed VC dimension?",
                    "label": 0
                },
                {
                    "sent": "Well, that's like vaguely T log T times the VC dimension of the basis class.",
                    "label": 0
                },
                {
                    "sent": "Alright, you know there's you can get a result of that form, so you can say something when you measure complexity in terms of the number of functions that we combine.",
                    "label": 0
                },
                {
                    "sent": "You could also say something for complexity in terms of the radius of the L1 ball in parameter space we saw that also, right?",
                    "label": 0
                },
                {
                    "sent": "That's another way of measuring the complexity of the class of functions that Adaboost is producing, but that one is a little harder to control here because it's not.",
                    "label": 0
                },
                {
                    "sent": "Adaboost is not keeping track of it step size, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the key problem in analyzing its behavior that way that in order to.",
                    "label": 0
                },
                {
                    "sent": "To ensure that we have nice statistical properties, we want to make sure that that L1 norm isn't too big.",
                    "label": 0
                },
                {
                    "sent": "So actually we use both of those measures of complexity, and it seems to be essential to do that in proving the universal consistency of Adaboost, right?",
                    "label": 0
                },
                {
                    "sent": "Both the number of functions that we combine and the L1L1 ball.",
                    "label": 0
                },
                {
                    "sent": "In order to deal with the number of functions you know to use that that sort of theory.",
                    "label": 0
                },
                {
                    "sent": "We measure complexity according to the size of the of the combination that the number of functions in a combination we need to clip our function.",
                    "label": 0
                },
                {
                    "sent": "So we worry about, you know the how large the larger values the functions have.",
                    "label": 1
                },
                {
                    "sent": "So we define this clipping function.",
                    "label": 0
                },
                {
                    "sent": "That takes the value of our function and always keeps it between minus, Lambda and Lambda, right?",
                    "label": 0
                },
                {
                    "sent": "So we never we never go above Lambda.",
                    "label": 0
                },
                {
                    "sent": "We never go below minus, Lambda and then we look at the fire risk of this thing and we ensure that it approaches the optimal fire risk.",
                    "label": 0
                },
                {
                    "sent": "And that tells us that the risk right, the probability of misclassification of the clipped guy approaches the optimal well the risk of the clip guy is the same as the risk of the of the original.",
                    "label": 0
                },
                {
                    "sent": "The unclipped right because we're just thresholding here.",
                    "label": 1
                },
                {
                    "sent": "OK, we work with this so that we can get a handle on its its complexity using the VC sort of theory.",
                    "label": 0
                },
                {
                    "sent": "OK and of course in order to have this true well we really do need to allow Lambda end to go to Infinity if the conditional probabilities are close to zero or one somewhere then we need very big values in magnitude of the function, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to have this Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Getting large, but you know we'll see.",
                    "label": 0
                },
                {
                    "sent": "But that's not that's an innocuous requirement.",
                    "label": 0
                },
                {
                    "sent": "OK, so so in looking at viewing these functions were working with as combinations of T functions from the basis class we could just use the standard results involving the VC dimension of these basis functions to say that the fire risk is not much bigger than the empirical fire risk.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And yeah, where this is the notation I'm using here for empirical fire.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The sample average of the minus wife of ex.",
                    "label": 0
                },
                {
                    "sent": "The clipping isn't changing things very much OK, because we're clipping these functions.",
                    "label": 0
                },
                {
                    "sent": "If we consider the exponential loss where clipping down at minus Lambda and Lambda up here clipping at Lambda is helping us, possibly a lot right on the on the negative side.",
                    "label": 0
                },
                {
                    "sent": "On the positive side, clipping at Lambda is only hurting us by maybe either the minus Lambda, maybe as much as that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so clipping doesn't hurt very much.",
                    "label": 0
                },
                {
                    "sent": "And there's a whole lot of detail here that I'm skipping over.",
                    "label": 0
                },
                {
                    "sent": "This is really.",
                    "label": 0
                },
                {
                    "sent": "The the main bulk of the proof in fact is in this in this step, but let me just say that you know there is that you can use some nice properties that the exponential function has to apply without without.",
                    "label": 0
                },
                {
                    "sent": "Without any additional assumptions, like stepsize assumptions to apply a numerical convergence result of vehicle and writ of to say that we after T steps, we do almost as well as some function in.",
                    "label": 1
                },
                {
                    "sent": "I should define this in the Lambda scaled convex Hull of our class, right?",
                    "label": 0
                },
                {
                    "sent": "So so we this result tells us that after T steps we're doing almost as well as a function in an L1 ball in parameter space, OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We have to.",
                    "label": 0
                },
                {
                    "sent": "I said VC theory is not really the case at this step.",
                    "label": 0
                },
                {
                    "sent": "We use this the size of the 01 ball as the measure of complexity to tell us that.",
                    "label": 0
                },
                {
                    "sent": "That once again.",
                    "label": 0
                },
                {
                    "sent": "The empirical fire risk for this guy who's in who's in a small ball is close to the true risk, so we need to use both notions of complexity to get to get a result of this form and then by playing around with all the parameters, making things go to zero or Infinity appropriately, we have the result right, and the crucial point is back here.",
                    "label": 1
                },
                {
                    "sent": "OK, we need this term to go to zero, which means that T needs to grow.",
                    "label": 0
                },
                {
                    "sent": "We want T log T / N and there's something else here that's.",
                    "label": 0
                },
                {
                    "sent": "That's going to 0 slow going to Infinity slowly, but basically T log T / N needs to go to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, that's where the sub linear thing comes in.",
                    "label": 0
                },
                {
                    "sent": "So you could refine it from sub sub linear to this.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Faster than in log in and over login.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's that's it.",
                    "label": 0
                },
                {
                    "sent": "So let me let me finish with a few open problems.",
                    "label": 0
                },
                {
                    "sent": "All of this is for the exponential cost function, right?",
                    "label": 0
                },
                {
                    "sent": "So without without any modification, Adaboost has this nice asymptotic property.",
                    "label": 0
                },
                {
                    "sent": "What about other cost functions?",
                    "label": 0
                },
                {
                    "sent": "You know people have proposed the logic function and many others.",
                    "label": 0
                },
                {
                    "sent": "You know the exponential function seems to seems to really be well suited, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's very easy to work with the difficulty with using something like this is we don't have as nice a handle on the second derivative of the function we want.",
                    "label": 0
                },
                {
                    "sent": "We want the second derivative to be large whenever the risk is large, and that's not the case for such a function.",
                    "label": 1
                },
                {
                    "sent": "We've given a result in terms of plus, minus one value basis functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's also been versions of Adaboost proposed that involve real value basis functions.",
                    "label": 0
                },
                {
                    "sent": "We can't say our result does not apply.",
                    "label": 0
                },
                {
                    "sent": "In that case, the same issue arises, right?",
                    "label": 1
                },
                {
                    "sent": "The relationship between the second derivative in the direction of a basis function and the empirical risk virus criterion is just.",
                    "label": 1
                },
                {
                    "sent": "Is just true because we have these plus or minus one value functions, so you know again this is an extension that we don't know how to do when you look at the actual rates, assume some approximation rate and then say now what's the optimal.",
                    "label": 0
                },
                {
                    "sent": "What's the actual rate of convergence of the of the risk to the the Bayes risk?",
                    "label": 0
                },
                {
                    "sent": "The rates that pop out of this method of proof, so we have an asymptotically result.",
                    "label": 0
                },
                {
                    "sent": "It actually gives explicit rates.",
                    "label": 0
                },
                {
                    "sent": "You know constants at all, but the rates that pop out are really pretty awful, and the bottleneck is in the numerical results, right?",
                    "label": 1
                },
                {
                    "sent": "With the rate of decrease of the empirical fire risk to to some comparison for the empirical risk for some comparison function.",
                    "label": 0
                },
                {
                    "sent": "So we're appealing there too.",
                    "label": 0
                },
                {
                    "sent": "To a previous work of Bickleton ripped off and that gives a one over square root log T right right as the number of iterations T well, this is.",
                    "label": 0
                },
                {
                    "sent": "This is really terrible.",
                    "label": 0
                },
                {
                    "sent": "And that everything else seems well behaved.",
                    "label": 0
                },
                {
                    "sent": "It's really just the numerical part that lets us down.",
                    "label": 0
                },
                {
                    "sent": "This really seems pleasant.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick, it seems likely that it could be improved, but we don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "Alright, are there any questions?",
                    "label": 0
                },
                {
                    "sent": "That's the result depend on where there is a base learner optimized desktop problem.",
                    "label": 0
                },
                {
                    "sent": "Essentially, that's that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I didn't mention that.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "I'm assuming what I presented as the Adaboost algorithm was the base learner was producing the optimum right?",
                    "label": 0
                },
                {
                    "sent": "The optimal function, the?",
                    "label": 0
                },
                {
                    "sent": "The minimizer, it turns out that you can relax that that the result is also true if you if you just.",
                    "label": 0
                },
                {
                    "sent": "If the base learner produces a function that is within some constant factor of the optimum.",
                    "label": 0
                },
                {
                    "sent": "Right and then everything goes through.",
                    "label": 0
                },
                {
                    "sent": "It goes through it away.",
                    "label": 0
                },
                {
                    "sent": "That's curiously also dependent on the form of the exponential loss, right?",
                    "label": 0
                },
                {
                    "sent": "So it's funny.",
                    "label": 0
                },
                {
                    "sent": "You know that if we want to relax the algorithm to one that that.",
                    "label": 0
                },
                {
                    "sent": "Involves just a weak learner that's really rather weak then.",
                    "label": 0
                },
                {
                    "sent": "There would be another point in the proof where things breakdown if we if we move away from the exponential criterion so you know everything seems to be tide to the exponential criterion is sort of strange that the exponential loss function in a strange way.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "It can be.",
                    "label": 0
                },
                {
                    "sent": "Restated for the everything here is true when the weak learner gets within a constant factor of optimum.",
                    "label": 0
                },
                {
                    "sent": "'cause I'm wondering with this and ultimate proofing proof technique to involve the tool to take this exponential loss actually should do a log in front of it and go to the dual.",
                    "label": 0
                },
                {
                    "sent": "It becomes minimizing the relative entropy subject to linear constraint, which is been very well studied.",
                    "label": 0
                },
                {
                    "sent": "And you know you may be able to prove convergence in that domain and then right?",
                    "label": 0
                },
                {
                    "sent": "And the logical is hardly any different from the other.",
                    "label": 0
                },
                {
                    "sent": "One is just right, so that might that might work to give us, you know something, some improvement in the numerical part, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess what we?",
                    "label": 0
                },
                {
                    "sent": "What we're doing here is not worrying about you know the convergence of the empirical criterion, that that's one step in the proof, right?",
                    "label": 0
                },
                {
                    "sent": "The convergence of the empirical fire risk to its optimal value.",
                    "label": 0
                },
                {
                    "sent": "Right, you know the the.",
                    "label": 0
                },
                {
                    "sent": "The most of the work is in is, I guess in applying a result of that form.",
                    "label": 0
                },
                {
                    "sent": "But you know this is one step in the whole process, but it's true.",
                    "label": 0
                },
                {
                    "sent": "It may be that we could get that we could use alternative techniques to look at that kind of convergence to get a result or result of this form.",
                    "label": 0
                },
                {
                    "sent": "By sensor and then when you minimize the diversion center to demonstrate some convergence properties.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean I don't know about that book.",
                    "label": 0
                },
                {
                    "sent": "I would be interested to see it the the big and rich of result is the only one that I know of that actually gives you a suitable.",
                    "label": 0
                },
                {
                    "sent": "Convergence property with respect to a comparison function, so all of the other techniques that I'm aware of.",
                    "label": 0
                },
                {
                    "sent": "Involve.",
                    "label": 0
                },
                {
                    "sent": "Constraints.",
                    "label": 0
                },
                {
                    "sent": "Don't apply to this kind of.",
                    "label": 0
                },
                {
                    "sent": "An algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "The greedy, the forward stepwise optimization that Adaboost uses.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But it may be that you know considering it as considering this dual optimization problem, you may be able to.",
                    "label": 0
                },
                {
                    "sent": "Find some property of such a greedy scheme in the jewel.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "Be careful, his excellent post.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Beautiful.",
                    "label": 0
                },
                {
                    "sent": "Tiffany thank you, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So after you see one more piece that you have your summer, you can have your summer better.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "Thank you this is you know for me and I'm, you know, from solid National University and what I am going to present here is about the implementation of a condom machines with daily molecules and this is kind of computing which uses the molecules as competing materials and is also used.",
                    "label": 0
                },
                {
                    "sent": "May tell Kenny Computing an the objective of my research is make a model of.",
                    "label": 0
                },
                {
                    "sent": "Concrete fires in which can be implemented with DNA molecules, and here are my contents and easier.",
                    "label": 0
                },
                {
                    "sent": "That process is of the.",
                    "label": 0
                },
                {
                    "sent": "Is needed in gaining the corner case buyers and some pictures.",
                    "label": 0
                },
                {
                    "sent": "Can explain my conversions and this is pretty much like the machines in software solutions that data in some space and we make contact from them and make some optimization.",
                    "label": 0
                },
                {
                    "sent": "Optimizing process and finally despiteful is classified as blue an in game content machines.",
                    "label": 0
                },
                {
                    "sent": "The data included a scaling molecules, an hybrid.",
                    "label": 0
                },
                {
                    "sent": "We need to hybridize station to make corner an after some iterative selections we can make the distinction of single strength.",
                    "label": 0
                },
                {
                    "sent": "An if we put white once because this fight once more to the Brewers.",
                    "label": 0
                },
                {
                    "sent": "This is this is disconnected, you blue.",
                    "label": 0
                },
                {
                    "sent": "And here is my contact again.",
                    "label": 0
                },
                {
                    "sent": "The color element is intended to prevent the simulated between DNS change and this is defined as like this.",
                    "label": 0
                },
                {
                    "sent": "The corner element ID is defined as the.",
                    "label": 0
                },
                {
                    "sent": "The amount of double Saints who Jerry much are single stranded DNA index.",
                    "label": 0
                },
                {
                    "sent": "I an complementary option English sent the next Friday and I took.",
                    "label": 0
                },
                {
                    "sent": "I explained this again later and here is my hybrid hybrid model.",
                    "label": 0
                },
                {
                    "sent": "When hyper hyper you know cause the energy changes and the amount of energy change is.",
                    "label": 0
                },
                {
                    "sent": "Take created by summing each pairs energy change, which is a scientist minus 7.2, four, 80 and Cape Air and minus 9, four CJ&GC pair and my and minus 5.3 four orders and with some definition some pages of entropy and with some definition of.",
                    "label": 0
                },
                {
                    "sent": "Change your favorite case.",
                    "label": 0
                },
                {
                    "sent": "We finally get the amount of hybridization and amount of nature and so this we make the image the 1st order differential equation.",
                    "label": 0
                },
                {
                    "sent": "Anne here here.",
                    "label": 0
                },
                {
                    "sent": "The first I hit here is the first.",
                    "label": 0
                },
                {
                    "sent": "How the corner is made and we first synthesized anywhere, gives operating today to like this and some of them may be labeled as plus one and some of them may be leveled.",
                    "label": 0
                },
                {
                    "sent": "It's minus one an.",
                    "label": 0
                },
                {
                    "sent": "This can be included like this, for example an another one can be encoded as like this and we make the complementary sequence for this and.",
                    "label": 0
                },
                {
                    "sent": "And this should be included like this, cause the compliment to go even one is.",
                    "label": 0
                },
                {
                    "sent": "Input it like this and they make perfect complementary and after we doing hybridization we finally get to corner and put into our definition the Continental 2 one is.",
                    "label": 0
                },
                {
                    "sent": "Get an ace captain by the amount of double change of these Ann.",
                    "label": 0
                },
                {
                    "sent": "This element is gotten by the amount of docs change, which is composed of these two elements.",
                    "label": 0
                },
                {
                    "sent": "And definitely we have no guarantee that this corner is positive definite and the problem is that the 1st order differential equation is difficult to make.",
                    "label": 0
                },
                {
                    "sent": "The General general condition for the positive definiteness of our corner.",
                    "label": 0
                },
                {
                    "sent": "However, we have some hints that we can make put different corner though for City states if we in some high temperature where the little diagonal anyone produces we can make.",
                    "label": 0
                },
                {
                    "sent": "Diamonite dominant men case, which is definitely positive definite.",
                    "label": 0
                },
                {
                    "sent": "An surely this this is.",
                    "label": 0
                },
                {
                    "sent": "This could be based pass and the Secondly second hint is that with some of submission and with some appropriate control temperature we can get pushed.",
                    "label": 0
                },
                {
                    "sent": "It definitely corner and I will explain this here and the first ocean depth ocean.",
                    "label": 0
                },
                {
                    "sent": "Very simple models.",
                    "label": 0
                },
                {
                    "sent": "Which is constant speed hybrid hybridization an the hybridization of course with constant with.",
                    "label": 0
                },
                {
                    "sent": "We will concentrate see before some.",
                    "label": 0
                },
                {
                    "sent": "With constant speed, see below.",
                    "label": 0
                },
                {
                    "sent": "Some temperature key, an above this temperature no hybridization of course, and this is our solution over contested hybrid daily station and the special temperature K is determined by the energy.",
                    "label": 0
                },
                {
                    "sent": "Binding energy and if we control temperature like this, constantly decrease from key 12TB we guess.",
                    "label": 0
                },
                {
                    "sent": "We can calculate each element on any query and here is the.",
                    "label": 0
                },
                {
                    "sent": "Here is the amount of double stage and actually I put some other variables here which is that.",
                    "label": 0
                },
                {
                    "sent": "Actually I inserted common SQL Server links M While the origonal consist length, is there an the corner can be changed so much differently and here we have another layer would be which represents the average binding energy of Commerce equals an.",
                    "label": 0
                },
                {
                    "sent": "This can be disabled.",
                    "label": 0
                },
                {
                    "sent": "Can help from minus 7.22.",
                    "label": 0
                },
                {
                    "sent": "Nine point circle and from this we can get some pound fish guarantees the positive definiteness an we 2 pounds.",
                    "label": 0
                },
                {
                    "sent": "We can get this.",
                    "label": 0
                },
                {
                    "sent": "Fingers and hoping to pick values that are some.",
                    "label": 0
                },
                {
                    "sent": "If some different figures which like this and like this, and this is somewhere in between an if we control the temperature from this region about some T one and two this region, we guarantee that we can get the definite corner.",
                    "label": 0
                },
                {
                    "sent": "With this activities, this Dior hint and actually we must make the hybrid agent dynamics and.",
                    "label": 0
                },
                {
                    "sent": "I think we make Edgware don't go through in detail because of the time limit and the.",
                    "label": 0
                },
                {
                    "sent": "Patient can be efficient like this and we can serve the first or the differential equations and so this we finally we can get this kind of videos and these figures.",
                    "label": 0
                },
                {
                    "sent": "The rag might increase fire with the corner previously.",
                    "label": 0
                },
                {
                    "sent": "Unquote an this figure is the figure that we control the temperature constantly at very high temperature and this shows the dominant video and which is over 58 and we cannot determine in these points and this is the figure that the temperature is quote from 60 degrees to 30 TV and this shows so much better figure.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "Match the video and this is the figure somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "And this is the picture that we control the temperature at.",
                    "label": 0
                },
                {
                    "sent": "We kept the temperature at 3030 degree an we failed to make the positive definite corner and disappear.",
                    "label": 0
                },
                {
                    "sent": "Picture we have got an actually.",
                    "label": 0
                },
                {
                    "sent": "You know that in Gaussian corner we can get similar figures an according to corner parameters and then finally to stay small.",
                    "label": 0
                },
                {
                    "sent": "We can get this type of video and appropriate.",
                    "label": 0
                },
                {
                    "sent": "You can just type or figure an here we can know from this test.",
                    "label": 0
                },
                {
                    "sent": "The temperature control can be used as an animator.",
                    "label": 0
                },
                {
                    "sent": "Seemed encounter machines.",
                    "label": 0
                },
                {
                    "sent": "Anne here, until now we I spend over the preceding text about Connors an.",
                    "label": 0
                },
                {
                    "sent": "We'd like to optimize some parameters to operate this kernel to discrimination an.",
                    "label": 0
                },
                {
                    "sent": "The ultimatum to conventional optimization method of large machines is additional very complex and difficult, so it is very difficult to implement it with to be implemented with the.",
                    "label": 0
                },
                {
                    "sent": "Then America's because training workers has only simple operations like hybridization, anti nature, an.",
                    "label": 0
                },
                {
                    "sent": "So I pushed I. I suggest another method, Java with meditation and this is for performance.",
                    "label": 0
                },
                {
                    "sent": "Really sticks an.",
                    "label": 0
                },
                {
                    "sent": "Where after we talked if Lee.",
                    "label": 0
                },
                {
                    "sent": "Update this corner.",
                    "label": 0
                },
                {
                    "sent": "According to this equation, and this is the case.",
                    "label": 0
                },
                {
                    "sent": "Update The corner which is denoted as K subscript K and this is.",
                    "label": 0
                },
                {
                    "sent": "This can be gotten from summing up or corner elements whose love is different from the rabbits and.",
                    "label": 0
                },
                {
                    "sent": "Why we update this corner?",
                    "label": 0
                },
                {
                    "sent": "We can get some parameters from the updated corner an with these parameters.",
                    "label": 0
                },
                {
                    "sent": "We can do discrimination and I explain this again.",
                    "label": 0
                },
                {
                    "sent": "This is Polly Reader that I operate the kernel text message an at 4C.",
                    "label": 0
                },
                {
                    "sent": "At Dell's update it is donkey berries uniform and this video source.",
                    "label": 0
                },
                {
                    "sent": "Plus an update goes on.",
                    "label": 0
                },
                {
                    "sent": "The discrimination region finds the appropriate region an from.",
                    "label": 0
                },
                {
                    "sent": "From this picture, the training is settled by the.",
                    "label": 0
                },
                {
                    "sent": "Product key approaches to the parameters that is consumed from randomizing classifier optimized by randomizing classifier an.",
                    "label": 0
                },
                {
                    "sent": "This kind of figure is the correlation of exactly 1.",
                    "label": 0
                },
                {
                    "sent": "And this piece of text messages can be implemented in a market like this, and this is very simple.",
                    "label": 0
                },
                {
                    "sent": "If we this is a corner made by hybridization, and if you choose the double standard molecules of different levels, for example.",
                    "label": 0
                },
                {
                    "sent": "Much pressure, massive an which elements are minus impressive reselling photo storage trains an put on put on another test tube and we denature an eye hybridize again.",
                    "label": 0
                },
                {
                    "sent": "Then we get the updated Connor.",
                    "label": 0
                },
                {
                    "sent": "This is the simple process of an update and.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "An at the case of date you can give him for CIPK which can be which represents the population of single strange the ice English change and we can get we have the.",
                    "label": 0
                },
                {
                    "sent": "Number of people meters Anne Anne with this parameters if we do.",
                    "label": 0
                },
                {
                    "sent": "If we put a new data and count.",
                    "label": 0
                },
                {
                    "sent": "Which library of science is?",
                    "label": 0
                },
                {
                    "sent": "But it's more we.",
                    "label": 0
                },
                {
                    "sent": "King get the discriminated level.",
                    "label": 0
                },
                {
                    "sent": "And here is some agent provide a major data which is a very easy problem and this is a gene expression data.",
                    "label": 0
                },
                {
                    "sent": "I included this as the high efficiency and low expression HA an.",
                    "label": 0
                },
                {
                    "sent": "Brake disc including we got the running cover like this an update commensals got very.",
                    "label": 0
                },
                {
                    "sent": "Capital P approach to the target performance an.",
                    "label": 0
                },
                {
                    "sent": "Here this is the figure that I put.",
                    "label": 0
                },
                {
                    "sent": "1st first we started with little with little amounts of data an.",
                    "label": 0
                },
                {
                    "sent": "Continually we put some amount of data again and again and again.",
                    "label": 0
                },
                {
                    "sent": "An actor shows that.",
                    "label": 0
                },
                {
                    "sent": "It is very quick recovery to the performance and the overall performance increases as the training data increases an.",
                    "label": 0
                },
                {
                    "sent": "This is the performance of a corner update and this is slightly lower than subject matter machine, which is large margin classifier and if we constructed this equation has no bias term.",
                    "label": 0
                },
                {
                    "sent": "Compared with SVM with no bias, which is the performance of this one, this is not very.",
                    "label": 0
                },
                {
                    "sent": "Basically, yeah, so this is the end of my presentation and please give some comments or questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Other speakers present OK so thanks for that.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Use.",
                    "label": 0
                },
                {
                    "sent": "Turn off with the SVN some fairly.",
                    "label": 0
                },
                {
                    "sent": "Design Colonel yeah I designed a corner.",
                    "label": 0
                },
                {
                    "sent": "He kicked an eel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have been getting Neil Neil.",
                    "label": 0
                },
                {
                    "sent": "You know this is a simulator and this is designed for in vitro experiment an.",
                    "label": 0
                },
                {
                    "sent": "Cause in between extent we cannot use the conventional method of making corner an optimization method and the corner itself is designed OK and the optimization process is again.",
                    "label": 0
                },
                {
                    "sent": "So we both knew here.",
                    "label": 0
                },
                {
                    "sent": "So you just narrow down the regions for anneal temperature?",
                    "label": 0
                },
                {
                    "sent": "Yeah please.",
                    "label": 0
                },
                {
                    "sent": "Under some simplified condition and I put the consequences to every sequence to make.",
                    "label": 0
                },
                {
                    "sent": "It makes the region never so qualify in the graph.",
                    "label": 0
                },
                {
                    "sent": "I showed the axis is the length of.",
                    "label": 0
                },
                {
                    "sent": ", ice, encourages the temperature region.",
                    "label": 0
                },
                {
                    "sent": "'cause yeah.",
                    "label": 0
                },
                {
                    "sent": "About",
                    "label": 0
                }
            ]
        }
    }
}