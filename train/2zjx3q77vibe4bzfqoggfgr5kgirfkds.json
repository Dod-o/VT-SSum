{
    "id": "2zjx3q77vibe4bzfqoggfgr5kgirfkds",
    "title": "Measuring the Similarity between Implicit Semantic Relations from the Web",
    "info": {
        "author": [
            "Danushka Bollegala, University of Tokyo",
            "Yutaka Matsuo, University of Tokyo",
            "Mitsuru Ishizuka, University of Tokyo"
        ],
        "published": "May 20, 2009",
        "recorded": "April 2009",
        "category": [
            "Top->Computer Science->Web Mining",
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/www09_bollegala_mtsisr/",
    "segmentation": [
        [
            "So I'm Danish, people regular from the University of Tokyo and I'm going to present our work on measuring the similarity between implicit semantic relations, the different type of similarity using web search engines, and these are collaborative work with you.",
            "Takamatsu and Mitsui Jessica."
        ],
        [
            "So before going into details I would first like to elaborate on two types of similarity measures.",
            "We have attributional and relational similarity measures, so I will first explain these two types of measures.",
            "First, let's look at Attribution similarity, so Attribution similarities.",
            "The correspondence between attributes of two objects.",
            "For example, here you have Jaguar and Cat, and by looking at the pictures, you see that there they are.",
            "Apparently quite similar.",
            "So why are they similar?",
            "Because they have similar types of attributes.",
            "For example, Jaguar has is a carnivorous animal and a mammal.",
            "Ann has four legs, and similarly, if you look at the cat, it is also fundamentally carnivorous.",
            "Animal, animal, Mammal, Ann has full legs, so this type of attributes are common to these two objects, so we would say that these two objects are these two words or concepts.",
            "They have a high degree of Attribution similarity, however the main in the focus of this talk is going to be relation similarity.",
            "So in this case of Attribution similarity, we have a function of two para meters where the the two 2 arguments X&Y corresponds to the two things that we are interested in.",
            "Measuring the similarity, on the other hand, the relational similarity is."
        ],
        [
            "Mine does the correspondence between relations that exist between two entity or object pairs.",
            "For example, let's take the pair ostrich bird and the pair lion cat.",
            "So we know that ostriches quite a large bird.",
            "Actually the largest on Earth and at the same time Lion is a large cat.",
            "So the relation exists.",
            "A large Y is common between the two words.",
            "In each of these word pairs.",
            "So we would say the ostrich bird.",
            "Here and the lincat pair has a high degree of relational similarity.",
            "So.",
            "In theory, if you have if you have a lot of analogical things, then you would expect to find a high degree of relational similarity.",
            "Whereas if you have synonymous things, you know it was word.",
            "For example, you would expect to have a high degree of Attribution similarity, so in this case, if you want write it down, write this down as a similarity function.",
            "Then it would be a function of two arguments.",
            "In this case the full words that appear in the two word pairs.",
            "So what are the use?"
        ],
        [
            "This offer measuring relational similarity.",
            "This has been extensively studied in natural language processing community, mainly to extract the analogies from natural language text.",
            "For example, we know that traffic flows in the road whereas water flows in a pipe.",
            "So these type of analogies have been extracted.",
            "Methods have been proposed to extract these top panel edges from natural language text Ann.",
            "It's also quite helpful in order to classify the.",
            "Uh.",
            "Not modifier an noun words according to the relation that exists between the noun and the head down, and the modifier.",
            "For example, if you have laser printer, is an instrument which using laser and you have concert Hall.",
            "The purpose of the Hall is to have concerts and you have student discounts.",
            "The beneficiary of the discount is a student and a more interesting, more web oriented application of relation similarity potential application.",
            "I would say of relation similarity would be.",
            "To search using implicit relations, for example, say you are given to a word pair a B and another word C An you are expected to find D such that a B&CDR and the largest.",
            "For example, if you have a BS, the Christian Bible Anuar.",
            "Given CS myslym, you'd want to find the Quran SD.",
            "So this type of searches are quite different from what we're used to in the keyword based search, so these are potential applications of relation simuel."
        ],
        [
            "The problem of relation similarity is not anywhere new IT has been studied in the context of artificial intelligence under structure mapping theory.",
            "So in AI community analogy is defined as a mapping between two knowledge.",
            "Domains one is called the target, another the one that is being mapped is called 1 install source and the other one is called the target and analogy is defined as a mapping between these two domains.",
            "So in the context of structure mapping theory you would not map attributes, but we would only map relations between the source and the target.",
            "For example, if you have booze rutherfords.",
            "Nuclear model atomique models we have electrons revolving around the nucleus.",
            "An assumed logic would be the planets revolving around the sun.",
            "So the relation that X revolves around why is mappable between these two domains and which relations gets mapped is determined by the systematic principle.",
            "So relation similarity has quite a big history and it has been studied in various contexts."
        ],
        [
            "However, there are several challenges that one must face if if that person is to measure relational relation similarity accurately to begin with, we are given only implicit in facilitated relations, stated using word pairs.",
            "So you would first need to find out what are the relations that hold between the two words in a word pair.",
            "There can be multiple relations as well as.",
            "Ambiguous relations.",
            "So the first step of measuring relation similarity would be to, in some out some way, to explicitly state the relations that exist between the two words.",
            "So in this work we are focusing on lexical patterns as a method to explicitly state the implicitly stated relations.",
            "The second problem is.",
            "How to identify the different ways of saying expressing the same relation?",
            "For example, the question relationship between two companies.",
            "If you are searching for this relation you will find a lot of different lexical patterns on the web.",
            "Some patterns might say it's acquired, why?",
            "So I question why?",
            "Or another another place it would be stated as why is bought by X.",
            "So you would need some way to know that these different ways of saying the same relation in order to accurately measure the relation similarity.",
            "So in this work we are.",
            "Looking in a clustering approach to identify the different ways of saying the expressing the same semantic relation.",
            "And the next difficult challenges.",
            "Semantic relations are not always independent.",
            "For example, if you take the easy relation and has relation.",
            "So if you say Australia is a bird, then it somehow follows that ostrich must have feathers.",
            "So these type of relations are generally not independent.",
            "So when you are measuring relational similarity between two word pairs, you should have some way of knowing that what relations are independent and which degree there independent.",
            "So in this work we are.",
            "Looking in the distance metric which is not Euclidean and non Euclidean arnobius distance measure and.",
            "We also need to find out which relations contribute relation similarity.",
            "So although there are a lot of relations, for example, the end relation extend why so?",
            "These are very ambiguous and very common relation.",
            "It can mean anything.",
            "So we need to kind of down with these type of relations.",
            "So what we're taking the approach that we're taking here is a supervised approach, so we're trying to learn the Mahalanobis distance measure using training data.",
            "So for this approach we're using a previously proposed method calling information theoretic metric learning.",
            "So these are the different challenges that we must face and we will.",
            "I will explain how we're going to address each of these challenges in the."
        ],
        [
            "Slides to come.",
            "So the first challenge is to how to explicitly say the relations between 2 words in a word pair."
        ],
        [
            "So for this, as I mentioned earlier, we're taking a lexical pattern based approach.",
            "For example, if you are interested in finding the relations between lion and Cat, we would issue a query that connects line and cat with these wild card operators.",
            "Typically this upper wild card operator would must an for one or none of words, so you would find if you issue this cure it for example, for Google, it will actually return the sniper, that is that is shown here.",
            "The line is a large list, large, heavily build social cat of open rocky areas in Africa.",
            "So if you read this snippet carefully, we find the relation that lion is a large cat.",
            "So, but the words are in here and there, and there are other other other modifiers as well.",
            "It is a social cat and heavily built, and we would like to extract the relation.",
            "X is a large white.",
            "So what we do here is we generate all subsequences from the snippets that includes the two words that we are interested in.",
            "For example in this case some of the some of the patterns that we will end up with these.",
            "A large way Xyx liked by off this type of different patterns.",
            "Of course.",
            "Generating sub sequences of a string is very costly operation so we take we are taking this prefix span algorithm which is efficient and it can also consider gaps so we can specify the length of a gap that we are considering.",
            "Of course this approach is noisy and it generates sort of misspellings.",
            "An ungrammatical sentence, especially because the snippets are fragmented."
        ],
        [
            "So we need to identify the different patterns that talk about the same semantic relation and a very robust way.",
            "So for that purpose."
        ],
        [
            "Yeah, proposing a clustering algorithm to cluster the lexical patterns so the previous approach generates around 750,000 patterns in our training data for the training data that we experimented.",
            "And not all these lexical patterns describe independent the semantic relations.",
            "So we need to identify the different lexical patterns that express the same semantic relation.",
            "So in order to do this, we are using another quite quite old and very, very well used.",
            "Idea called distribution hypothesis.",
            "So regional distribution hypothesis states that if the context of words are similar, then the words themselves should be similar.",
            "So we are having a kind of extension of this hypothesis.",
            "We say the patterns are distributed equally across the same set of word.",
            "Pairs must be similar, so.",
            "So to test this approach, I will first show."
        ],
        [
            "For example, so here we have full lexical patterns.",
            "It's vice why acquires Y by Cor fix an why chief Executive X?",
            "So these patterns if a human look at these patterns.",
            "Intuitively, that person would know that expires Vianex acquires.",
            "Why talks about the same relation whereas the latter two patterns also talk about same relation.",
            "And I've shown the distribution of these two patterns across word pairs and the frequency as we normalized.",
            "So we see that the two first 2 patterns have a high overlap.",
            "Whereas the latter two patterns also have a high overlap.",
            "So even if you just measure the similarity using for something like cosine similarity, you would get a high similarity value for the first 2 patterns and the last two patterns so."
        ],
        [
            "Using this idea here in the distributional hypothesis to measure the similarity between lexical patterns and now we have a similarity measure.",
            "The next goal would be to cluster the patterns.",
            "So considering the number of patterns, it's quite high.",
            "It's not feasible to do a pairwise comparison as required by, for example, agglomerative clustering.",
            "So here we are looking at a more sequential greedy pattern clustering algorithm."
        ],
        [
            "So here is the outline of the clustering algorithm.",
            "The details are given in the paper, so I will briefly explain the algorithm.",
            "So what we do is we first sort the patterns according to their total frequency in all word pairs.",
            "So this gives us the more common the commonly occurring relations and the patterns that are represented by those relations in our data set.",
            "And then we take each pattern and we have a threshold value.",
            "And what we do is if we have if we find a cluster that is similar to this pattern.",
            "Greater than the rather than the threshold value, we will add the pattern to the cluster or otherwise.",
            "We will formalize a new cluster using that pattern.",
            "So we would repeat this process until we're done with all the patterns.",
            "So it's a very simple algorithm.",
            "It has one perimetre the clustering threshold, so this procedure.",
            "Has a several.",
            "Nice properties to begin with, but it can be done in a linear time with the number of patterns, so we only needed one pass through the set of patterns and because we are sorting the patterns according to the frequency we get the more general and the more common the commonly occurring relations at the beginning and we get the outliers at the end.",
            "So this way we can if you want to filter out the outliers.",
            "This would be a nice property and there's only one para meters across threshold that we need to do now.",
            "Cross validate or whatever you want to do it with it and it doesn't need to need the number of clusters in advance, and because of the sequential nature of the algorithm, it inherits their voice all pairwise comparisons, which can be costly with a large number of lexical patterns.",
            "But it is a greedy clustering algorithm."
        ],
        [
            "So so far we have extracted the relations.",
            "We are lexical patterns that describe the implicit relations in a word pair, and we have also done clustering of lexical patterns that gives us which patterns are semantically similar.",
            "So the next challenge is that we must solve how to account for the interdependence between semantic relations.",
            "For example, as I explained earlier, the Easter relation enhance relation and the next challenge that we must face is how to compute the relation similarity out of the clusters that we have.",
            "That you have created.",
            "So to solve this."
        ],
        [
            "Challenges we are proposing.",
            "The following approach, where we first represent a bird pair using a feature vector so that it's in dimensional feature vector where each cluster produces.",
            "Feature, so we're taking all the all the patterns that were required for a specific word pair, and we would aggregate all the pattern frequencies that correspond to the cluster.",
            "So this would give us a feature representation vector representation of the word pair, and then we're taking a supervised approach where we try to learn the distance.",
            "Or in this case mabius distance between word pairs.",
            "So here.",
            "Yeah, you are utilizing a label.",
            "The data set of positive and negative instances where we have labeled that these two word pairs are relational, similar and these two word processor relationally not similar.",
            "And once you have done that you can you can we can compute the Mahalanobis distance between the two feature vectors that correspond to the two word pairs using the equation shown here.",
            "So the whole training process boils down to computing the.",
            "The modern, obvious metrics A so this can be done quite efficiently using a recently proposed Mahalanobis distance metric metric learning algorithm, which is called ihtml information theoretic metric learning algorithm, and it has nice properties.",
            "For example, you don't need to do costly eigenvalue eigenvector computations and it can be scalable to larger datasets using low rank approximation of the melodious metrics, and it has also the nice properties that you can.",
            "Incorporates active variables.",
            "You don't have to.",
            "Credit classify all the instances in the data set, so the learning algorithm is detailed in the paper and for the limited availability of time, I would I would not go into the details of the of the learning algorithm here, but you have interested.",
            "You can read the paper.",
            "And."
        ],
        [
            "Next I will describe the various experiments that we did to evaluate the different steps and the proposals that we have made."
        ],
        [
            "For this purpose we have used two datasets.",
            "The first data set is called anti datasets that only contains the named entities.",
            "It has 100 entity pairs.",
            "Accumulated 45 different semantic relations.",
            "So we have acquired query relationship.",
            "For example Google and YouTube and we have a person birthplace relation for example Charlie Chaplin was born born in London and we have see you and company of the CEO relation company headquarters relation.",
            "An person feels personal relation is basically distinguished people who are known as authorities of that field.",
            "And we downloaded snippets using using a search engine and we would run the pattern extraction procedure for these snippets.",
            "We are also using another data set called S80 word energy data set which has been used extensively in previous work of relational similarity and this data set contains 374 S 80 word analogic questions.",
            "So basically these these questions have 5 multiple choices out of which only one is correct.",
            "And as the question you are given a word pair.",
            "And how to find the word pair that is most analagous to the question?",
            "So that's the task that is being solved that is being attempted in the second data set."
        ],
        [
            "And we do.",
            "We first do a relation classification experiment on the E&T data set.",
            "So here what we do is we measure the relation similarity between each of the instances in the data set and the rest of the rest of the entity pairs and we would rank the entity pairs according to their relation similarity with the seed and we then use a K nearest neighbor classification.",
            "In this experiment we took cast in and we would assign the label that you can find mostly.",
            "Among the the the key nearest neighbors as the label Relation label of the of the entity pair.",
            "And we repeat this process for all entity pairs in the data set, so we can model this problem as ranking problem.",
            "We are given a named entity pair you would like to find more relational similar entity pairs at the top.",
            "So we measure these using average precision, which is a common measure used in information retrieval to evaluate ranking problems."
        ],
        [
            "We compared with the vector space model based relational similarity where you don't do any clustering.",
            "The word pairs are represented as feature vectors of lexical pattern frequencies, and then the similarities measured using cosine similarity.",
            "So this would be a kind of a baseline, and also it's a previous work.",
            "It would give us idea whether the clustering actually helps and the second method is the state of our latent relation analysis.",
            "It uses singular value decomposition instead of clustering to reduce the dimensionality.",
            "And we have Euclidean distance between cluster vectors as the third baseline.",
            "So in this case we do not use the Mahalanobis distance.",
            "So implicitly it means you're using the unit metrics as the obvious metrics, which means there are relations independent.",
            "So this will give us an idea of not taking into consideration the correlation between semantic relations.",
            "And the last one is the proposed method, which which learns melodious distance metric measure using training data.",
            "So we have shown experimental results on the E&T data set.",
            "That you have five semantic relations and the different different methods that we have compared.",
            "So we see the proposed method obtaining high average precision scores for all the relation types and.",
            "And we do see Luo in performance on the person birthplace relation.",
            "This is mainly because the the material, for example the places there where singles, an actress first performed, was incorrectly extracted as their birth places so.",
            "Overall, we have a average precision of around 74%."
        ],
        [
            "And here I have shown the different clusters that we have extracted altogether.",
            "We extract around 6500 lexical pattern clusters and all these classes have more than two lexical patterns in them, so we don't have any singletons in this set.",
            "And for example, if you see the first cluster, it is mainly about the acquisition relationship.",
            "You have different lexical patterns that you can use to express acquisition relation, and we also see.",
            "The cluster #4, which is also talking about an acquisition but using different set of words.",
            "Several set up verbs in this case by confirm and also purchase.",
            "So we see the clusters themselves are not actually independent, so it makes sense to it.",
            "Kind of motivates the fact that we should need Mahalanobis distance to account for these type of intercluster correlation.",
            "And."
        ],
        [
            "Here I have shown the results on the essay T data set, so here, but we're trying to do is in the city examination questions word analogic questions as I described earlier.",
            "There are.",
            "There's one stem word pair which access the question and then there are for example typically 5 candidate.",
            "The answers out of which you had to choose one, and if you do it randomly, you have a chance of getting a 20% score because you have five choices.",
            "And there's a different previously proposed methods evaluated on this data set, as reported by the original papers, and we see the human performance on this task is also quite low.",
            "It has been a difficult task even for the native speakers to solve word analogic questions and state of the art method.",
            "Hillary has a score of 56% and the proposed method has around 51%.",
            "However, we also must.",
            "I must also note that it takes around 8:00 or 8 days actually, as described in the original paper to process the 374 word pairs, whereas the proposed method can produce results in less than six servers.",
            "So it's worth looking into why from where these differences come in."
        ],
        [
            "During the late relational analysis, if I briefly explain the algorithm here, which let's start with the metrics metrics of word pairs and lexical patterns, and the values of the elements, would be the frequency of a pattern occurring for the particular word pair and then a singular value decomposition is performed on these metrics.",
            "This is in itself is a quite a costly task because there around, say, individual paper there around like.",
            "8000 lexical patterns and around 2000 word pairs.",
            "So it's quite a large metrics to perform single value decomposition, and it also takes into consideration the cinnamon synonyms of the original words and also takes the average of relational similarity between all word pairs as the.",
            "The correct relation similarity between the word pairs that we are interested in.",
            "So this is the.",
            "Larry algorithm, and in this case, of course, if you have a number of.",
            "Lexical patterns then, in order to compute the relational similarity between two word pairs, you would at least need to a number of web queries because you have to substitute each word pair in lexical pattern and then issue a query to search engine and get the page counts.",
            "So this is quite a costly operation, whereas the proposed method only focuses on the snippets and searches.",
            "Lexical patterns only in the snippets, so it only needs to issue for example, two queries.",
            "To compute the relation similarity between one query, you cannot retrieve around 100 snippets.",
            "So and also it does not need singular value decomposition, which must be repeated if you are in an online setting where you have new word pairs coming in continuously will have to add them to the matrix and then perform singular decomposition and then compute the relation similarity.",
            "So these are the main benefits of the proposed method in terms of performance."
        ],
        [
            "In conclusion, there are certain takeaways from this talk first.",
            "We saw that distribution similarity over word pairs is a very good way of identifying lexical patterns that talk about the same semantic relation and.",
            "We saw that clustering lexical patterns prior to measuring similarity can help with the accuracy of the similarity measure and greedy sequential clustering algorithm efficiently avoided all pairwise comparisons.",
            "Ann is a good way of generating the common semantic relations ahead and also we saw that modern obvious distance outperforms Euclidean distance, mainly because semantic relations and also the clusters are not independent and our future work VR.",
            "Focusing on extracting the relations for using the proposed relation similarity measure for analogical search as the example that I gave at the beginning, so I would like to take any questions."
        ],
        [
            "Time for.",
            "At least you mentioned.",
            "So.",
            "The examples you gave.",
            "Similarities between relations will be detected for relationships are functional or at least partially into angle.",
            "Factor is not very different, so if you had, if you had like very network relations like for example knows the social network, is that observation right?",
            "That this would not apply to such very highly mixed with no.",
            "These are actually you should have the two words in a single sentence for in order for this to work.",
            "So it would not.",
            "Yes, yeah, yeah.",
            "So it's not.",
            "It does not consider the transitivity relations in this case.",
            "Yeah.",
            "Maybe I did not.",
            "Yes yes.",
            "So we can become actually to the list."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm Danish, people regular from the University of Tokyo and I'm going to present our work on measuring the similarity between implicit semantic relations, the different type of similarity using web search engines, and these are collaborative work with you.",
                    "label": 0
                },
                {
                    "sent": "Takamatsu and Mitsui Jessica.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before going into details I would first like to elaborate on two types of similarity measures.",
                    "label": 0
                },
                {
                    "sent": "We have attributional and relational similarity measures, so I will first explain these two types of measures.",
                    "label": 0
                },
                {
                    "sent": "First, let's look at Attribution similarity, so Attribution similarities.",
                    "label": 0
                },
                {
                    "sent": "The correspondence between attributes of two objects.",
                    "label": 1
                },
                {
                    "sent": "For example, here you have Jaguar and Cat, and by looking at the pictures, you see that there they are.",
                    "label": 0
                },
                {
                    "sent": "Apparently quite similar.",
                    "label": 0
                },
                {
                    "sent": "So why are they similar?",
                    "label": 0
                },
                {
                    "sent": "Because they have similar types of attributes.",
                    "label": 0
                },
                {
                    "sent": "For example, Jaguar has is a carnivorous animal and a mammal.",
                    "label": 0
                },
                {
                    "sent": "Ann has four legs, and similarly, if you look at the cat, it is also fundamentally carnivorous.",
                    "label": 0
                },
                {
                    "sent": "Animal, animal, Mammal, Ann has full legs, so this type of attributes are common to these two objects, so we would say that these two objects are these two words or concepts.",
                    "label": 1
                },
                {
                    "sent": "They have a high degree of Attribution similarity, however the main in the focus of this talk is going to be relation similarity.",
                    "label": 0
                },
                {
                    "sent": "So in this case of Attribution similarity, we have a function of two para meters where the the two 2 arguments X&Y corresponds to the two things that we are interested in.",
                    "label": 1
                },
                {
                    "sent": "Measuring the similarity, on the other hand, the relational similarity is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mine does the correspondence between relations that exist between two entity or object pairs.",
                    "label": 1
                },
                {
                    "sent": "For example, let's take the pair ostrich bird and the pair lion cat.",
                    "label": 0
                },
                {
                    "sent": "So we know that ostriches quite a large bird.",
                    "label": 1
                },
                {
                    "sent": "Actually the largest on Earth and at the same time Lion is a large cat.",
                    "label": 1
                },
                {
                    "sent": "So the relation exists.",
                    "label": 0
                },
                {
                    "sent": "A large Y is common between the two words.",
                    "label": 0
                },
                {
                    "sent": "In each of these word pairs.",
                    "label": 0
                },
                {
                    "sent": "So we would say the ostrich bird.",
                    "label": 1
                },
                {
                    "sent": "Here and the lincat pair has a high degree of relational similarity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In theory, if you have if you have a lot of analogical things, then you would expect to find a high degree of relational similarity.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you have synonymous things, you know it was word.",
                    "label": 0
                },
                {
                    "sent": "For example, you would expect to have a high degree of Attribution similarity, so in this case, if you want write it down, write this down as a similarity function.",
                    "label": 0
                },
                {
                    "sent": "Then it would be a function of two arguments.",
                    "label": 0
                },
                {
                    "sent": "In this case the full words that appear in the two word pairs.",
                    "label": 0
                },
                {
                    "sent": "So what are the use?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This offer measuring relational similarity.",
                    "label": 0
                },
                {
                    "sent": "This has been extensively studied in natural language processing community, mainly to extract the analogies from natural language text.",
                    "label": 0
                },
                {
                    "sent": "For example, we know that traffic flows in the road whereas water flows in a pipe.",
                    "label": 0
                },
                {
                    "sent": "So these type of analogies have been extracted.",
                    "label": 0
                },
                {
                    "sent": "Methods have been proposed to extract these top panel edges from natural language text Ann.",
                    "label": 0
                },
                {
                    "sent": "It's also quite helpful in order to classify the.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Not modifier an noun words according to the relation that exists between the noun and the head down, and the modifier.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have laser printer, is an instrument which using laser and you have concert Hall.",
                    "label": 1
                },
                {
                    "sent": "The purpose of the Hall is to have concerts and you have student discounts.",
                    "label": 0
                },
                {
                    "sent": "The beneficiary of the discount is a student and a more interesting, more web oriented application of relation similarity potential application.",
                    "label": 0
                },
                {
                    "sent": "I would say of relation similarity would be.",
                    "label": 0
                },
                {
                    "sent": "To search using implicit relations, for example, say you are given to a word pair a B and another word C An you are expected to find D such that a B&CDR and the largest.",
                    "label": 1
                },
                {
                    "sent": "For example, if you have a BS, the Christian Bible Anuar.",
                    "label": 0
                },
                {
                    "sent": "Given CS myslym, you'd want to find the Quran SD.",
                    "label": 0
                },
                {
                    "sent": "So this type of searches are quite different from what we're used to in the keyword based search, so these are potential applications of relation simuel.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of relation similarity is not anywhere new IT has been studied in the context of artificial intelligence under structure mapping theory.",
                    "label": 0
                },
                {
                    "sent": "So in AI community analogy is defined as a mapping between two knowledge.",
                    "label": 1
                },
                {
                    "sent": "Domains one is called the target, another the one that is being mapped is called 1 install source and the other one is called the target and analogy is defined as a mapping between these two domains.",
                    "label": 1
                },
                {
                    "sent": "So in the context of structure mapping theory you would not map attributes, but we would only map relations between the source and the target.",
                    "label": 1
                },
                {
                    "sent": "For example, if you have booze rutherfords.",
                    "label": 0
                },
                {
                    "sent": "Nuclear model atomique models we have electrons revolving around the nucleus.",
                    "label": 0
                },
                {
                    "sent": "An assumed logic would be the planets revolving around the sun.",
                    "label": 0
                },
                {
                    "sent": "So the relation that X revolves around why is mappable between these two domains and which relations gets mapped is determined by the systematic principle.",
                    "label": 0
                },
                {
                    "sent": "So relation similarity has quite a big history and it has been studied in various contexts.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there are several challenges that one must face if if that person is to measure relational relation similarity accurately to begin with, we are given only implicit in facilitated relations, stated using word pairs.",
                    "label": 0
                },
                {
                    "sent": "So you would first need to find out what are the relations that hold between the two words in a word pair.",
                    "label": 0
                },
                {
                    "sent": "There can be multiple relations as well as.",
                    "label": 1
                },
                {
                    "sent": "Ambiguous relations.",
                    "label": 0
                },
                {
                    "sent": "So the first step of measuring relation similarity would be to, in some out some way, to explicitly state the relations that exist between the two words.",
                    "label": 0
                },
                {
                    "sent": "So in this work we are focusing on lexical patterns as a method to explicitly state the implicitly stated relations.",
                    "label": 1
                },
                {
                    "sent": "The second problem is.",
                    "label": 0
                },
                {
                    "sent": "How to identify the different ways of saying expressing the same relation?",
                    "label": 1
                },
                {
                    "sent": "For example, the question relationship between two companies.",
                    "label": 0
                },
                {
                    "sent": "If you are searching for this relation you will find a lot of different lexical patterns on the web.",
                    "label": 0
                },
                {
                    "sent": "Some patterns might say it's acquired, why?",
                    "label": 0
                },
                {
                    "sent": "So I question why?",
                    "label": 1
                },
                {
                    "sent": "Or another another place it would be stated as why is bought by X.",
                    "label": 0
                },
                {
                    "sent": "So you would need some way to know that these different ways of saying the same relation in order to accurately measure the relation similarity.",
                    "label": 1
                },
                {
                    "sent": "So in this work we are.",
                    "label": 0
                },
                {
                    "sent": "Looking in a clustering approach to identify the different ways of saying the expressing the same semantic relation.",
                    "label": 0
                },
                {
                    "sent": "And the next difficult challenges.",
                    "label": 0
                },
                {
                    "sent": "Semantic relations are not always independent.",
                    "label": 0
                },
                {
                    "sent": "For example, if you take the easy relation and has relation.",
                    "label": 1
                },
                {
                    "sent": "So if you say Australia is a bird, then it somehow follows that ostrich must have feathers.",
                    "label": 0
                },
                {
                    "sent": "So these type of relations are generally not independent.",
                    "label": 0
                },
                {
                    "sent": "So when you are measuring relational similarity between two word pairs, you should have some way of knowing that what relations are independent and which degree there independent.",
                    "label": 0
                },
                {
                    "sent": "So in this work we are.",
                    "label": 0
                },
                {
                    "sent": "Looking in the distance metric which is not Euclidean and non Euclidean arnobius distance measure and.",
                    "label": 0
                },
                {
                    "sent": "We also need to find out which relations contribute relation similarity.",
                    "label": 0
                },
                {
                    "sent": "So although there are a lot of relations, for example, the end relation extend why so?",
                    "label": 0
                },
                {
                    "sent": "These are very ambiguous and very common relation.",
                    "label": 0
                },
                {
                    "sent": "It can mean anything.",
                    "label": 1
                },
                {
                    "sent": "So we need to kind of down with these type of relations.",
                    "label": 0
                },
                {
                    "sent": "So what we're taking the approach that we're taking here is a supervised approach, so we're trying to learn the Mahalanobis distance measure using training data.",
                    "label": 1
                },
                {
                    "sent": "So for this approach we're using a previously proposed method calling information theoretic metric learning.",
                    "label": 0
                },
                {
                    "sent": "So these are the different challenges that we must face and we will.",
                    "label": 0
                },
                {
                    "sent": "I will explain how we're going to address each of these challenges in the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slides to come.",
                    "label": 0
                },
                {
                    "sent": "So the first challenge is to how to explicitly say the relations between 2 words in a word pair.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this, as I mentioned earlier, we're taking a lexical pattern based approach.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are interested in finding the relations between lion and Cat, we would issue a query that connects line and cat with these wild card operators.",
                    "label": 0
                },
                {
                    "sent": "Typically this upper wild card operator would must an for one or none of words, so you would find if you issue this cure it for example, for Google, it will actually return the sniper, that is that is shown here.",
                    "label": 0
                },
                {
                    "sent": "The line is a large list, large, heavily build social cat of open rocky areas in Africa.",
                    "label": 1
                },
                {
                    "sent": "So if you read this snippet carefully, we find the relation that lion is a large cat.",
                    "label": 0
                },
                {
                    "sent": "So, but the words are in here and there, and there are other other other modifiers as well.",
                    "label": 1
                },
                {
                    "sent": "It is a social cat and heavily built, and we would like to extract the relation.",
                    "label": 0
                },
                {
                    "sent": "X is a large white.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is we generate all subsequences from the snippets that includes the two words that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "For example in this case some of the some of the patterns that we will end up with these.",
                    "label": 0
                },
                {
                    "sent": "A large way Xyx liked by off this type of different patterns.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 1
                },
                {
                    "sent": "Generating sub sequences of a string is very costly operation so we take we are taking this prefix span algorithm which is efficient and it can also consider gaps so we can specify the length of a gap that we are considering.",
                    "label": 0
                },
                {
                    "sent": "Of course this approach is noisy and it generates sort of misspellings.",
                    "label": 0
                },
                {
                    "sent": "An ungrammatical sentence, especially because the snippets are fragmented.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to identify the different patterns that talk about the same semantic relation and a very robust way.",
                    "label": 0
                },
                {
                    "sent": "So for that purpose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, proposing a clustering algorithm to cluster the lexical patterns so the previous approach generates around 750,000 patterns in our training data for the training data that we experimented.",
                    "label": 1
                },
                {
                    "sent": "And not all these lexical patterns describe independent the semantic relations.",
                    "label": 0
                },
                {
                    "sent": "So we need to identify the different lexical patterns that express the same semantic relation.",
                    "label": 1
                },
                {
                    "sent": "So in order to do this, we are using another quite quite old and very, very well used.",
                    "label": 0
                },
                {
                    "sent": "Idea called distribution hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So regional distribution hypothesis states that if the context of words are similar, then the words themselves should be similar.",
                    "label": 0
                },
                {
                    "sent": "So we are having a kind of extension of this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We say the patterns are distributed equally across the same set of word.",
                    "label": 0
                },
                {
                    "sent": "Pairs must be similar, so.",
                    "label": 0
                },
                {
                    "sent": "So to test this approach, I will first show.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, so here we have full lexical patterns.",
                    "label": 0
                },
                {
                    "sent": "It's vice why acquires Y by Cor fix an why chief Executive X?",
                    "label": 1
                },
                {
                    "sent": "So these patterns if a human look at these patterns.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, that person would know that expires Vianex acquires.",
                    "label": 0
                },
                {
                    "sent": "Why talks about the same relation whereas the latter two patterns also talk about same relation.",
                    "label": 0
                },
                {
                    "sent": "And I've shown the distribution of these two patterns across word pairs and the frequency as we normalized.",
                    "label": 0
                },
                {
                    "sent": "So we see that the two first 2 patterns have a high overlap.",
                    "label": 0
                },
                {
                    "sent": "Whereas the latter two patterns also have a high overlap.",
                    "label": 0
                },
                {
                    "sent": "So even if you just measure the similarity using for something like cosine similarity, you would get a high similarity value for the first 2 patterns and the last two patterns so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this idea here in the distributional hypothesis to measure the similarity between lexical patterns and now we have a similarity measure.",
                    "label": 1
                },
                {
                    "sent": "The next goal would be to cluster the patterns.",
                    "label": 1
                },
                {
                    "sent": "So considering the number of patterns, it's quite high.",
                    "label": 1
                },
                {
                    "sent": "It's not feasible to do a pairwise comparison as required by, for example, agglomerative clustering.",
                    "label": 0
                },
                {
                    "sent": "So here we are looking at a more sequential greedy pattern clustering algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the outline of the clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "The details are given in the paper, so I will briefly explain the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we first sort the patterns according to their total frequency in all word pairs.",
                    "label": 1
                },
                {
                    "sent": "So this gives us the more common the commonly occurring relations and the patterns that are represented by those relations in our data set.",
                    "label": 0
                },
                {
                    "sent": "And then we take each pattern and we have a threshold value.",
                    "label": 1
                },
                {
                    "sent": "And what we do is if we have if we find a cluster that is similar to this pattern.",
                    "label": 1
                },
                {
                    "sent": "Greater than the rather than the threshold value, we will add the pattern to the cluster or otherwise.",
                    "label": 0
                },
                {
                    "sent": "We will formalize a new cluster using that pattern.",
                    "label": 0
                },
                {
                    "sent": "So we would repeat this process until we're done with all the patterns.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "It has one perimetre the clustering threshold, so this procedure.",
                    "label": 0
                },
                {
                    "sent": "Has a several.",
                    "label": 0
                },
                {
                    "sent": "Nice properties to begin with, but it can be done in a linear time with the number of patterns, so we only needed one pass through the set of patterns and because we are sorting the patterns according to the frequency we get the more general and the more common the commonly occurring relations at the beginning and we get the outliers at the end.",
                    "label": 0
                },
                {
                    "sent": "So this way we can if you want to filter out the outliers.",
                    "label": 0
                },
                {
                    "sent": "This would be a nice property and there's only one para meters across threshold that we need to do now.",
                    "label": 1
                },
                {
                    "sent": "Cross validate or whatever you want to do it with it and it doesn't need to need the number of clusters in advance, and because of the sequential nature of the algorithm, it inherits their voice all pairwise comparisons, which can be costly with a large number of lexical patterns.",
                    "label": 0
                },
                {
                    "sent": "But it is a greedy clustering algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so far we have extracted the relations.",
                    "label": 0
                },
                {
                    "sent": "We are lexical patterns that describe the implicit relations in a word pair, and we have also done clustering of lexical patterns that gives us which patterns are semantically similar.",
                    "label": 0
                },
                {
                    "sent": "So the next challenge is that we must solve how to account for the interdependence between semantic relations.",
                    "label": 1
                },
                {
                    "sent": "For example, as I explained earlier, the Easter relation enhance relation and the next challenge that we must face is how to compute the relation similarity out of the clusters that we have.",
                    "label": 0
                },
                {
                    "sent": "That you have created.",
                    "label": 0
                },
                {
                    "sent": "So to solve this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Challenges we are proposing.",
                    "label": 0
                },
                {
                    "sent": "The following approach, where we first represent a bird pair using a feature vector so that it's in dimensional feature vector where each cluster produces.",
                    "label": 0
                },
                {
                    "sent": "Feature, so we're taking all the all the patterns that were required for a specific word pair, and we would aggregate all the pattern frequencies that correspond to the cluster.",
                    "label": 0
                },
                {
                    "sent": "So this would give us a feature representation vector representation of the word pair, and then we're taking a supervised approach where we try to learn the distance.",
                    "label": 0
                },
                {
                    "sent": "Or in this case mabius distance between word pairs.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you are utilizing a label.",
                    "label": 0
                },
                {
                    "sent": "The data set of positive and negative instances where we have labeled that these two word pairs are relational, similar and these two word processor relationally not similar.",
                    "label": 1
                },
                {
                    "sent": "And once you have done that you can you can we can compute the Mahalanobis distance between the two feature vectors that correspond to the two word pairs using the equation shown here.",
                    "label": 0
                },
                {
                    "sent": "So the whole training process boils down to computing the.",
                    "label": 0
                },
                {
                    "sent": "The modern, obvious metrics A so this can be done quite efficiently using a recently proposed Mahalanobis distance metric metric learning algorithm, which is called ihtml information theoretic metric learning algorithm, and it has nice properties.",
                    "label": 1
                },
                {
                    "sent": "For example, you don't need to do costly eigenvalue eigenvector computations and it can be scalable to larger datasets using low rank approximation of the melodious metrics, and it has also the nice properties that you can.",
                    "label": 0
                },
                {
                    "sent": "Incorporates active variables.",
                    "label": 0
                },
                {
                    "sent": "You don't have to.",
                    "label": 0
                },
                {
                    "sent": "Credit classify all the instances in the data set, so the learning algorithm is detailed in the paper and for the limited availability of time, I would I would not go into the details of the of the learning algorithm here, but you have interested.",
                    "label": 0
                },
                {
                    "sent": "You can read the paper.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next I will describe the various experiments that we did to evaluate the different steps and the proposals that we have made.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this purpose we have used two datasets.",
                    "label": 0
                },
                {
                    "sent": "The first data set is called anti datasets that only contains the named entities.",
                    "label": 0
                },
                {
                    "sent": "It has 100 entity pairs.",
                    "label": 1
                },
                {
                    "sent": "Accumulated 45 different semantic relations.",
                    "label": 0
                },
                {
                    "sent": "So we have acquired query relationship.",
                    "label": 0
                },
                {
                    "sent": "For example Google and YouTube and we have a person birthplace relation for example Charlie Chaplin was born born in London and we have see you and company of the CEO relation company headquarters relation.",
                    "label": 0
                },
                {
                    "sent": "An person feels personal relation is basically distinguished people who are known as authorities of that field.",
                    "label": 0
                },
                {
                    "sent": "And we downloaded snippets using using a search engine and we would run the pattern extraction procedure for these snippets.",
                    "label": 0
                },
                {
                    "sent": "We are also using another data set called S80 word energy data set which has been used extensively in previous work of relational similarity and this data set contains 374 S 80 word analogic questions.",
                    "label": 0
                },
                {
                    "sent": "So basically these these questions have 5 multiple choices out of which only one is correct.",
                    "label": 1
                },
                {
                    "sent": "And as the question you are given a word pair.",
                    "label": 0
                },
                {
                    "sent": "And how to find the word pair that is most analagous to the question?",
                    "label": 0
                },
                {
                    "sent": "So that's the task that is being solved that is being attempted in the second data set.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do.",
                    "label": 0
                },
                {
                    "sent": "We first do a relation classification experiment on the E&T data set.",
                    "label": 0
                },
                {
                    "sent": "So here what we do is we measure the relation similarity between each of the instances in the data set and the rest of the rest of the entity pairs and we would rank the entity pairs according to their relation similarity with the seed and we then use a K nearest neighbor classification.",
                    "label": 1
                },
                {
                    "sent": "In this experiment we took cast in and we would assign the label that you can find mostly.",
                    "label": 0
                },
                {
                    "sent": "Among the the the key nearest neighbors as the label Relation label of the of the entity pair.",
                    "label": 1
                },
                {
                    "sent": "And we repeat this process for all entity pairs in the data set, so we can model this problem as ranking problem.",
                    "label": 0
                },
                {
                    "sent": "We are given a named entity pair you would like to find more relational similar entity pairs at the top.",
                    "label": 0
                },
                {
                    "sent": "So we measure these using average precision, which is a common measure used in information retrieval to evaluate ranking problems.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compared with the vector space model based relational similarity where you don't do any clustering.",
                    "label": 1
                },
                {
                    "sent": "The word pairs are represented as feature vectors of lexical pattern frequencies, and then the similarities measured using cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "So this would be a kind of a baseline, and also it's a previous work.",
                    "label": 0
                },
                {
                    "sent": "It would give us idea whether the clustering actually helps and the second method is the state of our latent relation analysis.",
                    "label": 0
                },
                {
                    "sent": "It uses singular value decomposition instead of clustering to reduce the dimensionality.",
                    "label": 1
                },
                {
                    "sent": "And we have Euclidean distance between cluster vectors as the third baseline.",
                    "label": 1
                },
                {
                    "sent": "So in this case we do not use the Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "So implicitly it means you're using the unit metrics as the obvious metrics, which means there are relations independent.",
                    "label": 0
                },
                {
                    "sent": "So this will give us an idea of not taking into consideration the correlation between semantic relations.",
                    "label": 0
                },
                {
                    "sent": "And the last one is the proposed method, which which learns melodious distance metric measure using training data.",
                    "label": 0
                },
                {
                    "sent": "So we have shown experimental results on the E&T data set.",
                    "label": 0
                },
                {
                    "sent": "That you have five semantic relations and the different different methods that we have compared.",
                    "label": 0
                },
                {
                    "sent": "So we see the proposed method obtaining high average precision scores for all the relation types and.",
                    "label": 0
                },
                {
                    "sent": "And we do see Luo in performance on the person birthplace relation.",
                    "label": 0
                },
                {
                    "sent": "This is mainly because the the material, for example the places there where singles, an actress first performed, was incorrectly extracted as their birth places so.",
                    "label": 0
                },
                {
                    "sent": "Overall, we have a average precision of around 74%.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here I have shown the different clusters that we have extracted altogether.",
                    "label": 0
                },
                {
                    "sent": "We extract around 6500 lexical pattern clusters and all these classes have more than two lexical patterns in them, so we don't have any singletons in this set.",
                    "label": 0
                },
                {
                    "sent": "And for example, if you see the first cluster, it is mainly about the acquisition relationship.",
                    "label": 0
                },
                {
                    "sent": "You have different lexical patterns that you can use to express acquisition relation, and we also see.",
                    "label": 0
                },
                {
                    "sent": "The cluster #4, which is also talking about an acquisition but using different set of words.",
                    "label": 0
                },
                {
                    "sent": "Several set up verbs in this case by confirm and also purchase.",
                    "label": 0
                },
                {
                    "sent": "So we see the clusters themselves are not actually independent, so it makes sense to it.",
                    "label": 0
                },
                {
                    "sent": "Kind of motivates the fact that we should need Mahalanobis distance to account for these type of intercluster correlation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I have shown the results on the essay T data set, so here, but we're trying to do is in the city examination questions word analogic questions as I described earlier.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "There's one stem word pair which access the question and then there are for example typically 5 candidate.",
                    "label": 0
                },
                {
                    "sent": "The answers out of which you had to choose one, and if you do it randomly, you have a chance of getting a 20% score because you have five choices.",
                    "label": 0
                },
                {
                    "sent": "And there's a different previously proposed methods evaluated on this data set, as reported by the original papers, and we see the human performance on this task is also quite low.",
                    "label": 0
                },
                {
                    "sent": "It has been a difficult task even for the native speakers to solve word analogic questions and state of the art method.",
                    "label": 0
                },
                {
                    "sent": "Hillary has a score of 56% and the proposed method has around 51%.",
                    "label": 0
                },
                {
                    "sent": "However, we also must.",
                    "label": 0
                },
                {
                    "sent": "I must also note that it takes around 8:00 or 8 days actually, as described in the original paper to process the 374 word pairs, whereas the proposed method can produce results in less than six servers.",
                    "label": 0
                },
                {
                    "sent": "So it's worth looking into why from where these differences come in.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "During the late relational analysis, if I briefly explain the algorithm here, which let's start with the metrics metrics of word pairs and lexical patterns, and the values of the elements, would be the frequency of a pattern occurring for the particular word pair and then a singular value decomposition is performed on these metrics.",
                    "label": 1
                },
                {
                    "sent": "This is in itself is a quite a costly task because there around, say, individual paper there around like.",
                    "label": 0
                },
                {
                    "sent": "8000 lexical patterns and around 2000 word pairs.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a large metrics to perform single value decomposition, and it also takes into consideration the cinnamon synonyms of the original words and also takes the average of relational similarity between all word pairs as the.",
                    "label": 0
                },
                {
                    "sent": "The correct relation similarity between the word pairs that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 1
                },
                {
                    "sent": "Larry algorithm, and in this case, of course, if you have a number of.",
                    "label": 1
                },
                {
                    "sent": "Lexical patterns then, in order to compute the relational similarity between two word pairs, you would at least need to a number of web queries because you have to substitute each word pair in lexical pattern and then issue a query to search engine and get the page counts.",
                    "label": 1
                },
                {
                    "sent": "So this is quite a costly operation, whereas the proposed method only focuses on the snippets and searches.",
                    "label": 0
                },
                {
                    "sent": "Lexical patterns only in the snippets, so it only needs to issue for example, two queries.",
                    "label": 0
                },
                {
                    "sent": "To compute the relation similarity between one query, you cannot retrieve around 100 snippets.",
                    "label": 1
                },
                {
                    "sent": "So and also it does not need singular value decomposition, which must be repeated if you are in an online setting where you have new word pairs coming in continuously will have to add them to the matrix and then perform singular decomposition and then compute the relation similarity.",
                    "label": 0
                },
                {
                    "sent": "So these are the main benefits of the proposed method in terms of performance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, there are certain takeaways from this talk first.",
                    "label": 0
                },
                {
                    "sent": "We saw that distribution similarity over word pairs is a very good way of identifying lexical patterns that talk about the same semantic relation and.",
                    "label": 0
                },
                {
                    "sent": "We saw that clustering lexical patterns prior to measuring similarity can help with the accuracy of the similarity measure and greedy sequential clustering algorithm efficiently avoided all pairwise comparisons.",
                    "label": 1
                },
                {
                    "sent": "Ann is a good way of generating the common semantic relations ahead and also we saw that modern obvious distance outperforms Euclidean distance, mainly because semantic relations and also the clusters are not independent and our future work VR.",
                    "label": 0
                },
                {
                    "sent": "Focusing on extracting the relations for using the proposed relation similarity measure for analogical search as the example that I gave at the beginning, so I would like to take any questions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for.",
                    "label": 0
                },
                {
                    "sent": "At least you mentioned.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The examples you gave.",
                    "label": 0
                },
                {
                    "sent": "Similarities between relations will be detected for relationships are functional or at least partially into angle.",
                    "label": 0
                },
                {
                    "sent": "Factor is not very different, so if you had, if you had like very network relations like for example knows the social network, is that observation right?",
                    "label": 0
                },
                {
                    "sent": "That this would not apply to such very highly mixed with no.",
                    "label": 0
                },
                {
                    "sent": "These are actually you should have the two words in a single sentence for in order for this to work.",
                    "label": 0
                },
                {
                    "sent": "So it would not.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "It does not consider the transitivity relations in this case.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Maybe I did not.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "So we can become actually to the list.",
                    "label": 0
                }
            ]
        }
    }
}