{
    "id": "gz4npooyfrqttuonepzagfjifusdz64u",
    "title": "Reinforcement learning: Tutorial + Rethinking State, Action & Reward",
    "info": {
        "author": [
            "Satinder Singh, Electrical Engineering and Computer Science Department, University of Michigan"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010_singh_rlt/",
    "segmentation": [
        [
            "Well, thank you all for staying till the very end.",
            "I know you've enjoyed or suffered through many talks and you'll suffer through one more.",
            "Actually, it's quite amazing that so many of you are still here, though I guess partly that has to do with the fact that you can't really escape.",
            "So I'm sitting there saying from the computer science and Engineering Department, University of Michigan, and my understanding is that only really been one proper reinforcement learning talk though Chris.",
            "It's about 3 reinforcement learning these days.",
            "I don't know how much he talked about by order quick pull because I can accelerate to the first tutorial part depending on what the poll tells me.",
            "How many of you know MDP's value iteration?",
            "Q learning?",
            "Keep quite a few OK good so maybe I but I I was told that there's a fair bit of Fair mix of cognitive science and machine learning people here, so not everybody sees or else I'm going to do at least a little bit of tutorial part and then get on to this sort of thing that occupies my interest these days, which is rethinking state action reward.",
            "So I will go through fairly quick intro to RL.",
            "So first."
        ],
        [
            "The obligatory slide of pictures of all my collaborators and people have influenced me.",
            "On the top you'll see people I've collaborated with at the bottom here, all the students that have done most of the work that I'm going to tell you about, I won't say their names."
        ],
        [
            "So here's the the overall.",
            "Structural talk I'll give it tutorial very quickly.",
            "Define MDP's tell you what the policy evaluation problem is, what the optimal control problem is, and then give you 1.",
            "Idea for rethinking actions.",
            "One idea for rethinking States and one for rethinking rewards and then finish with a random thought or two."
        ],
        [
            "OK, so here is the problem.",
            "You have an agent interacting environment, taking actions, getting perceptions and rewards.",
            "It's a complete agent, temporally situated in its world, continually learning and planning.",
            "And of course the object here as opposed to most of the talks you've heard about, which are focused on inference is to affect the environment.",
            "So decision making and action.",
            "Though of course there's been other talks and decision making.",
            "The environment can be stochastic, uncertain, and so in some sense RL is like life and here is."
        ],
        [
            "The more abstract picture of this and the goal of the oral agent is to maximize payoff for some time horizon.",
            "So this view of life is that life is an optimal control problem."
        ],
        [
            "OK. Don't know if I can say this supervised and unsupervised and reinforcement learning at three legs of machine learning.",
            "I guess.",
            "One thing I'd like to say is if you've never heard of RL, here's one way to characterize what makes RL different from supervised and unsupervised learning.",
            "You play a game of chess.",
            "You make 100 moves, you lose.",
            "You have to figure out which of those hundred moves were good, which one of those are bad, and this is called a temporal credit assignment problem.",
            "This problem is not there in supervised learning unsupervised learning.",
            "This is what distinguishes reinforcement learning.",
            "It's about solving.",
            "In some ways the the temporary credit assignment problem, though of course subsumes unsupervised and supervised learning as well, because all the issues of generalization and density estimation.",
            "Also there in reinforcement learning.",
            "OK, lots of applique."
        ],
        [
            "Shains flashlight up."
        ],
        [
            "Here's an MDP.",
            "Just for notation, as a state space, A action space transition probabilities, the probability of seeing state.",
            "I am taking action against AJ, so reward function function of state could be more general than that pie.",
            "The deterministic policy.",
            "There's a notion of the return or utility of executing policy Pi in state I, which is just the expected value of the discounted sum of rewards you get when you behave according to policy Pi in some start state I so V of Pi we superpipe.",
            "I is the is the utility for policy \u03c0 or the value of the return y'all equivalent words?",
            "NRL forced forced a tie in policy Pi, and the optimal policy is of course the one that maximizes utility, so this is it.",
            "This is a quick intro to MVP's if you haven't seen this before.",
            "Hopefully this will do something for you for the rest of the talk."
        ],
        [
            "And policy evaluation problem.",
            "You're given a fixed policy.",
            "You have to evaluate it.",
            "The Markov assumption is the key thing I want you to understand from this equation.",
            "So the equation in the middle of the slide for all S in state space V Pi of S, the value of executing policy pine state taxes.",
            "This recursive form is the immediate reward you get plus the discounted expected value of.",
            "Next state is an equation that takes that first principle definition of the value of the state and transforms it into a system of equations.",
            "One for each state and the problem of evaluating how good a particular policy is.",
            "Amount to solving this system of equations.",
            "Guest policy evaluation problem.",
            "Optimal control.",
            "The problem of finding the optimal value function or effect the optimal policy can be written in this Q notation order.",
            "State action values with the best you can do from a state action pair is the immediate reward.",
            "You'll get cluster discounted expected.",
            "Best you can do in the state to get to again capturing the Markov assumption.",
            "So these Markov assumption leads you to these recursive Bellman optimality equations.",
            "And of course the best you can do in terms of action in the state.",
            "The optimal action estate Pi star of S. Is then just the optimal is just the greedy action with respect to the optimal Q values just very quick?",
            "Policy evaluation not controller.",
            "Twin problems that are is often concerned with evaluating a particular policy or finding the optimal policy."
        ],
        [
            "OK, so I'll give you one algorithm for each of these before and then that'll be the tutorial.",
            "Policy evaluation if you the planning version of it is you have a model you know the reward function.",
            "You know the transition property that you know the policy you want to evaluate it.",
            "You basically take the recursion I showed you before and you iterate it on the left hand side on there.",
            "Sorry, the right hand side, I'm actually can't do left and right without simulating writing on the right hand side.",
            "You plug in the current value.",
            "You current guess V sub K the cat guess and out on the left hand side pops the new value in your way.",
            "Can you stop when the value function stops changing more than a certain amount?",
            "Epsilon an you can start an arbitrary value function and this works.",
            "So this is the planning version of policy Val."
        ],
        [
            "Duration when you know the model.",
            "If you know the model again, the planning version of control, you do exactly the same thing except with these state action value equations.",
            "So this is Q value iteration, same stopping criteria.",
            "OK, I'm going to very fast.",
            "The value functions are important things."
        ],
        [
            "The crucial mathematical property that makes all of our L value function based RL work is this notion of contraction, which is that when you do an iteration, you might start off with us.",
            "So imagine your 2 dimensional value function Space 2 dimensional state space, the origin and their access is a true is.",
            "The axes are labeled.",
            "A star is a true value.",
            "The optimal value function you start with some arbitrary guess that might be .1, and basically after every iteration the maximum the box norm shrinks by a multiplicative factor of gamma, so the maximum error across any state from with respect to the unknown true value function shrinks by multiplicative factor of gamma.",
            "This is the underlying principle of the planning algorithm roll to the reinforcement learning algorithms at the TD and the Q learning algorithms that everyone is at least heard off probably.",
            "OK."
        ],
        [
            "That was planning learning is the situation where you don't have a real system.",
            "You don't have a model, so you don't have a model, but you can play in a real system and so you can generate experience.",
            "You can still be in some state as subzero subscription or time.",
            "You take an action, you get reward, next state action reward so you see experience like this.",
            "And here's a pictorial representation here.",
            "In some state you have a choice of action.",
            "You take one action.",
            "Which could lead to a choice of distribution over next states.",
            "You sample one next state choice of action, so you see a particular trajectory like this.",
            "This is what life looks like.",
            "We are taking actions, we getting percepts and so on.",
            "There are two classes and methods, indirect methods."
        ],
        [
            "Direct methods.",
            "What is the indirect method?",
            "Most obvious thing you can imagine you take your experience, you estimate a model.",
            "Here is a simplest non parametric form of a model estimation.",
            "You just take the empirical estimate of the probability of seeing state J on action in state A.",
            "Take your current model, find the policy that's optimal with respect to it and that's your estimate of the optimal policy is called a certainty equivalent policy.",
            "As long as you get enough data you model to converge, and the optimal policy will converge.",
            "Of course you can do parametric models, you can do much, much smarter than things that is, but this is a flavor of an indirect."
        ],
        [
            "Indirect method the direct method.",
            "Chris Watkins developed this Q learning, which is exactly the same.",
            "Recursive form that you've seen before, except of course you can't take expectations, so you just take the the part that I don't have a pointer to the part that's in the square brackets with Alpha in front is the sample of the recursive form you would the recursion to iteration.",
            "You would do so the actual reward you get the discounted best value of the actual state to get because you're getting a noisy estimate, you don't move fully to the new estimate, you mix it with the old estimate.",
            "That's Q learning.",
            "OK."
        ],
        [
            "And basically it's a noisy noisy contraction.",
            "Stochastic approximation theory works, and that's the result that Q learning converges property one."
        ],
        [
            "So much more so.",
            "So far, I've told you what an MVP definition is, what the planning and learning algorithms are for policy evaluation and for optimal control.",
            "Like it was the first Q learning, I thought I'd say this because Chris is here was the first approved.",
            "We converted adaptive optimal control algorithm and made a huge huge social, logical and practical impact in reinforcement learning.",
            "This sometimes works, it's stochastic, sometimes works, sometimes doesn't OK.",
            "So why aren't we done?",
            "Well, part of it is I haven't told you to solve the expiration exploitation problem yet.",
            "That is, how do we actually the Q?",
            "Learning just learn the value function.",
            "It doesn't tell you how to behave.",
            "Now you've got the optimal Q value function.",
            "Then you know how to behave, but you don't know how do you.",
            "How do you behave while learning?"
        ],
        [
            "OK, well one answer is epsilon greedy.",
            "Just some small property explore?",
            "Or is Alice exploit and you can provide conditions under which this works.",
            "And we have the other really interesting class of ideas, which I'll just save one.",
            "Sentence, but there are tons of papers that I saw which I refer to here is called the idea of optimism under uncertainty.",
            "So this beautiful work that's been done in solving the exploration exploitation problem with the following idea.",
            "Whenever you uncertain about something.",
            "About transition probabilities somewhere or so on, or your value functions, you assume more optimism than maybe it's warranted.",
            "So you pretend that things you don't know are better than things you know.",
            "And that leads.",
            "But of course if you keep doing that, then you'll forever.",
            "Explore, and that's not good.",
            "So that amount of optimism has to go down overtime, so shrinking optimism under uncertainty shrinking in time, optimism under uncertainty, is a class of ideas starting from exploration bonus work of Peter, Diane Buttery, cubed.",
            "First sort of algorithm that really did this right, and then a whole bunch of algorithms that have been are being produced even today.",
            "Exploit this idea of optimism, optimism under uncertainty, and it's.",
            "You get really nice strong results polynomial time results for optimism and certainty.",
            "So if you if you if you find this topic interesting, there's lots of things to read about here and finally, which I'll say again only in one sentence.",
            "There is a Bayesian class of methods where you're doing planning if you like in the state of knowledge rather than the physical state.",
            "And if you plan the state of knowledge, you can make you can optimally solve the expiration exploitation problem.",
            "I won't say much more about that, so that's that's that's solutions to exploration exploitation.",
            "So now again, what are we done?",
            "We can we do?",
            "Can we solve the problem of building agents that can live in the real world?",
            "It really is stochastic."
        ],
        [
            "OK, of course not because all the things that I told you about more or less for all for look up tables, but that of course is not practical.",
            "So how do we deal with scaling?",
            "And again, this is the last bit I'll do on tutorial and then I'll move on to the more interesting part very quickly for those if you've never or not been exposed to enforcement, at least maybe 40% of you didn't raise your hand or something like that.",
            "I'll give you a quick tour of these."
        ],
        [
            "So the function approximation idea is the most obvious 1.",
            "Instead of storing things in a look up table, you store things in a function approximation.",
            "You can just favorite function approximator back property on left radial basis, function C, Max nearest neighbor, decision trees, whatever.",
            "Just replace indexing things into a table with a."
        ],
        [
            "Parametric form whatever the parametric form is linear.",
            "I'm very partial to linear, but then the only idea of course here is what is the target value?",
            "I go back."
        ],
        [
            "What is a target in a index look up table used to place value with the target?",
            "What is the target you know in a in a function approximation scheme?"
        ],
        [
            "It's just you take the error and you do the gradient.",
            "Right, so it's just the square that you think of the thing inside the square brackets as a squared error.",
            "You will get.",
            "You will get the obvious algorithms for doing function."
        ],
        [
            "I'm partial to sparse coding, so I'll throw it up.",
            "This is my favorite.",
            "Everybody has their favorite."
        ],
        [
            "Function approximation that was my favorite one.",
            "What do we know about function approximation, linear function approximation?",
            "Worst case divergent can happen.",
            "In practice it often works.",
            "Nonlinear neural net theory is not well developed.",
            "Nearest neighbor methods are probably not divergent.",
            "Not very often used, I don't think, but there's not much theoretical guidance in this in this in this story.",
            "Alright, feel free to stop me and ask anything.",
            "I'm not saying anything terribly exciting yet, just just review."
        ],
        [
            "One last bit of review, and then I'll then I'll get any more interesting things.",
            "Idea Montecarlo has been has been.",
            "You know promoted here in this workshop quite a bit.",
            "It's been very useful.",
            "Here is an instance of Monte Carlo doing really interesting things in reinforcement learning, and in fact I would say the current state of the art algorithms and reinforcement learning are basically sampling tree based approaches for many, many classes of applications.",
            "At."
        ],
        [
            "Is the idea of sparse sampling?",
            "Here's a foundational idea.",
            "So suppose you want to find the best action in a current state.",
            "You're in some current state.",
            "You want to find the best action.",
            "You have a model.",
            "That is, you can sample actions in next dates, so you imagine you have two actions, the solid action and the dotted action.",
            "You sample the solid action a few times, so you sampled some next takes you sample, the dashed action a few times, and from each of those states you sample that other actions attached action, and you build out a tree.",
            "So doing Monte Carlo Tree, constructing his Monte Carlo tree, of trying every action every sample stick.",
            "And then you build the tree out to some extent, and then you back up values.",
            "You take values.",
            "Leaves could be 0 and you do the Max sort of the average of the values, and you take the Max or action.",
            "Then you build up the value and you compute the value of every action in the root node and you pick the best one.",
            "The empirically sampled best one.",
            "It turns out that this is an algorithm whose whose complexity is completely independent of the size of the state space.",
            "And produces with high probability a near optimal action in the root node in the root state.",
            "Now of course it you you when you not getting a free lunch here.",
            "So all the algorithms value function based algorithms have at least linear dependence on state space.",
            "This has no dependence on state space.",
            "In fact this doesn't even make the Markov assumption.",
            "So you can apply this in the non Markov setting just as well as a non is in the Markov setting.",
            "So you can do it in continuous state spaces and non Markov settings.",
            "All of these results hold of course it's exponential in the horizon.",
            "Right, so the discount factor is very close to one.",
            "Then the horizon has to be very deep and that of course can be a problem.",
            "And so what's the insight here?",
            "The inside here is the obvious Monte Carlo.",
            "One right Monte Carlo beats curse of dimensionality because to get an estimate a good enough to get a certain accuracy of the expectation of a random variable, the number of samples is completely independent of the number of values or random variable can take.",
            "And it's just that idea exploited in this tree form, and that's how it's beating the curse of dimensionality.",
            "And the and the current state of the art algorithms are inspired by UCT, which are much cleverer than this.",
            "I'm just giving the foundational idea.",
            "They decide much more cleverly which actions to explore, which ones not.",
            "But but underlying idea is this."
        ],
        [
            "OK, so I'm done with this tutorial, lots of lots of nice results and applications are available.",
            "Reinforcement learning but.",
            "All of them crucially assume you notice state space.",
            "You know the action space, you know the reward space.",
            "What I'm interested in, what drives my interest these days has been for awhile with for the near future, is trying to build flexibly intelligent.",
            "AI systems, that's how it connects to cognitive science because the the exemplars of flexible intelligence are animals and human beings.",
            "If you're doing narrow, are you doing?",
            "Now is not a pejorative word now, it just means you have a particular problem you want to make a helicopter fly.",
            "You want to schedule, you know the trucking companies.",
            "Cross if you want to solve a specific engineering problem.",
            "You notice state space because thousands of person years of effort has been put into figuring out what's a good state base.",
            "What are the good representation of actions?",
            "What the objective is?",
            "What is reward function?",
            "You know all that?",
            "Great, we have good methods for not off the shelf yet, but pretty good methods for solving that class of problems.",
            "Those types of problems if, on the other hand, you're interested in flexibly IPS, are rarely given, right?",
            "OK.",
            "So if, on the other hand, you are, you magically get the hardware for an Android.",
            "You log onto your Amazon.com and you order a household robot that is going to be a companion in your home.",
            "What is it going to be?",
            "State space?",
            "How is it going to represent his actions?",
            "What reward function should Amazon program into that into that robot?",
            "Maybe Amazon's reward function should be don't come back, but you know if you come back, that's bad.",
            "If you're sent back, that's bad for all of these questions are much trickier now, and those are the questions that I want to give you 1.",
            "Basic idea about for each of those questions for the, for the rest, for the rest of this.",
            "So this is my tutorial.",
            "If anybody has questions about this part, how long did that take?",
            "25 minutes.",
            "Any questions about this part?",
            "Yes.",
            "Agents we don't have a real decline or well defined goals, so after I guess well, we should consider.",
            "Right, so I haven't defined flexible AI what that means, except in opposition to classical engineering optimal optimal control where they were very well defined problem and you can take flexibility in many different directions, and I'm not going to redefine it except with the way of motivating these questions or what the state space should be, what the action space will be, whether award space B. Adjustable economy is an interesting approach that agent people have made progress on, but let me come back to that in the end and talk about.",
            "I'm not going to talk much about multi agent systems, so let's let's hold off on that.",
            "Maybe if there's time at the end we can, we can."
        ],
        [
            "On this OK, so let's see.",
            "I've already said all this.",
            "What should be the state space for a dog or a person?",
            "What's our state space is not very clear, right?",
            "What we know what our perceptions are.",
            "We don't know what our state spaces simultaneously have too many sensors and two few sensors, right?",
            "We have very high dimensional visual inputs but lots of things that are important for decisions are not immediately available in our percept.",
            "So we have the simultaneous problem that we have to we have to.",
            "OK, so how are we making progress as a field in this?"
        ],
        [
            "Most of the methods form into the fall in the first category, so basically they are Bayesian methods.",
            "Graphical model, probabilistic relational models, combinations of logic and probability.",
            "That's where the bulk of the effort, and that in fact has been much of the focus of this summer school.",
            "I don't want to say anything more about that except to say that it's been a very fruitful approach and people are making progress on this.",
            "What I want to tell you about is an alternative.",
            "Which we've been working on, which you call predictive representations of state.",
            "And there's a bunch of work that has gone into it and I'm going to tell you the basic idea often.",
            "So what's the distinction between these two approaches?",
            "The Bayesian sort of structured relational hierarchical models essentially focuses on saying there are latent variables in the world that define my perceptions.",
            "Let me try to infer those latent variables, and it's about trying to infer those latent variables.",
            "The second idea that I'm going to the alternative that I will tell you about, takes a perspective that maybe the way we want to think about it is that we have a very rich notion of internal state, and in this rich notion of internal state.",
            "We have perceptions may be processed.",
            "Perceptions clustered abstracted perceptions, but we also have predictions that the human brain is very good is adapted to be able to make predictions and maybe predictions as part of our internal state.",
            "Representations can do many of the things that latent variables are often used for, and so it's that idea that I'm going to try and communicate to you with one concrete result on that idea.",
            "So the one idea I want to communicate is there's a class of things that latent variables are used for.",
            "There are many things in fact, that latent variables are used for.",
            "I'm going to show you in one particular setting.",
            "How do use, how to replace the notion of latent variables with.",
            "Instead just predictions about the future.",
            "Using predictions about the future as part of your internal state representation to achieve the same end as A and of latent variables as an end of latent variables.",
            "OK, that's that's something I want to communicate on the state."
        ],
        [
            "So the class of words I'm going to look at a discrete time.",
            "Finite observations may be controlled, maybe uncontrolled.",
            "By that I mean you have actions, or you may not have actions.",
            "The class of words.",
            "I'm specifically going to look at are the class of words that you often model with HMMS, or equivalently, Palm DP's so here."
        ],
        [
            "A graphical representation of a palm DP.",
            "If you throw away the action nodes, which are the green nodes, you basically get Hmm's.",
            "So the blue notes are the latent variables or the states.",
            "The pink nodes can't tell if it's pink, pink nodes are the observations and time progresses this way, so this you've seen this before.",
            "Hmm's actions with actions they become palm DP.",
            "So what is the role of latent variables here?",
            "The role of latent variables here is to make the world.",
            "It is to capture all the knowledge there is in the history of observations so that you can predict you can get the conditional independence you want.",
            "Right, that is the blue nodes.",
            "The distribution overly blue nodes, the belief state space, which are distributions over hidden states, is a sufficient statistic of history so that you can make the same predictions about the observations on the future given the belief state that you would have made had you kept the entire history of observation.",
            "So that's a function of latent variables, and I'm going to show you how to replace that notion with a notion of this predictions about the future as being part of the representation of state.",
            "That's why I'm going."
        ],
        [
            "OK, so let's work towards that.",
            "What's what I mean by future in an uncontrolled system?",
            "Is a sequence of possible observations.",
            "You might get, so this is a length K future, so superscripts here denote possible future observations in a controlled setting.",
            "Future would be a sequence of actions, an observation, because obviously I'll take actions in the future as well in a control system.",
            "And the kinds of predictions I want to make.",
            "Is in control system is just a probability of seeing a particular future in an in the control system with the conditional probability of seeing a future given that the agent does this particular sequence of action.",
            "So that's the conditional prediction.",
            "What I want is."
        ],
        [
            "What I want is this.",
            "So here is the extensional definition of a system, the extensional definition of system is the prediction of all possible futures.",
            "So I lay down all futures in a vector form.",
            "Sister conceptual Definition writes an extensional definition, so all futures of length one all features of length two or features like 3 and so on.",
            "All possible futures and these predictions.",
            "Are just the probabilities of those features an if I give you this vector, or every system corresponds to a vector like this is 1 to one mapping between vectors and systems like this.",
            "So in building a model of such a system where I want to be able to do is build a compact intentional representation that is able to reproduce reproduce this vector.",
            "Now, of course lots of constraints on the entries that is vector, otherwise you'll be in trouble.",
            "You'll have a very large description of the system.",
            "So any but the point is any exact model of such a system ought to be able to generate this vector."
        ],
        [
            "OK, now I'm going to take this vector and convert it into a redundant representation.",
            "Again, a conceptual one, which you called a system dynamics matrix.",
            "Now I have rose.",
            "Added on to that first row, the top row is the vector already showed you, which was all possible.",
            "Futures in the null history, so all histories, all futures an now the matrix has a conditional represented as the conditional probability of a future given history.",
            "Where am I going to work?",
            "I'm building up towards a representation where I can show you that certain futures capture the same information.",
            "Certain predictions capture the same information as latent variables, just just compact him just to show you that part.",
            "OK, so just to give you the definition of a future given history is just the conditional property of the future.",
            "Given the history and in the.",
            "Control system it's the conditional property of the observations in the future.",
            "Given the history and the actions you would do in the future."
        ],
        [
            "OK, good so.",
            "Again, this is just a conceptual representation.",
            "This is not the representation obviously, because this is infinite.",
            "You can't really do this important thing to observe is that all the roses determined by the very first row.",
            "Now here's the interesting quantity.",
            "What's the complexity of the system?",
            "It is a way of defining the complexity of a physical system that has this conceptual representation is the rank of this matrix, which is called the linear dimension of the system.",
            "So if there are N linearly independent columns, then the rank of this system is N. Why's that important?",
            "Because I'm going to relate that to the complexity of this system of possible other ways of modeling the system.",
            "OK, so again I haven't constrained the system.",
            "By the way, in any way yet other than of course, discrete time, finite observations.",
            "This is a completely general discrete time discrete observation system.",
            "No constraints have been imposed on this other end, of course, I'm saying the system has some rank in which could potentially be infinite."
        ],
        [
            "OK, so how would we model such a system?",
            "Now before I talk about that in different ways of modeling system, if the rank of the system is N, that means I can pick out.",
            "Anne columns.",
            "Any possible predictions and possible futures that such that if I give you those N numbers?",
            "For any history I can predict the.",
            "Probability of any future in that history as a linear combination of those end numbers, right?",
            "That's the definition of rank.",
            "I can make any prediction of any any future in that history, just as a linear function of these these numbers.",
            "OK, so that's just this equation that the prediction of any future T in that history age is a linear combination of the predictions of these particular numbers and futures.",
            "For PFQ the vector PFQ given H is a vector.",
            "And a linear combination and subte the weight of the linear combination ends of T. Crucially, empty is not a function of age.",
            "That is, this entire column for this future T is the same linear combination of course, of all of these columns, like it doesn't.",
            "The linear weights the combination don't depend on the particular history I'm looking at, so that the weights empty don't depend on H. Otherwise it be wouldn't be interesting.",
            "OK, so."
        ],
        [
            "So now let's talk about alternative models.",
            "Suppose you have an Ant order Markov model that is a system is such that remembering the last N observations is state.",
            "Captures all the information there is in history.",
            "What's the rank of such a system where there are N?",
            "If there are.",
            "If there are any length.",
            "Basically, if if the last N observations matter and there are K possible observations, then the number of distinct histories in this matrix is bounded, right?",
            "How many basically any two histories that are the same last N observations will have exactly the same rose?",
            "So the rank of this system can't be very much.",
            "Right, if there are N unique.",
            "If there are K unique N length histories, then the rank system is at most K. Just obviously OK, so that's not, but that's not the interesting, interesting case.",
            "Let's look at the more in."
        ],
        [
            "Sting case I just said what I just said.",
            "Let's look at."
        ],
        [
            "What's special about this case boils?",
            "It can be model is hmm.",
            "What's interesting about words can be modeled as Hmm's is that there is no finite history?",
            "That's a sufficient statistic of history of all of history, right?",
            "That is, you have to remember things potentially infinitely far back in time.",
            "Now you might think, therefore, that the rank of a system of this dynamics matrix that is an HMM is potentially infinite.",
            "And the answer, of course, is that it isn't what I want to show you is that the Rank Office hmm system that has N underlying hidden states is no more than N. OK, and then I'll show you how the PSR representation works, and then I'll just summarize some of the research that has gone on.",
            "PS I just want to leave you with this idea.",
            "Those if you've not seen the PSR representation want to leave you with what appears.",
            "Our basic is and I'll tell you about other things."
        ],
        [
            "OK, so I'm going to show you now how I'm going to finish the proof.",
            "I'm going to show you why it is the case that if the number of underlying states is at most N, the rank of the corresponding systems matrix is at most N. So.",
            "Basically, if you haven't seen belief state updates in HMM, this will not make much sense, but if you have, then hopefully should.",
            "So what I'm showing you in this equation B is the belief state on seeing history hjo.",
            "If you, if you think of Hmm's and drop the actions right, then it would be just H. Oh is this sort of transformation of the belief at history?",
            "H is basically be of age.",
            "The old belief state times some transition problem, matrix B normalized.",
            "Right, so there's a linear transformation normalized that takes the belief data history age and computes the belief state history HL.",
            "What that means is that the prediction that we care about prediction of any future given any history is actually a linear function of the belief state.",
            "Not terribly surprising if you know.",
            "Hmm, so you would know this right?",
            "Same thing, also palm DP's.",
            "So the critical thing about Hmm's or Palm DP's is that the prediction of any future given any history is a linear function of release date.",
            "OK, now that's true."
        ],
        [
            "Dictate my words, that's true.",
            "Then I'm going to just take the system dynamics matrix and replace all of these predictions by.",
            "By the predictions computer from the belief states.",
            "And here's the idea.",
            "Think of the history.",
            "Each of the histories corresponds to a belief state, right.",
            "Each histories corresponds to belief state.",
            "Think of belief states that correspond to unit basis.",
            "Belief states where all the probabilities in one underlying hidden states.",
            "Find such how many of those that can be and such belief states right?",
            "Because there is an underlying hidden states, so the belief vector dimension is N, which means their end unit basis belief states.",
            "Find the corresponding histories.",
            "Each history, then, is a linear combination of the each role of this matrix is a linear combination of the rows that correspond to unit basis belief factors.",
            "In fact, what linear combination precisely the beliefs?",
            "Hence there are at most N linearly independent rows in this matrix.",
            "Therefore, the rank of matrix is at most N. Now there's a very quick proof, but all of this is can be easily shown.",
            "OK, So what does that mean?",
            "That means that every palm TV with N nominal states hidden states is a linear dynamical system of linear dimension at most."
        ],
        [
            "At most it could be much smaller then OK, so now I'm going to show you well forget this bit."
        ],
        [
            "What I'm going to show you now is the PSR representations.",
            "Up until now, just not showing the representation.",
            "So again, if the rank of the matrix is N, there are N. Futures that are correspond to N linearly independent columns.",
            "If I give you.",
            "Give you let's call but I will say this before.",
            "Let's call those N test score test.",
            "So here is my representation of state now.",
            "Not the latent variable belief states, but instead the predictions.",
            "My representation of state for history is my prediction of each of these futures correspond to the corporate core test of core futures.",
            "So what I'm going to show you now is that if I just keep around these N numbers.",
            "Which are predictions of the future then?",
            "I'm basically keeping unbelief stay in effect.",
            "When I'm keeping it on state, to be honest, to be set up better word.",
            "Sufficient statistics histories?",
            "How do I show you that I'm going to show you that this is in fact a sufficient statistic of history?",
            "So here's how I do that.",
            "So you're in some history H. You have these end numbers as my representation of state.",
            "I can predict any future I want in that history if I.",
            "No answer MMFT."
        ],
        [
            "I can make any prediction I want if I know the corresponding weights, so let me show you how to update these end numbers when in history.",
            "Hi, take action A and observe observe.",
            "Oh again, if you if you're in hmm land, drop the A. OK, so how do I compute?"
        ],
        [
            "Then numbers.",
            "Well, here's the idea.",
            "The prediction the remember then numbers have a meaning.",
            "Have a non latent meaning they have a predictive meaning, so one of those end numbers is the probability of future KUSA by for his end core tests in the new history, HAO and just by probability by by you know by chain rule you get this equation.",
            "That is, it is the prediction is that it's the.",
            "It's the prediction of the extended future, a oqi given history.",
            "H hang up just second divided by the prediction of just observing oh it action A.",
            "Given age and of course, these can be computed linearly given the old representation of state PQ of age.",
            "So now if I just keep around the linear weight combinations for just the action observation pairs.",
            "That's this part for the denominator and the one step extensions to the end core tests.",
            "Then I can keep maintaining state representation and make any prediction turned out I can make any prediction I want by just these parameters, so these M's are now the parameters of my model, just like transition probabilities in observation probabilities are the parameters of a belief state model of an HMM model.",
            "OK, so all I really want to say about PSR's is a definite as I just tell you what PSR now I want to tell you about what the import of all this is in a minute, but you had to hang up.",
            "What the parameters are?",
            "These are the parameters of the model.",
            "OK, so.",
            "Yeah, this I can I can skip."
        ],
        [
            "I just said that, So what I've shown you now is that every discrete time dynamical system dimension N is equivalent to a linear PSR because of linear pair size with N core tests.",
            "Hence, I've shown you that every hmm with at most with N nominal States and hidden states can be always be represented by on a linear PSR with no more than N cortex OK, what's the?",
            "What's the big deal Huawei?",
            "Is this potentially interesting?",
            "This is potentially interesting for a variety of reasons, but one of the reasons, of course, is that latent variables.",
            "Or like you can't observe them.",
            "Predictions you can make and you can evaluate and you can test.",
            "You can make consistent algorithms for computing predictions.",
            "And to the extent that you can do that consistently as opposed to in a local minima M like way you can get benefits that you might not have from latent variables.",
            "I sort of 1 obvious sort of benefit.",
            "The other benefit potential benefit is that you know we're very finely tuned to make good predictions.",
            "And it's interesting that predictions can accomplish.",
            "Some of the things that latent variables are typically used for now.",
            "You can argue that latent variable models are good for generalization.",
            "I think we can similarly argue that having predictions that are internal state representations might be very good for generalization in particular, and this is not yet a point that has been computationally demonstrated but to speculative speculative point planning is about projecting into the future, predicting the future.",
            "Planning involves predicting, planning involves.",
            "Predicting possible futures to the extent that our internal state representation contains projections of the future, it may make planning a lot easier than with representations that are not explicitly about that.",
            "Again, this is just an idea.",
            "It hasn't.",
            "We're beginning to explore this idea.",
            "We haven't really nailed out any of this sort of thing, so this is all I have to say about PSR's.",
            "Any questions about this?"
        ],
        [
            "I think this is all I have to say about this.",
            "I want to switch to actions.",
            "Any questions about this come up right now?",
            "OK.",
            "Yes.",
            "Some more what?",
            "Nonlinear, yes, we have done nonlinear PSR's we're we're working on that.",
            "In fact, I looked at the AI stats AI STATS program, and there is a talk by Jeff Gordon and some of his colleagues CMU on.",
            "On spectral clustering, methods for learning PSR like things.",
            "So there is not a lot of work I would say, but there is some work on extending to nonlinear methods to to connecting this to dimensionality reduction approaches and so on.",
            "So there is some big inning here.",
            "Here is the core of an idea.",
            "Basically what you're trying to do is compress this infinite matrix.",
            "You want to find a compressed compressed representation of this infinite thing and now you can.",
            "Think of all sorts of ways to take this infinite conceptual matrix, of which you can actually get samples.",
            "And take samples of this infinite by infinite matrix and try to find compressed representations.",
            "So people are beginning to go down that go down that Rd.",
            "But it's not.",
            "There's a whole industry of people working on this yet.",
            "OK, the action spot is actually goes much further back in time, so I'm going to go very fast through it, but I want to get to the rewards part, which is much more recent in time.",
            "In fact, how many of you know where options are and know how options work and.",
            "The RL notion of options, not the financial notion of options.",
            "Actually, not very many, so maybe I should spend.",
            "I should spend time on options.",
            "OK actions."
        ],
        [
            "So what's the question here?",
            "The question here is of course.",
            "MDP's assume a uniform timescale notion of actions, right.",
            "Actions that take one time step, but we can clearly planet all kinds of temporal scales right when you plan to come to Sardinia you thought about flights you thought about calling taxis.",
            "You thought about walking to your door.",
            "You talked about picking up your bag.",
            "All of these things are completely different temporal scales with just completely flexibly able to plan and reason.",
            "Not just strictly hierarchically, is not that it's strictly hierarchical.",
            "I plan at one level, then I planted a lower level than I planned.",
            "No, we can just flexibly plan at all kinds of levels simultaneously.",
            "Question is, how do we begin to have a formalism that affords us this?",
            "And this is work we did.",
            "Oh, I don't know.",
            "Maybe 15 years ago and I want to just at least as a as a want to explain the insight data and then I'll move on.",
            "Turns out there are many different ways of thinking about this.",
            "All worked on in reinforcement learning.",
            "It connects the idea of macro actions, right abstract actions, macro actions.",
            "Back, depending on what community comes from, there's different words for these things.",
            "Options is something we developed which connects to Max Q.",
            "Though if you know Max cure hams, they're all quite related."
        ],
        [
            "Here's the idea.",
            "Instead of an action that last one uniform time step, we're going to think of just general ways of behaving.",
            "I want to be able to do this decision process stuff with general ways of behaving.",
            "And so the key question is how do we think about general ways of how do we encapsulate general ways of behaving so that we can literally take all the MDP formalisms?",
            "We had all the algorithms, all the Bellman optimality equations, all the things that come from that, and just replace primitive actions with these more flexible things.",
            "And everything is still the same.",
            "All the theory, all the algorithms, everything works.",
            "That is a question about to answer their options answered.",
            "So here's the idea.",
            "What's an option?",
            "An option is is is defined by.",
            "This three couple is right is triple the set of states.",
            "The option is available in.",
            "That's I the policy.",
            "You follow the behavior generation mechanism when you're executing the option.",
            "And by the way, do you think about the policy or anything else doesn't matter?",
            "Some procedure for acting could be anything could be open loop could be closed, whatever it is, and then something that decides when the option terminates.",
            "So some termination probability in every state.",
            "You could have a property that says.",
            "I told me they're only in that state when I get to that state.",
            "Or you can say I terminated in 10 time steps.",
            "Or you can say I terminate, you know, with some probability different states, that's it if you just define options this way.",
            "All of the theory of MDP's basically extends.",
            "Everything works, all the bellman optimality equations I'm going to show you.",
            "Of course I'm going to say more about this, but the point is.",
            "Everything everything is recovered.",
            "Basically anything on the RL side pretty much is recovered.",
            "All the results that you know or that have been obtained for strict MVP's are recovered with actions being replaced by this notion of option."
        ],
        [
            "So let's give some more intuition about an option you know.",
            "You could have a simple rooms were like this.",
            "Your actions primitive actions could be going North, South East, West.",
            "Or you could have behaviors that say get to the door.",
            "So here is a behavior that basically option one gets you to the door of that of that room.",
            "So they might have.",
            "You might have options like that, OK?"
        ],
        [
            "So what's the advantage?",
            "Again, the advantages MDP's have a uniform discrete time step when you have options, you get the formalism.",
            "What's called semi Markov decision process and summer Markov decision process is a basically a decision process in which the temporal duration between choice points.",
            "Where you're making a new decision is a random variable because you know you pick an action, go to the door and might take a random number of time steps and then you get to the door or you fail somehow, you fail, and then you could make another decision.",
            "Mike, so it's just it allows this flexibility.",
            "So Sam Arkoff decision process is basically recover.",
            "Or you can do all the things that you do for options with Sam Arkoff decision policies.",
            "Now the distinction between Sam, Markov decision processes and MVP's options is that we have we have both.",
            "We have the ability to be executing an option for also to be able to look inside the option and know what we're doing at a lower level, and that has certain advantages that the options were explored."
        ],
        [
            "So I wanted to.",
            "So I will do 2 quick things and then I'll move on, move onto towards.",
            "You can define.",
            "You can define now an action set which includes these behaviors.",
            "Call that capital O and you can define optimal value functions with respect to the V star of sub OS onvista.",
            "NQ stars above S, Oh right.",
            "Wherever you had a little a, you can now have little.",
            "Oh for these options and all the value function like things that we talked about before can be can be can be recovered and but let me give you some more insight about that.",
            "How can they be recovered?",
            "Here's how they recovered.",
            "Remember to reward and the transition probabilities.",
            "The reward of primitive action is the number you get when you take an action and the transition probabilities, other transitions probabilities in the one time step.",
            "The action takes of the distribution of the next.",
            "It's how do we define these beasts?",
            "These quantities for an option?",
            "If you can do that, everything else will be will workout."
        ],
        [
            "So here's the idea.",
            "The reward part for an option is just the expected reward expected sum of discounted reward you get when you start that option until termination.",
            "So that's just the reward part.",
            "It's so it's a number in this state.",
            "If you start that option, what's the expected reward you expect to get until the behavior for that option terminates?",
            "The more tricky part is the transition probabilities.",
            "That's actually not all you do is simply asked what is the expected.",
            "Probability of terminating impossible states.",
            "Now it's not just as simple as that, because you have big discounting into account, so is the discounted probability, because you know if you if you donate in state five in two steps, that's a different.",
            "Dan terminating State 5 in 10 steps.",
            "So you predict discounted probability.",
            "So in other words these these transition probabilities are no longer sum to one because they have discount factors taken into account, but that's it, they're just expected discounted probabilities of terminating in different states.",
            "And with these definitions, you can write down the Bellman optimality equation."
        ],
        [
            "Means you can.",
            "You can do it are value iteration or Q value iteration of Q learning or whatever it is and everything.",
            "Everything just works.",
            "All the map just falls out beautifully.",
            "So very simple idea.",
            "Were able to recover.",
            "Now the flexibility with planning or learning.",
            "With different time scales, because these options could be at many different timescales, right?",
            "It's not a strict hierarchical notion.",
            "You could have.",
            "Some options are small induration, some options in a long in duration, and all that's fine, everything works, so we have the representation for being able to capture the ability to plan and learn with flexible time scale.",
            "Of course the hard part is what should these options B and there's a lot of work on trying to figure out.",
            "You know where the options come from?",
            "How do we use experience in the world to learn options?",
            "And then you know, think of options, skills.",
            "We go about the world and we accumulate skills.",
            "We know how to open the door.",
            "We now to build some of his notable tables.",
            "Some of us have a skill of, you know, PowerPoint.",
            "Just kind of a shame, but so we have these skills and we we we can now plan and learn in terms of these skills and so there's obvious sorts of thoughts about how options might come from.",
            "Doors are bottlenecks you can think of, bottlenecks more generally, so you may have you may build options towards bottlenecks, reusable skills that are Paula sub policies that are reused over and over again might be useful to encapsulate in form of a scale or an option.",
            "People have played around with these sorts of ideas.",
            "Is there is there an off the shelf method for learning options now?",
            "But there are simple things that have been explored in terms of learning in terms of learning options, so I was going to say about options.",
            "Just want to give you a sense of we are able to represent.",
            "Ways of behaving at flexibel levels of abstract temporal abstraction and do planning and learning with just the same algorithms.",
            "Everything just works beautifully.",
            "Questions about that I'm going to skip all this in the in.",
            "Yeah."
        ],
        [
            "And you know many, many results become available.",
            "So here is a. I'll give you one flavor of a result.",
            "So here is, uh, you know continuous 2 dimensional domain is a possible landmarks.",
            "We might have options that take us to landmarks from some local neighborhood, that Landmark.",
            "So each circle is a local neighborhood centered or a landmark.",
            "And you might know how to get to the landmark from that neighborhood.",
            "And then you can begin planning with these.",
            "You can plan to go from neighborhood, from Landmark to landmark.",
            "Has a way of navigating and can do much more faster planning this way than if you were to plan with low level continuous continuous behaviors of course."
        ],
        [
            "You could do something smarter than that.",
            "You could realize, and this is called.",
            "This is a sort of simple result that we have that says basically you could start a 1 landmark.",
            "Head head towards the other landmark, which is a red line.",
            "But as soon as you reach a location in which if you start it again, you would have switched to a different landmark.",
            "You can switch and it turns out, so you can terminate options early and obtain benefits of planning so early termination of options is an easy, very easy thing that falls out of planning that can obtain great benefit.",
            "OK, I'm going to stop with this.",
            "You can buy, they learn."
        ],
        [
            "Oceans, his other little thing.",
            "I'll highlight again, just those if you find this interesting, you can find all kinds of literature on this.",
            "So how do we learn models of options?",
            "The reward and transition probabilities?",
            "If we could only learn about options we're actually executing would be dead, right?",
            "Because you can only behave in one way.",
            "It can only learn about the way you're behaving.",
            "You can't learn very much, so you have to be able to learn about ways of behaving that you're not behaving it.",
            "So how do we do that?",
            "Here's an idea.",
            "Suppose you've just taken a small primitive action, which is a first link.",
            "In that change, the black chain, so you're behaving according to some option, which is a black chain using the first action.",
            "But that action might be consistent with many options.",
            "Say the dashed option.",
            "Turns out there are very simple ways of learning about all options, updating the model of all options that are consistent with that primitive action you just took.",
            "Even though your intention is to behave going to one of the options, you can update all options simultaneously that are consistent with the way you're currently locally behaving, and this is a very powerful idea that allows you to exploit recall interruption methods, so there's lots of clever ways of doing this."
        ],
        [
            "OK, Final day rewards.",
            "Which is the most current work that I'm doing and occupies.",
            "Most of my current time.",
            "Have a half are good."
        ],
        [
            "Alright, this is a question of where the rewards come from.",
            "Again, in narrow engineering tasks this may be completely clear, right?",
            "You are trying to schedule an elevator system.",
            "You know, somehow you've decided that what you want to do is minimize wait time.",
            "Reward function is clear actually.",
            "If you really honest about it.",
            "Turns out even in narrow engineering task, it's often not clear what the reward function is.",
            "An Andrew ING, I don't know how many of you heard this talk from Andrew Yang, but he is a great example of this.",
            "He's got this.",
            "Wheeled legged robots that got legs with wheels at the end.",
            "And he's got.",
            "He's trying to make this wheeled legged robot.",
            "I know it must be a better name for it to be able to climb a stair climber step.",
            "And so he gives it the obvious toward function.",
            "You know you want to minimize the amount of time before it is on top of the step, so it gives a reward function, does reinforcement learning, and the agent does something really weird is sort of drags its wheels, or whatever it doesn't, and he said no, no, that's not what I wanted to do.",
            "So he said, OK, well, I don't want to drag its wheels so it gives it.",
            "He modifies the reward function.",
            "Do not do that to penalize it for doing that, and then it does something completely different in bizarre.",
            "So no, that's not what I want.",
            "He goes to these four or five steps before it does something that he actually wanted to do.",
            "Now of course, he didn't really know how to program what you want to do right?",
            "But the behavior looks find so even in narrow engineering task, it's actually often not clear what the reward function should be.",
            "In fact, people do play around with this.",
            "So recently I've been playing out of idea what we call internal reward an essentially this thing called breaking the content out about tell you about.",
            "Now.",
            "Many people have played with internal rewards.",
            "Jurgen Schmidhuber has 15 papers or something like that on this topic, including on curiosity, novelty, great stuff.",
            "So he's got a perspective in a line of work on internal towards what I'm about to tell you about is coming at it from a different angle."
        ],
        [
            "OK, so so.",
            "One reason that reinforcement learning is powerful.",
            "Is a reward functions.",
            "Permit specification of what an agent should do without.",
            "Without telling it exactly how to do it.",
            "What the agent is to do without telling it how to do it right?",
            "So RL theory and algorithms are completely insensitive to the source of the rewards.",
            "Rewards just somehow given NRL starts from there.",
            "And that's explains the generality so they can be applied to lots of things.",
            "But of course, if you're doing cognitive science.",
            "A fundamental question of interest is you know where do words come from where L is optimized.",
            "And we're going to look.",
            "We're going to try to look at that very abstractly.",
            "And of course, from the machine learning AI point of view is that this this generality comes at the cost of a decremental preferences parameters confound, so I might tell you what that is now.",
            "What is this confound?",
            "If I can go to the next?"
        ],
        [
            "Slide OK. What is this preferences parameters confound?",
            "Basically, what is the reward function contacted by the way disconnects a little bit, or at least in my judgment, it connects.",
            "It's a way of trying to do what Nick was, or a way of approaching what Nick was talking about, so I'll try to make that connection as I get to it.",
            "So reward function confounds two roles simultaneously in reinforcement learning agents, right?",
            "It expresses the preferences of the agent designer.",
            "I, as an agent designer, I'm trying to build an agent.",
            "I have a preference or ways of behaving.",
            "And that reward function.",
            "And if I have a sufficiently general notion of preferences, have a utility function or reward function or ways of behaving.",
            "So that's one rule.",
            "That's the preferences role, but also what's usually done is.",
            "Then I take that same reward function and put it into the agent and say that's your reward function optimize that.",
            "That is, I I reach into the agents hardware software an insert that same reward function.",
            "I make the agents goals the same as my goals as agent, Designer and of course, the way the agent will behave if it's an RL agent is going to try to achieve maximized behave so as to maximize their reward function.",
            "So this is confounding the reward function for an agent are the parameters that the agent designer is setting.",
            "That determine the agent's behavior through whatever algorithm the agent is doing.",
            "But the reward function really is as the preference my as the agent designers preferences using cognitive science.",
            "Evolution is the agent designer and the of course the agent and natural agents results are distinct and is no a priori reason.",
            "They should be confounded.",
            "Though most often when I say this, people say wait a minute.",
            "Of course there should be the same.",
            "Why should the agent have different goals in the agent designer?",
            "Because the agent is trying to serve the agent designer White should goals be.",
            "Why should the agents goals be different?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, thank you all for staying till the very end.",
                    "label": 0
                },
                {
                    "sent": "I know you've enjoyed or suffered through many talks and you'll suffer through one more.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's quite amazing that so many of you are still here, though I guess partly that has to do with the fact that you can't really escape.",
                    "label": 0
                },
                {
                    "sent": "So I'm sitting there saying from the computer science and Engineering Department, University of Michigan, and my understanding is that only really been one proper reinforcement learning talk though Chris.",
                    "label": 0
                },
                {
                    "sent": "It's about 3 reinforcement learning these days.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much he talked about by order quick pull because I can accelerate to the first tutorial part depending on what the poll tells me.",
                    "label": 0
                },
                {
                    "sent": "How many of you know MDP's value iteration?",
                    "label": 0
                },
                {
                    "sent": "Q learning?",
                    "label": 0
                },
                {
                    "sent": "Keep quite a few OK good so maybe I but I I was told that there's a fair bit of Fair mix of cognitive science and machine learning people here, so not everybody sees or else I'm going to do at least a little bit of tutorial part and then get on to this sort of thing that occupies my interest these days, which is rethinking state action reward.",
                    "label": 0
                },
                {
                    "sent": "So I will go through fairly quick intro to RL.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The obligatory slide of pictures of all my collaborators and people have influenced me.",
                    "label": 0
                },
                {
                    "sent": "On the top you'll see people I've collaborated with at the bottom here, all the students that have done most of the work that I'm going to tell you about, I won't say their names.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the the overall.",
                    "label": 0
                },
                {
                    "sent": "Structural talk I'll give it tutorial very quickly.",
                    "label": 0
                },
                {
                    "sent": "Define MDP's tell you what the policy evaluation problem is, what the optimal control problem is, and then give you 1.",
                    "label": 0
                },
                {
                    "sent": "Idea for rethinking actions.",
                    "label": 0
                },
                {
                    "sent": "One idea for rethinking States and one for rethinking rewards and then finish with a random thought or two.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the problem.",
                    "label": 0
                },
                {
                    "sent": "You have an agent interacting environment, taking actions, getting perceptions and rewards.",
                    "label": 0
                },
                {
                    "sent": "It's a complete agent, temporally situated in its world, continually learning and planning.",
                    "label": 0
                },
                {
                    "sent": "And of course the object here as opposed to most of the talks you've heard about, which are focused on inference is to affect the environment.",
                    "label": 0
                },
                {
                    "sent": "So decision making and action.",
                    "label": 0
                },
                {
                    "sent": "Though of course there's been other talks and decision making.",
                    "label": 0
                },
                {
                    "sent": "The environment can be stochastic, uncertain, and so in some sense RL is like life and here is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The more abstract picture of this and the goal of the oral agent is to maximize payoff for some time horizon.",
                    "label": 0
                },
                {
                    "sent": "So this view of life is that life is an optimal control problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Don't know if I can say this supervised and unsupervised and reinforcement learning at three legs of machine learning.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "One thing I'd like to say is if you've never heard of RL, here's one way to characterize what makes RL different from supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "You play a game of chess.",
                    "label": 0
                },
                {
                    "sent": "You make 100 moves, you lose.",
                    "label": 0
                },
                {
                    "sent": "You have to figure out which of those hundred moves were good, which one of those are bad, and this is called a temporal credit assignment problem.",
                    "label": 0
                },
                {
                    "sent": "This problem is not there in supervised learning unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "This is what distinguishes reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It's about solving.",
                    "label": 0
                },
                {
                    "sent": "In some ways the the temporary credit assignment problem, though of course subsumes unsupervised and supervised learning as well, because all the issues of generalization and density estimation.",
                    "label": 0
                },
                {
                    "sent": "Also there in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "OK, lots of applique.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shains flashlight up.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an MDP.",
                    "label": 0
                },
                {
                    "sent": "Just for notation, as a state space, A action space transition probabilities, the probability of seeing state.",
                    "label": 0
                },
                {
                    "sent": "I am taking action against AJ, so reward function function of state could be more general than that pie.",
                    "label": 0
                },
                {
                    "sent": "The deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "There's a notion of the return or utility of executing policy Pi in state I, which is just the expected value of the discounted sum of rewards you get when you behave according to policy Pi in some start state I so V of Pi we superpipe.",
                    "label": 0
                },
                {
                    "sent": "I is the is the utility for policy \u03c0 or the value of the return y'all equivalent words?",
                    "label": 0
                },
                {
                    "sent": "NRL forced forced a tie in policy Pi, and the optimal policy is of course the one that maximizes utility, so this is it.",
                    "label": 0
                },
                {
                    "sent": "This is a quick intro to MVP's if you haven't seen this before.",
                    "label": 0
                },
                {
                    "sent": "Hopefully this will do something for you for the rest of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And policy evaluation problem.",
                    "label": 0
                },
                {
                    "sent": "You're given a fixed policy.",
                    "label": 0
                },
                {
                    "sent": "You have to evaluate it.",
                    "label": 0
                },
                {
                    "sent": "The Markov assumption is the key thing I want you to understand from this equation.",
                    "label": 0
                },
                {
                    "sent": "So the equation in the middle of the slide for all S in state space V Pi of S, the value of executing policy pine state taxes.",
                    "label": 0
                },
                {
                    "sent": "This recursive form is the immediate reward you get plus the discounted expected value of.",
                    "label": 0
                },
                {
                    "sent": "Next state is an equation that takes that first principle definition of the value of the state and transforms it into a system of equations.",
                    "label": 0
                },
                {
                    "sent": "One for each state and the problem of evaluating how good a particular policy is.",
                    "label": 0
                },
                {
                    "sent": "Amount to solving this system of equations.",
                    "label": 0
                },
                {
                    "sent": "Guest policy evaluation problem.",
                    "label": 0
                },
                {
                    "sent": "Optimal control.",
                    "label": 0
                },
                {
                    "sent": "The problem of finding the optimal value function or effect the optimal policy can be written in this Q notation order.",
                    "label": 0
                },
                {
                    "sent": "State action values with the best you can do from a state action pair is the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "You'll get cluster discounted expected.",
                    "label": 0
                },
                {
                    "sent": "Best you can do in the state to get to again capturing the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "So these Markov assumption leads you to these recursive Bellman optimality equations.",
                    "label": 0
                },
                {
                    "sent": "And of course the best you can do in terms of action in the state.",
                    "label": 0
                },
                {
                    "sent": "The optimal action estate Pi star of S. Is then just the optimal is just the greedy action with respect to the optimal Q values just very quick?",
                    "label": 0
                },
                {
                    "sent": "Policy evaluation not controller.",
                    "label": 0
                },
                {
                    "sent": "Twin problems that are is often concerned with evaluating a particular policy or finding the optimal policy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll give you one algorithm for each of these before and then that'll be the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Policy evaluation if you the planning version of it is you have a model you know the reward function.",
                    "label": 0
                },
                {
                    "sent": "You know the transition property that you know the policy you want to evaluate it.",
                    "label": 0
                },
                {
                    "sent": "You basically take the recursion I showed you before and you iterate it on the left hand side on there.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the right hand side, I'm actually can't do left and right without simulating writing on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "You plug in the current value.",
                    "label": 0
                },
                {
                    "sent": "You current guess V sub K the cat guess and out on the left hand side pops the new value in your way.",
                    "label": 0
                },
                {
                    "sent": "Can you stop when the value function stops changing more than a certain amount?",
                    "label": 0
                },
                {
                    "sent": "Epsilon an you can start an arbitrary value function and this works.",
                    "label": 0
                },
                {
                    "sent": "So this is the planning version of policy Val.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duration when you know the model.",
                    "label": 0
                },
                {
                    "sent": "If you know the model again, the planning version of control, you do exactly the same thing except with these state action value equations.",
                    "label": 0
                },
                {
                    "sent": "So this is Q value iteration, same stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to very fast.",
                    "label": 0
                },
                {
                    "sent": "The value functions are important things.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The crucial mathematical property that makes all of our L value function based RL work is this notion of contraction, which is that when you do an iteration, you might start off with us.",
                    "label": 0
                },
                {
                    "sent": "So imagine your 2 dimensional value function Space 2 dimensional state space, the origin and their access is a true is.",
                    "label": 0
                },
                {
                    "sent": "The axes are labeled.",
                    "label": 0
                },
                {
                    "sent": "A star is a true value.",
                    "label": 0
                },
                {
                    "sent": "The optimal value function you start with some arbitrary guess that might be .1, and basically after every iteration the maximum the box norm shrinks by a multiplicative factor of gamma, so the maximum error across any state from with respect to the unknown true value function shrinks by multiplicative factor of gamma.",
                    "label": 0
                },
                {
                    "sent": "This is the underlying principle of the planning algorithm roll to the reinforcement learning algorithms at the TD and the Q learning algorithms that everyone is at least heard off probably.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was planning learning is the situation where you don't have a real system.",
                    "label": 0
                },
                {
                    "sent": "You don't have a model, so you don't have a model, but you can play in a real system and so you can generate experience.",
                    "label": 0
                },
                {
                    "sent": "You can still be in some state as subzero subscription or time.",
                    "label": 0
                },
                {
                    "sent": "You take an action, you get reward, next state action reward so you see experience like this.",
                    "label": 0
                },
                {
                    "sent": "And here's a pictorial representation here.",
                    "label": 0
                },
                {
                    "sent": "In some state you have a choice of action.",
                    "label": 0
                },
                {
                    "sent": "You take one action.",
                    "label": 0
                },
                {
                    "sent": "Which could lead to a choice of distribution over next states.",
                    "label": 0
                },
                {
                    "sent": "You sample one next state choice of action, so you see a particular trajectory like this.",
                    "label": 0
                },
                {
                    "sent": "This is what life looks like.",
                    "label": 0
                },
                {
                    "sent": "We are taking actions, we getting percepts and so on.",
                    "label": 0
                },
                {
                    "sent": "There are two classes and methods, indirect methods.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Direct methods.",
                    "label": 0
                },
                {
                    "sent": "What is the indirect method?",
                    "label": 0
                },
                {
                    "sent": "Most obvious thing you can imagine you take your experience, you estimate a model.",
                    "label": 0
                },
                {
                    "sent": "Here is a simplest non parametric form of a model estimation.",
                    "label": 0
                },
                {
                    "sent": "You just take the empirical estimate of the probability of seeing state J on action in state A.",
                    "label": 0
                },
                {
                    "sent": "Take your current model, find the policy that's optimal with respect to it and that's your estimate of the optimal policy is called a certainty equivalent policy.",
                    "label": 0
                },
                {
                    "sent": "As long as you get enough data you model to converge, and the optimal policy will converge.",
                    "label": 0
                },
                {
                    "sent": "Of course you can do parametric models, you can do much, much smarter than things that is, but this is a flavor of an indirect.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Indirect method the direct method.",
                    "label": 0
                },
                {
                    "sent": "Chris Watkins developed this Q learning, which is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Recursive form that you've seen before, except of course you can't take expectations, so you just take the the part that I don't have a pointer to the part that's in the square brackets with Alpha in front is the sample of the recursive form you would the recursion to iteration.",
                    "label": 0
                },
                {
                    "sent": "You would do so the actual reward you get the discounted best value of the actual state to get because you're getting a noisy estimate, you don't move fully to the new estimate, you mix it with the old estimate.",
                    "label": 0
                },
                {
                    "sent": "That's Q learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically it's a noisy noisy contraction.",
                    "label": 0
                },
                {
                    "sent": "Stochastic approximation theory works, and that's the result that Q learning converges property one.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much more so.",
                    "label": 0
                },
                {
                    "sent": "So far, I've told you what an MVP definition is, what the planning and learning algorithms are for policy evaluation and for optimal control.",
                    "label": 0
                },
                {
                    "sent": "Like it was the first Q learning, I thought I'd say this because Chris is here was the first approved.",
                    "label": 0
                },
                {
                    "sent": "We converted adaptive optimal control algorithm and made a huge huge social, logical and practical impact in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "This sometimes works, it's stochastic, sometimes works, sometimes doesn't OK.",
                    "label": 0
                },
                {
                    "sent": "So why aren't we done?",
                    "label": 0
                },
                {
                    "sent": "Well, part of it is I haven't told you to solve the expiration exploitation problem yet.",
                    "label": 0
                },
                {
                    "sent": "That is, how do we actually the Q?",
                    "label": 0
                },
                {
                    "sent": "Learning just learn the value function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell you how to behave.",
                    "label": 0
                },
                {
                    "sent": "Now you've got the optimal Q value function.",
                    "label": 0
                },
                {
                    "sent": "Then you know how to behave, but you don't know how do you.",
                    "label": 0
                },
                {
                    "sent": "How do you behave while learning?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well one answer is epsilon greedy.",
                    "label": 0
                },
                {
                    "sent": "Just some small property explore?",
                    "label": 0
                },
                {
                    "sent": "Or is Alice exploit and you can provide conditions under which this works.",
                    "label": 0
                },
                {
                    "sent": "And we have the other really interesting class of ideas, which I'll just save one.",
                    "label": 0
                },
                {
                    "sent": "Sentence, but there are tons of papers that I saw which I refer to here is called the idea of optimism under uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So this beautiful work that's been done in solving the exploration exploitation problem with the following idea.",
                    "label": 0
                },
                {
                    "sent": "Whenever you uncertain about something.",
                    "label": 0
                },
                {
                    "sent": "About transition probabilities somewhere or so on, or your value functions, you assume more optimism than maybe it's warranted.",
                    "label": 0
                },
                {
                    "sent": "So you pretend that things you don't know are better than things you know.",
                    "label": 0
                },
                {
                    "sent": "And that leads.",
                    "label": 0
                },
                {
                    "sent": "But of course if you keep doing that, then you'll forever.",
                    "label": 0
                },
                {
                    "sent": "Explore, and that's not good.",
                    "label": 0
                },
                {
                    "sent": "So that amount of optimism has to go down overtime, so shrinking optimism under uncertainty shrinking in time, optimism under uncertainty, is a class of ideas starting from exploration bonus work of Peter, Diane Buttery, cubed.",
                    "label": 0
                },
                {
                    "sent": "First sort of algorithm that really did this right, and then a whole bunch of algorithms that have been are being produced even today.",
                    "label": 0
                },
                {
                    "sent": "Exploit this idea of optimism, optimism under uncertainty, and it's.",
                    "label": 0
                },
                {
                    "sent": "You get really nice strong results polynomial time results for optimism and certainty.",
                    "label": 0
                },
                {
                    "sent": "So if you if you if you find this topic interesting, there's lots of things to read about here and finally, which I'll say again only in one sentence.",
                    "label": 0
                },
                {
                    "sent": "There is a Bayesian class of methods where you're doing planning if you like in the state of knowledge rather than the physical state.",
                    "label": 0
                },
                {
                    "sent": "And if you plan the state of knowledge, you can make you can optimally solve the expiration exploitation problem.",
                    "label": 0
                },
                {
                    "sent": "I won't say much more about that, so that's that's that's solutions to exploration exploitation.",
                    "label": 0
                },
                {
                    "sent": "So now again, what are we done?",
                    "label": 0
                },
                {
                    "sent": "We can we do?",
                    "label": 0
                },
                {
                    "sent": "Can we solve the problem of building agents that can live in the real world?",
                    "label": 0
                },
                {
                    "sent": "It really is stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, of course not because all the things that I told you about more or less for all for look up tables, but that of course is not practical.",
                    "label": 0
                },
                {
                    "sent": "So how do we deal with scaling?",
                    "label": 0
                },
                {
                    "sent": "And again, this is the last bit I'll do on tutorial and then I'll move on to the more interesting part very quickly for those if you've never or not been exposed to enforcement, at least maybe 40% of you didn't raise your hand or something like that.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a quick tour of these.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the function approximation idea is the most obvious 1.",
                    "label": 0
                },
                {
                    "sent": "Instead of storing things in a look up table, you store things in a function approximation.",
                    "label": 0
                },
                {
                    "sent": "You can just favorite function approximator back property on left radial basis, function C, Max nearest neighbor, decision trees, whatever.",
                    "label": 0
                },
                {
                    "sent": "Just replace indexing things into a table with a.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parametric form whatever the parametric form is linear.",
                    "label": 0
                },
                {
                    "sent": "I'm very partial to linear, but then the only idea of course here is what is the target value?",
                    "label": 0
                },
                {
                    "sent": "I go back.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is a target in a index look up table used to place value with the target?",
                    "label": 0
                },
                {
                    "sent": "What is the target you know in a in a function approximation scheme?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just you take the error and you do the gradient.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just the square that you think of the thing inside the square brackets as a squared error.",
                    "label": 0
                },
                {
                    "sent": "You will get.",
                    "label": 0
                },
                {
                    "sent": "You will get the obvious algorithms for doing function.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm partial to sparse coding, so I'll throw it up.",
                    "label": 0
                },
                {
                    "sent": "This is my favorite.",
                    "label": 0
                },
                {
                    "sent": "Everybody has their favorite.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function approximation that was my favorite one.",
                    "label": 0
                },
                {
                    "sent": "What do we know about function approximation, linear function approximation?",
                    "label": 0
                },
                {
                    "sent": "Worst case divergent can happen.",
                    "label": 0
                },
                {
                    "sent": "In practice it often works.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear neural net theory is not well developed.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor methods are probably not divergent.",
                    "label": 0
                },
                {
                    "sent": "Not very often used, I don't think, but there's not much theoretical guidance in this in this in this story.",
                    "label": 0
                },
                {
                    "sent": "Alright, feel free to stop me and ask anything.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying anything terribly exciting yet, just just review.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One last bit of review, and then I'll then I'll get any more interesting things.",
                    "label": 0
                },
                {
                    "sent": "Idea Montecarlo has been has been.",
                    "label": 0
                },
                {
                    "sent": "You know promoted here in this workshop quite a bit.",
                    "label": 0
                },
                {
                    "sent": "It's been very useful.",
                    "label": 0
                },
                {
                    "sent": "Here is an instance of Monte Carlo doing really interesting things in reinforcement learning, and in fact I would say the current state of the art algorithms and reinforcement learning are basically sampling tree based approaches for many, many classes of applications.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the idea of sparse sampling?",
                    "label": 0
                },
                {
                    "sent": "Here's a foundational idea.",
                    "label": 0
                },
                {
                    "sent": "So suppose you want to find the best action in a current state.",
                    "label": 0
                },
                {
                    "sent": "You're in some current state.",
                    "label": 0
                },
                {
                    "sent": "You want to find the best action.",
                    "label": 0
                },
                {
                    "sent": "You have a model.",
                    "label": 0
                },
                {
                    "sent": "That is, you can sample actions in next dates, so you imagine you have two actions, the solid action and the dotted action.",
                    "label": 0
                },
                {
                    "sent": "You sample the solid action a few times, so you sampled some next takes you sample, the dashed action a few times, and from each of those states you sample that other actions attached action, and you build out a tree.",
                    "label": 0
                },
                {
                    "sent": "So doing Monte Carlo Tree, constructing his Monte Carlo tree, of trying every action every sample stick.",
                    "label": 0
                },
                {
                    "sent": "And then you build the tree out to some extent, and then you back up values.",
                    "label": 0
                },
                {
                    "sent": "You take values.",
                    "label": 0
                },
                {
                    "sent": "Leaves could be 0 and you do the Max sort of the average of the values, and you take the Max or action.",
                    "label": 0
                },
                {
                    "sent": "Then you build up the value and you compute the value of every action in the root node and you pick the best one.",
                    "label": 0
                },
                {
                    "sent": "The empirically sampled best one.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this is an algorithm whose whose complexity is completely independent of the size of the state space.",
                    "label": 0
                },
                {
                    "sent": "And produces with high probability a near optimal action in the root node in the root state.",
                    "label": 0
                },
                {
                    "sent": "Now of course it you you when you not getting a free lunch here.",
                    "label": 0
                },
                {
                    "sent": "So all the algorithms value function based algorithms have at least linear dependence on state space.",
                    "label": 0
                },
                {
                    "sent": "This has no dependence on state space.",
                    "label": 0
                },
                {
                    "sent": "In fact this doesn't even make the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "So you can apply this in the non Markov setting just as well as a non is in the Markov setting.",
                    "label": 0
                },
                {
                    "sent": "So you can do it in continuous state spaces and non Markov settings.",
                    "label": 0
                },
                {
                    "sent": "All of these results hold of course it's exponential in the horizon.",
                    "label": 0
                },
                {
                    "sent": "Right, so the discount factor is very close to one.",
                    "label": 0
                },
                {
                    "sent": "Then the horizon has to be very deep and that of course can be a problem.",
                    "label": 0
                },
                {
                    "sent": "And so what's the insight here?",
                    "label": 0
                },
                {
                    "sent": "The inside here is the obvious Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "One right Monte Carlo beats curse of dimensionality because to get an estimate a good enough to get a certain accuracy of the expectation of a random variable, the number of samples is completely independent of the number of values or random variable can take.",
                    "label": 0
                },
                {
                    "sent": "And it's just that idea exploited in this tree form, and that's how it's beating the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And the and the current state of the art algorithms are inspired by UCT, which are much cleverer than this.",
                    "label": 0
                },
                {
                    "sent": "I'm just giving the foundational idea.",
                    "label": 0
                },
                {
                    "sent": "They decide much more cleverly which actions to explore, which ones not.",
                    "label": 0
                },
                {
                    "sent": "But but underlying idea is this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm done with this tutorial, lots of lots of nice results and applications are available.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning but.",
                    "label": 0
                },
                {
                    "sent": "All of them crucially assume you notice state space.",
                    "label": 0
                },
                {
                    "sent": "You know the action space, you know the reward space.",
                    "label": 0
                },
                {
                    "sent": "What I'm interested in, what drives my interest these days has been for awhile with for the near future, is trying to build flexibly intelligent.",
                    "label": 0
                },
                {
                    "sent": "AI systems, that's how it connects to cognitive science because the the exemplars of flexible intelligence are animals and human beings.",
                    "label": 0
                },
                {
                    "sent": "If you're doing narrow, are you doing?",
                    "label": 0
                },
                {
                    "sent": "Now is not a pejorative word now, it just means you have a particular problem you want to make a helicopter fly.",
                    "label": 0
                },
                {
                    "sent": "You want to schedule, you know the trucking companies.",
                    "label": 0
                },
                {
                    "sent": "Cross if you want to solve a specific engineering problem.",
                    "label": 0
                },
                {
                    "sent": "You notice state space because thousands of person years of effort has been put into figuring out what's a good state base.",
                    "label": 0
                },
                {
                    "sent": "What are the good representation of actions?",
                    "label": 0
                },
                {
                    "sent": "What the objective is?",
                    "label": 0
                },
                {
                    "sent": "What is reward function?",
                    "label": 0
                },
                {
                    "sent": "You know all that?",
                    "label": 0
                },
                {
                    "sent": "Great, we have good methods for not off the shelf yet, but pretty good methods for solving that class of problems.",
                    "label": 0
                },
                {
                    "sent": "Those types of problems if, on the other hand, you're interested in flexibly IPS, are rarely given, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if, on the other hand, you are, you magically get the hardware for an Android.",
                    "label": 0
                },
                {
                    "sent": "You log onto your Amazon.com and you order a household robot that is going to be a companion in your home.",
                    "label": 0
                },
                {
                    "sent": "What is it going to be?",
                    "label": 0
                },
                {
                    "sent": "State space?",
                    "label": 0
                },
                {
                    "sent": "How is it going to represent his actions?",
                    "label": 0
                },
                {
                    "sent": "What reward function should Amazon program into that into that robot?",
                    "label": 0
                },
                {
                    "sent": "Maybe Amazon's reward function should be don't come back, but you know if you come back, that's bad.",
                    "label": 0
                },
                {
                    "sent": "If you're sent back, that's bad for all of these questions are much trickier now, and those are the questions that I want to give you 1.",
                    "label": 0
                },
                {
                    "sent": "Basic idea about for each of those questions for the, for the rest, for the rest of this.",
                    "label": 0
                },
                {
                    "sent": "So this is my tutorial.",
                    "label": 0
                },
                {
                    "sent": "If anybody has questions about this part, how long did that take?",
                    "label": 0
                },
                {
                    "sent": "25 minutes.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this part?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Agents we don't have a real decline or well defined goals, so after I guess well, we should consider.",
                    "label": 0
                },
                {
                    "sent": "Right, so I haven't defined flexible AI what that means, except in opposition to classical engineering optimal optimal control where they were very well defined problem and you can take flexibility in many different directions, and I'm not going to redefine it except with the way of motivating these questions or what the state space should be, what the action space will be, whether award space B. Adjustable economy is an interesting approach that agent people have made progress on, but let me come back to that in the end and talk about.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk much about multi agent systems, so let's let's hold off on that.",
                    "label": 0
                },
                {
                    "sent": "Maybe if there's time at the end we can, we can.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this OK, so let's see.",
                    "label": 0
                },
                {
                    "sent": "I've already said all this.",
                    "label": 0
                },
                {
                    "sent": "What should be the state space for a dog or a person?",
                    "label": 0
                },
                {
                    "sent": "What's our state space is not very clear, right?",
                    "label": 0
                },
                {
                    "sent": "What we know what our perceptions are.",
                    "label": 0
                },
                {
                    "sent": "We don't know what our state spaces simultaneously have too many sensors and two few sensors, right?",
                    "label": 0
                },
                {
                    "sent": "We have very high dimensional visual inputs but lots of things that are important for decisions are not immediately available in our percept.",
                    "label": 0
                },
                {
                    "sent": "So we have the simultaneous problem that we have to we have to.",
                    "label": 0
                },
                {
                    "sent": "OK, so how are we making progress as a field in this?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of the methods form into the fall in the first category, so basically they are Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "Graphical model, probabilistic relational models, combinations of logic and probability.",
                    "label": 0
                },
                {
                    "sent": "That's where the bulk of the effort, and that in fact has been much of the focus of this summer school.",
                    "label": 0
                },
                {
                    "sent": "I don't want to say anything more about that except to say that it's been a very fruitful approach and people are making progress on this.",
                    "label": 0
                },
                {
                    "sent": "What I want to tell you about is an alternative.",
                    "label": 0
                },
                {
                    "sent": "Which we've been working on, which you call predictive representations of state.",
                    "label": 0
                },
                {
                    "sent": "And there's a bunch of work that has gone into it and I'm going to tell you the basic idea often.",
                    "label": 0
                },
                {
                    "sent": "So what's the distinction between these two approaches?",
                    "label": 0
                },
                {
                    "sent": "The Bayesian sort of structured relational hierarchical models essentially focuses on saying there are latent variables in the world that define my perceptions.",
                    "label": 0
                },
                {
                    "sent": "Let me try to infer those latent variables, and it's about trying to infer those latent variables.",
                    "label": 0
                },
                {
                    "sent": "The second idea that I'm going to the alternative that I will tell you about, takes a perspective that maybe the way we want to think about it is that we have a very rich notion of internal state, and in this rich notion of internal state.",
                    "label": 0
                },
                {
                    "sent": "We have perceptions may be processed.",
                    "label": 0
                },
                {
                    "sent": "Perceptions clustered abstracted perceptions, but we also have predictions that the human brain is very good is adapted to be able to make predictions and maybe predictions as part of our internal state.",
                    "label": 0
                },
                {
                    "sent": "Representations can do many of the things that latent variables are often used for, and so it's that idea that I'm going to try and communicate to you with one concrete result on that idea.",
                    "label": 0
                },
                {
                    "sent": "So the one idea I want to communicate is there's a class of things that latent variables are used for.",
                    "label": 0
                },
                {
                    "sent": "There are many things in fact, that latent variables are used for.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you in one particular setting.",
                    "label": 0
                },
                {
                    "sent": "How do use, how to replace the notion of latent variables with.",
                    "label": 0
                },
                {
                    "sent": "Instead just predictions about the future.",
                    "label": 0
                },
                {
                    "sent": "Using predictions about the future as part of your internal state representation to achieve the same end as A and of latent variables as an end of latent variables.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's something I want to communicate on the state.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the class of words I'm going to look at a discrete time.",
                    "label": 0
                },
                {
                    "sent": "Finite observations may be controlled, maybe uncontrolled.",
                    "label": 0
                },
                {
                    "sent": "By that I mean you have actions, or you may not have actions.",
                    "label": 0
                },
                {
                    "sent": "The class of words.",
                    "label": 0
                },
                {
                    "sent": "I'm specifically going to look at are the class of words that you often model with HMMS, or equivalently, Palm DP's so here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A graphical representation of a palm DP.",
                    "label": 0
                },
                {
                    "sent": "If you throw away the action nodes, which are the green nodes, you basically get Hmm's.",
                    "label": 0
                },
                {
                    "sent": "So the blue notes are the latent variables or the states.",
                    "label": 0
                },
                {
                    "sent": "The pink nodes can't tell if it's pink, pink nodes are the observations and time progresses this way, so this you've seen this before.",
                    "label": 0
                },
                {
                    "sent": "Hmm's actions with actions they become palm DP.",
                    "label": 0
                },
                {
                    "sent": "So what is the role of latent variables here?",
                    "label": 0
                },
                {
                    "sent": "The role of latent variables here is to make the world.",
                    "label": 0
                },
                {
                    "sent": "It is to capture all the knowledge there is in the history of observations so that you can predict you can get the conditional independence you want.",
                    "label": 0
                },
                {
                    "sent": "Right, that is the blue nodes.",
                    "label": 0
                },
                {
                    "sent": "The distribution overly blue nodes, the belief state space, which are distributions over hidden states, is a sufficient statistic of history so that you can make the same predictions about the observations on the future given the belief state that you would have made had you kept the entire history of observation.",
                    "label": 0
                },
                {
                    "sent": "So that's a function of latent variables, and I'm going to show you how to replace that notion with a notion of this predictions about the future as being part of the representation of state.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's work towards that.",
                    "label": 0
                },
                {
                    "sent": "What's what I mean by future in an uncontrolled system?",
                    "label": 0
                },
                {
                    "sent": "Is a sequence of possible observations.",
                    "label": 0
                },
                {
                    "sent": "You might get, so this is a length K future, so superscripts here denote possible future observations in a controlled setting.",
                    "label": 0
                },
                {
                    "sent": "Future would be a sequence of actions, an observation, because obviously I'll take actions in the future as well in a control system.",
                    "label": 0
                },
                {
                    "sent": "And the kinds of predictions I want to make.",
                    "label": 0
                },
                {
                    "sent": "Is in control system is just a probability of seeing a particular future in an in the control system with the conditional probability of seeing a future given that the agent does this particular sequence of action.",
                    "label": 0
                },
                {
                    "sent": "So that's the conditional prediction.",
                    "label": 0
                },
                {
                    "sent": "What I want is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I want is this.",
                    "label": 0
                },
                {
                    "sent": "So here is the extensional definition of a system, the extensional definition of system is the prediction of all possible futures.",
                    "label": 0
                },
                {
                    "sent": "So I lay down all futures in a vector form.",
                    "label": 0
                },
                {
                    "sent": "Sister conceptual Definition writes an extensional definition, so all futures of length one all features of length two or features like 3 and so on.",
                    "label": 0
                },
                {
                    "sent": "All possible futures and these predictions.",
                    "label": 0
                },
                {
                    "sent": "Are just the probabilities of those features an if I give you this vector, or every system corresponds to a vector like this is 1 to one mapping between vectors and systems like this.",
                    "label": 0
                },
                {
                    "sent": "So in building a model of such a system where I want to be able to do is build a compact intentional representation that is able to reproduce reproduce this vector.",
                    "label": 0
                },
                {
                    "sent": "Now, of course lots of constraints on the entries that is vector, otherwise you'll be in trouble.",
                    "label": 0
                },
                {
                    "sent": "You'll have a very large description of the system.",
                    "label": 0
                },
                {
                    "sent": "So any but the point is any exact model of such a system ought to be able to generate this vector.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I'm going to take this vector and convert it into a redundant representation.",
                    "label": 0
                },
                {
                    "sent": "Again, a conceptual one, which you called a system dynamics matrix.",
                    "label": 0
                },
                {
                    "sent": "Now I have rose.",
                    "label": 0
                },
                {
                    "sent": "Added on to that first row, the top row is the vector already showed you, which was all possible.",
                    "label": 0
                },
                {
                    "sent": "Futures in the null history, so all histories, all futures an now the matrix has a conditional represented as the conditional probability of a future given history.",
                    "label": 0
                },
                {
                    "sent": "Where am I going to work?",
                    "label": 0
                },
                {
                    "sent": "I'm building up towards a representation where I can show you that certain futures capture the same information.",
                    "label": 0
                },
                {
                    "sent": "Certain predictions capture the same information as latent variables, just just compact him just to show you that part.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to give you the definition of a future given history is just the conditional property of the future.",
                    "label": 0
                },
                {
                    "sent": "Given the history and in the.",
                    "label": 0
                },
                {
                    "sent": "Control system it's the conditional property of the observations in the future.",
                    "label": 0
                },
                {
                    "sent": "Given the history and the actions you would do in the future.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good so.",
                    "label": 0
                },
                {
                    "sent": "Again, this is just a conceptual representation.",
                    "label": 0
                },
                {
                    "sent": "This is not the representation obviously, because this is infinite.",
                    "label": 0
                },
                {
                    "sent": "You can't really do this important thing to observe is that all the roses determined by the very first row.",
                    "label": 0
                },
                {
                    "sent": "Now here's the interesting quantity.",
                    "label": 0
                },
                {
                    "sent": "What's the complexity of the system?",
                    "label": 0
                },
                {
                    "sent": "It is a way of defining the complexity of a physical system that has this conceptual representation is the rank of this matrix, which is called the linear dimension of the system.",
                    "label": 0
                },
                {
                    "sent": "So if there are N linearly independent columns, then the rank of this system is N. Why's that important?",
                    "label": 0
                },
                {
                    "sent": "Because I'm going to relate that to the complexity of this system of possible other ways of modeling the system.",
                    "label": 0
                },
                {
                    "sent": "OK, so again I haven't constrained the system.",
                    "label": 0
                },
                {
                    "sent": "By the way, in any way yet other than of course, discrete time, finite observations.",
                    "label": 0
                },
                {
                    "sent": "This is a completely general discrete time discrete observation system.",
                    "label": 0
                },
                {
                    "sent": "No constraints have been imposed on this other end, of course, I'm saying the system has some rank in which could potentially be infinite.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how would we model such a system?",
                    "label": 0
                },
                {
                    "sent": "Now before I talk about that in different ways of modeling system, if the rank of the system is N, that means I can pick out.",
                    "label": 0
                },
                {
                    "sent": "Anne columns.",
                    "label": 0
                },
                {
                    "sent": "Any possible predictions and possible futures that such that if I give you those N numbers?",
                    "label": 0
                },
                {
                    "sent": "For any history I can predict the.",
                    "label": 0
                },
                {
                    "sent": "Probability of any future in that history as a linear combination of those end numbers, right?",
                    "label": 0
                },
                {
                    "sent": "That's the definition of rank.",
                    "label": 0
                },
                {
                    "sent": "I can make any prediction of any any future in that history, just as a linear function of these these numbers.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just this equation that the prediction of any future T in that history age is a linear combination of the predictions of these particular numbers and futures.",
                    "label": 0
                },
                {
                    "sent": "For PFQ the vector PFQ given H is a vector.",
                    "label": 0
                },
                {
                    "sent": "And a linear combination and subte the weight of the linear combination ends of T. Crucially, empty is not a function of age.",
                    "label": 0
                },
                {
                    "sent": "That is, this entire column for this future T is the same linear combination of course, of all of these columns, like it doesn't.",
                    "label": 0
                },
                {
                    "sent": "The linear weights the combination don't depend on the particular history I'm looking at, so that the weights empty don't depend on H. Otherwise it be wouldn't be interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's talk about alternative models.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have an Ant order Markov model that is a system is such that remembering the last N observations is state.",
                    "label": 0
                },
                {
                    "sent": "Captures all the information there is in history.",
                    "label": 0
                },
                {
                    "sent": "What's the rank of such a system where there are N?",
                    "label": 0
                },
                {
                    "sent": "If there are.",
                    "label": 0
                },
                {
                    "sent": "If there are any length.",
                    "label": 0
                },
                {
                    "sent": "Basically, if if the last N observations matter and there are K possible observations, then the number of distinct histories in this matrix is bounded, right?",
                    "label": 0
                },
                {
                    "sent": "How many basically any two histories that are the same last N observations will have exactly the same rose?",
                    "label": 0
                },
                {
                    "sent": "So the rank of this system can't be very much.",
                    "label": 0
                },
                {
                    "sent": "Right, if there are N unique.",
                    "label": 0
                },
                {
                    "sent": "If there are K unique N length histories, then the rank system is at most K. Just obviously OK, so that's not, but that's not the interesting, interesting case.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the more in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sting case I just said what I just said.",
                    "label": 0
                },
                {
                    "sent": "Let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's special about this case boils?",
                    "label": 0
                },
                {
                    "sent": "It can be model is hmm.",
                    "label": 0
                },
                {
                    "sent": "What's interesting about words can be modeled as Hmm's is that there is no finite history?",
                    "label": 0
                },
                {
                    "sent": "That's a sufficient statistic of history of all of history, right?",
                    "label": 0
                },
                {
                    "sent": "That is, you have to remember things potentially infinitely far back in time.",
                    "label": 0
                },
                {
                    "sent": "Now you might think, therefore, that the rank of a system of this dynamics matrix that is an HMM is potentially infinite.",
                    "label": 0
                },
                {
                    "sent": "And the answer, of course, is that it isn't what I want to show you is that the Rank Office hmm system that has N underlying hidden states is no more than N. OK, and then I'll show you how the PSR representation works, and then I'll just summarize some of the research that has gone on.",
                    "label": 0
                },
                {
                    "sent": "PS I just want to leave you with this idea.",
                    "label": 0
                },
                {
                    "sent": "Those if you've not seen the PSR representation want to leave you with what appears.",
                    "label": 0
                },
                {
                    "sent": "Our basic is and I'll tell you about other things.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to show you now how I'm going to finish the proof.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you why it is the case that if the number of underlying states is at most N, the rank of the corresponding systems matrix is at most N. So.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you haven't seen belief state updates in HMM, this will not make much sense, but if you have, then hopefully should.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you in this equation B is the belief state on seeing history hjo.",
                    "label": 0
                },
                {
                    "sent": "If you, if you think of Hmm's and drop the actions right, then it would be just H. Oh is this sort of transformation of the belief at history?",
                    "label": 0
                },
                {
                    "sent": "H is basically be of age.",
                    "label": 0
                },
                {
                    "sent": "The old belief state times some transition problem, matrix B normalized.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's a linear transformation normalized that takes the belief data history age and computes the belief state history HL.",
                    "label": 0
                },
                {
                    "sent": "What that means is that the prediction that we care about prediction of any future given any history is actually a linear function of the belief state.",
                    "label": 0
                },
                {
                    "sent": "Not terribly surprising if you know.",
                    "label": 0
                },
                {
                    "sent": "Hmm, so you would know this right?",
                    "label": 0
                },
                {
                    "sent": "Same thing, also palm DP's.",
                    "label": 0
                },
                {
                    "sent": "So the critical thing about Hmm's or Palm DP's is that the prediction of any future given any history is a linear function of release date.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's true.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dictate my words, that's true.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to just take the system dynamics matrix and replace all of these predictions by.",
                    "label": 0
                },
                {
                    "sent": "By the predictions computer from the belief states.",
                    "label": 0
                },
                {
                    "sent": "And here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Think of the history.",
                    "label": 0
                },
                {
                    "sent": "Each of the histories corresponds to a belief state, right.",
                    "label": 0
                },
                {
                    "sent": "Each histories corresponds to belief state.",
                    "label": 0
                },
                {
                    "sent": "Think of belief states that correspond to unit basis.",
                    "label": 0
                },
                {
                    "sent": "Belief states where all the probabilities in one underlying hidden states.",
                    "label": 0
                },
                {
                    "sent": "Find such how many of those that can be and such belief states right?",
                    "label": 0
                },
                {
                    "sent": "Because there is an underlying hidden states, so the belief vector dimension is N, which means their end unit basis belief states.",
                    "label": 0
                },
                {
                    "sent": "Find the corresponding histories.",
                    "label": 0
                },
                {
                    "sent": "Each history, then, is a linear combination of the each role of this matrix is a linear combination of the rows that correspond to unit basis belief factors.",
                    "label": 0
                },
                {
                    "sent": "In fact, what linear combination precisely the beliefs?",
                    "label": 0
                },
                {
                    "sent": "Hence there are at most N linearly independent rows in this matrix.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the rank of matrix is at most N. Now there's a very quick proof, but all of this is can be easily shown.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that every palm TV with N nominal states hidden states is a linear dynamical system of linear dimension at most.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At most it could be much smaller then OK, so now I'm going to show you well forget this bit.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I'm going to show you now is the PSR representations.",
                    "label": 0
                },
                {
                    "sent": "Up until now, just not showing the representation.",
                    "label": 0
                },
                {
                    "sent": "So again, if the rank of the matrix is N, there are N. Futures that are correspond to N linearly independent columns.",
                    "label": 0
                },
                {
                    "sent": "If I give you.",
                    "label": 0
                },
                {
                    "sent": "Give you let's call but I will say this before.",
                    "label": 0
                },
                {
                    "sent": "Let's call those N test score test.",
                    "label": 0
                },
                {
                    "sent": "So here is my representation of state now.",
                    "label": 0
                },
                {
                    "sent": "Not the latent variable belief states, but instead the predictions.",
                    "label": 0
                },
                {
                    "sent": "My representation of state for history is my prediction of each of these futures correspond to the corporate core test of core futures.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to show you now is that if I just keep around these N numbers.",
                    "label": 0
                },
                {
                    "sent": "Which are predictions of the future then?",
                    "label": 0
                },
                {
                    "sent": "I'm basically keeping unbelief stay in effect.",
                    "label": 0
                },
                {
                    "sent": "When I'm keeping it on state, to be honest, to be set up better word.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics histories?",
                    "label": 0
                },
                {
                    "sent": "How do I show you that I'm going to show you that this is in fact a sufficient statistic of history?",
                    "label": 0
                },
                {
                    "sent": "So here's how I do that.",
                    "label": 0
                },
                {
                    "sent": "So you're in some history H. You have these end numbers as my representation of state.",
                    "label": 0
                },
                {
                    "sent": "I can predict any future I want in that history if I.",
                    "label": 0
                },
                {
                    "sent": "No answer MMFT.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can make any prediction I want if I know the corresponding weights, so let me show you how to update these end numbers when in history.",
                    "label": 0
                },
                {
                    "sent": "Hi, take action A and observe observe.",
                    "label": 0
                },
                {
                    "sent": "Oh again, if you if you're in hmm land, drop the A. OK, so how do I compute?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then numbers.",
                    "label": 0
                },
                {
                    "sent": "Well, here's the idea.",
                    "label": 0
                },
                {
                    "sent": "The prediction the remember then numbers have a meaning.",
                    "label": 0
                },
                {
                    "sent": "Have a non latent meaning they have a predictive meaning, so one of those end numbers is the probability of future KUSA by for his end core tests in the new history, HAO and just by probability by by you know by chain rule you get this equation.",
                    "label": 0
                },
                {
                    "sent": "That is, it is the prediction is that it's the.",
                    "label": 0
                },
                {
                    "sent": "It's the prediction of the extended future, a oqi given history.",
                    "label": 0
                },
                {
                    "sent": "H hang up just second divided by the prediction of just observing oh it action A.",
                    "label": 0
                },
                {
                    "sent": "Given age and of course, these can be computed linearly given the old representation of state PQ of age.",
                    "label": 0
                },
                {
                    "sent": "So now if I just keep around the linear weight combinations for just the action observation pairs.",
                    "label": 0
                },
                {
                    "sent": "That's this part for the denominator and the one step extensions to the end core tests.",
                    "label": 0
                },
                {
                    "sent": "Then I can keep maintaining state representation and make any prediction turned out I can make any prediction I want by just these parameters, so these M's are now the parameters of my model, just like transition probabilities in observation probabilities are the parameters of a belief state model of an HMM model.",
                    "label": 0
                },
                {
                    "sent": "OK, so all I really want to say about PSR's is a definite as I just tell you what PSR now I want to tell you about what the import of all this is in a minute, but you had to hang up.",
                    "label": 0
                },
                {
                    "sent": "What the parameters are?",
                    "label": 0
                },
                {
                    "sent": "These are the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this I can I can skip.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just said that, So what I've shown you now is that every discrete time dynamical system dimension N is equivalent to a linear PSR because of linear pair size with N core tests.",
                    "label": 0
                },
                {
                    "sent": "Hence, I've shown you that every hmm with at most with N nominal States and hidden states can be always be represented by on a linear PSR with no more than N cortex OK, what's the?",
                    "label": 0
                },
                {
                    "sent": "What's the big deal Huawei?",
                    "label": 0
                },
                {
                    "sent": "Is this potentially interesting?",
                    "label": 0
                },
                {
                    "sent": "This is potentially interesting for a variety of reasons, but one of the reasons, of course, is that latent variables.",
                    "label": 0
                },
                {
                    "sent": "Or like you can't observe them.",
                    "label": 0
                },
                {
                    "sent": "Predictions you can make and you can evaluate and you can test.",
                    "label": 0
                },
                {
                    "sent": "You can make consistent algorithms for computing predictions.",
                    "label": 0
                },
                {
                    "sent": "And to the extent that you can do that consistently as opposed to in a local minima M like way you can get benefits that you might not have from latent variables.",
                    "label": 0
                },
                {
                    "sent": "I sort of 1 obvious sort of benefit.",
                    "label": 0
                },
                {
                    "sent": "The other benefit potential benefit is that you know we're very finely tuned to make good predictions.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting that predictions can accomplish.",
                    "label": 0
                },
                {
                    "sent": "Some of the things that latent variables are typically used for now.",
                    "label": 0
                },
                {
                    "sent": "You can argue that latent variable models are good for generalization.",
                    "label": 0
                },
                {
                    "sent": "I think we can similarly argue that having predictions that are internal state representations might be very good for generalization in particular, and this is not yet a point that has been computationally demonstrated but to speculative speculative point planning is about projecting into the future, predicting the future.",
                    "label": 0
                },
                {
                    "sent": "Planning involves predicting, planning involves.",
                    "label": 0
                },
                {
                    "sent": "Predicting possible futures to the extent that our internal state representation contains projections of the future, it may make planning a lot easier than with representations that are not explicitly about that.",
                    "label": 0
                },
                {
                    "sent": "Again, this is just an idea.",
                    "label": 0
                },
                {
                    "sent": "It hasn't.",
                    "label": 0
                },
                {
                    "sent": "We're beginning to explore this idea.",
                    "label": 0
                },
                {
                    "sent": "We haven't really nailed out any of this sort of thing, so this is all I have to say about PSR's.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think this is all I have to say about this.",
                    "label": 0
                },
                {
                    "sent": "I want to switch to actions.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this come up right now?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Some more what?",
                    "label": 0
                },
                {
                    "sent": "Nonlinear, yes, we have done nonlinear PSR's we're we're working on that.",
                    "label": 0
                },
                {
                    "sent": "In fact, I looked at the AI stats AI STATS program, and there is a talk by Jeff Gordon and some of his colleagues CMU on.",
                    "label": 0
                },
                {
                    "sent": "On spectral clustering, methods for learning PSR like things.",
                    "label": 0
                },
                {
                    "sent": "So there is not a lot of work I would say, but there is some work on extending to nonlinear methods to to connecting this to dimensionality reduction approaches and so on.",
                    "label": 0
                },
                {
                    "sent": "So there is some big inning here.",
                    "label": 0
                },
                {
                    "sent": "Here is the core of an idea.",
                    "label": 0
                },
                {
                    "sent": "Basically what you're trying to do is compress this infinite matrix.",
                    "label": 0
                },
                {
                    "sent": "You want to find a compressed compressed representation of this infinite thing and now you can.",
                    "label": 0
                },
                {
                    "sent": "Think of all sorts of ways to take this infinite conceptual matrix, of which you can actually get samples.",
                    "label": 0
                },
                {
                    "sent": "And take samples of this infinite by infinite matrix and try to find compressed representations.",
                    "label": 0
                },
                {
                    "sent": "So people are beginning to go down that go down that Rd.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "There's a whole industry of people working on this yet.",
                    "label": 0
                },
                {
                    "sent": "OK, the action spot is actually goes much further back in time, so I'm going to go very fast through it, but I want to get to the rewards part, which is much more recent in time.",
                    "label": 0
                },
                {
                    "sent": "In fact, how many of you know where options are and know how options work and.",
                    "label": 0
                },
                {
                    "sent": "The RL notion of options, not the financial notion of options.",
                    "label": 0
                },
                {
                    "sent": "Actually, not very many, so maybe I should spend.",
                    "label": 0
                },
                {
                    "sent": "I should spend time on options.",
                    "label": 0
                },
                {
                    "sent": "OK actions.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the question here?",
                    "label": 0
                },
                {
                    "sent": "The question here is of course.",
                    "label": 0
                },
                {
                    "sent": "MDP's assume a uniform timescale notion of actions, right.",
                    "label": 0
                },
                {
                    "sent": "Actions that take one time step, but we can clearly planet all kinds of temporal scales right when you plan to come to Sardinia you thought about flights you thought about calling taxis.",
                    "label": 0
                },
                {
                    "sent": "You thought about walking to your door.",
                    "label": 0
                },
                {
                    "sent": "You talked about picking up your bag.",
                    "label": 0
                },
                {
                    "sent": "All of these things are completely different temporal scales with just completely flexibly able to plan and reason.",
                    "label": 0
                },
                {
                    "sent": "Not just strictly hierarchically, is not that it's strictly hierarchical.",
                    "label": 0
                },
                {
                    "sent": "I plan at one level, then I planted a lower level than I planned.",
                    "label": 0
                },
                {
                    "sent": "No, we can just flexibly plan at all kinds of levels simultaneously.",
                    "label": 0
                },
                {
                    "sent": "Question is, how do we begin to have a formalism that affords us this?",
                    "label": 0
                },
                {
                    "sent": "And this is work we did.",
                    "label": 0
                },
                {
                    "sent": "Oh, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe 15 years ago and I want to just at least as a as a want to explain the insight data and then I'll move on.",
                    "label": 0
                },
                {
                    "sent": "Turns out there are many different ways of thinking about this.",
                    "label": 0
                },
                {
                    "sent": "All worked on in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It connects the idea of macro actions, right abstract actions, macro actions.",
                    "label": 0
                },
                {
                    "sent": "Back, depending on what community comes from, there's different words for these things.",
                    "label": 0
                },
                {
                    "sent": "Options is something we developed which connects to Max Q.",
                    "label": 0
                },
                {
                    "sent": "Though if you know Max cure hams, they're all quite related.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Instead of an action that last one uniform time step, we're going to think of just general ways of behaving.",
                    "label": 0
                },
                {
                    "sent": "I want to be able to do this decision process stuff with general ways of behaving.",
                    "label": 0
                },
                {
                    "sent": "And so the key question is how do we think about general ways of how do we encapsulate general ways of behaving so that we can literally take all the MDP formalisms?",
                    "label": 0
                },
                {
                    "sent": "We had all the algorithms, all the Bellman optimality equations, all the things that come from that, and just replace primitive actions with these more flexible things.",
                    "label": 0
                },
                {
                    "sent": "And everything is still the same.",
                    "label": 0
                },
                {
                    "sent": "All the theory, all the algorithms, everything works.",
                    "label": 0
                },
                {
                    "sent": "That is a question about to answer their options answered.",
                    "label": 1
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "What's an option?",
                    "label": 0
                },
                {
                    "sent": "An option is is is defined by.",
                    "label": 1
                },
                {
                    "sent": "This three couple is right is triple the set of states.",
                    "label": 0
                },
                {
                    "sent": "The option is available in.",
                    "label": 0
                },
                {
                    "sent": "That's I the policy.",
                    "label": 0
                },
                {
                    "sent": "You follow the behavior generation mechanism when you're executing the option.",
                    "label": 0
                },
                {
                    "sent": "And by the way, do you think about the policy or anything else doesn't matter?",
                    "label": 0
                },
                {
                    "sent": "Some procedure for acting could be anything could be open loop could be closed, whatever it is, and then something that decides when the option terminates.",
                    "label": 0
                },
                {
                    "sent": "So some termination probability in every state.",
                    "label": 0
                },
                {
                    "sent": "You could have a property that says.",
                    "label": 0
                },
                {
                    "sent": "I told me they're only in that state when I get to that state.",
                    "label": 0
                },
                {
                    "sent": "Or you can say I terminated in 10 time steps.",
                    "label": 0
                },
                {
                    "sent": "Or you can say I terminate, you know, with some probability different states, that's it if you just define options this way.",
                    "label": 0
                },
                {
                    "sent": "All of the theory of MDP's basically extends.",
                    "label": 0
                },
                {
                    "sent": "Everything works, all the bellman optimality equations I'm going to show you.",
                    "label": 0
                },
                {
                    "sent": "Of course I'm going to say more about this, but the point is.",
                    "label": 0
                },
                {
                    "sent": "Everything everything is recovered.",
                    "label": 0
                },
                {
                    "sent": "Basically anything on the RL side pretty much is recovered.",
                    "label": 0
                },
                {
                    "sent": "All the results that you know or that have been obtained for strict MVP's are recovered with actions being replaced by this notion of option.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's give some more intuition about an option you know.",
                    "label": 0
                },
                {
                    "sent": "You could have a simple rooms were like this.",
                    "label": 0
                },
                {
                    "sent": "Your actions primitive actions could be going North, South East, West.",
                    "label": 0
                },
                {
                    "sent": "Or you could have behaviors that say get to the door.",
                    "label": 0
                },
                {
                    "sent": "So here is a behavior that basically option one gets you to the door of that of that room.",
                    "label": 0
                },
                {
                    "sent": "So they might have.",
                    "label": 0
                },
                {
                    "sent": "You might have options like that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the advantage?",
                    "label": 0
                },
                {
                    "sent": "Again, the advantages MDP's have a uniform discrete time step when you have options, you get the formalism.",
                    "label": 0
                },
                {
                    "sent": "What's called semi Markov decision process and summer Markov decision process is a basically a decision process in which the temporal duration between choice points.",
                    "label": 0
                },
                {
                    "sent": "Where you're making a new decision is a random variable because you know you pick an action, go to the door and might take a random number of time steps and then you get to the door or you fail somehow, you fail, and then you could make another decision.",
                    "label": 0
                },
                {
                    "sent": "Mike, so it's just it allows this flexibility.",
                    "label": 0
                },
                {
                    "sent": "So Sam Arkoff decision process is basically recover.",
                    "label": 0
                },
                {
                    "sent": "Or you can do all the things that you do for options with Sam Arkoff decision policies.",
                    "label": 0
                },
                {
                    "sent": "Now the distinction between Sam, Markov decision processes and MVP's options is that we have we have both.",
                    "label": 0
                },
                {
                    "sent": "We have the ability to be executing an option for also to be able to look inside the option and know what we're doing at a lower level, and that has certain advantages that the options were explored.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I wanted to.",
                    "label": 0
                },
                {
                    "sent": "So I will do 2 quick things and then I'll move on, move onto towards.",
                    "label": 0
                },
                {
                    "sent": "You can define.",
                    "label": 0
                },
                {
                    "sent": "You can define now an action set which includes these behaviors.",
                    "label": 0
                },
                {
                    "sent": "Call that capital O and you can define optimal value functions with respect to the V star of sub OS onvista.",
                    "label": 0
                },
                {
                    "sent": "NQ stars above S, Oh right.",
                    "label": 0
                },
                {
                    "sent": "Wherever you had a little a, you can now have little.",
                    "label": 0
                },
                {
                    "sent": "Oh for these options and all the value function like things that we talked about before can be can be can be recovered and but let me give you some more insight about that.",
                    "label": 0
                },
                {
                    "sent": "How can they be recovered?",
                    "label": 0
                },
                {
                    "sent": "Here's how they recovered.",
                    "label": 0
                },
                {
                    "sent": "Remember to reward and the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "The reward of primitive action is the number you get when you take an action and the transition probabilities, other transitions probabilities in the one time step.",
                    "label": 0
                },
                {
                    "sent": "The action takes of the distribution of the next.",
                    "label": 0
                },
                {
                    "sent": "It's how do we define these beasts?",
                    "label": 0
                },
                {
                    "sent": "These quantities for an option?",
                    "label": 0
                },
                {
                    "sent": "If you can do that, everything else will be will workout.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "The reward part for an option is just the expected reward expected sum of discounted reward you get when you start that option until termination.",
                    "label": 0
                },
                {
                    "sent": "So that's just the reward part.",
                    "label": 1
                },
                {
                    "sent": "It's so it's a number in this state.",
                    "label": 0
                },
                {
                    "sent": "If you start that option, what's the expected reward you expect to get until the behavior for that option terminates?",
                    "label": 0
                },
                {
                    "sent": "The more tricky part is the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "That's actually not all you do is simply asked what is the expected.",
                    "label": 0
                },
                {
                    "sent": "Probability of terminating impossible states.",
                    "label": 0
                },
                {
                    "sent": "Now it's not just as simple as that, because you have big discounting into account, so is the discounted probability, because you know if you if you donate in state five in two steps, that's a different.",
                    "label": 1
                },
                {
                    "sent": "Dan terminating State 5 in 10 steps.",
                    "label": 0
                },
                {
                    "sent": "So you predict discounted probability.",
                    "label": 0
                },
                {
                    "sent": "So in other words these these transition probabilities are no longer sum to one because they have discount factors taken into account, but that's it, they're just expected discounted probabilities of terminating in different states.",
                    "label": 0
                },
                {
                    "sent": "And with these definitions, you can write down the Bellman optimality equation.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means you can.",
                    "label": 0
                },
                {
                    "sent": "You can do it are value iteration or Q value iteration of Q learning or whatever it is and everything.",
                    "label": 0
                },
                {
                    "sent": "Everything just works.",
                    "label": 0
                },
                {
                    "sent": "All the map just falls out beautifully.",
                    "label": 0
                },
                {
                    "sent": "So very simple idea.",
                    "label": 0
                },
                {
                    "sent": "Were able to recover.",
                    "label": 0
                },
                {
                    "sent": "Now the flexibility with planning or learning.",
                    "label": 0
                },
                {
                    "sent": "With different time scales, because these options could be at many different timescales, right?",
                    "label": 0
                },
                {
                    "sent": "It's not a strict hierarchical notion.",
                    "label": 0
                },
                {
                    "sent": "You could have.",
                    "label": 0
                },
                {
                    "sent": "Some options are small induration, some options in a long in duration, and all that's fine, everything works, so we have the representation for being able to capture the ability to plan and learn with flexible time scale.",
                    "label": 0
                },
                {
                    "sent": "Of course the hard part is what should these options B and there's a lot of work on trying to figure out.",
                    "label": 0
                },
                {
                    "sent": "You know where the options come from?",
                    "label": 0
                },
                {
                    "sent": "How do we use experience in the world to learn options?",
                    "label": 0
                },
                {
                    "sent": "And then you know, think of options, skills.",
                    "label": 0
                },
                {
                    "sent": "We go about the world and we accumulate skills.",
                    "label": 0
                },
                {
                    "sent": "We know how to open the door.",
                    "label": 0
                },
                {
                    "sent": "We now to build some of his notable tables.",
                    "label": 0
                },
                {
                    "sent": "Some of us have a skill of, you know, PowerPoint.",
                    "label": 0
                },
                {
                    "sent": "Just kind of a shame, but so we have these skills and we we we can now plan and learn in terms of these skills and so there's obvious sorts of thoughts about how options might come from.",
                    "label": 0
                },
                {
                    "sent": "Doors are bottlenecks you can think of, bottlenecks more generally, so you may have you may build options towards bottlenecks, reusable skills that are Paula sub policies that are reused over and over again might be useful to encapsulate in form of a scale or an option.",
                    "label": 0
                },
                {
                    "sent": "People have played around with these sorts of ideas.",
                    "label": 0
                },
                {
                    "sent": "Is there is there an off the shelf method for learning options now?",
                    "label": 0
                },
                {
                    "sent": "But there are simple things that have been explored in terms of learning in terms of learning options, so I was going to say about options.",
                    "label": 0
                },
                {
                    "sent": "Just want to give you a sense of we are able to represent.",
                    "label": 0
                },
                {
                    "sent": "Ways of behaving at flexibel levels of abstract temporal abstraction and do planning and learning with just the same algorithms.",
                    "label": 0
                },
                {
                    "sent": "Everything just works beautifully.",
                    "label": 0
                },
                {
                    "sent": "Questions about that I'm going to skip all this in the in.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know many, many results become available.",
                    "label": 0
                },
                {
                    "sent": "So here is a. I'll give you one flavor of a result.",
                    "label": 0
                },
                {
                    "sent": "So here is, uh, you know continuous 2 dimensional domain is a possible landmarks.",
                    "label": 0
                },
                {
                    "sent": "We might have options that take us to landmarks from some local neighborhood, that Landmark.",
                    "label": 0
                },
                {
                    "sent": "So each circle is a local neighborhood centered or a landmark.",
                    "label": 0
                },
                {
                    "sent": "And you might know how to get to the landmark from that neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And then you can begin planning with these.",
                    "label": 0
                },
                {
                    "sent": "You can plan to go from neighborhood, from Landmark to landmark.",
                    "label": 0
                },
                {
                    "sent": "Has a way of navigating and can do much more faster planning this way than if you were to plan with low level continuous continuous behaviors of course.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could do something smarter than that.",
                    "label": 0
                },
                {
                    "sent": "You could realize, and this is called.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of simple result that we have that says basically you could start a 1 landmark.",
                    "label": 0
                },
                {
                    "sent": "Head head towards the other landmark, which is a red line.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you reach a location in which if you start it again, you would have switched to a different landmark.",
                    "label": 0
                },
                {
                    "sent": "You can switch and it turns out, so you can terminate options early and obtain benefits of planning so early termination of options is an easy, very easy thing that falls out of planning that can obtain great benefit.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to stop with this.",
                    "label": 0
                },
                {
                    "sent": "You can buy, they learn.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans, his other little thing.",
                    "label": 0
                },
                {
                    "sent": "I'll highlight again, just those if you find this interesting, you can find all kinds of literature on this.",
                    "label": 0
                },
                {
                    "sent": "So how do we learn models of options?",
                    "label": 0
                },
                {
                    "sent": "The reward and transition probabilities?",
                    "label": 0
                },
                {
                    "sent": "If we could only learn about options we're actually executing would be dead, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can only behave in one way.",
                    "label": 0
                },
                {
                    "sent": "It can only learn about the way you're behaving.",
                    "label": 0
                },
                {
                    "sent": "You can't learn very much, so you have to be able to learn about ways of behaving that you're not behaving it.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Here's an idea.",
                    "label": 0
                },
                {
                    "sent": "Suppose you've just taken a small primitive action, which is a first link.",
                    "label": 0
                },
                {
                    "sent": "In that change, the black chain, so you're behaving according to some option, which is a black chain using the first action.",
                    "label": 0
                },
                {
                    "sent": "But that action might be consistent with many options.",
                    "label": 0
                },
                {
                    "sent": "Say the dashed option.",
                    "label": 0
                },
                {
                    "sent": "Turns out there are very simple ways of learning about all options, updating the model of all options that are consistent with that primitive action you just took.",
                    "label": 0
                },
                {
                    "sent": "Even though your intention is to behave going to one of the options, you can update all options simultaneously that are consistent with the way you're currently locally behaving, and this is a very powerful idea that allows you to exploit recall interruption methods, so there's lots of clever ways of doing this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, Final day rewards.",
                    "label": 0
                },
                {
                    "sent": "Which is the most current work that I'm doing and occupies.",
                    "label": 0
                },
                {
                    "sent": "Most of my current time.",
                    "label": 0
                },
                {
                    "sent": "Have a half are good.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, this is a question of where the rewards come from.",
                    "label": 0
                },
                {
                    "sent": "Again, in narrow engineering tasks this may be completely clear, right?",
                    "label": 0
                },
                {
                    "sent": "You are trying to schedule an elevator system.",
                    "label": 0
                },
                {
                    "sent": "You know, somehow you've decided that what you want to do is minimize wait time.",
                    "label": 0
                },
                {
                    "sent": "Reward function is clear actually.",
                    "label": 0
                },
                {
                    "sent": "If you really honest about it.",
                    "label": 0
                },
                {
                    "sent": "Turns out even in narrow engineering task, it's often not clear what the reward function is.",
                    "label": 0
                },
                {
                    "sent": "An Andrew ING, I don't know how many of you heard this talk from Andrew Yang, but he is a great example of this.",
                    "label": 0
                },
                {
                    "sent": "He's got this.",
                    "label": 0
                },
                {
                    "sent": "Wheeled legged robots that got legs with wheels at the end.",
                    "label": 0
                },
                {
                    "sent": "And he's got.",
                    "label": 0
                },
                {
                    "sent": "He's trying to make this wheeled legged robot.",
                    "label": 0
                },
                {
                    "sent": "I know it must be a better name for it to be able to climb a stair climber step.",
                    "label": 0
                },
                {
                    "sent": "And so he gives it the obvious toward function.",
                    "label": 0
                },
                {
                    "sent": "You know you want to minimize the amount of time before it is on top of the step, so it gives a reward function, does reinforcement learning, and the agent does something really weird is sort of drags its wheels, or whatever it doesn't, and he said no, no, that's not what I wanted to do.",
                    "label": 0
                },
                {
                    "sent": "So he said, OK, well, I don't want to drag its wheels so it gives it.",
                    "label": 0
                },
                {
                    "sent": "He modifies the reward function.",
                    "label": 0
                },
                {
                    "sent": "Do not do that to penalize it for doing that, and then it does something completely different in bizarre.",
                    "label": 0
                },
                {
                    "sent": "So no, that's not what I want.",
                    "label": 0
                },
                {
                    "sent": "He goes to these four or five steps before it does something that he actually wanted to do.",
                    "label": 0
                },
                {
                    "sent": "Now of course, he didn't really know how to program what you want to do right?",
                    "label": 0
                },
                {
                    "sent": "But the behavior looks find so even in narrow engineering task, it's actually often not clear what the reward function should be.",
                    "label": 0
                },
                {
                    "sent": "In fact, people do play around with this.",
                    "label": 0
                },
                {
                    "sent": "So recently I've been playing out of idea what we call internal reward an essentially this thing called breaking the content out about tell you about.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Many people have played with internal rewards.",
                    "label": 0
                },
                {
                    "sent": "Jurgen Schmidhuber has 15 papers or something like that on this topic, including on curiosity, novelty, great stuff.",
                    "label": 0
                },
                {
                    "sent": "So he's got a perspective in a line of work on internal towards what I'm about to tell you about is coming at it from a different angle.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "One reason that reinforcement learning is powerful.",
                    "label": 0
                },
                {
                    "sent": "Is a reward functions.",
                    "label": 0
                },
                {
                    "sent": "Permit specification of what an agent should do without.",
                    "label": 0
                },
                {
                    "sent": "Without telling it exactly how to do it.",
                    "label": 0
                },
                {
                    "sent": "What the agent is to do without telling it how to do it right?",
                    "label": 0
                },
                {
                    "sent": "So RL theory and algorithms are completely insensitive to the source of the rewards.",
                    "label": 0
                },
                {
                    "sent": "Rewards just somehow given NRL starts from there.",
                    "label": 0
                },
                {
                    "sent": "And that's explains the generality so they can be applied to lots of things.",
                    "label": 0
                },
                {
                    "sent": "But of course, if you're doing cognitive science.",
                    "label": 0
                },
                {
                    "sent": "A fundamental question of interest is you know where do words come from where L is optimized.",
                    "label": 0
                },
                {
                    "sent": "And we're going to look.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to look at that very abstractly.",
                    "label": 0
                },
                {
                    "sent": "And of course, from the machine learning AI point of view is that this this generality comes at the cost of a decremental preferences parameters confound, so I might tell you what that is now.",
                    "label": 0
                },
                {
                    "sent": "What is this confound?",
                    "label": 0
                },
                {
                    "sent": "If I can go to the next?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slide OK. What is this preferences parameters confound?",
                    "label": 0
                },
                {
                    "sent": "Basically, what is the reward function contacted by the way disconnects a little bit, or at least in my judgment, it connects.",
                    "label": 0
                },
                {
                    "sent": "It's a way of trying to do what Nick was, or a way of approaching what Nick was talking about, so I'll try to make that connection as I get to it.",
                    "label": 0
                },
                {
                    "sent": "So reward function confounds two roles simultaneously in reinforcement learning agents, right?",
                    "label": 1
                },
                {
                    "sent": "It expresses the preferences of the agent designer.",
                    "label": 0
                },
                {
                    "sent": "I, as an agent designer, I'm trying to build an agent.",
                    "label": 0
                },
                {
                    "sent": "I have a preference or ways of behaving.",
                    "label": 0
                },
                {
                    "sent": "And that reward function.",
                    "label": 0
                },
                {
                    "sent": "And if I have a sufficiently general notion of preferences, have a utility function or reward function or ways of behaving.",
                    "label": 0
                },
                {
                    "sent": "So that's one rule.",
                    "label": 0
                },
                {
                    "sent": "That's the preferences role, but also what's usually done is.",
                    "label": 0
                },
                {
                    "sent": "Then I take that same reward function and put it into the agent and say that's your reward function optimize that.",
                    "label": 0
                },
                {
                    "sent": "That is, I I reach into the agents hardware software an insert that same reward function.",
                    "label": 0
                },
                {
                    "sent": "I make the agents goals the same as my goals as agent, Designer and of course, the way the agent will behave if it's an RL agent is going to try to achieve maximized behave so as to maximize their reward function.",
                    "label": 0
                },
                {
                    "sent": "So this is confounding the reward function for an agent are the parameters that the agent designer is setting.",
                    "label": 0
                },
                {
                    "sent": "That determine the agent's behavior through whatever algorithm the agent is doing.",
                    "label": 0
                },
                {
                    "sent": "But the reward function really is as the preference my as the agent designers preferences using cognitive science.",
                    "label": 0
                },
                {
                    "sent": "Evolution is the agent designer and the of course the agent and natural agents results are distinct and is no a priori reason.",
                    "label": 0
                },
                {
                    "sent": "They should be confounded.",
                    "label": 0
                },
                {
                    "sent": "Though most often when I say this, people say wait a minute.",
                    "label": 0
                },
                {
                    "sent": "Of course there should be the same.",
                    "label": 0
                },
                {
                    "sent": "Why should the agent have different goals in the agent designer?",
                    "label": 0
                },
                {
                    "sent": "Because the agent is trying to serve the agent designer White should goals be.",
                    "label": 0
                },
                {
                    "sent": "Why should the agents goals be different?",
                    "label": 0
                }
            ]
        }
    }
}