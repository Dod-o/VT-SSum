{
    "id": "cayvy7boxmcyz4g5mj3y4rfwaj24mwns",
    "title": "Machine Learning in Computational Biology (the frequentist approach)",
    "info": {
        "author": [
            "Jean-Philippe Vert, MINES ParisTech"
        ],
        "published": "May 13, 2014",
        "recorded": "April 2014",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Medicine"
        ]
    },
    "url": "http://videolectures.net/mlpmsummerschool2013_vert_computational_biology/",
    "segmentation": [
        [
            "Yeah, so I'm sorry I I forgot what I had to talk about and I realized.",
            "It's not that.",
            "So OK, machine only for personal genomics.",
            "I kind of related will see.",
            "So no, I actually I did a subtitle frequentist approach just becausw.",
            "Obviously if you were here this morning to listen to Neil's presentation equal many things and he said many times you will see JP will do this afternoon so I had to follow his talk and I think the main difference between our talks.",
            "Easy, I will talk about similar objects and similar approaches, but not from a probabilistic abbasian POV just presents you the parallel you know in statistics and semester machine learning.",
            "There are like for historical reasons, communities and one one way to split the community into is to say their vision and frequentists.",
            "So I did not realize I was frequentist until recently, but I think you would qualify this talk as frequentist as opposed to the beige and.",
            "And because I'm the second one to speak.",
            "I suppose it's because I'm right and it's wrong, but.",
            "But we'll see."
        ],
        [
            "OK, so uh, actually I'm quite a domestic about what I want to talk about too.",
            "You know, there are many things because of need.",
            "Again, you totally said I have to talk about this, this and this.",
            "So I try to put everything here more.",
            "Is the goal here is, you know most of you are students, so this talk is for the students.",
            "It's not for the advanced researchers.",
            "You can go and grab a coffee if you want AB.",
            "Suppose I assume some some of you, probably most of you have some background in machine learning statistics, but not all of you.",
            "So what I plan to do is to expose you to some of the.",
            "Classical and and you know things that tend to work, and in particular there are like two big two big topics.",
            "I want to cover one.",
            "I will try to cover more less today is this domain of learning with kernels.",
            "So we already talked about them this week, but I will try to go back to the basis and give you some.",
            "I mean, the way I understand them and how they can be helpful for you.",
            "So this will be the second part and the second big thing I want to cover mostly tomorrow will be this idea of sparsity, an structured sparsity which is very popular in machine learning, which is also a big part, so it should be, you know, just for the second part I have a course I teach across in 20 hours here I have to condense that in just 40 minutes, so obviously I will not go to the details, but hopefully try to give you some idea, Nan.",
            "So you know, as Bob said, I will give you some Hammers, so you have the Hammers of kernels, then the Hammers of Lasso and company, and with that hopefully will be able to invent new Hammers to solve the problems which are partially solved.",
            "OK, but let's start so."
        ],
        [
            "Let's start with some broad introduction, and I think so.",
            "The good intuition is to show that these two approaches, which sometimes you know people say, are you doing kernels or sparsity.",
            "In fact, it's about the same and you can relate them at least in the frequentist way or in a general framework, which I think is the most important part of this presentation.",
            "Trying to give you my point of view or some insight of how you can do machine learning with complicated data in high dimension, at least one way to do it, and we see that the implementation of this ideal is to either kernel or sparsity or other things."
        ],
        [
            "So examples."
        ],
        [
            "Uh, how many examples that they were given this week on why why we made sense to the machine learning for in biology and personalized medicine.",
            "I will not go through everything, just show you some examples.",
            "I will try to take us illustrations or some of the applications.",
            "Some of the methods that will describe so one of them.",
            "So I walked in Cancer Genomics a bit and you know?"
        ],
        [
            "Some other things I don't know if you can see some of the things we're interested typically is trying to understand what's the difference between a cancer cell and a non cancer cell.",
            "If you have a cancer cell or cancer patient with cancer, what's the prognosis?",
            "Which is what's going to happen in the future?",
            "Is there risk of relapse?",
            "What kind of trades would work?",
            "So for that we make measurements on people we talked alot of Geno types deep.",
            "Some mutations there are other things you can which are which have a lower resolution which which you can look at.",
            "Look at like if you look at the karyotype so the set of chromosomes that you see in a cell.",
            "It's well known that if it's the first time you you come from machine learning, this may be the first time you see that you know cancer cell.",
            "You know it's a human cell, but it's not made of 23 pairs of chromosomes.",
            "Very often the chromosomes have been reorganized, shuffled.",
            "Sometimes you have three chromosomes.",
            "Sometimes you have fusions of things, so this is always the case and so."
        ],
        [
            "Measure that either this was a picture, but we have technologies now to to measure a bit through the genome, like how many copies of of of.",
            "Offer loss or place in DNA.",
            "There is in a cell normally in each of your cells have two copies, but sometimes in cancer cells you have three or less or more, so you can measure that attended data."
        ],
        [
            "And you can come up with problems which are very practical problems like suppose you collect data on cancer cells and normal cells.",
            "Well, this is this is not relevant, but for example in this case for pregnancy, suppose you have different types of cancers.",
            "I mean one type of cancer but different outcome like answers that lead to metastasis within five years.",
            "Or does that don't can you?",
            "From the structure of the genome, guess who's going to risk of relapse?",
            "OK, so one way to do it, and that's how it's done.",
            "Now you know it's to collect data.",
            "And to create a database.",
            "So here's this point one.",
            "But of five patients on the left, this is the profile.",
            "So he tinkered with the structure of the genome.",
            "5 disarmer animals.",
            "So 5 profiles of minimize that did not relapse within five years and not the right design minimized.",
            "I did relapse and a natural question is can you process this data to learn some model that would be able to predict what's the risk of relapse and therefore it without the therapeutic choice of course."
        ],
        [
            "That's a classic car example, so this was a you know you can do that at the level of the genome.",
            "You can also look at of course at the level of what happens after the genome, meaning gene expression.",
            "Perhaps proteomics and one of the problems."
        ],
        [
            "Classical applications of.",
            "You know, machine learning, like surfer for president as a junior genomics is is looking at gene expression data either using the microarrays or using sequencing technologies."
        ],
        [
            "And making these these studies of either diagnosis, meaning when you have sub types of cancers, can you from the profile of gene expression guess or learn a model that would discriminate between different subtypes of concerns?"
        ],
        [
            "Or can you predict again what's the risk of relapse, and this is probably one of the biggest impact of this research at the clinic today, which is the development of molecular signatures based on gene expression to predict the risk of relapse in early breast cancer.",
            "OK, so this has to be done.",
            "I think from the statistical point of view is not a solved problem, it's an open problem, but at least you already have products which are on the user, the clinics which measure markers, and I have estimated the predictive model from.",
            "In this case, you expression to predict the risk of having a relapse."
        ],
        [
            "There are many other examples of machine learning, so this is less about personal genomics.",
            "But you know."
        ],
        [
            "When you, when you when you sequence genomes it could be pathogens or it can be new things.",
            "You you you you can now we can sequence everything so it's easy to sequence to read the where are the jeans and then you have questions of an automatic annotations when you detect and Eugene, you metagenomic sample.",
            "Can you automatically predict what's the function of the of the gene?",
            "What's the structure?",
            "And this is directly to machine learning problems like you have sequences of proteins which are known to have a function, this is.",
            "Just one example.",
            "You have sequences.",
            "Then you ask the question from these sequences.",
            "Can I run a model that, given a new sequence that is obtained from sequencing, to automatically annotate this the sequence as this type of protein with these functions with this structure etc.",
            "So again, this kind of things are nowadays are solved by machine learning like approaches which have been trained on data using various models that we saw that I will present.",
            "So this is working."
        ],
        [
            "And of course, so I think this is what the customer called all day by infirmity.",
            "So I mean, it's not supposed to be very MoD, classic classic.",
            "So enjoy discovery.",
            "This is a classic problems, but it's also really something that where machine learning is used is when you want to design a new drive.",
            "One way to do is to design A Droid because you feel you can do it.",
            "Another way is to screen molecules, banks of molecules and try to measure if some of them are active or walking some say.",
            "And then you collect data like many molecules and some of them are T. Some of them are not active and from this you want to learn a rule that would predict if a molecule is active or not based on this data.",
            "OK, so this is a classical by infirmities nowadays.",
            "We do that with other like finding a drug that works on a patient with a given genotype and gene expression.",
            "So it's just the same question, except that you just don't only have the drag, but you also have the profile of the patient and this is then personalized medicine to guess to find a rule that would predict which drug will work on which profile.",
            "OK, so I want to."
        ],
        [
            "The longer path I can skip this one."
        ],
        [
            "All these examples, so I choose, you know, a small number of examples which turned out to be in all cases exactly the same when you conceptualize them.",
            "These are instances of very typical problem of machine learning, which is supervised binary supervised classification."
        ],
        [
            "Basically you have samples.",
            "You have examples that have two types here.",
            "In this picture, each dot is a sample or molecule you have active and non active or you have a relapse or no relapse etc.",
            "And from this data you want to learn in this case since you could separate operation, the two populations by by your line so."
        ],
        [
            "At at the end, what you want is when you have new new object, new samples or numeric rules, be able to predict.",
            "Which one will be active or which one will not be active?"
        ],
        [
            "OK, so it's all very classical."
        ],
        [
            "But the thing that that is a bit specific, so not specific to only two to personalized medicine, but that makes it difficult, is that you know if this was ready."
        ],
        [
            "Picture then it sold.",
            "It has been sold 400 years is done now.",
            "The problem is with the difficulty with the problems we have is to."
        ],
        [
            "You see that we have data which have usually very high dimensions.",
            "Obviously if you measure thousands or millions of snips, or profiles or descriptors, then you don't live in two dimension.",
            "You live in dimension millions.",
            "So this is to imagine it and this huge space is filled with just a few few points.",
            "Be 'cause you know when you have cancer patients, so you can collect a few hundreds, perhaps, but you will never have 10 to the 50 points in this space, so.",
            "So it's a it's a particularity of modern machine learning to try to solve problems with small number of samples in high dimensions.",
            "Also in most of the examples I assured that some in some cases the data very structured like I sort of the molecules are like graphs.",
            "When you get the profiles of copy number, it's not just a vector, it seems to be a noisy profile.",
            "There's something underlying it etc.",
            "So there is this.",
            "This so question is should we treat this data?",
            "Are just points in?",
            "Rachel Space or should we walk in the spaces you know, with some structure that describes the underlying phenomena.",
            "There other properties.",
            "But like often we have models with different types of data.",
            "This is very important.",
            "We need faster implementations etc.",
            "But so in the rest of the talk we mostly focus at least to today on.",
            "How do we approach?",
            "So what are the in the frequentist?",
            "Well, at least the ways that have been found to try to solve problems in high dimension.",
            "With few samples and sometimes structured data and we will try to know too.",
            "This will be my most important message to give you some flavor.",
            "Some idea of how we can beat the curse of dimensionality, innocence, try to learn.",
            "Even though we don't have enough points in a huge space."
        ],
        [
            "OK, so this will be the second part of the introduction.",
            "How do we learn in high dimension?"
        ],
        [
            "So so to try so.",
            "In the talk, there will be a few equations I don't know difficult equations, but I will use mathematical notation so just not to lose.",
            "I mean to make sure you're with me the basic notations we try to keep throughout the talk are very classical, I assume so to formalize these problems, I will assume that we have a set of patterns, But these are the data, like the genomic profiles of the molecule cycle that X OK, it lives in a space, capital leaks and what I want to predict is a. Spence, why so?",
            "In this case it was binary, but it could be continuous valued.",
            "It could be structured.",
            "There could be other things.",
            "I would just focus in binary case, which is called classification or sometimes so trying to produce something continues.",
            "It's called regression.",
            "It's not a big deal, and so the input of what we want to design is just a set of labeled examples.",
            "So labeled example is a pair where you have X&Y molecule and the label active or not active, and you want you have any of them.",
            "OK, this is the input.",
            "And why you want to design is a machine, is a is an algorithm program that takes this as input and outputs a function F. That goes from the Space East to the space.",
            "Why?",
            "Because this is a function that will make your prediction.",
            "So if you have a new object, XF will predict value Y, which could be binary or continues.",
            "By applying the function F to the point X. OK, so all the question of machine learning is how do you go from this set South, which is the training set to the function F."
        ],
        [
            "Let's look at an example.",
            "So these are just stories important and I forgot to write this taken from a very nice book by his teacher.",
            "Any elements of statical learning.",
            "So this is a case where you went into D and I hope you see the colors.",
            "We have a blue dots and the orange dots at the question is can you learn a rule to separate them?",
            "Perhaps this is in the space of active or not.",
            "So one thing you can do, I will detail it but apply your method called least squares.",
            "OK so you say blue icon.",
            "Emma bless one arrange minus one.",
            "I do least square regression and it gives me a line that's more separated them.",
            "So if you look at that.",
            "It doesn't seem very very good for so.",
            "Sorry for some reason, and in particular it seems that there is a bit more structure in the space.",
            "It doesn't seem that the line is very good despite them because these are not two Gaussians innocence are real data, so you say well here the problem here is that we see the model seems to be too simple."
        ],
        [
            "Now another algorithm that you can use is called one nearest neighbor.",
            "I will not present it in detail, but I assume also you know it is just when you have a new sample you will just like if you want to predict the color of a new point, you will check what is the closest point and you will predict the color of its closest friend.",
            "OK, so in terms of how it separates the space in predictions, blue or orange is just you know for each point like this point the closest is blue, so it's in the blue part and there is just you know Frontier here like.",
            "Then you are probably at the same distance from arranged to a blue, so you don't know.",
            "But if you're in the blue part, you make a brew prediction in the range maker arrange position so you see that here you when you apply.",
            "You know this technique of nearest neighbor.",
            "You create function F. So this is the function F which makes a prediction which is a.",
            "There is something better than the line because it seems that you capture more of the perhaps complex structure of the of the of the space.",
            "But there are things like here.",
            "You would predict all these region in in blue, even though it seems that this could just be an outlier like if you assume some, you know some enduring freedom.",
            "This should be in the orange part and here you overfit the training data.",
            "So."
        ],
        [
            "These are important important concepts here.",
            "In both cases we see we don't do a very good job, but for two different reasons.",
            "One we do.",
            "On the left we do.",
            "Underfitting means that we have two strong assumption it's ready to desponding stocks.",
            "We make assumption on the left.",
            "We assume a linear model and probably is not well adapted and on the right we don't.",
            "There is no assumption about the model, but it seems that when we when we learn the model, we fit too much today to an so I will try to show now that these two things we.",
            "Death many names but one way to call them is the notion of bias and various and and see that in fact there is a tradeoff here that that's the key thing to understand.",
            "To run in high dimension.",
            "So what I mean by biases that this model is probably very far from the.",
            "True underlying model.",
            "If there could be a trend right model.",
            "So there is a difference between what we can fit and the true model.",
            "What I called the variance is more the idea that here what I draw here seems to be very sensitive to the training data.",
            "Like if I change the color of 1.1, blue point becomes one or energy will change.",
            "This is not the case on the left but on the right it is and this is the reason why on the right it will not be good.",
            "It's too sensitive to the training and we call that variance."
        ],
        [
            "So a very so, by the way.",
            "Of course the slides will be available, so sometimes they will go quick and you don't need to go to the details.",
            "You will have time to look at them, but if you want to formalize it, we can.",
            "We can, you know, under some assumptions made very simple computation and here without going to the details, this is just an exercise.",
            "If you if you want to get familiar with it, we can formalize this idea of variance and bias.",
            "But just saying, suppose you know I have my data, I want to run a function F. OK, it's my function F and then I make a prediction on the new point.",
            "Dicks, how good is the prediction?",
            "And if you want to measure, you know and there are some you average over the training.",
            "Set it so if you want to measure how good your prediction so F hat is what you have learned.",
            "If you predict on a point X not value effort of is not and the true value is why you not?",
            "You can try to estimate the difference and with just a few a few lines you will see that the difference under some assumptions.",
            "Decompose into a sum of three terms and won.",
            "The first term is the noise in your data, so this is something you will never beat.",
            "It means that you know, suppose the label are random.",
            "I mean if they if the if the label is something plus some noise, you will never be able to get the noise.",
            "So the best we can do is predict the noiseless version.",
            "So there is the noise in the error which you cannot beat.",
            "But then there are two terms which are exactly the bias and variance, so the bias is how far on average you're so.",
            "You know, on average you will make some prediction and sometimes on average duration is not a good one.",
            "So the difference is called the bias an."
        ],
        [
            "It's rated a bit on the left.",
            "You can show that with the line on average will not make a good prediction because you're restricted to linear model and if the true model is not in our you will not be able to measure the true on, so it's easier."
        ],
        [
            "There's an you have the second one, which is the variance, which is how you know depending on the training data you position will vary, and if it varies too much then it has a cost at the end.",
            "OK, so in this case the simple computation really tries to quantify the notion of of this trade off of this balance between bias and variance, meaning that at the end the error you make is the sum of bias and variance.",
            "I insist on that 'cause when you talk to biologists in OK so are doctors etc.",
            "It is very easy to to imagine models with very small bias be cause you know you say I want to predict phenotype from genotype the what is the true model and everybody has an ID.",
            "It has to include many snips.",
            "It has to include many environmental factors.",
            "It has to include other things so it's very easy to imagine what could be the true model and to imagine designing a model with good covariates and perhaps many covariates that that would have a small bias, meaning that potentially could fit that remodel.",
            "But very often when we do that we are dead because of brians.",
            "And very often this would be the spirit of machine learning.",
            "It's much better to run with the wrong model.",
            "We know it's wrong, but.",
            "I tried to reset it.",
            "If we can decrease the variance at the cost of increasing the bias at the end we make you better prediction.",
            "OK, so this is a very important message.",
            "I would really straight it, but the idea that when we do learning we don't only care about what would be a realistic model or what are the true features that contribute to the prediction.",
            "What we want at the end is to make fewer errors and to make sure it works.",
            "We need to control the complexity of the model and sometimes accept 2 is good to have wrong models but wrong and simple models are often better than.",
            "Realistic but complicated models."
        ],
        [
            "So let's go back to the to the least square again.",
            "I will not go to the technical details, but probably most of you have seen that before.",
            "If you want to do linear regression, you know you can write any question like I want to infer a function F, which I so I will use the beta instead of the value, but I assume that my linear model is parameterized by a vector beta, which is the slope of the.",
            "Of the of the of the linear model, and therefore that I run a model made of sum of terms which like this morning I I summarized by an inner product between the vector beta and the data X OK, the least square is simply.",
            "So when I say I fit the least square model means that I try to find the vector beta that minimizes the sum of squares.",
            "So this is a frequentist way to describe it.",
            "I don't assume there is Goshen model etc.",
            "I mean I don't say it, I just say that I define.",
            "The cost of loss function and I said it would be Theresa Bitter that in total we have a small average or sum of squared error between my model and the true data.",
            "OK so if you say I want to minimize this this sum of squares just write the equations, differentiate and you will find that there is at least if you can invert a matrix there is a solution which is the least square solution.",
            "This equation.",
            "OK so this is the least square solution to regression."
        ],
        [
            "Now this thing is interesting and it has been around for for many years, and in particular there is a very important theorem.",
            "Innocence, saying that this is the best estimator you can have at least the best.",
            "So when I say the best.",
            "It's called the blue estimator, so blue means is the best, but linear OK unbiased estimator, meaning that if you consider always to make a linear model not only by minimizing the the sum of squares with any dinner model such that on average it does the good position if the true model is dinner.",
            "So here I assume I have a true linear model an I want to estimate it by your linear model.",
            "Then this theorem says that the best you can do is to use the square.",
            "At least if you don't want to have a bias model, and in many cases in many cases in a sense to refuse the buyers because you say, well, you know, bias is a systematic error, so it seems to be stupid to say I want systematic errors.",
            "So among the class of models with linear models without systematic errors.",
            "This quote is the best.",
            "So what I mean by best it has a smaller variance, yeah?",
            "Yeah, yeah, yeah.",
            "So probably I should shuffle my size.",
            "I would say sorry so the question.",
            "Yeah I'm trying I control myself because I went too fast so at the end of my message will be this one but I make you know short thing like if you take your book in statistiques they will tell you do the square because it's blue.",
            "And when you do that, it it makes sense if you do classical statistics in small dimension with many points in this case.",
            "The the bias error is more important than the variance because the variance decreases with the number of points and so in my equal."
        ],
        [
            "And if I come back to this equation, it means that if you constrain yourself to having a 0 here, then the minimizer of the of the variance is the square.",
            "OK, now exactly as you point out what I want to say, and I will say it again now is that even though this car is the best among the models with their bias.",
            "It's not the best in terms of total error becausw you can do better if you increase the bias at the cost of decreasing variance.",
            "So the question then is what?"
        ],
        [
            "Is the variance of this quite easy?"
        ],
        [
            "Pretty big."
        ],
        [
            "And in fact, you know this morning near showed a picture of the Marathon runner.",
            "Instead we want to predict the future.",
            "And I have the question at the end.",
            "It takes preparation, but he said it's a it's a way to represent high dimensional spaces because in high dimension the test points are never in the same place as the threading point.",
            "So my way to represent high dimension would be more this picture here.",
            "These are the pictures I showed you initially saying this is linear model and this is 1 nearest neighbor.",
            "Now I change my way of thinking and I just say these are two linear models.",
            "But this one is in small dimension and this one is in high dimension and in fact it's more so it's wrong, but morally correct if you you know if you make will talk of Gaussian regression like Gaussian processes etc.",
            "These are linear models in some infinite dimensional spaces.",
            "So the way to visualize it is this way and for me the difference between small and standard small dimension and statistics.",
            "This is the left and here we use the square.",
            "But if we're in high dimension which is modern machine learning and personal.",
            "It is seen we leave here and this is a very simple model.",
            "Is it?",
            "It would be what the square would do.",
            "This is a result of this square and we see that in high dimension even the least square, which is simple as huge variance.",
            "In fact you can you know the variance is proportional to the number of parameters.",
            "So if you have millions of parameters to estimate and the variance of your model would be very large.",
            "OK so my message here is that there is a big difference between small dimension and high dimension an in high dimension.",
            "In this case, it seems you use a simple model simple linear model, but still the error is dominated by variance.",
            "You need to do something, otherwise you cannot the error, which is the sum of bias and error will be large because of the variance term."
        ],
        [
            "OK, so how to?",
            "How to kill the variance?",
            "How to do something that would you know, increase, improve the model, make better prediction by trying to reduce the variance at the cost, perhaps of increasing the bias?",
            "Well, a solution.",
            "So this is a frequentist solution and you could rephrase it vision approach using priors etc.",
            "But I will not go in this direction.",
            "One way to do is to use what I call shrinkage estimators.",
            "There are many names for it, but let's look at what it is.",
            "So shrinkage estimator is simply a recipe that requires three ingredients.",
            "The first one is to say we will focus on the family of model.",
            "So for example in all my talks it will be linear models, so I receive my set to request off linear models."
        ],
        [
            "Now for independently of that you will.",
            "I will define some objective function which is related to a risk or loss function.",
            "So for this square we said we quantify evil in our model is good in terms of the total squared error so.",
            "Again, no problem speaking.",
            "There burns just arbitrary loss function an I say for any model, any linear model, but I can check if it's good or not based on it's it's called the risk in in our domain.",
            "So based on its risk which is there in this case, the average squared error.",
            "And so at least square was just a minimize the risk, subject to the fact that you are in our model.",
            "This was re square what those are.",
            "Shrinkage estimator is to other third layer in this in this architect."
        ],
        [
            "Sure, which is to add a constraint so it is very simple.",
            "You know there is nothing complicated.",
            "Here is just to say because when we just look at all in our models, it's already too complicated.",
            "We are already in high dimension, huge variance away to reduce the variance is simply to constrain the linear model an, for example to constrain it by some penalty so.",
            "The rest of the talk of my two talks will be about how to make this penalty, but the broad picture to say will make a penalty on the set of linear models function Omega that will quantify the model is complicated or not.",
            "For example, and we will say the shrinkage estimator will be just the best model.",
            "So the model with the smaller streaks risk.",
            "But constrained to have bounded penalty, so there will be one parameter in this model which is the C. Here the C defines the set of models that we consider.",
            "So instead of saying we take any RENA models, we will define the penalty and say we just restrict ourselves to the linear models with the penalties mordancy.",
            "And then among these models will find the best one.",
            "OK, so very small difference between the square under the square just as not this term.",
            "But this is intuitively how we can.",
            "You know, reduce the space of candidates and therefore reduce what I called the variance because process we have a picture on that reducer variance.",
            "But we will introduce a bias again, so I called the biases that perhaps the good model has a large penalty.",
            "So if we restrict the penalty to be small, we may never be able to find the best model, the true model.",
            "There was a true model, so this is the price we pay.",
            "We accept that perhaps the good model have has a large penalty, but we are sickle cell too.",
            "Small penalties, you know those who control the variance."
        ],
        [
            "Let's try to look at the same thing, but visually so I tried to make a graphic.",
            "I'm not very good at that, but I tried.",
            "So suppose suppose this is a space of models you want to learn my messages that you said there may be some true model here that you don't see.",
            "You just observe data and this one would be the guy that makes the best predictions in the future.",
            "But we don't know it, OK?"
        ],
        [
            "When you, when you when you do least square regression, you will estimate the model and I said perhaps your estimated model will be very different from the true model just because it's hard to fit the model in high dimension.",
            "You know there is no cetera, so the error is about the distance between the two models.",
            "the ID of shrinkage is just."
        ],
        [
            "Say let's arbitrarily define, you know, in this case a circle and just walking in this circle.",
            "So not that the true model is not here.",
            "OK, but what we do nevertheless is to say, let's look at the best model in the circle based on the data."
        ],
        [
            "So because we restrict ourselves to be in the in the yellow circle, the best model will not be here anymore, it will."
        ],
        [
            "To the circle somewhere and you see here that perhaps it was rocky, but the constraint model is closer to the true model is still quite far, but it moved closer.",
            "So why did it move closer?",
            "Just cause in this case this is an assertion that the distance was mostly because of the variance in this case.",
            "So we constrain that.",
            "Not that because we restrict ourselves to the to the yellow circle, we can never reach the truth, the true model, but perhaps there is among the secure one best model which which are right here and so on.",
            "This P."
        ],
        [
            "Sure, what I called the bias in the variance would be interpreted as the distance between how far the true model is from my set and the variance is ready to the size of the of the constraint sets.",
            "OK, so the total decomposition of errors is a bit to say that the total distance is the sum of bias and variance.",
            "This is conceptual.",
            "Don't try to define a real distance, here is just an ID.",
            "OK, so this is what we will do."
        ],
        [
            "Now."
        ],
        [
            "Something that that so this is the first key ID.",
            "The first idea is to run in high dimension.",
            "We need to do something and the way we do it is constraining this.",
            "Now it's not the end of the story, because once you know once I say decrease the variance, it will be better.",
            "There are many ways to do it, and in fact once we decrease the variance we increase the base and it could be costly.",
            "You know if you restrict yourself to stupid models it will never be good.",
            "So this is where there is a second ID which is perhaps in the choice of this penalty.",
            "We can use some prior knowledge we have about the data or but the the problem and try to design this constraint set in such a way that is not too far from the true model.",
            "Because if we do that, we could simultaneously decrease the variance because we constrain and not increase the bias too much.",
            "OK. And for example, if for some reason you know the true model is not too far from the horizontal axis, it might be stupid with yourself to a circle you made."
        ],
        [
            "Just repeat yourself."
        ],
        [
            "So this was a circle, but."
        ],
        [
            "If you restrict yourself to this kind of space in total, you know the volume of these of these.",
            "Ellipsoid is not different from the circle, so the variance will be the same as the previous one, but the bias has decreased because now you're much closer to model.",
            "So in basin services it would be the role of the prior innocence to design A priority to what's your belief of the true model and make a good priority in the prior weather.",
            "Truman's not too far for us.",
            "It will be trying to design A penalty that is not too far from the tree model."
        ],
        [
            "And if we do that, we can decrease the bias and still control the variance and therefore still have a better model.",
            "OK, so this is a.",
            "These are the two key ideas.",
            "Again, the first key idea is we need to do something an.",
            "In my talk, I decide to define a penalty, which is a way to reduce the variance and the second ID is how to design A penalty.",
            "The good way to do would be to try to design A penalty that decrease in the bio spinning, trying to find simple models which are as close as possible to the good model.",
            "This is quite set this where it looks like I don't need half an hour to go to this point, but this is where the key idea here.",
            "OK, so once you do that."
        ],
        [
            "A couple of more general comments.",
            "In practice though, the way you know the way you solve these problems is not only to define."
        ],
        [
            "Wanna leave Sweden do that but you can, you know, make a series of ellipsoids like a small to big.",
            "This gives you a series of models and then typically you set the best ones by cross validation.",
            "So this is what's called."
        ],
        [
            "Structure learning social regression.",
            "So at the end of the architecture of our running models would be to define a penalty and when we move the constraint we have simple model complicated model.",
            "We have the bias and variance that decrease and we optimized the complexity of the model typically by cross validation that was described this morning.",
            "So I don't need to describe it in order to have a final model."
        ],
        [
            "OK."
        ],
        [
            "So in summary, I did it so this is the main idea.",
            "Now I'll try to do this straight."
        ],
        [
            "How to make the penalty?",
            "And this is where now I can go on the right on the left making kernels or making other things.",
            "So turns out that over the years, so I contributed to this activity of designing kernels or designing penalties, and I found a nice website.",
            "You know where they explain how to make paper lanterns and you will see that this shape here is actually one of the shapes that will be designing at the end, so this is a this is called Latin Group latter group lasso, which is very successful too.",
            "Anyway, things but at the end this notion of so I will talk of Omega, how to make Omega and this kind of shape will typically the kind of thing we design.",
            "I try to explain to you how you can come up with this idea of designing these penalties.",
            "The key being trying to define a small space so that you think that there are good models in this space here.",
            "OK, but so so so now I will take 2 or so.",
            "So today I will focus on what's called.",
            "The kernel industry so so cannot Morris is a family of models wear for me.",
            "It corresponds more or less to designing penalties Omega which are smooth typically which which will be quadratic in some up to some change of variable.",
            "OK, so it's a bit Bush and priorat quadratic in some in something, so it's a huge topic.",
            "You know kernel learning and not the best one.",
            "Probably here to talk about it, but it has been around for many years and I will over summarize it in or fewer.",
            "A few minutes and the second approach will be, which is also very popular and and you know that came a bit after the kernels, but probably is more popular now is to is to define what I called the non smooth so non smooth.",
            "What I mean by Nonsmooth is typically you see I mean if you can see it in this Lantern, it has a nice convex shape, but there are like edges here, and there is a nice industry now to design things that with edges and edges are exactly the points where you put your prior knowledge becausw.",
            "After some authorities you will see that.",
            "When you design such a shape into the penalty, what's important is that on the edge you have some good models, so there is a new topic, and then we use rate it, which is how to define nonsmooth penalties with edges.",
            "Search that in the edges you have good models.",
            "This will be the last order group, last sold, this kind of stuff."
        ],
        [
            "OK, are there any question that is?",
            "At this step.",
            "So if not."
        ],
        [
            "So let's let's enter the kernel business.",
            "So again, I assume.",
            "Many of you are know know a lot about kennel.",
            "I will try to to mix between notions for beginners and also there will be some homework, some exercises.",
            "So if you feel too two board you can try to do the exercises.",
            "I will show I don't have place for all the other proves, but so the idea of channel.",
            "I mean for me at in this in this setting is just to say we will penalize will use a penalty which is a Euclidean norm.",
            "It's just just just that."
        ],
        [
            "OK, so the first cannon method so I will go back to kernel in a few minutes.",
            "So let's let's forget about, because it's not worth it here.",
            "We just use as a penalty to constrain the reduce the variance.",
            "The squad.",
            "No man this this directly to from the standard square regression towards Coral Ridge regression.",
            "So for me Ridge regression is the first instance.",
            "Simplest instance of this notion of running in high dimension compared to running in small dimension.",
            "So what is Ridge regression is just taking the same set of models as.",
            "Linear regression, so you take linear models."
        ],
        [
            "You take the same objective function, so you try to minimize the risk, but the third."
        ],
        [
            "Ingredients that you add a penalty, which is for any vector beta subitize the vector of weights you will compute its squared Euclidean norm, meaning the sum of the squares of the coefficients.",
            "OK, so for any beta you compute that and what we do in Ridge regression is to say let's minimize the risk over the linear models search that the UK number of PETA is bounded by a constant.",
            "When you do that, you do Ridge regression, so I must say I forgot to say it.",
            "Earlier.",
            "And it's it's obvious for many, but perhaps not to anybody.",
            "I will constantly shift between two ways to express.",
            "Did I write it?",
            "No, I did not.",
            "So I just want to.",
            "OK I will write it.",
            "I will not be very rigorous, but.",
            "In all my slides, it's the same to minimize.",
            "That when you minimize the risk.",
            "Search that.",
            "Omega of Beta is bounded by a constant.",
            "Another way to write it.",
            "It's equivalent to minimizing.",
            "The risk of bit Vita plus Lambda times the penalty.",
            "So when I say it's equivalent, you know it's equivalent.",
            "If this is convex, is the regular way to write it.",
            "I will not go to the details, but I will you know constantly.",
            "Sometimes write it this way or this way, so either you constrain the penalty to be bounded by a constant or you just add it.",
            "This way college penalty you know you penalize the risk by it.",
            "It gives the same solution.",
            "So what's?",
            "Here you have see here you have Lambda.",
            "There is a correspondence between CR and number, but on average, so I mean without going through the technical details, this is the same for me.",
            "OK, so up to now I motivated this thing by looking at this, but in terms of algorithm like Ridge regression, I can write it this way or this way.",
            "And therefore let's look at Ridge regression returned the 2nd way, so Ridge regression I say we minimize the squared error with is called the rich.",
            "I mean.",
            "So the ridges you will see why this panel?"
        ],
        [
            "And therefore you can write down the equation.",
            "So Ridge regression is just minimizing the risk plus Lambda times the penalty.",
            "See the second form and at times the squared error.",
            "And if you write it in matrix form, it's easier size to write it in matrix form."
        ],
        [
            "To find that there is an explicit solution, which is that the beta, the solution of this is a bit like the least square regression, except that we added.",
            "So there was a nice version we had to inverse the matrix, each transpose X, which is not always invertible.",
            "In Ridge regression, the Lambda parameter that we put in a penalty ends up in terms of algorithm as some additional term on the diagonal of these metrics.",
            "So the good news is that then the matrix is inversible.",
            "Invertible at least you can solve it, and when you play with Rhonda you obtain this.",
            "The set of solutions.",
            "OK, so."
        ],
        [
            "For illustration, again taken from the same book here, if you look at the I don't remember what we want to predict, but maybe diabites or something like that.",
            "You want to predict some response variables from covariates on the left to have the weights of the least square regression.",
            "And when you do Ridge regression, meaning that you increase this Lambda, you move to the right and you see the coefficients which are shrink to zero OK and at the end when you penalize too much, it means that you will see yourself to two vectors beta with Euclidean up 0 meaning.",
            "All the weights are going to 0, so this is the factor of shrinkage here.",
            "This is Y equals shrinkage.",
            "It shrinks the things to zero an again.",
            "Intuitively I don't know what.",
            "What is a good model, but probably on the right you know you you shrink too much so you are too far away from the good model.",
            "You start to increase the bias too much, but certainly you decrease the variance because when you constrain them they cannot move too much and this is you can write, you know cross validation could tell you where to start, where you can have a good balance between having.",
            "Not to NYC model, but which can be learned efficiently."
        ],
        [
            "So once you have Ridge regression, you can easily generalize it to other things, and in particular, one way to generalize this to change the objective function.",
            "So for the moment I just said let's look at the sum of squared error.",
            "One way to design many learning algorithms is to change the risk and for example to define what's called a loss function.",
            "So loss function is simply a function that these two terms, as it puts the first time, is what you predict and the 2nd is what you should predict.",
            "OK, and the last quantifies how bad you are.",
            "So this square was using the squared difference between the two arguments.",
            "To quantify how bad you are.",
            "But if you change the loss then you can again apply the same principle by saying that for any loss I have a risk so I can look for a linear model that minimizes risk, which is the average loss according to my own interest.",
            "My own loss but still penalize it by a regression bias or the squared error.",
            "And this gives the family of things which.",
            "Turn out to be quite famous and and sometimes very.",
            "You know state of the art in terms of performance.",
            "Let's look at them."
        ],
        [
            "So what kind of clothes do we have?",
            "So I talked of the squared loss, which is the first one.",
            "This is to Ridge regression.",
            "But you know, if you want to be a bit more robust this classical to use losses that that are not quadratic, because if you have outliers, you don't want to pay too much.",
            "If you have a point very far away from what you predict.",
            "So typically there are things called the Huber robust loss or the epsilon insensitive loss.",
            "So these are examples I just read that to say that you can you can use them again.",
            "It's.",
            "Not based on the model is it could be based on what?",
            "What at the end you you're ready to pay or what what you qualify as a loss.",
            "But so all these things you need when you when you plug any any of these loss functions in the general framework of minimizing the mean loss plus Lambda times the L2 norm.",
            "This leads to an algorithm so the something a bit special about Ridge regression."
        ],
        [
            "Is that here?",
            "You see, you have the explicit solution, so you can implement it by saying if I have data X and label Y.",
            "This is my model in general, there is no explicit form, because if you if you change your loss, it will not be easy.",
            "You know analytically to write down.",
            "What is the best model, but still what you would do in this case is to implement some numerical algorithm to optimize and minimize the loss.",
            "So there will be no explicit form, but at least you will be able to implement an algorithm.",
            "That will find the good good solution so far."
        ],
        [
            "Over these losses you can implement an algorithm."
        ],
        [
            "Now let's talk a bit.",
            "About lazy So what I say can be used for regression or classification.",
            "The case of classification is quite interesting because in this case the.",
            "So for binary classification the why it takes only two values, which I will incur does plus one and minus one.",
            "So this is where the blue arrange or active not active or metastasis don't metastasis.",
            "So we call that one and minus one and.",
            "In machine learning there's been a lot of interest in what's called a large margin classifiers so large margin are simply all these classifiers that define a loss function.",
            "Of a particular kind, which depends on what's called the margin.",
            "So to try to explain it.",
            "You know when we make a loss function, so we need to compare.",
            "So we make a prediction.",
            "We are previous write it.",
            "The question is you predict a value F of X.",
            "And the true, you know, the true label is why?",
            "So this guy is either minus one or plus one.",
            "What you predict?",
            "Typically it will be easier to work with functions that predict continuous values, 'cause we know how to make linear model in R model.",
            "Mack makes a linear prediction, so this kind of thing typically predicts real value the number.",
            "So the question is when you predict.",
            ".5 is it good or not?",
            "If the label is plus one or minus one, so you need to have you know to relate the continuous value to the label so the most and other ways to say.",
            "Well, if I predict a real value, the number I have a number I would just say that if it's positive it means that I think the label is press one and if it's negative I think the label is minus one.",
            "OK, so in this case you could say well to compare the disposition to why.",
            "I just check if the sign is coherent.",
            "OK so I could look at.",
            "Why times ever weeks?",
            "If this thing is positive, it means that the two of the the same, the same sign, so it's good.",
            "There is no loss, and if it's negative then there is a loss because I predict it will be black, but it's it's quite OK, so this would be one way to do, but in fact.",
            "To go a bit beyond that, when you took off.",
            "So this thing in fact is called.",
            "This thing is called the margin.",
            "And in fact, in the margin when we talk of large margin, the idea is that not only you want these things to be to be the same size, the same sign, but also you would like your prediction to be as confident as possible and the idea is that if you predict a real value number, you know you can try to say well if the if the value is large.",
            "I make a confident prediction, either positive or negative, and so if you look at the end of this number, if you look at the sign, it will tell you if your prediction is correct.",
            "But if you look at the value that.",
            "If it's positive, it means you have made a good prediction.",
            "But in addition, if it's large it was a good prediction with good confidence and on the opposite way.",
            "If it was negative with large value, you would be very confident in your prediction, but it will be an error.",
            "So the large margin method just look at this thing and define a loss function that is a decreasing function of this thing and which typically looks.",
            "Like this so, so these are the loss functions which are used by many machine learning algorithms where horizontally you have the margin.",
            "OK, so the blue one is the one I was is the 01.",
            "It means that if the sign is positive there is no loss is a good prediction if the sign is negative it costs one.",
            "It means I made one mistake.",
            "So this would be one way to quantify, so the blue one corresponds to a risk which is the number of mistakes you make now.",
            "The other ones are variants.",
            "You see they tend to be decreasing, so one is a bit different but.",
            "They don't care, they are decreasing, meaning that when you on the right you make good predictions with good confidence and this has no no cost.",
            "It's nice when you on the left.",
            "Here you make your butt prediction, then with increasing confidence in this, you want to cast it alot.",
            "OK, so all these this leads to a variety of penalties and we will see some of them in more detail.",
            "But so so this idea of large margin classifiers is when you design A function of this that tries to say I want to learn something that pushes the margin to the right, which says good prediction with good confidence."
        ],
        [
            "OK, so it's full of equation, but we will not look at them.",
            "Simplest example, perhaps it's the logistic regression.",
            "What is logistic regression for me is just a particular function which is."
        ],
        [
            "The green one.",
            "OK, you see there is a green one here.",
            "This loss function as its decreasing its convex.",
            "It's essentially it's zero, so it does nothing to make a good position with confidence and assembly on the left it will be linear, so you have a linear cost and the."
        ],
        [
            "Mission is this are the ready question here.",
            "So the log of 1 plus exponential minus the margin.",
            "This is the question of this green line when you when you decide you take this loss function at the end you obtain an algorithm which is called Ridge regression there sorry logistic regression, in this case regularizer, GC creation.",
            "So the algorithm itself would be.",
            "We need to find Arena model beta that minimizes the average loss penalized by some health L2 regularizer.",
            "If you want to implement it.",
            "This is a case where there is no explicit solution.",
            "I cannot write Pizza Hut is the inverse of this.",
            "I don't know what it is, but this is an example where classic Lee.",
            "You can just you know you need to minimize a convex smooth function so you can use things at the Newton method, which is the most obvious thing to do, and for this you just need to compute.",
            "The gradient Hessian you plug that into a solve Newton and then you get after a few iterations dissolution of the rich.",
            "So regularizer GC creation."
        ],
        [
            "So exercise just to follow up on is nice talk.",
            "In fact, when you look at logistic regression on Wikipedia or wherever, this is not the way it's presented.",
            "You know, I just say that for me, which progression is one of these loss functions?",
            "But in general, another way to derive the same algorithm is to make a model and you can check just computation that.",
            "In fact, logistic regression can be seen as a maximum likelihood estimator when you define a particular model where you say.",
            "So he said you were you model the conditional probability of Y given X by this kind of equation and you say, let's look at the maximum conditional likelihood estimator.",
            "So let's maximize the log likelihood of P. Given this model, if you write down the equation after three lines, so we obtain that what you do."
        ],
        [
            "Who is this thing?",
            "Which is what I presented as minimizing loss function?",
            "OK, so loss function or probability model?"
        ],
        [
            "He's the same."
        ],
        [
            "Let's look at another example.",
            "So again I think this one is quite famous.",
            "I will start with the classic old way to present SVM and we see that in fact this gym is just a slightly different loss function.",
            "So classic away to present as, Umm, I assume all of you have seen that.",
            "Or I mean if you came to being without seeing that.",
            "Is dangerous because this is a video you know, like one of the places where SVM are have been developed.",
            "So what is SVM?",
            "Well, as Jim is just one algorithm to solve, so let's look at what's the simplest hard margin SVM.",
            "So suppose I give you this points.",
            "I hope you see the colors here is blue here, it's red, so you want to discriminate.",
            "It seems that the line is enough here, so you could say well."
        ],
        [
            "Let's let's get here.",
            "If you get here you will pray."
        ],
        [
            "The value on this point, which is read this guy is probably read now you observe."
        ],
        [
            "That there are other ways to cut the spacing."
        ],
        [
            "For example, you could."
        ],
        [
            "It's here and here the prediction."
        ],
        [
            "The difference will predict."
        ],
        [
            "Blue"
        ],
        [
            "So a question is, which one is the best?",
            "An the fantastic idea of ECM is to say one of them is better than the other one.",
            "This one is better.",
            "OK, and why is it better?",
            "Because intuitively.",
            "It's, you know, this one.",
            "This one seems a bit very close to these red points.",
            "If you have the choice, why do you go so close to the to the red point?",
            "If you just want to separate both of them so to quantify that you."
        ],
        [
            "You just say, well, let's look at the."
        ],
        [
            "The margin, so it's related to that that that this is the geometric way to define the margin, so the margin of a line here would be.",
            "You know the minimum distance to some to some points, so here you know this line separates the rest from the blue with some distance."
        ],
        [
            "This one to some distance and."
        ],
        [
            "You compare the two of them.",
            "You see that intuitively, I said this one is better.",
            "Becausw this tube is brother.",
            "You have been able to be more the center and so the definition of how much in SVM in this case, is the algorithm that finds the line with the largest tube.",
            "So it's neither of them.",
            "If you look a bit, I mean you you think you go inside and you push, you push until you cannot push anymore, you will."
        ],
        [
            "This one and this one is the solution of how margin SVM.",
            "OK, so why do I talk about this?",
            "It doesn't seem to be too related to the penalty cetera."
        ],
        [
            "But this is the next exercise, which again is not very difficult, but if you cannot do it, it means that you need to work a bit more on this concept or it is rated when you do this.",
            "So this operation of finding a line with a large tube and tube as large as possible is 1 instance of my generic framework of saying I learn a linear model that minimizes the average loss plus.",
            "A rich penalty, the L2 penalty, but this time with the particulars and the question is what is the loss used here to express this?",
            "This line, as in this framework.",
            "OK, so it's not the logistic.",
            "Wanda Logistics will not do that in the case of how machine SVM there is a loss function, so that if the true label is why I predict this, you need to output a number and when we minimize this average number plus this penalty then you get Hard Machine's gym.",
            "So I leave it as exercise.",
            "If she were students, want to discuss a I will explain to them that thing but just to say that the classical geometric way to present them.",
            "Equivalently, could be presented directed his way by saying let's look at this loss function as GM.",
            "Is this thing?",
            "OK.",
            "So by the way, this is what I will do now, so this this was hard.",
            "Marginalism is like the simplest obvious SVM which nobody uses, because in most cases is very hard to find datasets where it's easy to separate the two groups by value line, so."
        ],
        [
            "There is another flavor of SVM which is the one used in practice, which which is a soft margin as Jim, but we just call it as VM, which in this case are represented in terms of loss function.",
            "So the Soft Machine SVM is just one instance of.",
            "Of my of my framework where I minimize some risk by penalizing by controlling the L2 norm, so we're still always penalizing the L2 norm of beta.",
            "But this case with this function, so this function, let's go back here.",
            "We saw it earlier was one of the functions for class."
        ],
        [
            "Vacations, you know on this graph I said the green one is logistic regression, then the red one is.",
            "If you use the red one, so the red one is 0 up to here, and then it's linear.",
            "If you use this loss function then you have made what's called SVM.",
            "OK's name is just using this function, so this function is called the hinge loss.",
            "You already have the solution to the, so the heat, the hardener genus is not written here that.",
            "Yeah yeah, so it says sorry exercise for students.",
            "OK, so.",
            "But still."
        ],
        [
            "Not much in hard margin can be formulated in terms of losses and you have the right to define losses that go to Infinity.",
            "OK, so so."
        ],
        [
            "Margie know one way too so SCM on way to describe it is I choose a particular function, I regularize it and this gives them in terms of implementation, it will be quite different for majestic regression becausw here the last function you know has a King, so you cannot find a derivative or hidden, so you cannot apply Newton method.",
            "So the way to implement it will be different.",
            "But the spirit of it is not different from majestic regression, and in fact in practice, when you when you train a model with Ridge regression or SVM, there is no.",
            "Never seen any difference in terms of performance.",
            "For example, if you you know in both cases if you regularize by the L2 norm, so you do regularizer Ridge regression or azienda SM is more fancy, but there is no difference in performance.",
            "So."
        ],
        [
            "Or perhaps an exercise if you want is for the for the SVM normal SVM.",
            "This is."
        ],
        [
            "This loss function and you can go from this function to get some graphical interpretation.",
            "In the case of."
        ],
        [
            "Dim the."
        ],
        [
            "Interpretation is that it."
        ],
        [
            "We learn a line, so this is the linear model that has both a large margin and that can make errors, but not too big.",
            "So in this case you know."
        ],
        [
            "If I draw this line, I can draw a tube, so the Hodgin was trying to have the two bus as large as broad as possible.",
            "The source machine will also try to make YouTube as broad as possible, but sometimes he will accept.",
            "We have like this blue points far away from where it should be in the same folder for the red one."
        ],
        [
            "And you can show.",
            "Smaller size that when you use the hinge loss so when you minimize the average hinge loss plus Lambda times the square number of beta in terms of graphics.",
            "What you do is that you find a line and a tube that minimize a compromise between having sorcery margins on the good warrior.",
            "Let's call that the width of the tube, so the margin is how larger tubes.",
            "So you want the machine to be large, so we decide to say we minimize the inverse of the margin plus a constant times.",
            "The errors and errors are related to the red lines here, so how long they are?",
            "OK, so this is the graphical interpretation of James.",
            "It's it finds a compromise between large Cuban making few errors.",
            "If you translate in mathematically, you will end up with the."
        ],
        [
            "The hinge loss so the hinge loss is one way to quantify the error.",
            "And there is a link between the Lambda which is here.",
            "And the."
        ],
        [
            "The see which is here which is different from this one.",
            "OK, so this was a SVM."
        ],
        [
            "So summary, you know in just a few minutes we introduced some of the most.",
            "Important algorithms in a in machine learning, at least in the last 10 years as VMS Ridge regression in are, you know logistics, so all of them in a sense are examples.",
            "They are just variations over a loss that can vary, but all the other times penalized by this.",
            "And the reason why they are popular.",
            "And I've been popular for successfully many applications is really this simple idea of regularising and controlling the complexity by playing with the Lambda that goes from very regular asmodel to less regularize then finding.",
            "A good tradeoff between complexity of the model and fitting of the data."
        ],
        [
            "OK, so I see that there are two to say, so let's try now to go to the more fancy and interesting kernel business.",
            "So here it seems a bit.",
            "Ridge regression is not very fashionable, it has been around for four years now.",
            "What's funny is that we can extend all these approaches to much bigger spaces in a sense.",
            "So I said we're in high dimension, but let's go to even higher dimensions now."
        ],
        [
            "This is something I think Austin already showed, so sometimes you know I say let's do linear models, but sometimes in our models are not.",
            "You know, even even if you regularize them, they are not good like here.",
            "Don't want to ever earlier model.",
            "How to talk with that well?"
        ],
        [
            "It has been said before, a classical ideas to redefine the features to change the space.",
            "So for example here if you have to coordinate this one and X2 if you define new features which are is 1 square an is to square this picture on the left becomes the picture on the right.",
            "So suddenly you can separate the the white from the black with the line and intuitively you could say, well, let's do SVM or just information on the right.",
            "It will learn a line linear model here.",
            "And this in Amador, here in the original space is a circle.",
            "This is relative to what I said earlier when I said in high dimension form, either in a model can be very complicated.",
            "It's a bit decided that here it's a line, but it could be a bit more complicated in what you see."
        ],
        [
            "OK, so what's the link between sorry?"
        ],
        [
            "Was the link between this and what I said was so follow it?",
            "I said, let's run a linear model.",
            "We will try to see how to extend all these.",
            "L2 penalize so you know when you penalized by the wider here norm?",
            "How how to do that in this space and we see that there is a trick that allows you to learn all of these Ridge regression or logistic regression or ACM in this space with various simple computation."
        ],
        [
            "So probably again I purchased for for the people familiar with it.",
            "It's not a scoop for them, but we use the kernel trick.",
            "So what is the kernel?",
            "So when we have this way to map a space to some other space, you know a point X is mapped to a .5 weeks.",
            "We define a function K we call the kernel as the inner product.",
            "On the on the on the right hand space.",
            "OK so the kernel between Easton is Primesense farm.",
            "They live in the original spaces.",
            "Are your data, your profiles, or whatever you might do some space, and then when you take the inner product, you get a function which we call a kernel."
        ],
        [
            "The reason why so?",
            "I chose to to follow the.",
            "You know the more mathematics mathematical way to present it.",
            "The reason why kernels are useful is that there is a theorem called the representative theorem.",
            "Which has been proved by several persons, but in particular burnout and others, which in a sense tells you that.",
            "When you so up to now I was saying let's look at linear models that are obtained by minimizing.",
            "A risk penalized by L2 norm OK.",
            "Suppose you run this this algorithm in the space on the right.",
            "So in the feature space, the space of five.",
            "Then the theorem tells you that when you will run it, it could you know mathematically there will be a solution.",
            "Then it tells you that the solution that will find it's a linear model on the right.",
            "OK, so you can write it as beta times 5X.",
            "It's a linear model in the space of five, but also you can always write it as an expansion over the kernels.",
            "Centered on the training points.",
            "So this is this fundamental equation where the theorem says that the solution of this problem.",
            "So the function you want to learn at the end will always be written.",
            "You can always write it as a sum for over the training points, so you have 10 points initially valued.",
            "The samples that you can always write the solution as a sum of Alpha times the kernel.",
            "So the Colonel was the inner product between Zion X, which is the point where you want to make your prediction.",
            "OK, and so if you know that the solution has this shape, you know what you have changed that instead of looking for beta.",
            "Beta was a vector that lived in the space of five.",
            "Instead of looking for bitter now you will look for Alpha.",
            "It's a different way to Parameterise F An how to find Alpha?",
            "Well, you can just replace F everywhere here by the corresponding Alpha expression, and in fact you will get a new optimization problem, which is exactly the same as the first one.",
            "But in terms of Alpha.",
            "Out of beta.",
            "OK, so the theorem tells you that instead of working in the space of beta, you can work in the space of Alpha.",
            "I will not prove the perhaps I will.",
            "Yeah, I will very quickly prove the theorem just to show you that this is a. I mean it, this is a theorem that has huge impact, but which is very.",
            "I will give you some idea of how on how to prove it.",
            "It's quite it's quite easy.",
            "The idea is just.",
            "So what we want to prove again is that F can be written this way.",
            "In fact, what we will prove is that beta.",
            "So the beta that you want to run the solution can be written as a sum of Alpha I5 XIA.",
            "OK, for I = 1 to N. So if you can prove that, then you can prove that theorem becausw F beta would be beta times beta scarifier Vicks.",
            "So it will be the sum of Alpha Phi of inside times 5 weeks and this would be the kernel which is this equation here?",
            "OK, So what we want to show is that any solution of the problem on the top can be expanded leaves in the span of the training points if you want so geometrically what happens?",
            "You know you live in the space and you have fireworks one somewhere.",
            "This is your first training point.",
            "Mapped the space you have firefix to somewhere else.",
            "And you have all the end entry points that live in the space.",
            "So what you can draw from this is the linear span of this point.",
            "So I will because I have limited ability to write in high dimension, I will just assume you have two points and we live in 3D.",
            "So these two points they you know they span a planner.",
            "Central at the origin.",
            "So if you look at the of the subspace, which is a set of linear combinations of the of the training points, what you want to show is that the solution.",
            "So the R rated Beta had like the solution of the optimization problem.",
            "What you want to show is that the solution is in this space OK, which which has dimension maximum N as opposed to being outside of the space.",
            "How to prove it?",
            "It's just.",
            "If you look at any vector orbital, let's look at this bitter here that could be outside of the space.",
            "This vector beta.",
            "You can always the composite.",
            "You know you can make it orthogonal projection.",
            "To the linear span of the training points, and you can always write beta as a component.",
            "Which belongs to the spine.",
            "So let's write it bitter S plus a component here, which is orthogonal bit after gonal.",
            "OK, any vector beta you can decompose it as its projection plus the the compliment OK and the goal is to show that if beta minimizes the top function then the orthogonal component is reduced to 0.",
            "That would mean that beta is in the linear span.",
            "Therefore that beta can be expanded this way.",
            "Therefore that F can explain it this way.",
            "OK, now if you write it, it's almost done because if you look at the objective function, let's look at the second term first.",
            "What you see is that what we will show is that this this guy so beta S which is the projection of bitter internal space is always better than beetle in terms of the objective function.",
            "OK, beta it would be a candidate.",
            "Then we say that when you project it, this new guy will always have a lower objective function.",
            "So if you look for example at the second term, the second time was the square norm of beta.",
            "It's clear that this guy is smaller than than beta because you have beta.",
            "You project it so you reduce the L2 norm.",
            "OK, so the L2 norm of BTS is smaller than the autonomy of beta.",
            "So in terms of objective function BTS who would be better than data if there was just the norm, it is better to be small and what you see two is that if you look at the first term so the first time is more complicated is the loss of the training sample.",
            "But to define the loss you need to compute the value of F beta on this point is 1 and X2 and F beta is the inner product between beta.",
            "And fire of X1 orbiter in five weeks two and here you see that the inner product cause.",
            "The you know product between beta and each of these guy in fact is equal to the inner product between beta S and each of this guy because by definition the compliment is orthogonal.",
            "OK, so I'm sorry I probably lost the majority of the room, but the conclusion is that you can show that the objective function is always better here than here, and therefore that the optimum has to be here.",
            "And therefore that the solution data had, even though the space could be infinite dimensional, you know that the solution is always in this small space, whose dimension is just the number of points.",
            "It's N. OK, so it's a way to work in a possibly huge space whatever the dimension, you know that you can write the.",
            "The learning problem in high dimension, but still the solution will be inspired by tension."
        ],
        [
            "So that's the same thing, but returned with the equations."
        ],
        [
            "OK, so so let's let's let's do it so for example, what what?",
            "What does it mean for Ridge regression?",
            "I said Ridge regression.",
            "It's always the same.",
            "You know, we minimize the squared error penalized by Elton.",
            "Also, earlier I gave the solution, we can write the solution.",
            "Let's try to write it again in a different way.",
            "We know that the solution here the function can be expanded in terms of Alpha and if we want to find the Alpha, we just need to plug back the Alpha in these equations.",
            "So you replace F of excited by the sum of Alpha K of XJ, XI cetera.",
            "Then you have any question in Alpha."
        ],
        [
            "Of course I will skip the details."
        ],
        [
            "But you get, you know this kind of equation in red that is the same objective function but parameterized in Alpha instead of beta.",
            "This one is again quadratic function that you can minimize.",
            "You compute the gradient to set the gradient to zero.",
            "You need to take."
        ],
        [
            "Of inverting matrices etc.",
            "But at the end."
        ],
        [
            "And what you get is that so it's the same function as before.",
            "In order earlier I wrote the equation for beta, so beta was its transpose person, die cetera.",
            "Now the same equation gives you that F. So the linear function can be written as a sum of Alfie kernel between Scion X.",
            "That's rather case with Alpha given this way.",
            "So you again have an explicit solution in Alpha.",
            "OK, so in practice you know if you have to do a regression in dimension 10.",
            "You can either use the previous equation that gives you beta or you can use this equation that gives you Alpha.",
            "There we go inside they are the same, the same of the seminar function.",
            "So it seems to be not very interesting, but it becomes interesting."
        ],
        [
            "In the cases and possibly."
        ],
        [
            "But in the cases where you work, so let's look."
        ],
        [
            "And it is interesting.",
            "It's interesting in cases where the first approach cannot be applied, like if you're in very, very high dimension, you don't want to.",
            "You know, right?",
            "Your mattress with millions or billions or infinite number of rows and columns.",
            "You know if you work in infinite dimension, there is no way you can write your vector beta.",
            "But this guy you can always write it because its dimension is number of points.",
            "So this is this is called the Jewel way to do Ridge regression.",
            "When you do that, you estimate the same function, but you just estimate numbers.",
            "Which are the weights of each training point as opposed to estimating the vector explicitly in very high dimension?"
        ],
        [
            "So we can do the same for."
        ],
        [
            "So other things, but now what's the link between this notion and channels?",
            "Well, the link is that, so something in."
        ],
        [
            "We see here again is that here you see there is no fly anymore.",
            "I said you have each day are not too too space fire VIX Anfi is connected to a channel with the inner product and to find the Alpha you see there just into the kernel.",
            "So it becomes interesting if you want to work in a space where the file can be very complicated, infinite dimensional, but you cannot.",
            "K is easy to compute and there are many such cases.",
            "This is what's called the kernel trick.",
            "You can express very complicated inner product by simple functions sometimes."
        ],
        [
            "So an example this this one like if you if you map a point with.",
            "In polynomials you can see that when you take your points here, you take the inner product and you square it.",
            "Then it's called the kernel because it corresponds to the inner product on the right hand side on the other space.",
            "So this is a way to go from a inner product in a small space to some inner product in a slightly higher space, and for example."
        ],
        [
            "Well, you know if you take some inner product and you take it to some power then it becomes some inner product.",
            "But in a space which is even higher.",
            "So when you increase the dimension here, it means you're able to compute at no cost because you know taking the power of a number is something you can do easily on your computer.",
            "It corresponds to work in some space that was dimension increases.",
            "So this means that in this case it's better to do this Ridge regression or any other kind of methods in the space of Alpha.",
            "But just using the kernel."
        ],
        [
            "So this is to, you know, thinking of what are the So what can I use as a kernel function?",
            "I said perhaps you can take the inner product to some power.",
            "It's it's a kernel, so there is a natural question and I will go quick on that, which is what are the functions K. That correspond to inner products because where I say that as soon as you have function K to compare two points, that is an inner product, then you can plug this K into my formulas and you can estimate a Ridge regression.",
            "Or I didn't say I said it quickly, but you can also do logistic regression as VMS thanks to the representative theorem.",
            "So you can do everything if you have a kennel case.",
            "So natural question is.",
            "Let's define a function K as a kernel.",
            "If it's an inner product.",
            "So by the way, this is exactly the same notion as the notion of covariance that was presented this morning, and the question is, can we characterize the function K?",
            "Or can we list the functions which are kernels?"
        ],
        [
            "The."
        ],
        [
            "In fact, we can."
        ],
        [
            "So there is again an important theorems that what that was proof some some time ago that characterizes.",
            "So that gives another description, exactly the function K which you can use OK was proved by Aaron John and it says that a function K. So if you take a function K of XX prime, it's an inner product.",
            "It's a.",
            "It's OK now if and only if it has a property that it's what's called positive definite.",
            "So what is positive definite?"
        ],
        [
            "Positive definite There is an ugly way to write it this way, but is just a function K such that when you have points you take any points, then you can apply your function to all the pair of points you obtain a matrix of kennel values.",
            "These metrics always needs to be symmetric, so the function is to be symmetric and these metrics should not have negative eigenvalues.",
            "It's called some positive semi definite OK, so it's a bit abstract if you're not familiar with this notion of.",
            "I can raise this issue, but at least we can characterize them, it's."
        ],
        [
            "So generate function that that explains what are the functions that can be used as kernels.",
            "So if you want to look at the."
        ],
        [
            "For the theorem it's in the slides."
        ],
        [
            "And then now we can try to, you know, we can list what could be interesting function.",
            "So it is very relative to the covariances.",
            "What can we describe?",
            "Some functions which you can use as kernels and find many of them are.",
            "You may be familiar with them, so I already mentioned the polynomial kernels, meaning that if you have vectors you take the inner product, take them to some power, then you have a kernel this morning.",
            "So I call it the Goshen RBF and not the exponentiality.",
            "The quadratic something.",
            "But it's the same, so it's not normalized.",
            "If you take this Gaussian function.",
            "Sorry, it's positive definite.",
            "OK, so it's not obvious, and in fact.",
            "There is an exercise for you.",
            "You can write it at some inner product, meaning that these apparently natural function you can write it as fire waste times 5X prime in some space.",
            "OK, so in this case the space is a bit abstract and it in fact it's infinite dimensional, so it's not a it's not 345 dimensions, it's been.",
            "Dimension is called a Hilbert space, but it's an inner product and therefore.",
            "All that I say, like Ridge regression.",
            "You can apply it in the feature space of the kernel.",
            "Gaussian kernel an gets Ridge regression with Goshen Camel.",
            "Laplace Kernel is a bit the same without the square, but it has a very different properties at the end.",
            "Another one that that's funny, so we also saw it this morning, but in a different approach is when you have two #2 positive numbers, so the rest one in fact is very used in image processing.",
            "For example when you when you want to.",
            "When you have images and you describe an image or Patch by a histogram of colors, you need to compare histograms and histograms are positive numbers, and when you when you want to compare, attend to.",
            "So one histogram with standing the other 118.",
            "How do you compare them?",
            "While in the case of histograms, of course, the one of the best way to compare them is to take the smallest of the two the mean.",
            "So the mean between 10 and 18 is 10.",
            "This is a this is a Colonel.",
            "It's an inner product.",
            "I'll leave it to you to check how you can write it as some some inner product, but this kind of thing as soon as you know it's positive definite, you can plug it so you can.",
            "You know in the Ridge regression or SVM equation you take the mean of the two numbers.",
            "You need to estimate a function and you have you have learned a model.",
            "OK, so so the list of course is not finished.",
            "I mean there is, there is no end, but these are some that are widely used antique, so this one is just less common in the implementations, but it's used by some people, but this is typically where you know when you run a SVM in R and in Python whatsoever you choose the kernel, you need to choose this kind of thing.",
            "So that the different one is just to take the sum of the mean, but the other one is is just a problem.",
            "Did you work on that?",
            "So you take there is something that the mean of other Max, which is to combine the means and the Mac season."
        ],
        [
            "OK, So what kind of things do you learn?",
            "Well, you see I said these are linear models, but when you make a linear model in the space of the Gaussian kernel, it's linear.",
            "But when you look at it in 2D, this is the kind of thing you can learn 'cause you know, linear model Representative theorem tells you that the linear model in the high dimensional space estimates the function and the function.",
            "You can represent it by your some of Alpha I times the kernel.",
            "So if you use the Gaussian kernel, it means that what you do in practice is you learn.",
            "A combination of kennel, you know, some of Goshen, Goshen, Colonel, centered on the 20 point and what you do is optimize the Alpha.",
            "The Alpha is what we learn in the in the function, and so you can imagine that a sum of Gaussians can take this kind of shape, for example.",
            "So This is why I was saying that this is typically a linear model in high dimensional space space, and this one is smooth 'cause we have constrained it to have a smaller normal small Euclidean norm.",
            "If you don't control the norm in the highly mental space, you can get with Gaussian functions.",
            "We can get any any function.",
            "OK, so probably this is a good way.",
            "I mean, one good way to look at high dimension is these are linear functions and the norm is the smoothness.",
            "The norm of the vector beta is the smoothness of this function."
        ],
        [
            "So perhaps I will just finish with a few.",
            "Two thoughts and examples of color.",
            "Because you know what I say for the moment is there are many can also you can use them.",
            "You can affirm.",
            "You can learn all these things.",
            "One of the key question is how do you choose a kernel?",
            "And perhaps how do you design A kernel?",
            "And this is related and I said initially, let's let's constrain the beta to be in this fear and this is to learn in high dimension.",
            "But the second thing was if we have some prior knowledge of where the true model R, how can we not take this fear but this ellipse?",
            "That will go closer to the to the true model, so this notion of... actually with annotation.",
            "This would be how to design A penalty such that there are good functions in the penalty.",
            "And now we said, let's if we just take for penalties the L2 norm.",
            "The only thing we can change is the choice of Colonel.",
            "So the kernel defines the feature space and therefore the Altoona me cetera.",
            "So for us the kernel is where we can put prior knowledge where we could try to, you know, put knowledge to decrease by us and control variance etc.",
            "So how do you make a kernel?",
            "You can take one of the list and try them.",
            "I was just mentioned 3 three other possibilities or first, I don't really know, but at least I know how to make some of them.",
            "Probably some useful principles to make sure no one would be to design features because you know, I said kernel is some inner product space or features and you work in this space.",
            "So if a predator you know what are the important features, just take them and this is your feature space.",
            "OK, so you could make your features and then learn a model.",
            "Nice pictures.",
            "A second way to do is.",
            "And I found it very useful in applications.",
            "I will give one example is you know to look at not really in terms of features, but to look at the distance it defines.",
            "Because what's important at the end is you define a geometry, new points, you have your points in your space you want to manage to some other space and you will learn something that will be smooth there.",
            "So if you have some prior knowledge of what are the similarities or distances between how we should measure similarity between two samples or between two molecules, you may directly define a distance, then translate the distance to a kernel and then feed the kernel to the machine.",
            "Will do that and the last one.",
            "OK I will I will I will.",
            "In one in one minute I will explain it is is a designer regularizer."
        ],
        [
            "Jessica, let's be concrete.",
            "So the first one is this.",
            "Any feature, so I think this is a custom show.",
            "The first day you know civil.",
            "I don't know if you remember, but it talked about the molecules and graphs and graph kernels and how when you have a graph you can represent it as a vector of features and then sometimes you can make you can out of it.",
            "This has been done and this is a typical example where if you want to.",
            "So if you want to make a kernel between molecules, one way to do is what are the features.",
            "I want to have in my future.",
            "Face OK and then the only added value of having a kernel is that perhaps you don't need to enumerate all the features, but first you can find a formula to directly compute inner product, but the starting point would be what other features I want.",
            "So 11 example.",
            "One famous example of graph kernel is when you have graphs G1 and G2.",
            "I give you the the magic formula and then explain what it is.",
            "The magic formula is when when when you give Me 2 graphs you make what's called a product graph.",
            "From the picture you can guess how it's made is just.",
            "You know combining the two graphs together to make a graph G 1 * G Two this this graph has some adjacency matrix a while you just some you take the mattress to the power N and you sum all the elements.",
            "This gives you a number.",
            "This is accountable.",
            "OK so in terms of algorithm, this is how you would implement it, but why so?",
            "It's not needed it it's a.",
            "Probably from the paper the first papers of Geithner and Cash IMA so this is the end formula.",
            "But when you do that, the reason to do that is that you can show that in this case you make a feature space which is quite large, which for each graph it strikes all the walks of length N, meaning that when you have a graph you could you imagine you walk on it and when you walk on it, in this case it could be a blue, yellow, yellow, yellow, blue, yellow, blue.",
            "This is.",
            "This would be one dimension of your feature space and what these ugly formula so very efficient but strange formula doors is computing this inner product.",
            "This space.",
            "OK, so take your messages that there are many magic formula in kernels 1st and 2nd.",
            "In this case the way to derive this magic formula was to think of what should be.",
            "How do we represent a molecule as a vector and then find the trick to compute the inner product.",
            "Here that's the 1st way to make a kernel.",
            "I'm sorry, I I Russia bit."
        ],
        [
            "2nd way is you you work with colleagues or you come from your domain and you know if you ask your colleague when you have two protein sequences, how you compare them.",
            "Well they never heard about kernels but I know how to measure similarity between proteins.",
            "You know you need to align them and once you align them you have a score.",
            "the Smith Waterman scored that computes try to align them, penalizes the gaps, compares them University cetera.",
            "So in bioinformatics.",
            "So if you want to design A kernel for protein sequences.",
            "It will be hard to communicate if it's very different from what everybody uses in terms of of distances like it seems that the defacto distance used in this domain is related to aligning the sequences.",
            "So one approach to make you cannot in such a situation would be to say, well, if these guys if all the Community uses this distance there may be some reasons is because you know over the years they have found that it's a good way to measure the evolutionary distance and therefore it is a nice notion of distance to to infer too general.",
            "But in this case, the only problem is that the sort of score using informatics is not a kernel, it's not positive, so it's not a kernel, meaning that it's not an inner product, so it creates difficulties.",
            "So one way you know if you're in this situation, perhaps you can try to think of it and find some variance in this case.",
            "But there is a function which is a bit ugly, but you can compute efficiently so that this is a function that borrows the concepts of aligning the sequences.",
            "So again you try to align them.",
            "You scored the alignment, the only difference is that instead of taking the best alignment, you sum all the alignments, weighted by how good they are, and you obtain your number and at the end is number is positive definite, meaning that you can directly use this formula, plug it to your kernel function and learn over the space with geometry is defined by the by the expert knowledge.",
            "So my experience is that this kind of approaches could be very successful because again expressing your domain know what they do and if they use a distance, you know how to measure distance between samples or molecular whatsoever.",
            "Often they do it, because empirically they found it's good to describe the geometry.",
            "So if you want to scale method, you just need to tweak a bit, and sometimes you can even not wreck it and use it directly."
        ],
        [
            "OK, finally and this will be the end of for today another way which which we will use a tomorrow to make your kernel is not too thinking there's a feature space, nor in terms of distance, but to say you know when I use a kernel because I want to learn a function which is a linear function, I know that the penalty I use is the L2 norm of beta or worse, but sometimes this altinum of beta can be expressed directly in terms of something, some property of the function F of the renal function.",
            "And for example.",
            "If you take the Gaussian kernel, probably the best way to understand it is not in terms of features etc or distance.",
            "It's a bit strange, but one way to understand is that if you use this kernel, you can show that the L2 norm of beta, so bitter lives in your space you cannot see.",
            "You cannot imagine but the Altima Vitacca response to something in terms of function F. Is this again a great question, but the take home message is that when you have a function F, the F hat is the four year transform.",
            "So you have a function.",
            "And if you want to to evaluate the penalty, you just map it to the Fourier domain to see where the energy of your function and what you do is that you compute the norm in the free domain.",
            "But by penalizing the frequency, meaning that if you have a function with a lot of energy at high frequency, the penalty would be very higher.",
            "OK, so you could directly start from that and say I want to learn a function and I know you want it has to be smooth.",
            "For me smoothness means that the Fourier transform.",
            "Does not have a lot of energy at high frequency.",
            "Therefore I could define directly this penalty an if I define this penalty, I can deduce what is the equation of the kernel that would correspond to this penalty.",
            "Another example, by the way, I said earlier that the mean kernel is is a kernel, and it's widely used if you use them in kernel, you can interpret it in terms of penalty and corresponds is what's called the Sobolev norm is just mean that when you have a function it qualifies the smoothness.",
            "But just looking at its derivative so it penalizes functions which have a.",
            "Uh huh that very quickly because the rate is very high, so this is an example where it's a bit strange to think in terms of this explicit equation of what does a kernel.",
            "But when you look at that, you know what you're doing.",
            "What you're doing is that you you infer, function the function it will infer.",
            "You control its complicity by controlling the L2 norm of its derivative.",
            "So at least this would be so again, one way to make channel then, would be, say, what kind of constraint do I want to have on the function F?",
            "And if it's quadratic right then you can derive a kernel that would do it.",
            "OK, I'm sorry I was away too longer."
        ],
        [
            "I did not go as far as I wanted, but at least I finished the main thing on Colonel so I don't know if we have time for questions or if we should."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so I'm sorry I I forgot what I had to talk about and I realized.",
                    "label": 0
                },
                {
                    "sent": "It's not that.",
                    "label": 0
                },
                {
                    "sent": "So OK, machine only for personal genomics.",
                    "label": 0
                },
                {
                    "sent": "I kind of related will see.",
                    "label": 0
                },
                {
                    "sent": "So no, I actually I did a subtitle frequentist approach just becausw.",
                    "label": 1
                },
                {
                    "sent": "Obviously if you were here this morning to listen to Neil's presentation equal many things and he said many times you will see JP will do this afternoon so I had to follow his talk and I think the main difference between our talks.",
                    "label": 1
                },
                {
                    "sent": "Easy, I will talk about similar objects and similar approaches, but not from a probabilistic abbasian POV just presents you the parallel you know in statistics and semester machine learning.",
                    "label": 0
                },
                {
                    "sent": "There are like for historical reasons, communities and one one way to split the community into is to say their vision and frequentists.",
                    "label": 0
                },
                {
                    "sent": "So I did not realize I was frequentist until recently, but I think you would qualify this talk as frequentist as opposed to the beige and.",
                    "label": 0
                },
                {
                    "sent": "And because I'm the second one to speak.",
                    "label": 0
                },
                {
                    "sent": "I suppose it's because I'm right and it's wrong, but.",
                    "label": 0
                },
                {
                    "sent": "But we'll see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so uh, actually I'm quite a domestic about what I want to talk about too.",
                    "label": 0
                },
                {
                    "sent": "You know, there are many things because of need.",
                    "label": 0
                },
                {
                    "sent": "Again, you totally said I have to talk about this, this and this.",
                    "label": 0
                },
                {
                    "sent": "So I try to put everything here more.",
                    "label": 0
                },
                {
                    "sent": "Is the goal here is, you know most of you are students, so this talk is for the students.",
                    "label": 0
                },
                {
                    "sent": "It's not for the advanced researchers.",
                    "label": 0
                },
                {
                    "sent": "You can go and grab a coffee if you want AB.",
                    "label": 0
                },
                {
                    "sent": "Suppose I assume some some of you, probably most of you have some background in machine learning statistics, but not all of you.",
                    "label": 0
                },
                {
                    "sent": "So what I plan to do is to expose you to some of the.",
                    "label": 0
                },
                {
                    "sent": "Classical and and you know things that tend to work, and in particular there are like two big two big topics.",
                    "label": 0
                },
                {
                    "sent": "I want to cover one.",
                    "label": 0
                },
                {
                    "sent": "I will try to cover more less today is this domain of learning with kernels.",
                    "label": 1
                },
                {
                    "sent": "So we already talked about them this week, but I will try to go back to the basis and give you some.",
                    "label": 0
                },
                {
                    "sent": "I mean, the way I understand them and how they can be helpful for you.",
                    "label": 0
                },
                {
                    "sent": "So this will be the second part and the second big thing I want to cover mostly tomorrow will be this idea of sparsity, an structured sparsity which is very popular in machine learning, which is also a big part, so it should be, you know, just for the second part I have a course I teach across in 20 hours here I have to condense that in just 40 minutes, so obviously I will not go to the details, but hopefully try to give you some idea, Nan.",
                    "label": 0
                },
                {
                    "sent": "So you know, as Bob said, I will give you some Hammers, so you have the Hammers of kernels, then the Hammers of Lasso and company, and with that hopefully will be able to invent new Hammers to solve the problems which are partially solved.",
                    "label": 0
                },
                {
                    "sent": "OK, but let's start so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start with some broad introduction, and I think so.",
                    "label": 0
                },
                {
                    "sent": "The good intuition is to show that these two approaches, which sometimes you know people say, are you doing kernels or sparsity.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's about the same and you can relate them at least in the frequentist way or in a general framework, which I think is the most important part of this presentation.",
                    "label": 0
                },
                {
                    "sent": "Trying to give you my point of view or some insight of how you can do machine learning with complicated data in high dimension, at least one way to do it, and we see that the implementation of this ideal is to either kernel or sparsity or other things.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So examples.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, how many examples that they were given this week on why why we made sense to the machine learning for in biology and personalized medicine.",
                    "label": 0
                },
                {
                    "sent": "I will not go through everything, just show you some examples.",
                    "label": 0
                },
                {
                    "sent": "I will try to take us illustrations or some of the applications.",
                    "label": 0
                },
                {
                    "sent": "Some of the methods that will describe so one of them.",
                    "label": 0
                },
                {
                    "sent": "So I walked in Cancer Genomics a bit and you know?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some other things I don't know if you can see some of the things we're interested typically is trying to understand what's the difference between a cancer cell and a non cancer cell.",
                    "label": 0
                },
                {
                    "sent": "If you have a cancer cell or cancer patient with cancer, what's the prognosis?",
                    "label": 0
                },
                {
                    "sent": "Which is what's going to happen in the future?",
                    "label": 0
                },
                {
                    "sent": "Is there risk of relapse?",
                    "label": 0
                },
                {
                    "sent": "What kind of trades would work?",
                    "label": 0
                },
                {
                    "sent": "So for that we make measurements on people we talked alot of Geno types deep.",
                    "label": 0
                },
                {
                    "sent": "Some mutations there are other things you can which are which have a lower resolution which which you can look at.",
                    "label": 0
                },
                {
                    "sent": "Look at like if you look at the karyotype so the set of chromosomes that you see in a cell.",
                    "label": 0
                },
                {
                    "sent": "It's well known that if it's the first time you you come from machine learning, this may be the first time you see that you know cancer cell.",
                    "label": 0
                },
                {
                    "sent": "You know it's a human cell, but it's not made of 23 pairs of chromosomes.",
                    "label": 0
                },
                {
                    "sent": "Very often the chromosomes have been reorganized, shuffled.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have three chromosomes.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you have fusions of things, so this is always the case and so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measure that either this was a picture, but we have technologies now to to measure a bit through the genome, like how many copies of of of.",
                    "label": 0
                },
                {
                    "sent": "Offer loss or place in DNA.",
                    "label": 0
                },
                {
                    "sent": "There is in a cell normally in each of your cells have two copies, but sometimes in cancer cells you have three or less or more, so you can measure that attended data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can come up with problems which are very practical problems like suppose you collect data on cancer cells and normal cells.",
                    "label": 0
                },
                {
                    "sent": "Well, this is this is not relevant, but for example in this case for pregnancy, suppose you have different types of cancers.",
                    "label": 0
                },
                {
                    "sent": "I mean one type of cancer but different outcome like answers that lead to metastasis within five years.",
                    "label": 0
                },
                {
                    "sent": "Or does that don't can you?",
                    "label": 0
                },
                {
                    "sent": "From the structure of the genome, guess who's going to risk of relapse?",
                    "label": 0
                },
                {
                    "sent": "OK, so one way to do it, and that's how it's done.",
                    "label": 0
                },
                {
                    "sent": "Now you know it's to collect data.",
                    "label": 0
                },
                {
                    "sent": "And to create a database.",
                    "label": 0
                },
                {
                    "sent": "So here's this point one.",
                    "label": 0
                },
                {
                    "sent": "But of five patients on the left, this is the profile.",
                    "label": 0
                },
                {
                    "sent": "So he tinkered with the structure of the genome.",
                    "label": 0
                },
                {
                    "sent": "5 disarmer animals.",
                    "label": 0
                },
                {
                    "sent": "So 5 profiles of minimize that did not relapse within five years and not the right design minimized.",
                    "label": 0
                },
                {
                    "sent": "I did relapse and a natural question is can you process this data to learn some model that would be able to predict what's the risk of relapse and therefore it without the therapeutic choice of course.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a classic car example, so this was a you know you can do that at the level of the genome.",
                    "label": 0
                },
                {
                    "sent": "You can also look at of course at the level of what happens after the genome, meaning gene expression.",
                    "label": 0
                },
                {
                    "sent": "Perhaps proteomics and one of the problems.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classical applications of.",
                    "label": 0
                },
                {
                    "sent": "You know, machine learning, like surfer for president as a junior genomics is is looking at gene expression data either using the microarrays or using sequencing technologies.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And making these these studies of either diagnosis, meaning when you have sub types of cancers, can you from the profile of gene expression guess or learn a model that would discriminate between different subtypes of concerns?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or can you predict again what's the risk of relapse, and this is probably one of the biggest impact of this research at the clinic today, which is the development of molecular signatures based on gene expression to predict the risk of relapse in early breast cancer.",
                    "label": 1
                },
                {
                    "sent": "OK, so this has to be done.",
                    "label": 0
                },
                {
                    "sent": "I think from the statistical point of view is not a solved problem, it's an open problem, but at least you already have products which are on the user, the clinics which measure markers, and I have estimated the predictive model from.",
                    "label": 0
                },
                {
                    "sent": "In this case, you expression to predict the risk of having a relapse.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are many other examples of machine learning, so this is less about personal genomics.",
                    "label": 0
                },
                {
                    "sent": "But you know.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you, when you when you sequence genomes it could be pathogens or it can be new things.",
                    "label": 0
                },
                {
                    "sent": "You you you you can now we can sequence everything so it's easy to sequence to read the where are the jeans and then you have questions of an automatic annotations when you detect and Eugene, you metagenomic sample.",
                    "label": 0
                },
                {
                    "sent": "Can you automatically predict what's the function of the of the gene?",
                    "label": 0
                },
                {
                    "sent": "What's the structure?",
                    "label": 0
                },
                {
                    "sent": "And this is directly to machine learning problems like you have sequences of proteins which are known to have a function, this is.",
                    "label": 0
                },
                {
                    "sent": "Just one example.",
                    "label": 0
                },
                {
                    "sent": "You have sequences.",
                    "label": 0
                },
                {
                    "sent": "Then you ask the question from these sequences.",
                    "label": 0
                },
                {
                    "sent": "Can I run a model that, given a new sequence that is obtained from sequencing, to automatically annotate this the sequence as this type of protein with these functions with this structure etc.",
                    "label": 0
                },
                {
                    "sent": "So again, this kind of things are nowadays are solved by machine learning like approaches which have been trained on data using various models that we saw that I will present.",
                    "label": 0
                },
                {
                    "sent": "So this is working.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, so I think this is what the customer called all day by infirmity.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's not supposed to be very MoD, classic classic.",
                    "label": 1
                },
                {
                    "sent": "So enjoy discovery.",
                    "label": 1
                },
                {
                    "sent": "This is a classic problems, but it's also really something that where machine learning is used is when you want to design a new drive.",
                    "label": 0
                },
                {
                    "sent": "One way to do is to design A Droid because you feel you can do it.",
                    "label": 0
                },
                {
                    "sent": "Another way is to screen molecules, banks of molecules and try to measure if some of them are active or walking some say.",
                    "label": 0
                },
                {
                    "sent": "And then you collect data like many molecules and some of them are T. Some of them are not active and from this you want to learn a rule that would predict if a molecule is active or not based on this data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a classical by infirmities nowadays.",
                    "label": 0
                },
                {
                    "sent": "We do that with other like finding a drug that works on a patient with a given genotype and gene expression.",
                    "label": 0
                },
                {
                    "sent": "So it's just the same question, except that you just don't only have the drag, but you also have the profile of the patient and this is then personalized medicine to guess to find a rule that would predict which drug will work on which profile.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The longer path I can skip this one.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these examples, so I choose, you know, a small number of examples which turned out to be in all cases exactly the same when you conceptualize them.",
                    "label": 0
                },
                {
                    "sent": "These are instances of very typical problem of machine learning, which is supervised binary supervised classification.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically you have samples.",
                    "label": 0
                },
                {
                    "sent": "You have examples that have two types here.",
                    "label": 0
                },
                {
                    "sent": "In this picture, each dot is a sample or molecule you have active and non active or you have a relapse or no relapse etc.",
                    "label": 0
                },
                {
                    "sent": "And from this data you want to learn in this case since you could separate operation, the two populations by by your line so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At at the end, what you want is when you have new new object, new samples or numeric rules, be able to predict.",
                    "label": 0
                },
                {
                    "sent": "Which one will be active or which one will not be active?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it's all very classical.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the thing that that is a bit specific, so not specific to only two to personalized medicine, but that makes it difficult, is that you know if this was ready.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture then it sold.",
                    "label": 0
                },
                {
                    "sent": "It has been sold 400 years is done now.",
                    "label": 0
                },
                {
                    "sent": "The problem is with the difficulty with the problems we have is to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see that we have data which have usually very high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you measure thousands or millions of snips, or profiles or descriptors, then you don't live in two dimension.",
                    "label": 0
                },
                {
                    "sent": "You live in dimension millions.",
                    "label": 0
                },
                {
                    "sent": "So this is to imagine it and this huge space is filled with just a few few points.",
                    "label": 0
                },
                {
                    "sent": "Be 'cause you know when you have cancer patients, so you can collect a few hundreds, perhaps, but you will never have 10 to the 50 points in this space, so.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a particularity of modern machine learning to try to solve problems with small number of samples in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Also in most of the examples I assured that some in some cases the data very structured like I sort of the molecules are like graphs.",
                    "label": 0
                },
                {
                    "sent": "When you get the profiles of copy number, it's not just a vector, it seems to be a noisy profile.",
                    "label": 0
                },
                {
                    "sent": "There's something underlying it etc.",
                    "label": 0
                },
                {
                    "sent": "So there is this.",
                    "label": 0
                },
                {
                    "sent": "This so question is should we treat this data?",
                    "label": 0
                },
                {
                    "sent": "Are just points in?",
                    "label": 0
                },
                {
                    "sent": "Rachel Space or should we walk in the spaces you know, with some structure that describes the underlying phenomena.",
                    "label": 0
                },
                {
                    "sent": "There other properties.",
                    "label": 0
                },
                {
                    "sent": "But like often we have models with different types of data.",
                    "label": 0
                },
                {
                    "sent": "This is very important.",
                    "label": 0
                },
                {
                    "sent": "We need faster implementations etc.",
                    "label": 0
                },
                {
                    "sent": "But so in the rest of the talk we mostly focus at least to today on.",
                    "label": 0
                },
                {
                    "sent": "How do we approach?",
                    "label": 0
                },
                {
                    "sent": "So what are the in the frequentist?",
                    "label": 0
                },
                {
                    "sent": "Well, at least the ways that have been found to try to solve problems in high dimension.",
                    "label": 0
                },
                {
                    "sent": "With few samples and sometimes structured data and we will try to know too.",
                    "label": 1
                },
                {
                    "sent": "This will be my most important message to give you some flavor.",
                    "label": 0
                },
                {
                    "sent": "Some idea of how we can beat the curse of dimensionality, innocence, try to learn.",
                    "label": 0
                },
                {
                    "sent": "Even though we don't have enough points in a huge space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this will be the second part of the introduction.",
                    "label": 0
                },
                {
                    "sent": "How do we learn in high dimension?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so to try so.",
                    "label": 0
                },
                {
                    "sent": "In the talk, there will be a few equations I don't know difficult equations, but I will use mathematical notation so just not to lose.",
                    "label": 0
                },
                {
                    "sent": "I mean to make sure you're with me the basic notations we try to keep throughout the talk are very classical, I assume so to formalize these problems, I will assume that we have a set of patterns, But these are the data, like the genomic profiles of the molecule cycle that X OK, it lives in a space, capital leaks and what I want to predict is a. Spence, why so?",
                    "label": 0
                },
                {
                    "sent": "In this case it was binary, but it could be continuous valued.",
                    "label": 0
                },
                {
                    "sent": "It could be structured.",
                    "label": 0
                },
                {
                    "sent": "There could be other things.",
                    "label": 0
                },
                {
                    "sent": "I would just focus in binary case, which is called classification or sometimes so trying to produce something continues.",
                    "label": 0
                },
                {
                    "sent": "It's called regression.",
                    "label": 0
                },
                {
                    "sent": "It's not a big deal, and so the input of what we want to design is just a set of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "So labeled example is a pair where you have X&Y molecule and the label active or not active, and you want you have any of them.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the input.",
                    "label": 0
                },
                {
                    "sent": "And why you want to design is a machine, is a is an algorithm program that takes this as input and outputs a function F. That goes from the Space East to the space.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because this is a function that will make your prediction.",
                    "label": 0
                },
                {
                    "sent": "So if you have a new object, XF will predict value Y, which could be binary or continues.",
                    "label": 0
                },
                {
                    "sent": "By applying the function F to the point X. OK, so all the question of machine learning is how do you go from this set South, which is the training set to the function F.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "So these are just stories important and I forgot to write this taken from a very nice book by his teacher.",
                    "label": 0
                },
                {
                    "sent": "Any elements of statical learning.",
                    "label": 0
                },
                {
                    "sent": "So this is a case where you went into D and I hope you see the colors.",
                    "label": 0
                },
                {
                    "sent": "We have a blue dots and the orange dots at the question is can you learn a rule to separate them?",
                    "label": 0
                },
                {
                    "sent": "Perhaps this is in the space of active or not.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can do, I will detail it but apply your method called least squares.",
                    "label": 0
                },
                {
                    "sent": "OK so you say blue icon.",
                    "label": 0
                },
                {
                    "sent": "Emma bless one arrange minus one.",
                    "label": 0
                },
                {
                    "sent": "I do least square regression and it gives me a line that's more separated them.",
                    "label": 0
                },
                {
                    "sent": "So if you look at that.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem very very good for so.",
                    "label": 0
                },
                {
                    "sent": "Sorry for some reason, and in particular it seems that there is a bit more structure in the space.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem that the line is very good despite them because these are not two Gaussians innocence are real data, so you say well here the problem here is that we see the model seems to be too simple.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another algorithm that you can use is called one nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "I will not present it in detail, but I assume also you know it is just when you have a new sample you will just like if you want to predict the color of a new point, you will check what is the closest point and you will predict the color of its closest friend.",
                    "label": 0
                },
                {
                    "sent": "OK, so in terms of how it separates the space in predictions, blue or orange is just you know for each point like this point the closest is blue, so it's in the blue part and there is just you know Frontier here like.",
                    "label": 0
                },
                {
                    "sent": "Then you are probably at the same distance from arranged to a blue, so you don't know.",
                    "label": 0
                },
                {
                    "sent": "But if you're in the blue part, you make a brew prediction in the range maker arrange position so you see that here you when you apply.",
                    "label": 0
                },
                {
                    "sent": "You know this technique of nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "You create function F. So this is the function F which makes a prediction which is a.",
                    "label": 0
                },
                {
                    "sent": "There is something better than the line because it seems that you capture more of the perhaps complex structure of the of the of the space.",
                    "label": 0
                },
                {
                    "sent": "But there are things like here.",
                    "label": 0
                },
                {
                    "sent": "You would predict all these region in in blue, even though it seems that this could just be an outlier like if you assume some, you know some enduring freedom.",
                    "label": 0
                },
                {
                    "sent": "This should be in the orange part and here you overfit the training data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are important important concepts here.",
                    "label": 0
                },
                {
                    "sent": "In both cases we see we don't do a very good job, but for two different reasons.",
                    "label": 0
                },
                {
                    "sent": "One we do.",
                    "label": 0
                },
                {
                    "sent": "On the left we do.",
                    "label": 0
                },
                {
                    "sent": "Underfitting means that we have two strong assumption it's ready to desponding stocks.",
                    "label": 0
                },
                {
                    "sent": "We make assumption on the left.",
                    "label": 0
                },
                {
                    "sent": "We assume a linear model and probably is not well adapted and on the right we don't.",
                    "label": 0
                },
                {
                    "sent": "There is no assumption about the model, but it seems that when we when we learn the model, we fit too much today to an so I will try to show now that these two things we.",
                    "label": 0
                },
                {
                    "sent": "Death many names but one way to call them is the notion of bias and various and and see that in fact there is a tradeoff here that that's the key thing to understand.",
                    "label": 0
                },
                {
                    "sent": "To run in high dimension.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by biases that this model is probably very far from the.",
                    "label": 0
                },
                {
                    "sent": "True underlying model.",
                    "label": 0
                },
                {
                    "sent": "If there could be a trend right model.",
                    "label": 0
                },
                {
                    "sent": "So there is a difference between what we can fit and the true model.",
                    "label": 0
                },
                {
                    "sent": "What I called the variance is more the idea that here what I draw here seems to be very sensitive to the training data.",
                    "label": 0
                },
                {
                    "sent": "Like if I change the color of 1.1, blue point becomes one or energy will change.",
                    "label": 0
                },
                {
                    "sent": "This is not the case on the left but on the right it is and this is the reason why on the right it will not be good.",
                    "label": 0
                },
                {
                    "sent": "It's too sensitive to the training and we call that variance.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a very so, by the way.",
                    "label": 0
                },
                {
                    "sent": "Of course the slides will be available, so sometimes they will go quick and you don't need to go to the details.",
                    "label": 1
                },
                {
                    "sent": "You will have time to look at them, but if you want to formalize it, we can.",
                    "label": 0
                },
                {
                    "sent": "We can, you know, under some assumptions made very simple computation and here without going to the details, this is just an exercise.",
                    "label": 0
                },
                {
                    "sent": "If you if you want to get familiar with it, we can formalize this idea of variance and bias.",
                    "label": 0
                },
                {
                    "sent": "But just saying, suppose you know I have my data, I want to run a function F. OK, it's my function F and then I make a prediction on the new point.",
                    "label": 0
                },
                {
                    "sent": "Dicks, how good is the prediction?",
                    "label": 0
                },
                {
                    "sent": "And if you want to measure, you know and there are some you average over the training.",
                    "label": 1
                },
                {
                    "sent": "Set it so if you want to measure how good your prediction so F hat is what you have learned.",
                    "label": 0
                },
                {
                    "sent": "If you predict on a point X not value effort of is not and the true value is why you not?",
                    "label": 1
                },
                {
                    "sent": "You can try to estimate the difference and with just a few a few lines you will see that the difference under some assumptions.",
                    "label": 0
                },
                {
                    "sent": "Decompose into a sum of three terms and won.",
                    "label": 1
                },
                {
                    "sent": "The first term is the noise in your data, so this is something you will never beat.",
                    "label": 0
                },
                {
                    "sent": "It means that you know, suppose the label are random.",
                    "label": 0
                },
                {
                    "sent": "I mean if they if the if the label is something plus some noise, you will never be able to get the noise.",
                    "label": 0
                },
                {
                    "sent": "So the best we can do is predict the noiseless version.",
                    "label": 0
                },
                {
                    "sent": "So there is the noise in the error which you cannot beat.",
                    "label": 1
                },
                {
                    "sent": "But then there are two terms which are exactly the bias and variance, so the bias is how far on average you're so.",
                    "label": 0
                },
                {
                    "sent": "You know, on average you will make some prediction and sometimes on average duration is not a good one.",
                    "label": 0
                },
                {
                    "sent": "So the difference is called the bias an.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's rated a bit on the left.",
                    "label": 0
                },
                {
                    "sent": "You can show that with the line on average will not make a good prediction because you're restricted to linear model and if the true model is not in our you will not be able to measure the true on, so it's easier.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's an you have the second one, which is the variance, which is how you know depending on the training data you position will vary, and if it varies too much then it has a cost at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case the simple computation really tries to quantify the notion of of this trade off of this balance between bias and variance, meaning that at the end the error you make is the sum of bias and variance.",
                    "label": 0
                },
                {
                    "sent": "I insist on that 'cause when you talk to biologists in OK so are doctors etc.",
                    "label": 0
                },
                {
                    "sent": "It is very easy to to imagine models with very small bias be cause you know you say I want to predict phenotype from genotype the what is the true model and everybody has an ID.",
                    "label": 0
                },
                {
                    "sent": "It has to include many snips.",
                    "label": 0
                },
                {
                    "sent": "It has to include many environmental factors.",
                    "label": 0
                },
                {
                    "sent": "It has to include other things so it's very easy to imagine what could be the true model and to imagine designing a model with good covariates and perhaps many covariates that that would have a small bias, meaning that potentially could fit that remodel.",
                    "label": 0
                },
                {
                    "sent": "But very often when we do that we are dead because of brians.",
                    "label": 0
                },
                {
                    "sent": "And very often this would be the spirit of machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's much better to run with the wrong model.",
                    "label": 0
                },
                {
                    "sent": "We know it's wrong, but.",
                    "label": 0
                },
                {
                    "sent": "I tried to reset it.",
                    "label": 0
                },
                {
                    "sent": "If we can decrease the variance at the cost of increasing the bias at the end we make you better prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very important message.",
                    "label": 0
                },
                {
                    "sent": "I would really straight it, but the idea that when we do learning we don't only care about what would be a realistic model or what are the true features that contribute to the prediction.",
                    "label": 0
                },
                {
                    "sent": "What we want at the end is to make fewer errors and to make sure it works.",
                    "label": 0
                },
                {
                    "sent": "We need to control the complexity of the model and sometimes accept 2 is good to have wrong models but wrong and simple models are often better than.",
                    "label": 0
                },
                {
                    "sent": "Realistic but complicated models.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go back to the to the least square again.",
                    "label": 0
                },
                {
                    "sent": "I will not go to the technical details, but probably most of you have seen that before.",
                    "label": 0
                },
                {
                    "sent": "If you want to do linear regression, you know you can write any question like I want to infer a function F, which I so I will use the beta instead of the value, but I assume that my linear model is parameterized by a vector beta, which is the slope of the.",
                    "label": 0
                },
                {
                    "sent": "Of the of the of the linear model, and therefore that I run a model made of sum of terms which like this morning I I summarized by an inner product between the vector beta and the data X OK, the least square is simply.",
                    "label": 0
                },
                {
                    "sent": "So when I say I fit the least square model means that I try to find the vector beta that minimizes the sum of squares.",
                    "label": 0
                },
                {
                    "sent": "So this is a frequentist way to describe it.",
                    "label": 0
                },
                {
                    "sent": "I don't assume there is Goshen model etc.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't say it, I just say that I define.",
                    "label": 0
                },
                {
                    "sent": "The cost of loss function and I said it would be Theresa Bitter that in total we have a small average or sum of squared error between my model and the true data.",
                    "label": 0
                },
                {
                    "sent": "OK so if you say I want to minimize this this sum of squares just write the equations, differentiate and you will find that there is at least if you can invert a matrix there is a solution which is the least square solution.",
                    "label": 0
                },
                {
                    "sent": "This equation.",
                    "label": 0
                },
                {
                    "sent": "OK so this is the least square solution to regression.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this thing is interesting and it has been around for for many years, and in particular there is a very important theorem.",
                    "label": 0
                },
                {
                    "sent": "Innocence, saying that this is the best estimator you can have at least the best.",
                    "label": 0
                },
                {
                    "sent": "So when I say the best.",
                    "label": 0
                },
                {
                    "sent": "It's called the blue estimator, so blue means is the best, but linear OK unbiased estimator, meaning that if you consider always to make a linear model not only by minimizing the the sum of squares with any dinner model such that on average it does the good position if the true model is dinner.",
                    "label": 0
                },
                {
                    "sent": "So here I assume I have a true linear model an I want to estimate it by your linear model.",
                    "label": 0
                },
                {
                    "sent": "Then this theorem says that the best you can do is to use the square.",
                    "label": 0
                },
                {
                    "sent": "At least if you don't want to have a bias model, and in many cases in many cases in a sense to refuse the buyers because you say, well, you know, bias is a systematic error, so it seems to be stupid to say I want systematic errors.",
                    "label": 0
                },
                {
                    "sent": "So among the class of models with linear models without systematic errors.",
                    "label": 0
                },
                {
                    "sent": "This quote is the best.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by best it has a smaller variance, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So probably I should shuffle my size.",
                    "label": 0
                },
                {
                    "sent": "I would say sorry so the question.",
                    "label": 0
                },
                {
                    "sent": "Yeah I'm trying I control myself because I went too fast so at the end of my message will be this one but I make you know short thing like if you take your book in statistiques they will tell you do the square because it's blue.",
                    "label": 0
                },
                {
                    "sent": "And when you do that, it it makes sense if you do classical statistics in small dimension with many points in this case.",
                    "label": 0
                },
                {
                    "sent": "The the bias error is more important than the variance because the variance decreases with the number of points and so in my equal.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if I come back to this equation, it means that if you constrain yourself to having a 0 here, then the minimizer of the of the variance is the square.",
                    "label": 0
                },
                {
                    "sent": "OK, now exactly as you point out what I want to say, and I will say it again now is that even though this car is the best among the models with their bias.",
                    "label": 0
                },
                {
                    "sent": "It's not the best in terms of total error becausw you can do better if you increase the bias at the cost of decreasing variance.",
                    "label": 0
                },
                {
                    "sent": "So the question then is what?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the variance of this quite easy?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty big.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, you know this morning near showed a picture of the Marathon runner.",
                    "label": 0
                },
                {
                    "sent": "Instead we want to predict the future.",
                    "label": 0
                },
                {
                    "sent": "And I have the question at the end.",
                    "label": 0
                },
                {
                    "sent": "It takes preparation, but he said it's a it's a way to represent high dimensional spaces because in high dimension the test points are never in the same place as the threading point.",
                    "label": 0
                },
                {
                    "sent": "So my way to represent high dimension would be more this picture here.",
                    "label": 0
                },
                {
                    "sent": "These are the pictures I showed you initially saying this is linear model and this is 1 nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Now I change my way of thinking and I just say these are two linear models.",
                    "label": 0
                },
                {
                    "sent": "But this one is in small dimension and this one is in high dimension and in fact it's more so it's wrong, but morally correct if you you know if you make will talk of Gaussian regression like Gaussian processes etc.",
                    "label": 0
                },
                {
                    "sent": "These are linear models in some infinite dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "So the way to visualize it is this way and for me the difference between small and standard small dimension and statistics.",
                    "label": 0
                },
                {
                    "sent": "This is the left and here we use the square.",
                    "label": 0
                },
                {
                    "sent": "But if we're in high dimension which is modern machine learning and personal.",
                    "label": 0
                },
                {
                    "sent": "It is seen we leave here and this is a very simple model.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "It would be what the square would do.",
                    "label": 0
                },
                {
                    "sent": "This is a result of this square and we see that in high dimension even the least square, which is simple as huge variance.",
                    "label": 0
                },
                {
                    "sent": "In fact you can you know the variance is proportional to the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "So if you have millions of parameters to estimate and the variance of your model would be very large.",
                    "label": 0
                },
                {
                    "sent": "OK so my message here is that there is a big difference between small dimension and high dimension an in high dimension.",
                    "label": 0
                },
                {
                    "sent": "In this case, it seems you use a simple model simple linear model, but still the error is dominated by variance.",
                    "label": 0
                },
                {
                    "sent": "You need to do something, otherwise you cannot the error, which is the sum of bias and error will be large because of the variance term.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how to?",
                    "label": 0
                },
                {
                    "sent": "How to kill the variance?",
                    "label": 0
                },
                {
                    "sent": "How to do something that would you know, increase, improve the model, make better prediction by trying to reduce the variance at the cost, perhaps of increasing the bias?",
                    "label": 0
                },
                {
                    "sent": "Well, a solution.",
                    "label": 0
                },
                {
                    "sent": "So this is a frequentist solution and you could rephrase it vision approach using priors etc.",
                    "label": 0
                },
                {
                    "sent": "But I will not go in this direction.",
                    "label": 0
                },
                {
                    "sent": "One way to do is to use what I call shrinkage estimators.",
                    "label": 1
                },
                {
                    "sent": "There are many names for it, but let's look at what it is.",
                    "label": 0
                },
                {
                    "sent": "So shrinkage estimator is simply a recipe that requires three ingredients.",
                    "label": 0
                },
                {
                    "sent": "The first one is to say we will focus on the family of model.",
                    "label": 1
                },
                {
                    "sent": "So for example in all my talks it will be linear models, so I receive my set to request off linear models.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for independently of that you will.",
                    "label": 0
                },
                {
                    "sent": "I will define some objective function which is related to a risk or loss function.",
                    "label": 0
                },
                {
                    "sent": "So for this square we said we quantify evil in our model is good in terms of the total squared error so.",
                    "label": 0
                },
                {
                    "sent": "Again, no problem speaking.",
                    "label": 0
                },
                {
                    "sent": "There burns just arbitrary loss function an I say for any model, any linear model, but I can check if it's good or not based on it's it's called the risk in in our domain.",
                    "label": 0
                },
                {
                    "sent": "So based on its risk which is there in this case, the average squared error.",
                    "label": 0
                },
                {
                    "sent": "And so at least square was just a minimize the risk, subject to the fact that you are in our model.",
                    "label": 0
                },
                {
                    "sent": "This was re square what those are.",
                    "label": 0
                },
                {
                    "sent": "Shrinkage estimator is to other third layer in this in this architect.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, which is to add a constraint so it is very simple.",
                    "label": 0
                },
                {
                    "sent": "You know there is nothing complicated.",
                    "label": 0
                },
                {
                    "sent": "Here is just to say because when we just look at all in our models, it's already too complicated.",
                    "label": 0
                },
                {
                    "sent": "We are already in high dimension, huge variance away to reduce the variance is simply to constrain the linear model an, for example to constrain it by some penalty so.",
                    "label": 0
                },
                {
                    "sent": "The rest of the talk of my two talks will be about how to make this penalty, but the broad picture to say will make a penalty on the set of linear models function Omega that will quantify the model is complicated or not.",
                    "label": 0
                },
                {
                    "sent": "For example, and we will say the shrinkage estimator will be just the best model.",
                    "label": 0
                },
                {
                    "sent": "So the model with the smaller streaks risk.",
                    "label": 0
                },
                {
                    "sent": "But constrained to have bounded penalty, so there will be one parameter in this model which is the C. Here the C defines the set of models that we consider.",
                    "label": 0
                },
                {
                    "sent": "So instead of saying we take any RENA models, we will define the penalty and say we just restrict ourselves to the linear models with the penalties mordancy.",
                    "label": 0
                },
                {
                    "sent": "And then among these models will find the best one.",
                    "label": 0
                },
                {
                    "sent": "OK, so very small difference between the square under the square just as not this term.",
                    "label": 0
                },
                {
                    "sent": "But this is intuitively how we can.",
                    "label": 0
                },
                {
                    "sent": "You know, reduce the space of candidates and therefore reduce what I called the variance because process we have a picture on that reducer variance.",
                    "label": 0
                },
                {
                    "sent": "But we will introduce a bias again, so I called the biases that perhaps the good model has a large penalty.",
                    "label": 0
                },
                {
                    "sent": "So if we restrict the penalty to be small, we may never be able to find the best model, the true model.",
                    "label": 0
                },
                {
                    "sent": "There was a true model, so this is the price we pay.",
                    "label": 0
                },
                {
                    "sent": "We accept that perhaps the good model have has a large penalty, but we are sickle cell too.",
                    "label": 0
                },
                {
                    "sent": "Small penalties, you know those who control the variance.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's try to look at the same thing, but visually so I tried to make a graphic.",
                    "label": 0
                },
                {
                    "sent": "I'm not very good at that, but I tried.",
                    "label": 0
                },
                {
                    "sent": "So suppose suppose this is a space of models you want to learn my messages that you said there may be some true model here that you don't see.",
                    "label": 0
                },
                {
                    "sent": "You just observe data and this one would be the guy that makes the best predictions in the future.",
                    "label": 0
                },
                {
                    "sent": "But we don't know it, OK?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you, when you when you do least square regression, you will estimate the model and I said perhaps your estimated model will be very different from the true model just because it's hard to fit the model in high dimension.",
                    "label": 0
                },
                {
                    "sent": "You know there is no cetera, so the error is about the distance between the two models.",
                    "label": 0
                },
                {
                    "sent": "the ID of shrinkage is just.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say let's arbitrarily define, you know, in this case a circle and just walking in this circle.",
                    "label": 0
                },
                {
                    "sent": "So not that the true model is not here.",
                    "label": 0
                },
                {
                    "sent": "OK, but what we do nevertheless is to say, let's look at the best model in the circle based on the data.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So because we restrict ourselves to be in the in the yellow circle, the best model will not be here anymore, it will.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the circle somewhere and you see here that perhaps it was rocky, but the constraint model is closer to the true model is still quite far, but it moved closer.",
                    "label": 0
                },
                {
                    "sent": "So why did it move closer?",
                    "label": 0
                },
                {
                    "sent": "Just cause in this case this is an assertion that the distance was mostly because of the variance in this case.",
                    "label": 0
                },
                {
                    "sent": "So we constrain that.",
                    "label": 0
                },
                {
                    "sent": "Not that because we restrict ourselves to the to the yellow circle, we can never reach the truth, the true model, but perhaps there is among the secure one best model which which are right here and so on.",
                    "label": 0
                },
                {
                    "sent": "This P.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, what I called the bias in the variance would be interpreted as the distance between how far the true model is from my set and the variance is ready to the size of the of the constraint sets.",
                    "label": 0
                },
                {
                    "sent": "OK, so the total decomposition of errors is a bit to say that the total distance is the sum of bias and variance.",
                    "label": 0
                },
                {
                    "sent": "This is conceptual.",
                    "label": 0
                },
                {
                    "sent": "Don't try to define a real distance, here is just an ID.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we will do.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that that so this is the first key ID.",
                    "label": 0
                },
                {
                    "sent": "The first idea is to run in high dimension.",
                    "label": 0
                },
                {
                    "sent": "We need to do something and the way we do it is constraining this.",
                    "label": 0
                },
                {
                    "sent": "Now it's not the end of the story, because once you know once I say decrease the variance, it will be better.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to do it, and in fact once we decrease the variance we increase the base and it could be costly.",
                    "label": 0
                },
                {
                    "sent": "You know if you restrict yourself to stupid models it will never be good.",
                    "label": 0
                },
                {
                    "sent": "So this is where there is a second ID which is perhaps in the choice of this penalty.",
                    "label": 0
                },
                {
                    "sent": "We can use some prior knowledge we have about the data or but the the problem and try to design this constraint set in such a way that is not too far from the true model.",
                    "label": 0
                },
                {
                    "sent": "Because if we do that, we could simultaneously decrease the variance because we constrain and not increase the bias too much.",
                    "label": 0
                },
                {
                    "sent": "OK. And for example, if for some reason you know the true model is not too far from the horizontal axis, it might be stupid with yourself to a circle you made.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just repeat yourself.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was a circle, but.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you restrict yourself to this kind of space in total, you know the volume of these of these.",
                    "label": 0
                },
                {
                    "sent": "Ellipsoid is not different from the circle, so the variance will be the same as the previous one, but the bias has decreased because now you're much closer to model.",
                    "label": 0
                },
                {
                    "sent": "So in basin services it would be the role of the prior innocence to design A priority to what's your belief of the true model and make a good priority in the prior weather.",
                    "label": 0
                },
                {
                    "sent": "Truman's not too far for us.",
                    "label": 0
                },
                {
                    "sent": "It will be trying to design A penalty that is not too far from the tree model.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we do that, we can decrease the bias and still control the variance and therefore still have a better model.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a.",
                    "label": 0
                },
                {
                    "sent": "These are the two key ideas.",
                    "label": 0
                },
                {
                    "sent": "Again, the first key idea is we need to do something an.",
                    "label": 0
                },
                {
                    "sent": "In my talk, I decide to define a penalty, which is a way to reduce the variance and the second ID is how to design A penalty.",
                    "label": 0
                },
                {
                    "sent": "The good way to do would be to try to design A penalty that decrease in the bio spinning, trying to find simple models which are as close as possible to the good model.",
                    "label": 0
                },
                {
                    "sent": "This is quite set this where it looks like I don't need half an hour to go to this point, but this is where the key idea here.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you do that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of more general comments.",
                    "label": 0
                },
                {
                    "sent": "In practice though, the way you know the way you solve these problems is not only to define.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanna leave Sweden do that but you can, you know, make a series of ellipsoids like a small to big.",
                    "label": 0
                },
                {
                    "sent": "This gives you a series of models and then typically you set the best ones by cross validation.",
                    "label": 0
                },
                {
                    "sent": "So this is what's called.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure learning social regression.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the architecture of our running models would be to define a penalty and when we move the constraint we have simple model complicated model.",
                    "label": 0
                },
                {
                    "sent": "We have the bias and variance that decrease and we optimized the complexity of the model typically by cross validation that was described this morning.",
                    "label": 0
                },
                {
                    "sent": "So I don't need to describe it in order to have a final model.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, I did it so this is the main idea.",
                    "label": 0
                },
                {
                    "sent": "Now I'll try to do this straight.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to make the penalty?",
                    "label": 0
                },
                {
                    "sent": "And this is where now I can go on the right on the left making kernels or making other things.",
                    "label": 0
                },
                {
                    "sent": "So turns out that over the years, so I contributed to this activity of designing kernels or designing penalties, and I found a nice website.",
                    "label": 0
                },
                {
                    "sent": "You know where they explain how to make paper lanterns and you will see that this shape here is actually one of the shapes that will be designing at the end, so this is a this is called Latin Group latter group lasso, which is very successful too.",
                    "label": 0
                },
                {
                    "sent": "Anyway, things but at the end this notion of so I will talk of Omega, how to make Omega and this kind of shape will typically the kind of thing we design.",
                    "label": 0
                },
                {
                    "sent": "I try to explain to you how you can come up with this idea of designing these penalties.",
                    "label": 0
                },
                {
                    "sent": "The key being trying to define a small space so that you think that there are good models in this space here.",
                    "label": 0
                },
                {
                    "sent": "OK, but so so so now I will take 2 or so.",
                    "label": 0
                },
                {
                    "sent": "So today I will focus on what's called.",
                    "label": 0
                },
                {
                    "sent": "The kernel industry so so cannot Morris is a family of models wear for me.",
                    "label": 0
                },
                {
                    "sent": "It corresponds more or less to designing penalties Omega which are smooth typically which which will be quadratic in some up to some change of variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a bit Bush and priorat quadratic in some in something, so it's a huge topic.",
                    "label": 0
                },
                {
                    "sent": "You know kernel learning and not the best one.",
                    "label": 0
                },
                {
                    "sent": "Probably here to talk about it, but it has been around for many years and I will over summarize it in or fewer.",
                    "label": 0
                },
                {
                    "sent": "A few minutes and the second approach will be, which is also very popular and and you know that came a bit after the kernels, but probably is more popular now is to is to define what I called the non smooth so non smooth.",
                    "label": 0
                },
                {
                    "sent": "What I mean by Nonsmooth is typically you see I mean if you can see it in this Lantern, it has a nice convex shape, but there are like edges here, and there is a nice industry now to design things that with edges and edges are exactly the points where you put your prior knowledge becausw.",
                    "label": 0
                },
                {
                    "sent": "After some authorities you will see that.",
                    "label": 0
                },
                {
                    "sent": "When you design such a shape into the penalty, what's important is that on the edge you have some good models, so there is a new topic, and then we use rate it, which is how to define nonsmooth penalties with edges.",
                    "label": 0
                },
                {
                    "sent": "Search that in the edges you have good models.",
                    "label": 0
                },
                {
                    "sent": "This will be the last order group, last sold, this kind of stuff.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, are there any question that is?",
                    "label": 0
                },
                {
                    "sent": "At this step.",
                    "label": 0
                },
                {
                    "sent": "So if not.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let's enter the kernel business.",
                    "label": 0
                },
                {
                    "sent": "So again, I assume.",
                    "label": 0
                },
                {
                    "sent": "Many of you are know know a lot about kennel.",
                    "label": 0
                },
                {
                    "sent": "I will try to to mix between notions for beginners and also there will be some homework, some exercises.",
                    "label": 0
                },
                {
                    "sent": "So if you feel too two board you can try to do the exercises.",
                    "label": 0
                },
                {
                    "sent": "I will show I don't have place for all the other proves, but so the idea of channel.",
                    "label": 0
                },
                {
                    "sent": "I mean for me at in this in this setting is just to say we will penalize will use a penalty which is a Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "It's just just just that.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first cannon method so I will go back to kernel in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "So let's let's forget about, because it's not worth it here.",
                    "label": 0
                },
                {
                    "sent": "We just use as a penalty to constrain the reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "The squad.",
                    "label": 0
                },
                {
                    "sent": "No man this this directly to from the standard square regression towards Coral Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So for me Ridge regression is the first instance.",
                    "label": 0
                },
                {
                    "sent": "Simplest instance of this notion of running in high dimension compared to running in small dimension.",
                    "label": 1
                },
                {
                    "sent": "So what is Ridge regression is just taking the same set of models as.",
                    "label": 0
                },
                {
                    "sent": "Linear regression, so you take linear models.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You take the same objective function, so you try to minimize the risk, but the third.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ingredients that you add a penalty, which is for any vector beta subitize the vector of weights you will compute its squared Euclidean norm, meaning the sum of the squares of the coefficients.",
                    "label": 1
                },
                {
                    "sent": "OK, so for any beta you compute that and what we do in Ridge regression is to say let's minimize the risk over the linear models search that the UK number of PETA is bounded by a constant.",
                    "label": 0
                },
                {
                    "sent": "When you do that, you do Ridge regression, so I must say I forgot to say it.",
                    "label": 0
                },
                {
                    "sent": "Earlier.",
                    "label": 0
                },
                {
                    "sent": "And it's it's obvious for many, but perhaps not to anybody.",
                    "label": 0
                },
                {
                    "sent": "I will constantly shift between two ways to express.",
                    "label": 0
                },
                {
                    "sent": "Did I write it?",
                    "label": 0
                },
                {
                    "sent": "No, I did not.",
                    "label": 0
                },
                {
                    "sent": "So I just want to.",
                    "label": 0
                },
                {
                    "sent": "OK I will write it.",
                    "label": 0
                },
                {
                    "sent": "I will not be very rigorous, but.",
                    "label": 0
                },
                {
                    "sent": "In all my slides, it's the same to minimize.",
                    "label": 0
                },
                {
                    "sent": "That when you minimize the risk.",
                    "label": 0
                },
                {
                    "sent": "Search that.",
                    "label": 0
                },
                {
                    "sent": "Omega of Beta is bounded by a constant.",
                    "label": 0
                },
                {
                    "sent": "Another way to write it.",
                    "label": 0
                },
                {
                    "sent": "It's equivalent to minimizing.",
                    "label": 0
                },
                {
                    "sent": "The risk of bit Vita plus Lambda times the penalty.",
                    "label": 0
                },
                {
                    "sent": "So when I say it's equivalent, you know it's equivalent.",
                    "label": 0
                },
                {
                    "sent": "If this is convex, is the regular way to write it.",
                    "label": 0
                },
                {
                    "sent": "I will not go to the details, but I will you know constantly.",
                    "label": 0
                },
                {
                    "sent": "Sometimes write it this way or this way, so either you constrain the penalty to be bounded by a constant or you just add it.",
                    "label": 0
                },
                {
                    "sent": "This way college penalty you know you penalize the risk by it.",
                    "label": 0
                },
                {
                    "sent": "It gives the same solution.",
                    "label": 0
                },
                {
                    "sent": "So what's?",
                    "label": 0
                },
                {
                    "sent": "Here you have see here you have Lambda.",
                    "label": 0
                },
                {
                    "sent": "There is a correspondence between CR and number, but on average, so I mean without going through the technical details, this is the same for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so up to now I motivated this thing by looking at this, but in terms of algorithm like Ridge regression, I can write it this way or this way.",
                    "label": 0
                },
                {
                    "sent": "And therefore let's look at Ridge regression returned the 2nd way, so Ridge regression I say we minimize the squared error with is called the rich.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "So the ridges you will see why this panel?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And therefore you can write down the equation.",
                    "label": 0
                },
                {
                    "sent": "So Ridge regression is just minimizing the risk plus Lambda times the penalty.",
                    "label": 0
                },
                {
                    "sent": "See the second form and at times the squared error.",
                    "label": 0
                },
                {
                    "sent": "And if you write it in matrix form, it's easier size to write it in matrix form.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To find that there is an explicit solution, which is that the beta, the solution of this is a bit like the least square regression, except that we added.",
                    "label": 0
                },
                {
                    "sent": "So there was a nice version we had to inverse the matrix, each transpose X, which is not always invertible.",
                    "label": 0
                },
                {
                    "sent": "In Ridge regression, the Lambda parameter that we put in a penalty ends up in terms of algorithm as some additional term on the diagonal of these metrics.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that then the matrix is inversible.",
                    "label": 0
                },
                {
                    "sent": "Invertible at least you can solve it, and when you play with Rhonda you obtain this.",
                    "label": 0
                },
                {
                    "sent": "The set of solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For illustration, again taken from the same book here, if you look at the I don't remember what we want to predict, but maybe diabites or something like that.",
                    "label": 0
                },
                {
                    "sent": "You want to predict some response variables from covariates on the left to have the weights of the least square regression.",
                    "label": 0
                },
                {
                    "sent": "And when you do Ridge regression, meaning that you increase this Lambda, you move to the right and you see the coefficients which are shrink to zero OK and at the end when you penalize too much, it means that you will see yourself to two vectors beta with Euclidean up 0 meaning.",
                    "label": 0
                },
                {
                    "sent": "All the weights are going to 0, so this is the factor of shrinkage here.",
                    "label": 0
                },
                {
                    "sent": "This is Y equals shrinkage.",
                    "label": 0
                },
                {
                    "sent": "It shrinks the things to zero an again.",
                    "label": 0
                },
                {
                    "sent": "Intuitively I don't know what.",
                    "label": 0
                },
                {
                    "sent": "What is a good model, but probably on the right you know you you shrink too much so you are too far away from the good model.",
                    "label": 0
                },
                {
                    "sent": "You start to increase the bias too much, but certainly you decrease the variance because when you constrain them they cannot move too much and this is you can write, you know cross validation could tell you where to start, where you can have a good balance between having.",
                    "label": 0
                },
                {
                    "sent": "Not to NYC model, but which can be learned efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once you have Ridge regression, you can easily generalize it to other things, and in particular, one way to generalize this to change the objective function.",
                    "label": 0
                },
                {
                    "sent": "So for the moment I just said let's look at the sum of squared error.",
                    "label": 0
                },
                {
                    "sent": "One way to design many learning algorithms is to change the risk and for example to define what's called a loss function.",
                    "label": 0
                },
                {
                    "sent": "So loss function is simply a function that these two terms, as it puts the first time, is what you predict and the 2nd is what you should predict.",
                    "label": 0
                },
                {
                    "sent": "OK, and the last quantifies how bad you are.",
                    "label": 0
                },
                {
                    "sent": "So this square was using the squared difference between the two arguments.",
                    "label": 0
                },
                {
                    "sent": "To quantify how bad you are.",
                    "label": 0
                },
                {
                    "sent": "But if you change the loss then you can again apply the same principle by saying that for any loss I have a risk so I can look for a linear model that minimizes risk, which is the average loss according to my own interest.",
                    "label": 0
                },
                {
                    "sent": "My own loss but still penalize it by a regression bias or the squared error.",
                    "label": 0
                },
                {
                    "sent": "And this gives the family of things which.",
                    "label": 0
                },
                {
                    "sent": "Turn out to be quite famous and and sometimes very.",
                    "label": 0
                },
                {
                    "sent": "You know state of the art in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "Let's look at them.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what kind of clothes do we have?",
                    "label": 0
                },
                {
                    "sent": "So I talked of the squared loss, which is the first one.",
                    "label": 0
                },
                {
                    "sent": "This is to Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "But you know, if you want to be a bit more robust this classical to use losses that that are not quadratic, because if you have outliers, you don't want to pay too much.",
                    "label": 0
                },
                {
                    "sent": "If you have a point very far away from what you predict.",
                    "label": 0
                },
                {
                    "sent": "So typically there are things called the Huber robust loss or the epsilon insensitive loss.",
                    "label": 0
                },
                {
                    "sent": "So these are examples I just read that to say that you can you can use them again.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Not based on the model is it could be based on what?",
                    "label": 0
                },
                {
                    "sent": "What at the end you you're ready to pay or what what you qualify as a loss.",
                    "label": 0
                },
                {
                    "sent": "But so all these things you need when you when you plug any any of these loss functions in the general framework of minimizing the mean loss plus Lambda times the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "This leads to an algorithm so the something a bit special about Ridge regression.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that here?",
                    "label": 0
                },
                {
                    "sent": "You see, you have the explicit solution, so you can implement it by saying if I have data X and label Y.",
                    "label": 0
                },
                {
                    "sent": "This is my model in general, there is no explicit form, because if you if you change your loss, it will not be easy.",
                    "label": 0
                },
                {
                    "sent": "You know analytically to write down.",
                    "label": 0
                },
                {
                    "sent": "What is the best model, but still what you would do in this case is to implement some numerical algorithm to optimize and minimize the loss.",
                    "label": 0
                },
                {
                    "sent": "So there will be no explicit form, but at least you will be able to implement an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That will find the good good solution so far.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over these losses you can implement an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's talk a bit.",
                    "label": 0
                },
                {
                    "sent": "About lazy So what I say can be used for regression or classification.",
                    "label": 0
                },
                {
                    "sent": "The case of classification is quite interesting because in this case the.",
                    "label": 0
                },
                {
                    "sent": "So for binary classification the why it takes only two values, which I will incur does plus one and minus one.",
                    "label": 0
                },
                {
                    "sent": "So this is where the blue arrange or active not active or metastasis don't metastasis.",
                    "label": 0
                },
                {
                    "sent": "So we call that one and minus one and.",
                    "label": 0
                },
                {
                    "sent": "In machine learning there's been a lot of interest in what's called a large margin classifiers so large margin are simply all these classifiers that define a loss function.",
                    "label": 0
                },
                {
                    "sent": "Of a particular kind, which depends on what's called the margin.",
                    "label": 1
                },
                {
                    "sent": "So to try to explain it.",
                    "label": 0
                },
                {
                    "sent": "You know when we make a loss function, so we need to compare.",
                    "label": 0
                },
                {
                    "sent": "So we make a prediction.",
                    "label": 0
                },
                {
                    "sent": "We are previous write it.",
                    "label": 0
                },
                {
                    "sent": "The question is you predict a value F of X.",
                    "label": 0
                },
                {
                    "sent": "And the true, you know, the true label is why?",
                    "label": 0
                },
                {
                    "sent": "So this guy is either minus one or plus one.",
                    "label": 0
                },
                {
                    "sent": "What you predict?",
                    "label": 0
                },
                {
                    "sent": "Typically it will be easier to work with functions that predict continuous values, 'cause we know how to make linear model in R model.",
                    "label": 0
                },
                {
                    "sent": "Mack makes a linear prediction, so this kind of thing typically predicts real value the number.",
                    "label": 0
                },
                {
                    "sent": "So the question is when you predict.",
                    "label": 0
                },
                {
                    "sent": ".5 is it good or not?",
                    "label": 0
                },
                {
                    "sent": "If the label is plus one or minus one, so you need to have you know to relate the continuous value to the label so the most and other ways to say.",
                    "label": 0
                },
                {
                    "sent": "Well, if I predict a real value, the number I have a number I would just say that if it's positive it means that I think the label is press one and if it's negative I think the label is minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case you could say well to compare the disposition to why.",
                    "label": 0
                },
                {
                    "sent": "I just check if the sign is coherent.",
                    "label": 0
                },
                {
                    "sent": "OK so I could look at.",
                    "label": 0
                },
                {
                    "sent": "Why times ever weeks?",
                    "label": 0
                },
                {
                    "sent": "If this thing is positive, it means that the two of the the same, the same sign, so it's good.",
                    "label": 0
                },
                {
                    "sent": "There is no loss, and if it's negative then there is a loss because I predict it will be black, but it's it's quite OK, so this would be one way to do, but in fact.",
                    "label": 0
                },
                {
                    "sent": "To go a bit beyond that, when you took off.",
                    "label": 0
                },
                {
                    "sent": "So this thing in fact is called.",
                    "label": 0
                },
                {
                    "sent": "This thing is called the margin.",
                    "label": 1
                },
                {
                    "sent": "And in fact, in the margin when we talk of large margin, the idea is that not only you want these things to be to be the same size, the same sign, but also you would like your prediction to be as confident as possible and the idea is that if you predict a real value number, you know you can try to say well if the if the value is large.",
                    "label": 0
                },
                {
                    "sent": "I make a confident prediction, either positive or negative, and so if you look at the end of this number, if you look at the sign, it will tell you if your prediction is correct.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the value that.",
                    "label": 0
                },
                {
                    "sent": "If it's positive, it means you have made a good prediction.",
                    "label": 0
                },
                {
                    "sent": "But in addition, if it's large it was a good prediction with good confidence and on the opposite way.",
                    "label": 0
                },
                {
                    "sent": "If it was negative with large value, you would be very confident in your prediction, but it will be an error.",
                    "label": 0
                },
                {
                    "sent": "So the large margin method just look at this thing and define a loss function that is a decreasing function of this thing and which typically looks.",
                    "label": 1
                },
                {
                    "sent": "Like this so, so these are the loss functions which are used by many machine learning algorithms where horizontally you have the margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so the blue one is the one I was is the 01.",
                    "label": 0
                },
                {
                    "sent": "It means that if the sign is positive there is no loss is a good prediction if the sign is negative it costs one.",
                    "label": 0
                },
                {
                    "sent": "It means I made one mistake.",
                    "label": 0
                },
                {
                    "sent": "So this would be one way to quantify, so the blue one corresponds to a risk which is the number of mistakes you make now.",
                    "label": 0
                },
                {
                    "sent": "The other ones are variants.",
                    "label": 0
                },
                {
                    "sent": "You see they tend to be decreasing, so one is a bit different but.",
                    "label": 0
                },
                {
                    "sent": "They don't care, they are decreasing, meaning that when you on the right you make good predictions with good confidence and this has no no cost.",
                    "label": 0
                },
                {
                    "sent": "It's nice when you on the left.",
                    "label": 0
                },
                {
                    "sent": "Here you make your butt prediction, then with increasing confidence in this, you want to cast it alot.",
                    "label": 0
                },
                {
                    "sent": "OK, so all these this leads to a variety of penalties and we will see some of them in more detail.",
                    "label": 0
                },
                {
                    "sent": "But so so this idea of large margin classifiers is when you design A function of this that tries to say I want to learn something that pushes the margin to the right, which says good prediction with good confidence.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it's full of equation, but we will not look at them.",
                    "label": 0
                },
                {
                    "sent": "Simplest example, perhaps it's the logistic regression.",
                    "label": 0
                },
                {
                    "sent": "What is logistic regression for me is just a particular function which is.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The green one.",
                    "label": 0
                },
                {
                    "sent": "OK, you see there is a green one here.",
                    "label": 0
                },
                {
                    "sent": "This loss function as its decreasing its convex.",
                    "label": 0
                },
                {
                    "sent": "It's essentially it's zero, so it does nothing to make a good position with confidence and assembly on the left it will be linear, so you have a linear cost and the.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission is this are the ready question here.",
                    "label": 0
                },
                {
                    "sent": "So the log of 1 plus exponential minus the margin.",
                    "label": 0
                },
                {
                    "sent": "This is the question of this green line when you when you decide you take this loss function at the end you obtain an algorithm which is called Ridge regression there sorry logistic regression, in this case regularizer, GC creation.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm itself would be.",
                    "label": 0
                },
                {
                    "sent": "We need to find Arena model beta that minimizes the average loss penalized by some health L2 regularizer.",
                    "label": 0
                },
                {
                    "sent": "If you want to implement it.",
                    "label": 0
                },
                {
                    "sent": "This is a case where there is no explicit solution.",
                    "label": 0
                },
                {
                    "sent": "I cannot write Pizza Hut is the inverse of this.",
                    "label": 0
                },
                {
                    "sent": "I don't know what it is, but this is an example where classic Lee.",
                    "label": 0
                },
                {
                    "sent": "You can just you know you need to minimize a convex smooth function so you can use things at the Newton method, which is the most obvious thing to do, and for this you just need to compute.",
                    "label": 0
                },
                {
                    "sent": "The gradient Hessian you plug that into a solve Newton and then you get after a few iterations dissolution of the rich.",
                    "label": 0
                },
                {
                    "sent": "So regularizer GC creation.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So exercise just to follow up on is nice talk.",
                    "label": 0
                },
                {
                    "sent": "In fact, when you look at logistic regression on Wikipedia or wherever, this is not the way it's presented.",
                    "label": 1
                },
                {
                    "sent": "You know, I just say that for me, which progression is one of these loss functions?",
                    "label": 0
                },
                {
                    "sent": "But in general, another way to derive the same algorithm is to make a model and you can check just computation that.",
                    "label": 0
                },
                {
                    "sent": "In fact, logistic regression can be seen as a maximum likelihood estimator when you define a particular model where you say.",
                    "label": 1
                },
                {
                    "sent": "So he said you were you model the conditional probability of Y given X by this kind of equation and you say, let's look at the maximum conditional likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "So let's maximize the log likelihood of P. Given this model, if you write down the equation after three lines, so we obtain that what you do.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is this thing?",
                    "label": 0
                },
                {
                    "sent": "Which is what I presented as minimizing loss function?",
                    "label": 0
                },
                {
                    "sent": "OK, so loss function or probability model?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's the same.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at another example.",
                    "label": 0
                },
                {
                    "sent": "So again I think this one is quite famous.",
                    "label": 0
                },
                {
                    "sent": "I will start with the classic old way to present SVM and we see that in fact this gym is just a slightly different loss function.",
                    "label": 0
                },
                {
                    "sent": "So classic away to present as, Umm, I assume all of you have seen that.",
                    "label": 0
                },
                {
                    "sent": "Or I mean if you came to being without seeing that.",
                    "label": 0
                },
                {
                    "sent": "Is dangerous because this is a video you know, like one of the places where SVM are have been developed.",
                    "label": 0
                },
                {
                    "sent": "So what is SVM?",
                    "label": 0
                },
                {
                    "sent": "Well, as Jim is just one algorithm to solve, so let's look at what's the simplest hard margin SVM.",
                    "label": 0
                },
                {
                    "sent": "So suppose I give you this points.",
                    "label": 0
                },
                {
                    "sent": "I hope you see the colors here is blue here, it's red, so you want to discriminate.",
                    "label": 0
                },
                {
                    "sent": "It seems that the line is enough here, so you could say well.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's let's get here.",
                    "label": 0
                },
                {
                    "sent": "If you get here you will pray.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The value on this point, which is read this guy is probably read now you observe.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That there are other ways to cut the spacing.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, you could.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's here and here the prediction.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The difference will predict.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blue",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a question is, which one is the best?",
                    "label": 0
                },
                {
                    "sent": "An the fantastic idea of ECM is to say one of them is better than the other one.",
                    "label": 0
                },
                {
                    "sent": "This one is better.",
                    "label": 0
                },
                {
                    "sent": "OK, and why is it better?",
                    "label": 0
                },
                {
                    "sent": "Because intuitively.",
                    "label": 0
                },
                {
                    "sent": "It's, you know, this one.",
                    "label": 0
                },
                {
                    "sent": "This one seems a bit very close to these red points.",
                    "label": 0
                },
                {
                    "sent": "If you have the choice, why do you go so close to the to the red point?",
                    "label": 0
                },
                {
                    "sent": "If you just want to separate both of them so to quantify that you.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just say, well, let's look at the.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The margin, so it's related to that that that this is the geometric way to define the margin, so the margin of a line here would be.",
                    "label": 0
                },
                {
                    "sent": "You know the minimum distance to some to some points, so here you know this line separates the rest from the blue with some distance.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one to some distance and.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You compare the two of them.",
                    "label": 0
                },
                {
                    "sent": "You see that intuitively, I said this one is better.",
                    "label": 0
                },
                {
                    "sent": "Becausw this tube is brother.",
                    "label": 0
                },
                {
                    "sent": "You have been able to be more the center and so the definition of how much in SVM in this case, is the algorithm that finds the line with the largest tube.",
                    "label": 0
                },
                {
                    "sent": "So it's neither of them.",
                    "label": 0
                },
                {
                    "sent": "If you look a bit, I mean you you think you go inside and you push, you push until you cannot push anymore, you will.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one and this one is the solution of how margin SVM.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do I talk about this?",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to be too related to the penalty cetera.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is the next exercise, which again is not very difficult, but if you cannot do it, it means that you need to work a bit more on this concept or it is rated when you do this.",
                    "label": 0
                },
                {
                    "sent": "So this operation of finding a line with a large tube and tube as large as possible is 1 instance of my generic framework of saying I learn a linear model that minimizes the average loss plus.",
                    "label": 0
                },
                {
                    "sent": "A rich penalty, the L2 penalty, but this time with the particulars and the question is what is the loss used here to express this?",
                    "label": 0
                },
                {
                    "sent": "This line, as in this framework.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not the logistic.",
                    "label": 0
                },
                {
                    "sent": "Wanda Logistics will not do that in the case of how machine SVM there is a loss function, so that if the true label is why I predict this, you need to output a number and when we minimize this average number plus this penalty then you get Hard Machine's gym.",
                    "label": 0
                },
                {
                    "sent": "So I leave it as exercise.",
                    "label": 0
                },
                {
                    "sent": "If she were students, want to discuss a I will explain to them that thing but just to say that the classical geometric way to present them.",
                    "label": 0
                },
                {
                    "sent": "Equivalently, could be presented directed his way by saying let's look at this loss function as GM.",
                    "label": 0
                },
                {
                    "sent": "Is this thing?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So by the way, this is what I will do now, so this this was hard.",
                    "label": 0
                },
                {
                    "sent": "Marginalism is like the simplest obvious SVM which nobody uses, because in most cases is very hard to find datasets where it's easy to separate the two groups by value line, so.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is another flavor of SVM which is the one used in practice, which which is a soft margin as Jim, but we just call it as VM, which in this case are represented in terms of loss function.",
                    "label": 0
                },
                {
                    "sent": "So the Soft Machine SVM is just one instance of.",
                    "label": 0
                },
                {
                    "sent": "Of my of my framework where I minimize some risk by penalizing by controlling the L2 norm, so we're still always penalizing the L2 norm of beta.",
                    "label": 0
                },
                {
                    "sent": "But this case with this function, so this function, let's go back here.",
                    "label": 0
                },
                {
                    "sent": "We saw it earlier was one of the functions for class.",
                    "label": 1
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacations, you know on this graph I said the green one is logistic regression, then the red one is.",
                    "label": 0
                },
                {
                    "sent": "If you use the red one, so the red one is 0 up to here, and then it's linear.",
                    "label": 0
                },
                {
                    "sent": "If you use this loss function then you have made what's called SVM.",
                    "label": 0
                },
                {
                    "sent": "OK's name is just using this function, so this function is called the hinge loss.",
                    "label": 1
                },
                {
                    "sent": "You already have the solution to the, so the heat, the hardener genus is not written here that.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, so it says sorry exercise for students.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "But still.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not much in hard margin can be formulated in terms of losses and you have the right to define losses that go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Margie know one way too so SCM on way to describe it is I choose a particular function, I regularize it and this gives them in terms of implementation, it will be quite different for majestic regression becausw here the last function you know has a King, so you cannot find a derivative or hidden, so you cannot apply Newton method.",
                    "label": 0
                },
                {
                    "sent": "So the way to implement it will be different.",
                    "label": 0
                },
                {
                    "sent": "But the spirit of it is not different from majestic regression, and in fact in practice, when you when you train a model with Ridge regression or SVM, there is no.",
                    "label": 0
                },
                {
                    "sent": "Never seen any difference in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "For example, if you you know in both cases if you regularize by the L2 norm, so you do regularizer Ridge regression or azienda SM is more fancy, but there is no difference in performance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or perhaps an exercise if you want is for the for the SVM normal SVM.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This loss function and you can go from this function to get some graphical interpretation.",
                    "label": 0
                },
                {
                    "sent": "In the case of.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dim the.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpretation is that it.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We learn a line, so this is the linear model that has both a large margin and that can make errors, but not too big.",
                    "label": 0
                },
                {
                    "sent": "So in this case you know.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I draw this line, I can draw a tube, so the Hodgin was trying to have the two bus as large as broad as possible.",
                    "label": 0
                },
                {
                    "sent": "The source machine will also try to make YouTube as broad as possible, but sometimes he will accept.",
                    "label": 0
                },
                {
                    "sent": "We have like this blue points far away from where it should be in the same folder for the red one.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can show.",
                    "label": 0
                },
                {
                    "sent": "Smaller size that when you use the hinge loss so when you minimize the average hinge loss plus Lambda times the square number of beta in terms of graphics.",
                    "label": 0
                },
                {
                    "sent": "What you do is that you find a line and a tube that minimize a compromise between having sorcery margins on the good warrior.",
                    "label": 0
                },
                {
                    "sent": "Let's call that the width of the tube, so the margin is how larger tubes.",
                    "label": 0
                },
                {
                    "sent": "So you want the machine to be large, so we decide to say we minimize the inverse of the margin plus a constant times.",
                    "label": 0
                },
                {
                    "sent": "The errors and errors are related to the red lines here, so how long they are?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the graphical interpretation of James.",
                    "label": 0
                },
                {
                    "sent": "It's it finds a compromise between large Cuban making few errors.",
                    "label": 0
                },
                {
                    "sent": "If you translate in mathematically, you will end up with the.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hinge loss so the hinge loss is one way to quantify the error.",
                    "label": 0
                },
                {
                    "sent": "And there is a link between the Lambda which is here.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The see which is here which is different from this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was a SVM.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So summary, you know in just a few minutes we introduced some of the most.",
                    "label": 0
                },
                {
                    "sent": "Important algorithms in a in machine learning, at least in the last 10 years as VMS Ridge regression in are, you know logistics, so all of them in a sense are examples.",
                    "label": 0
                },
                {
                    "sent": "They are just variations over a loss that can vary, but all the other times penalized by this.",
                    "label": 0
                },
                {
                    "sent": "And the reason why they are popular.",
                    "label": 0
                },
                {
                    "sent": "And I've been popular for successfully many applications is really this simple idea of regularising and controlling the complexity by playing with the Lambda that goes from very regular asmodel to less regularize then finding.",
                    "label": 0
                },
                {
                    "sent": "A good tradeoff between complexity of the model and fitting of the data.",
                    "label": 1
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I see that there are two to say, so let's try now to go to the more fancy and interesting kernel business.",
                    "label": 0
                },
                {
                    "sent": "So here it seems a bit.",
                    "label": 0
                },
                {
                    "sent": "Ridge regression is not very fashionable, it has been around for four years now.",
                    "label": 0
                },
                {
                    "sent": "What's funny is that we can extend all these approaches to much bigger spaces in a sense.",
                    "label": 0
                },
                {
                    "sent": "So I said we're in high dimension, but let's go to even higher dimensions now.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is something I think Austin already showed, so sometimes you know I say let's do linear models, but sometimes in our models are not.",
                    "label": 0
                },
                {
                    "sent": "You know, even even if you regularize them, they are not good like here.",
                    "label": 0
                },
                {
                    "sent": "Don't want to ever earlier model.",
                    "label": 0
                },
                {
                    "sent": "How to talk with that well?",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has been said before, a classical ideas to redefine the features to change the space.",
                    "label": 0
                },
                {
                    "sent": "So for example here if you have to coordinate this one and X2 if you define new features which are is 1 square an is to square this picture on the left becomes the picture on the right.",
                    "label": 0
                },
                {
                    "sent": "So suddenly you can separate the the white from the black with the line and intuitively you could say, well, let's do SVM or just information on the right.",
                    "label": 0
                },
                {
                    "sent": "It will learn a line linear model here.",
                    "label": 0
                },
                {
                    "sent": "And this in Amador, here in the original space is a circle.",
                    "label": 0
                },
                {
                    "sent": "This is relative to what I said earlier when I said in high dimension form, either in a model can be very complicated.",
                    "label": 0
                },
                {
                    "sent": "It's a bit decided that here it's a line, but it could be a bit more complicated in what you see.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's the link between sorry?",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was the link between this and what I said was so follow it?",
                    "label": 0
                },
                {
                    "sent": "I said, let's run a linear model.",
                    "label": 0
                },
                {
                    "sent": "We will try to see how to extend all these.",
                    "label": 0
                },
                {
                    "sent": "L2 penalize so you know when you penalized by the wider here norm?",
                    "label": 0
                },
                {
                    "sent": "How how to do that in this space and we see that there is a trick that allows you to learn all of these Ridge regression or logistic regression or ACM in this space with various simple computation.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So probably again I purchased for for the people familiar with it.",
                    "label": 0
                },
                {
                    "sent": "It's not a scoop for them, but we use the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So what is the kernel?",
                    "label": 0
                },
                {
                    "sent": "So when we have this way to map a space to some other space, you know a point X is mapped to a .5 weeks.",
                    "label": 0
                },
                {
                    "sent": "We define a function K we call the kernel as the inner product.",
                    "label": 0
                },
                {
                    "sent": "On the on the on the right hand space.",
                    "label": 0
                },
                {
                    "sent": "OK so the kernel between Easton is Primesense farm.",
                    "label": 0
                },
                {
                    "sent": "They live in the original spaces.",
                    "label": 0
                },
                {
                    "sent": "Are your data, your profiles, or whatever you might do some space, and then when you take the inner product, you get a function which we call a kernel.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason why so?",
                    "label": 0
                },
                {
                    "sent": "I chose to to follow the.",
                    "label": 0
                },
                {
                    "sent": "You know the more mathematics mathematical way to present it.",
                    "label": 0
                },
                {
                    "sent": "The reason why kernels are useful is that there is a theorem called the representative theorem.",
                    "label": 0
                },
                {
                    "sent": "Which has been proved by several persons, but in particular burnout and others, which in a sense tells you that.",
                    "label": 0
                },
                {
                    "sent": "When you so up to now I was saying let's look at linear models that are obtained by minimizing.",
                    "label": 0
                },
                {
                    "sent": "A risk penalized by L2 norm OK.",
                    "label": 0
                },
                {
                    "sent": "Suppose you run this this algorithm in the space on the right.",
                    "label": 0
                },
                {
                    "sent": "So in the feature space, the space of five.",
                    "label": 1
                },
                {
                    "sent": "Then the theorem tells you that when you will run it, it could you know mathematically there will be a solution.",
                    "label": 0
                },
                {
                    "sent": "Then it tells you that the solution that will find it's a linear model on the right.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can write it as beta times 5X.",
                    "label": 0
                },
                {
                    "sent": "It's a linear model in the space of five, but also you can always write it as an expansion over the kernels.",
                    "label": 0
                },
                {
                    "sent": "Centered on the training points.",
                    "label": 0
                },
                {
                    "sent": "So this is this fundamental equation where the theorem says that the solution of this problem.",
                    "label": 0
                },
                {
                    "sent": "So the function you want to learn at the end will always be written.",
                    "label": 0
                },
                {
                    "sent": "You can always write it as a sum for over the training points, so you have 10 points initially valued.",
                    "label": 0
                },
                {
                    "sent": "The samples that you can always write the solution as a sum of Alpha times the kernel.",
                    "label": 1
                },
                {
                    "sent": "So the Colonel was the inner product between Zion X, which is the point where you want to make your prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, and so if you know that the solution has this shape, you know what you have changed that instead of looking for beta.",
                    "label": 0
                },
                {
                    "sent": "Beta was a vector that lived in the space of five.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking for bitter now you will look for Alpha.",
                    "label": 0
                },
                {
                    "sent": "It's a different way to Parameterise F An how to find Alpha?",
                    "label": 0
                },
                {
                    "sent": "Well, you can just replace F everywhere here by the corresponding Alpha expression, and in fact you will get a new optimization problem, which is exactly the same as the first one.",
                    "label": 0
                },
                {
                    "sent": "But in terms of Alpha.",
                    "label": 0
                },
                {
                    "sent": "Out of beta.",
                    "label": 0
                },
                {
                    "sent": "OK, so the theorem tells you that instead of working in the space of beta, you can work in the space of Alpha.",
                    "label": 0
                },
                {
                    "sent": "I will not prove the perhaps I will.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I will very quickly prove the theorem just to show you that this is a. I mean it, this is a theorem that has huge impact, but which is very.",
                    "label": 0
                },
                {
                    "sent": "I will give you some idea of how on how to prove it.",
                    "label": 0
                },
                {
                    "sent": "It's quite it's quite easy.",
                    "label": 0
                },
                {
                    "sent": "The idea is just.",
                    "label": 0
                },
                {
                    "sent": "So what we want to prove again is that F can be written this way.",
                    "label": 0
                },
                {
                    "sent": "In fact, what we will prove is that beta.",
                    "label": 0
                },
                {
                    "sent": "So the beta that you want to run the solution can be written as a sum of Alpha I5 XIA.",
                    "label": 0
                },
                {
                    "sent": "OK, for I = 1 to N. So if you can prove that, then you can prove that theorem becausw F beta would be beta times beta scarifier Vicks.",
                    "label": 0
                },
                {
                    "sent": "So it will be the sum of Alpha Phi of inside times 5 weeks and this would be the kernel which is this equation here?",
                    "label": 0
                },
                {
                    "sent": "OK, So what we want to show is that any solution of the problem on the top can be expanded leaves in the span of the training points if you want so geometrically what happens?",
                    "label": 0
                },
                {
                    "sent": "You know you live in the space and you have fireworks one somewhere.",
                    "label": 0
                },
                {
                    "sent": "This is your first training point.",
                    "label": 0
                },
                {
                    "sent": "Mapped the space you have firefix to somewhere else.",
                    "label": 0
                },
                {
                    "sent": "And you have all the end entry points that live in the space.",
                    "label": 0
                },
                {
                    "sent": "So what you can draw from this is the linear span of this point.",
                    "label": 0
                },
                {
                    "sent": "So I will because I have limited ability to write in high dimension, I will just assume you have two points and we live in 3D.",
                    "label": 0
                },
                {
                    "sent": "So these two points they you know they span a planner.",
                    "label": 0
                },
                {
                    "sent": "Central at the origin.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the of the subspace, which is a set of linear combinations of the of the training points, what you want to show is that the solution.",
                    "label": 0
                },
                {
                    "sent": "So the R rated Beta had like the solution of the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "What you want to show is that the solution is in this space OK, which which has dimension maximum N as opposed to being outside of the space.",
                    "label": 0
                },
                {
                    "sent": "How to prove it?",
                    "label": 0
                },
                {
                    "sent": "It's just.",
                    "label": 0
                },
                {
                    "sent": "If you look at any vector orbital, let's look at this bitter here that could be outside of the space.",
                    "label": 0
                },
                {
                    "sent": "This vector beta.",
                    "label": 0
                },
                {
                    "sent": "You can always the composite.",
                    "label": 0
                },
                {
                    "sent": "You know you can make it orthogonal projection.",
                    "label": 0
                },
                {
                    "sent": "To the linear span of the training points, and you can always write beta as a component.",
                    "label": 0
                },
                {
                    "sent": "Which belongs to the spine.",
                    "label": 0
                },
                {
                    "sent": "So let's write it bitter S plus a component here, which is orthogonal bit after gonal.",
                    "label": 0
                },
                {
                    "sent": "OK, any vector beta you can decompose it as its projection plus the the compliment OK and the goal is to show that if beta minimizes the top function then the orthogonal component is reduced to 0.",
                    "label": 0
                },
                {
                    "sent": "That would mean that beta is in the linear span.",
                    "label": 0
                },
                {
                    "sent": "Therefore that beta can be expanded this way.",
                    "label": 0
                },
                {
                    "sent": "Therefore that F can explain it this way.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you write it, it's almost done because if you look at the objective function, let's look at the second term first.",
                    "label": 0
                },
                {
                    "sent": "What you see is that what we will show is that this this guy so beta S which is the projection of bitter internal space is always better than beetle in terms of the objective function.",
                    "label": 0
                },
                {
                    "sent": "OK, beta it would be a candidate.",
                    "label": 0
                },
                {
                    "sent": "Then we say that when you project it, this new guy will always have a lower objective function.",
                    "label": 0
                },
                {
                    "sent": "So if you look for example at the second term, the second time was the square norm of beta.",
                    "label": 0
                },
                {
                    "sent": "It's clear that this guy is smaller than than beta because you have beta.",
                    "label": 0
                },
                {
                    "sent": "You project it so you reduce the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so the L2 norm of BTS is smaller than the autonomy of beta.",
                    "label": 0
                },
                {
                    "sent": "So in terms of objective function BTS who would be better than data if there was just the norm, it is better to be small and what you see two is that if you look at the first term so the first time is more complicated is the loss of the training sample.",
                    "label": 0
                },
                {
                    "sent": "But to define the loss you need to compute the value of F beta on this point is 1 and X2 and F beta is the inner product between beta.",
                    "label": 0
                },
                {
                    "sent": "And fire of X1 orbiter in five weeks two and here you see that the inner product cause.",
                    "label": 0
                },
                {
                    "sent": "The you know product between beta and each of these guy in fact is equal to the inner product between beta S and each of this guy because by definition the compliment is orthogonal.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm sorry I probably lost the majority of the room, but the conclusion is that you can show that the objective function is always better here than here, and therefore that the optimum has to be here.",
                    "label": 0
                },
                {
                    "sent": "And therefore that the solution data had, even though the space could be infinite dimensional, you know that the solution is always in this small space, whose dimension is just the number of points.",
                    "label": 0
                },
                {
                    "sent": "It's N. OK, so it's a way to work in a possibly huge space whatever the dimension, you know that you can write the.",
                    "label": 0
                },
                {
                    "sent": "The learning problem in high dimension, but still the solution will be inspired by tension.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the same thing, but returned with the equations.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so let's let's let's do it so for example, what what?",
                    "label": 0
                },
                {
                    "sent": "What does it mean for Ridge regression?",
                    "label": 0
                },
                {
                    "sent": "I said Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "It's always the same.",
                    "label": 0
                },
                {
                    "sent": "You know, we minimize the squared error penalized by Elton.",
                    "label": 0
                },
                {
                    "sent": "Also, earlier I gave the solution, we can write the solution.",
                    "label": 0
                },
                {
                    "sent": "Let's try to write it again in a different way.",
                    "label": 0
                },
                {
                    "sent": "We know that the solution here the function can be expanded in terms of Alpha and if we want to find the Alpha, we just need to plug back the Alpha in these equations.",
                    "label": 0
                },
                {
                    "sent": "So you replace F of excited by the sum of Alpha K of XJ, XI cetera.",
                    "label": 0
                },
                {
                    "sent": "Then you have any question in Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course I will skip the details.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you get, you know this kind of equation in red that is the same objective function but parameterized in Alpha instead of beta.",
                    "label": 0
                },
                {
                    "sent": "This one is again quadratic function that you can minimize.",
                    "label": 0
                },
                {
                    "sent": "You compute the gradient to set the gradient to zero.",
                    "label": 0
                },
                {
                    "sent": "You need to take.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of inverting matrices etc.",
                    "label": 0
                },
                {
                    "sent": "But at the end.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you get is that so it's the same function as before.",
                    "label": 0
                },
                {
                    "sent": "In order earlier I wrote the equation for beta, so beta was its transpose person, die cetera.",
                    "label": 0
                },
                {
                    "sent": "Now the same equation gives you that F. So the linear function can be written as a sum of Alfie kernel between Scion X.",
                    "label": 0
                },
                {
                    "sent": "That's rather case with Alpha given this way.",
                    "label": 0
                },
                {
                    "sent": "So you again have an explicit solution in Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so in practice you know if you have to do a regression in dimension 10.",
                    "label": 0
                },
                {
                    "sent": "You can either use the previous equation that gives you beta or you can use this equation that gives you Alpha.",
                    "label": 0
                },
                {
                    "sent": "There we go inside they are the same, the same of the seminar function.",
                    "label": 0
                },
                {
                    "sent": "So it seems to be not very interesting, but it becomes interesting.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the cases and possibly.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in the cases where you work, so let's look.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it is interesting.",
                    "label": 0
                },
                {
                    "sent": "It's interesting in cases where the first approach cannot be applied, like if you're in very, very high dimension, you don't want to.",
                    "label": 0
                },
                {
                    "sent": "You know, right?",
                    "label": 0
                },
                {
                    "sent": "Your mattress with millions or billions or infinite number of rows and columns.",
                    "label": 0
                },
                {
                    "sent": "You know if you work in infinite dimension, there is no way you can write your vector beta.",
                    "label": 0
                },
                {
                    "sent": "But this guy you can always write it because its dimension is number of points.",
                    "label": 0
                },
                {
                    "sent": "So this is this is called the Jewel way to do Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "When you do that, you estimate the same function, but you just estimate numbers.",
                    "label": 0
                },
                {
                    "sent": "Which are the weights of each training point as opposed to estimating the vector explicitly in very high dimension?",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do the same for.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So other things, but now what's the link between this notion and channels?",
                    "label": 0
                },
                {
                    "sent": "Well, the link is that, so something in.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see here again is that here you see there is no fly anymore.",
                    "label": 0
                },
                {
                    "sent": "I said you have each day are not too too space fire VIX Anfi is connected to a channel with the inner product and to find the Alpha you see there just into the kernel.",
                    "label": 0
                },
                {
                    "sent": "So it becomes interesting if you want to work in a space where the file can be very complicated, infinite dimensional, but you cannot.",
                    "label": 0
                },
                {
                    "sent": "K is easy to compute and there are many such cases.",
                    "label": 0
                },
                {
                    "sent": "This is what's called the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "You can express very complicated inner product by simple functions sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an example this this one like if you if you map a point with.",
                    "label": 0
                },
                {
                    "sent": "In polynomials you can see that when you take your points here, you take the inner product and you square it.",
                    "label": 0
                },
                {
                    "sent": "Then it's called the kernel because it corresponds to the inner product on the right hand side on the other space.",
                    "label": 0
                },
                {
                    "sent": "So this is a way to go from a inner product in a small space to some inner product in a slightly higher space, and for example.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you know if you take some inner product and you take it to some power then it becomes some inner product.",
                    "label": 0
                },
                {
                    "sent": "But in a space which is even higher.",
                    "label": 0
                },
                {
                    "sent": "So when you increase the dimension here, it means you're able to compute at no cost because you know taking the power of a number is something you can do easily on your computer.",
                    "label": 0
                },
                {
                    "sent": "It corresponds to work in some space that was dimension increases.",
                    "label": 0
                },
                {
                    "sent": "So this means that in this case it's better to do this Ridge regression or any other kind of methods in the space of Alpha.",
                    "label": 0
                },
                {
                    "sent": "But just using the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is to, you know, thinking of what are the So what can I use as a kernel function?",
                    "label": 0
                },
                {
                    "sent": "I said perhaps you can take the inner product to some power.",
                    "label": 0
                },
                {
                    "sent": "It's it's a kernel, so there is a natural question and I will go quick on that, which is what are the functions K. That correspond to inner products because where I say that as soon as you have function K to compare two points, that is an inner product, then you can plug this K into my formulas and you can estimate a Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "Or I didn't say I said it quickly, but you can also do logistic regression as VMS thanks to the representative theorem.",
                    "label": 0
                },
                {
                    "sent": "So you can do everything if you have a kennel case.",
                    "label": 0
                },
                {
                    "sent": "So natural question is.",
                    "label": 0
                },
                {
                    "sent": "Let's define a function K as a kernel.",
                    "label": 0
                },
                {
                    "sent": "If it's an inner product.",
                    "label": 1
                },
                {
                    "sent": "So by the way, this is exactly the same notion as the notion of covariance that was presented this morning, and the question is, can we characterize the function K?",
                    "label": 0
                },
                {
                    "sent": "Or can we list the functions which are kernels?",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, we can.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is again an important theorems that what that was proof some some time ago that characterizes.",
                    "label": 0
                },
                {
                    "sent": "So that gives another description, exactly the function K which you can use OK was proved by Aaron John and it says that a function K. So if you take a function K of XX prime, it's an inner product.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's OK now if and only if it has a property that it's what's called positive definite.",
                    "label": 0
                },
                {
                    "sent": "So what is positive definite?",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Positive definite There is an ugly way to write it this way, but is just a function K such that when you have points you take any points, then you can apply your function to all the pair of points you obtain a matrix of kennel values.",
                    "label": 0
                },
                {
                    "sent": "These metrics always needs to be symmetric, so the function is to be symmetric and these metrics should not have negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "It's called some positive semi definite OK, so it's a bit abstract if you're not familiar with this notion of.",
                    "label": 0
                },
                {
                    "sent": "I can raise this issue, but at least we can characterize them, it's.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So generate function that that explains what are the functions that can be used as kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you want to look at the.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the theorem it's in the slides.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then now we can try to, you know, we can list what could be interesting function.",
                    "label": 0
                },
                {
                    "sent": "So it is very relative to the covariances.",
                    "label": 0
                },
                {
                    "sent": "What can we describe?",
                    "label": 0
                },
                {
                    "sent": "Some functions which you can use as kernels and find many of them are.",
                    "label": 0
                },
                {
                    "sent": "You may be familiar with them, so I already mentioned the polynomial kernels, meaning that if you have vectors you take the inner product, take them to some power, then you have a kernel this morning.",
                    "label": 0
                },
                {
                    "sent": "So I call it the Goshen RBF and not the exponentiality.",
                    "label": 0
                },
                {
                    "sent": "The quadratic something.",
                    "label": 0
                },
                {
                    "sent": "But it's the same, so it's not normalized.",
                    "label": 0
                },
                {
                    "sent": "If you take this Gaussian function.",
                    "label": 0
                },
                {
                    "sent": "Sorry, it's positive definite.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not obvious, and in fact.",
                    "label": 0
                },
                {
                    "sent": "There is an exercise for you.",
                    "label": 0
                },
                {
                    "sent": "You can write it at some inner product, meaning that these apparently natural function you can write it as fire waste times 5X prime in some space.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case the space is a bit abstract and it in fact it's infinite dimensional, so it's not a it's not 345 dimensions, it's been.",
                    "label": 0
                },
                {
                    "sent": "Dimension is called a Hilbert space, but it's an inner product and therefore.",
                    "label": 0
                },
                {
                    "sent": "All that I say, like Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "You can apply it in the feature space of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Gaussian kernel an gets Ridge regression with Goshen Camel.",
                    "label": 0
                },
                {
                    "sent": "Laplace Kernel is a bit the same without the square, but it has a very different properties at the end.",
                    "label": 0
                },
                {
                    "sent": "Another one that that's funny, so we also saw it this morning, but in a different approach is when you have two #2 positive numbers, so the rest one in fact is very used in image processing.",
                    "label": 0
                },
                {
                    "sent": "For example when you when you want to.",
                    "label": 0
                },
                {
                    "sent": "When you have images and you describe an image or Patch by a histogram of colors, you need to compare histograms and histograms are positive numbers, and when you when you want to compare, attend to.",
                    "label": 0
                },
                {
                    "sent": "So one histogram with standing the other 118.",
                    "label": 0
                },
                {
                    "sent": "How do you compare them?",
                    "label": 0
                },
                {
                    "sent": "While in the case of histograms, of course, the one of the best way to compare them is to take the smallest of the two the mean.",
                    "label": 0
                },
                {
                    "sent": "So the mean between 10 and 18 is 10.",
                    "label": 0
                },
                {
                    "sent": "This is a this is a Colonel.",
                    "label": 0
                },
                {
                    "sent": "It's an inner product.",
                    "label": 0
                },
                {
                    "sent": "I'll leave it to you to check how you can write it as some some inner product, but this kind of thing as soon as you know it's positive definite, you can plug it so you can.",
                    "label": 0
                },
                {
                    "sent": "You know in the Ridge regression or SVM equation you take the mean of the two numbers.",
                    "label": 0
                },
                {
                    "sent": "You need to estimate a function and you have you have learned a model.",
                    "label": 0
                },
                {
                    "sent": "OK, so so the list of course is not finished.",
                    "label": 0
                },
                {
                    "sent": "I mean there is, there is no end, but these are some that are widely used antique, so this one is just less common in the implementations, but it's used by some people, but this is typically where you know when you run a SVM in R and in Python whatsoever you choose the kernel, you need to choose this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So that the different one is just to take the sum of the mean, but the other one is is just a problem.",
                    "label": 0
                },
                {
                    "sent": "Did you work on that?",
                    "label": 0
                },
                {
                    "sent": "So you take there is something that the mean of other Max, which is to combine the means and the Mac season.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what kind of things do you learn?",
                    "label": 0
                },
                {
                    "sent": "Well, you see I said these are linear models, but when you make a linear model in the space of the Gaussian kernel, it's linear.",
                    "label": 0
                },
                {
                    "sent": "But when you look at it in 2D, this is the kind of thing you can learn 'cause you know, linear model Representative theorem tells you that the linear model in the high dimensional space estimates the function and the function.",
                    "label": 0
                },
                {
                    "sent": "You can represent it by your some of Alpha I times the kernel.",
                    "label": 0
                },
                {
                    "sent": "So if you use the Gaussian kernel, it means that what you do in practice is you learn.",
                    "label": 0
                },
                {
                    "sent": "A combination of kennel, you know, some of Goshen, Goshen, Colonel, centered on the 20 point and what you do is optimize the Alpha.",
                    "label": 0
                },
                {
                    "sent": "The Alpha is what we learn in the in the function, and so you can imagine that a sum of Gaussians can take this kind of shape, for example.",
                    "label": 0
                },
                {
                    "sent": "So This is why I was saying that this is typically a linear model in high dimensional space space, and this one is smooth 'cause we have constrained it to have a smaller normal small Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "If you don't control the norm in the highly mental space, you can get with Gaussian functions.",
                    "label": 0
                },
                {
                    "sent": "We can get any any function.",
                    "label": 0
                },
                {
                    "sent": "OK, so probably this is a good way.",
                    "label": 0
                },
                {
                    "sent": "I mean, one good way to look at high dimension is these are linear functions and the norm is the smoothness.",
                    "label": 0
                },
                {
                    "sent": "The norm of the vector beta is the smoothness of this function.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So perhaps I will just finish with a few.",
                    "label": 0
                },
                {
                    "sent": "Two thoughts and examples of color.",
                    "label": 0
                },
                {
                    "sent": "Because you know what I say for the moment is there are many can also you can use them.",
                    "label": 0
                },
                {
                    "sent": "You can affirm.",
                    "label": 0
                },
                {
                    "sent": "You can learn all these things.",
                    "label": 0
                },
                {
                    "sent": "One of the key question is how do you choose a kernel?",
                    "label": 0
                },
                {
                    "sent": "And perhaps how do you design A kernel?",
                    "label": 0
                },
                {
                    "sent": "And this is related and I said initially, let's let's constrain the beta to be in this fear and this is to learn in high dimension.",
                    "label": 0
                },
                {
                    "sent": "But the second thing was if we have some prior knowledge of where the true model R, how can we not take this fear but this ellipse?",
                    "label": 0
                },
                {
                    "sent": "That will go closer to the to the true model, so this notion of... actually with annotation.",
                    "label": 0
                },
                {
                    "sent": "This would be how to design A penalty such that there are good functions in the penalty.",
                    "label": 0
                },
                {
                    "sent": "And now we said, let's if we just take for penalties the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "The only thing we can change is the choice of Colonel.",
                    "label": 0
                },
                {
                    "sent": "So the kernel defines the feature space and therefore the Altoona me cetera.",
                    "label": 0
                },
                {
                    "sent": "So for us the kernel is where we can put prior knowledge where we could try to, you know, put knowledge to decrease by us and control variance etc.",
                    "label": 0
                },
                {
                    "sent": "So how do you make a kernel?",
                    "label": 0
                },
                {
                    "sent": "You can take one of the list and try them.",
                    "label": 0
                },
                {
                    "sent": "I was just mentioned 3 three other possibilities or first, I don't really know, but at least I know how to make some of them.",
                    "label": 0
                },
                {
                    "sent": "Probably some useful principles to make sure no one would be to design features because you know, I said kernel is some inner product space or features and you work in this space.",
                    "label": 0
                },
                {
                    "sent": "So if a predator you know what are the important features, just take them and this is your feature space.",
                    "label": 0
                },
                {
                    "sent": "OK, so you could make your features and then learn a model.",
                    "label": 0
                },
                {
                    "sent": "Nice pictures.",
                    "label": 0
                },
                {
                    "sent": "A second way to do is.",
                    "label": 0
                },
                {
                    "sent": "And I found it very useful in applications.",
                    "label": 0
                },
                {
                    "sent": "I will give one example is you know to look at not really in terms of features, but to look at the distance it defines.",
                    "label": 0
                },
                {
                    "sent": "Because what's important at the end is you define a geometry, new points, you have your points in your space you want to manage to some other space and you will learn something that will be smooth there.",
                    "label": 0
                },
                {
                    "sent": "So if you have some prior knowledge of what are the similarities or distances between how we should measure similarity between two samples or between two molecules, you may directly define a distance, then translate the distance to a kernel and then feed the kernel to the machine.",
                    "label": 0
                },
                {
                    "sent": "Will do that and the last one.",
                    "label": 0
                },
                {
                    "sent": "OK I will I will I will.",
                    "label": 0
                },
                {
                    "sent": "In one in one minute I will explain it is is a designer regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jessica, let's be concrete.",
                    "label": 0
                },
                {
                    "sent": "So the first one is this.",
                    "label": 0
                },
                {
                    "sent": "Any feature, so I think this is a custom show.",
                    "label": 0
                },
                {
                    "sent": "The first day you know civil.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you remember, but it talked about the molecules and graphs and graph kernels and how when you have a graph you can represent it as a vector of features and then sometimes you can make you can out of it.",
                    "label": 0
                },
                {
                    "sent": "This has been done and this is a typical example where if you want to.",
                    "label": 0
                },
                {
                    "sent": "So if you want to make a kernel between molecules, one way to do is what are the features.",
                    "label": 0
                },
                {
                    "sent": "I want to have in my future.",
                    "label": 1
                },
                {
                    "sent": "Face OK and then the only added value of having a kernel is that perhaps you don't need to enumerate all the features, but first you can find a formula to directly compute inner product, but the starting point would be what other features I want.",
                    "label": 0
                },
                {
                    "sent": "So 11 example.",
                    "label": 0
                },
                {
                    "sent": "One famous example of graph kernel is when you have graphs G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "I give you the the magic formula and then explain what it is.",
                    "label": 0
                },
                {
                    "sent": "The magic formula is when when when you give Me 2 graphs you make what's called a product graph.",
                    "label": 0
                },
                {
                    "sent": "From the picture you can guess how it's made is just.",
                    "label": 0
                },
                {
                    "sent": "You know combining the two graphs together to make a graph G 1 * G Two this this graph has some adjacency matrix a while you just some you take the mattress to the power N and you sum all the elements.",
                    "label": 0
                },
                {
                    "sent": "This gives you a number.",
                    "label": 0
                },
                {
                    "sent": "This is accountable.",
                    "label": 0
                },
                {
                    "sent": "OK so in terms of algorithm, this is how you would implement it, but why so?",
                    "label": 0
                },
                {
                    "sent": "It's not needed it it's a.",
                    "label": 0
                },
                {
                    "sent": "Probably from the paper the first papers of Geithner and Cash IMA so this is the end formula.",
                    "label": 0
                },
                {
                    "sent": "But when you do that, the reason to do that is that you can show that in this case you make a feature space which is quite large, which for each graph it strikes all the walks of length N, meaning that when you have a graph you could you imagine you walk on it and when you walk on it, in this case it could be a blue, yellow, yellow, yellow, blue, yellow, blue.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "This would be one dimension of your feature space and what these ugly formula so very efficient but strange formula doors is computing this inner product.",
                    "label": 0
                },
                {
                    "sent": "This space.",
                    "label": 0
                },
                {
                    "sent": "OK, so take your messages that there are many magic formula in kernels 1st and 2nd.",
                    "label": 0
                },
                {
                    "sent": "In this case the way to derive this magic formula was to think of what should be.",
                    "label": 0
                },
                {
                    "sent": "How do we represent a molecule as a vector and then find the trick to compute the inner product.",
                    "label": 0
                },
                {
                    "sent": "Here that's the 1st way to make a kernel.",
                    "label": 1
                },
                {
                    "sent": "I'm sorry, I I Russia bit.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd way is you you work with colleagues or you come from your domain and you know if you ask your colleague when you have two protein sequences, how you compare them.",
                    "label": 0
                },
                {
                    "sent": "Well they never heard about kernels but I know how to measure similarity between proteins.",
                    "label": 0
                },
                {
                    "sent": "You know you need to align them and once you align them you have a score.",
                    "label": 0
                },
                {
                    "sent": "the Smith Waterman scored that computes try to align them, penalizes the gaps, compares them University cetera.",
                    "label": 0
                },
                {
                    "sent": "So in bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "So if you want to design A kernel for protein sequences.",
                    "label": 0
                },
                {
                    "sent": "It will be hard to communicate if it's very different from what everybody uses in terms of of distances like it seems that the defacto distance used in this domain is related to aligning the sequences.",
                    "label": 0
                },
                {
                    "sent": "So one approach to make you cannot in such a situation would be to say, well, if these guys if all the Community uses this distance there may be some reasons is because you know over the years they have found that it's a good way to measure the evolutionary distance and therefore it is a nice notion of distance to to infer too general.",
                    "label": 0
                },
                {
                    "sent": "But in this case, the only problem is that the sort of score using informatics is not a kernel, it's not positive, so it's not a kernel, meaning that it's not an inner product, so it creates difficulties.",
                    "label": 0
                },
                {
                    "sent": "So one way you know if you're in this situation, perhaps you can try to think of it and find some variance in this case.",
                    "label": 0
                },
                {
                    "sent": "But there is a function which is a bit ugly, but you can compute efficiently so that this is a function that borrows the concepts of aligning the sequences.",
                    "label": 0
                },
                {
                    "sent": "So again you try to align them.",
                    "label": 0
                },
                {
                    "sent": "You scored the alignment, the only difference is that instead of taking the best alignment, you sum all the alignments, weighted by how good they are, and you obtain your number and at the end is number is positive definite, meaning that you can directly use this formula, plug it to your kernel function and learn over the space with geometry is defined by the by the expert knowledge.",
                    "label": 0
                },
                {
                    "sent": "So my experience is that this kind of approaches could be very successful because again expressing your domain know what they do and if they use a distance, you know how to measure distance between samples or molecular whatsoever.",
                    "label": 0
                },
                {
                    "sent": "Often they do it, because empirically they found it's good to describe the geometry.",
                    "label": 0
                },
                {
                    "sent": "So if you want to scale method, you just need to tweak a bit, and sometimes you can even not wreck it and use it directly.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, finally and this will be the end of for today another way which which we will use a tomorrow to make your kernel is not too thinking there's a feature space, nor in terms of distance, but to say you know when I use a kernel because I want to learn a function which is a linear function, I know that the penalty I use is the L2 norm of beta or worse, but sometimes this altinum of beta can be expressed directly in terms of something, some property of the function F of the renal function.",
                    "label": 0
                },
                {
                    "sent": "And for example.",
                    "label": 0
                },
                {
                    "sent": "If you take the Gaussian kernel, probably the best way to understand it is not in terms of features etc or distance.",
                    "label": 0
                },
                {
                    "sent": "It's a bit strange, but one way to understand is that if you use this kernel, you can show that the L2 norm of beta, so bitter lives in your space you cannot see.",
                    "label": 0
                },
                {
                    "sent": "You cannot imagine but the Altima Vitacca response to something in terms of function F. Is this again a great question, but the take home message is that when you have a function F, the F hat is the four year transform.",
                    "label": 0
                },
                {
                    "sent": "So you have a function.",
                    "label": 0
                },
                {
                    "sent": "And if you want to to evaluate the penalty, you just map it to the Fourier domain to see where the energy of your function and what you do is that you compute the norm in the free domain.",
                    "label": 0
                },
                {
                    "sent": "But by penalizing the frequency, meaning that if you have a function with a lot of energy at high frequency, the penalty would be very higher.",
                    "label": 0
                },
                {
                    "sent": "OK, so you could directly start from that and say I want to learn a function and I know you want it has to be smooth.",
                    "label": 0
                },
                {
                    "sent": "For me smoothness means that the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "Does not have a lot of energy at high frequency.",
                    "label": 0
                },
                {
                    "sent": "Therefore I could define directly this penalty an if I define this penalty, I can deduce what is the equation of the kernel that would correspond to this penalty.",
                    "label": 0
                },
                {
                    "sent": "Another example, by the way, I said earlier that the mean kernel is is a kernel, and it's widely used if you use them in kernel, you can interpret it in terms of penalty and corresponds is what's called the Sobolev norm is just mean that when you have a function it qualifies the smoothness.",
                    "label": 0
                },
                {
                    "sent": "But just looking at its derivative so it penalizes functions which have a.",
                    "label": 0
                },
                {
                    "sent": "Uh huh that very quickly because the rate is very high, so this is an example where it's a bit strange to think in terms of this explicit equation of what does a kernel.",
                    "label": 0
                },
                {
                    "sent": "But when you look at that, you know what you're doing.",
                    "label": 0
                },
                {
                    "sent": "What you're doing is that you you infer, function the function it will infer.",
                    "label": 0
                },
                {
                    "sent": "You control its complicity by controlling the L2 norm of its derivative.",
                    "label": 0
                },
                {
                    "sent": "So at least this would be so again, one way to make channel then, would be, say, what kind of constraint do I want to have on the function F?",
                    "label": 0
                },
                {
                    "sent": "And if it's quadratic right then you can derive a kernel that would do it.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sorry I was away too longer.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I did not go as far as I wanted, but at least I finished the main thing on Colonel so I don't know if we have time for questions or if we should.",
                    "label": 0
                }
            ]
        }
    }
}