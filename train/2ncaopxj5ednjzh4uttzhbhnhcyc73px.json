{
    "id": "2ncaopxj5ednjzh4uttzhbhnhcyc73px",
    "title": "Bayesian Optimization in a Billion Dimensions via Random Embeddings",
    "info": {
        "author": [
            "Nando de Freitas, Department of Computer Science, University of Oxford"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Decision Support"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_de_freitas_random_embeddings/",
    "segmentation": [
        [
            "Alright, so it's Bayesian optimization in high dimensions and actually high here can be as high as your computer can handle it, provided that intrinsic dimension is low, so this has to do with some of the things that Ramey talked to us about, so."
        ],
        [
            "So.",
            "Remy pretty much set up the problem for me, but just to rate a rate a rate with trying to do Bayesian optimization, and we're trying to maximize some black box function.",
            "So the assumption here is that I don't have.",
            "I don't necessarily have a close form expression for this function typically open.",
            "We're interested in cases where this function is expensive to evaluate, in which case I want to minimize the number of times I tried this function.",
            "And I will assume that there is this function has some constraints based typically box constraints, so it has some bounds.",
            "And.",
            "And it may or may not be that this function has derivatives.",
            "If I have access to the derivatives, I could exploit it derivatives to do better, but often this is derivative free optimization.",
            "So."
        ],
        [
            "Technique that I like to use for this is Bayesian optimization.",
            "An why Bayesian so basin?",
            "Because the function I'm trying to optimize is something that I don't know.",
            "In all I can get is samples of this function, so the idea then if I had a prior over the space of functions that I believe the true function comes from that I'm trying to optimize, I could do better.",
            "If I have a prior, if I do not have a prive, and we've sold lots of priors this morning, smoothness priors if I have an idea of what the smoothness about the function is, I can do better.",
            "Maybe I have some other types of assumptions about the function that will allow me to succeed if I have no knowledge whatsoever about the function and indeed the function said the characteristic function, so I have no hope of succeeding with this approach, but I believe in many practical problems we do know that the functions are.",
            "Smooth and so on.",
            "So in physical systems so we can actually profit from these assumptions.",
            "OK, So what are the elements?",
            "Let's assume that we've so far sampled the function at two points, so I'm going to do this here in one D and I'm going to do it in a continuous 1D space.",
            "Of course, if this applies to higher dimensions and you could apply to a mix a mix of both categorical and continuous variables.",
            "But for illustration purposes, let's think of the one the case.",
            "So I've sampled the true function at two points.",
            "The true function here is this dashed line.",
            "Which I don't know I just.",
            "Get to observed at where I pick a value of XI.",
            "Observe the value.",
            "But what I can do is if I had a prior over the function.",
            "In this case, the Gaussian process prior tells me something about the smoothness of the function.",
            "Then I could fit.",
            "A posterior model to these two observations, and so I get a posterior distribution telling me exactly what I expect that function to be.",
            "So I have in this case, I mean for what I expect a true function to be, and I have some confidence intervals.",
            "And here being based in so good idea because when you have very few points you want good confidence estimates.",
            "If I had if I was using frequentist techniques where it would fail is that I wouldn't get good confidence estimates, or at least I don't know how to get them.",
            "So then the question becomes where do we pick the next point?",
            "And let's assume that we have for now.",
            "This magic utility function, which often called the acquisition function and that acquisition function will tell us where to sample next and where to sample next.",
            "In this case is always sort of a tradeoff between mean and variance, so I've done very well with this point, 'cause it gives me a higher value of the objective, but on the other hand, there's these places where the variance is large, which.",
            "Or unexplored, so those who could also be good points to look at and then taking a tradeoff between these often will give me a utility function.",
            "Alot of other considerations come into the construction of the utility function, but for the time being, let's just think of exploit by going forward.",
            "The mean is high.",
            "Explore for where the variance is large.",
            "So in this case, at the next iteration, so suggest.",
            "At this point I tried a new point.",
            "Once I get a new point, I re fit the Gaussian process and I continue."
        ],
        [
            "Doing that and so very quickly, I zoom on on the optimum.",
            "So the algorithm is very simple, so at each round.",
            "I will optimize.",
            "I will find the best point X.",
            "So the X axis the best point by maximizing some utility function.",
            "This utility function is often if you have a close form expression for this.",
            "So for example, it could be the mean plus some factor times the variance as in the upper confidence bounds that we saw this morning.",
            "Typically this is optimized using any of the shelf algorithm, including direct for low, moderate dimensions, 68 dimensions offered, people use different types of discretisation.",
            "I'll come back to the question of how to optimize this late.",
            "And once you get a new point being recommended, you simply added to your data, set your set of observations found so far, and you fit the Gaussian process again.",
            "Obtain the mean, obtain the variance, and then maximize utility.",
            "As I've presented, you could use many ways of constructing this utility, and there's indeed we'll see those dozens of ways of doing this.",
            "Also.",
            "Dozens, probably more hundreds of ways of how to place a prior on functions and how to learn.",
            "So people typically use Gaussian processes, but I think there's much better things to do in many cases.",
            "If you have way too many observations and don't use a Gaussian process, you know use a parametric technique that's probably hand deals better with the number of points, or if you have lots of partitions, then maybe what you want to use is you want to use a random forest with Gaussian processes in the leaves.",
            "Or something like that, you know, so one shouldn't think.",
            "By being Bayesian we shouldn't think it should just use Gaussian processes, we just tend to use them 'cause they're easy to manipulate and the code is just two lines.",
            "But there's really a whole family of Bayesian models that we could use here."
        ],
        [
            "And here's an illustration in the case of minimization, which is very similar to what we saw this morning as well, where you sort of focus more and more where the optimum is and you don't visit most of the space."
        ],
        [
            "Now coming to the question of utility functions.",
            "There's many utility functions out there in the literature that they proposed in different fields.",
            "And the one thing that you find when you work with bandits and Bayesian optimization, and incidentally, I forgot to say that, but you could also think about this as a bandit problem, where you have infinite number of arms and all these arms are correlated.",
            "When you work with bandits and experimental design based on experiment design that you know these are sort of mark markedly different fields and so different people tend to come up with their own ways of solving their problems, and somehow they always call what the other field.",
            "Does Harris ticks and they call their own method of principled way and?",
            "If you really go into the literature, you'll see that there's a lot of tension here.",
            "And then.",
            "There's also a lot of ignorance about what the other people are doing, so they some communities might not realize, as some other communities care about cumulative loss as opposed to simple laws or all sorts of little details that actually matter a lot, but.",
            "Nonetheless, I think over the last few years we've seen a lot more people work in this area, so there's more consensus as to what are the problems and so on.",
            "Typical ways.",
            "That people have come up with.",
            "What's the probability that that I'm going to do better than the best value that I've seen so far?",
            "So there's something called a probability of improvement.",
            "It goes back to control Thierry.",
            "And if I were to use that utility for this friend thresholds, so you want to do.",
            "Better than the best that you've seen by Epsilon, and if I chose different epsilons then we start seeing different curves, so this is kind of like when we were looking at DO this morning.",
            "Expected improvement this came up by Mokas.",
            "Came up with this and he's the guy that actually coined the term Bayesian optimization.",
            "And he was using this to actually automatically configure algorithms way back in the day, but I think he was ahead of his time, didn't have all the computers that we have today.",
            "He come up with some very smart ways of doing this, and this particular notion.",
            "Expect the expectation that you will do better than the best point that you've tried so far, which would be the expectation that you do better than this.",
            "Often works pretty well in practice and for many applications it will work as well as these upper confidence bounds.",
            "And for some it worked.",
            "This also assuming same U of X. Yeah so that's another.",
            "Very cool.",
            "Oh mute.",
            "Oh, so mu is the mean that you estimate from data.",
            "MU is the mean that you estimate from data.",
            "Sigma is the variance.",
            "So for this you have close form expression, so it's the mean of the GP.",
            "Pardon.",
            "Or you assume that you're in the model class or that you have a reasonable estimate.",
            "Yeah, but isn't the error that you made.",
            "Process.",
            "Much bigger than these differences.",
            "So if your GP does not capture the function, the true function, then you do have a problem of mismatch.",
            "So I'm making the assumption that I am able to capture this function.",
            "Of course, if my function has a certain smoothness and I completely picked the wrong model for it.",
            "I'm not going to be suboptimal in that case.",
            "So we saw some examples this morning where revenue was showing is that if you pick the wrong smoothness, you can actually do poorly.",
            "But then, in addition to fitting the model properly, then you have to face the issue of which utility to use, and then there's Thompson sampling.",
            "So Thompson sampling with GPS would be basically given that this is what you have so far.",
            "Your model you would draw Gaussian.",
            "You would draw function from this.",
            "And then you find the optimum of that function.",
            "The highest point of that function, and that's the point that you would sample next an.",
            "That strategy actually works pretty well.",
            "In practice, probability matching is a limiting form of this.",
            "And there's really a huge other literature on these methods, and there's even portfolios of these things that stand on top."
        ],
        [
            "The intuition for why these methods work is very similar to the sort of intuition that Remy went over this morning, so.",
            "My student must have come up with a proof where he what he was looking at.",
            "If we assume that the true function here, the solid blue thing is a sample from a Gaussian process, and you basically assume that you can capture it if you know the smoothness.",
            "If you know its fourth derivatives and so on.",
            "Then we can play similar games to what we saw this morning.",
            "The Lipschitz Type games, except that this is sort of a soft probabilistic Lipschitz with high probability.",
            "I know that if.",
            "So for any.",
            "In any interview, I always note the lower confidence estimate and the upper confidence estimate.",
            "So whatever the let's call the highest of the lower confidence estimates at this point.",
            "So we're going to call it a Max.",
            "Lower confidence bound an I know that whatever the highest confidence bound is lower than the lowest confidence bound since the function is trapped.",
            "In this interval, the function cannot be possibly higher than this point, so that allows me to discard this huge part of the space immediately.",
            "So that's why we can do global optimization.",
            "With high probability, that's correct.",
            "Yeah.",
            "That's correct.",
            "So it's a soft.",
            "It's a probabilistic Lipschitz.",
            "Confidence always.",
            "Sample.",
            "In your example here, this.",
            "Maximum overload lower confidence interval is the point that you sample.",
            "So these are the points that I've sampled sampled this, this, this and this.",
            "So in this case I'm considering the terminal deterministic function, so then the highest will be this point that I've actually picked.",
            "Yeah, one of the points that are chosen so the condition is.",
            "Functions of that would be true, but otherwise it does not otherwise.",
            "So you would you would have to come up with slightly different argument.",
            "This argument is intuitive because it allows you, at least in the domestic case.",
            "It's very easy to see why you carve the space and so the question is so one of the things that we were, we've tried to study is as if you were to use a a grid of, say, looking at end points and we worked a half the grid to be half of the size.",
            "What would happen with this?",
            "The height of this variance?",
            "How quickly does the envelope go to 0?",
            "As we decrease the grid size and so the good news is that you can prove a result like this that the variance actually if the grid size is Delta, the variances.",
            "It's actually going is upper bound by Delta squared, so we have this exponentiation in the decrease of the Heights as we decrease the interval.",
            "So that's really nice, and that's what gives you this exponential rates on regret, so you very quickly can exponentially get to the optimal.",
            "Of course, if we when we prove this, we were using a grid which is not very efficient and we were helping the size.",
            "But if you were to use a trip rotation.",
            "And you were to use these assumptions.",
            "the GP assumptions.",
            "Then you could actually get the best of both worlds.",
            "You could get a more efficient algorithm that which so.",
            "We would have to do this, but I think the way the world would go through.",
            "How long is an expectation?",
            "Which is is it like a valuation?",
            "I mean, do you do an expectation over or is this for a given function?",
            "Or you do and expect it's for it's for this, um, yeah, yeah, that's correct.",
            "Assuming that the function.",
            "That you do this prior because you use it in the expectation of the regret which is typed.",
            "This prior is given by this, that's correct.",
            "Mention there was.",
            "This is the dimension, so in this case this is doing poorly with the dimension because I'm using a grid, I'm not being smart.",
            "There's another thing that I'm obviating here, and this is the problem of many of the proofs out there.",
            "They all assume that a digit duration you have maximized the expected improvement, or the UCB or whatever, and actually maximizing the acquisition function is an NP hard problem, so a lot of the proofs are conditional and you having solving an NP hard problem.",
            "If you dig in the literature, you'll see that.",
            "This is one exception, ESO.",
            "That's why I actually like the idea of trying to mix this with this.",
            "So 'cause that will get us away from sort of somewhat dodgy proofs."
        ],
        [
            "Which nonetheless have been extremely useful toward understanding the method.",
            "So."
        ],
        [
            "Let's essentially what Bayesian optimization is, and I was doing an introduction 'cause the next talk will be also be on this.",
            "So when Mark comes."
        ],
        [
            "Log because there's lots of applications to it.",
            "So one of the applications that I'm very interested in is this information extraction, so we've already seen that for news recommendation.",
            "So on this morning often you take part of speech you take.",
            "Named entity recognition, you take a bunch of her wrist sticks in order to construct the pipeline that recommends a set of features that describes the document.",
            "And when I work with organisations doing this recommendation, they often tend to do that, and so one of the things that I would like to do is to automate those pipeline so that when you get user feedback, the actual the whole information extraction pipeline gets better so.",
            "Even for things as simple as concept extraction, where you just basically try to understand that the words Brad Pitt and Angelina Jolie actually mean.",
            "Not four separate words, but it's one concept which is Brad Gelina, which we all know about.",
            "There's this paper, which is one of the best techniques out there for that, by actor Garcia Molina, which has 18 free parameters and a load of NLP.",
            "Information extraction techniques have lots of free parameters.",
            "And by using these techniques of Bayesian optimization where you just try some parameters, you look at the performance that it has out there.",
            "You can actually optimize this in this entire pipelines, and if you have several teams in your company all contributing a component of the software towards your product, you can actually optimize what the entire company is doing."
        ],
        [
            "That's one of the things I like about it.",
            "It's also very useful for just optimizing the algorithms that we already use in practice, so I just love working with Monte Carlo methods.",
            "Here's a history of applying different types of neural networks with different type of Monte Carlo methods and two very simple sort of classical data set of folks used to use.",
            "When I was doing my PhD so different people got different errors and there was a race to minimize this error, which involve all sorts of pretty much every technique that's out there in a machine learning book, Gaussian approximations, Laplace, reversible jump, HMC, HMC with automatic relevance that are relation.",
            "I spent three years in Cambridge to get this, and one of my students just took exactly the code of Radford nails and then just put it around it and optimizer that will try to minimize some criterion for performance of HMC and just by doing that he was able to do better than everyone else.",
            "So this took me 3 years.",
            "This took him and after.",
            "So that's a sort of scale of improvements in productivity for new users these techniques.",
            "An even when folks try to design adaptive algorithms that are very good, like nuts for Hyper Montecarlo which is very popular approach by Andrew Gelman by doing Bayesian optimization at least in several datasets we have found that we can actually do much better than these engineered adaptation schemes."
        ],
        [
            "And that allows us to attack all sorts of problems where we can do better than things like Swendsen Wang, which is a sort of classical method for these type of ferromagnetic models.",
            "So antiferromagnetic models.",
            "Pardon.",
            "Self avoiding random walk.",
            "It's one of the it's sort of like a hyper Montecarlo but for discrete state space is that we've come up with but it has too many free parameters and then the only way so one of the things that based on optimizations done for us.",
            "This algorithm design is in the setting is that we can now allow for the algorithms to have a lot more degrees of freedom.",
            "And then we have the optimization on top to automatically configure those degrees of freedom.",
            "'cause I would say based on my knowledge of trying methods for this creates the systems Monte Carlo methods that they give.",
            "Sample is probably the best, is just because anyone can code it in.",
            "It's simple and so on.",
            "If you want to have something more sophisticated it would be nice to have an automatic way of tuning that method to the problem.",
            "Foundational.",
            "I one of the things I've done is to change it with cross validation and we should discuss that offline.",
            "Because that in itself is something novel, but we've also June.",
            "You can tune it with any measure of performance that someone might think is a good way of measuring the performance of your Monte Carlo method.",
            "If you have a predictive task like you want to use your neural network to classify MNIST, then cross validation makes perfect sense."
        ],
        [
            "Um?",
            "Bayesian optimization is playing a huge role also in the world of analytics where there's a very nice tutorial by Steve Scott where you produce your not produce websites dynamically with different features and then to maximize the content.",
            "And of course here we actually have some people that actually work on this.",
            "And also for like I often consult for this company, can't agent and they what they have is they automatically configure your online games to maximize revenue for the advert for the game producers.",
            "And so all these are bandit type problems of basic black box optimization problems."
        ],
        [
            "It's very useful for interfaces whenever you deal with humans.",
            "05 minutes I'm going to speed up, so one of the things that really interests me is humans are actually very expensive black boxes.",
            "If I want to optimize something for a person, whether it's news recommendation.",
            "But even as we'll see different applications, then it's important to be able to figure out quickly with the human wants, not ask too many silly questions.",
            "So in one application we actually what we were trying to do is assist animator.",
            "So let's assume that animator.",
            "Wants to create a sphere that has this particular lighting.",
            "That animator might not know about differential equations and so on, and how to set all the parameters by hand, but that animator is very good at saying out of two options, which one she thinks looks more like with what she has in mind.",
            "Which is this.",
            "And using Bayesian optimization we can keep generating pairs.",
            "And what we do in this case is we fit with fitting the Gaussian process and the latent space of all the degrees of freedom of satyrs, beard for lighting conditions.",
            "And then we zoom in into finding exactly what the animator wants very quickly.",
            "So in this case we're doing we configuring an interface for the, for the animator, and we've used this for."
        ],
        [
            "Animation and so on.",
            "More applications, sensor networks.",
            "If you have a highway, this is sort of classical example of under ascraeus.",
            "Where do you make measurements?"
        ],
        [
            "And there's a huge family of methods you could apply there.",
            "Here's a comparison we did in this particular example.",
            "There's all sorts of issues as to whether you have a finite number of observations that you can make.",
            "So there's some method for which there's where the interest is on finding the optimum.",
            "The best armor as opposed to minimizing cumulative regret.",
            "Modeling the correlation.",
            "So these methods, although are very good for that, they don't model the correlations between the sensors.",
            "If you model the correlation between the sensors, you do much better.",
            "So.",
            "The moral hit the important message here is.",
            "Spend time on the model, don't just think that because you're going to try a different expected improvement versus UCB results will be much better.",
            "Really.",
            "A good model matches."
        ],
        [
            "And I mean, ultimately, if you have a toolbox like in this case, this is scikit learn the Python toolbox for machine learning.",
            "You can actually if you give me a regression problem, I should be able to automatically tell you which function whether to use an SVM or whether to use lasu, whether to use a random forest and automatically also choose the parameters for that technique.",
            "I think ultimately this is for people working with Big Data.",
            "This is the kind of tools we want to give them.",
            "This automatic machine learning tool boxes."
        ],
        [
            "I'm gonna skip over the rest of the application."
        ],
        [
            "There's a lot more.",
            "But in order to."
        ],
        [
            "Make this a reality.",
            "And actually."
        ],
        [
            "I'll just mention some more applications.",
            "I mean, you could think black box optimization optimization problems are everywhere.",
            "It's like in terms of configuring planners in terms of configuring controls.",
            "We'll see more of that soon, but you could configure hardware software at centers, compilers.",
            "You could do it all at the same time, find out what's the best parameter for my algorithm and model and hardware, GPU etc so that I get the best performance and you should do that simultaneously as we know 'cause.",
            "Otherwise there's a loss of.",
            "Performance."
        ],
        [
            "But to make that possible, we need to deal with.",
            "High dimensions.",
            "There's been some work."
        ],
        [
            "What one of the things that what this talk was meant to be about was exploiting a particular property of many functions that we've observed to happen in practice.",
            "It doesn't happen always, but it does happen often enough.",
            "That is interesting.",
            "Some functions are of this nature where along One Direction the function changes a lot, but along another direction the function doesn't change much.",
            "OK, so this is a typical case when you have joining several parameters and you change one parameter.",
            "Things don't change much, but by changing another parameter you get a big difference in performance.",
            "So if your function is of this form.",
            "Then if you only search instead of searching in 2D, if you just throw a random array.",
            "And you just search along this 1D space.",
            "Or if you look at here is the top view of the what's happening there.",
            "If you search along the 1D space, you should be able to find the optimum.",
            "OK."
        ],
        [
            "And that's essentially the property that we will exploit.",
            "And the algorithm is very similar to what I showed before.",
            "The only difference is now we're going to generate a random matrix that will take it from the high dimensional Heidi to low dimensions Lodi.",
            "So completely random and we need to create bonds for the space in low dimensions.",
            "And other than that, algorithm is exactly the same, except that we only fit the GP in low dimensions and we only optimize the GP in low dimensions.",
            "And when you want an evaluation and you can only evaluate the function of high dimensions, you project from low dimensions to high dimensions to evaluate the function.",
            "But all your optimization is happening in low dimensions, even though this function exists in high dimensions.",
            "And this is orthogonal to whether you use Thompson sampling or AI, or UCB or or which model you use for the function."
        ],
        [
            "So yeah, the kernel can either be specified directly, no dimensions.",
            "And there's a few things you need to take care of.",
            "Think in the interest of time.",
            "I'm going to skip the details or the kernel can be specified in high dimensions, but in which case you still have to take into account.",
            "The optimization of the expected improvements or UCP still only happens in low dimensions.",
            "So still efficient.",
            "Um?",
            "Of course, we're doing bounded optimization.",
            "If it was not bounded, it would be trivial because there's it's bounded.",
            "When we project a space, the we need to ensure that this embedding covers this other space.",
            "When we do projections, so."
        ],
        [
            "And that's essentially what we do with the theorie.",
            "First we showed that this if if there is an optimum X, there will be an optimum Y that allows us to recover."
        ],
        [
            "For the optimal X, we control the size of the intervals that we need."
        ],
        [
            "And then we show that the regret only depends on what I mentioned.",
            "So essentially sort of theory."
        ],
        [
            "Results we can show here some comparison, So what happens?",
            "There's a technique out there that is by Bo Chen, an ex student of mine working with Andreas Kraus that does the best compared to.",
            "Ramble, which is our random embedding technique in comparison to the standard patient optimization.",
            "That technique is not rotation invariant.",
            "Random embeddings are rotation invariant.",
            "What boat does is he does hypothesis testing initially and uses a spherical kernel, and then he picks the relevant dimensions.",
            "But the moment I take that function, just rotated a bit the approach break."
        ],
        [
            "Down.",
            "Awesome about every approach of breaks down when I go to.",
            "1 billion dimensions but random embeddings doesn't breakdown because I'm just in this case the synthetic function.",
            "I know exactly the intrinsic dimensionality, so if I project there my optimize that my algorithm is still working the law dimension, so it doesn't matter how high the dimension of the function is.",
            "We could have made this higher, but then she started getting into problems on multiplying the mate."
        ],
        [
            "Tricks?",
            "We've applied this to some.",
            "Problems out there.",
            "There are of great practical interest, so in particular we're trying to configure this mixed integer program LP solve, which is sort of very popular, very popular package.",
            "The parameters there are oral binary, so there actually choices.",
            "Should I do this or should I do that and that's actually most of the type of parameters that interests me, 'cause I think often when people are writing code there, making discrete choices and those discrete choices are parameters.",
            "An and we compared to many other techniques out there.",
            "We tend to do.",
            "Compara Bulto, the best state of the art best engineered methods for doing this type of thing."
        ],
        [
            "Fine, I'll just conclude by saying that all the codes available here and you can actually try it.",
            "In their own time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so it's Bayesian optimization in high dimensions and actually high here can be as high as your computer can handle it, provided that intrinsic dimension is low, so this has to do with some of the things that Ramey talked to us about, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Remy pretty much set up the problem for me, but just to rate a rate a rate with trying to do Bayesian optimization, and we're trying to maximize some black box function.",
                    "label": 0
                },
                {
                    "sent": "So the assumption here is that I don't have.",
                    "label": 0
                },
                {
                    "sent": "I don't necessarily have a close form expression for this function typically open.",
                    "label": 0
                },
                {
                    "sent": "We're interested in cases where this function is expensive to evaluate, in which case I want to minimize the number of times I tried this function.",
                    "label": 0
                },
                {
                    "sent": "And I will assume that there is this function has some constraints based typically box constraints, so it has some bounds.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And it may or may not be that this function has derivatives.",
                    "label": 0
                },
                {
                    "sent": "If I have access to the derivatives, I could exploit it derivatives to do better, but often this is derivative free optimization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Technique that I like to use for this is Bayesian optimization.",
                    "label": 0
                },
                {
                    "sent": "An why Bayesian so basin?",
                    "label": 0
                },
                {
                    "sent": "Because the function I'm trying to optimize is something that I don't know.",
                    "label": 0
                },
                {
                    "sent": "In all I can get is samples of this function, so the idea then if I had a prior over the space of functions that I believe the true function comes from that I'm trying to optimize, I could do better.",
                    "label": 0
                },
                {
                    "sent": "If I have a prior, if I do not have a prive, and we've sold lots of priors this morning, smoothness priors if I have an idea of what the smoothness about the function is, I can do better.",
                    "label": 0
                },
                {
                    "sent": "Maybe I have some other types of assumptions about the function that will allow me to succeed if I have no knowledge whatsoever about the function and indeed the function said the characteristic function, so I have no hope of succeeding with this approach, but I believe in many practical problems we do know that the functions are.",
                    "label": 0
                },
                {
                    "sent": "Smooth and so on.",
                    "label": 0
                },
                {
                    "sent": "So in physical systems so we can actually profit from these assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the elements?",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we've so far sampled the function at two points, so I'm going to do this here in one D and I'm going to do it in a continuous 1D space.",
                    "label": 0
                },
                {
                    "sent": "Of course, if this applies to higher dimensions and you could apply to a mix a mix of both categorical and continuous variables.",
                    "label": 0
                },
                {
                    "sent": "But for illustration purposes, let's think of the one the case.",
                    "label": 0
                },
                {
                    "sent": "So I've sampled the true function at two points.",
                    "label": 0
                },
                {
                    "sent": "The true function here is this dashed line.",
                    "label": 0
                },
                {
                    "sent": "Which I don't know I just.",
                    "label": 0
                },
                {
                    "sent": "Get to observed at where I pick a value of XI.",
                    "label": 0
                },
                {
                    "sent": "Observe the value.",
                    "label": 0
                },
                {
                    "sent": "But what I can do is if I had a prior over the function.",
                    "label": 0
                },
                {
                    "sent": "In this case, the Gaussian process prior tells me something about the smoothness of the function.",
                    "label": 0
                },
                {
                    "sent": "Then I could fit.",
                    "label": 0
                },
                {
                    "sent": "A posterior model to these two observations, and so I get a posterior distribution telling me exactly what I expect that function to be.",
                    "label": 0
                },
                {
                    "sent": "So I have in this case, I mean for what I expect a true function to be, and I have some confidence intervals.",
                    "label": 0
                },
                {
                    "sent": "And here being based in so good idea because when you have very few points you want good confidence estimates.",
                    "label": 0
                },
                {
                    "sent": "If I had if I was using frequentist techniques where it would fail is that I wouldn't get good confidence estimates, or at least I don't know how to get them.",
                    "label": 0
                },
                {
                    "sent": "So then the question becomes where do we pick the next point?",
                    "label": 0
                },
                {
                    "sent": "And let's assume that we have for now.",
                    "label": 0
                },
                {
                    "sent": "This magic utility function, which often called the acquisition function and that acquisition function will tell us where to sample next and where to sample next.",
                    "label": 0
                },
                {
                    "sent": "In this case is always sort of a tradeoff between mean and variance, so I've done very well with this point, 'cause it gives me a higher value of the objective, but on the other hand, there's these places where the variance is large, which.",
                    "label": 0
                },
                {
                    "sent": "Or unexplored, so those who could also be good points to look at and then taking a tradeoff between these often will give me a utility function.",
                    "label": 0
                },
                {
                    "sent": "Alot of other considerations come into the construction of the utility function, but for the time being, let's just think of exploit by going forward.",
                    "label": 0
                },
                {
                    "sent": "The mean is high.",
                    "label": 0
                },
                {
                    "sent": "Explore for where the variance is large.",
                    "label": 0
                },
                {
                    "sent": "So in this case, at the next iteration, so suggest.",
                    "label": 0
                },
                {
                    "sent": "At this point I tried a new point.",
                    "label": 0
                },
                {
                    "sent": "Once I get a new point, I re fit the Gaussian process and I continue.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing that and so very quickly, I zoom on on the optimum.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is very simple, so at each round.",
                    "label": 0
                },
                {
                    "sent": "I will optimize.",
                    "label": 0
                },
                {
                    "sent": "I will find the best point X.",
                    "label": 0
                },
                {
                    "sent": "So the X axis the best point by maximizing some utility function.",
                    "label": 0
                },
                {
                    "sent": "This utility function is often if you have a close form expression for this.",
                    "label": 0
                },
                {
                    "sent": "So for example, it could be the mean plus some factor times the variance as in the upper confidence bounds that we saw this morning.",
                    "label": 0
                },
                {
                    "sent": "Typically this is optimized using any of the shelf algorithm, including direct for low, moderate dimensions, 68 dimensions offered, people use different types of discretisation.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to the question of how to optimize this late.",
                    "label": 0
                },
                {
                    "sent": "And once you get a new point being recommended, you simply added to your data, set your set of observations found so far, and you fit the Gaussian process again.",
                    "label": 0
                },
                {
                    "sent": "Obtain the mean, obtain the variance, and then maximize utility.",
                    "label": 0
                },
                {
                    "sent": "As I've presented, you could use many ways of constructing this utility, and there's indeed we'll see those dozens of ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Dozens, probably more hundreds of ways of how to place a prior on functions and how to learn.",
                    "label": 0
                },
                {
                    "sent": "So people typically use Gaussian processes, but I think there's much better things to do in many cases.",
                    "label": 0
                },
                {
                    "sent": "If you have way too many observations and don't use a Gaussian process, you know use a parametric technique that's probably hand deals better with the number of points, or if you have lots of partitions, then maybe what you want to use is you want to use a random forest with Gaussian processes in the leaves.",
                    "label": 0
                },
                {
                    "sent": "Or something like that, you know, so one shouldn't think.",
                    "label": 0
                },
                {
                    "sent": "By being Bayesian we shouldn't think it should just use Gaussian processes, we just tend to use them 'cause they're easy to manipulate and the code is just two lines.",
                    "label": 0
                },
                {
                    "sent": "But there's really a whole family of Bayesian models that we could use here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's an illustration in the case of minimization, which is very similar to what we saw this morning as well, where you sort of focus more and more where the optimum is and you don't visit most of the space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now coming to the question of utility functions.",
                    "label": 0
                },
                {
                    "sent": "There's many utility functions out there in the literature that they proposed in different fields.",
                    "label": 0
                },
                {
                    "sent": "And the one thing that you find when you work with bandits and Bayesian optimization, and incidentally, I forgot to say that, but you could also think about this as a bandit problem, where you have infinite number of arms and all these arms are correlated.",
                    "label": 0
                },
                {
                    "sent": "When you work with bandits and experimental design based on experiment design that you know these are sort of mark markedly different fields and so different people tend to come up with their own ways of solving their problems, and somehow they always call what the other field.",
                    "label": 0
                },
                {
                    "sent": "Does Harris ticks and they call their own method of principled way and?",
                    "label": 0
                },
                {
                    "sent": "If you really go into the literature, you'll see that there's a lot of tension here.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "There's also a lot of ignorance about what the other people are doing, so they some communities might not realize, as some other communities care about cumulative loss as opposed to simple laws or all sorts of little details that actually matter a lot, but.",
                    "label": 0
                },
                {
                    "sent": "Nonetheless, I think over the last few years we've seen a lot more people work in this area, so there's more consensus as to what are the problems and so on.",
                    "label": 0
                },
                {
                    "sent": "Typical ways.",
                    "label": 0
                },
                {
                    "sent": "That people have come up with.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that that I'm going to do better than the best value that I've seen so far?",
                    "label": 0
                },
                {
                    "sent": "So there's something called a probability of improvement.",
                    "label": 0
                },
                {
                    "sent": "It goes back to control Thierry.",
                    "label": 0
                },
                {
                    "sent": "And if I were to use that utility for this friend thresholds, so you want to do.",
                    "label": 0
                },
                {
                    "sent": "Better than the best that you've seen by Epsilon, and if I chose different epsilons then we start seeing different curves, so this is kind of like when we were looking at DO this morning.",
                    "label": 0
                },
                {
                    "sent": "Expected improvement this came up by Mokas.",
                    "label": 0
                },
                {
                    "sent": "Came up with this and he's the guy that actually coined the term Bayesian optimization.",
                    "label": 0
                },
                {
                    "sent": "And he was using this to actually automatically configure algorithms way back in the day, but I think he was ahead of his time, didn't have all the computers that we have today.",
                    "label": 0
                },
                {
                    "sent": "He come up with some very smart ways of doing this, and this particular notion.",
                    "label": 0
                },
                {
                    "sent": "Expect the expectation that you will do better than the best point that you've tried so far, which would be the expectation that you do better than this.",
                    "label": 0
                },
                {
                    "sent": "Often works pretty well in practice and for many applications it will work as well as these upper confidence bounds.",
                    "label": 0
                },
                {
                    "sent": "And for some it worked.",
                    "label": 0
                },
                {
                    "sent": "This also assuming same U of X. Yeah so that's another.",
                    "label": 0
                },
                {
                    "sent": "Very cool.",
                    "label": 0
                },
                {
                    "sent": "Oh mute.",
                    "label": 0
                },
                {
                    "sent": "Oh, so mu is the mean that you estimate from data.",
                    "label": 0
                },
                {
                    "sent": "MU is the mean that you estimate from data.",
                    "label": 0
                },
                {
                    "sent": "Sigma is the variance.",
                    "label": 0
                },
                {
                    "sent": "So for this you have close form expression, so it's the mean of the GP.",
                    "label": 0
                },
                {
                    "sent": "Pardon.",
                    "label": 0
                },
                {
                    "sent": "Or you assume that you're in the model class or that you have a reasonable estimate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but isn't the error that you made.",
                    "label": 0
                },
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "Much bigger than these differences.",
                    "label": 0
                },
                {
                    "sent": "So if your GP does not capture the function, the true function, then you do have a problem of mismatch.",
                    "label": 0
                },
                {
                    "sent": "So I'm making the assumption that I am able to capture this function.",
                    "label": 0
                },
                {
                    "sent": "Of course, if my function has a certain smoothness and I completely picked the wrong model for it.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be suboptimal in that case.",
                    "label": 0
                },
                {
                    "sent": "So we saw some examples this morning where revenue was showing is that if you pick the wrong smoothness, you can actually do poorly.",
                    "label": 0
                },
                {
                    "sent": "But then, in addition to fitting the model properly, then you have to face the issue of which utility to use, and then there's Thompson sampling.",
                    "label": 0
                },
                {
                    "sent": "So Thompson sampling with GPS would be basically given that this is what you have so far.",
                    "label": 1
                },
                {
                    "sent": "Your model you would draw Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You would draw function from this.",
                    "label": 0
                },
                {
                    "sent": "And then you find the optimum of that function.",
                    "label": 0
                },
                {
                    "sent": "The highest point of that function, and that's the point that you would sample next an.",
                    "label": 0
                },
                {
                    "sent": "That strategy actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "In practice, probability matching is a limiting form of this.",
                    "label": 1
                },
                {
                    "sent": "And there's really a huge other literature on these methods, and there's even portfolios of these things that stand on top.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The intuition for why these methods work is very similar to the sort of intuition that Remy went over this morning, so.",
                    "label": 0
                },
                {
                    "sent": "My student must have come up with a proof where he what he was looking at.",
                    "label": 0
                },
                {
                    "sent": "If we assume that the true function here, the solid blue thing is a sample from a Gaussian process, and you basically assume that you can capture it if you know the smoothness.",
                    "label": 0
                },
                {
                    "sent": "If you know its fourth derivatives and so on.",
                    "label": 0
                },
                {
                    "sent": "Then we can play similar games to what we saw this morning.",
                    "label": 0
                },
                {
                    "sent": "The Lipschitz Type games, except that this is sort of a soft probabilistic Lipschitz with high probability.",
                    "label": 0
                },
                {
                    "sent": "I know that if.",
                    "label": 0
                },
                {
                    "sent": "So for any.",
                    "label": 0
                },
                {
                    "sent": "In any interview, I always note the lower confidence estimate and the upper confidence estimate.",
                    "label": 0
                },
                {
                    "sent": "So whatever the let's call the highest of the lower confidence estimates at this point.",
                    "label": 0
                },
                {
                    "sent": "So we're going to call it a Max.",
                    "label": 0
                },
                {
                    "sent": "Lower confidence bound an I know that whatever the highest confidence bound is lower than the lowest confidence bound since the function is trapped.",
                    "label": 0
                },
                {
                    "sent": "In this interval, the function cannot be possibly higher than this point, so that allows me to discard this huge part of the space immediately.",
                    "label": 0
                },
                {
                    "sent": "So that's why we can do global optimization.",
                    "label": 0
                },
                {
                    "sent": "With high probability, that's correct.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's correct.",
                    "label": 0
                },
                {
                    "sent": "So it's a soft.",
                    "label": 0
                },
                {
                    "sent": "It's a probabilistic Lipschitz.",
                    "label": 0
                },
                {
                    "sent": "Confidence always.",
                    "label": 0
                },
                {
                    "sent": "Sample.",
                    "label": 0
                },
                {
                    "sent": "In your example here, this.",
                    "label": 0
                },
                {
                    "sent": "Maximum overload lower confidence interval is the point that you sample.",
                    "label": 0
                },
                {
                    "sent": "So these are the points that I've sampled sampled this, this, this and this.",
                    "label": 0
                },
                {
                    "sent": "So in this case I'm considering the terminal deterministic function, so then the highest will be this point that I've actually picked.",
                    "label": 0
                },
                {
                    "sent": "Yeah, one of the points that are chosen so the condition is.",
                    "label": 0
                },
                {
                    "sent": "Functions of that would be true, but otherwise it does not otherwise.",
                    "label": 0
                },
                {
                    "sent": "So you would you would have to come up with slightly different argument.",
                    "label": 0
                },
                {
                    "sent": "This argument is intuitive because it allows you, at least in the domestic case.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to see why you carve the space and so the question is so one of the things that we were, we've tried to study is as if you were to use a a grid of, say, looking at end points and we worked a half the grid to be half of the size.",
                    "label": 0
                },
                {
                    "sent": "What would happen with this?",
                    "label": 0
                },
                {
                    "sent": "The height of this variance?",
                    "label": 0
                },
                {
                    "sent": "How quickly does the envelope go to 0?",
                    "label": 0
                },
                {
                    "sent": "As we decrease the grid size and so the good news is that you can prove a result like this that the variance actually if the grid size is Delta, the variances.",
                    "label": 0
                },
                {
                    "sent": "It's actually going is upper bound by Delta squared, so we have this exponentiation in the decrease of the Heights as we decrease the interval.",
                    "label": 0
                },
                {
                    "sent": "So that's really nice, and that's what gives you this exponential rates on regret, so you very quickly can exponentially get to the optimal.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we when we prove this, we were using a grid which is not very efficient and we were helping the size.",
                    "label": 0
                },
                {
                    "sent": "But if you were to use a trip rotation.",
                    "label": 0
                },
                {
                    "sent": "And you were to use these assumptions.",
                    "label": 0
                },
                {
                    "sent": "the GP assumptions.",
                    "label": 0
                },
                {
                    "sent": "Then you could actually get the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "You could get a more efficient algorithm that which so.",
                    "label": 0
                },
                {
                    "sent": "We would have to do this, but I think the way the world would go through.",
                    "label": 0
                },
                {
                    "sent": "How long is an expectation?",
                    "label": 0
                },
                {
                    "sent": "Which is is it like a valuation?",
                    "label": 0
                },
                {
                    "sent": "I mean, do you do an expectation over or is this for a given function?",
                    "label": 0
                },
                {
                    "sent": "Or you do and expect it's for it's for this, um, yeah, yeah, that's correct.",
                    "label": 0
                },
                {
                    "sent": "Assuming that the function.",
                    "label": 0
                },
                {
                    "sent": "That you do this prior because you use it in the expectation of the regret which is typed.",
                    "label": 0
                },
                {
                    "sent": "This prior is given by this, that's correct.",
                    "label": 0
                },
                {
                    "sent": "Mention there was.",
                    "label": 0
                },
                {
                    "sent": "This is the dimension, so in this case this is doing poorly with the dimension because I'm using a grid, I'm not being smart.",
                    "label": 0
                },
                {
                    "sent": "There's another thing that I'm obviating here, and this is the problem of many of the proofs out there.",
                    "label": 0
                },
                {
                    "sent": "They all assume that a digit duration you have maximized the expected improvement, or the UCB or whatever, and actually maximizing the acquisition function is an NP hard problem, so a lot of the proofs are conditional and you having solving an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "If you dig in the literature, you'll see that.",
                    "label": 0
                },
                {
                    "sent": "This is one exception, ESO.",
                    "label": 0
                },
                {
                    "sent": "That's why I actually like the idea of trying to mix this with this.",
                    "label": 0
                },
                {
                    "sent": "So 'cause that will get us away from sort of somewhat dodgy proofs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which nonetheless have been extremely useful toward understanding the method.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's essentially what Bayesian optimization is, and I was doing an introduction 'cause the next talk will be also be on this.",
                    "label": 0
                },
                {
                    "sent": "So when Mark comes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Log because there's lots of applications to it.",
                    "label": 0
                },
                {
                    "sent": "So one of the applications that I'm very interested in is this information extraction, so we've already seen that for news recommendation.",
                    "label": 0
                },
                {
                    "sent": "So on this morning often you take part of speech you take.",
                    "label": 0
                },
                {
                    "sent": "Named entity recognition, you take a bunch of her wrist sticks in order to construct the pipeline that recommends a set of features that describes the document.",
                    "label": 0
                },
                {
                    "sent": "And when I work with organisations doing this recommendation, they often tend to do that, and so one of the things that I would like to do is to automate those pipeline so that when you get user feedback, the actual the whole information extraction pipeline gets better so.",
                    "label": 0
                },
                {
                    "sent": "Even for things as simple as concept extraction, where you just basically try to understand that the words Brad Pitt and Angelina Jolie actually mean.",
                    "label": 0
                },
                {
                    "sent": "Not four separate words, but it's one concept which is Brad Gelina, which we all know about.",
                    "label": 0
                },
                {
                    "sent": "There's this paper, which is one of the best techniques out there for that, by actor Garcia Molina, which has 18 free parameters and a load of NLP.",
                    "label": 0
                },
                {
                    "sent": "Information extraction techniques have lots of free parameters.",
                    "label": 1
                },
                {
                    "sent": "And by using these techniques of Bayesian optimization where you just try some parameters, you look at the performance that it has out there.",
                    "label": 0
                },
                {
                    "sent": "You can actually optimize this in this entire pipelines, and if you have several teams in your company all contributing a component of the software towards your product, you can actually optimize what the entire company is doing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's one of the things I like about it.",
                    "label": 0
                },
                {
                    "sent": "It's also very useful for just optimizing the algorithms that we already use in practice, so I just love working with Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "Here's a history of applying different types of neural networks with different type of Monte Carlo methods and two very simple sort of classical data set of folks used to use.",
                    "label": 0
                },
                {
                    "sent": "When I was doing my PhD so different people got different errors and there was a race to minimize this error, which involve all sorts of pretty much every technique that's out there in a machine learning book, Gaussian approximations, Laplace, reversible jump, HMC, HMC with automatic relevance that are relation.",
                    "label": 0
                },
                {
                    "sent": "I spent three years in Cambridge to get this, and one of my students just took exactly the code of Radford nails and then just put it around it and optimizer that will try to minimize some criterion for performance of HMC and just by doing that he was able to do better than everyone else.",
                    "label": 0
                },
                {
                    "sent": "So this took me 3 years.",
                    "label": 0
                },
                {
                    "sent": "This took him and after.",
                    "label": 0
                },
                {
                    "sent": "So that's a sort of scale of improvements in productivity for new users these techniques.",
                    "label": 0
                },
                {
                    "sent": "An even when folks try to design adaptive algorithms that are very good, like nuts for Hyper Montecarlo which is very popular approach by Andrew Gelman by doing Bayesian optimization at least in several datasets we have found that we can actually do much better than these engineered adaptation schemes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that allows us to attack all sorts of problems where we can do better than things like Swendsen Wang, which is a sort of classical method for these type of ferromagnetic models.",
                    "label": 0
                },
                {
                    "sent": "So antiferromagnetic models.",
                    "label": 0
                },
                {
                    "sent": "Pardon.",
                    "label": 0
                },
                {
                    "sent": "Self avoiding random walk.",
                    "label": 0
                },
                {
                    "sent": "It's one of the it's sort of like a hyper Montecarlo but for discrete state space is that we've come up with but it has too many free parameters and then the only way so one of the things that based on optimizations done for us.",
                    "label": 0
                },
                {
                    "sent": "This algorithm design is in the setting is that we can now allow for the algorithms to have a lot more degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And then we have the optimization on top to automatically configure those degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "'cause I would say based on my knowledge of trying methods for this creates the systems Monte Carlo methods that they give.",
                    "label": 0
                },
                {
                    "sent": "Sample is probably the best, is just because anyone can code it in.",
                    "label": 0
                },
                {
                    "sent": "It's simple and so on.",
                    "label": 0
                },
                {
                    "sent": "If you want to have something more sophisticated it would be nice to have an automatic way of tuning that method to the problem.",
                    "label": 0
                },
                {
                    "sent": "Foundational.",
                    "label": 0
                },
                {
                    "sent": "I one of the things I've done is to change it with cross validation and we should discuss that offline.",
                    "label": 0
                },
                {
                    "sent": "Because that in itself is something novel, but we've also June.",
                    "label": 0
                },
                {
                    "sent": "You can tune it with any measure of performance that someone might think is a good way of measuring the performance of your Monte Carlo method.",
                    "label": 0
                },
                {
                    "sent": "If you have a predictive task like you want to use your neural network to classify MNIST, then cross validation makes perfect sense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Bayesian optimization is playing a huge role also in the world of analytics where there's a very nice tutorial by Steve Scott where you produce your not produce websites dynamically with different features and then to maximize the content.",
                    "label": 0
                },
                {
                    "sent": "And of course here we actually have some people that actually work on this.",
                    "label": 0
                },
                {
                    "sent": "And also for like I often consult for this company, can't agent and they what they have is they automatically configure your online games to maximize revenue for the advert for the game producers.",
                    "label": 0
                },
                {
                    "sent": "And so all these are bandit type problems of basic black box optimization problems.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's very useful for interfaces whenever you deal with humans.",
                    "label": 0
                },
                {
                    "sent": "05 minutes I'm going to speed up, so one of the things that really interests me is humans are actually very expensive black boxes.",
                    "label": 1
                },
                {
                    "sent": "If I want to optimize something for a person, whether it's news recommendation.",
                    "label": 0
                },
                {
                    "sent": "But even as we'll see different applications, then it's important to be able to figure out quickly with the human wants, not ask too many silly questions.",
                    "label": 0
                },
                {
                    "sent": "So in one application we actually what we were trying to do is assist animator.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that animator.",
                    "label": 0
                },
                {
                    "sent": "Wants to create a sphere that has this particular lighting.",
                    "label": 0
                },
                {
                    "sent": "That animator might not know about differential equations and so on, and how to set all the parameters by hand, but that animator is very good at saying out of two options, which one she thinks looks more like with what she has in mind.",
                    "label": 0
                },
                {
                    "sent": "Which is this.",
                    "label": 0
                },
                {
                    "sent": "And using Bayesian optimization we can keep generating pairs.",
                    "label": 0
                },
                {
                    "sent": "And what we do in this case is we fit with fitting the Gaussian process and the latent space of all the degrees of freedom of satyrs, beard for lighting conditions.",
                    "label": 0
                },
                {
                    "sent": "And then we zoom in into finding exactly what the animator wants very quickly.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're doing we configuring an interface for the, for the animator, and we've used this for.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Animation and so on.",
                    "label": 0
                },
                {
                    "sent": "More applications, sensor networks.",
                    "label": 0
                },
                {
                    "sent": "If you have a highway, this is sort of classical example of under ascraeus.",
                    "label": 0
                },
                {
                    "sent": "Where do you make measurements?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a huge family of methods you could apply there.",
                    "label": 0
                },
                {
                    "sent": "Here's a comparison we did in this particular example.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of issues as to whether you have a finite number of observations that you can make.",
                    "label": 0
                },
                {
                    "sent": "So there's some method for which there's where the interest is on finding the optimum.",
                    "label": 0
                },
                {
                    "sent": "The best armor as opposed to minimizing cumulative regret.",
                    "label": 0
                },
                {
                    "sent": "Modeling the correlation.",
                    "label": 0
                },
                {
                    "sent": "So these methods, although are very good for that, they don't model the correlations between the sensors.",
                    "label": 0
                },
                {
                    "sent": "If you model the correlation between the sensors, you do much better.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The moral hit the important message here is.",
                    "label": 0
                },
                {
                    "sent": "Spend time on the model, don't just think that because you're going to try a different expected improvement versus UCB results will be much better.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "A good model matches.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I mean, ultimately, if you have a toolbox like in this case, this is scikit learn the Python toolbox for machine learning.",
                    "label": 0
                },
                {
                    "sent": "You can actually if you give me a regression problem, I should be able to automatically tell you which function whether to use an SVM or whether to use lasu, whether to use a random forest and automatically also choose the parameters for that technique.",
                    "label": 0
                },
                {
                    "sent": "I think ultimately this is for people working with Big Data.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of tools we want to give them.",
                    "label": 0
                },
                {
                    "sent": "This automatic machine learning tool boxes.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm gonna skip over the rest of the application.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a lot more.",
                    "label": 0
                },
                {
                    "sent": "But in order to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make this a reality.",
                    "label": 0
                },
                {
                    "sent": "And actually.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just mention some more applications.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could think black box optimization optimization problems are everywhere.",
                    "label": 0
                },
                {
                    "sent": "It's like in terms of configuring planners in terms of configuring controls.",
                    "label": 0
                },
                {
                    "sent": "We'll see more of that soon, but you could configure hardware software at centers, compilers.",
                    "label": 1
                },
                {
                    "sent": "You could do it all at the same time, find out what's the best parameter for my algorithm and model and hardware, GPU etc so that I get the best performance and you should do that simultaneously as we know 'cause.",
                    "label": 0
                },
                {
                    "sent": "Otherwise there's a loss of.",
                    "label": 0
                },
                {
                    "sent": "Performance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But to make that possible, we need to deal with.",
                    "label": 0
                },
                {
                    "sent": "High dimensions.",
                    "label": 0
                },
                {
                    "sent": "There's been some work.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What one of the things that what this talk was meant to be about was exploiting a particular property of many functions that we've observed to happen in practice.",
                    "label": 0
                },
                {
                    "sent": "It doesn't happen always, but it does happen often enough.",
                    "label": 0
                },
                {
                    "sent": "That is interesting.",
                    "label": 0
                },
                {
                    "sent": "Some functions are of this nature where along One Direction the function changes a lot, but along another direction the function doesn't change much.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a typical case when you have joining several parameters and you change one parameter.",
                    "label": 0
                },
                {
                    "sent": "Things don't change much, but by changing another parameter you get a big difference in performance.",
                    "label": 0
                },
                {
                    "sent": "So if your function is of this form.",
                    "label": 0
                },
                {
                    "sent": "Then if you only search instead of searching in 2D, if you just throw a random array.",
                    "label": 0
                },
                {
                    "sent": "And you just search along this 1D space.",
                    "label": 0
                },
                {
                    "sent": "Or if you look at here is the top view of the what's happening there.",
                    "label": 0
                },
                {
                    "sent": "If you search along the 1D space, you should be able to find the optimum.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's essentially the property that we will exploit.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm is very similar to what I showed before.",
                    "label": 0
                },
                {
                    "sent": "The only difference is now we're going to generate a random matrix that will take it from the high dimensional Heidi to low dimensions Lodi.",
                    "label": 0
                },
                {
                    "sent": "So completely random and we need to create bonds for the space in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "And other than that, algorithm is exactly the same, except that we only fit the GP in low dimensions and we only optimize the GP in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "And when you want an evaluation and you can only evaluate the function of high dimensions, you project from low dimensions to high dimensions to evaluate the function.",
                    "label": 0
                },
                {
                    "sent": "But all your optimization is happening in low dimensions, even though this function exists in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "And this is orthogonal to whether you use Thompson sampling or AI, or UCB or or which model you use for the function.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, the kernel can either be specified directly, no dimensions.",
                    "label": 0
                },
                {
                    "sent": "And there's a few things you need to take care of.",
                    "label": 0
                },
                {
                    "sent": "Think in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'm going to skip the details or the kernel can be specified in high dimensions, but in which case you still have to take into account.",
                    "label": 0
                },
                {
                    "sent": "The optimization of the expected improvements or UCP still only happens in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "So still efficient.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of course, we're doing bounded optimization.",
                    "label": 0
                },
                {
                    "sent": "If it was not bounded, it would be trivial because there's it's bounded.",
                    "label": 0
                },
                {
                    "sent": "When we project a space, the we need to ensure that this embedding covers this other space.",
                    "label": 0
                },
                {
                    "sent": "When we do projections, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's essentially what we do with the theorie.",
                    "label": 0
                },
                {
                    "sent": "First we showed that this if if there is an optimum X, there will be an optimum Y that allows us to recover.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the optimal X, we control the size of the intervals that we need.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we show that the regret only depends on what I mentioned.",
                    "label": 0
                },
                {
                    "sent": "So essentially sort of theory.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results we can show here some comparison, So what happens?",
                    "label": 0
                },
                {
                    "sent": "There's a technique out there that is by Bo Chen, an ex student of mine working with Andreas Kraus that does the best compared to.",
                    "label": 0
                },
                {
                    "sent": "Ramble, which is our random embedding technique in comparison to the standard patient optimization.",
                    "label": 0
                },
                {
                    "sent": "That technique is not rotation invariant.",
                    "label": 0
                },
                {
                    "sent": "Random embeddings are rotation invariant.",
                    "label": 0
                },
                {
                    "sent": "What boat does is he does hypothesis testing initially and uses a spherical kernel, and then he picks the relevant dimensions.",
                    "label": 0
                },
                {
                    "sent": "But the moment I take that function, just rotated a bit the approach break.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Down.",
                    "label": 0
                },
                {
                    "sent": "Awesome about every approach of breaks down when I go to.",
                    "label": 0
                },
                {
                    "sent": "1 billion dimensions but random embeddings doesn't breakdown because I'm just in this case the synthetic function.",
                    "label": 1
                },
                {
                    "sent": "I know exactly the intrinsic dimensionality, so if I project there my optimize that my algorithm is still working the law dimension, so it doesn't matter how high the dimension of the function is.",
                    "label": 0
                },
                {
                    "sent": "We could have made this higher, but then she started getting into problems on multiplying the mate.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tricks?",
                    "label": 0
                },
                {
                    "sent": "We've applied this to some.",
                    "label": 0
                },
                {
                    "sent": "Problems out there.",
                    "label": 0
                },
                {
                    "sent": "There are of great practical interest, so in particular we're trying to configure this mixed integer program LP solve, which is sort of very popular, very popular package.",
                    "label": 0
                },
                {
                    "sent": "The parameters there are oral binary, so there actually choices.",
                    "label": 0
                },
                {
                    "sent": "Should I do this or should I do that and that's actually most of the type of parameters that interests me, 'cause I think often when people are writing code there, making discrete choices and those discrete choices are parameters.",
                    "label": 0
                },
                {
                    "sent": "An and we compared to many other techniques out there.",
                    "label": 0
                },
                {
                    "sent": "We tend to do.",
                    "label": 0
                },
                {
                    "sent": "Compara Bulto, the best state of the art best engineered methods for doing this type of thing.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fine, I'll just conclude by saying that all the codes available here and you can actually try it.",
                    "label": 0
                },
                {
                    "sent": "In their own time.",
                    "label": 0
                }
            ]
        }
    }
}