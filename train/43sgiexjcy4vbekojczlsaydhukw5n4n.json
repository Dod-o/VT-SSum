{
    "id": "43sgiexjcy4vbekojczlsaydhukw5n4n",
    "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
    "info": {
        "author": [
            "Tegan Maharaj, Montreal Institute for Learning Algorithms (MILA), University of Montreal"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_maharaj_zoneout/",
    "segmentation": [
        [
            "So my name is tag and I'm presenting zone."
        ],
        [
            "Mount, which is a regularizer for RNS that stochastically forces hidden units to keep their previous values."
        ],
        [
            "This is joint work with David Krueger, Yash Kumar, Mohammed Possessky, Nicholas Rosemary and Anki, Android Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville and Chris Paul.",
            "This is a very collaborative project, is a great experience for me.",
            "If you ever have the opportunity to work with any of these great people, I highly recommend them."
        ],
        [
            "Structure of the talk.",
            "I'll start by presenting the basic idea of zone out.",
            "It's a pretty simple idea.",
            "Then I'll show how it works in RNS."
        ],
        [
            "Colosseums how you implement it.",
            "I'll talk a little bit about how."
        ],
        [
            "How and why we think it works and then I will show you."
        ],
        [
            "Works in some experimental results."
        ],
        [
            "The basic idea of zone out is that you have a random probability at each time step for each unit of keeping the hidden state that you had from the previous time step.",
            "In other words, were stochastically introducing identity connections between time steps.",
            "This is a lot like dropout it, zoning out a unit is a lot like dropping it out, but with an identity mask instead of a zero mask.",
            "And I'll explain that further later, as well as why we think identity masking is more appropriate in RNS than zero masking."
        ],
        [
            "Just to make sure everybody is on the same page, this is a recurrent neural network.",
            "At each time step you have an input.",
            "You do your WX plus be an produce an output using the same parameters at every time step."
        ],
        [
            "In a single layer RNN."
        ],
        [
            "This dashed line represents the zone out skip connection.",
            "The dashed line represents that that skip connection is taken stochastically with some probability.",
            "We're calling it zed.",
            "Most of the time the zone out probability."
        ],
        [
            "In an LCM, the internal dynamics are lit."
        ],
        [
            "But different, but for zone out."
        ],
        [
            "Basically the same.",
            "You have."
        ],
        [
            "A stochastic probability of taking this skip connection between time steps for both hidden states ansells.",
            "These diagrams are mostly taken from Chris Olas blog, which has a great description of LCMS and RNS.",
            "If you want to know more about how they work."
        ],
        [
            "I said I would talk more about zone out as an identity mask.",
            "If you consider an RNN is applying a transition operator at every time step drop out on certain units, is the equivalent of multiplying by a zero in that transition operator, whereas zone out multiplies by a one and this is much more stable within an RNN when you're multiplying by the same transition operator at each at each time step."
        ],
        [
            "If, like me, he parsed code a little more easily than equations, this is how you would implement zone out in Python.",
            "So in Tensorflow or Theano you sample your masks and then inside the LST em after computing the states as usual, you just multiply by the zone out mask and you can see here that if the zone out states were zero, you'd have zero probability of taking this previous state and H would just be equal to H. As usual, whereas if so no probability were one total zone out, you would only ever have your previous states, not recommended."
        ],
        [
            "So why does this work?",
            "Like dropout?",
            "Zona trains a pseudo ensemble suit on samples of concept introduced by Philip Bachman a couple years ago, and I'll show you friend wanna pick up and NIPS 2014?",
            "Think of this as taking the model that you would have unregularized perturbing the states a little bit and then averaging those results.",
            "That's what zone out gives you."
        ],
        [
            "You can also think of zone out as a per unit application of stochastic depth in RNS, stochastic depth is recently introduced technique to train very, very deep neural networks feedforward network site similar to Resnet, but instead of adding the identity connection like resonates do stochastic depth drops.",
            "Random subsets of layers and replaces them with the identity.",
            "To our knowledge, stochastic depth has not been applied to recurrent neural networks.",
            "We did try a variant of sort of a naive applet."
        ],
        [
            "Nation of stochastic depth.",
            "In recurrent neural networks.",
            "But in an RNN, that would be equivalent to dropping an entire time step, which actually works better than you would expect.",
            "Tools I'll show results, but per unit version empirically works better and I think is a little bit better motivated."
        ],
        [
            "So if it is really poorly formatted slide, but this is some other related work.",
            "Most previous work introducing dropout in RNS works on dropout.",
            "Trying to introduce zero masking in RNS.",
            "Let's see the SIM and some new to at all.",
            "Recurrent dropout variant does something very similar to zone out, but applies an identity mask only on the input gate, so you don't get the actual identity connections from time step time step that zone out does that.",
            "As you mentioned yesterday, give you a better condition Jacobian from time step, time step and so we think that's why I zone out outperforms.",
            "That variant swap out is a recent paper that did different variants of dropout residual connections in feedforward networks, and they tested something like zone out in a feedforward setting where they call it skip forward.",
            "They found it did not perform as well in feedforward networks, but we do find very good performance in recurrent neural networks."
        ],
        [
            "And I'll talk about it now.",
            "Here's a plot of the normalized average gradient norms of the cost.",
            "This is sequential emnace, so the cost is a classification error with respect to time step.",
            "So you get the cost at the end.",
            "This is an endless image, so it's 28 by 28, so you have 784 pixels and this is the average norm of the gradients.",
            "Of that cost with respect to the given time step.",
            "So as you go further back in time, obviously the.",
            "There is less information.",
            "But with analysis, TM is very good at preserving this information.",
            "Sort of addressing the vanishing gradient problem.",
            "And zone out is as well if you apply zero masking if you apply dropout where you would apply zone out you can see this is the blue line here that all of the gradient information is being assigned to just the last few times steps that the RNN observes, whereas we get the regularization benefits perfectly analogous to drop out.",
            "But we still maintain the propagation of gradients like in analysis TM."
        ],
        [
            "Here are the training and validation curves for permitted sequential hymnist.",
            "I'm not sure if you can see, but that lighter blue curve at the bottom.",
            "This is zone out.",
            "These are."
        ],
        [
            "It's probably easier to see these are the percent error rates on computed sequential amnesty.",
            "Zone out does not beat state of the art alone, which is recurrent batch normalization by Tim humans at all from mill as well this year.",
            "But combining zone out and recurrent batch norm.",
            "They seem to play well together and we set a new state of the art 4.1."
        ],
        [
            "Error rate on permitted specialist."
        ],
        [
            "We also that was a classification task.",
            "This is a character level language modeling task Carol Penn Treebank.",
            "This is an exploration of different zone out probabilities and I should mention this is the only hyperparameter sort of search that we did.",
            "We searched over zone out probabilities on cells and states, but other than that we just dropped in zone out on the existing state of the art architecture and didn't search over like units, layer size or optimization algorithms or anything like that.",
            "So we found our best performing zone out is .5 on cells and .05 on states that blue line at the bottom."
        ],
        [
            "And this is that zone out.",
            "That dark blue line compared to a bunch of other regularizers.",
            "Best performances on character level.",
            "Penn Treebank and you can see that zone out has very consistent regularization."
        ],
        [
            "It's.",
            "This is the training and test curves."
        ],
        [
            "And these are the numbers that 1.29 is state of the art result.",
            "Anne."
        ],
        [
            "And by training on somewhat overlapping data, it's not really overlapping, but if you take random crops of the input data at every epoch, this is a technique that again coosemans at all from recurrent batch norm used were able to improve that state of the."
        ],
        [
            "Result two 1.2 seven and there's this like apocryphal.",
            "Well not reproducible result from Alex Graves on character level language modeling of 1.26, so getting this 1.2 seven he trained on a different train validation or a different test split, but we're pretty happy with this result."
        ],
        [
            "On word level Penn Treebank, you can see there are a lot of curves on this graph but the lowest red one is zone out."
        ],
        [
            "And this is a relative performance to other regularizers.",
            "These results are not like these perplexity numbers are not competitive with state of the art, which is, I think, yarn galor volchek saramba somewhere around 70.",
            "We're not sure exactly why there's so much higher, but both of their code bases are in torch.",
            "We tried.",
            "We started implementing zone and torch, so hopefully will have directly comperable results to there's their architecture seems to be a little bit odd.",
            "They have like extra embedding layers before the LCM.",
            "No forget gate biases, they copy last hidden states to the first initial state when doing truncated back through time.",
            "These are things that we didn't do in our initial implementation for little battery Bank, so version 2.0 of the paper should have directly compareable results.",
            "But you can see that Zona does give.",
            "A strong relative performance boost.",
            "All of those results are reproducible from our code on GitHub, and this is our paper on archive.",
            "Hopefully we'll have a new result.",
            "The new set of results from text, date, and some other tasks available soon."
        ],
        [
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my name is tag and I'm presenting zone.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mount, which is a regularizer for RNS that stochastically forces hidden units to keep their previous values.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is joint work with David Krueger, Yash Kumar, Mohammed Possessky, Nicholas Rosemary and Anki, Android Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville and Chris Paul.",
                    "label": 1
                },
                {
                    "sent": "This is a very collaborative project, is a great experience for me.",
                    "label": 0
                },
                {
                    "sent": "If you ever have the opportunity to work with any of these great people, I highly recommend them.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll start by presenting the basic idea of zone out.",
                    "label": 1
                },
                {
                    "sent": "It's a pretty simple idea.",
                    "label": 1
                },
                {
                    "sent": "Then I'll show how it works in RNS.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Colosseums how you implement it.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit about how.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How and why we think it works and then I will show you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works in some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea of zone out is that you have a random probability at each time step for each unit of keeping the hidden state that you had from the previous time step.",
                    "label": 1
                },
                {
                    "sent": "In other words, were stochastically introducing identity connections between time steps.",
                    "label": 0
                },
                {
                    "sent": "This is a lot like dropout it, zoning out a unit is a lot like dropping it out, but with an identity mask instead of a zero mask.",
                    "label": 0
                },
                {
                    "sent": "And I'll explain that further later, as well as why we think identity masking is more appropriate in RNS than zero masking.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to make sure everybody is on the same page, this is a recurrent neural network.",
                    "label": 1
                },
                {
                    "sent": "At each time step you have an input.",
                    "label": 0
                },
                {
                    "sent": "You do your WX plus be an produce an output using the same parameters at every time step.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a single layer RNN.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This dashed line represents the zone out skip connection.",
                    "label": 0
                },
                {
                    "sent": "The dashed line represents that that skip connection is taken stochastically with some probability.",
                    "label": 0
                },
                {
                    "sent": "We're calling it zed.",
                    "label": 0
                },
                {
                    "sent": "Most of the time the zone out probability.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In an LCM, the internal dynamics are lit.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But different, but for zone out.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically the same.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A stochastic probability of taking this skip connection between time steps for both hidden states ansells.",
                    "label": 1
                },
                {
                    "sent": "These diagrams are mostly taken from Chris Olas blog, which has a great description of LCMS and RNS.",
                    "label": 0
                },
                {
                    "sent": "If you want to know more about how they work.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said I would talk more about zone out as an identity mask.",
                    "label": 0
                },
                {
                    "sent": "If you consider an RNN is applying a transition operator at every time step drop out on certain units, is the equivalent of multiplying by a zero in that transition operator, whereas zone out multiplies by a one and this is much more stable within an RNN when you're multiplying by the same transition operator at each at each time step.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If, like me, he parsed code a little more easily than equations, this is how you would implement zone out in Python.",
                    "label": 0
                },
                {
                    "sent": "So in Tensorflow or Theano you sample your masks and then inside the LST em after computing the states as usual, you just multiply by the zone out mask and you can see here that if the zone out states were zero, you'd have zero probability of taking this previous state and H would just be equal to H. As usual, whereas if so no probability were one total zone out, you would only ever have your previous states, not recommended.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why does this work?",
                    "label": 0
                },
                {
                    "sent": "Like dropout?",
                    "label": 0
                },
                {
                    "sent": "Zona trains a pseudo ensemble suit on samples of concept introduced by Philip Bachman a couple years ago, and I'll show you friend wanna pick up and NIPS 2014?",
                    "label": 1
                },
                {
                    "sent": "Think of this as taking the model that you would have unregularized perturbing the states a little bit and then averaging those results.",
                    "label": 0
                },
                {
                    "sent": "That's what zone out gives you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also think of zone out as a per unit application of stochastic depth in RNS, stochastic depth is recently introduced technique to train very, very deep neural networks feedforward network site similar to Resnet, but instead of adding the identity connection like resonates do stochastic depth drops.",
                    "label": 0
                },
                {
                    "sent": "Random subsets of layers and replaces them with the identity.",
                    "label": 1
                },
                {
                    "sent": "To our knowledge, stochastic depth has not been applied to recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "We did try a variant of sort of a naive applet.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation of stochastic depth.",
                    "label": 0
                },
                {
                    "sent": "In recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "But in an RNN, that would be equivalent to dropping an entire time step, which actually works better than you would expect.",
                    "label": 0
                },
                {
                    "sent": "Tools I'll show results, but per unit version empirically works better and I think is a little bit better motivated.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if it is really poorly formatted slide, but this is some other related work.",
                    "label": 1
                },
                {
                    "sent": "Most previous work introducing dropout in RNS works on dropout.",
                    "label": 0
                },
                {
                    "sent": "Trying to introduce zero masking in RNS.",
                    "label": 1
                },
                {
                    "sent": "Let's see the SIM and some new to at all.",
                    "label": 0
                },
                {
                    "sent": "Recurrent dropout variant does something very similar to zone out, but applies an identity mask only on the input gate, so you don't get the actual identity connections from time step time step that zone out does that.",
                    "label": 0
                },
                {
                    "sent": "As you mentioned yesterday, give you a better condition Jacobian from time step, time step and so we think that's why I zone out outperforms.",
                    "label": 1
                },
                {
                    "sent": "That variant swap out is a recent paper that did different variants of dropout residual connections in feedforward networks, and they tested something like zone out in a feedforward setting where they call it skip forward.",
                    "label": 0
                },
                {
                    "sent": "They found it did not perform as well in feedforward networks, but we do find very good performance in recurrent neural networks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll talk about it now.",
                    "label": 0
                },
                {
                    "sent": "Here's a plot of the normalized average gradient norms of the cost.",
                    "label": 0
                },
                {
                    "sent": "This is sequential emnace, so the cost is a classification error with respect to time step.",
                    "label": 0
                },
                {
                    "sent": "So you get the cost at the end.",
                    "label": 0
                },
                {
                    "sent": "This is an endless image, so it's 28 by 28, so you have 784 pixels and this is the average norm of the gradients.",
                    "label": 0
                },
                {
                    "sent": "Of that cost with respect to the given time step.",
                    "label": 0
                },
                {
                    "sent": "So as you go further back in time, obviously the.",
                    "label": 0
                },
                {
                    "sent": "There is less information.",
                    "label": 0
                },
                {
                    "sent": "But with analysis, TM is very good at preserving this information.",
                    "label": 0
                },
                {
                    "sent": "Sort of addressing the vanishing gradient problem.",
                    "label": 0
                },
                {
                    "sent": "And zone out is as well if you apply zero masking if you apply dropout where you would apply zone out you can see this is the blue line here that all of the gradient information is being assigned to just the last few times steps that the RNN observes, whereas we get the regularization benefits perfectly analogous to drop out.",
                    "label": 0
                },
                {
                    "sent": "But we still maintain the propagation of gradients like in analysis TM.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the training and validation curves for permitted sequential hymnist.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you can see, but that lighter blue curve at the bottom.",
                    "label": 0
                },
                {
                    "sent": "This is zone out.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's probably easier to see these are the percent error rates on computed sequential amnesty.",
                    "label": 0
                },
                {
                    "sent": "Zone out does not beat state of the art alone, which is recurrent batch normalization by Tim humans at all from mill as well this year.",
                    "label": 1
                },
                {
                    "sent": "But combining zone out and recurrent batch norm.",
                    "label": 0
                },
                {
                    "sent": "They seem to play well together and we set a new state of the art 4.1.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error rate on permitted specialist.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also that was a classification task.",
                    "label": 0
                },
                {
                    "sent": "This is a character level language modeling task Carol Penn Treebank.",
                    "label": 1
                },
                {
                    "sent": "This is an exploration of different zone out probabilities and I should mention this is the only hyperparameter sort of search that we did.",
                    "label": 0
                },
                {
                    "sent": "We searched over zone out probabilities on cells and states, but other than that we just dropped in zone out on the existing state of the art architecture and didn't search over like units, layer size or optimization algorithms or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So we found our best performing zone out is .5 on cells and .05 on states that blue line at the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is that zone out.",
                    "label": 0
                },
                {
                    "sent": "That dark blue line compared to a bunch of other regularizers.",
                    "label": 0
                },
                {
                    "sent": "Best performances on character level.",
                    "label": 0
                },
                {
                    "sent": "Penn Treebank and you can see that zone out has very consistent regularization.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "This is the training and test curves.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the numbers that 1.29 is state of the art result.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by training on somewhat overlapping data, it's not really overlapping, but if you take random crops of the input data at every epoch, this is a technique that again coosemans at all from recurrent batch norm used were able to improve that state of the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result two 1.2 seven and there's this like apocryphal.",
                    "label": 0
                },
                {
                    "sent": "Well not reproducible result from Alex Graves on character level language modeling of 1.26, so getting this 1.2 seven he trained on a different train validation or a different test split, but we're pretty happy with this result.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On word level Penn Treebank, you can see there are a lot of curves on this graph but the lowest red one is zone out.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a relative performance to other regularizers.",
                    "label": 0
                },
                {
                    "sent": "These results are not like these perplexity numbers are not competitive with state of the art, which is, I think, yarn galor volchek saramba somewhere around 70.",
                    "label": 0
                },
                {
                    "sent": "We're not sure exactly why there's so much higher, but both of their code bases are in torch.",
                    "label": 0
                },
                {
                    "sent": "We tried.",
                    "label": 0
                },
                {
                    "sent": "We started implementing zone and torch, so hopefully will have directly comperable results to there's their architecture seems to be a little bit odd.",
                    "label": 0
                },
                {
                    "sent": "They have like extra embedding layers before the LCM.",
                    "label": 0
                },
                {
                    "sent": "No forget gate biases, they copy last hidden states to the first initial state when doing truncated back through time.",
                    "label": 0
                },
                {
                    "sent": "These are things that we didn't do in our initial implementation for little battery Bank, so version 2.0 of the paper should have directly compareable results.",
                    "label": 0
                },
                {
                    "sent": "But you can see that Zona does give.",
                    "label": 0
                },
                {
                    "sent": "A strong relative performance boost.",
                    "label": 0
                },
                {
                    "sent": "All of those results are reproducible from our code on GitHub, and this is our paper on archive.",
                    "label": 0
                },
                {
                    "sent": "Hopefully we'll have a new result.",
                    "label": 0
                },
                {
                    "sent": "The new set of results from text, date, and some other tasks available soon.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}