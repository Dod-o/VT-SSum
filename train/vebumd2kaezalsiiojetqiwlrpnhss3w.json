{
    "id": "vebumd2kaezalsiiojetqiwlrpnhss3w",
    "title": "The Fixed Points of Off-Policy TD",
    "info": {
        "author": [
            "J. Zico Kolter, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_kolter_fixedpoints/",
    "segmentation": [
        [
            "So this paper I'm talking about is going to be about off policy TD, which is the problem that was motivated and introduced great by rich earlier said here is you have standard setting where you have a state action reward and next state.",
            "And now given this data for an arbitrary policy, want to determine the value of this new policy, not the one that generate experience, but a new policy.",
            "And it's really the fundamental question of generalization, right?",
            "What can we say about behavior not just of what we observe but about new behavior?"
        ],
        [
            "And in theory at least, this can be solved by temporal difference learning.",
            "The idea is you take your experience and you subsample those states.",
            "Action pairs that are consistent with your policy.",
            "You're trying to evaluate.",
            "You do that and then apply the TD update and hopefully this will converge to a good estimate of the value function.",
            "The problem though, is that as introduced earlier, when you have value function approximation which would always want for a big domain, there can be some problems here.",
            "So first of all TD.",
            "May not converge.",
            "I might not get a actual solution, but this really has been fixed I think by the messages that rich presented earlier and we now can have convergent TD algorithms.",
            "Another problem though, which I think is a pretty big problem, is that even when you get a TD solution, it turns out this solution can actually be arbitrarily poor, meaning that even if the true value function is almost within the span of our bases, we still can get an arbitrarily bad estimate of the value function.",
            "When we employ off policy TD learning.",
            "This is a big problem I think, and what are we going to do about it?"
        ],
        [
            "And this paper is about fixing this element of TD learning.",
            "So the idea here is we're going to re wait.",
            "The samples are subsampled states so that we can get solution guarantees about two solution.",
            "Technical motivation here is that off policy sampling.",
            "What this means is that our states, the filter states are no longer distributed according to the stationary distribution of our policy and with this causes that was called a TD operator, which is like the bellman operator but with a projection.",
            "Is no longer a contraction, and this is actually the technical issue that causes all the problems of off policy learning and what we show in this paper is we describe the set of all distributions over states that in fact do have this to the operator be a contraction and we show this is actually a convex set representable efficiently with linear matrix inequality.",
            "So what we do then is we our algorithm is we re weight the samples by projecting the empirical distribution.",
            "On to this convex set, and that's how we're going to get a better reweighting of our samples."
        ],
        [
            "So what we can say now is if we do this, we can now in fact make guarantees about the quality of the solution of the quality of the solution.",
            "So we can say that the solution we find is bounded by some approximation factor to the best possible value function approximation in our class.",
            "The actual approximation itself is kind of tricky, but we also develop an efficient method using low rank optimization on the dual problem.",
            "To do this efficiently, and finally more than just a theoretical result is actually does perform better in practice as well, so these graphs show the error of the TV solution.",
            "The Apollos solution in red here, and if we use the exact same algorithm but just re weight the samples using our projection algorithm, we get the error lines in blue here, which actually do perform quite a bit better than than the off pulse decays by default.",
            "So in summary, if Richard methods sort of solve the convergence issue of TD methods on the auto policy case, I think this work goes a long way towards solving the solution quality issues in the awful city case.",
            "The posters T6, so I hope to see you there, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this paper I'm talking about is going to be about off policy TD, which is the problem that was motivated and introduced great by rich earlier said here is you have standard setting where you have a state action reward and next state.",
                    "label": 1
                },
                {
                    "sent": "And now given this data for an arbitrary policy, want to determine the value of this new policy, not the one that generate experience, but a new policy.",
                    "label": 1
                },
                {
                    "sent": "And it's really the fundamental question of generalization, right?",
                    "label": 0
                },
                {
                    "sent": "What can we say about behavior not just of what we observe but about new behavior?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in theory at least, this can be solved by temporal difference learning.",
                    "label": 1
                },
                {
                    "sent": "The idea is you take your experience and you subsample those states.",
                    "label": 0
                },
                {
                    "sent": "Action pairs that are consistent with your policy.",
                    "label": 0
                },
                {
                    "sent": "You're trying to evaluate.",
                    "label": 0
                },
                {
                    "sent": "You do that and then apply the TD update and hopefully this will converge to a good estimate of the value function.",
                    "label": 0
                },
                {
                    "sent": "The problem though, is that as introduced earlier, when you have value function approximation which would always want for a big domain, there can be some problems here.",
                    "label": 0
                },
                {
                    "sent": "So first of all TD.",
                    "label": 0
                },
                {
                    "sent": "May not converge.",
                    "label": 0
                },
                {
                    "sent": "I might not get a actual solution, but this really has been fixed I think by the messages that rich presented earlier and we now can have convergent TD algorithms.",
                    "label": 1
                },
                {
                    "sent": "Another problem though, which I think is a pretty big problem, is that even when you get a TD solution, it turns out this solution can actually be arbitrarily poor, meaning that even if the true value function is almost within the span of our bases, we still can get an arbitrarily bad estimate of the value function.",
                    "label": 0
                },
                {
                    "sent": "When we employ off policy TD learning.",
                    "label": 0
                },
                {
                    "sent": "This is a big problem I think, and what are we going to do about it?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this paper is about fixing this element of TD learning.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is we're going to re wait.",
                    "label": 1
                },
                {
                    "sent": "The samples are subsampled states so that we can get solution guarantees about two solution.",
                    "label": 0
                },
                {
                    "sent": "Technical motivation here is that off policy sampling.",
                    "label": 0
                },
                {
                    "sent": "What this means is that our states, the filter states are no longer distributed according to the stationary distribution of our policy and with this causes that was called a TD operator, which is like the bellman operator but with a projection.",
                    "label": 1
                },
                {
                    "sent": "Is no longer a contraction, and this is actually the technical issue that causes all the problems of off policy learning and what we show in this paper is we describe the set of all distributions over states that in fact do have this to the operator be a contraction and we show this is actually a convex set representable efficiently with linear matrix inequality.",
                    "label": 1
                },
                {
                    "sent": "So what we do then is we our algorithm is we re weight the samples by projecting the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "On to this convex set, and that's how we're going to get a better reweighting of our samples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we can say now is if we do this, we can now in fact make guarantees about the quality of the solution of the quality of the solution.",
                    "label": 0
                },
                {
                    "sent": "So we can say that the solution we find is bounded by some approximation factor to the best possible value function approximation in our class.",
                    "label": 1
                },
                {
                    "sent": "The actual approximation itself is kind of tricky, but we also develop an efficient method using low rank optimization on the dual problem.",
                    "label": 0
                },
                {
                    "sent": "To do this efficiently, and finally more than just a theoretical result is actually does perform better in practice as well, so these graphs show the error of the TV solution.",
                    "label": 0
                },
                {
                    "sent": "The Apollos solution in red here, and if we use the exact same algorithm but just re weight the samples using our projection algorithm, we get the error lines in blue here, which actually do perform quite a bit better than than the off pulse decays by default.",
                    "label": 0
                },
                {
                    "sent": "So in summary, if Richard methods sort of solve the convergence issue of TD methods on the auto policy case, I think this work goes a long way towards solving the solution quality issues in the awful city case.",
                    "label": 0
                },
                {
                    "sent": "The posters T6, so I hope to see you there, thank you.",
                    "label": 0
                }
            ]
        }
    }
}