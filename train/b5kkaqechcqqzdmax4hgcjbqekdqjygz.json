{
    "id": "b5kkaqechcqqzdmax4hgcjbqekdqjygz",
    "title": "Reducing the Sampling Complexity of Topic Models",
    "info": {
        "author": [
            "Aaron Li, Computer Science Department, Carnegie Mellon University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_li_sampling_complexity/",
    "segmentation": [
        [
            "Hello, I'm Aaron Lee from Carnegie Mellon and it's my pleasure to present you the work the joint work ways, amorous O'jays and Alex.",
            "Today I'm going to talk about some fundamental techniques to make copy models run faster."
        ],
        [
            "So will three of you.",
            "Here already, experts of the subjects.",
            "So let me just quickly go through the basics and current state of the art, the shortcomings, and then I'll show you how our alias method.",
            "Speed things up.",
            "It doesn't only work for LDA, but also can be generalized to work with more sophisticated models.",
            "For example, Pitman Gore topping models an hierarchical Dirichlet process."
        ],
        [
            "So OK, let's get started."
        ],
        [
            "This is the good old LDA model that everyone loves."
        ],
        [
            "And this is a standard application by LDS user across all fields of data science to analyze billions of documents, images, videos and user activities and everything."
        ],
        [
            "And this is a standard sampling equation for club Skip sampler.",
            "So let me introduce the notations I use.",
            "Anti is the topic dogs document.",
            "I is the document index and Jasa word index.",
            "Everything else is standard.",
            "For example, NTD is a document topic count, an NTW is work topic word count."
        ],
        [
            "The standard way to speed up clubs Gibbs sampler.",
            "Is to look at some sparsity's of each term As for example NTD part have only a few non zero entries for OT becausw documents are short and each token in the document contributes no more than one topic.",
            "So similarly NTW is also sparse.",
            "If the average word frequencies low, which is generally true for small collections."
        ],
        [
            "Sparse LDA, sparsely ideas are created using this principle.",
            "The sparse terms are expanded as a multiplier for each term, and it's very effective for small collections and you can see that each term shared several sparse."
        ],
        [
            "But it's not true for large collections in large collections, the word frequency is a lot higher and the NTW variable becomes dense.",
            "So something performance falls back to naive algorithm in the worst case."
        ],
        [
            "They say the same issues also appear in other topic models and that is possible and Richard Process."
        ],
        [
            "And the other is hierarchical directional process.",
            "There are just two examples, so they're simply.",
            "Equations are very complex and they don't even decompose to sparse terms, even for small collections."
        ],
        [
            "But we do something different in addition to sparse decomposition, like in the state of the art.",
            "And theories and often neglected property in topic models.",
            "That is, the word emission models slow changing, unlike other, unlike the topic distribution for each document.",
            "How use LDA as an example, show you how we use this property to approximate the soul changing part.",
            "So on the right and reduce the sampling complexity to only one sparse term.",
            "Namely, I'm talking about the part on the left.",
            "And fundamentally reduce the running time for all topic models."
        ],
        [
            "That's the first thing gradient is.",
            "Metropolis Hastings sampler is very simple.",
            "In every static static book."
        ],
        [
            "First of all, we decompose the sampling equation in a different way to make the dense part match exactly the slow training part in the model.",
            "So as discussed before.",
            "So stars, the sparse part can be sampled very quickly."
        ],
        [
            "The instead of sampling from the dense part directly.",
            "Compute the probability for each outcome every time.",
            "We can just draw samples from approximate static distribution and."
        ],
        [
            "Static distribution has a constant normalizer, apparently.",
            "Therefore at first we reduce the normal normalization complexity from order K down to order KD, where K is a number of topics and Katie is a number of non zero topic counts in the current document D. To make things clear.",
            "We can rewrite the equation of simple way, a sparse term depending on the document in the current word and dense term depending only on the current word."
        ],
        [
            "Having only the approximate distribution Q at our disposal with Metropolis Hastings, we are still able to sample from the true distribution P. If we follow this procedure, assume we have an old sample called X Prime an we draw a sample X from Q.",
            "The proposal distribution, and then we compute the acceptance probability of X prime 2X with the equation in the middle.",
            "And this can be done very quickly, because we only need to evaluate to ratios in constant time."
        ],
        [
            "So therefore, if there is a way to sample quickly from the dense part.",
            "We would be able to reduce the sampling complexity down to order.",
            "KD is the quantity doesn't really change with respect to the side of the corpus, and in practice very small."
        ],
        [
            "In addition to that, enter the generalized form on the top.",
            "Our method.",
            "Could work on many topic models as long as we're able to simply rewrite the model as a summation of these two terms."
        ],
        [
            "So to quickly sample from the dense distribution as I mentioned before, we need a second ingredient is called alias sampling."
        ],
        [
            "Walker's alias Method is developed by Alistair Walker in 1977.",
            "It works this way given a discreet discrete distribution.",
            "It compiles the distribution, it compiles the distribution into a static table.",
            "And afterwards drawing the sample from the static table only takes constant time.",
            "Instead of sometime linear to the number of outcomes.",
            "So let me briefly go through the algorithm."
        ],
        [
            "So at first we have a distribution where each outcome has has different proportions."
        ],
        [
            "We go, we go through all the outcomes and find the average value of the proportions."
        ],
        [
            "And then for each outcome with proportions less than the average, we take part of the proportion.",
            "Over another outcome, and to make it reach the average.",
            "As we take from an outcome, we keep track of the origin."
        ],
        [
            "And we keep doing this until we get all outcomes.",
            "Reach the average proportion."
        ],
        [
            "So in the end welcomes reach exactly the average proportion.",
            "That's why it's called average.",
            "Open set this point.",
            "Each outcome is composed by no more than two parts, 2 components, the origonal proportion and part of the proportion from another outcome.",
            "So to Joe samples from this static table, we only need to generate 2 random numbers.",
            "First round Number decides which column we're going.",
            "To look at and second one decide which component we're going to take."
        ],
        [
            "Go back to the original sampling equation.",
            "The first term is sparse and can be sampled in order KD and second term is dense but static, and but we use the alias method.",
            "It's the perfect way to compute a static approximation for the dense term and generate samples from it.",
            "The boot process of the alias tables can take place in the background thread for every word repeatedly in parallel with the main sampler, so this ensures the sampling complexity of our alias.",
            "Sampler is no more than order KD and the approximation distribution is close enough to the real distribution.",
            "So that the acceptance probability in the Metropolis Hastings sampling is high enough."
        ],
        [
            "But let me show you some results of Audi Apdp and HTP in terms of quality and speed.",
            "And all the results are generated using a single thread C++ implementation on a laptop with one point 7, one point 7.",
            "Three gigahertz of CPU."
        ],
        [
            "The 1st result is between sparse LDA, an alias LDA.",
            "We vary the number of topics we compute the running time against.",
            "We compare the running time against the number of iterations with different number of topics.",
            "The solid lines are running time of sparse LDA and outlines are for Alias LDA and when the number of topics 256 they are in the bottom.",
            "They are about the same, however, if if the number of topics increases, the alias LDA scales a lot better, and it's a lot faster than sparsely LDA and in the end is 100 * 100% faster and it concerns with our theory KD change slower than with respect to K as apposed kW."
        ],
        [
            "Similarly, as alias LDA scales a lot better than sparse LDA when we vary the amount of data."
        ],
        [
            "Before sophisticated topic models like HTP PDP time complexity is directly reduced form over K down to order KD and the speedup is huge.",
            "So by the way, is the we employed block table indicator sampling developed by rebound times of previous speaker?",
            "And he's also my Bachelor Honours thesis supervisor back in Australia.",
            "And his students China Chen and Lundo."
        ],
        [
            "And all this speed up.",
            "Comes without any sacrifice in convergence, time or quality.",
            "The alias method and the original method converge.",
            "The same exact perplexity in the end."
        ],
        [
            "And you can also see it in the middle as well.",
            "And so to summarize.",
            "We showed four things.",
            "And we showed a new way of sparse decomposition of general topic models.",
            "We also showed a new something method using my chocolates Hasting sampler and Walker's alias method.",
            "And we showed a significant speedup over the previous state of the art.",
            "And a sampler is us able to process more than a million tokens per second with only a single thread on a laptop as far more.",
            "Is far more than was capable of the previous state of the art.",
            "For example, Yahoo LDA, which processes about 250,000 tokens per second with eight cores."
        ],
        [
            "I finished, I want to show you some some of our extended results.",
            "At Google at Google, the implementation for LDA extended into a parallel and distributed environment with thousands of machines."
        ],
        [
            "And turns out Alias LDA on the same scale.",
            "It's not only faster, but also converge better.",
            "Converge to better perplexity compared to sparse LDA."
        ],
        [
            "And the differences in speed is in orders of magnitude.",
            "Well.",
            "That's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, I'm Aaron Lee from Carnegie Mellon and it's my pleasure to present you the work the joint work ways, amorous O'jays and Alex.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to talk about some fundamental techniques to make copy models run faster.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So will three of you.",
                    "label": 0
                },
                {
                    "sent": "Here already, experts of the subjects.",
                    "label": 0
                },
                {
                    "sent": "So let me just quickly go through the basics and current state of the art, the shortcomings, and then I'll show you how our alias method.",
                    "label": 1
                },
                {
                    "sent": "Speed things up.",
                    "label": 0
                },
                {
                    "sent": "It doesn't only work for LDA, but also can be generalized to work with more sophisticated models.",
                    "label": 1
                },
                {
                    "sent": "For example, Pitman Gore topping models an hierarchical Dirichlet process.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the good old LDA model that everyone loves.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a standard application by LDS user across all fields of data science to analyze billions of documents, images, videos and user activities and everything.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a standard sampling equation for club Skip sampler.",
                    "label": 0
                },
                {
                    "sent": "So let me introduce the notations I use.",
                    "label": 0
                },
                {
                    "sent": "Anti is the topic dogs document.",
                    "label": 0
                },
                {
                    "sent": "I is the document index and Jasa word index.",
                    "label": 0
                },
                {
                    "sent": "Everything else is standard.",
                    "label": 0
                },
                {
                    "sent": "For example, NTD is a document topic count, an NTW is work topic word count.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The standard way to speed up clubs Gibbs sampler.",
                    "label": 1
                },
                {
                    "sent": "Is to look at some sparsity's of each term As for example NTD part have only a few non zero entries for OT becausw documents are short and each token in the document contributes no more than one topic.",
                    "label": 0
                },
                {
                    "sent": "So similarly NTW is also sparse.",
                    "label": 0
                },
                {
                    "sent": "If the average word frequencies low, which is generally true for small collections.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparse LDA, sparsely ideas are created using this principle.",
                    "label": 0
                },
                {
                    "sent": "The sparse terms are expanded as a multiplier for each term, and it's very effective for small collections and you can see that each term shared several sparse.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's not true for large collections in large collections, the word frequency is a lot higher and the NTW variable becomes dense.",
                    "label": 0
                },
                {
                    "sent": "So something performance falls back to naive algorithm in the worst case.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They say the same issues also appear in other topic models and that is possible and Richard Process.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other is hierarchical directional process.",
                    "label": 0
                },
                {
                    "sent": "There are just two examples, so they're simply.",
                    "label": 0
                },
                {
                    "sent": "Equations are very complex and they don't even decompose to sparse terms, even for small collections.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we do something different in addition to sparse decomposition, like in the state of the art.",
                    "label": 0
                },
                {
                    "sent": "And theories and often neglected property in topic models.",
                    "label": 0
                },
                {
                    "sent": "That is, the word emission models slow changing, unlike other, unlike the topic distribution for each document.",
                    "label": 1
                },
                {
                    "sent": "How use LDA as an example, show you how we use this property to approximate the soul changing part.",
                    "label": 0
                },
                {
                    "sent": "So on the right and reduce the sampling complexity to only one sparse term.",
                    "label": 0
                },
                {
                    "sent": "Namely, I'm talking about the part on the left.",
                    "label": 0
                },
                {
                    "sent": "And fundamentally reduce the running time for all topic models.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the first thing gradient is.",
                    "label": 0
                },
                {
                    "sent": "Metropolis Hastings sampler is very simple.",
                    "label": 0
                },
                {
                    "sent": "In every static static book.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First of all, we decompose the sampling equation in a different way to make the dense part match exactly the slow training part in the model.",
                    "label": 0
                },
                {
                    "sent": "So as discussed before.",
                    "label": 0
                },
                {
                    "sent": "So stars, the sparse part can be sampled very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The instead of sampling from the dense part directly.",
                    "label": 0
                },
                {
                    "sent": "Compute the probability for each outcome every time.",
                    "label": 0
                },
                {
                    "sent": "We can just draw samples from approximate static distribution and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Static distribution has a constant normalizer, apparently.",
                    "label": 0
                },
                {
                    "sent": "Therefore at first we reduce the normal normalization complexity from order K down to order KD, where K is a number of topics and Katie is a number of non zero topic counts in the current document D. To make things clear.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite the equation of simple way, a sparse term depending on the document in the current word and dense term depending only on the current word.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having only the approximate distribution Q at our disposal with Metropolis Hastings, we are still able to sample from the true distribution P. If we follow this procedure, assume we have an old sample called X Prime an we draw a sample X from Q.",
                    "label": 1
                },
                {
                    "sent": "The proposal distribution, and then we compute the acceptance probability of X prime 2X with the equation in the middle.",
                    "label": 0
                },
                {
                    "sent": "And this can be done very quickly, because we only need to evaluate to ratios in constant time.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So therefore, if there is a way to sample quickly from the dense part.",
                    "label": 0
                },
                {
                    "sent": "We would be able to reduce the sampling complexity down to order.",
                    "label": 0
                },
                {
                    "sent": "KD is the quantity doesn't really change with respect to the side of the corpus, and in practice very small.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition to that, enter the generalized form on the top.",
                    "label": 0
                },
                {
                    "sent": "Our method.",
                    "label": 0
                },
                {
                    "sent": "Could work on many topic models as long as we're able to simply rewrite the model as a summation of these two terms.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to quickly sample from the dense distribution as I mentioned before, we need a second ingredient is called alias sampling.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Walker's alias Method is developed by Alistair Walker in 1977.",
                    "label": 1
                },
                {
                    "sent": "It works this way given a discreet discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "It compiles the distribution, it compiles the distribution into a static table.",
                    "label": 0
                },
                {
                    "sent": "And afterwards drawing the sample from the static table only takes constant time.",
                    "label": 0
                },
                {
                    "sent": "Instead of sometime linear to the number of outcomes.",
                    "label": 0
                },
                {
                    "sent": "So let me briefly go through the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at first we have a distribution where each outcome has has different proportions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go, we go through all the outcomes and find the average value of the proportions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for each outcome with proportions less than the average, we take part of the proportion.",
                    "label": 0
                },
                {
                    "sent": "Over another outcome, and to make it reach the average.",
                    "label": 0
                },
                {
                    "sent": "As we take from an outcome, we keep track of the origin.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we keep doing this until we get all outcomes.",
                    "label": 0
                },
                {
                    "sent": "Reach the average proportion.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the end welcomes reach exactly the average proportion.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called average.",
                    "label": 0
                },
                {
                    "sent": "Open set this point.",
                    "label": 0
                },
                {
                    "sent": "Each outcome is composed by no more than two parts, 2 components, the origonal proportion and part of the proportion from another outcome.",
                    "label": 0
                },
                {
                    "sent": "So to Joe samples from this static table, we only need to generate 2 random numbers.",
                    "label": 0
                },
                {
                    "sent": "First round Number decides which column we're going.",
                    "label": 0
                },
                {
                    "sent": "To look at and second one decide which component we're going to take.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go back to the original sampling equation.",
                    "label": 0
                },
                {
                    "sent": "The first term is sparse and can be sampled in order KD and second term is dense but static, and but we use the alias method.",
                    "label": 1
                },
                {
                    "sent": "It's the perfect way to compute a static approximation for the dense term and generate samples from it.",
                    "label": 0
                },
                {
                    "sent": "The boot process of the alias tables can take place in the background thread for every word repeatedly in parallel with the main sampler, so this ensures the sampling complexity of our alias.",
                    "label": 0
                },
                {
                    "sent": "Sampler is no more than order KD and the approximation distribution is close enough to the real distribution.",
                    "label": 0
                },
                {
                    "sent": "So that the acceptance probability in the Metropolis Hastings sampling is high enough.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let me show you some results of Audi Apdp and HTP in terms of quality and speed.",
                    "label": 0
                },
                {
                    "sent": "And all the results are generated using a single thread C++ implementation on a laptop with one point 7, one point 7.",
                    "label": 0
                },
                {
                    "sent": "Three gigahertz of CPU.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The 1st result is between sparse LDA, an alias LDA.",
                    "label": 0
                },
                {
                    "sent": "We vary the number of topics we compute the running time against.",
                    "label": 1
                },
                {
                    "sent": "We compare the running time against the number of iterations with different number of topics.",
                    "label": 0
                },
                {
                    "sent": "The solid lines are running time of sparse LDA and outlines are for Alias LDA and when the number of topics 256 they are in the bottom.",
                    "label": 0
                },
                {
                    "sent": "They are about the same, however, if if the number of topics increases, the alias LDA scales a lot better, and it's a lot faster than sparsely LDA and in the end is 100 * 100% faster and it concerns with our theory KD change slower than with respect to K as apposed kW.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, as alias LDA scales a lot better than sparse LDA when we vary the amount of data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before sophisticated topic models like HTP PDP time complexity is directly reduced form over K down to order KD and the speedup is huge.",
                    "label": 0
                },
                {
                    "sent": "So by the way, is the we employed block table indicator sampling developed by rebound times of previous speaker?",
                    "label": 0
                },
                {
                    "sent": "And he's also my Bachelor Honours thesis supervisor back in Australia.",
                    "label": 0
                },
                {
                    "sent": "And his students China Chen and Lundo.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all this speed up.",
                    "label": 0
                },
                {
                    "sent": "Comes without any sacrifice in convergence, time or quality.",
                    "label": 0
                },
                {
                    "sent": "The alias method and the original method converge.",
                    "label": 0
                },
                {
                    "sent": "The same exact perplexity in the end.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can also see it in the middle as well.",
                    "label": 0
                },
                {
                    "sent": "And so to summarize.",
                    "label": 0
                },
                {
                    "sent": "We showed four things.",
                    "label": 0
                },
                {
                    "sent": "And we showed a new way of sparse decomposition of general topic models.",
                    "label": 0
                },
                {
                    "sent": "We also showed a new something method using my chocolates Hasting sampler and Walker's alias method.",
                    "label": 0
                },
                {
                    "sent": "And we showed a significant speedup over the previous state of the art.",
                    "label": 0
                },
                {
                    "sent": "And a sampler is us able to process more than a million tokens per second with only a single thread on a laptop as far more.",
                    "label": 0
                },
                {
                    "sent": "Is far more than was capable of the previous state of the art.",
                    "label": 0
                },
                {
                    "sent": "For example, Yahoo LDA, which processes about 250,000 tokens per second with eight cores.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I finished, I want to show you some some of our extended results.",
                    "label": 0
                },
                {
                    "sent": "At Google at Google, the implementation for LDA extended into a parallel and distributed environment with thousands of machines.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And turns out Alias LDA on the same scale.",
                    "label": 0
                },
                {
                    "sent": "It's not only faster, but also converge better.",
                    "label": 0
                },
                {
                    "sent": "Converge to better perplexity compared to sparse LDA.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the differences in speed is in orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        }
    }
}