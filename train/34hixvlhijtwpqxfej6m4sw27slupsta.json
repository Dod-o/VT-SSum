{
    "id": "34hixvlhijtwpqxfej6m4sw27slupsta",
    "title": "Optimizing Semantic Reasoning on Memory-Constrained Platforms using the RETE Algorithm",
    "info": {
        "author": [
            "William Van Woensel, Computer Science Department, Vrije Universiteit Brussel"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_van_woensel_semantic_reasoning/",
    "segmentation": [
        [
            "So.",
            "Good afternoon everyone.",
            "My name is William Baldwin, so I'm here to present a paper that we published at this conference called Optimizing Semantic Reasoning on memory constrained platforms.",
            "Due to using the RTI algorithm.",
            "So obviously the presentation will focus on the contents of the paper, but will also talk about some other work that we've done in this general domain of semantic reasoning on resource constrained platforms.",
            "Let's start with some can't."
        ],
        [
            "I'm a member of a research group.",
            "Called niche that specializes in health informatics, so application of computer science on healthcare to somehow improve patient management hopefully.",
            "But that's kind of the context in which this project would place particular so-called clinical clinical practice guidelines or CPG.",
            "Our evidence based recommendations for dealing with a certain illness and these include context sensitive care recommendations, quite elaborate clinical workflows of all sorts of relevant activities.",
            "These are quite extensive.",
            "Think in a range of 100 plus pages for one illness, right?",
            "So quite extensive updated every few years as well.",
            "Based on the latest evidence.",
            "So you can imagine there quite hard to follow to in detail for anyone.",
            "So to support a physician in following these clinical practice guidelines, so-called clinical decision support systems are loaded with these kinds of computerized CPG and support the physician in the related decision processes like diagnosis, prognoses, treatment and so on.",
            "Using the best and latest available knowledge on that illness.",
            "A number of computer eyes Asian languages exist which are actually used in practice.",
            "Glyph pro forma as blue as the A star, which is actually based on Hasbro.",
            "I think our research group introduced to CPG ontology.",
            "To represent CPG there's a number of implementations as well.",
            "First order logic, a constraint logic, programming able to DL.",
            "So most of our focus in our research group is on using out to DL to try and represent an execute CPG in a CD SS.",
            "So just as an example, and."
        ],
        [
            "Not supposed to be able to read these labels that are way too small is just an example.",
            "For instance, this branch here would be represented using the CPG ontology as such, right?",
            "So you would have an entry point that has a task that task as a decision option.",
            "The task is followed by XY and Z and so on and so on, but you get the idea.",
            "Second major piece of context lies in supporting the patient to better self manage their illness.",
            "Involving them in their own long-term care, in other words.",
            "Two important goals here are to increase the patient's health.",
            "Their own self sufficiency and their quality of life right?",
            "By involving them in their own care and secondary to reduce the health care costs as well.",
            "In this context, we developed so-called mobile patient diary.",
            "This was designed to help patients with atrial fibrillation better self manage.",
            "Their illness or we see some screenshots here and so to realize these two goals here, that two important features firstly allows them to collect self collector health data at anytime in place, either manually or automatically using those kinds of fancy Bluetooth measurement devices right there, they put them on.",
            "They talked to the smartphone and enter the blood pressure and heart rate automatically into the patient diary.",
            "And Secondly."
        ],
        [
            "This is a feature will be focusing on the diary aloud for instant health alerts and feedback based on a clinical decision support that was deployed in the background.",
            "Right on this decision, support was both for the physician, right?",
            "They had this fancy website dashboard where they could manage their AF patient population as well as the patient, right?",
            "They would get health alerts as well directly on the mobile application."
        ],
        [
            "So there were."
        ],
        [
            "Important requirements that this patient diary needed to adhere to.",
            "First and foremost.",
            "Even in light of limited or no connectivity or response latency and so on, the usage of the patient diary should not be encumbered, right?",
            "Even if there is no connectivity at all, patients should still be able to use the diary, right?",
            "So to realize that we deployed a local clinical decision support directly on the mobile app, right?",
            "So completely independently of connectivity.",
            "This local decision support allows the patient to use the diary.",
            "And this led to a distributed setup.",
            "So locally on the mobile platform we had this CD S that was loaded with.",
            "Time sensitive, time sensitive, part of the CPG apart of the CPG that indicates adverse acute health events like your your vitals are too high.",
            "Your values are too low.",
            "You have some weird symptoms that were were worried about right?",
            "So in that case the app can directly issue those alerts that a patient without having to 1st go to a server and ask the server about these.",
            "These kinds of health condition.",
            "And then at the remote site we still had our heavyweight clinical decision support right, and this would be for for less time sensitive processes.",
            "For instance the physician.",
            "For instance, saying oh, maybe based on this latest lab results, I should change the patient's prescription in the next few weeks or so, right?",
            "Nothing too time sensitive.",
            "So in our mobile patient diary architecture, this is where the decision support kind of fits it.",
            "Right on a rule engine that was loaded with a rule based version of that part of the CPG it would."
        ],
        [
            "Give inferences back to the clinical decision support that then may issue health alerts depending on those inferences.",
            "So because our CDs relied on a rule engine, and because it needed to be deployed on a mobile platform, this got us interested in the performance of rule engines on mobile.",
            "So we benchmarked and these two works.",
            "We benchmark for rule engines on Android or if query RDF, store JS Noodles and Andro Gina.",
            "We use the rule set in data set that was specific for our problem, right managing atrial fibrillation.",
            "Regrettably, the results were quite sobering.",
            "Regardless of the fancy specifications you read on these smartphones compared to a PC, there are really, at least for these kinds of processes a lot slower.",
            "Now, of course, these rule engines were not designed for mobile platforms, right?",
            "You can see some of these first three or actually JavaScript engines that we deployed on mobile.",
            "There's a reason for that.",
            "Our patient diary was actually written in JavaScript.",
            "That's because we relied on.",
            "On a cross platform development tool called Apache Cordova that required you to write your codebase in in JavaScript.",
            "Android Gina was really the only Java reason are there and that's actually a port of Apache Gina to Android not written by us but written by by other people that be reused.",
            "In five here we also introduced mobile benchmarking framework that allows others to reproduce these experiments or run their own experiments.",
            "So this kind of got us interested."
        ],
        [
            "Well, how to support ontology based reasoning on mobile right?",
            "For instance, our CPG ontology with its altitude yellow implementation.",
            "How can we support that?",
            "So in the state of the art on ontology based reasoning on mobile, there is quite some work done there already.",
            "I'll to DL has been deemed at least up until now too.",
            "Resource intensive, recent empirical and quite comprehensive work by Bulbhead showed that PC outperforms Android by a few orders of magnitude in many cases.",
            "In this work, reasoners like Hermit Pellet Jay Fact and so on were manually ported to Android and then just compared to the PC version as so see how they did.",
            "As a result, most of the works in a state of yard are rule based.",
            "Their rule based approaches to implementing ontology reasoning either using a custom entailment ruleset or using the W. 3C standard L2RL.",
            "Which brings us to our two or all right.",
            "It's a very promising solution for ontology based reasoning on mobile.",
            "It promises scalable reasoning without sacrificing too much expressivity.",
            "That sounds good, actually.",
            "We are also interested in a few other features, quote unquote of outoor L is that it very easily allows you to adjust the reasoning complexity to your scenario into your needs by simply selecting subsets of that of those rules.",
            "Those rules are simply text files.",
            "Simply a text file, so we can simply drop the rules that you don't need and then just use that subset and then hopefully improve performance.",
            "Secondly, you can simply use a number 2 or L rule set or subset of those rules, add them to an existing ruleset, for instance for implementing CPG and all of a sudden you have ontology based reasoning features, right.",
            "Also very nifty application of L2 or L."
        ],
        [
            "So our first approach in this vein was to optimize the usage of that out to rule set on.",
            "On mobile, so we presented a methodology in this in this reference here to select subsets of the L2 RL ruleset based on criteria such as the stability of the ontology and conformance to the L2 or L specification.",
            "So I won't go into too much detail here because it's not the content of our paper.",
            "The first step here was to select another two or L rule subset that is still complete with regards to the outdoor L rule set.",
            "So it leaves out a number of rules, but it still yields the exact same inferences.",
            "So we introduced a concept called instance completeness.",
            "That derives all instance inferences that are covered by all too well, not necessarily all schema inferences, but still all the instance inference.",
            "The second step we differentiate between purpose and reference based subsets.",
            "So in in the first effort here we had a rule set that referred to both instances and schema assertions.",
            "A second rule set that only referred to schema assertions.",
            "So this allows for the kind of pipeline where inferences schema is applied initially and whenever you ontology changes then we drop that rule set and whenever instances are added we simply have to.",
            "Apply inference instances because inference schema doesn't even reference instances.",
            "So why we execute that rule set?",
            "Robbers quite led to quite an optimization.",
            "And we I believe we supplied a proof that proves that this actually retains completeness with regards to L2RL.",
            "We also applied a domain based ruleset selection, so where we have this forward chaining algorithm that looks at the ontology, it looks at the data set an it's simply selects rules that are relevant for that ontology and data set.",
            "So clearly these two efforts, they're useful when your ontology is relatively stable, right?",
            "If schema assertions are added often, you'll have to reapply inference schema anyway quite frequently.",
            "So kind of defeats the purpose.",
            "The same for domain based.",
            "You would have to reapply this whenever the schema changes whenever the important data patterns in your data change, you would have to re select your domain specific ruleset so mostly mostly useful statement Paula Geez.",
            "Anyway, For more information I would refer to this reference.",
            "So."
        ],
        [
            "And our second effort was to really get into the nitty gritty and look at the underlying reasoning mechanism and see to what extent that is optimal for an hour 2 RL ruleset.",
            "So this is an example.",
            "I will torreilles rule pertaining to to an implementation of intersection and this would lead to the following greedy structure.",
            "So we focused on the RTI algorithm because reedy or variant of RTI is still the mainstay of most rule based systems.",
            "So in a nutshell, each rule premise corresponds to an Alpha node.",
            "With an Alpha memory, keeping all the facts that were matched by that Alpha node joins are represented by better nodes.",
            "For instance, join represented by these shared variable X is represented by node beta one to join, represented by shared variable sees represented by node beta 2.",
            "Similar to before, a beta memory keeps the results of each join.",
            "On the rule, action is represented by the terminal node, in our case terminal node Rule action is adding a new fact, you are an inferred fact to our two RDF store.",
            "For instance, a token a two is added at time T1 is matched by node A2.",
            "And would be stored because it's matched stored in its Alpha memory.",
            "So this token would essentially keep concrete values for the variables in that premise, right?",
            "It's essentially an incoming RDF triple.",
            "Time T2, Token A1 is injected into the network stored matched by node A1, stored in its memory.",
            "At this point, token beta one has two new tokens, one at each of its inputs, so it will attempt to join between these two tokens.",
            "This Joyner successful so to join Token is stored in its beta memory.",
            "Right time T3 Token a three is injected into the network, stored in the intermediate in the Alpha memory.",
            "Same as before node beta two now has two new tokens at its inputs.",
            "Attempts to join that is successful.",
            "Leading to talk in a 123, but in fact as concrete values for all of the variables in our in our rule.",
            "And so this token can then be used to infer new fact.",
            "In a nutshell, that's that's essentially Holidays.",
            "So this this aspect is actually very important.",
            "The fact that it stores this join result right to calculate the A123 token.",
            "It doesn't need to re execute this previous join, it simply reuses.",
            "The the previous result that was already stored, so it's a classical example of trading memory for performance, right?",
            "This incremental nature also makes it useful in dynamic scenarios.",
            "There are some examples of really being used in RDF stream reasoning, for instance."
        ],
        [
            "So there are some issues when I actually just naively apply RTI to to work with the L2 or L ruleset, firstly, out to RL features.",
            "Many generic rule premises.",
            "This kind of wild card premise actually occurs 40 times in the other two or L ruleset and in in other kinds of rule sets.",
            "This is typically not the case.",
            "Rules are are more concrete.",
            "So we remember how this works, so any fact that is matched by an Alpha node will be stored in its memory.",
            "So in this case this memory would essentially store all of the incoming facts, so its memory would duplicate all the other data that exists in your in your routine network.",
            "Right, so this this premise memory would essentially subsume all the other memory contents in your network.",
            "Secondly, in a lot of semantic web scenarios, they already involve an RDF store, be it for querying, be it for serving a website, or a query endpoint, and also used to load data into the RDF into our reaching network.",
            "So in that vein, the Alpha memories will always duplicate data from that store, right?",
            "So that's another issue.",
            "So our proposed solution is we T."
        ],
        [
            "Cool, another name implies we pull Alpha memories into a single shared memory, so all the Alpha memories are simply combined into one big large memory pool, which avoids that duplication of tokens that I talked about.",
            "Secondly, we allow reusing that existing RDF store.",
            "As a shared memory pool.",
            "Which well avoids duplication with that multipurpose RDF store right because we simply directly reuse that that that store.",
            "So just as an example, how how Reedy pool works.",
            "So this is kind of how we structure would look like in an RTI pool instead of actual concrete Alpha memories.",
            "We keep virtual Alpha memories and this is based on word by Hanson at I'll some some time ago.",
            "And this virtual Alpha memory keeps a mask on the shared memory pool, and that mask is simply the premise associated with the Alpha, no?",
            "So and every join then becomes a search constraint search in your memory pool.",
            "So say at time TX Token A12 is injected or at least appears here in the network node beta two will attempt to join it.",
            "So what happens then?",
            "So in this case Token A12 supplies the following concrete values for the variables in your rule.",
            "So, uh, search constraint will be generated based on both the mask and the concrete variables in your and create value concrete values in your token.",
            "So in this case RDF type is used as agreed value for the predicate and since the shared variable here is C, it's concrete value will be used in the Geo location.",
            "This will be used to query the data set and some result will be returned.",
            "So a single search both targets only relevant tokens, right tokens that are relevant to this Alpha node.",
            "And performs the join at the same time as well, right by filling in that concrete value for the shared variable.",
            "Now I don't know itself with."
        ],
        [
            "People there also issues refund.",
            "In particular, each join, as I showed, involves accessing quite a large shared memory pool.",
            "How large it is depends on the store, obviously, but before this was a relatively small Alpha memory that needed to be accessed right now is this huge shared memory pool.",
            "So we saw huge increases in memory access times.",
            "So the solution here was to allow for configurations that re trade memory with performance, right?",
            "Clearly the degree of duplication and Alpha memory, right?",
            "Initial problem that we tried to solve depends on the selectivity of the premise, right?",
            "So we introduced selectivity threshold that whenever the threshold is exceeded we will use a virtual Alpha memory.",
            "If it is not exceeded, we will use a regular Alpha memory, right?",
            "And so this selectively threshold represents the amount of data.",
            "That would be selected by the premise from a from our data set, right?",
            "A second issue that we encountered and I won't go into detail on is the so called reciprocal joint issue.",
            "In this, this occurs when you start from a pre loaded RDF store, right?",
            "You you reuse an existing RDF storm.",
            "That's multi-purpose Ann, and it comes pre populated.",
            "We had some issues there I would refer to the paper for for what we found the solution to that.",
            "So we ran a bunch of experiments."
        ],
        [
            "To validate, obviously our our approach reasoning systems one the first one is really based on our baseline system.",
            "Was Apache Gina and Andrew Gina which is a ported version of Gina to add to Android and we extended those with standard optimizations to make it kind of more of a fair comparison.",
            "These optimizations involved reusing RTI nodes and memories where possible, indexing the memories, join ordering and so on.",
            "This was not standard in Apache Gina, so we extended that.",
            "Or we implemented that ingenia Ricky Pool that implements our algorithm.",
            "On top of this wikibase.",
            "We use this rule and data set.",
            "We use these particular platforms for details.",
            "Again, I would I would refer to the paper.",
            "I won't go over them.",
            "One important note here is 'cause we used slightly different sets of ontologies.",
            "For PC and mobile, the performance results are not directly compareable.",
            "So when I show you the results you will see.",
            "You will think of mobile is not that much slower than PC.",
            "That's because the results are not come parables because we use different ontologies for mobile.",
            "In hindsight that may not have been the best idea, but we just wanted to try everything we could for PC and that maybe we were able to try more ontologies for PC because PC has more memory, right?",
            "We have some configure."
        ],
        [
            "Nations that we benchmarked.",
            "Ricky base obviously, as I mentioned, that's the baseline system.",
            "Read the full pool simply always uses a virtual Alpha memory.",
            "Reach Alpha node retreat part Pool only uses a virtual memory once that selectively threshold is exceeded, right?",
            "And we try these values and orthogonally to this we.",
            "Um?",
            "We consider two scenarios, semantic web scenarios, one where a shared memory pool is introduced for the sole purpose of supporting RTI pool and one where an existing RDF store is reused to support review pool right?",
            "Well, this is how the result table looks like, so some notes on that."
        ],
        [
            "A total memory size represents the size of the Alpha memory contents plus the size of the RDF store.",
            "If one is needed right for Rekey base, it does not need an RDF store, so we don't actually include anything extra under the total size.",
            "Here, for instance, you can see as a threshold increases more and more regular memories will be used and less and less virtual memories right, which which is of course the result of that threshold, right?",
            "Median so all the results are like median, min, Max respectively.",
            "This refers to ontologies that led to the median minimum and maximum number of Alpha memory tokens actually, so it refers to the size of the ontology or at least the size that it takes up in Oreti network.",
            "So if you look at our first.",
            "How much time OK, 3 minutes if you look at our first semantic web scenario where we introduced this separate memory pool for the sole purpose of of 3D pool, we need to consider the total memory size, right?",
            "Because we need this RDF store for for our algorithm, right?",
            "So to be fair, we need to include that in a memory size of.",
            "Our approach takes up.",
            "So.",
            "Even then."
        ],
        [
            "We see a significant memory saving.",
            "Between Ricky Basin, RTI pool, around 60% less memory is used by RTI.",
            "Full pool at least."
        ],
        [
            "Meridian Park pool.",
            "With the threshold of .1, we see that there's still memory savings right?",
            "30% of memory is less is used less than than RTI base."
        ],
        [
            "For the other ones where you have a higher threshold.",
            "You see, the memory savings dropped significantly and actually approach there.",
            "The memory savings of the initial baseline system, right?",
            "So it's not that not that and something we were too happy with."
        ],
        [
            "We look at performance.",
            "This really drove us to introduce that selectivity threshold.",
            "It's really dramatic with 3D full pool and actually performance reduces with a factor of 3 * 3 or 2.8 depending on the platform."
        ],
        [
            "It's actually a lot better for these part pool configurations where you have this selectivity threshold."
        ],
        [
            "So for this configuration we found we depart tool with a .1 selectivity threshold to perform or have the best memory and performance balance at least.",
            "Have good memory savings.",
            "Ana relatively limited performance penalty.",
            "Obviously on mobile plus 4.3 seconds.",
            "Not too good, but on PC it's still still acceptable."
        ],
        [
            "For our second scenario, where we reuse an existing RDF store right to support our ET pool algorithm, we don't consider the total memory size right because they already have store already exists.",
            "We just reuse it, right?",
            "So we don't include it when we're comparing memory sizes.",
            "Here the memory savings are dramatic, right?",
            "Let me.",
            "Compare with the full pool 3D base.",
            "Maybe save about 98.7% memory for the median ontology."
        ],
        [
            "Just to summarize, for this scenario we foundry depart pool with a .5 selectively threshold to be best in memory performance balance, significant memory savings 40%, whereas a performance penalty is quite much lower, right 1.5 seconds for mobile, whereas before it was.",
            "For second something also, you have a quick note about these reasoning times are quite high.",
            "Even on PC.",
            "This is because we introduced we include, I think around 7 quite resource intensive ontologies.",
            "If we left those out it really the performance improves by an order of magnitude that would be 1.5 seconds instead of 15 is just we included all ontologies that didn't lead to memory issues or that didn't lead to reasoning times longer than 10 minutes.",
            "That's why these times are so high."
        ],
        [
            "So there's plenty of future work.",
            "Clearly our work in this context is quite preliminary.",
            "Didn't really do anything groundbreaking up until now.",
            "I already showed you this figure right where the wild card premise SPO essentially subsumes all other memories.",
            "And our current solution is to simply while we pool all memories to avoid duplication resulting from that subsumption.",
            "Perhaps a better solution would be to directly represent that subsumption in your memory structure that you use nested memory structure, right?",
            "So, for instance, this is what the memory for SPO would look like.",
            "We indexed on SMP because those are shared variables.",
            "For instance for S, for instance, where this subject value we list tokens 1, two and three, and then for RDF type we will also have a number of tokens, but instead of listing those.",
            "Or simply directly include the memory structure for S, RDF type.",
            "Oh right, that has a concrete value for RDF type in the predicate position.",
            "Right, so we essentially avoid duplication by simply referencing that memory structure, and the same goes for S all same.",
            "Oh, we can do them the same thing in that nested memory structure, right?",
            "For object property as a concrete object value.",
            "We can then include directly to memory structure for S, RDF type owl object property, right.",
            "Nicely reflecting that subsumption that I showed in the in the above figure.",
            "2nd.",
            "An opportunity for."
        ],
        [
            "Future work is to virtually materialize out to semantics.",
            "Essentially, there we would be performing some sort of domain specific reasoning extended with ontology based features, right?",
            "So the operators match ontology.",
            "Sorry, match operator.",
            "The join operator would be extended with semantic features to avoid explicit materialization of those of those infer triples.",
            "I for instance say here we have an incoming token with property D. Obviously this doesn't match property a, but actually the property hierarchy says property.",
            "Deze sub property of property A.",
            "So let's say that we would use able to relative explicitly materialize these.",
            "Inferences these two triples would be materialized, and then X would be linked to Y1 using property a right simply so it would be matched by this Alpha node.",
            "And well, would you store in the memory and so on and so forth.",
            "So why not just extend that match operator to automatically take into account that property hierarchy?",
            "Right, there's many, many other opportunities also along this line of virtually materializing these semantics."
        ],
        [
            "So conclusion for mobile reasoning for clinical decision support, we did in that context it's useful in other ways.",
            "In other contexts as well, right need to use less remote cloud resources?",
            "You reduce bandwidth usage 'cause you're not constantly talking to a server.",
            "We indicated that special consideration is needed for reasoning on mobile platforms.",
            "This let us to introduce our two approaches 1.",
            "Automatic selection of outdoor L Rulesets, another one to directly optimize the RTI algorithm for our two RL.",
            "And we orthogonally consider these two dimensions or scenarios rather the indicated what the best configuration was, and that's about it.",
            "So the references here on in the presentation.",
            "Thank you for your attention.",
            "So, have you compared this to RDF folks?",
            "We haven't compared this to to any other system.",
            "The goal here was really to compared to a baseline that is outfitted with standard optimizations.",
            "Whether it perform better, it's not a mature system, right?",
            "RDF Fox.",
            "All all the other kinds of more advanced RDF stores, I don't think we would perform better.",
            "Let's say that this was really focused on a particular technique right pooling Alpha memories and seeing to what extent that would improve the baseline, not necessarily whether this would be better than existing mature systems with other optimizations in place.",
            "And so why didn't you implement that optimization into an existing system?",
            "We did.",
            "We extended Apache, Gina and Regina.",
            "With those optimization, I mean a state of the art system.",
            "Well, yeah, the extent to which Apache Gina can be considered state of the art is maybe an object of discussion.",
            "Yeah, that's about it really.",
            "It had it had a well too good, too easy to understand RTI implementation.",
            "That's really why we chose it.",
            "It was pretty basic, which is also what we wanted.",
            "We wouldn't want to overcomplicate things because our goal was to really see if that technique performs better.",
            "Not to see how it interacts with other kinds of optimizations.",
            "Did are you also considering and free logic?",
            "In what?",
            "And free logic, no mccurley not know.",
            "Can I have one also, um, so your desktop and mobile devices?",
            "So what about the more constrained devices like no sense or something like this?",
            "So is this applicable or you have to reduce the more than the reset?",
            "That's a good question.",
            "We haven't really considered any kind of more resource constrained systems.",
            "That's really because of the context in which this work took place, right in this mobile patient diary.",
            "But on smartphones we were more interested in seeing how that would be possible for sure deploying it on sensors and the like would be very interesting, but it's certainly future work.",
            "Thank you.",
            "If you have any other questions, so let's thanks in this picture again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is William Baldwin, so I'm here to present a paper that we published at this conference called Optimizing Semantic Reasoning on memory constrained platforms.",
                    "label": 1
                },
                {
                    "sent": "Due to using the RTI algorithm.",
                    "label": 0
                },
                {
                    "sent": "So obviously the presentation will focus on the contents of the paper, but will also talk about some other work that we've done in this general domain of semantic reasoning on resource constrained platforms.",
                    "label": 0
                },
                {
                    "sent": "Let's start with some can't.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm a member of a research group.",
                    "label": 0
                },
                {
                    "sent": "Called niche that specializes in health informatics, so application of computer science on healthcare to somehow improve patient management hopefully.",
                    "label": 0
                },
                {
                    "sent": "But that's kind of the context in which this project would place particular so-called clinical clinical practice guidelines or CPG.",
                    "label": 0
                },
                {
                    "sent": "Our evidence based recommendations for dealing with a certain illness and these include context sensitive care recommendations, quite elaborate clinical workflows of all sorts of relevant activities.",
                    "label": 0
                },
                {
                    "sent": "These are quite extensive.",
                    "label": 0
                },
                {
                    "sent": "Think in a range of 100 plus pages for one illness, right?",
                    "label": 0
                },
                {
                    "sent": "So quite extensive updated every few years as well.",
                    "label": 1
                },
                {
                    "sent": "Based on the latest evidence.",
                    "label": 1
                },
                {
                    "sent": "So you can imagine there quite hard to follow to in detail for anyone.",
                    "label": 0
                },
                {
                    "sent": "So to support a physician in following these clinical practice guidelines, so-called clinical decision support systems are loaded with these kinds of computerized CPG and support the physician in the related decision processes like diagnosis, prognoses, treatment and so on.",
                    "label": 1
                },
                {
                    "sent": "Using the best and latest available knowledge on that illness.",
                    "label": 0
                },
                {
                    "sent": "A number of computer eyes Asian languages exist which are actually used in practice.",
                    "label": 0
                },
                {
                    "sent": "Glyph pro forma as blue as the A star, which is actually based on Hasbro.",
                    "label": 0
                },
                {
                    "sent": "I think our research group introduced to CPG ontology.",
                    "label": 1
                },
                {
                    "sent": "To represent CPG there's a number of implementations as well.",
                    "label": 0
                },
                {
                    "sent": "First order logic, a constraint logic, programming able to DL.",
                    "label": 0
                },
                {
                    "sent": "So most of our focus in our research group is on using out to DL to try and represent an execute CPG in a CD SS.",
                    "label": 0
                },
                {
                    "sent": "So just as an example, and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not supposed to be able to read these labels that are way too small is just an example.",
                    "label": 0
                },
                {
                    "sent": "For instance, this branch here would be represented using the CPG ontology as such, right?",
                    "label": 0
                },
                {
                    "sent": "So you would have an entry point that has a task that task as a decision option.",
                    "label": 0
                },
                {
                    "sent": "The task is followed by XY and Z and so on and so on, but you get the idea.",
                    "label": 0
                },
                {
                    "sent": "Second major piece of context lies in supporting the patient to better self manage their illness.",
                    "label": 0
                },
                {
                    "sent": "Involving them in their own long-term care, in other words.",
                    "label": 0
                },
                {
                    "sent": "Two important goals here are to increase the patient's health.",
                    "label": 0
                },
                {
                    "sent": "Their own self sufficiency and their quality of life right?",
                    "label": 0
                },
                {
                    "sent": "By involving them in their own care and secondary to reduce the health care costs as well.",
                    "label": 0
                },
                {
                    "sent": "In this context, we developed so-called mobile patient diary.",
                    "label": 0
                },
                {
                    "sent": "This was designed to help patients with atrial fibrillation better self manage.",
                    "label": 0
                },
                {
                    "sent": "Their illness or we see some screenshots here and so to realize these two goals here, that two important features firstly allows them to collect self collector health data at anytime in place, either manually or automatically using those kinds of fancy Bluetooth measurement devices right there, they put them on.",
                    "label": 0
                },
                {
                    "sent": "They talked to the smartphone and enter the blood pressure and heart rate automatically into the patient diary.",
                    "label": 0
                },
                {
                    "sent": "And Secondly.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a feature will be focusing on the diary aloud for instant health alerts and feedback based on a clinical decision support that was deployed in the background.",
                    "label": 1
                },
                {
                    "sent": "Right on this decision, support was both for the physician, right?",
                    "label": 0
                },
                {
                    "sent": "They had this fancy website dashboard where they could manage their AF patient population as well as the patient, right?",
                    "label": 0
                },
                {
                    "sent": "They would get health alerts as well directly on the mobile application.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there were.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important requirements that this patient diary needed to adhere to.",
                    "label": 0
                },
                {
                    "sent": "First and foremost.",
                    "label": 0
                },
                {
                    "sent": "Even in light of limited or no connectivity or response latency and so on, the usage of the patient diary should not be encumbered, right?",
                    "label": 1
                },
                {
                    "sent": "Even if there is no connectivity at all, patients should still be able to use the diary, right?",
                    "label": 1
                },
                {
                    "sent": "So to realize that we deployed a local clinical decision support directly on the mobile app, right?",
                    "label": 1
                },
                {
                    "sent": "So completely independently of connectivity.",
                    "label": 0
                },
                {
                    "sent": "This local decision support allows the patient to use the diary.",
                    "label": 0
                },
                {
                    "sent": "And this led to a distributed setup.",
                    "label": 0
                },
                {
                    "sent": "So locally on the mobile platform we had this CD S that was loaded with.",
                    "label": 0
                },
                {
                    "sent": "Time sensitive, time sensitive, part of the CPG apart of the CPG that indicates adverse acute health events like your your vitals are too high.",
                    "label": 0
                },
                {
                    "sent": "Your values are too low.",
                    "label": 1
                },
                {
                    "sent": "You have some weird symptoms that were were worried about right?",
                    "label": 0
                },
                {
                    "sent": "So in that case the app can directly issue those alerts that a patient without having to 1st go to a server and ask the server about these.",
                    "label": 0
                },
                {
                    "sent": "These kinds of health condition.",
                    "label": 0
                },
                {
                    "sent": "And then at the remote site we still had our heavyweight clinical decision support right, and this would be for for less time sensitive processes.",
                    "label": 0
                },
                {
                    "sent": "For instance the physician.",
                    "label": 0
                },
                {
                    "sent": "For instance, saying oh, maybe based on this latest lab results, I should change the patient's prescription in the next few weeks or so, right?",
                    "label": 0
                },
                {
                    "sent": "Nothing too time sensitive.",
                    "label": 0
                },
                {
                    "sent": "So in our mobile patient diary architecture, this is where the decision support kind of fits it.",
                    "label": 0
                },
                {
                    "sent": "Right on a rule engine that was loaded with a rule based version of that part of the CPG it would.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give inferences back to the clinical decision support that then may issue health alerts depending on those inferences.",
                    "label": 0
                },
                {
                    "sent": "So because our CDs relied on a rule engine, and because it needed to be deployed on a mobile platform, this got us interested in the performance of rule engines on mobile.",
                    "label": 1
                },
                {
                    "sent": "So we benchmarked and these two works.",
                    "label": 1
                },
                {
                    "sent": "We benchmark for rule engines on Android or if query RDF, store JS Noodles and Andro Gina.",
                    "label": 1
                },
                {
                    "sent": "We use the rule set in data set that was specific for our problem, right managing atrial fibrillation.",
                    "label": 0
                },
                {
                    "sent": "Regrettably, the results were quite sobering.",
                    "label": 0
                },
                {
                    "sent": "Regardless of the fancy specifications you read on these smartphones compared to a PC, there are really, at least for these kinds of processes a lot slower.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, these rule engines were not designed for mobile platforms, right?",
                    "label": 1
                },
                {
                    "sent": "You can see some of these first three or actually JavaScript engines that we deployed on mobile.",
                    "label": 0
                },
                {
                    "sent": "There's a reason for that.",
                    "label": 0
                },
                {
                    "sent": "Our patient diary was actually written in JavaScript.",
                    "label": 0
                },
                {
                    "sent": "That's because we relied on.",
                    "label": 0
                },
                {
                    "sent": "On a cross platform development tool called Apache Cordova that required you to write your codebase in in JavaScript.",
                    "label": 0
                },
                {
                    "sent": "Android Gina was really the only Java reason are there and that's actually a port of Apache Gina to Android not written by us but written by by other people that be reused.",
                    "label": 0
                },
                {
                    "sent": "In five here we also introduced mobile benchmarking framework that allows others to reproduce these experiments or run their own experiments.",
                    "label": 0
                },
                {
                    "sent": "So this kind of got us interested.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, how to support ontology based reasoning on mobile right?",
                    "label": 0
                },
                {
                    "sent": "For instance, our CPG ontology with its altitude yellow implementation.",
                    "label": 0
                },
                {
                    "sent": "How can we support that?",
                    "label": 0
                },
                {
                    "sent": "So in the state of the art on ontology based reasoning on mobile, there is quite some work done there already.",
                    "label": 0
                },
                {
                    "sent": "I'll to DL has been deemed at least up until now too.",
                    "label": 0
                },
                {
                    "sent": "Resource intensive, recent empirical and quite comprehensive work by Bulbhead showed that PC outperforms Android by a few orders of magnitude in many cases.",
                    "label": 1
                },
                {
                    "sent": "In this work, reasoners like Hermit Pellet Jay Fact and so on were manually ported to Android and then just compared to the PC version as so see how they did.",
                    "label": 0
                },
                {
                    "sent": "As a result, most of the works in a state of yard are rule based.",
                    "label": 0
                },
                {
                    "sent": "Their rule based approaches to implementing ontology reasoning either using a custom entailment ruleset or using the W. 3C standard L2RL.",
                    "label": 0
                },
                {
                    "sent": "Which brings us to our two or all right.",
                    "label": 0
                },
                {
                    "sent": "It's a very promising solution for ontology based reasoning on mobile.",
                    "label": 0
                },
                {
                    "sent": "It promises scalable reasoning without sacrificing too much expressivity.",
                    "label": 1
                },
                {
                    "sent": "That sounds good, actually.",
                    "label": 0
                },
                {
                    "sent": "We are also interested in a few other features, quote unquote of outoor L is that it very easily allows you to adjust the reasoning complexity to your scenario into your needs by simply selecting subsets of that of those rules.",
                    "label": 0
                },
                {
                    "sent": "Those rules are simply text files.",
                    "label": 0
                },
                {
                    "sent": "Simply a text file, so we can simply drop the rules that you don't need and then just use that subset and then hopefully improve performance.",
                    "label": 0
                },
                {
                    "sent": "Secondly, you can simply use a number 2 or L rule set or subset of those rules, add them to an existing ruleset, for instance for implementing CPG and all of a sudden you have ontology based reasoning features, right.",
                    "label": 0
                },
                {
                    "sent": "Also very nifty application of L2 or L.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our first approach in this vein was to optimize the usage of that out to rule set on.",
                    "label": 0
                },
                {
                    "sent": "On mobile, so we presented a methodology in this in this reference here to select subsets of the L2 RL ruleset based on criteria such as the stability of the ontology and conformance to the L2 or L specification.",
                    "label": 1
                },
                {
                    "sent": "So I won't go into too much detail here because it's not the content of our paper.",
                    "label": 0
                },
                {
                    "sent": "The first step here was to select another two or L rule subset that is still complete with regards to the outdoor L rule set.",
                    "label": 0
                },
                {
                    "sent": "So it leaves out a number of rules, but it still yields the exact same inferences.",
                    "label": 0
                },
                {
                    "sent": "So we introduced a concept called instance completeness.",
                    "label": 0
                },
                {
                    "sent": "That derives all instance inferences that are covered by all too well, not necessarily all schema inferences, but still all the instance inference.",
                    "label": 1
                },
                {
                    "sent": "The second step we differentiate between purpose and reference based subsets.",
                    "label": 0
                },
                {
                    "sent": "So in in the first effort here we had a rule set that referred to both instances and schema assertions.",
                    "label": 0
                },
                {
                    "sent": "A second rule set that only referred to schema assertions.",
                    "label": 0
                },
                {
                    "sent": "So this allows for the kind of pipeline where inferences schema is applied initially and whenever you ontology changes then we drop that rule set and whenever instances are added we simply have to.",
                    "label": 0
                },
                {
                    "sent": "Apply inference instances because inference schema doesn't even reference instances.",
                    "label": 0
                },
                {
                    "sent": "So why we execute that rule set?",
                    "label": 0
                },
                {
                    "sent": "Robbers quite led to quite an optimization.",
                    "label": 0
                },
                {
                    "sent": "And we I believe we supplied a proof that proves that this actually retains completeness with regards to L2RL.",
                    "label": 0
                },
                {
                    "sent": "We also applied a domain based ruleset selection, so where we have this forward chaining algorithm that looks at the ontology, it looks at the data set an it's simply selects rules that are relevant for that ontology and data set.",
                    "label": 0
                },
                {
                    "sent": "So clearly these two efforts, they're useful when your ontology is relatively stable, right?",
                    "label": 0
                },
                {
                    "sent": "If schema assertions are added often, you'll have to reapply inference schema anyway quite frequently.",
                    "label": 0
                },
                {
                    "sent": "So kind of defeats the purpose.",
                    "label": 0
                },
                {
                    "sent": "The same for domain based.",
                    "label": 0
                },
                {
                    "sent": "You would have to reapply this whenever the schema changes whenever the important data patterns in your data change, you would have to re select your domain specific ruleset so mostly mostly useful statement Paula Geez.",
                    "label": 0
                },
                {
                    "sent": "Anyway, For more information I would refer to this reference.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our second effort was to really get into the nitty gritty and look at the underlying reasoning mechanism and see to what extent that is optimal for an hour 2 RL ruleset.",
                    "label": 0
                },
                {
                    "sent": "So this is an example.",
                    "label": 0
                },
                {
                    "sent": "I will torreilles rule pertaining to to an implementation of intersection and this would lead to the following greedy structure.",
                    "label": 0
                },
                {
                    "sent": "So we focused on the RTI algorithm because reedy or variant of RTI is still the mainstay of most rule based systems.",
                    "label": 0
                },
                {
                    "sent": "So in a nutshell, each rule premise corresponds to an Alpha node.",
                    "label": 0
                },
                {
                    "sent": "With an Alpha memory, keeping all the facts that were matched by that Alpha node joins are represented by better nodes.",
                    "label": 1
                },
                {
                    "sent": "For instance, join represented by these shared variable X is represented by node beta one to join, represented by shared variable sees represented by node beta 2.",
                    "label": 0
                },
                {
                    "sent": "Similar to before, a beta memory keeps the results of each join.",
                    "label": 0
                },
                {
                    "sent": "On the rule, action is represented by the terminal node, in our case terminal node Rule action is adding a new fact, you are an inferred fact to our two RDF store.",
                    "label": 0
                },
                {
                    "sent": "For instance, a token a two is added at time T1 is matched by node A2.",
                    "label": 0
                },
                {
                    "sent": "And would be stored because it's matched stored in its Alpha memory.",
                    "label": 0
                },
                {
                    "sent": "So this token would essentially keep concrete values for the variables in that premise, right?",
                    "label": 0
                },
                {
                    "sent": "It's essentially an incoming RDF triple.",
                    "label": 0
                },
                {
                    "sent": "Time T2, Token A1 is injected into the network stored matched by node A1, stored in its memory.",
                    "label": 0
                },
                {
                    "sent": "At this point, token beta one has two new tokens, one at each of its inputs, so it will attempt to join between these two tokens.",
                    "label": 0
                },
                {
                    "sent": "This Joyner successful so to join Token is stored in its beta memory.",
                    "label": 0
                },
                {
                    "sent": "Right time T3 Token a three is injected into the network, stored in the intermediate in the Alpha memory.",
                    "label": 0
                },
                {
                    "sent": "Same as before node beta two now has two new tokens at its inputs.",
                    "label": 0
                },
                {
                    "sent": "Attempts to join that is successful.",
                    "label": 0
                },
                {
                    "sent": "Leading to talk in a 123, but in fact as concrete values for all of the variables in our in our rule.",
                    "label": 0
                },
                {
                    "sent": "And so this token can then be used to infer new fact.",
                    "label": 0
                },
                {
                    "sent": "In a nutshell, that's that's essentially Holidays.",
                    "label": 0
                },
                {
                    "sent": "So this this aspect is actually very important.",
                    "label": 0
                },
                {
                    "sent": "The fact that it stores this join result right to calculate the A123 token.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to re execute this previous join, it simply reuses.",
                    "label": 0
                },
                {
                    "sent": "The the previous result that was already stored, so it's a classical example of trading memory for performance, right?",
                    "label": 1
                },
                {
                    "sent": "This incremental nature also makes it useful in dynamic scenarios.",
                    "label": 1
                },
                {
                    "sent": "There are some examples of really being used in RDF stream reasoning, for instance.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some issues when I actually just naively apply RTI to to work with the L2 or L ruleset, firstly, out to RL features.",
                    "label": 0
                },
                {
                    "sent": "Many generic rule premises.",
                    "label": 0
                },
                {
                    "sent": "This kind of wild card premise actually occurs 40 times in the other two or L ruleset and in in other kinds of rule sets.",
                    "label": 0
                },
                {
                    "sent": "This is typically not the case.",
                    "label": 0
                },
                {
                    "sent": "Rules are are more concrete.",
                    "label": 0
                },
                {
                    "sent": "So we remember how this works, so any fact that is matched by an Alpha node will be stored in its memory.",
                    "label": 0
                },
                {
                    "sent": "So in this case this memory would essentially store all of the incoming facts, so its memory would duplicate all the other data that exists in your in your routine network.",
                    "label": 0
                },
                {
                    "sent": "Right, so this this premise memory would essentially subsume all the other memory contents in your network.",
                    "label": 0
                },
                {
                    "sent": "Secondly, in a lot of semantic web scenarios, they already involve an RDF store, be it for querying, be it for serving a website, or a query endpoint, and also used to load data into the RDF into our reaching network.",
                    "label": 1
                },
                {
                    "sent": "So in that vein, the Alpha memories will always duplicate data from that store, right?",
                    "label": 0
                },
                {
                    "sent": "So that's another issue.",
                    "label": 0
                },
                {
                    "sent": "So our proposed solution is we T.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool, another name implies we pull Alpha memories into a single shared memory, so all the Alpha memories are simply combined into one big large memory pool, which avoids that duplication of tokens that I talked about.",
                    "label": 1
                },
                {
                    "sent": "Secondly, we allow reusing that existing RDF store.",
                    "label": 1
                },
                {
                    "sent": "As a shared memory pool.",
                    "label": 1
                },
                {
                    "sent": "Which well avoids duplication with that multipurpose RDF store right because we simply directly reuse that that that store.",
                    "label": 0
                },
                {
                    "sent": "So just as an example, how how Reedy pool works.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of how we structure would look like in an RTI pool instead of actual concrete Alpha memories.",
                    "label": 0
                },
                {
                    "sent": "We keep virtual Alpha memories and this is based on word by Hanson at I'll some some time ago.",
                    "label": 1
                },
                {
                    "sent": "And this virtual Alpha memory keeps a mask on the shared memory pool, and that mask is simply the premise associated with the Alpha, no?",
                    "label": 0
                },
                {
                    "sent": "So and every join then becomes a search constraint search in your memory pool.",
                    "label": 0
                },
                {
                    "sent": "So say at time TX Token A12 is injected or at least appears here in the network node beta two will attempt to join it.",
                    "label": 0
                },
                {
                    "sent": "So what happens then?",
                    "label": 0
                },
                {
                    "sent": "So in this case Token A12 supplies the following concrete values for the variables in your rule.",
                    "label": 0
                },
                {
                    "sent": "So, uh, search constraint will be generated based on both the mask and the concrete variables in your and create value concrete values in your token.",
                    "label": 0
                },
                {
                    "sent": "So in this case RDF type is used as agreed value for the predicate and since the shared variable here is C, it's concrete value will be used in the Geo location.",
                    "label": 0
                },
                {
                    "sent": "This will be used to query the data set and some result will be returned.",
                    "label": 0
                },
                {
                    "sent": "So a single search both targets only relevant tokens, right tokens that are relevant to this Alpha node.",
                    "label": 0
                },
                {
                    "sent": "And performs the join at the same time as well, right by filling in that concrete value for the shared variable.",
                    "label": 0
                },
                {
                    "sent": "Now I don't know itself with.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People there also issues refund.",
                    "label": 0
                },
                {
                    "sent": "In particular, each join, as I showed, involves accessing quite a large shared memory pool.",
                    "label": 0
                },
                {
                    "sent": "How large it is depends on the store, obviously, but before this was a relatively small Alpha memory that needed to be accessed right now is this huge shared memory pool.",
                    "label": 1
                },
                {
                    "sent": "So we saw huge increases in memory access times.",
                    "label": 0
                },
                {
                    "sent": "So the solution here was to allow for configurations that re trade memory with performance, right?",
                    "label": 1
                },
                {
                    "sent": "Clearly the degree of duplication and Alpha memory, right?",
                    "label": 1
                },
                {
                    "sent": "Initial problem that we tried to solve depends on the selectivity of the premise, right?",
                    "label": 0
                },
                {
                    "sent": "So we introduced selectivity threshold that whenever the threshold is exceeded we will use a virtual Alpha memory.",
                    "label": 0
                },
                {
                    "sent": "If it is not exceeded, we will use a regular Alpha memory, right?",
                    "label": 0
                },
                {
                    "sent": "And so this selectively threshold represents the amount of data.",
                    "label": 0
                },
                {
                    "sent": "That would be selected by the premise from a from our data set, right?",
                    "label": 0
                },
                {
                    "sent": "A second issue that we encountered and I won't go into detail on is the so called reciprocal joint issue.",
                    "label": 0
                },
                {
                    "sent": "In this, this occurs when you start from a pre loaded RDF store, right?",
                    "label": 0
                },
                {
                    "sent": "You you reuse an existing RDF storm.",
                    "label": 0
                },
                {
                    "sent": "That's multi-purpose Ann, and it comes pre populated.",
                    "label": 0
                },
                {
                    "sent": "We had some issues there I would refer to the paper for for what we found the solution to that.",
                    "label": 0
                },
                {
                    "sent": "So we ran a bunch of experiments.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To validate, obviously our our approach reasoning systems one the first one is really based on our baseline system.",
                    "label": 0
                },
                {
                    "sent": "Was Apache Gina and Andrew Gina which is a ported version of Gina to add to Android and we extended those with standard optimizations to make it kind of more of a fair comparison.",
                    "label": 0
                },
                {
                    "sent": "These optimizations involved reusing RTI nodes and memories where possible, indexing the memories, join ordering and so on.",
                    "label": 1
                },
                {
                    "sent": "This was not standard in Apache Gina, so we extended that.",
                    "label": 0
                },
                {
                    "sent": "Or we implemented that ingenia Ricky Pool that implements our algorithm.",
                    "label": 1
                },
                {
                    "sent": "On top of this wikibase.",
                    "label": 0
                },
                {
                    "sent": "We use this rule and data set.",
                    "label": 0
                },
                {
                    "sent": "We use these particular platforms for details.",
                    "label": 0
                },
                {
                    "sent": "Again, I would I would refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "I won't go over them.",
                    "label": 0
                },
                {
                    "sent": "One important note here is 'cause we used slightly different sets of ontologies.",
                    "label": 0
                },
                {
                    "sent": "For PC and mobile, the performance results are not directly compareable.",
                    "label": 0
                },
                {
                    "sent": "So when I show you the results you will see.",
                    "label": 0
                },
                {
                    "sent": "You will think of mobile is not that much slower than PC.",
                    "label": 0
                },
                {
                    "sent": "That's because the results are not come parables because we use different ontologies for mobile.",
                    "label": 0
                },
                {
                    "sent": "In hindsight that may not have been the best idea, but we just wanted to try everything we could for PC and that maybe we were able to try more ontologies for PC because PC has more memory, right?",
                    "label": 0
                },
                {
                    "sent": "We have some configure.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nations that we benchmarked.",
                    "label": 0
                },
                {
                    "sent": "Ricky base obviously, as I mentioned, that's the baseline system.",
                    "label": 0
                },
                {
                    "sent": "Read the full pool simply always uses a virtual Alpha memory.",
                    "label": 0
                },
                {
                    "sent": "Reach Alpha node retreat part Pool only uses a virtual memory once that selectively threshold is exceeded, right?",
                    "label": 0
                },
                {
                    "sent": "And we try these values and orthogonally to this we.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We consider two scenarios, semantic web scenarios, one where a shared memory pool is introduced for the sole purpose of supporting RTI pool and one where an existing RDF store is reused to support review pool right?",
                    "label": 1
                },
                {
                    "sent": "Well, this is how the result table looks like, so some notes on that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A total memory size represents the size of the Alpha memory contents plus the size of the RDF store.",
                    "label": 0
                },
                {
                    "sent": "If one is needed right for Rekey base, it does not need an RDF store, so we don't actually include anything extra under the total size.",
                    "label": 0
                },
                {
                    "sent": "Here, for instance, you can see as a threshold increases more and more regular memories will be used and less and less virtual memories right, which which is of course the result of that threshold, right?",
                    "label": 0
                },
                {
                    "sent": "Median so all the results are like median, min, Max respectively.",
                    "label": 0
                },
                {
                    "sent": "This refers to ontologies that led to the median minimum and maximum number of Alpha memory tokens actually, so it refers to the size of the ontology or at least the size that it takes up in Oreti network.",
                    "label": 0
                },
                {
                    "sent": "So if you look at our first.",
                    "label": 0
                },
                {
                    "sent": "How much time OK, 3 minutes if you look at our first semantic web scenario where we introduced this separate memory pool for the sole purpose of of 3D pool, we need to consider the total memory size, right?",
                    "label": 0
                },
                {
                    "sent": "Because we need this RDF store for for our algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So to be fair, we need to include that in a memory size of.",
                    "label": 0
                },
                {
                    "sent": "Our approach takes up.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Even then.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see a significant memory saving.",
                    "label": 0
                },
                {
                    "sent": "Between Ricky Basin, RTI pool, around 60% less memory is used by RTI.",
                    "label": 0
                },
                {
                    "sent": "Full pool at least.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meridian Park pool.",
                    "label": 0
                },
                {
                    "sent": "With the threshold of .1, we see that there's still memory savings right?",
                    "label": 0
                },
                {
                    "sent": "30% of memory is less is used less than than RTI base.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the other ones where you have a higher threshold.",
                    "label": 0
                },
                {
                    "sent": "You see, the memory savings dropped significantly and actually approach there.",
                    "label": 0
                },
                {
                    "sent": "The memory savings of the initial baseline system, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not that not that and something we were too happy with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at performance.",
                    "label": 0
                },
                {
                    "sent": "This really drove us to introduce that selectivity threshold.",
                    "label": 0
                },
                {
                    "sent": "It's really dramatic with 3D full pool and actually performance reduces with a factor of 3 * 3 or 2.8 depending on the platform.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's actually a lot better for these part pool configurations where you have this selectivity threshold.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this configuration we found we depart tool with a .1 selectivity threshold to perform or have the best memory and performance balance at least.",
                    "label": 0
                },
                {
                    "sent": "Have good memory savings.",
                    "label": 0
                },
                {
                    "sent": "Ana relatively limited performance penalty.",
                    "label": 0
                },
                {
                    "sent": "Obviously on mobile plus 4.3 seconds.",
                    "label": 0
                },
                {
                    "sent": "Not too good, but on PC it's still still acceptable.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For our second scenario, where we reuse an existing RDF store right to support our ET pool algorithm, we don't consider the total memory size right because they already have store already exists.",
                    "label": 0
                },
                {
                    "sent": "We just reuse it, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't include it when we're comparing memory sizes.",
                    "label": 0
                },
                {
                    "sent": "Here the memory savings are dramatic, right?",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                },
                {
                    "sent": "Compare with the full pool 3D base.",
                    "label": 0
                },
                {
                    "sent": "Maybe save about 98.7% memory for the median ontology.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to summarize, for this scenario we foundry depart pool with a .5 selectively threshold to be best in memory performance balance, significant memory savings 40%, whereas a performance penalty is quite much lower, right 1.5 seconds for mobile, whereas before it was.",
                    "label": 0
                },
                {
                    "sent": "For second something also, you have a quick note about these reasoning times are quite high.",
                    "label": 0
                },
                {
                    "sent": "Even on PC.",
                    "label": 0
                },
                {
                    "sent": "This is because we introduced we include, I think around 7 quite resource intensive ontologies.",
                    "label": 0
                },
                {
                    "sent": "If we left those out it really the performance improves by an order of magnitude that would be 1.5 seconds instead of 15 is just we included all ontologies that didn't lead to memory issues or that didn't lead to reasoning times longer than 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "That's why these times are so high.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's plenty of future work.",
                    "label": 1
                },
                {
                    "sent": "Clearly our work in this context is quite preliminary.",
                    "label": 0
                },
                {
                    "sent": "Didn't really do anything groundbreaking up until now.",
                    "label": 0
                },
                {
                    "sent": "I already showed you this figure right where the wild card premise SPO essentially subsumes all other memories.",
                    "label": 0
                },
                {
                    "sent": "And our current solution is to simply while we pool all memories to avoid duplication resulting from that subsumption.",
                    "label": 1
                },
                {
                    "sent": "Perhaps a better solution would be to directly represent that subsumption in your memory structure that you use nested memory structure, right?",
                    "label": 1
                },
                {
                    "sent": "So, for instance, this is what the memory for SPO would look like.",
                    "label": 0
                },
                {
                    "sent": "We indexed on SMP because those are shared variables.",
                    "label": 0
                },
                {
                    "sent": "For instance for S, for instance, where this subject value we list tokens 1, two and three, and then for RDF type we will also have a number of tokens, but instead of listing those.",
                    "label": 0
                },
                {
                    "sent": "Or simply directly include the memory structure for S, RDF type.",
                    "label": 0
                },
                {
                    "sent": "Oh right, that has a concrete value for RDF type in the predicate position.",
                    "label": 0
                },
                {
                    "sent": "Right, so we essentially avoid duplication by simply referencing that memory structure, and the same goes for S all same.",
                    "label": 0
                },
                {
                    "sent": "Oh, we can do them the same thing in that nested memory structure, right?",
                    "label": 0
                },
                {
                    "sent": "For object property as a concrete object value.",
                    "label": 0
                },
                {
                    "sent": "We can then include directly to memory structure for S, RDF type owl object property, right.",
                    "label": 0
                },
                {
                    "sent": "Nicely reflecting that subsumption that I showed in the in the above figure.",
                    "label": 0
                },
                {
                    "sent": "2nd.",
                    "label": 0
                },
                {
                    "sent": "An opportunity for.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Future work is to virtually materialize out to semantics.",
                    "label": 1
                },
                {
                    "sent": "Essentially, there we would be performing some sort of domain specific reasoning extended with ontology based features, right?",
                    "label": 0
                },
                {
                    "sent": "So the operators match ontology.",
                    "label": 0
                },
                {
                    "sent": "Sorry, match operator.",
                    "label": 0
                },
                {
                    "sent": "The join operator would be extended with semantic features to avoid explicit materialization of those of those infer triples.",
                    "label": 1
                },
                {
                    "sent": "I for instance say here we have an incoming token with property D. Obviously this doesn't match property a, but actually the property hierarchy says property.",
                    "label": 0
                },
                {
                    "sent": "Deze sub property of property A.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we would use able to relative explicitly materialize these.",
                    "label": 0
                },
                {
                    "sent": "Inferences these two triples would be materialized, and then X would be linked to Y1 using property a right simply so it would be matched by this Alpha node.",
                    "label": 0
                },
                {
                    "sent": "And well, would you store in the memory and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So why not just extend that match operator to automatically take into account that property hierarchy?",
                    "label": 0
                },
                {
                    "sent": "Right, there's many, many other opportunities also along this line of virtually materializing these semantics.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusion for mobile reasoning for clinical decision support, we did in that context it's useful in other ways.",
                    "label": 0
                },
                {
                    "sent": "In other contexts as well, right need to use less remote cloud resources?",
                    "label": 1
                },
                {
                    "sent": "You reduce bandwidth usage 'cause you're not constantly talking to a server.",
                    "label": 0
                },
                {
                    "sent": "We indicated that special consideration is needed for reasoning on mobile platforms.",
                    "label": 0
                },
                {
                    "sent": "This let us to introduce our two approaches 1.",
                    "label": 0
                },
                {
                    "sent": "Automatic selection of outdoor L Rulesets, another one to directly optimize the RTI algorithm for our two RL.",
                    "label": 0
                },
                {
                    "sent": "And we orthogonally consider these two dimensions or scenarios rather the indicated what the best configuration was, and that's about it.",
                    "label": 0
                },
                {
                    "sent": "So the references here on in the presentation.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So, have you compared this to RDF folks?",
                    "label": 0
                },
                {
                    "sent": "We haven't compared this to to any other system.",
                    "label": 0
                },
                {
                    "sent": "The goal here was really to compared to a baseline that is outfitted with standard optimizations.",
                    "label": 0
                },
                {
                    "sent": "Whether it perform better, it's not a mature system, right?",
                    "label": 0
                },
                {
                    "sent": "RDF Fox.",
                    "label": 0
                },
                {
                    "sent": "All all the other kinds of more advanced RDF stores, I don't think we would perform better.",
                    "label": 0
                },
                {
                    "sent": "Let's say that this was really focused on a particular technique right pooling Alpha memories and seeing to what extent that would improve the baseline, not necessarily whether this would be better than existing mature systems with other optimizations in place.",
                    "label": 0
                },
                {
                    "sent": "And so why didn't you implement that optimization into an existing system?",
                    "label": 0
                },
                {
                    "sent": "We did.",
                    "label": 0
                },
                {
                    "sent": "We extended Apache, Gina and Regina.",
                    "label": 0
                },
                {
                    "sent": "With those optimization, I mean a state of the art system.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, the extent to which Apache Gina can be considered state of the art is maybe an object of discussion.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's about it really.",
                    "label": 0
                },
                {
                    "sent": "It had it had a well too good, too easy to understand RTI implementation.",
                    "label": 0
                },
                {
                    "sent": "That's really why we chose it.",
                    "label": 0
                },
                {
                    "sent": "It was pretty basic, which is also what we wanted.",
                    "label": 0
                },
                {
                    "sent": "We wouldn't want to overcomplicate things because our goal was to really see if that technique performs better.",
                    "label": 0
                },
                {
                    "sent": "Not to see how it interacts with other kinds of optimizations.",
                    "label": 0
                },
                {
                    "sent": "Did are you also considering and free logic?",
                    "label": 0
                },
                {
                    "sent": "In what?",
                    "label": 0
                },
                {
                    "sent": "And free logic, no mccurley not know.",
                    "label": 0
                },
                {
                    "sent": "Can I have one also, um, so your desktop and mobile devices?",
                    "label": 0
                },
                {
                    "sent": "So what about the more constrained devices like no sense or something like this?",
                    "label": 0
                },
                {
                    "sent": "So is this applicable or you have to reduce the more than the reset?",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "We haven't really considered any kind of more resource constrained systems.",
                    "label": 0
                },
                {
                    "sent": "That's really because of the context in which this work took place, right in this mobile patient diary.",
                    "label": 0
                },
                {
                    "sent": "But on smartphones we were more interested in seeing how that would be possible for sure deploying it on sensors and the like would be very interesting, but it's certainly future work.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "If you have any other questions, so let's thanks in this picture again.",
                    "label": 0
                }
            ]
        }
    }
}