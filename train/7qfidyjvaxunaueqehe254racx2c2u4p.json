{
    "id": "7qfidyjvaxunaueqehe254racx2c2u4p",
    "title": "Probabilistic Machine Learning in Computational Advertising",
    "info": {
        "author": [
            "Thore Graepel, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_graepel_pmlca/",
    "segmentation": [
        [
            "Yeah, I'm I'm replacing Joaquina or Candela who was originally going to give this talk, but he had too early head back home so.",
            "So I'm doing it for him and this talk will be of quite a different nature, because what I'll be describing is really.",
            "A large scale machine learning application within Microsoft with a little bit of context of what the machine learning problem is, why we need to solve it, what the impact is an then drilling down a little bit apone our Bayesian methodology, and if there's some time I'll touch a little bit upon how will address the scale problems in this.",
            "So this is joint work with Thomas Parker Drill Fabric and and her keen at our online services and advertising research group in the.",
            "Microsoft Research, Cambridge.",
            "So here's the out."
        ],
        [
            "Line of the talk.",
            "I'll give you first an idea of the application within which we are working, which is."
        ],
        [
            "8 search advertising within the Bing search engine and I'll describe our model at predictor, which over the last summer was introduced into the search engine and is now driving 100% traffic.",
            "So the stuff that you hear about is actually the stuff running within our search engine.",
            "And then I'm not quite sure if I'll have time for these these other things.",
            "So let's let's begin by describing the task."
        ],
        [
            "The the general context just to give you an idea about the advertising industry business size, this is split according to various advertising channels here.",
            "And you see, it's been steadily going up.",
            "And this is the GDP of Denmark.",
            "This is Microsoft revenue, so that's that.",
            "Must be the reason why why our management is interested in pursuing this despite the harsh competition that's out there.",
            "But we keep trying.",
            "So if we look a little bit at the growth figure."
        ],
        [
            "So just to give some more motivation to why one would want to be in this business, here are the more traditional advertising channels that people have been using and they are all hovering at around a 10% growth rate per year.",
            "And this is online advertising.",
            "And so again, it's it comes from a relatively low level, but certainly there seemed to be some great opportunities out there, and it's well known that a lot of budget is going from from the Sky over to two online advertising as well.",
            "So this is our humble attempt."
        ],
        [
            "At search, I would recommend to try it every now and then, because the competition certainly must be a good thing.",
            "Even in this market.",
            "And we have an advertising model that works pretty much like Yahoo's and Google's model as well.",
            "So this is basically an explanation of those systems as well."
        ],
        [
            "Someone enters a query into the box here and there, organic search results, and then their paid search results on top, here and on the sidebar here and advertisers are keen on getting those because there's a great targeting opportunity because the user indicates some intent through the query and advertisers would like to capitalize on that and potentially offer their services and goods to that customer so they.",
            "Do that through an auction and they bid a certain amount, so this advertiser might have been $1.",
            "This one might have been $2, and so on.",
            "In order to get their stuff displayed on the page, because there's typically more slots than there are people.",
            "Sorry fewer slots than there are people interested in the slots, but more importantly, the top slots are much more interesting in terms of click through rate than these bottom slots here, which are hardly ever clicked, and that makes for a for healthy competition in that market.",
            "So how does this work now to do, we just sort these by these numbers and then allocate those spaces?",
            "No, that's not how it works.",
            "There's an additional quantity coming in this click through right here.",
            "The estimated probability of click for this particular ad if it was shown in this main line to actually be clicked, because as a search engine we're interested in maximizing the product of the bid and the probability of click, because that's the expected revenue that we would get.",
            "And again, this is all true for Yahoo and Google as well, of course.",
            "And So what is actually sorted by is the expected revenue based on bid and click and then now we don't use the first price auction as would normally be the case, but it generalized second price auction, which creates a nice incentive structures for people to be more likely to actually bid what they think The thing is worth without having to fear that they're being exploited.",
            "And that's how these people are.",
            "These figures here are arrived at and.",
            "Basically, you can think of the ordering as the we order by this product.",
            "Bid times probability of click.",
            "So the one for the first guy needs to be greater than the one for the second guy and so on.",
            "And then we want to make sure that we only charge this first guy so much that he would have maintained his position with that lower bid.",
            "So basically what happens is we put an equality sign here and we solve for the bid.",
            "Which then gives basically this times this divided by that.",
            "So whatever the key point here is that the click through rate estimation enters in two important places.",
            "It enters the system in deciding which adds actually get get displayed and the second thing it determines is the price that we charge to the advertisers.",
            "That's why it is such a central quantity to estimate in these markets.",
            "And if we do it well, then we can increase user satisfaction because the targeting will be better.",
            "We will charge bet the advertisers in the fair away which will lead to a more efficient marketplace and we may be able to increase revenue.",
            "So increasing improving this targeting is really a great thing to work on because it helps every."
        ],
        [
            "So the way we could've got pulled into this type of work was actually through an internal competition within Microsoft.",
            "So at some point the ad center folks realized that their click through rate prediction system wasn't actually that great and they kicked off this competition and they had a symbolic price of a one week luxury holiday in Hawaii.",
            "And it was less about that.",
            "I have to say, but it it it wasn't extra sweetener.",
            "So so various teams within Microsoft started working on this.",
            "And there was a very nice setup which was relatively close to the production environment in terms of the amount of time that you would have for training and the time you would have for prediction and just to give you some very rough figures from the competition, there were these 7 billion impressions that we had to work on and we had two weeks of training time.",
            "And that alone gives us pretty tight bounds on how fast our algorithm needs to learn.",
            "And of course there were ideas for there were was a parallel environment there, but then that has its own problems, of course.",
            "So we're definitely talking about a an application that that has a big scale problem.",
            "So here was our."
        ],
        [
            "Entry to that competition."
        ],
        [
            "It also eventually made it into production then, so it's actually from I guess from a machine learning point of view.",
            "It's a relatively simple algorithm, although it does have some sophistication in the in the Bayesian updates, but in terms of an algorithm that's actually running at scale in the production system, it's relative sophisticated, so you know there's this gap between what's running in production and what we know as a scientific community, and I think this this is probably somewhere in the middle.",
            "So how does this work?",
            "We have an impression here.",
            "This is called an impression if an ad is displayed on another page, and an impression comes with all kinds of different descriptors.",
            "So for example, we have descriptors for the ad, which could be the ad ID or the match type, or the idea of the advertiser or the text in the ad, or all that kinds of other things.",
            "We have some context.",
            "Where is it being displayed?",
            "What's the date and time when it is displayed displayed?",
            "What was the query of the user?",
            "And all kinds of descriptors that might help us predict if this thing will be clicked."
        ],
        [
            "And so here's kind of the the raw bit of the model.",
            "We have these features.",
            "One might be client RP, one might be matched by type.",
            "Man might be position and only ever one of them can be on in in this binary coding here.",
            "And we have weights for each of these we add them up.",
            "We have a probate function and that gives us a peek click, so it's a relatively straightforward generalized linear model that we're working with.",
            "But there are some extra.",
            "Difficulties here because for example, if you compare the match type feature, that one can only ever take 2, maybe four or five values, depending on how we describe the match between the keyword bid for and the and the query.",
            "While a feature like client IP takes millions of values, so the algorithm would need to be able to cope."
        ],
        [
            "That problem, and so.",
            "Here's here's how we do that.",
            "Very roughly, you can think of this quantity of click potential, which is the which is the sum before we squeeze it through the probe.",
            "It and we add up the weights for these different feature values here to arrive at some some click potential for the impression, and then if it's if it's negative."
        ],
        [
            "There's no click, and if it's positive there's a click, but then we assume an extra noise processed per impression, which makes this whole thing soft, so there's noise on this.",
            "Sorry, there's noise on this impression here, and that means that our prediction will actually be probabilistic and the mass and the discussion in the positive will be the click probability.",
            "This mass here and this mass here will be the no click probability and."
        ],
        [
            "And then of course leads to the probit link function that that we had in this slide earlier."
        ],
        [
            "But there's another search, another source of uncertainty, and that's the one that's particularly crucial if we want to deal with these disparate cardinality's in our feature set with these IP address type features that take millions of values, and these say gender or match type type features which take very few values, and so there's uncertainty also in these different weights for the features that enters the process, and correspondingly with."
        ],
        [
            "Is here illustrated as samples there's.",
            "There's also uncertainty about what the total click Patel."
        ],
        [
            "For a given impression is and then if we squeeze that uncertainty through our pro bit function through the function that takes us from minus Infinity Infinity to 01, then of course we also get a distribution here with an associated mean, and that will be our our probability of click estimate."
        ],
        [
            "So if we go back to this picture, some things are in fact a bit more complex, because as I was saying, these are not just weights, they're they're independent Gaussian weight distributions on these weights here and then we combine those distributions to arrive at a click potential distribution, which will then give us a distribution over P clicks.",
            "You can think a little bit of something like a beta distribution over a Bernoulli probability.",
            "This is a different way of Parramatta Rising it, which makes it more composable on this.",
            "End and that's basically the reason why well, this is done.",
            "Now the beautiful thing is that this is not just a step."
        ],
        [
            "I think prediction problem, but in fact this this predictor will be in a big loop and it will help decide about its own training data.",
            "But because it will decide which ads actually get displayed and then those will get clicked and that will be fed back into the predictor.",
            "So we need some means of exploration because if we only ever show those ads that we already know are good, we were not ever find the opportunities for the for the interesting ones you can see here how the how our probability over the probability of click, this beta like probability distributions can have.",
            "Let's do that because we can.",
            "For example, instead of just taking the expected P click, we can sample P click from these distributions so that in this particular case this ad here has already a fairly established low variance probability of click here, which is greater than the one for this thing.",
            "But this ad will have a chance of sampling P clicks that will be bigger than for this one.",
            "So in fact we will be able to collect information about it.",
            "Which in turn will reduce our belief variance for it, and so we can explore the entire pool of ads that we have available.",
            "So."
        ],
        [
            "Does the training work?",
            "We know how this forward model works now, but we need to understand how we actually update those weights.",
            "And here's here's the factor graph that basically does it.",
            "We have belief distributions here and here, and then we view this as a message passing algorithm here, so the messages come from the prior go down are combined into this.",
            "Some squash through the nonlinearity and give us some kind of prediction.",
            "And now of course, when we observe either then no click on the click.",
            "Event.",
            "Then messages are flowing the other way around.",
            "We get an updated belief distribution for this quantity here, and that is then distributed to these weights here.",
            "And the important effect here is that await, which already had a fairly small variants in its prior, will only receive a small update and await that will that has a broader variance in this prior will receive a bigger update, so we'll have that assignment of responsibility, so to speak, to those ads.",
            "Where we are.",
            "All those features where we're less certain."
        ],
        [
            "Here the update rules in detail.",
            "And if we quickly go through them because I think there are actually quite quite elegant, because there are closed form, you don't have to view it as as a message passing, so we've basically collapsed the messages I was showing before, and now this is basically the closed form update, and so this is the update equation for the mean of feature weight I.",
            "And this is the update equation for the variance of feature weight I.",
            "And so the first thing is we see that this is an additive update here for the mu, and the update is scaled by the variance of the weight as you would expect if the variance is big, you take a big update step.",
            "If it is small, you take a small update step.",
            "You have the similar construction here.",
            "This is a multiplicative update for the Sigma for the Sigma squared, and again the Sigma I squared the old One act as a scaling parameter as as a step size parameter if you like so that accounts for the fact that weights with larger variants are updated more than weight with smaller variants.",
            "And now we have these terms here.",
            "These functions here and they basically encode the fact that if this some of the weights of the muse of the weights.",
            "Was negative, so we're on this side here.",
            "Then there will be some kind of surprise we observed a click.",
            "This is the.",
            "These are the updates for the click event, but the system predicted no click.",
            "So then we'll have a big update here.",
            "Whereas if we if this one is already positive, then we'll only have a small update here.",
            "So they all this comes out of an analysis of approximate Bayesian inference using expectation propagation here or.",
            "So these are."
        ],
        [
            "Basically the the updates rules that we're using, and now we can look a little bit of what the what type of parameters come out of this.",
            "For example, here we are looking at all the weights of the client IP parameter.",
            "The client IP can be interesting things.",
            "The client AP can be interesting because it tells you about particular users.",
            "For example about maybe using a particular client IP.",
            "So you would want to know that could be very good in explaining why that part is always clicking or never clicking and.",
            "Yeah, you can see here that in fact this feature alone allows us to find low clickers and these are probably bots that never click and these are high clickers that will click with very high probability and we also relayed this information to our fraud Department because.",
            "Click clicks by bots clearly are nothing.",
            "You wanna charge your customers for."
        ],
        [
            "Something that was very important for us was to have a good calibration and so.",
            "We were looking a lot of these calibration plots, So what this basically is a plot of the predicted probability of click versus the empirical probability of click, where of course the empirical probability of click.",
            "We can only get if we have.",
            "If we have some binning going on here.",
            "So this is basically just binning this line here into 100, or in this case maybe 1000 bins, and looking at all the impressions, seeing what prediction we make for them that decides in which bin they end up here and then averaging for those bins.",
            "What the empirical probability of click was and you see here that this is a very nicely calibrated system which is not always something that you would automatically expect.",
            "For example, if you're using a naive Bayes classifier instead."
        ],
        [
            "You will see a lot of over prediction going on here because those features will be highly correlated.",
            "Sometimes that you have as inputs and then you will have double counting in the.",
            "In the Naive Bayes algorithm.",
            "So just to wrap."
        ],
        [
            "Up well, this system gives us in production is an automatic learning rate per feature.",
            "Wait, it is highly calibrated, which is very desirable if you want to charge your customers based on on this estimate, we can use a lot of features even if they are correlated.",
            "Because we have this effect that in and update the presence of features are taken into account by the updates, there are not assumed independent as in conditionally independent as in base.",
            "We model the uncertainty explicitly and the cost for that is basically since we assume a diagonal prior just twice as many parameters, because instead of just waits we have means and variances for the weights and it turned out that that was was a really good decision to enable these other things.",
            "It also gives us this natural exploration mode where we can really explore the pool of ads in this dynamic system where the predictor is in the loop and decides about its own input data.",
            "And I'm afraid I will have very little time to call to talk about the specifics of the parallelization.",
            "And we're also still working on this.",
            "But if you're interested, I could spend it."
        ],
        [
            "Time out it, thanks.",
            "Yeah, yeah, that's a good question.",
            "Why logistic versus probate?",
            "So in this framework we could do the derive those closed form equations and for logistic regression we would have had to do numerical integration in order to do this, and it's not particularly good match of using a Bayesian of using a Gaussian prior and a likelihood of probit likelihood that is also derived from the Gaussian, which allows us to do the.",
            "Then the integrals and closed form exactly the reason, yeah.",
            "Not so great.",
            "Yeah, see the problem is there are a lot of interesting pre processing steps that you can take an but your system becomes very complex so there is a case for that.",
            "If you have a deep understanding of the mechanisms at work and the fraud people do some extra filtering of things based on rules, but we always found these rule based systems very unwieldy.",
            "You know you do some extra preprocessing.",
            "Here you do something extra there and here this system just.",
            "You know estimates a low probability of click for those IP's that never click a high one.",
            "For those that always click, it's kind of automatic and you don't need to worry about it anymore.",
            "So if the complexity of these systems is mindboggling in production, and so we were happy to be able to keep the complexity down.",
            "But I think yeah there if you have more knowledge about what's going on you you should probably use that.",
            "Yeah, good question.",
            "So we we tide for first place with Chris Burgess, TMB a Redmond based team.",
            "And they were using.",
            "I think at the time they were using marked.",
            "But they couldn't scale it up.",
            "You know multiple adaptive regression trees, but they couldn't quite scale it up to run it on the entire data set.",
            "So what they ended up doing is to collect aggregate data on some.",
            "For example, they would.",
            "They would look at the IPS and and and 1st go over the data once and calculate statistics on those and then feed them as features into into Mart and reduce the sample size overall.",
            "And that obviously because they tie we tide so that that was a competitive approach.",
            "But it made again the system very complex.",
            "You know it wasn't a single path system anymore, and so our belief had always been.",
            "This needs to be essentially an online learning type of.",
            "Type of system and if you need to go over your data multiple times, a lot of additional questions will come up at what rate you do that?",
            "And so it makes it more complex.",
            "But I think in principle one could squeeze out even better prediction performance if one was willing to disregard the performance issues.",
            "But there's so much data coming in you want to always use the fresh data in this story.",
            "We think it's very important because otherwise how would you bring together the the low cardinality and the high cardinality features?",
            "So in the low cardinality features, will the weight variance will very quickly collapse to a very small value for the medium cardinality values it will take a little longer and the high cardinality values will.",
            "We remain with a very broad distribution for a long time, and so essentially, if you have these variances, they act the the low cardinality features will act as anchors.",
            "They will probably pretty much zoom into a specific value and from then on you could pretty much consider them fixed and this system basically figures out automatically how to adjust the learning step sizes across all of these very disparate features.",
            "So I think it's crucial, but I wouldn't be able to give you.",
            "To give you a numbers on this at the moment.",
            "Yep.",
            "So it's a.",
            "So it's a generalized linear regression, right?",
            "Because of the link function and.",
            "I'm not quite sure how.",
            "What would that be then?",
            "But as a pure Bayesian, I would never learn it prior, but as I see I mean we could have introduced we could have in terms of hierarchical Bayes, we could have introduced hyperparameters for noise levels, for example, and we didn't do that.",
            "We we did them by hand pretty much, but in principle it could be done within the message passing framework, but extra computational cost of course.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, I'm I'm replacing Joaquina or Candela who was originally going to give this talk, but he had too early head back home so.",
                    "label": 0
                },
                {
                    "sent": "So I'm doing it for him and this talk will be of quite a different nature, because what I'll be describing is really.",
                    "label": 0
                },
                {
                    "sent": "A large scale machine learning application within Microsoft with a little bit of context of what the machine learning problem is, why we need to solve it, what the impact is an then drilling down a little bit apone our Bayesian methodology, and if there's some time I'll touch a little bit upon how will address the scale problems in this.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Thomas Parker Drill Fabric and and her keen at our online services and advertising research group in the.",
                    "label": 1
                },
                {
                    "sent": "Microsoft Research, Cambridge.",
                    "label": 0
                },
                {
                    "sent": "So here's the out.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Line of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll give you first an idea of the application within which we are working, which is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8 search advertising within the Bing search engine and I'll describe our model at predictor, which over the last summer was introduced into the search engine and is now driving 100% traffic.",
                    "label": 0
                },
                {
                    "sent": "So the stuff that you hear about is actually the stuff running within our search engine.",
                    "label": 0
                },
                {
                    "sent": "And then I'm not quite sure if I'll have time for these these other things.",
                    "label": 0
                },
                {
                    "sent": "So let's let's begin by describing the task.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the general context just to give you an idea about the advertising industry business size, this is split according to various advertising channels here.",
                    "label": 1
                },
                {
                    "sent": "And you see, it's been steadily going up.",
                    "label": 0
                },
                {
                    "sent": "And this is the GDP of Denmark.",
                    "label": 1
                },
                {
                    "sent": "This is Microsoft revenue, so that's that.",
                    "label": 0
                },
                {
                    "sent": "Must be the reason why why our management is interested in pursuing this despite the harsh competition that's out there.",
                    "label": 0
                },
                {
                    "sent": "But we keep trying.",
                    "label": 0
                },
                {
                    "sent": "So if we look a little bit at the growth figure.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give some more motivation to why one would want to be in this business, here are the more traditional advertising channels that people have been using and they are all hovering at around a 10% growth rate per year.",
                    "label": 0
                },
                {
                    "sent": "And this is online advertising.",
                    "label": 0
                },
                {
                    "sent": "And so again, it's it comes from a relatively low level, but certainly there seemed to be some great opportunities out there, and it's well known that a lot of budget is going from from the Sky over to two online advertising as well.",
                    "label": 0
                },
                {
                    "sent": "So this is our humble attempt.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At search, I would recommend to try it every now and then, because the competition certainly must be a good thing.",
                    "label": 0
                },
                {
                    "sent": "Even in this market.",
                    "label": 0
                },
                {
                    "sent": "And we have an advertising model that works pretty much like Yahoo's and Google's model as well.",
                    "label": 0
                },
                {
                    "sent": "So this is basically an explanation of those systems as well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone enters a query into the box here and there, organic search results, and then their paid search results on top, here and on the sidebar here and advertisers are keen on getting those because there's a great targeting opportunity because the user indicates some intent through the query and advertisers would like to capitalize on that and potentially offer their services and goods to that customer so they.",
                    "label": 0
                },
                {
                    "sent": "Do that through an auction and they bid a certain amount, so this advertiser might have been $1.",
                    "label": 0
                },
                {
                    "sent": "This one might have been $2, and so on.",
                    "label": 0
                },
                {
                    "sent": "In order to get their stuff displayed on the page, because there's typically more slots than there are people.",
                    "label": 0
                },
                {
                    "sent": "Sorry fewer slots than there are people interested in the slots, but more importantly, the top slots are much more interesting in terms of click through rate than these bottom slots here, which are hardly ever clicked, and that makes for a for healthy competition in that market.",
                    "label": 0
                },
                {
                    "sent": "So how does this work now to do, we just sort these by these numbers and then allocate those spaces?",
                    "label": 0
                },
                {
                    "sent": "No, that's not how it works.",
                    "label": 0
                },
                {
                    "sent": "There's an additional quantity coming in this click through right here.",
                    "label": 0
                },
                {
                    "sent": "The estimated probability of click for this particular ad if it was shown in this main line to actually be clicked, because as a search engine we're interested in maximizing the product of the bid and the probability of click, because that's the expected revenue that we would get.",
                    "label": 0
                },
                {
                    "sent": "And again, this is all true for Yahoo and Google as well, of course.",
                    "label": 0
                },
                {
                    "sent": "And So what is actually sorted by is the expected revenue based on bid and click and then now we don't use the first price auction as would normally be the case, but it generalized second price auction, which creates a nice incentive structures for people to be more likely to actually bid what they think The thing is worth without having to fear that they're being exploited.",
                    "label": 0
                },
                {
                    "sent": "And that's how these people are.",
                    "label": 0
                },
                {
                    "sent": "These figures here are arrived at and.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can think of the ordering as the we order by this product.",
                    "label": 0
                },
                {
                    "sent": "Bid times probability of click.",
                    "label": 0
                },
                {
                    "sent": "So the one for the first guy needs to be greater than the one for the second guy and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we want to make sure that we only charge this first guy so much that he would have maintained his position with that lower bid.",
                    "label": 0
                },
                {
                    "sent": "So basically what happens is we put an equality sign here and we solve for the bid.",
                    "label": 0
                },
                {
                    "sent": "Which then gives basically this times this divided by that.",
                    "label": 0
                },
                {
                    "sent": "So whatever the key point here is that the click through rate estimation enters in two important places.",
                    "label": 0
                },
                {
                    "sent": "It enters the system in deciding which adds actually get get displayed and the second thing it determines is the price that we charge to the advertisers.",
                    "label": 0
                },
                {
                    "sent": "That's why it is such a central quantity to estimate in these markets.",
                    "label": 0
                },
                {
                    "sent": "And if we do it well, then we can increase user satisfaction because the targeting will be better.",
                    "label": 0
                },
                {
                    "sent": "We will charge bet the advertisers in the fair away which will lead to a more efficient marketplace and we may be able to increase revenue.",
                    "label": 0
                },
                {
                    "sent": "So increasing improving this targeting is really a great thing to work on because it helps every.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we could've got pulled into this type of work was actually through an internal competition within Microsoft.",
                    "label": 0
                },
                {
                    "sent": "So at some point the ad center folks realized that their click through rate prediction system wasn't actually that great and they kicked off this competition and they had a symbolic price of a one week luxury holiday in Hawaii.",
                    "label": 0
                },
                {
                    "sent": "And it was less about that.",
                    "label": 0
                },
                {
                    "sent": "I have to say, but it it it wasn't extra sweetener.",
                    "label": 0
                },
                {
                    "sent": "So so various teams within Microsoft started working on this.",
                    "label": 0
                },
                {
                    "sent": "And there was a very nice setup which was relatively close to the production environment in terms of the amount of time that you would have for training and the time you would have for prediction and just to give you some very rough figures from the competition, there were these 7 billion impressions that we had to work on and we had two weeks of training time.",
                    "label": 0
                },
                {
                    "sent": "And that alone gives us pretty tight bounds on how fast our algorithm needs to learn.",
                    "label": 0
                },
                {
                    "sent": "And of course there were ideas for there were was a parallel environment there, but then that has its own problems, of course.",
                    "label": 0
                },
                {
                    "sent": "So we're definitely talking about a an application that that has a big scale problem.",
                    "label": 0
                },
                {
                    "sent": "So here was our.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entry to that competition.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It also eventually made it into production then, so it's actually from I guess from a machine learning point of view.",
                    "label": 0
                },
                {
                    "sent": "It's a relatively simple algorithm, although it does have some sophistication in the in the Bayesian updates, but in terms of an algorithm that's actually running at scale in the production system, it's relative sophisticated, so you know there's this gap between what's running in production and what we know as a scientific community, and I think this this is probably somewhere in the middle.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "We have an impression here.",
                    "label": 0
                },
                {
                    "sent": "This is called an impression if an ad is displayed on another page, and an impression comes with all kinds of different descriptors.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have descriptors for the ad, which could be the ad ID or the match type, or the idea of the advertiser or the text in the ad, or all that kinds of other things.",
                    "label": 0
                },
                {
                    "sent": "We have some context.",
                    "label": 0
                },
                {
                    "sent": "Where is it being displayed?",
                    "label": 0
                },
                {
                    "sent": "What's the date and time when it is displayed displayed?",
                    "label": 0
                },
                {
                    "sent": "What was the query of the user?",
                    "label": 0
                },
                {
                    "sent": "And all kinds of descriptors that might help us predict if this thing will be clicked.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here's kind of the the raw bit of the model.",
                    "label": 0
                },
                {
                    "sent": "We have these features.",
                    "label": 0
                },
                {
                    "sent": "One might be client RP, one might be matched by type.",
                    "label": 0
                },
                {
                    "sent": "Man might be position and only ever one of them can be on in in this binary coding here.",
                    "label": 0
                },
                {
                    "sent": "And we have weights for each of these we add them up.",
                    "label": 0
                },
                {
                    "sent": "We have a probate function and that gives us a peek click, so it's a relatively straightforward generalized linear model that we're working with.",
                    "label": 0
                },
                {
                    "sent": "But there are some extra.",
                    "label": 0
                },
                {
                    "sent": "Difficulties here because for example, if you compare the match type feature, that one can only ever take 2, maybe four or five values, depending on how we describe the match between the keyword bid for and the and the query.",
                    "label": 0
                },
                {
                    "sent": "While a feature like client IP takes millions of values, so the algorithm would need to be able to cope.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That problem, and so.",
                    "label": 0
                },
                {
                    "sent": "Here's here's how we do that.",
                    "label": 0
                },
                {
                    "sent": "Very roughly, you can think of this quantity of click potential, which is the which is the sum before we squeeze it through the probe.",
                    "label": 0
                },
                {
                    "sent": "It and we add up the weights for these different feature values here to arrive at some some click potential for the impression, and then if it's if it's negative.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's no click, and if it's positive there's a click, but then we assume an extra noise processed per impression, which makes this whole thing soft, so there's noise on this.",
                    "label": 0
                },
                {
                    "sent": "Sorry, there's noise on this impression here, and that means that our prediction will actually be probabilistic and the mass and the discussion in the positive will be the click probability.",
                    "label": 0
                },
                {
                    "sent": "This mass here and this mass here will be the no click probability and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course leads to the probit link function that that we had in this slide earlier.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's another search, another source of uncertainty, and that's the one that's particularly crucial if we want to deal with these disparate cardinality's in our feature set with these IP address type features that take millions of values, and these say gender or match type type features which take very few values, and so there's uncertainty also in these different weights for the features that enters the process, and correspondingly with.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is here illustrated as samples there's.",
                    "label": 0
                },
                {
                    "sent": "There's also uncertainty about what the total click Patel.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For a given impression is and then if we squeeze that uncertainty through our pro bit function through the function that takes us from minus Infinity Infinity to 01, then of course we also get a distribution here with an associated mean, and that will be our our probability of click estimate.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we go back to this picture, some things are in fact a bit more complex, because as I was saying, these are not just weights, they're they're independent Gaussian weight distributions on these weights here and then we combine those distributions to arrive at a click potential distribution, which will then give us a distribution over P clicks.",
                    "label": 0
                },
                {
                    "sent": "You can think a little bit of something like a beta distribution over a Bernoulli probability.",
                    "label": 0
                },
                {
                    "sent": "This is a different way of Parramatta Rising it, which makes it more composable on this.",
                    "label": 0
                },
                {
                    "sent": "End and that's basically the reason why well, this is done.",
                    "label": 0
                },
                {
                    "sent": "Now the beautiful thing is that this is not just a step.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think prediction problem, but in fact this this predictor will be in a big loop and it will help decide about its own training data.",
                    "label": 0
                },
                {
                    "sent": "But because it will decide which ads actually get displayed and then those will get clicked and that will be fed back into the predictor.",
                    "label": 0
                },
                {
                    "sent": "So we need some means of exploration because if we only ever show those ads that we already know are good, we were not ever find the opportunities for the for the interesting ones you can see here how the how our probability over the probability of click, this beta like probability distributions can have.",
                    "label": 0
                },
                {
                    "sent": "Let's do that because we can.",
                    "label": 0
                },
                {
                    "sent": "For example, instead of just taking the expected P click, we can sample P click from these distributions so that in this particular case this ad here has already a fairly established low variance probability of click here, which is greater than the one for this thing.",
                    "label": 0
                },
                {
                    "sent": "But this ad will have a chance of sampling P clicks that will be bigger than for this one.",
                    "label": 0
                },
                {
                    "sent": "So in fact we will be able to collect information about it.",
                    "label": 0
                },
                {
                    "sent": "Which in turn will reduce our belief variance for it, and so we can explore the entire pool of ads that we have available.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does the training work?",
                    "label": 0
                },
                {
                    "sent": "We know how this forward model works now, but we need to understand how we actually update those weights.",
                    "label": 0
                },
                {
                    "sent": "And here's here's the factor graph that basically does it.",
                    "label": 0
                },
                {
                    "sent": "We have belief distributions here and here, and then we view this as a message passing algorithm here, so the messages come from the prior go down are combined into this.",
                    "label": 0
                },
                {
                    "sent": "Some squash through the nonlinearity and give us some kind of prediction.",
                    "label": 0
                },
                {
                    "sent": "And now of course, when we observe either then no click on the click.",
                    "label": 0
                },
                {
                    "sent": "Event.",
                    "label": 0
                },
                {
                    "sent": "Then messages are flowing the other way around.",
                    "label": 0
                },
                {
                    "sent": "We get an updated belief distribution for this quantity here, and that is then distributed to these weights here.",
                    "label": 0
                },
                {
                    "sent": "And the important effect here is that await, which already had a fairly small variants in its prior, will only receive a small update and await that will that has a broader variance in this prior will receive a bigger update, so we'll have that assignment of responsibility, so to speak, to those ads.",
                    "label": 0
                },
                {
                    "sent": "Where we are.",
                    "label": 0
                },
                {
                    "sent": "All those features where we're less certain.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the update rules in detail.",
                    "label": 0
                },
                {
                    "sent": "And if we quickly go through them because I think there are actually quite quite elegant, because there are closed form, you don't have to view it as as a message passing, so we've basically collapsed the messages I was showing before, and now this is basically the closed form update, and so this is the update equation for the mean of feature weight I.",
                    "label": 0
                },
                {
                    "sent": "And this is the update equation for the variance of feature weight I.",
                    "label": 0
                },
                {
                    "sent": "And so the first thing is we see that this is an additive update here for the mu, and the update is scaled by the variance of the weight as you would expect if the variance is big, you take a big update step.",
                    "label": 0
                },
                {
                    "sent": "If it is small, you take a small update step.",
                    "label": 0
                },
                {
                    "sent": "You have the similar construction here.",
                    "label": 0
                },
                {
                    "sent": "This is a multiplicative update for the Sigma for the Sigma squared, and again the Sigma I squared the old One act as a scaling parameter as as a step size parameter if you like so that accounts for the fact that weights with larger variants are updated more than weight with smaller variants.",
                    "label": 0
                },
                {
                    "sent": "And now we have these terms here.",
                    "label": 0
                },
                {
                    "sent": "These functions here and they basically encode the fact that if this some of the weights of the muse of the weights.",
                    "label": 0
                },
                {
                    "sent": "Was negative, so we're on this side here.",
                    "label": 0
                },
                {
                    "sent": "Then there will be some kind of surprise we observed a click.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "These are the updates for the click event, but the system predicted no click.",
                    "label": 1
                },
                {
                    "sent": "So then we'll have a big update here.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we if this one is already positive, then we'll only have a small update here.",
                    "label": 0
                },
                {
                    "sent": "So they all this comes out of an analysis of approximate Bayesian inference using expectation propagation here or.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically the the updates rules that we're using, and now we can look a little bit of what the what type of parameters come out of this.",
                    "label": 0
                },
                {
                    "sent": "For example, here we are looking at all the weights of the client IP parameter.",
                    "label": 0
                },
                {
                    "sent": "The client IP can be interesting things.",
                    "label": 0
                },
                {
                    "sent": "The client AP can be interesting because it tells you about particular users.",
                    "label": 0
                },
                {
                    "sent": "For example about maybe using a particular client IP.",
                    "label": 0
                },
                {
                    "sent": "So you would want to know that could be very good in explaining why that part is always clicking or never clicking and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can see here that in fact this feature alone allows us to find low clickers and these are probably bots that never click and these are high clickers that will click with very high probability and we also relayed this information to our fraud Department because.",
                    "label": 0
                },
                {
                    "sent": "Click clicks by bots clearly are nothing.",
                    "label": 0
                },
                {
                    "sent": "You wanna charge your customers for.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something that was very important for us was to have a good calibration and so.",
                    "label": 0
                },
                {
                    "sent": "We were looking a lot of these calibration plots, So what this basically is a plot of the predicted probability of click versus the empirical probability of click, where of course the empirical probability of click.",
                    "label": 0
                },
                {
                    "sent": "We can only get if we have.",
                    "label": 0
                },
                {
                    "sent": "If we have some binning going on here.",
                    "label": 0
                },
                {
                    "sent": "So this is basically just binning this line here into 100, or in this case maybe 1000 bins, and looking at all the impressions, seeing what prediction we make for them that decides in which bin they end up here and then averaging for those bins.",
                    "label": 0
                },
                {
                    "sent": "What the empirical probability of click was and you see here that this is a very nicely calibrated system which is not always something that you would automatically expect.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're using a naive Bayes classifier instead.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You will see a lot of over prediction going on here because those features will be highly correlated.",
                    "label": 0
                },
                {
                    "sent": "Sometimes that you have as inputs and then you will have double counting in the.",
                    "label": 0
                },
                {
                    "sent": "In the Naive Bayes algorithm.",
                    "label": 1
                },
                {
                    "sent": "So just to wrap.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up well, this system gives us in production is an automatic learning rate per feature.",
                    "label": 0
                },
                {
                    "sent": "Wait, it is highly calibrated, which is very desirable if you want to charge your customers based on on this estimate, we can use a lot of features even if they are correlated.",
                    "label": 0
                },
                {
                    "sent": "Because we have this effect that in and update the presence of features are taken into account by the updates, there are not assumed independent as in conditionally independent as in base.",
                    "label": 0
                },
                {
                    "sent": "We model the uncertainty explicitly and the cost for that is basically since we assume a diagonal prior just twice as many parameters, because instead of just waits we have means and variances for the weights and it turned out that that was was a really good decision to enable these other things.",
                    "label": 0
                },
                {
                    "sent": "It also gives us this natural exploration mode where we can really explore the pool of ads in this dynamic system where the predictor is in the loop and decides about its own input data.",
                    "label": 0
                },
                {
                    "sent": "And I'm afraid I will have very little time to call to talk about the specifics of the parallelization.",
                    "label": 0
                },
                {
                    "sent": "And we're also still working on this.",
                    "label": 0
                },
                {
                    "sent": "But if you're interested, I could spend it.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time out it, thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Why logistic versus probate?",
                    "label": 0
                },
                {
                    "sent": "So in this framework we could do the derive those closed form equations and for logistic regression we would have had to do numerical integration in order to do this, and it's not particularly good match of using a Bayesian of using a Gaussian prior and a likelihood of probit likelihood that is also derived from the Gaussian, which allows us to do the.",
                    "label": 0
                },
                {
                    "sent": "Then the integrals and closed form exactly the reason, yeah.",
                    "label": 0
                },
                {
                    "sent": "Not so great.",
                    "label": 0
                },
                {
                    "sent": "Yeah, see the problem is there are a lot of interesting pre processing steps that you can take an but your system becomes very complex so there is a case for that.",
                    "label": 0
                },
                {
                    "sent": "If you have a deep understanding of the mechanisms at work and the fraud people do some extra filtering of things based on rules, but we always found these rule based systems very unwieldy.",
                    "label": 0
                },
                {
                    "sent": "You know you do some extra preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Here you do something extra there and here this system just.",
                    "label": 0
                },
                {
                    "sent": "You know estimates a low probability of click for those IP's that never click a high one.",
                    "label": 0
                },
                {
                    "sent": "For those that always click, it's kind of automatic and you don't need to worry about it anymore.",
                    "label": 0
                },
                {
                    "sent": "So if the complexity of these systems is mindboggling in production, and so we were happy to be able to keep the complexity down.",
                    "label": 0
                },
                {
                    "sent": "But I think yeah there if you have more knowledge about what's going on you you should probably use that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good question.",
                    "label": 0
                },
                {
                    "sent": "So we we tide for first place with Chris Burgess, TMB a Redmond based team.",
                    "label": 0
                },
                {
                    "sent": "And they were using.",
                    "label": 0
                },
                {
                    "sent": "I think at the time they were using marked.",
                    "label": 0
                },
                {
                    "sent": "But they couldn't scale it up.",
                    "label": 0
                },
                {
                    "sent": "You know multiple adaptive regression trees, but they couldn't quite scale it up to run it on the entire data set.",
                    "label": 0
                },
                {
                    "sent": "So what they ended up doing is to collect aggregate data on some.",
                    "label": 0
                },
                {
                    "sent": "For example, they would.",
                    "label": 0
                },
                {
                    "sent": "They would look at the IPS and and and 1st go over the data once and calculate statistics on those and then feed them as features into into Mart and reduce the sample size overall.",
                    "label": 0
                },
                {
                    "sent": "And that obviously because they tie we tide so that that was a competitive approach.",
                    "label": 0
                },
                {
                    "sent": "But it made again the system very complex.",
                    "label": 0
                },
                {
                    "sent": "You know it wasn't a single path system anymore, and so our belief had always been.",
                    "label": 0
                },
                {
                    "sent": "This needs to be essentially an online learning type of.",
                    "label": 0
                },
                {
                    "sent": "Type of system and if you need to go over your data multiple times, a lot of additional questions will come up at what rate you do that?",
                    "label": 0
                },
                {
                    "sent": "And so it makes it more complex.",
                    "label": 0
                },
                {
                    "sent": "But I think in principle one could squeeze out even better prediction performance if one was willing to disregard the performance issues.",
                    "label": 0
                },
                {
                    "sent": "But there's so much data coming in you want to always use the fresh data in this story.",
                    "label": 0
                },
                {
                    "sent": "We think it's very important because otherwise how would you bring together the the low cardinality and the high cardinality features?",
                    "label": 0
                },
                {
                    "sent": "So in the low cardinality features, will the weight variance will very quickly collapse to a very small value for the medium cardinality values it will take a little longer and the high cardinality values will.",
                    "label": 0
                },
                {
                    "sent": "We remain with a very broad distribution for a long time, and so essentially, if you have these variances, they act the the low cardinality features will act as anchors.",
                    "label": 0
                },
                {
                    "sent": "They will probably pretty much zoom into a specific value and from then on you could pretty much consider them fixed and this system basically figures out automatically how to adjust the learning step sizes across all of these very disparate features.",
                    "label": 0
                },
                {
                    "sent": "So I think it's crucial, but I wouldn't be able to give you.",
                    "label": 0
                },
                {
                    "sent": "To give you a numbers on this at the moment.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "So it's a generalized linear regression, right?",
                    "label": 0
                },
                {
                    "sent": "Because of the link function and.",
                    "label": 0
                },
                {
                    "sent": "I'm not quite sure how.",
                    "label": 0
                },
                {
                    "sent": "What would that be then?",
                    "label": 0
                },
                {
                    "sent": "But as a pure Bayesian, I would never learn it prior, but as I see I mean we could have introduced we could have in terms of hierarchical Bayes, we could have introduced hyperparameters for noise levels, for example, and we didn't do that.",
                    "label": 0
                },
                {
                    "sent": "We we did them by hand pretty much, but in principle it could be done within the message passing framework, but extra computational cost of course.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}