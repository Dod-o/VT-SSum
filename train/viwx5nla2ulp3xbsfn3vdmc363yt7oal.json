{
    "id": "viwx5nla2ulp3xbsfn3vdmc363yt7oal",
    "title": "Follow the Leader with Dropout Perturbations",
    "info": {
        "author": [
            "Tim van Erven, Mathematical Institute, Leiden University"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_van_erven_leader/",
    "segmentation": [
        [
            "So this is joint work with white check.",
            "Kozlowski was in the room an Mumford Warmoth who really wanted to come, but it's more pressing obligations at home.",
            "I want to talk about using dropout, which is a technique to train neural networks in online learning setting.",
            "So I'll start with my cartoon introduction to dropout.",
            "And then I'll get to the online learning bits after death."
        ],
        [
            "So this is how I draw neural network.",
            "Their input features X and an output Y.",
            "They're related by a series of hidden units with connections that all have associated weights, for example, 1 hidden unit might."
        ],
        [
            "Incoming weights W1W 2W3.",
            "Think of a very big neural network, so there are many weights and then you want to train this network on data.",
            "Of course you have many weights.",
            "There are many parameters, so there's a risk of overfitting an A technique that has proved very useful recently to prevent overfitting is dropout."
        ],
        [
            "So how does that work?",
            "Well, they do stochastic gradient descent on the parameters on the weights.",
            "But not just the regular greatness stochastic gradient descent before each gradient descent update, they randomly remove every hidden unit independently with probability 1/2, or maybe some other fixed probability.",
            "An somehow magically, this prevents overfitting.",
            "So before every gradient descent update, they they restrict attention to random subnetwork that prevents overfitting somehow.",
            "Um?",
            "And there are."
        ],
        [
            "Intuitions about why this might be a good idea.",
            "But at least it works really well in applications like image classification and speech recognition.",
            "And there are people trying trying to understand this in the paper.",
            "I liked a lot was one at NIPS by Wagner woman Young.",
            "So that's that's dropout.",
            "In a nutshell, now let's get to online learning bit because we're going to use dropout in an online learning setting.",
            "So."
        ],
        [
            "This is the setting.",
            "It's an instance of prediction with expert advice.",
            "Set it up the following.",
            "Following way, we'll predict into predict during capital T rounds and in each round first we have to pick an expert from a finite set of capital K experts.",
            "We are allowed to randomize.",
            "Choice can be can be random.",
            "Then the second step we observe the losses of these K experts, which I assume are bounded pain, zero and one.",
            "And then of course we can compute our own laws.",
            "The regrets will will measure the performance.",
            "So expected regret is the difference between our cumulative loss after after capital T rounds and the cumulative loss of the best expert.",
            "So that's how we'll measure performance.",
            "And in this talk I'll consider two ways of generating the losses for these.",
            "For these experts, or the standard way, is to think of these losses aetherial, and then you want to guarantee that you regret is still small, like in the previous talk, but also talk about losses that might be generated IID.",
            "So the whole vector of losses in a particular round might come from a fixed distribution, and that's a much easier learning scenario for the most part of the talk is set for cereal, but I'll, I'll get to the ID bit as well.",
            "OK."
        ],
        [
            "So this is all standard introduction to online learning, but bear with me there's.",
            "An algorithm that's that's kind of natural, especially if you're thinking of the IID setting, is to follow the leader, and there is.",
            "It works pretty well.",
            "And that's two to pick the expert at anytime.",
            "T you pick the expert that has done best so far, that's the leader.",
            "Now that's a natural algorithm, but for adversarial losses it might get linear regret, which means it's not learning at all.",
            "So this algorithm is not robust against adverse aerial losses, and so there have been many proposals to fix this, and what I want to do is add a new proposal based on dropout.",
            "How can you?",
            "How can you fix this?",
            "Follow the leader algorithm.",
            "And we'll do that as follows."
        ],
        [
            "So it's kind of a classical interpretation of this prediction, with expert advice setting in which the experts correspond to features.",
            "And now what we're trying to learn is a linear combination of these are expected regret as a linear combination of these these experts, in which the weights correspond to the probability of selecting a certain expert.",
            "And if you take this interpretation and then apply dropout to it, so this is just a single single neuron.",
            "But still we can apply dropout.",
            "Then what it does is for each expert it will set its loss to zero with a certain probability.",
            "So with probability say 1 minus Alpha, we keep the loss with probability Alpha.",
            "We set it to 0.",
            "And then for the whole purpose of this talk, you can think of ELF as a half.",
            "There's no tuning necessary.",
            "Just think elf as a half.",
            "Then we won't follow the leader on these on these losses that we get after doing drop outs, at least perturbed losses.",
            "And that's the the algorithm I want to talk about.",
            "So the question is, does this does this work against it for serial losses?"
        ],
        [
            "And.",
            "It turns out it does.",
            "But let me talk about.",
            "I said losses are bounded between zero and one.",
            "But let me talk about special case first.",
            "Credit losses are binary, there either exactly 0 or exactly 1.",
            "And then we have a theorem that shows that in this case even against adverse aerial losses, this algorithm will have expected regret bounded by the square root of L star and the cumulative loss of the best expert, which is most the number of rounds T. And it also depends on the number of experts K logarithmically.",
            "So in terms of for bounds in this forum in terms of L star and K, this is the optimal type of bound.",
            "And it doesn't actually require any tuning.",
            "You can choose any Alpha, but in the interval zero and one this long as you don't pick it exactly one or exactly 0.",
            "And it will only affect the constants in this in this bound.",
            "So with this very simple perturbation trick you get robust behavior against the worst case for follow the leader.",
            "Um?",
            "So this is for binary losses an.",
            "When I analyzed previous algorithms, it was usually the case that binary losses are the most difficult.",
            "So when you get the result for binary losses to start thinking OK, and then it will hold for all the losses but in the interval zero to 1.",
            "Well, nothing."
        ],
        [
            "Case.",
            "It turns out if you do the exact same algorithm on losses between in the whole interval zero to 1.",
            "Then it might happen there exist sequences of losses where the regret grows linearly in the number of experts, so it has a sub optimal dependence on the number of experts.",
            "And I haven't checked for the dependence on L star, but already having a sub optimal dependence on the number of experts when there are many algorithms that get the optimal dependence would be unacceptable.",
            "So basically the algorithm breaks when you go from binary to continuous losses.",
            "Um?",
            "When you start thinking about this so I have a working algorithm for for the extreme case zero binary losses.",
            "Now I go in between.",
            "It doesn't work.",
            "Maybe I did the wrong generalization from binary to continuous and that's what's going on here, so it's actually rather easy."
        ],
        [
            "Fix by getting the right generalization, which I've written up here and this if the losses are binary, then this is exactly dropout as we did before.",
            "And it also has the same expected losses as dropout on the continuous losses.",
            "But we set things up in a different way.",
            "We've done something we call binary.",
            "Are always zero or one.",
            "And we just determine whether which it is by varying the probability and the probability depends on the actual losses.",
            "So when we do this, what we can show with it is actually a kind of kind of reduction.",
            "We have a short proof that says if you use this binarize dropout, you can and you can prove the bound for binary losses.",
            "Then it immediately follows with the same constants for all the losses in the interval zero to 1.",
            "So doing this fixes the problem.",
            "This is the right generalization to continuous losses.",
            "So.",
            "So this is our main result.",
            "We have this very simple algorithm.",
            "It doesn't have any any complicated tuning parameters and it gets this worst case optimal bound.",
            "What I'd like to do next is it's.",
            "Quickly look at at IID losses, which is much easier, so we'd like to have a better regret bound."
        ],
        [
            "And.",
            "The setup is we have the whole vector of losses in a particular round comes from it from a fixed distribution and there's a gap between the best experts expected loss of the best expert and expected losses of all the other experts.",
            "So learning should be kind of easy because I expect the cumulative losses to just diverge to grow at a linear rate, but the different linear rates, so it should be really easy to see which expert is the best one.",
            "In particular.",
            "If I use follow the leader and I will rapidly pick this up.",
            "So then if I used my previous bound, I would get a regret bound that grows like square root of T even though the problem is easy to this.",
            "That's kind of disappointing, but it turns out that.",
            "What you can get here is order log of number of experts.",
            "So a constant regret and we get that with our algorithm.",
            "Now there are other algorithms that get this as well.",
            "This is perhaps not a very difficult property to get.",
            "But it's not completely trivial because she can easily break it if you're doing some doubling trick on Teorell start, and you probably won't get this property, so it's not automatic for all methods.",
            "Alright, so that's my excursion into ID.",
            "Let's go back to the.",
            "Yeah, pretty real losses and let me give some context for this algorithm because there's a lot of related work."
        ],
        [
            "Right, So what are our algorithm where we perturb the losses?",
            "That's an instance of a family of follow the perturb leader style algorithms that work as follows.",
            "You take your losses and then you add a random variable PSI to perturb them.",
            "And in our case you can rewrite our method in this form and you get a particular site and this disk size 2 two features, one it it depends on the data.",
            "The perturbations depend on the data.",
            "And the other is, the perturbations are different between different experts.",
            "And this last property makes the algorithm.",
            "More complicated to analyze.",
            "It's kind of like having different learning rates for each expert.",
            "So.",
            "The Seminole Paper in this area is Bikali and Vempala.",
            "So what they did is their standard approaches to basically bound the probability.",
            "The probability of a leader change in something called to be the leader lemma.",
            "And then they get a very nice and simple proof that for particular choice of perturbations they get worst case optimal regret bounds.",
            "Now, unfortunately enough for our perturbations.",
            "This proof doesn't actually go through in that way because our.",
            "For our perturbations, you cannot bound the probability of a leader change that simply that easily.",
            "And that brings me to the second bit of related work that we used to to control the probability of a leader change."
        ],
        [
            "So there was an algorithm last year called random walk perturbation by destroy Lugosi.",
            "Annoyed.",
            "And it's equally simple as this dropout that I presented and what they do is they add a Bernoulli random variable in every round to the two losses to perturb them, and then they do follow the leader on that.",
            "And then they get a worst case optimal regret bound.",
            "There's a capital T instead of an L star, which is a little bit worse.",
            "But still they get a regret bound, and there's some similarity with what we do, because if the losses are one, this type of perturbation will be equivalent up to a shift by constant.",
            "It doesn't affect.",
            "We choose to be the leader.",
            "So it's kind of similar if the losses are all once.",
            "Of course the losses are not all one, so the algorithms are actually different.",
            "But still it's in the same spirit.",
            "And that's why we were wondering, does this algorithm also get this L Starbound, but unfortunately it doesn't, because the perturbations they don't adapt to the day-to-day are always the same.",
            "How am I doing on Titan?",
            "OK.",
            "So so they don't get this All Star bound, but reading the paper we still get got very useful technical ingredient to do our proof, and that's what I want to talk about next on next slide I want to go over the main points of the proof.",
            "And basically before the main points, this is 1 observation.",
            "Is the proof is very long, which is unfortunate, but in the end it was just happy that we got a proof at all.",
            "It was hard work."
        ],
        [
            "So what we do is is the following.",
            "We start by deriving the worst case loss sequence for our algorithm.",
            "And the way we get there is we have three operations.",
            "If you have any sequence of losses, then we have three operations on this sequence of losses that will only increase the regret for our algorithm.",
            "And then we repeatedly apply this operations until it converges until you end up with the final sequence of losses.",
            "And that's the work that must be the worst case sequence.",
            "We showed that that is the worst case sequence.",
            "So what does this worst case sequence look like?",
            "Let me show you by giving, giving an example."
        ],
        [
            "So in this example I fix the cumulative loss the experts will have at the end.",
            "At the end of the game.",
            "So three experts, one will have cumulative cumulative loss, 11 will have committed last three and will have cumulative loss 5.",
            "And then I start handing out losses to these experts in turn until they've reached their total cumulative loss.",
            "So the first experts I give all the experts one unit of loss, then the first expert has lost one and he cannot get anymore because he's reached his totals.",
            "Then I continue with the other two experts.",
            "I give them losses until the second has reached its budget.",
            "And then the third one until I'm done.",
            "So that's that's the worst case sequence for our algorithm that maximizes the regret.",
            "And then to to analyze this."
        ],
        [
            "Basically we have to split this sequence into into two parts which have different intuitive thinking.",
            "Things are intuitively slightly different in these parts, so in the one part, the cumulative losses of all the experts are kind of close to each other.",
            "They're all getting losses, so there's not a big difference, and then we appeal to a lemma from the previous from the guys from the previous slides to random walk perturbation over kind of concentration inequality to bound the probability of a leader change.",
            "We can do it every round, so we need to do some additional work to only do it once every every K rounds.",
            "And then we get.",
            "Do you have to be bound?",
            "So that's the first regime.",
            "That's the first part of the of this.",
            "This worst case loss sequence.",
            "The other parts.",
            "The first experts has stopped receiving losses.",
            "The others are still getting losses and there are no leader changes because the algorithm always picks the first expert, and then it's a simple liftings inequality to bound the probability of a leader change.",
            "So that's basically the proof."
        ],
        [
            "Let me let me summarize.",
            "We have a simple algorithm without any tuning that gets a worst case regret bound.",
            "And a better bound for IID losses.",
            "Review this is just the first step in understanding this type of algorithm, because we have many open questions."
        ],
        [
            "Oops.",
            "Which I hope you would be interested in discussing at the poster, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is joint work with white check.",
                    "label": 1
                },
                {
                    "sent": "Kozlowski was in the room an Mumford Warmoth who really wanted to come, but it's more pressing obligations at home.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about using dropout, which is a technique to train neural networks in online learning setting.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with my cartoon introduction to dropout.",
                    "label": 0
                },
                {
                    "sent": "And then I'll get to the online learning bits after death.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is how I draw neural network.",
                    "label": 0
                },
                {
                    "sent": "Their input features X and an output Y.",
                    "label": 0
                },
                {
                    "sent": "They're related by a series of hidden units with connections that all have associated weights, for example, 1 hidden unit might.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incoming weights W1W 2W3.",
                    "label": 0
                },
                {
                    "sent": "Think of a very big neural network, so there are many weights and then you want to train this network on data.",
                    "label": 0
                },
                {
                    "sent": "Of course you have many weights.",
                    "label": 0
                },
                {
                    "sent": "There are many parameters, so there's a risk of overfitting an A technique that has proved very useful recently to prevent overfitting is dropout.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does that work?",
                    "label": 0
                },
                {
                    "sent": "Well, they do stochastic gradient descent on the parameters on the weights.",
                    "label": 0
                },
                {
                    "sent": "But not just the regular greatness stochastic gradient descent before each gradient descent update, they randomly remove every hidden unit independently with probability 1/2, or maybe some other fixed probability.",
                    "label": 1
                },
                {
                    "sent": "An somehow magically, this prevents overfitting.",
                    "label": 0
                },
                {
                    "sent": "So before every gradient descent update, they they restrict attention to random subnetwork that prevents overfitting somehow.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And there are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intuitions about why this might be a good idea.",
                    "label": 0
                },
                {
                    "sent": "But at least it works really well in applications like image classification and speech recognition.",
                    "label": 1
                },
                {
                    "sent": "And there are people trying trying to understand this in the paper.",
                    "label": 0
                },
                {
                    "sent": "I liked a lot was one at NIPS by Wagner woman Young.",
                    "label": 0
                },
                {
                    "sent": "So that's that's dropout.",
                    "label": 0
                },
                {
                    "sent": "In a nutshell, now let's get to online learning bit because we're going to use dropout in an online learning setting.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the setting.",
                    "label": 0
                },
                {
                    "sent": "It's an instance of prediction with expert advice.",
                    "label": 1
                },
                {
                    "sent": "Set it up the following.",
                    "label": 0
                },
                {
                    "sent": "Following way, we'll predict into predict during capital T rounds and in each round first we have to pick an expert from a finite set of capital K experts.",
                    "label": 0
                },
                {
                    "sent": "We are allowed to randomize.",
                    "label": 0
                },
                {
                    "sent": "Choice can be can be random.",
                    "label": 0
                },
                {
                    "sent": "Then the second step we observe the losses of these K experts, which I assume are bounded pain, zero and one.",
                    "label": 0
                },
                {
                    "sent": "And then of course we can compute our own laws.",
                    "label": 0
                },
                {
                    "sent": "The regrets will will measure the performance.",
                    "label": 0
                },
                {
                    "sent": "So expected regret is the difference between our cumulative loss after after capital T rounds and the cumulative loss of the best expert.",
                    "label": 1
                },
                {
                    "sent": "So that's how we'll measure performance.",
                    "label": 0
                },
                {
                    "sent": "And in this talk I'll consider two ways of generating the losses for these.",
                    "label": 0
                },
                {
                    "sent": "For these experts, or the standard way, is to think of these losses aetherial, and then you want to guarantee that you regret is still small, like in the previous talk, but also talk about losses that might be generated IID.",
                    "label": 0
                },
                {
                    "sent": "So the whole vector of losses in a particular round might come from a fixed distribution, and that's a much easier learning scenario for the most part of the talk is set for cereal, but I'll, I'll get to the ID bit as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is all standard introduction to online learning, but bear with me there's.",
                    "label": 0
                },
                {
                    "sent": "An algorithm that's that's kind of natural, especially if you're thinking of the IID setting, is to follow the leader, and there is.",
                    "label": 0
                },
                {
                    "sent": "It works pretty well.",
                    "label": 0
                },
                {
                    "sent": "And that's two to pick the expert at anytime.",
                    "label": 0
                },
                {
                    "sent": "T you pick the expert that has done best so far, that's the leader.",
                    "label": 1
                },
                {
                    "sent": "Now that's a natural algorithm, but for adversarial losses it might get linear regret, which means it's not learning at all.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is not robust against adverse aerial losses, and so there have been many proposals to fix this, and what I want to do is add a new proposal based on dropout.",
                    "label": 0
                },
                {
                    "sent": "How can you?",
                    "label": 0
                },
                {
                    "sent": "How can you fix this?",
                    "label": 0
                },
                {
                    "sent": "Follow the leader algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we'll do that as follows.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's kind of a classical interpretation of this prediction, with expert advice setting in which the experts correspond to features.",
                    "label": 0
                },
                {
                    "sent": "And now what we're trying to learn is a linear combination of these are expected regret as a linear combination of these these experts, in which the weights correspond to the probability of selecting a certain expert.",
                    "label": 0
                },
                {
                    "sent": "And if you take this interpretation and then apply dropout to it, so this is just a single single neuron.",
                    "label": 0
                },
                {
                    "sent": "But still we can apply dropout.",
                    "label": 0
                },
                {
                    "sent": "Then what it does is for each expert it will set its loss to zero with a certain probability.",
                    "label": 0
                },
                {
                    "sent": "So with probability say 1 minus Alpha, we keep the loss with probability Alpha.",
                    "label": 0
                },
                {
                    "sent": "We set it to 0.",
                    "label": 0
                },
                {
                    "sent": "And then for the whole purpose of this talk, you can think of ELF as a half.",
                    "label": 0
                },
                {
                    "sent": "There's no tuning necessary.",
                    "label": 0
                },
                {
                    "sent": "Just think elf as a half.",
                    "label": 0
                },
                {
                    "sent": "Then we won't follow the leader on these on these losses that we get after doing drop outs, at least perturbed losses.",
                    "label": 0
                },
                {
                    "sent": "And that's the the algorithm I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "So the question is, does this does this work against it for serial losses?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It turns out it does.",
                    "label": 0
                },
                {
                    "sent": "But let me talk about.",
                    "label": 0
                },
                {
                    "sent": "I said losses are bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "But let me talk about special case first.",
                    "label": 0
                },
                {
                    "sent": "Credit losses are binary, there either exactly 0 or exactly 1.",
                    "label": 0
                },
                {
                    "sent": "And then we have a theorem that shows that in this case even against adverse aerial losses, this algorithm will have expected regret bounded by the square root of L star and the cumulative loss of the best expert, which is most the number of rounds T. And it also depends on the number of experts K logarithmically.",
                    "label": 0
                },
                {
                    "sent": "So in terms of for bounds in this forum in terms of L star and K, this is the optimal type of bound.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't actually require any tuning.",
                    "label": 0
                },
                {
                    "sent": "You can choose any Alpha, but in the interval zero and one this long as you don't pick it exactly one or exactly 0.",
                    "label": 0
                },
                {
                    "sent": "And it will only affect the constants in this in this bound.",
                    "label": 0
                },
                {
                    "sent": "So with this very simple perturbation trick you get robust behavior against the worst case for follow the leader.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is for binary losses an.",
                    "label": 1
                },
                {
                    "sent": "When I analyzed previous algorithms, it was usually the case that binary losses are the most difficult.",
                    "label": 0
                },
                {
                    "sent": "So when you get the result for binary losses to start thinking OK, and then it will hold for all the losses but in the interval zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Well, nothing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you do the exact same algorithm on losses between in the whole interval zero to 1.",
                    "label": 0
                },
                {
                    "sent": "Then it might happen there exist sequences of losses where the regret grows linearly in the number of experts, so it has a sub optimal dependence on the number of experts.",
                    "label": 1
                },
                {
                    "sent": "And I haven't checked for the dependence on L star, but already having a sub optimal dependence on the number of experts when there are many algorithms that get the optimal dependence would be unacceptable.",
                    "label": 0
                },
                {
                    "sent": "So basically the algorithm breaks when you go from binary to continuous losses.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "When you start thinking about this so I have a working algorithm for for the extreme case zero binary losses.",
                    "label": 0
                },
                {
                    "sent": "Now I go in between.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Maybe I did the wrong generalization from binary to continuous and that's what's going on here, so it's actually rather easy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fix by getting the right generalization, which I've written up here and this if the losses are binary, then this is exactly dropout as we did before.",
                    "label": 0
                },
                {
                    "sent": "And it also has the same expected losses as dropout on the continuous losses.",
                    "label": 0
                },
                {
                    "sent": "But we set things up in a different way.",
                    "label": 0
                },
                {
                    "sent": "We've done something we call binary.",
                    "label": 0
                },
                {
                    "sent": "Are always zero or one.",
                    "label": 0
                },
                {
                    "sent": "And we just determine whether which it is by varying the probability and the probability depends on the actual losses.",
                    "label": 0
                },
                {
                    "sent": "So when we do this, what we can show with it is actually a kind of kind of reduction.",
                    "label": 0
                },
                {
                    "sent": "We have a short proof that says if you use this binarize dropout, you can and you can prove the bound for binary losses.",
                    "label": 0
                },
                {
                    "sent": "Then it immediately follows with the same constants for all the losses in the interval zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So doing this fixes the problem.",
                    "label": 0
                },
                {
                    "sent": "This is the right generalization to continuous losses.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is our main result.",
                    "label": 0
                },
                {
                    "sent": "We have this very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have any any complicated tuning parameters and it gets this worst case optimal bound.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do next is it's.",
                    "label": 0
                },
                {
                    "sent": "Quickly look at at IID losses, which is much easier, so we'd like to have a better regret bound.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The setup is we have the whole vector of losses in a particular round comes from it from a fixed distribution and there's a gap between the best experts expected loss of the best expert and expected losses of all the other experts.",
                    "label": 1
                },
                {
                    "sent": "So learning should be kind of easy because I expect the cumulative losses to just diverge to grow at a linear rate, but the different linear rates, so it should be really easy to see which expert is the best one.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                },
                {
                    "sent": "If I use follow the leader and I will rapidly pick this up.",
                    "label": 0
                },
                {
                    "sent": "So then if I used my previous bound, I would get a regret bound that grows like square root of T even though the problem is easy to this.",
                    "label": 0
                },
                {
                    "sent": "That's kind of disappointing, but it turns out that.",
                    "label": 0
                },
                {
                    "sent": "What you can get here is order log of number of experts.",
                    "label": 0
                },
                {
                    "sent": "So a constant regret and we get that with our algorithm.",
                    "label": 1
                },
                {
                    "sent": "Now there are other algorithms that get this as well.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps not a very difficult property to get.",
                    "label": 0
                },
                {
                    "sent": "But it's not completely trivial because she can easily break it if you're doing some doubling trick on Teorell start, and you probably won't get this property, so it's not automatic for all methods.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's my excursion into ID.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, pretty real losses and let me give some context for this algorithm because there's a lot of related work.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, So what are our algorithm where we perturb the losses?",
                    "label": 0
                },
                {
                    "sent": "That's an instance of a family of follow the perturb leader style algorithms that work as follows.",
                    "label": 0
                },
                {
                    "sent": "You take your losses and then you add a random variable PSI to perturb them.",
                    "label": 0
                },
                {
                    "sent": "And in our case you can rewrite our method in this form and you get a particular site and this disk size 2 two features, one it it depends on the data.",
                    "label": 0
                },
                {
                    "sent": "The perturbations depend on the data.",
                    "label": 0
                },
                {
                    "sent": "And the other is, the perturbations are different between different experts.",
                    "label": 0
                },
                {
                    "sent": "And this last property makes the algorithm.",
                    "label": 0
                },
                {
                    "sent": "More complicated to analyze.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like having different learning rates for each expert.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The Seminole Paper in this area is Bikali and Vempala.",
                    "label": 0
                },
                {
                    "sent": "So what they did is their standard approaches to basically bound the probability.",
                    "label": 0
                },
                {
                    "sent": "The probability of a leader change in something called to be the leader lemma.",
                    "label": 1
                },
                {
                    "sent": "And then they get a very nice and simple proof that for particular choice of perturbations they get worst case optimal regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately enough for our perturbations.",
                    "label": 0
                },
                {
                    "sent": "This proof doesn't actually go through in that way because our.",
                    "label": 0
                },
                {
                    "sent": "For our perturbations, you cannot bound the probability of a leader change that simply that easily.",
                    "label": 0
                },
                {
                    "sent": "And that brings me to the second bit of related work that we used to to control the probability of a leader change.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there was an algorithm last year called random walk perturbation by destroy Lugosi.",
                    "label": 1
                },
                {
                    "sent": "Annoyed.",
                    "label": 0
                },
                {
                    "sent": "And it's equally simple as this dropout that I presented and what they do is they add a Bernoulli random variable in every round to the two losses to perturb them, and then they do follow the leader on that.",
                    "label": 0
                },
                {
                    "sent": "And then they get a worst case optimal regret bound.",
                    "label": 0
                },
                {
                    "sent": "There's a capital T instead of an L star, which is a little bit worse.",
                    "label": 0
                },
                {
                    "sent": "But still they get a regret bound, and there's some similarity with what we do, because if the losses are one, this type of perturbation will be equivalent up to a shift by constant.",
                    "label": 0
                },
                {
                    "sent": "It doesn't affect.",
                    "label": 0
                },
                {
                    "sent": "We choose to be the leader.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of similar if the losses are all once.",
                    "label": 0
                },
                {
                    "sent": "Of course the losses are not all one, so the algorithms are actually different.",
                    "label": 0
                },
                {
                    "sent": "But still it's in the same spirit.",
                    "label": 0
                },
                {
                    "sent": "And that's why we were wondering, does this algorithm also get this L Starbound, but unfortunately it doesn't, because the perturbations they don't adapt to the day-to-day are always the same.",
                    "label": 0
                },
                {
                    "sent": "How am I doing on Titan?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so they don't get this All Star bound, but reading the paper we still get got very useful technical ingredient to do our proof, and that's what I want to talk about next on next slide I want to go over the main points of the proof.",
                    "label": 0
                },
                {
                    "sent": "And basically before the main points, this is 1 observation.",
                    "label": 0
                },
                {
                    "sent": "Is the proof is very long, which is unfortunate, but in the end it was just happy that we got a proof at all.",
                    "label": 0
                },
                {
                    "sent": "It was hard work.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is is the following.",
                    "label": 0
                },
                {
                    "sent": "We start by deriving the worst case loss sequence for our algorithm.",
                    "label": 1
                },
                {
                    "sent": "And the way we get there is we have three operations.",
                    "label": 0
                },
                {
                    "sent": "If you have any sequence of losses, then we have three operations on this sequence of losses that will only increase the regret for our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then we repeatedly apply this operations until it converges until you end up with the final sequence of losses.",
                    "label": 0
                },
                {
                    "sent": "And that's the work that must be the worst case sequence.",
                    "label": 0
                },
                {
                    "sent": "We showed that that is the worst case sequence.",
                    "label": 0
                },
                {
                    "sent": "So what does this worst case sequence look like?",
                    "label": 0
                },
                {
                    "sent": "Let me show you by giving, giving an example.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this example I fix the cumulative loss the experts will have at the end.",
                    "label": 0
                },
                {
                    "sent": "At the end of the game.",
                    "label": 0
                },
                {
                    "sent": "So three experts, one will have cumulative cumulative loss, 11 will have committed last three and will have cumulative loss 5.",
                    "label": 0
                },
                {
                    "sent": "And then I start handing out losses to these experts in turn until they've reached their total cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "So the first experts I give all the experts one unit of loss, then the first expert has lost one and he cannot get anymore because he's reached his totals.",
                    "label": 0
                },
                {
                    "sent": "Then I continue with the other two experts.",
                    "label": 0
                },
                {
                    "sent": "I give them losses until the second has reached its budget.",
                    "label": 0
                },
                {
                    "sent": "And then the third one until I'm done.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the worst case sequence for our algorithm that maximizes the regret.",
                    "label": 0
                },
                {
                    "sent": "And then to to analyze this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically we have to split this sequence into into two parts which have different intuitive thinking.",
                    "label": 0
                },
                {
                    "sent": "Things are intuitively slightly different in these parts, so in the one part, the cumulative losses of all the experts are kind of close to each other.",
                    "label": 0
                },
                {
                    "sent": "They're all getting losses, so there's not a big difference, and then we appeal to a lemma from the previous from the guys from the previous slides to random walk perturbation over kind of concentration inequality to bound the probability of a leader change.",
                    "label": 0
                },
                {
                    "sent": "We can do it every round, so we need to do some additional work to only do it once every every K rounds.",
                    "label": 1
                },
                {
                    "sent": "And then we get.",
                    "label": 0
                },
                {
                    "sent": "Do you have to be bound?",
                    "label": 0
                },
                {
                    "sent": "So that's the first regime.",
                    "label": 0
                },
                {
                    "sent": "That's the first part of the of this.",
                    "label": 0
                },
                {
                    "sent": "This worst case loss sequence.",
                    "label": 1
                },
                {
                    "sent": "The other parts.",
                    "label": 0
                },
                {
                    "sent": "The first experts has stopped receiving losses.",
                    "label": 0
                },
                {
                    "sent": "The others are still getting losses and there are no leader changes because the algorithm always picks the first expert, and then it's a simple liftings inequality to bound the probability of a leader change.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the proof.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me let me summarize.",
                    "label": 0
                },
                {
                    "sent": "We have a simple algorithm without any tuning that gets a worst case regret bound.",
                    "label": 0
                },
                {
                    "sent": "And a better bound for IID losses.",
                    "label": 0
                },
                {
                    "sent": "Review this is just the first step in understanding this type of algorithm, because we have many open questions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Which I hope you would be interested in discussing at the poster, thank you.",
                    "label": 0
                }
            ]
        }
    }
}