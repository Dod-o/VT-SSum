{
    "id": "zh4mddjbbtpuenwu2vkbsskqldigmbke",
    "title": "Resourceful Contextual Bandits",
    "info": {
        "author": [
            "Aleksandrs Slivkins, Microsoft Research"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_slivkins_bandits/",
    "segmentation": [
        [
            "It's urgent work with Ashwin, but in the Euro and John Langford it's about contextual bandits with resource constraints."
        ],
        [
            "So let's start with the basics.",
            "The bandits with ID rewards.",
            "So here are the models.",
            "Just that in each round and algorithm pick some action from a fixed set of actions and the reward is drawn independently from some distribution that depends only on the chosen action and is not known to the algorithm.",
            "So the the goal is to maximize the total expected reward overtime, and you know, as usual it's going to fight via regret with respect to some.",
            "Benchmark, so the difference between the benchmark and the expected total worth of the algorithm.",
            "So the benchmark is essentially.",
            "The best all knowing algorithm that is the best algorithm that knows these hidden distributions.",
            "So in this particular case, it happens to be a simple.",
            "It happens to be just the best I fixed actually.",
            "OK, now as a concrete example for what we want to do in this."
        ],
        [
            "Paper let me talk about a dynamic pricing.",
            "So here an algorithm is a seller with unlimited supply of items and in each round and you customer arrives.",
            "The algorithm picks an action, namely a price and the customer either buys one item of this prize or leaves, and the sale happens independently with probability that depends only on the chosen price, and this probability is not known to the algorithm.",
            "So the goal is to maximize the total expected revenue.",
            "So so far this suggests a special case of the bandits with a dirty words defined in the previous slide, where the.",
            "Actions correspond to prices.",
            "Now we want to extend this into their actions."
        ],
        [
            "When is that day?",
            "Supply of items is limited and we need to stop when we run out of items and the other is that when a customer arrives it shows here shows up with a known profile and the sale probability depends both on the chosen price and the profile.",
            "The customer profile can include things like location, time of the day, gender, whatever.",
            "You know all things that you know Amazon knows about this.",
            "OK, so that's roughly what we're going to care about in this talk, so more generally.",
            "We care about we care about scenarios where algorithm consumes some limited resources and in each round there is a context.",
            "As in contextual bandits."
        ],
        [
            "So they high level feature here is that, well, we care about you.",
            "Directions know in which the basic model can be extended, limited resources and contextual bandits, and our model includes both and in fact we obtained the 1st result on well at the first nontrivial result.",
            "On any such model.",
            "Moreover in.",
            "Each of the two directions.",
            "We will subsume the most general model in the literature so far, respectivly business with knapsacks and contextual bandits with the policy sets.",
            "And then we obtain optimal regret for our model.",
            "So that's the high level picture.",
            "OK, so now let me get into some details and define the model."
        ],
        [
            "So there are delimited.",
            "There are multiple limited resources, you know.",
            "Maybe we're selling Donuts and cookies with a limited supply, which is, so there is a known budget on each resource and we need to stop whenever some resources exhausted.",
            "So what happens in each round?",
            "Well, a context arrives drawn IID from some known distribution over context, the algorithm picks an action from a fixed.",
            "A set of actions and then the outcome now is a vector.",
            "It consists of the reward.",
            "And the end, the consumption of each resource.",
            "So this vector is drawn independently from some distribution over vectors, and this distribution depends only on the context in this round and the chosen action, and this distribution is not known to the algorithm.",
            "So the goal is, as usual, to maximize the total expected reward anfora normalization, we assume that all per round rewards and off around consumptions are in the 01.",
            "So that's the model.",
            "That's what happens in this in our setting now.",
            "Yes, yes, and it's not a huge distraction, but.",
            "So yes, we're still open.",
            "Any resources exhaust so.",
            "And now."
        ],
        [
            "Well, as usual we care about regret.",
            "It's defined as the benchmark, minus the total spectator word of the algorithm.",
            "The benchmark is before the best all known algorithm, which in this case is, you know, the algorithm that knows these hidden distributions of the outcome vectors.",
            "Now.",
            "Well, this benchmark maybe a little bit too harsh, so we relax it in a very general way in the following way."
        ],
        [
            "So we consider a policies that is mappings from context to actions.",
            "That is fixed overtime just to fix mapping from context to actions, and we assume that the algorithm is given some set of policies, which maybe you know which may include some but all but not all policies and they.",
            "The benchmark is relative to the set of policies in the sense that it's the best all knowing algorithm that is restricted, you know that is restricted to only use these policies.",
            "So.",
            "Because of resource constraints.",
            "It may be the case that this benchmark is much better than the best fixed policy.",
            "And essentially it's the best fixed mixture of policies in the policies that so, and they mean this happens not only some weird cases, but essentially in any special case one would care about.",
            "OK, so now."
        ],
        [
            "The model is quite general, so in the pricing either dynamic pricing example that they give we have an algorithm being a seller with limited supply of items in each Round, Hill first one item for sale at the chosen price.",
            "They reward is essentially money and contact this customer profile.",
            "Another example is that an algorithm could be an employer in the crowd sourcing market with a limited budget of money and in each round he offers one test to perform at the chosen price.",
            "Selections are prices.",
            "And the rewards are essentially the completed tasks, and context could be the profile of a worker or other task.",
            "Yet another example, it's allocation of paper click ads where resources are limited advertisers budgets and in each round the algorithm picks one ad to display and observes a click on auto click and essentially the rewards are clicks and the context is profile of a page of or or a user.",
            "Now, each of these examples may be extended in many different ways, and there are a lot more examples in the paper.",
            "Another thing is that like so, one way to think about the policies set is that.",
            "So not sure if any big if anybody can see this, but that's capital Pi everywhere so is that.",
            "We fix a given method of learning from offline data such as linear regression or decision trees or whatnot, and we define the policies set as all policies that can possibly be learned from offline data using this method so.",
            "It's actually quite general.",
            "OK, so."
        ],
        [
            "What was in the related work?",
            "Well, the special case of no context has been defined and resolved in our recent folks paper, and there have been many, like prior papers on various special cases of that.",
            "Then the likewise the special case of no resources has also been studied in prior work.",
            "Moreover, there was a very.",
            "There is a very recent paper in this easy, which consider is another model for like.",
            "Contextual values with resource constraints, which is incompatible with ours in the sense that it's more general on how they model limited resources, but much less general on how they model contexts.",
            "Oak."
        ],
        [
            "OK, so I'll remain.",
            "The result is that you know we design an algorithm with this regret in terms of the various parameters.",
            "Now this formula is not supposed immediately I tell you anything but.",
            "Let me comment on this, so it's optimal in the sense that if we scale all constraints including time by some common factor Alpha, then regret scales as root above, which is known to be the optimal scaling and in.",
            "Bandit problems also if the benchmark is upper bounded by a constant fraction of the of the budget, then also obtain root of Katie regret up to work factors, which is also known to be optimal, and Moreover the logarithmic dependence on the number of policies is known to be optimal even in even without re services.",
            "So in this sense, this regret is optimal.",
            "Now one significant caveat is that our algorithm is very computationally inefficient, so it's essentially a proof of concept that optimal that such regret can be achieved information theoretically."
        ],
        [
            "OK, So what are the challenges in this problem compared to the standard bandit problem?",
            "Why is this interesting?",
            "You know why that's an interesting problem?",
            "Well, due to limited resources we cannot think of maximizing expected per round reward.",
            "It's really not the right goal because you know if we do this, we may exhaust resources way too fast.",
            "So instead we really must think about the expected total reward over the entire time horizon.",
            "Secondly, they mentioned the best all knowing algorithm here is not a fixed policy, but essentially best fixed mixture of policies.",
            "So essentially we need to search over these mixtures of policies.",
            "So now due to context, there are some additional complications.",
            "Well, we can have a trivial reduction to the prior work on the special case of no contexts where we just think of each policy as a you know meter arm in some bandit algorithm which uses in each round among local policies.",
            "So this is just a trivial reduction and we obtained regret the scales as a root of the number of policies.",
            "But instead we get a log.",
            "And so you know, it's an exponential improvement.",
            "And in order to achieve that, essentially in each round we need to explore many policies at once.",
            "We cannot just explore one policy, we need to explore a many policies at once.",
            "OK, so let me."
        ],
        [
            "Outline some many ideas in the algorithm so you know as usual and bandit problems we have.",
            "Explore exploit tradeoff and.",
            "As usual, in order to get the optimal regret, we need to adapt exploration to past observationes.",
            "We cannot just keep, you know, we cannot just explore uniformly at random.",
            "And also in some sense we need to explore and exploit at the same time.",
            "Now.",
            "Essentially, the goal is to zoom in on the optimal mixture of policies and the given policies set just because that's the that's what the benchmark is.",
            "So OK, what what do we do in each round?",
            "Well, we focus on a plausibly optimal mixture of policies, meaning mixtures that.",
            "Might be optimal given the data that we have so far and like I'm not defining many of the details.",
            "But you know, given the data that we have so far, we can form estimates and you know high confidence estimates and with these high confidence estimates, some things.",
            "Some mixtures are clearly suboptimal and we only care about the remaining ones.",
            "So in each round we pick some plausibly optimal mixture well, then with some low probability, we pick an actual uniformly at random with remaining probability withdrawal policy.",
            "You know, independently from this mixture and with your connection recommended by this point, I see so so far so good and essentially takes care of exploitation.",
            "Now the kicker is that we simultaneously need to take care of exploration, namely not only we need to pick a plausibly optimal make sure, but we need to pick it in a way that we get to explore every single a policy in the policy set.",
            "At once.",
            "What does it mean?",
            "It means that they action that we that our algorithm appears becausw we choose this make sure most coincide with the action recommended by.",
            "Any given policy a sorry, so for any given policy Pi star in the \u03a0 prime and the policy said the action that we pick must coincide with the action recommended by this policy with a near optimal probability in some sense of in Europe.",
            "So that's kind of the outline of what we do in the algorithm.",
            "And of course the technical details here are about, you know, defining exactly what we mean by near optimal probability and how we make sure that you know such optimal mixture can be found, and then you know analyzing regret.",
            "But I'm not going into these details there in the paper.",
            "So that's pretty much what I want to say for the technical quiet."
        ],
        [
            "There are bunch of open questions, so the main open question is that wanna get you know optimal regret and a good running sign.",
            "And this has been resolved for the special case of now contexts, and very, very recently in this HTML.",
            "For the special case of notary services.",
            "So there is a hope, but it looks like it's going to be very hard.",
            "And then there are many other very nice open questions, even for the special case of no context out of those, my personal favorite is to go beyond IAD environments.",
            "I mean, what happens if the environment can change overtime and?",
            "You know other serial or maybe like some limited adversarial way.",
            "Also manual regret bounds.",
            "I mean, our regret bounds are not tight for some important.",
            "Special cases, I mean, the tighten the general worst case, but for some important special cases there are not and finally you know if actions are prices, we can only handle.",
            "Our algorithm can only handle a finite number of those.",
            "So we need to discretize the prices and this turns out to be tricky even for a single limited resource.",
            "And it's completely not understood how to do it in the optimal way for multiple limited resources.",
            "You know, say we're selling cookies and doughnuts.",
            "It's completely not clear how to discretize.",
            "OK, so and there are a bunch of other nice open questions in the paper, so I guess with that I'd like to conclude."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's urgent work with Ashwin, but in the Euro and John Langford it's about contextual bandits with resource constraints.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the basics.",
                    "label": 0
                },
                {
                    "sent": "The bandits with ID rewards.",
                    "label": 1
                },
                {
                    "sent": "So here are the models.",
                    "label": 1
                },
                {
                    "sent": "Just that in each round and algorithm pick some action from a fixed set of actions and the reward is drawn independently from some distribution that depends only on the chosen action and is not known to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the the goal is to maximize the total expected reward overtime, and you know, as usual it's going to fight via regret with respect to some.",
                    "label": 0
                },
                {
                    "sent": "Benchmark, so the difference between the benchmark and the expected total worth of the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the benchmark is essentially.",
                    "label": 0
                },
                {
                    "sent": "The best all knowing algorithm that is the best algorithm that knows these hidden distributions.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, it happens to be a simple.",
                    "label": 0
                },
                {
                    "sent": "It happens to be just the best I fixed actually.",
                    "label": 0
                },
                {
                    "sent": "OK, now as a concrete example for what we want to do in this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper let me talk about a dynamic pricing.",
                    "label": 0
                },
                {
                    "sent": "So here an algorithm is a seller with unlimited supply of items and in each round and you customer arrives.",
                    "label": 1
                },
                {
                    "sent": "The algorithm picks an action, namely a price and the customer either buys one item of this prize or leaves, and the sale happens independently with probability that depends only on the chosen price, and this probability is not known to the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to maximize the total expected revenue.",
                    "label": 0
                },
                {
                    "sent": "So so far this suggests a special case of the bandits with a dirty words defined in the previous slide, where the.",
                    "label": 0
                },
                {
                    "sent": "Actions correspond to prices.",
                    "label": 0
                },
                {
                    "sent": "Now we want to extend this into their actions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When is that day?",
                    "label": 0
                },
                {
                    "sent": "Supply of items is limited and we need to stop when we run out of items and the other is that when a customer arrives it shows here shows up with a known profile and the sale probability depends both on the chosen price and the profile.",
                    "label": 1
                },
                {
                    "sent": "The customer profile can include things like location, time of the day, gender, whatever.",
                    "label": 0
                },
                {
                    "sent": "You know all things that you know Amazon knows about this.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's roughly what we're going to care about in this talk, so more generally.",
                    "label": 1
                },
                {
                    "sent": "We care about we care about scenarios where algorithm consumes some limited resources and in each round there is a context.",
                    "label": 0
                },
                {
                    "sent": "As in contextual bandits.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they high level feature here is that, well, we care about you.",
                    "label": 0
                },
                {
                    "sent": "Directions know in which the basic model can be extended, limited resources and contextual bandits, and our model includes both and in fact we obtained the 1st result on well at the first nontrivial result.",
                    "label": 0
                },
                {
                    "sent": "On any such model.",
                    "label": 0
                },
                {
                    "sent": "Moreover in.",
                    "label": 0
                },
                {
                    "sent": "Each of the two directions.",
                    "label": 0
                },
                {
                    "sent": "We will subsume the most general model in the literature so far, respectivly business with knapsacks and contextual bandits with the policy sets.",
                    "label": 1
                },
                {
                    "sent": "And then we obtain optimal regret for our model.",
                    "label": 1
                },
                {
                    "sent": "So that's the high level picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let me get into some details and define the model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are delimited.",
                    "label": 0
                },
                {
                    "sent": "There are multiple limited resources, you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe we're selling Donuts and cookies with a limited supply, which is, so there is a known budget on each resource and we need to stop whenever some resources exhausted.",
                    "label": 0
                },
                {
                    "sent": "So what happens in each round?",
                    "label": 0
                },
                {
                    "sent": "Well, a context arrives drawn IID from some known distribution over context, the algorithm picks an action from a fixed.",
                    "label": 1
                },
                {
                    "sent": "A set of actions and then the outcome now is a vector.",
                    "label": 0
                },
                {
                    "sent": "It consists of the reward.",
                    "label": 0
                },
                {
                    "sent": "And the end, the consumption of each resource.",
                    "label": 0
                },
                {
                    "sent": "So this vector is drawn independently from some distribution over vectors, and this distribution depends only on the context in this round and the chosen action, and this distribution is not known to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the goal is, as usual, to maximize the total expected reward anfora normalization, we assume that all per round rewards and off around consumptions are in the 01.",
                    "label": 1
                },
                {
                    "sent": "So that's the model.",
                    "label": 0
                },
                {
                    "sent": "That's what happens in this in our setting now.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, and it's not a huge distraction, but.",
                    "label": 0
                },
                {
                    "sent": "So yes, we're still open.",
                    "label": 0
                },
                {
                    "sent": "Any resources exhaust so.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, as usual we care about regret.",
                    "label": 0
                },
                {
                    "sent": "It's defined as the benchmark, minus the total spectator word of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The benchmark is before the best all known algorithm, which in this case is, you know, the algorithm that knows these hidden distributions of the outcome vectors.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Well, this benchmark maybe a little bit too harsh, so we relax it in a very general way in the following way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider a policies that is mappings from context to actions.",
                    "label": 1
                },
                {
                    "sent": "That is fixed overtime just to fix mapping from context to actions, and we assume that the algorithm is given some set of policies, which maybe you know which may include some but all but not all policies and they.",
                    "label": 0
                },
                {
                    "sent": "The benchmark is relative to the set of policies in the sense that it's the best all knowing algorithm that is restricted, you know that is restricted to only use these policies.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because of resource constraints.",
                    "label": 1
                },
                {
                    "sent": "It may be the case that this benchmark is much better than the best fixed policy.",
                    "label": 1
                },
                {
                    "sent": "And essentially it's the best fixed mixture of policies in the policies that so, and they mean this happens not only some weird cases, but essentially in any special case one would care about.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model is quite general, so in the pricing either dynamic pricing example that they give we have an algorithm being a seller with limited supply of items in each Round, Hill first one item for sale at the chosen price.",
                    "label": 1
                },
                {
                    "sent": "They reward is essentially money and contact this customer profile.",
                    "label": 0
                },
                {
                    "sent": "Another example is that an algorithm could be an employer in the crowd sourcing market with a limited budget of money and in each round he offers one test to perform at the chosen price.",
                    "label": 0
                },
                {
                    "sent": "Selections are prices.",
                    "label": 0
                },
                {
                    "sent": "And the rewards are essentially the completed tasks, and context could be the profile of a worker or other task.",
                    "label": 0
                },
                {
                    "sent": "Yet another example, it's allocation of paper click ads where resources are limited advertisers budgets and in each round the algorithm picks one ad to display and observes a click on auto click and essentially the rewards are clicks and the context is profile of a page of or or a user.",
                    "label": 0
                },
                {
                    "sent": "Now, each of these examples may be extended in many different ways, and there are a lot more examples in the paper.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that like so, one way to think about the policies set is that.",
                    "label": 0
                },
                {
                    "sent": "So not sure if any big if anybody can see this, but that's capital Pi everywhere so is that.",
                    "label": 0
                },
                {
                    "sent": "We fix a given method of learning from offline data such as linear regression or decision trees or whatnot, and we define the policies set as all policies that can possibly be learned from offline data using this method so.",
                    "label": 1
                },
                {
                    "sent": "It's actually quite general.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What was in the related work?",
                    "label": 1
                },
                {
                    "sent": "Well, the special case of no context has been defined and resolved in our recent folks paper, and there have been many, like prior papers on various special cases of that.",
                    "label": 1
                },
                {
                    "sent": "Then the likewise the special case of no resources has also been studied in prior work.",
                    "label": 0
                },
                {
                    "sent": "Moreover, there was a very.",
                    "label": 1
                },
                {
                    "sent": "There is a very recent paper in this easy, which consider is another model for like.",
                    "label": 1
                },
                {
                    "sent": "Contextual values with resource constraints, which is incompatible with ours in the sense that it's more general on how they model limited resources, but much less general on how they model contexts.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll remain.",
                    "label": 0
                },
                {
                    "sent": "The result is that you know we design an algorithm with this regret in terms of the various parameters.",
                    "label": 0
                },
                {
                    "sent": "Now this formula is not supposed immediately I tell you anything but.",
                    "label": 0
                },
                {
                    "sent": "Let me comment on this, so it's optimal in the sense that if we scale all constraints including time by some common factor Alpha, then regret scales as root above, which is known to be the optimal scaling and in.",
                    "label": 0
                },
                {
                    "sent": "Bandit problems also if the benchmark is upper bounded by a constant fraction of the of the budget, then also obtain root of Katie regret up to work factors, which is also known to be optimal, and Moreover the logarithmic dependence on the number of policies is known to be optimal even in even without re services.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, this regret is optimal.",
                    "label": 1
                },
                {
                    "sent": "Now one significant caveat is that our algorithm is very computationally inefficient, so it's essentially a proof of concept that optimal that such regret can be achieved information theoretically.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are the challenges in this problem compared to the standard bandit problem?",
                    "label": 0
                },
                {
                    "sent": "Why is this interesting?",
                    "label": 0
                },
                {
                    "sent": "You know why that's an interesting problem?",
                    "label": 0
                },
                {
                    "sent": "Well, due to limited resources we cannot think of maximizing expected per round reward.",
                    "label": 1
                },
                {
                    "sent": "It's really not the right goal because you know if we do this, we may exhaust resources way too fast.",
                    "label": 0
                },
                {
                    "sent": "So instead we really must think about the expected total reward over the entire time horizon.",
                    "label": 1
                },
                {
                    "sent": "Secondly, they mentioned the best all knowing algorithm here is not a fixed policy, but essentially best fixed mixture of policies.",
                    "label": 1
                },
                {
                    "sent": "So essentially we need to search over these mixtures of policies.",
                    "label": 0
                },
                {
                    "sent": "So now due to context, there are some additional complications.",
                    "label": 0
                },
                {
                    "sent": "Well, we can have a trivial reduction to the prior work on the special case of no contexts where we just think of each policy as a you know meter arm in some bandit algorithm which uses in each round among local policies.",
                    "label": 1
                },
                {
                    "sent": "So this is just a trivial reduction and we obtained regret the scales as a root of the number of policies.",
                    "label": 0
                },
                {
                    "sent": "But instead we get a log.",
                    "label": 0
                },
                {
                    "sent": "And so you know, it's an exponential improvement.",
                    "label": 0
                },
                {
                    "sent": "And in order to achieve that, essentially in each round we need to explore many policies at once.",
                    "label": 0
                },
                {
                    "sent": "We cannot just explore one policy, we need to explore a many policies at once.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Outline some many ideas in the algorithm so you know as usual and bandit problems we have.",
                    "label": 0
                },
                {
                    "sent": "Explore exploit tradeoff and.",
                    "label": 0
                },
                {
                    "sent": "As usual, in order to get the optimal regret, we need to adapt exploration to past observationes.",
                    "label": 1
                },
                {
                    "sent": "We cannot just keep, you know, we cannot just explore uniformly at random.",
                    "label": 0
                },
                {
                    "sent": "And also in some sense we need to explore and exploit at the same time.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the goal is to zoom in on the optimal mixture of policies and the given policies set just because that's the that's what the benchmark is.",
                    "label": 0
                },
                {
                    "sent": "So OK, what what do we do in each round?",
                    "label": 0
                },
                {
                    "sent": "Well, we focus on a plausibly optimal mixture of policies, meaning mixtures that.",
                    "label": 1
                },
                {
                    "sent": "Might be optimal given the data that we have so far and like I'm not defining many of the details.",
                    "label": 0
                },
                {
                    "sent": "But you know, given the data that we have so far, we can form estimates and you know high confidence estimates and with these high confidence estimates, some things.",
                    "label": 0
                },
                {
                    "sent": "Some mixtures are clearly suboptimal and we only care about the remaining ones.",
                    "label": 1
                },
                {
                    "sent": "So in each round we pick some plausibly optimal mixture well, then with some low probability, we pick an actual uniformly at random with remaining probability withdrawal policy.",
                    "label": 0
                },
                {
                    "sent": "You know, independently from this mixture and with your connection recommended by this point, I see so so far so good and essentially takes care of exploitation.",
                    "label": 0
                },
                {
                    "sent": "Now the kicker is that we simultaneously need to take care of exploration, namely not only we need to pick a plausibly optimal make sure, but we need to pick it in a way that we get to explore every single a policy in the policy set.",
                    "label": 0
                },
                {
                    "sent": "At once.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that they action that we that our algorithm appears becausw we choose this make sure most coincide with the action recommended by.",
                    "label": 0
                },
                {
                    "sent": "Any given policy a sorry, so for any given policy Pi star in the \u03a0 prime and the policy said the action that we pick must coincide with the action recommended by this policy with a near optimal probability in some sense of in Europe.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the outline of what we do in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And of course the technical details here are about, you know, defining exactly what we mean by near optimal probability and how we make sure that you know such optimal mixture can be found, and then you know analyzing regret.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going into these details there in the paper.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty much what I want to say for the technical quiet.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are bunch of open questions, so the main open question is that wanna get you know optimal regret and a good running sign.",
                    "label": 0
                },
                {
                    "sent": "And this has been resolved for the special case of now contexts, and very, very recently in this HTML.",
                    "label": 0
                },
                {
                    "sent": "For the special case of notary services.",
                    "label": 0
                },
                {
                    "sent": "So there is a hope, but it looks like it's going to be very hard.",
                    "label": 0
                },
                {
                    "sent": "And then there are many other very nice open questions, even for the special case of no context out of those, my personal favorite is to go beyond IAD environments.",
                    "label": 1
                },
                {
                    "sent": "I mean, what happens if the environment can change overtime and?",
                    "label": 0
                },
                {
                    "sent": "You know other serial or maybe like some limited adversarial way.",
                    "label": 0
                },
                {
                    "sent": "Also manual regret bounds.",
                    "label": 1
                },
                {
                    "sent": "I mean, our regret bounds are not tight for some important.",
                    "label": 0
                },
                {
                    "sent": "Special cases, I mean, the tighten the general worst case, but for some important special cases there are not and finally you know if actions are prices, we can only handle.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm can only handle a finite number of those.",
                    "label": 1
                },
                {
                    "sent": "So we need to discretize the prices and this turns out to be tricky even for a single limited resource.",
                    "label": 0
                },
                {
                    "sent": "And it's completely not understood how to do it in the optimal way for multiple limited resources.",
                    "label": 0
                },
                {
                    "sent": "You know, say we're selling cookies and doughnuts.",
                    "label": 0
                },
                {
                    "sent": "It's completely not clear how to discretize.",
                    "label": 0
                },
                {
                    "sent": "OK, so and there are a bunch of other nice open questions in the paper, so I guess with that I'd like to conclude.",
                    "label": 0
                }
            ]
        }
    }
}