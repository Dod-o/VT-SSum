{
    "id": "pnqdyypgsvfp7fhrmtmhawoat4htielt",
    "title": "Cross Language Text Classification via Multi-view Subspace Learning",
    "info": {
        "author": [
            "Yuhong Guo, Department of Computer and Information Sciences, Temple University"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Multilingual Information Access",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_guo_subspace_learning/",
    "segmentation": [
        [
            "I will present her work across language text classification with multi view subspace learning this is joint work with Michelle.",
            "Other ones me."
        ],
        [
            "So the problem we consider is the document categorization documents in different languages may share the same set of categories.",
            "For example, if we have newsgroup data in English and French.",
            "You know the early can share the same set of categories, so in this case if we use the Thunder, the monolingual text classification, we need to collect the substrate amount for labeled data in each language, and this leads to very expensive document annotation.",
            "So the question here is, how about we using the label data in one language to help classification in the other language?",
            "Or this is across language text classification."
        ],
        [
            "So cross out the idea of across language.",
            "Text classification is just we explore the labeled data existing in language A to classify documents in another language B.",
            "And that we can reduce the expensive RIP labeling process.",
            "The existing cross language text classification methods rely on machine translation, so it's basic idea is we first translate the documents from the source language into the target language B or vice versa, and then we can apply the standard model lingual text classification measures."
        ],
        [
            "But there are two problems for those simple cross lingual text classification methods first.",
            "1st is even we translate the documents into the other language, but there is a feature distribution divergent between the original documents in the translated documents in their language and the second problem is there is information lost in translation errors during the automatically machine translation because it's not high quality qualified translation.",
            "So to solve the first problem you know we can use the domain adaptation measures because domain dictation Cambridge is government divergance.",
            "For the second problem, actually we can use multi view learning because Matt will learning can access surgical documents in both languages.",
            "So we can elevate this translation loss.",
            "But the problem is how can we address those two problems simultaneously and then we can gain more advantages.",
            "So this is basically our motivation."
        ],
        [
            "So in this work we propose the subspace Co regularize multiview learning method.",
            "Still we rely relied on.",
            "Automatic machine translation.",
            "So first we translate documents in each language into the other one using machine translation and then we can form 2 parallel data matrixes.",
            "So it's like 2 views, one for each language.",
            "If we consider 2 languages and then we can explore the data from both views to elevate the translation loss.",
            "And based on those parallel matrices matrices.",
            "And then we learn a discriminative subspace representation of the multi view documents.",
            "So the subspace learning we we want to capture the interesting structure of the data and to bridge that domain divergent problem.",
            "So in."
        ],
        [
            "Particular in each view will first project the data into a low dimensional.",
            "Low dimensional subspace and design.",
            "We use a linear prediction function to learn the classification model.",
            "So for the ice view.",
            "So here is a linear prediction function.",
            "So for the X that Sergeant data we first projected into low dimensional space using orthogonal projection matrix matrix and then we learn a linear prediction function.",
            "And based on Sir."
        ],
        [
            "Your linear prediction function.",
            "We formulate the overall learning problem.",
            "As this minimization problem, so this is for the two we were learning, but you can expand it into materials in a very natural, simple way.",
            "So for this minimization problem we have three parts.",
            "For the first part is simply the loss function for predictors on the labeled data.",
            "So here we have two wheels and the second part is simply the regularizer for the linear prediction model parameter.",
            "So third part.",
            "Is subspace Co regularization.",
            "So we record the distance of the two views on the project low dimensional space.",
            "Because of their their parallel matrix matrix is, so we supposed to get very similar low dimensional representations.",
            "So this is a joint minimization problem over the projection matrices and linear prediction model perimetre."
        ],
        [
            "So how to solve it?",
            "So first we solve the minimization over the linear.",
            "Prediction model para meters and for closed form solution and then we can reformulate this whole overall minimization problem into minimization over the projection matrix is set to one is it 2?",
            "So here's the objective function.",
            "This is orthogonal constraint optimization problem.",
            "If you look at this objective function can see here is simply the least score prediction loss.",
            "For this part we have metrics in words.",
            "But actually it's not computationally expensive because the matrix inverse is on the low dimensional space.",
            "In our experiment, we use like a low dimensional space, like 10 or 20, so it works pretty well."
        ],
        [
            "So the question now is how we going to solve this orthogonal constraint.",
            "The general optimization problem?",
            "Our idea is that we used, we used a gradient descent procedure with curvilinear search, so the advantage of this procedure is it requires no local projections.",
            "And we will always stay in the visible orthogonal region and can converge to a local optimal solution."
        ],
        [
            "So here is a detail that I want to escape."
        ],
        [
            "We conducted some experiments on multilingual data set on the Reuters data.",
            "It has five languages, English, French, German, Italian, Spanish and we compare the.",
            "A group of four measure the first baseline measure the simple cross lingual text classification.",
            "So we translate the data from the source language into the target language and design to do modeling.",
            "Go classification training.",
            "The second type of algorithm is we not just translation translates data and then we apply domain reputation misers.",
            "A third type of approaches.",
            "We use multiple learning and so as the ways our proposed approach subspace code regularize material learning."
        ],
        [
            "So on the foul languages, we constructed a chanted chorus, longer detects classroom tasks, so for each pair ordered pair for languages and so here is a classification accurate results.",
            "So the last column is the proposed approach.",
            "So the proposed approach actually consistently.",
            "Outperforms both multi view learning and domain dietitian so the results were presented here.",
            "We use the lurton subspace alert and the martian's turn so actually we also tested using different low dimensional size like using 20 or 30.",
            "Performance does not change much so it's quite space.",
            "Yeah, so E2F word means we use English as a source branch as a target, and then E 2G is English as a source.",
            "Germany's target, so overall they're 20 pairs from a 5 languages.",
            "Languages that you can say.",
            "Translating between 2 languages or classification.",
            "Sorry I didn't get a question.",
            "This is easier than for others.",
            "Oh yeah, there.",
            "Well, we can see is.",
            "Yeah is.",
            "This seems a different language pairs so you can see we have different performance different where is a little bit.",
            "On some some languages like we can get over 90% for the accuracy, But the other ones is around like 80 Spanish, yeah?",
            "Interesting that it's not symmetric like.",
            "Spanish is very easy in Spanish.",
            "Because it look for the like English to French or French to English.",
            "The training data different right?",
            "Because OK, so this is we assume we have more label data in the source language.",
            "We have fewer labeled data in the target language.",
            "That's the main idea, right?",
            "If we have a lot of label data in the target language was appointed for use this source language at all.",
            "So that's why if you have different into English, French, French to English and should be different because one is, you have more Labor Day to English, the others you have more liberated.",
            "Lunch.",
            "What was the categorization schema which you're classifying into?",
            "Weather classification labels.",
            "Maybe you show them.",
            "This is a two class classification problem.",
            "Yeah, this is to class and what is the semantics of this deposit?",
            "We picked two class from this Reuters data set I forgot which from the road test data set."
        ],
        [
            "Here I think as a six class, so this is a router stateside multilingual.",
            "It has six class.",
            "We picked two Class 2 classes which larger two classes so we can have more testing data.",
            "So 20 glasses, so if you pictures.",
            "Actually another work, but not this.",
            "We also tested on multiclass classification for the six classes and still if you if we use both multiview learning subspace learning actually we can gain a substantial advantages."
        ],
        [
            "Yeah."
        ],
        [
            "It's."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will present her work across language text classification with multi view subspace learning this is joint work with Michelle.",
                    "label": 0
                },
                {
                    "sent": "Other ones me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem we consider is the document categorization documents in different languages may share the same set of categories.",
                    "label": 1
                },
                {
                    "sent": "For example, if we have newsgroup data in English and French.",
                    "label": 1
                },
                {
                    "sent": "You know the early can share the same set of categories, so in this case if we use the Thunder, the monolingual text classification, we need to collect the substrate amount for labeled data in each language, and this leads to very expensive document annotation.",
                    "label": 1
                },
                {
                    "sent": "So the question here is, how about we using the label data in one language to help classification in the other language?",
                    "label": 0
                },
                {
                    "sent": "Or this is across language text classification.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So cross out the idea of across language.",
                    "label": 0
                },
                {
                    "sent": "Text classification is just we explore the labeled data existing in language A to classify documents in another language B.",
                    "label": 1
                },
                {
                    "sent": "And that we can reduce the expensive RIP labeling process.",
                    "label": 0
                },
                {
                    "sent": "The existing cross language text classification methods rely on machine translation, so it's basic idea is we first translate the documents from the source language into the target language B or vice versa, and then we can apply the standard model lingual text classification measures.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there are two problems for those simple cross lingual text classification methods first.",
                    "label": 0
                },
                {
                    "sent": "1st is even we translate the documents into the other language, but there is a feature distribution divergent between the original documents in the translated documents in their language and the second problem is there is information lost in translation errors during the automatically machine translation because it's not high quality qualified translation.",
                    "label": 1
                },
                {
                    "sent": "So to solve the first problem you know we can use the domain adaptation measures because domain dictation Cambridge is government divergance.",
                    "label": 0
                },
                {
                    "sent": "For the second problem, actually we can use multi view learning because Matt will learning can access surgical documents in both languages.",
                    "label": 0
                },
                {
                    "sent": "So we can elevate this translation loss.",
                    "label": 1
                },
                {
                    "sent": "But the problem is how can we address those two problems simultaneously and then we can gain more advantages.",
                    "label": 0
                },
                {
                    "sent": "So this is basically our motivation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we propose the subspace Co regularize multiview learning method.",
                    "label": 1
                },
                {
                    "sent": "Still we rely relied on.",
                    "label": 0
                },
                {
                    "sent": "Automatic machine translation.",
                    "label": 0
                },
                {
                    "sent": "So first we translate documents in each language into the other one using machine translation and then we can form 2 parallel data matrixes.",
                    "label": 1
                },
                {
                    "sent": "So it's like 2 views, one for each language.",
                    "label": 1
                },
                {
                    "sent": "If we consider 2 languages and then we can explore the data from both views to elevate the translation loss.",
                    "label": 0
                },
                {
                    "sent": "And based on those parallel matrices matrices.",
                    "label": 0
                },
                {
                    "sent": "And then we learn a discriminative subspace representation of the multi view documents.",
                    "label": 0
                },
                {
                    "sent": "So the subspace learning we we want to capture the interesting structure of the data and to bridge that domain divergent problem.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particular in each view will first project the data into a low dimensional.",
                    "label": 1
                },
                {
                    "sent": "Low dimensional subspace and design.",
                    "label": 1
                },
                {
                    "sent": "We use a linear prediction function to learn the classification model.",
                    "label": 0
                },
                {
                    "sent": "So for the ice view.",
                    "label": 0
                },
                {
                    "sent": "So here is a linear prediction function.",
                    "label": 0
                },
                {
                    "sent": "So for the X that Sergeant data we first projected into low dimensional space using orthogonal projection matrix matrix and then we learn a linear prediction function.",
                    "label": 0
                },
                {
                    "sent": "And based on Sir.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your linear prediction function.",
                    "label": 0
                },
                {
                    "sent": "We formulate the overall learning problem.",
                    "label": 0
                },
                {
                    "sent": "As this minimization problem, so this is for the two we were learning, but you can expand it into materials in a very natural, simple way.",
                    "label": 0
                },
                {
                    "sent": "So for this minimization problem we have three parts.",
                    "label": 0
                },
                {
                    "sent": "For the first part is simply the loss function for predictors on the labeled data.",
                    "label": 1
                },
                {
                    "sent": "So here we have two wheels and the second part is simply the regularizer for the linear prediction model parameter.",
                    "label": 0
                },
                {
                    "sent": "So third part.",
                    "label": 0
                },
                {
                    "sent": "Is subspace Co regularization.",
                    "label": 0
                },
                {
                    "sent": "So we record the distance of the two views on the project low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "Because of their their parallel matrix matrix is, so we supposed to get very similar low dimensional representations.",
                    "label": 0
                },
                {
                    "sent": "So this is a joint minimization problem over the projection matrices and linear prediction model perimetre.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to solve it?",
                    "label": 0
                },
                {
                    "sent": "So first we solve the minimization over the linear.",
                    "label": 1
                },
                {
                    "sent": "Prediction model para meters and for closed form solution and then we can reformulate this whole overall minimization problem into minimization over the projection matrix is set to one is it 2?",
                    "label": 0
                },
                {
                    "sent": "So here's the objective function.",
                    "label": 0
                },
                {
                    "sent": "This is orthogonal constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "If you look at this objective function can see here is simply the least score prediction loss.",
                    "label": 0
                },
                {
                    "sent": "For this part we have metrics in words.",
                    "label": 0
                },
                {
                    "sent": "But actually it's not computationally expensive because the matrix inverse is on the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In our experiment, we use like a low dimensional space, like 10 or 20, so it works pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question now is how we going to solve this orthogonal constraint.",
                    "label": 0
                },
                {
                    "sent": "The general optimization problem?",
                    "label": 0
                },
                {
                    "sent": "Our idea is that we used, we used a gradient descent procedure with curvilinear search, so the advantage of this procedure is it requires no local projections.",
                    "label": 1
                },
                {
                    "sent": "And we will always stay in the visible orthogonal region and can converge to a local optimal solution.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a detail that I want to escape.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We conducted some experiments on multilingual data set on the Reuters data.",
                    "label": 0
                },
                {
                    "sent": "It has five languages, English, French, German, Italian, Spanish and we compare the.",
                    "label": 0
                },
                {
                    "sent": "A group of four measure the first baseline measure the simple cross lingual text classification.",
                    "label": 0
                },
                {
                    "sent": "So we translate the data from the source language into the target language and design to do modeling.",
                    "label": 0
                },
                {
                    "sent": "Go classification training.",
                    "label": 0
                },
                {
                    "sent": "The second type of algorithm is we not just translation translates data and then we apply domain reputation misers.",
                    "label": 0
                },
                {
                    "sent": "A third type of approaches.",
                    "label": 0
                },
                {
                    "sent": "We use multiple learning and so as the ways our proposed approach subspace code regularize material learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on the foul languages, we constructed a chanted chorus, longer detects classroom tasks, so for each pair ordered pair for languages and so here is a classification accurate results.",
                    "label": 0
                },
                {
                    "sent": "So the last column is the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "So the proposed approach actually consistently.",
                    "label": 0
                },
                {
                    "sent": "Outperforms both multi view learning and domain dietitian so the results were presented here.",
                    "label": 0
                },
                {
                    "sent": "We use the lurton subspace alert and the martian's turn so actually we also tested using different low dimensional size like using 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "Performance does not change much so it's quite space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so E2F word means we use English as a source branch as a target, and then E 2G is English as a source.",
                    "label": 0
                },
                {
                    "sent": "Germany's target, so overall they're 20 pairs from a 5 languages.",
                    "label": 0
                },
                {
                    "sent": "Languages that you can say.",
                    "label": 0
                },
                {
                    "sent": "Translating between 2 languages or classification.",
                    "label": 0
                },
                {
                    "sent": "Sorry I didn't get a question.",
                    "label": 0
                },
                {
                    "sent": "This is easier than for others.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, there.",
                    "label": 0
                },
                {
                    "sent": "Well, we can see is.",
                    "label": 0
                },
                {
                    "sent": "Yeah is.",
                    "label": 0
                },
                {
                    "sent": "This seems a different language pairs so you can see we have different performance different where is a little bit.",
                    "label": 0
                },
                {
                    "sent": "On some some languages like we can get over 90% for the accuracy, But the other ones is around like 80 Spanish, yeah?",
                    "label": 0
                },
                {
                    "sent": "Interesting that it's not symmetric like.",
                    "label": 0
                },
                {
                    "sent": "Spanish is very easy in Spanish.",
                    "label": 0
                },
                {
                    "sent": "Because it look for the like English to French or French to English.",
                    "label": 0
                },
                {
                    "sent": "The training data different right?",
                    "label": 0
                },
                {
                    "sent": "Because OK, so this is we assume we have more label data in the source language.",
                    "label": 0
                },
                {
                    "sent": "We have fewer labeled data in the target language.",
                    "label": 0
                },
                {
                    "sent": "That's the main idea, right?",
                    "label": 0
                },
                {
                    "sent": "If we have a lot of label data in the target language was appointed for use this source language at all.",
                    "label": 0
                },
                {
                    "sent": "So that's why if you have different into English, French, French to English and should be different because one is, you have more Labor Day to English, the others you have more liberated.",
                    "label": 0
                },
                {
                    "sent": "Lunch.",
                    "label": 0
                },
                {
                    "sent": "What was the categorization schema which you're classifying into?",
                    "label": 0
                },
                {
                    "sent": "Weather classification labels.",
                    "label": 0
                },
                {
                    "sent": "Maybe you show them.",
                    "label": 0
                },
                {
                    "sent": "This is a two class classification problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is to class and what is the semantics of this deposit?",
                    "label": 0
                },
                {
                    "sent": "We picked two class from this Reuters data set I forgot which from the road test data set.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I think as a six class, so this is a router stateside multilingual.",
                    "label": 0
                },
                {
                    "sent": "It has six class.",
                    "label": 0
                },
                {
                    "sent": "We picked two Class 2 classes which larger two classes so we can have more testing data.",
                    "label": 0
                },
                {
                    "sent": "So 20 glasses, so if you pictures.",
                    "label": 0
                },
                {
                    "sent": "Actually another work, but not this.",
                    "label": 0
                },
                {
                    "sent": "We also tested on multiclass classification for the six classes and still if you if we use both multiview learning subspace learning actually we can gain a substantial advantages.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        }
    }
}