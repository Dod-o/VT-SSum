{
    "id": "dh47dgzw4edqyuaabnr6g2jcdn3ar25u",
    "title": "High-dimensional Statistics: Prediction, Association and Causal Inference",
    "info": {
        "author": [
            "Peter B\u00fchlmann, ETH Zurich"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_buhlmann_hds/",
    "segmentation": [
        [
            "Thank you Isabelle for this nice introduction.",
            "Maybe it would have been better if my students would give the presentation because they want the competition and not myself.",
            "I tremendously enjoy also being here.",
            "Thank you for the invitation.",
            "It's a wonderful place, an exciting meeting, and it's really a good thing to be here.",
            "The tutorial will be about high dimensional statistics as he Isabel already said a bit and 1st I would really like to acknowledge somehow my."
        ],
        [
            "Main collaborators in this area over the last years, these are colleagues from ETS Zurich, Sarah family, here, Marlou, Smart 1000, Marcus Kalisz, and my former student, who is now in Oxford, Nikli Meinshausen."
        ],
        [
            "So first I would like to start with just giving two brief examples about high dimensional data and then I will develop further and say a bit something about methodology and some sort of theory and go on.",
            "So the first example is some sort of textbook example.",
            "By now it's classification of tumor samples, whether a sample is tumors or not, or whether it's a sub sub tumor a versus a subtype B and you try to classify using high dimensional gene expression.",
            "Measurements, so by now this is almost kind of a classical task, So what we have if we measure the expression of thousands of genes here, a bit more than 7000 and there are only a dozen people in the study.",
            "So here it's 49 and then you can run some algorithms you can look at the misclassification error, do some cross validation to come up with these numbers, and then you publish numbers.",
            "I mean, I'm not suspecting that you can read this, but you can compare different methods and just go on and then just discuss about how good you can do.",
            "So this is more a predictive task.",
            "It's I would say almost a textbook example by now.",
            "The."
        ],
        [
            "Other example is maybe a bit more about scientific understanding.",
            "This is about the riboflavin or vitamin production with Bacillus and the facility is called Bacillus subtilis.",
            "This is actually a project with a company with Dutch state mining company in Switzerland and a very general goal of the companies to improve the vitamin production rate of the bacillus using clever genetic engineering.",
            "If you succeed in improving that production rate, this is a huge revenue for the company.",
            "And of course, this is a huge problem and an aspect or some aspects of the problem involve also data and statistics and machine learning, and what we have here is just we measure response variables.",
            "Actually how effective the bacillus is in producing the vitamin.",
            "So we measure the riboflavin vitamin log production rate.",
            "This is my universe, your response variable Y and the game that have fairly high dimensional covariance XI measure the expressions from all the jeans.",
            "In the genome of Bacillus subtilis.",
            "So this is again a microarray chip.",
            "If you measure a bit more than 4000 variables, the expression of more than 4000 genes and sample size here is 115, so 115 individuals which you have measured.",
            "Apparently again the number of variables is much, much larger than sample size and this is high dimensional statistics.",
            "Now here's just a visualization of the gene expression data set without a wide without the response variable, and you just can visualize it a bit.",
            "You don't see much, and on the right hand side we see the very exploratory way of looking at the data set.",
            "You just do a couple of scatter plots and plot Y against some of these variables.",
            "Some of these genes, and actually we selected here 9 reasonable jeans, and when we can just see from an exploratory POV, for example here.",
            "And also there you see exploratory that there is some sort of marginal Association between the response Y and at least one of these jeans, or some of these genes.",
            "So here the question is really more, if not so much about prediction.",
            "It's more trying to find out which variables are actually important, which variables, which genes actually drive the production rate."
        ],
        [
            "OK, so in my tutorial I will largely focus on a linear model.",
            "A high dimensional linear model first of all because it's simple and Secondly I believe it's yet still very useful in many applications in high dimensional statistical inference.",
            "So what we're looking at is just a linear model of why Arjuna.",
            "Varied response Yi may have an intercept and have a linear combination of the covariates.",
            "XI always denote the variable J with this upper index J and.",
            "Eyes the sample index and then have a stochastic error term is mean zero.",
            "Everything is standard except that peed and number of covariates maybe or is typically much larger in sample size in, so this is the shorthand notation.",
            "Why is Expedia plus epsilon like in all textbooks in regression?",
            "And the girls are actually in the high dimensional statistical inference very similar to or in a sense the same.",
            "What we have also in the classical setting.",
            "We may be interested in prediction.",
            "How well can we predict the future observation we may be interested in estimating just the high dimensional parameter vector beta and then from there we may want to move on and say something about feature or variable selection.",
            "Saying which of these variables have corresponding regression coefficients different from zero?",
            "Which of these variables?",
            "Are important in that regard."
        ],
        [
            "Asian model.",
            "OK, so here's again a bit exemplifying the outline of the tutorial.",
            "Let's look again at one of these classification problems, so this is actually a binary lymph node classification problem.",
            "The outcome is just a label zero or one.",
            "If it's zero, it's non tumors and if it's one, it's a tumor sample.",
            "And again the covariates are gene expressions.",
            "A bit more than 7000 genes involved in actually a high noise problem.",
            "It's a difficult problem to do the classification.",
            "And So what you can do here, if you just look at this linear model and if you have the linear model in mind, you know that the target you're going for is actually the conditional mean.",
            "The conditional mean of Y given X.",
            "That doesn't sound like a classification, but if Y is binary, of course this is the same as just the conditional class probability.",
            "So in this special case where Y is binary disease, the conditional class probability denoted by PX.",
            "So if I can fit my linear model, if I can estimate my linear model.",
            "I get an estimate P hat X and then I can do classification if she had X is larger than 1/2, I assign one and if it's below half I sign a 0.",
            "Then you can run some cross validation.",
            "You rap across validation around it.",
            "He was randomly division in two third training, 1/3 test set and then you can look at a couple methods right here is the law so I will explain it here is boosting here other methods.",
            "Here's an SVM.",
            "The method in red here.",
            "They do feature selection they have built in variable or feature selection.",
            "There's the methods in black here.",
            "This is a one nearest neighbor, a diagonal linear discriminate analysis.",
            "And so they don't do any feature selection.",
            "OK, so you can just look at these numbers.",
            "And then you can argue well, from a practical point of view, if you trust in cross validation.",
            "If you just want to do prediction, you're done in a way, right.",
            "You just trust your cross validation.",
            "You get these numbers, you don't need any theory.",
            "Maybe, and then you."
        ],
        [
            "Say OK, I just try out which one is the best.",
            "OK?",
            "So again, you have to cross validation handle in your hand and you can validate how good you are.",
            "It's pretty easy if you're just interested in prediction.",
            "It's pretty easy to do."
        ],
        [
            "OK so however, of course we want to say something more than just stop there.",
            "The first criticism which is maybe semi convincing is that be kind of no.",
            "That cross validation is very variable, so actually these."
        ],
        [
            "Numbers are maybe not that accurate.",
            "OK, so."
        ],
        [
            "Maybe we still want to know a bit more, some sort of methodological and theoretical properties.",
            "What different methods are doing, and then if you go on beyond prediction if you go into estimation.",
            "So what can I say?",
            "How well my estimate beta hat approaches the true underlying regression parameter.",
            "Now sometimes in my presentation I denote the true parameter by a beta 0 or a beta, not to make sure that you know this is the true underlying regression parameter.",
            "So if you're interested in that kind of.",
            "Norm in kind of how good we can capture the true parameter.",
            "Then there is nothing like a cross validation at hand.",
            "I mean there's you cannot see from cross validation how well you do in kind of estimating the true parameter, and from there you can go on and you can say OK if I'm interested in getting actually the right feature.",
            "If I want to know the active set, the S 0, these are just the variables whose corresponding regression coefficients are different from zero.",
            "It's hard.",
            "I mean you cannot run across validation and see.",
            "Which made respect for feature selection versus another one.",
            "So, and this is in a way the outline what I want to do.",
            "First I go on the easy task on prediction and then I move onto estimation of the parameter to variable selection.",
            "I focus on the regression and classification and once we understand that we can move on the graphical models and at the end I want to say something about intervention causal analysis."
        ],
        [
            "OK, so in the linear model and actually many statistical regression type classification models, the lasso DL1 penalization technique is very popular in this.",
            "Also very useful, and I just go into.",
            "I mean I start from scratch, so here is again what the lasso is.",
            "So in my high dimensional linear model I want to estimate the unknown high dimensional regression parameter beta.",
            "So what you're going to do is you minimize residual sum of squares, right?",
            "And then you have to regularize because it's a high dimensional problem.",
            "So without this term here this is just order release squares, but that would heavily overfit your problem.",
            "So you regularize and the regularization is this by now famous L1 norm regularization or penalization.",
            "You take the L1 norm of your high dimensional coefficient vector.",
            "Just some of the absolute values.",
            "And here you have a regularization parameter Lambda which you have to choose.",
            "In practice, is the tuning parameter.",
            "So the advantage of doing this is this is extremely fast in computing because this is a convex optimization problem.",
            "The squared error loss is a convex function in beta and a penalty term is convex in beta as well.",
            "So this is beautiful.",
            "You can compute that very efficiently very fast, and so it's nice.",
            "So from a computational point of view this is great.",
            "And now we want to understand what is it actually doing for your statistical inference problem.",
            "How well can you now estimate your parameter and so on?",
            "And there are two basic properties.",
            "The first one is the last sort is L1 regularization technique.",
            "It does variable selection or feature selection in the sense that the estimate ear beta hat say for a component J maybe exactly 0 OK depending on how you choose your Lambda and this is very different from other sorts of regularization.",
            "So this kind of L1 norm regularization has what people call the sparsity principle or the sparsity property.",
            "It may be exactly 0, so it puts some of the coefficients exactly to zero, and this is because of the L1 geometry.",
            "I will just explain in the next slide and more generally you can think of the beta is some sort of shrunken least squares estimate.",
            "Sometimes you shrink the coefficients exactly 0 and sometimes not.",
            "OK so here is this L1 geometry of the problem.",
            "When you look at this, this is how in at least in statistics we typically write it down.",
            "This is in the LaGrange multiplier form, so he."
        ],
        [
            "Is my LaGrange term and so of course I can go to the other view and just look at what I call the primal problem.",
            "I could equivalently formulate the problem as OK, let's minimize residual sum of squares under the constraint that the solution has L1 norm less or equal in some value R. OK, you make constraint optimization with respect to the L1 norm and there is a one to one correspondence between the LaGrange multiplier Lambda and the parameter R. Unfortunately, this correspondence is not.",
            "I mean, this depends on the data.",
            "You cannot give a formula for this correspondence, but it always exists.",
            "OK, and the reason is, I mean you have here a convex function, the squared error and you have a convex constraint, so he can jump back and force from the primal to dual problem."
        ],
        [
            "OK, So what does it mean is in this picture here.",
            "So here on the left hand side you see in this picture this kind of L1 regularization is L1 world.",
            "So the contour lines are the residual sum of squares you squared error loss and you try to optimize that to minimize that.",
            "And what you see here in the middle.",
            "This is the ordinary least squares estimator.",
            "So this is for P = 2, just two dimensions.",
            "And now you're saying is the L1 penalization.",
            "You require that the solution must be within the L1 bowl and the L1 ball.",
            "Is this diamond here so the L1 norm ball in two dimensions?",
            "Is this region here, so you require your constrain your solution to be in this region, and now it's happening here in this example, what you see is that beta had one is exactly 0 because you hit the contour line at the edge of your diamond and this is really what I call L1 geometry of the problem.",
            "It's sparse because the solution in this in this constellation is forced to be 0.",
            "And this has to do because of the geometry of the of the of the unit ball.",
            "OK, yeah one.",
            "Now this is vastly different to L2 regularization.",
            "This is called rich regression.",
            "Sometimes it's called taken off regularization.",
            "There you constrain your solution to live within DL 2 bowl and this is the standard Bowl and then it will never be sparse.",
            "You see it from the picture.",
            "I mean, it never will happen that Beta had one is exactly 0 because this has a very different geometry."
        ],
        [
            "OK so here is again what I said already about the L2 world.",
            "If you do the regularization Misty L2 norm, you can put it in LaGrange multiplier form like this.",
            "This is called rich regression taken off regularization and it's vastly different from the L1 norm regularization because of the different geometry."
        ],
        [
            "So what is actually this L1 norm regularization?",
            "The last so doing in the simplest case, the simplest case is a linear model with orthonormal design, so here's my linear model and the design is orthonormal.",
            "Molly asked, maybe this is very artificial, but it packed it, maybe not.",
            "So if you use for example a wavelet expansion on the regular grid, you have an order normal design OK.",
            "So in this case, the last sodium one norm regularization has an explicit solution.",
            "Otherwise it's just economix optimization problem.",
            "But here it has an explicit solution and the solution is the famous soft threshold estimator.",
            "So what is it so?",
            "You can look at the ordinary least squares estimation in this problem and for the Jays component.",
            "This is simply what I call here, ZJ.",
            "This is the least squares estimator.",
            "Now the last.",
            "So what it is doing is a bit depicted in this figure here.",
            "So on the X axis you have disease, the least squares estimator, and on the Y axis you have the beta, the beta hand.",
            "Now the last solution it may be hard to see is this fine dotted blue line.",
            "So whenever the ordinary least squares estimator is less than a threshold here that Rachel is Lambda half the last, so zero.",
            "And if the owner really squishy estimator in absolute value is bigger than Lambda, have you move up like this?",
            "OK, this is this blue line here.",
            "In contrast, the hard threshold estimator is the red dashed line which state OK if it's less than Lambda half it's zero, and if it's bigger in land 1/2 it's the order really square system either, so you just truncate urinary least squares estimator and this is kind of a hard threshold function, and what you see a bit from this picture is that the last so really has a bias problem.",
            "OK, one norm regularization even if the order least squares estimate is huge, is very large in absolute value.",
            "If you way out here way out there.",
            "You would like to estimate this disorderly squares because it's unbiased, but the last so never approaches the red line here.",
            "So the latter actually always inherits a bias of the order of Lambda, and this is something which causes troubles.",
            "So already, in the simplest case, even if your signal, say for the JS component is huge, is very large, you end up with a bias Vista lasso use, one concede from this picture of it."
        ],
        [
            "OK.",
            "So let's just use the lasso and then we want to understand a bit more about some theoretical properties about it.",
            "So how?",
            "How do you use the last so well?",
            "It's very simple.",
            "They only thing what you need to do is to choose the Lambda.",
            "OK, this regularization parameter and what most people do, including myself, just run across validation and choose the best land which optimizes prediction.",
            "So in practice you would choose well he can use some sort of PC criterion as well, but most people just choose Lambda via cross validation and that's fine.",
            "And then you can use again some sort of cross validation to check overall how good you are.",
            "This is kind of what I've shown you at the very beginning, and this is again this binary lymph node classification problem.",
            "It's not that important that you see really, I mean the details here.",
            "I just want to tell you the last, so does pretty well.",
            "You just take this data, just stick it into this linear model.",
            "It's not even a logistic model, just a linear model.",
            "Do your classification just blindly and you end up with a pretty good.",
            "This classification error I'm saying pretty good in comparison to others it's still not a very good classification accuracy, but that's in part due because the problem is very hard, so boosting was a bit better than last.",
            "So here is a forward selection algorithm and here a couple others.",
            "OK, so we lost so you can prediction.",
            "It can evaluate how good your I said that already and you can also check how which variables you're choosing and now what's happening is if you do it on across valued in a cross validation setting with every new fold in the cross validation will pick a bit different variables.",
            "So you see already here it's a bit in stable.",
            "So depending actually what your data is you always have a bit different variables and on average what's happening here?",
            "In this example you pick about 13 variables on average.",
            "Out of the more than 7000 possible variables and jeans.",
            "OK, So what you can see?",
            "Just very exploratory, here is the lasso yields are very sparse, model fit just a dozen of variables out of the more than 7000 variables and I will try to explain a bit what can we actually expect from a result like this?",
            "What are the 13 variables?",
            "Are these now the real variables?",
            "And so on and so forth."
        ],
        [
            "OK, so I will now bring couple minutes of more theory.",
            "Maybe some of you are more interested in theory and then I will move back to methodology and bring it back in.",
            "A bit of theory and so on and so forth.",
            "OK, so the easier part is prediction and estimation, but it's actually in particularly estimation part will be important to say something more than about the feature in variable selection problem.",
            "And the first OK here is the setting.",
            "I just consider a linear model for the theoreticians.",
            "I assume fixed design.",
            "Random design is slightly different, but it's not a major difference.",
            "I've looked at a fixed design linear model and here is my true underlying regression parameter beta 0.",
            "And now there is just a very very simple mathematical inequality which I call the basic inequality, which reads as follows.",
            "And then we try to read off from this basic inequality what we can say.",
            "OK, the basic inequality here is kind of the prediction error.",
            "Expedia hat minus X beta 0.",
            "How well can I predict the true underlying regression surface you scale?",
            "It is 1 / N that's not important.",
            "So you take Euclidean norm squared.",
            "Then there is actually an additional term, is less or equal in something.",
            "I will discuss this here, below and here comes this other term, Lam that times the L1 norm of the true underlying parameter.",
            "OK, and this is kind of really the spirit of the whole last game.",
            "You say, OK, if my truth is sparse.",
            "If Beta zero is sparse, then it should work and what means sparse, sparse.",
            "Actually, when you use the last so typically means sparse with respect to the L1 norm, because this is the norm where you do the regularization.",
            "OK, So what do you have in mind?",
            "Is actually beta 01.",
            "Norm is not that large is sparse.",
            "So this term is kind of small because you believe that your truth is sparse.",
            "If it's not sparse, you're hopelessly lossed.",
            "You cannot do statistical inference in a non sparse problem.",
            "OK, so the basic inequality is really trivial.",
            "I just want to give you an idea how trivial it is.",
            "So first of all you just write down the different using the definition of the last so you know it minimizes the squared error plus DL1 norm.",
            "So whatever the beta had this always smaller or equal to then use any other value of paid and now the other value of beta.",
            "Just take that Rwanda Beta zero OK and then you just rewrite this term.",
            "This you split up Y is X beta 0 plus epsilon.",
            "You take it apart.",
            "You re write this term then you have here the prediction error.",
            "Then comes the epsilon and then comes this additional cross error term.",
            "It's really very easy to do and then on the right hand side you rewrite this error.",
            "This is just your epsilon, just your error just cleared in two norm and then you plug it in.",
            "This cancels and you end up with this basic inequality is very simple, just uses the definition of the last cell.",
            "OK, now.",
            "If you want to do some theory about linear models, and I mean all other models, generalized linear models, additive models, multitask models, it's always the same game going on here, so we need to say something about disturbed is cross term and the other one just sparsity will do it for us.",
            "So this is the main thing here.",
            "OK, let's just do it in the linear model."
        ],
        [
            "Here's a very simple, but actually quite sharp inequality.",
            "So here is my cross term and this is a trivial bound.",
            "This is just a triangle inequality, so you can bound it.",
            "You just take that apart and you always want to do something with the L1 norm, because last season L1 guided methodology.",
            "So here you have the L1 norm.",
            "This will be hopefully small if beta hat is doing a good job and then you have this other term two times.",
            "Their Max over these variables which come from epsilon transpose takes it's very easy to write down.",
            "OK now comes the trick and I mean all theoreticians at least what I see do this trick.",
            "You say, OK, let's take the probabilistic part aside, let's separate the probabilistic part.",
            "This is the right stuff.",
            "From the more analytical part, which I will discuss.",
            "So what you do is you say OK, here this guy.",
            "Let's just look on a set of events where is kind of maximum.",
            "Here is small.",
            "Let's look at the set there two times.",
            "The Max here of this is less or equal in a Lambda zero and Lambda Viro is sometimes called the noise level of the problem.",
            "OK, this is the probabilistic part of the problem.",
            "Then you know this part is list in Lander 0, so on tower it's just really 2 lines.",
            "I'm not going into the detail and then you can come up with this on this set tile which will have high probability which I will just argue you have this inequality here.",
            "So you have here the prediction error scaled even something more less than hear something.",
            "And again if beta theories spars with respect to the L1 norm, it is a small number here.",
            "OK.",
            "So in a way, this is just the trick, right?",
            "They just take it apart and now I need to say something about what is the probability for this set and this is really how current theory is."
        ],
        [
            "Showing so I need to say something.",
            "How should we choose these Lambda?"
        ],
        [
            "Here you see here for Lambda bigger than two Lambda 0, so I need to say something about how we choose the Lambda 0."
        ],
        [
            "And I want to say something about the probability for this set time.",
            "OK, the heuristics is."
        ],
        [
            "Choose Lambda smallest possible right.",
            "Then you make this error term as small as possible, but there is a tradeoff here.",
            "Land there shouldn't be too small.",
            "If I take it very small, it always has to be larger than two Lambda zero.",
            "If I take Lambda zero very small, then this set tile has not large probability anymore.",
            "So if Lambda zero is large, the set tile at large."
        ],
        [
            "Ability.",
            "OK, so there is this tradeoff."
        ],
        [
            "So what we have to do is actually to look a bit the probability for this set."
        ],
        [
            "OK.",
            "It's just rewrite.",
            "I mean this is you have to just put it on a piece of paper one line, but you can easily."
        ],
        [
            "Rewrite this in a scaled version."
        ],
        [
            "And then you denote by Vijay dies variables into Tao is just two times the Max over Vijay absolute value and then you re scale here again.",
            "And honestly, I mean there's many people who tried to make a big story out of it, but I think it's in a way not all what you."
        ],
        [
            "Have to do is really to say something about this under various assumptions on your epsilon and your ex is, and then you're done.",
            "Mr problem.",
            "I mean this is really is a coherent way how you can generalize to many different."
        ],
        [
            "Scenarios so the prime example is and it's quite a reasonable example.",
            "Let's assume the epsilons are iid Gaussian errors.",
            "OK, so here is the IID normal with mean zero and variance Sigma squared.",
            "And we assume that you have scaled covariance.",
            "That's not really a problem, it's fixed design.",
            "So you scale all your covariance to equal norm.",
            "OK, this is here, and then it's again trivial to see that the VJS are standard, normally distributed.",
            "It's just elementary.",
            "So all what you have to do in this example is to study the Max of P standard normally distributed random variables.",
            "Arguably, they're not independent, they may be dependent, but nevertheless this is not a big problem.",
            "You use some sort of Bonferroni Union bound and here is the solution.",
            "If you take Lambda zero of this form, an important part is actually in red.",
            "If Lambda Zero is of the order square root, log P over in.",
            "Then the set tile has large probability.",
            "OK. And once you've seen this trick, you can really do many other things.",
            "I mean, you can do it for non Gaussian errors, assuming some certainty kayson moments, you can look at the painting errors.",
            "It's in my view, not a big deal anymore, you just need to look at the Max of these random variables.",
            "Vijay, under slightly more technical assumptions and that's it.",
            "So what we see here is Lambda zero.",
            "This noise label is asymptotically or roughly."
        ],
        [
            "On the scale square root, log P over in, and that's also the scale which you should choose for you."
        ],
        [
            "So Lambda should be as small as possible."
        ],
        [
            "But bigger in two times land is 0, so."
        ],
        [
            "This is kind of the Grandview if you want to do prediction with L1 norm regularization that Lambda is of the order square root log P over in.",
            "That's how you should choose in a way you Lambda.",
            "Now in practice it doesn't work.",
            "I mean you have a constant in front of square root log, P orange.",
            "You still need some sort of cross validation, but it gives you the rough scale picture lambdas of this order."
        ],
        [
            "OK, so once you have this, choose the land of this order and this was the inequality we had before you end up with this at the end.",
            "OK, so here is the squared error loss for prediction X pada hat.",
            "This is the predicted value minus X beta zero.",
            "The true value, the squared error loss scaled by one over in comes out of this form.",
            "Beta 01 Norm I told you again this is the sparsity of the problem.",
            "You believe you don't know, but all this when you do these kind of things, you think implicitly that beta 01 norm is small.",
            "OK, times this square root log P over in.",
            "Which means if log P is of smaller ordered in sample size, you're in good shape.",
            "OK, P can be pretty large, but log here in should be small.",
            "Otherwise you run into problems.",
            "That's kind of the right scaling here.",
            "OK then from this you can derive the things.",
            "So for example, if the truth is sparse with respect to the L1 norm and kind of growth of a smaller order in square root in log P, then you can cancel it out.",
            "It still goes to zero.",
            "OK, so if the truth is sparse and as N increases, you may still get more and more variables, But then you still get a consistent estimation for prediction.",
            "It converges to 0.",
            "This has been a result by green standing ripped off it required.",
            "I think 20 pages of proofs.",
            "This requires about 15 lines of proofs now.",
            "Well, this is how things develop.",
            "People got more insight into the problem and it's now quite elegant to give the proof for that.",
            "Now, what theoreticians argue is while this convergence rate is very bad, So what we typically expect, if something without the square root.",
            "So if you go back to famous work of Donna and Johnson in the wavelet domain, it's always the number of non zero wavelet coefficients times log P over in not just the square root, and I will come back to that in the next slides.",
            "Important thing which I want to point out here is that I made no assumptions on my design.",
            "There is this belief and I can understand somehow it has a reason that you need strong assumptions that the loss of works well.",
            "But for prediction in this square root log P / N domain convergence rate you need no assumptions on your design.",
            "It just works if it's sparse.",
            "OK, so I said already a bit."
        ],
        [
            "People want to have a faster convergence rate, so the aim is something to half of the order lock P over in times S 01.",
            "This is actually the more theoretical papers.",
            "Now they all deal with this framework, so little S 0 is just the size of your active sent.",
            "Suppose you have thousands of variables P but only 22 of them are active.",
            "The little a series just 22, so it's 22 times log P over in the log pages.",
            "This is the price you pay by not knowing which of the variables or the true active ones.",
            "OK, so this becomes much more technical and it's not my goal to explain you this in more detail, but I want to give you some sort of idea and claims what people are doing here.",
            "So again, one starts with this basic inequality which I have put up.",
            "Here again, this was exactly the same as before.",
            "Then we have this trick on Tao.",
            "You know, we kind of said something about this structure here, and it's really simple.",
            "Rewriting an triangle inequality through transform this line into that line here, so that reads.",
            "This is again the prediction error, and Sigma hat is now the gram matrix into the minus One X transpose X or the empirical covariance.",
            "In Texas, so here's this quadratic form.",
            "And then when I write.",
            "Index S0 or S0 compliment.",
            "This means these are the components which correspond to the active set and these are the components which correspond to the nonactive set OK.",
            "This is just notation, so you see this inequality here without actually knowing exactly how it has arrived.",
            "OK, now comes this again.",
            "Some sort of trick, and I mean the more interested people have to just sit down and workout the trick, but the trick is.",
            "You want to re late this term with the quadratic form.",
            "This is what everybody does now.",
            "OK, it's here in red.",
            "Again we want to re late this one norm.",
            "This kind of complicated.",
            "It's more expressions with SO and so on and so forth.",
            "But the important point is you want to relate the L1 norm Ristic quadratic form.",
            "OK. And this is really a kind of an eigenvalue problem, and in this thing here it's restricted L1 eigenvalue problem.",
            "So why is this an eigenvalue problem?",
            "OK, here's the reminder.",
            "The classical problem, if you have to take my."
        ],
        [
            "At your gram matrix, and if your minimal eigenvalue is denoted here by Lambda squared mean.",
            "This is the minimal eigenvalue of your ground matrix.",
            "Then by definition you can relate the two norm to the quadratic form.",
            "So for, albeit as the two norm is less or equal to the quadratic form divided by the minimal eigenvalue, and if the minimal eigenvalue is small, the bound becomes terrible, right?",
            "OK, but that's you know why.",
            "80 idea of the minimal eigenvalue relate the two norm, Mr."
        ],
        [
            "Drastic form and here I do the same game.",
            "I relayed the L1 norm, not the two norm based in quadratic form.",
            "So people just."
        ],
        [
            "Go ahead and define what an L1 eigenvalues relating the L1 norm is.",
            "The quadratic form, and you don't need it for all your parameters beta.",
            "Here it's for all betas and it's actually a restricted L1 eigenvalue problem because you only have to work it out and here it becomes really technical.",
            "You only have to work it out for beta which satisfy this restriction, so we require less on your gram matrix Sigma hat.",
            "You only require that for, albeit a satisfying this restriction.",
            "There is.",
            "Here is the active set.",
            "You can relate it in this way.",
            "It's just technical, but the idea is the same relat DL1 norm or DL2 norm to the quadratic form if you can re late it then you're essentially done.",
            "If you can relay this you can pull it to the other side and get out a good inequality.",
            "OK so I'm just skipping the details now.",
            "You do that.",
            "You assume that you have this business.",
            "Certain re stricted L1 eigenvalue.",
            "This is now the analogue of a minimal eigenvalue theory strictly on eigenvalue."
        ],
        [
            "And then you can come up with people what they call an Oracle inequality.",
            "So for Lambda, bigger into Lambda zero, the same game as before, here is the prediction squared error.",
            "Here is the estimation error.",
            "One norm is less or equal to this term.",
            "Here for Lambda squared F 0 / 5 zero squared.",
            "So if five year the restricted L1 eigenvalue is bounded away from zero, you're in good shape, and if it's close to zero, it's a very bad inequality.",
            "And then, asymptotically again, the same game choose Lambda off the ordered square root log, be over in.",
            "Then you get this result for prediction.",
            "So prediction is indeed the number of true underlying active variables times the rate log P over in, and it depends how small you restricted L1 eigenvalue is.",
            "Now for my kind of further development here in the tutorial and also my own scientific interest.",
            "The other one is in a way even more important and more interesting, so this is the one estimation error I told you at the beginning.",
            "First prediction, then estimation and it can read it off.",
            "Here again from this inequality I just use this part left to equal in that and forget about this so you immediately get beta hat minus through beta zero in one norm is less than this bound and here we get the square root, but that doesn't matter.",
            "This is kind of the optimal rate actually, it's the square root because he divided by Lambda here again.",
            "OK, so with this kind of approach, assuming that Sigma had big Rame, Tricaster restricted L1 eigenvalue, which is well behaved, you get out fast convergence rates and an L1 estimation error bound.",
            "OK."
        ],
        [
            "So.",
            "Well, Isabelle said I made either a Journal, and I've seen many papers and so many people prove things.",
            "But what's going on here?",
            "Also, what this whole framework is?",
            "Is it just make the appropriate assumptions to prove what you would like to prove?",
            "I mean, it could be exaggerated, but this is not.",
            "This is not that hard.",
            "I mean, you just make the right definition to restrict their one eigenvalue and then you crank it out and it's not too much work.",
            "You know of course.",
            "Take some time to see how this can be done.",
            "But in a way, the real question for me is.",
            "How restrictive are such conditions?",
            "How restrictive is this?",
            "Restricted L1 eigenvalue condition?",
            "OK, and here is 1 answer.",
            "The more technical answer.",
            "Well, it's actually we crude in what most people use, at least in statistics.",
            "So far this is the re strict L2 eigenvalue assumption do typically rate of sipoc off.",
            "So this is what I just presented you before is slightly weaker.",
            "And here is kind of the more general picture.",
            "I don't want to."
        ],
        [
            "Pose that you understand that in detail, but the picture should give you somehow the Grandview of what's going on.",
            "So there's lots of huge literature in high dimensional statistical inference, and you see in this picture, here are various assumptions and then various implications.",
            "OK, and there's a tendency from the left to the right, so the left or the strong assumptions on the right, or the weak assumptions.",
            "Just if you look at this graph right?",
            "And way on the left is the restricted isometry property.",
            "This is Candace Tao.",
            "I mean this is great work, but it's very restrictive.",
            "Assumptions.",
            "OK, this isn't compressed sensing.",
            "I think many people work with the restricted isometry property, but it's just much more restrictive than what people have worked out now for the last.",
            "So in linear models, here are other assumptions here representable conduct assumption and then way out here is this compatibility or restricted L1 hiking value assumption.",
            "OK, so it seems to be a weak assumption, but nevertheless.",
            "I mean, that's not really fully convincing you."
        ],
        [
            "You know, I mean, if it's really what's going on in practice, does it hold at least approximately in practice?",
            "And the first thing is, it's not checkable, it's impossible to check it.",
            "You would need to go.",
            "You would need to know your true active set their zero, so you cannot check the assumption valid statistics.",
            "I mean, you cannot check whether linear model is correct or not, so I mean some people is quite interesting like you did skinny Mirowski they come up with checkable assumptions.",
            "This is nice, so you have a design matrix X you can check.",
            "Very assumptions are fulfilled, but the checker assumptions are substantially stronger in an unshakeable assumptions.",
            "So there's a tradeoff here again.",
            "And here is kind of a justification why we think this restricted L1 eigenvalue assumption is kind of reasonable in practice, and it's the following scenario.",
            "You say, OK, your ex is.",
            "In your matrix they have been sampled as IID rose.",
            "OK, so you say X1 extended arose in your matrix.",
            "They're iid have mean zero?",
            "That's another restriction.",
            "They have a covariance structure Sigma, So Sigma is the population covariance structure of your IID sampling for the matrix.",
            "And now you assume that the L1 restricted eigenvalue holds for your Sigma.",
            "For the population covariance.",
            "Now that's an issue, but here at the end has the end is not apparent anymore.",
            "This is there's no sample size involved in this anymore, it's just the P * P matrix.",
            "So we don't know whether it's high dimensional or not.",
            "The relation into PES lossed it's just a PDP matrix and it just want to know whether these PP matrix is reasonable.",
            "And so we assume actually it has well behaved restricted L1 eigenvalues to Sigma to population Sigma.",
            "And that's OK.",
            "I mean sometimes even Upnp matrix has well behaved eigenvalues.",
            "If you take it triplets matrix, it has well behaved eigenvalues.",
            "Even if P goes to a million.",
            "If you take a quick correlation, it has well behaved eigenvalues.",
            "So we kind of think it's not too unreasonable to say that the population Sigma has reasonably behaved restricted DL one eigenvalues.",
            "And then you need some moment conditions on X, for example, including the Gaussian case, and then again the sparsity principle.",
            "If we assume that the number of active variables in your linear model is in this regime not growing faster in square root in over log P, then you can come up with this bound with high probability.",
            "So then your empirical gram matrix has also well behaved restricted L1 eigenvalues.",
            "It's just losing a factor in half.",
            "And from my view, this is maybe the best justification why this restricted eigenvalue assumption kind of holds in practice.",
            "If you believe this kind of scenario in this sparsity regime, it holds with high probability.",
            "OK, so this was kind of my theory block."
        ],
        [
            "A summary for the last so so for fixed design linear models.",
            "If you make no design assumptions at all, it can be as crazy as possible and you just have a mild assumption on your ear at epsilon you get kind of a slow rate of convergence for prediction, so for prediction you can do the job and you can actually get consistent prediction if it is sparse.",
            "The fact 200 property two is, if this compatibility condition or is restricted L1 eigenvalue condition holds for your gram matrix and mild assumption of the error.",
            "That's really not a big issue.",
            "Then you get a fast rate for prediction and you get this L1 estimation error.",
            "How well you can actually estimate the true underlying high dimensional regression breakthrough, and I said it already.",
            "There is kind of a myth, I think that people say, yeah, the loss only works.",
            "Under very restrictive design assumptions, and I think this is not quite true of first of all fact one doesn't need it at all.",
            "In fact, to adjust argued a bit this this slide, at least in some regime.",
            "The restricted one, eigenvalue assumption is likely to hold."
        ],
        [
            "OK, so maybe I skipped that.",
            "I just want to say this kind of technique and thinking you can.",
            "It's technical more technically more challenging, but you can use the same ideas for essentially any convex loss function.",
            "So I mean have.",
            "Minimisation of an empirical loss plus and L1 penalty for any convex loss function.",
            "Essentially any you can come up with the same results Mr Kane of same type of thinking.",
            "You separate the probabilistic part from the analytical part.",
            "You can do additive models, multi task models.",
            "I mean there are things like the damn 6 selector, also quite different at first sight, orthogonal matching, pursuit boosting.",
            "But if you look actually what people have been worked out from the theory point of view.",
            "Say on the rough scale it's quite similar.",
            "I'm saying here on the rough scale.",
            "The details are different, but this is not my task.",
            "Today I just want to give you the rough picture."
        ],
        [
            "OK, so.",
            "The next thing is I want to move to variable selection or feature selection.",
            "And here's an example.",
            "It's motive regression and the task is to find the transcription factor binding sites on DNA sequence for a particular transcription factor.",
            "This is the Heath 1A.",
            "This is a transcription factor which is important or supposed to be important in diabetes too.",
            "So there is an interest in kind of wanna know where do you save 1A where this protein actually binds exactly to the DNA sequence.",
            "This is an interesting kind of question and apparently an interesting problem, so you would like to know somehow the exact position or positions where the phone Alpha protein binds to the DNA and binding to the DNA means it binds to a word like Sgt GC.",
            "So when you see these word you kind of would know the headphone Alpha binds to this world OK and now the task is what is the word on the DNA sequence maybe 5 to 15 base pairs long?",
            "Where is protein binds exactly to the DNA?",
            "That's kind of the question.",
            "And the data we have.",
            "Is I mean first you have univariate response variables Y which measure the rough binding intensity of this hip 1A protein on course DNA segments.",
            "This is CHIP chip data chip, chip experiments, and I'm aware this is not the newest technology anymore.",
            "People do cheap sick now.",
            "So the example is still a good illustration, but I think from the biology point of view it has changed by their technologies around, so people look slightly differently at this kind of problem now.",
            "OK, let's go the old fashioned way with the technology from 340 years ago.",
            "So you have the rough binding intensity of this here for now.",
            "For the DNA sequence on core segments, so I know in this region here for alphas binds much stronger than in another region, but that's not enough.",
            "You really would like to go on defined scale.",
            "And he would like to know really the exact word like 5 or 15 base pairs long.",
            "Is it really binding here or there and is he cannot read off from chip chip experiments?",
            "OK, so in order to do that, what people did is you generate actually candidate words which you think they're quite good candidates.",
            "Very actually, the protein would bind to these candidate words and you can generate those candidates from sequence data alone, and so these are the XI JS.",
            "It can generate these candidate words, say P candidate words, and you can measure their abundance on the rough DNA segment.",
            "I OK, so I have my DNA.",
            "I have rough segments and in every segment I know whether a candidate word actually is highly abundant or not.",
            "So I measure abundance course XIJ this is the abundant score of the candidate motive J in a DNA segment I and I create quite a few of these candidates.",
            "In our example I create 195 candidate words by some sort of algorithm."
        ],
        [
            "And the question is, how can we relate the binding intensity, why?",
            "And the abundance of these short candidate motifs.",
            "At first sight, you think?",
            "Well much should we do with this?",
            "And quite surprisingly, I mean quite a few years ago, John Liu and colleagues at Harvard, they said, well, just related with a linear model.",
            "OK re late the rough binding intensity Y&Y candidate abundant scores through a linear model.",
            "It's quite so.",
            "I mean certainly not mechanistic, but in terms of some sort of reasonable Association, this seems to be a simple but good enough model.",
            "So this is what we did.",
            "They called it motive regression, so we just re late the rough pounding intensity.",
            "Is this candidate abundant scores through a linear model?",
            "In our case, we have in 287 DNA segments and peas 195 candidate words.",
            "So it's not super high dimensional piece listing in, but nevertheless you need to regularize to come up with a good solution here.",
            "The goal is variable selection only, and I think from this point of view it's a good example.",
            "I'm not interested in predicting and you why this is totally, I mean scientifically, not interesting.",
            "The only thing I want to know is which of the candidate words are important for explaining why, and then hopefully these are good candidates for the biology.",
            "That's it.",
            "No prediction involved, it's only variable selection."
        ],
        [
            "OK, so of course you can do some sort of all subsets regression and things like this, but now the lasso technique is extremely efficient from a computational point of view.",
            "All what you do is you run your convex optimization program and you get at least some selected variables, so you can select your variables just as the variables which have corresponding non zero estimated regression coefficients.",
            "Right?",
            "It's very simple minded.",
            "You just say if it's non zero in your estimate it should be in and if it's zero it should be out.",
            "So each had Lambda is some sort of inference machinist limit for the true underlying active Set S 0, which is the one where the truth has non zero rational coefficients.",
            "And the first thing which strikes, at least the statisticians, is there's no testing involved, no measure of uncertainty.",
            "And well, in part because it's hard.",
            "How can you come up with a test in high dimensional scenarios?",
            "So this is simple minded solution.",
            "It's convex optimization only and this kind of maybe simple minded solutions.",
            "Sometimes you run into problems with Seymour troubles, but nevertheless it became quite popular to do it this way and I want to explain about what's going on if you do it this way."
        ],
        [
            "OK, so in my modification example is again the description.",
            "I'm on linear model Ann is 287 PS 195.",
            "The last selects 26 variables.",
            "Well, I'd chose the Lambda via cross validation.",
            "So my Lambda is kind of geared towards prediction.",
            "This may be a problem, but I have not another good way to do it at the moment, so the last searches selects 26 variables and while you could think of 26 interesting candidate motives.",
            "No way.",
            "I mean there are way too many there.",
            "Just maybe one or two or three but not 26.",
            "So biologically there way too many here for Mr Distichal POV would say OK 26 out of 195.",
            "That may be pretty reasonable.",
            "OK, so let's again do the analog.",
            "Less minutes here.",
            "So what do we know from theory and how should we interpret?"
        ],
        [
            "Result like this OK?",
            "So theory for the last Part 2 about variable selection.",
            "Again, I look at a fixed design linear model because it is the easiest scenario.",
            "My active set is S0 and here are two key assumptions and I will discuss them a bit.",
            "The first one is what we called the neighborhood stability condition.",
            "It's a condition on your design X and then later ping Zhao Bing, you reformulated it in a nicer form and it's called the representable condition for the design X.",
            "So it's a design condition.",
            "It's not the re strict, they want eigenvalues.",
            "Another design condition and the other condition discussed this a bit and the other one is kind of plausible.",
            "So you say non zero true coefficients.",
            "They have to be sufficiently large if you just have very small signal strings, there's no hope you will able to detect these variables right?",
            "And so actually the condition is the non zero regression coefficients, the true ones.",
            "They're either 0 and then the non 0 ones have to be larger than C Times Square root log P over in CS certain constant.",
            "Magic in this log P over in square root which comes up.",
            "OK."
        ],
        [
            "Now both these conditions.",
            "Are sufficient, that's great, but the trouble is there essentially necessary.",
            "So if they fail, your lasso will not give you the right active set of variables, so these conditions are sufficient and essentially necessary in order that is simple, is had, Lambda recovers the true S0 with high probability OK.",
            "So this is what he would like to have.",
            "It would be a great goal.",
            "It's a very ambitious goal.",
            "You would need to choose the lamb that be larger than for prediction.",
            "That's OK, maybe, but you have these two conditions, so we proved this quite awhile ago.",
            "And to point this, both these assumptions are really restrictive.",
            "OK, and I mean we have it in our paper, but people typically it's kind of nice.",
            "You read it in a positive way.",
            "You take the positive part of the paper and say OK, they proved under these conditions it works, but we also show if the conditions are violated it doesn't work and it's kind of a message which I want to pass here as well.",
            "It doesn't work if the conditions are violated, the conditions are sharp.",
            "OK, so this is the equivalent form."
        ],
        [
            "Just to give you an idea, this is the representable condition.",
            "Here is my gram matrix and what it boils down to.",
            "You partition your gram matrix into the part where you have the active through variables.",
            "You just order them before the 1st is zero variables.",
            "So here are the empirical covariance is among the active variables here about active and non active variables and so on and so forth and you representable condition reads like this.",
            "OK, so it's very compact, it's nice, it's beautiful, but of course it's non checkable, you have no chance to check this condition if somebody gives you a matrix.",
            "6.",
            "OK."
        ],
        [
            "So here is again this graph where are we with this condition?",
            "So again, here is the restricted isometry property and is very presentable.",
            "Condition is somewhere here in the middle, and the restricted L1 eigenvalue condition I told you is way more on the right and in my view I would really claim the representable condition for the variable selection problem is way more restrictive than the compatibility condition order restricted L1 eigenvalue condition so.",
            "If you're interested in doing the practical stuff, you should be worried about the zero, presentable condition."
        ],
        [
            "OK so I say it's not very realistic and so you can still run it.",
            "I did run it, I select 26 variables in my example.",
            "So what can I expect from this?",
            "And here is kind of a nice story which I think is important to bring over.",
            "So in my view this L1 restricted restrictor, one eigenvalue condition, or these compatibility condition is kind of reasonable in practice.",
            "So I told you before under this condition we get this bound on the estimation error, and I think this is kind of reasonable in practice.",
            "OK so I have this L1 norm bound.",
            "Is this kind of more technical formula here?",
            "But now this is really a trivial thought.",
            "Now you say OK, the hope is maybe to get at least the strong variables, right?",
            "So suppose some of the variables have huge regression coefficients.",
            "Can we get them?",
            "And so I define not.",
            "This is not the S 0.",
            "This is not the set of active variables.",
            "These are the variables.",
            "We just have sufficiently strong sufficiently large coefficients, so S relevant are the ones which have sufficiently large coefficients, and it's actually just copied from this formula, just a larger than this bound.",
            "And I just say here then clearly.",
            "And I mean this is really trivial.",
            "Suppose you have one of these variables.",
            "And suppose your estimate would be 0.",
            "OK, then this is an immediate contradiction that you have this L1 error norm bound.",
            "So again, suppose you would say Beta had J is 0, but in fact it's larger than this it cannot happen because you have this bound here and that has a very interesting interpretation.",
            "So clearly I know that with high probability from this stuff with high probability, the ASAT contains all the relevant variables.",
            "And that's great news in a way.",
            "I mean this is.",
            "Only one side of the story, but in a way I don't miss them.",
            "I get them and a couple of other stuff.",
            "OK.",
            "So what I'm saying is screening for detecting the relevant variables is quite likely to be possible in practice.",
            "OK, I selected 26 variables in my example.",
            "It kind of means the true stuff is hopefully among the 26."
        ],
        [
            "OK, so now you can move on.",
            "If you say OK, you.",
            "S0I mean this is the active state.",
            "Actually all the non zero coefficients are sufficiently large.",
            "Then you have screening for the active set, but that's maybe equational assumption, But if you're willing to say OK, either 0 or there is large enough, then you know the S hat from loss of will actually contain your true active variables."
        ],
        [
            "K. And so I didn't say it.",
            "What lasso stands for.",
            "So tips you ran in his crate paper.",
            "I mean coined the term lasso in each stands for least absolute shrinkage and selection operator.",
            "And here is my new translation of this.",
            "I think it should be read at least absolute shrinkage and screening operator.",
            "It really."
        ],
        [
            "Screens in the sense of you catch it, but a couple of other stuff you don't select really 480."
        ],
        [
            "OK, from a practical perspective, if you want to do that, it is still this Lambda round, right?",
            "And at least I don't know well.",
            "I come a bit later to this issue from another perspective, but it's hard to get a good Lambda for selection.",
            "Well, maybe some BICS would do the job, but at least nobody has proof, at least what I've seen.",
            "Something like that, so you just run your cross validation for the relevant variables sorted through variables so in practice is kind of nice.",
            "You have maybe two small line that you just get too many variables.",
            "Now, why is it is interesting?",
            "The other issue, which I should mention this, what is the size of this asset?",
            "Of course, if you have this, just all the variables, you always have the screening property.",
            "That's not an interesting screening operation is hacked for any kind of Lambda in absolute values, cardinality is less or equal in meaning P. This is trivial to show or easy to show, so the last so selects at most meaning P variables.",
            "Now if P is in the 10,000 or in the millions and then in the dozens or hundreds.",
            "You reduced to a dozen or 100 variables.",
            "OK, that's great, and so this is really.",
            "This screening is a huge dimensionality reduction for your problem.",
            "I mean, in the original covariance, you're not doing some sort of PCA, linear combination, or so.",
            "It's just huge dimensionality reduction in the original coherence, and it is a huge gain and very useful machine.",
            "Kind of in my experience."
        ],
        [
            "OK, so once you have it once we have done this reduction.",
            "You're down to a couple variables, so to speak.",
            "I mean, if any is not too large and then I just say now you just apply your favorite method of choice.",
            "It's not very high dimensional anymore.",
            "OK, and people have done some things that typically you would do some sort of re estimation."
        ],
        [
            "For example, you just do order release squares Re estimation.",
            "You're smaller set from S 0 and you apply some sort of PC penalty there.",
            "Or you do thresholding of the coefficients and OS Re estimation.",
            "Or you apply the adaptive lasso and so on and so forth.",
            "So The thing is really you cannot do it in one stage.",
            "You do the first stage for the screening, and in the second or even the third stage you need to do something else to get rid of the false positives."
        ],
        [
            "OK summary 2 for the lasso.",
            "Variable selection, so if you really want to infer the F0 that through active state is a very ambitious goal, and it requires necessarily teeth rather restrictive assumptions, the representable condition, and that the non zero regression coefficients are sufficiently large, both of them are restrictive.",
            "But as I kind of indicated, variable screening is much more realistic, it works based a much weaker restrict their one eigenvalue condition and then you get the result like this.",
            "Even if you have many small nonzero regression coefficient in the truth, you would still get.",
            "I mean, the ones which are relevant.",
            "The large coefficients you get, and if you assume the truth is such that it's either 0 or the coefficients are sufficiently large, you actually scream for the true underlying S 0."
        ],
        [
            "Again, I mainly focused on the last selling linear models.",
            "Many extensions have been worked out.",
            "Group lasso, fused lasso and so on and so forth.",
            "Some people work with concave penalties.",
            "They argue they have better selection properties.",
            "This cat MC Plus and so on.",
            "Matching pursuit boosting again on the rough scale.",
            "I would say it's kind of a similar story I'm available.",
            "I'm in the details are different, but in the Ralph scale I think this is really what's going on in high dimensional statistical inference and summarizing this prediction.",
            "Is really kind of easy estimation of parameters.",
            "Getting your parameter vector is substantially harder, but it's kind of realistic.",
            "In practice, variable screening is often kind of reasonably accurate, and if you really add variable selection, getting the right is 0.",
            "This is a very hard problem."
        ],
        [
            "Maybe continue with this before making it very shortly labrake, so once you know how to do the variable selection in a linear model, you almost immediately get to Gaussian graphical models, at least the undirected Gaussian graphical model.",
            "It's conceptually not very different, so here is again the setting.",
            "Suppose I have observations X1, XN, IID from AP dimensional Gaussian.",
            "This means zero and some covariance Sigma.",
            "That the goal in Gaussian graphical modeling, the undirected graph, is to infer the zeros of Sigma inverse.",
            "So Sigma inverses are very ill behaved object if P is much much larger than in.",
            "If you don't observe too many too many observations relative to the dimensionality.",
            "So why is sick my inverse an interesting quantity?",
            "Because it encodes the conditional dependence and independence between the variables.",
            "So, for example, XJ is conditionally dependent on XC given all other variables if and only if the JK entry in Sigma inverse is different from zero, and then you draw an edge in the graph.",
            "OK.",
            "So why is this almost the consequences of linear model and variable selection?",
            "You can transfer this problem into linear models as follows.",
            "So I say node wise regression can do the job.",
            "It's quite easy to show that now you just regress the Jays covariant to all others and then you have an error term.",
            "And then you look at these regression coefficients beta KJ and you can also regress the case to the Jason all others, so we can do the symmetric version.",
            "And now it happens, and it's not too hard to show that beta KJ the regression coefficient is different from zero if and only if Sigma inverse JK is different from zero, because beta KJ really is the partial correlation between XJ and XK.",
            "Given all others is exactly the thing like here up to scaling.",
            "These are the same things, in particular, the zeros are the same, so I can read off the edge just from a regression coefficient.",
            "And this is actually what we did couple."
        ],
        [
            "Years ago, so you just run P different lasso regressions.",
            "It's 1000 of regressions, but each of them is fast and the estimate DH if the estimated coefficient is different from zero, and sometimes because in the estimated version it's not symmetric anymore.",
            "You make a rule the estimated coefficient beta had KJ and beta.",
            "JK Hat is different from the Roses Symmetrize, or you don't.",
            "There's an and or overrule to solve this problem.",
            "And then you can just estimate very high dimensional Gaussian graphical models with this.",
            "And there's a story behind this, which is may be interesting to say.",
            "So the first criticism about running these pelosso regressions, it does not use the constraint that you should actually have a positive definite matrix Sigma.",
            "So you kind of lose out information on this.",
            "So you think this is not really?",
            "This is kind of a brute force idea.",
            "Doing it this way.",
            "And actually so meinshausen was my student and he first looked at what people later did.",
            "Just look what people say, the G lasso approach.",
            "You take the multivariate Gaussian and you estimate the Sigma inverse by L1 norm penalization.",
            "And he just came to my office and said, well, I think this doesn't work well because it needs so restrictive assumptions varies if you decouple it into various regressions, you need much less restrictive assumptions.",
            "So from a theory point of view, if you're interested in inferring the edge set the graph, it's substantially better to decouple it in unrelated linear regressions because the underlying representable condition based is node wise.",
            "The coupled approach are much weaker than if you go to simultaneous way with the glass.",
            "So if you're interested in estimating Sigma or Sigma inverse, I'm not making this claim so I just make the claim.",
            "If you're interested in the graph structure, I think just running the regression separately certainly requires less theoretical assumptions and empirically it's at least as good as the simultaneous approach."
        ],
        [
            "OK, maybe we should take a little break here to refresh your minds.",
            "Are there any questions?",
            "Although maybe just a 5 minutes break and OK?"
        ],
        [
            "Short one, so you special screening.",
            "Yeah.",
            "So.",
            "So you have a really short question, so on this screening property you multiple times used like those qualitative words like all should work mostly.",
            "Those things seem to be really easy to simulate.",
            "I wear if any work where like extensive simulations would be done and then actually show like OK, we're not missing more than 5% of something like that.",
            "Yeah.",
            "That's a good point.",
            "I mean people and also.",
            "We have simulated quite a lot.",
            "We have some empirical kind of picture, at least what we see and kind of what I said.",
            "It's more realistic that it works in practice is really based, maybe on my empirical observations.",
            "However, in a way.",
            "I agree, I mean simulation gives you a good feeling, but what the community wants to have is some sort of people call it performance guarantee, right?",
            "And that's just the simulation.",
            "I think it's a very good exercise for yourself or your group or your local colleagues, but it's hard to convince the colleagues by just saying, hey, I just have these great simulations.",
            "It just works well and.",
            "I tried to be really to give some sort of honest picture here.",
            "I do think this is representable condition is really a tricky condition, a tricky assumptions and you can fail quite substantially in getting is zero.",
            "However, in getting the screening property my experiences it works reasonably, so it depends alot what kind of design you're looking at.",
            "I mean So what I like to do.",
            "I will show it afterwards.",
            "Take a real design X from real data and create maybe artificial bait as an artificial errors.",
            "Then you at least you know you capture the real design X and not some sort of cooked up design from your from your mind.",
            "OK, I think it's a good idea.",
            "If you use the microphone."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Isabelle for this nice introduction.",
                    "label": 0
                },
                {
                    "sent": "Maybe it would have been better if my students would give the presentation because they want the competition and not myself.",
                    "label": 0
                },
                {
                    "sent": "I tremendously enjoy also being here.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the invitation.",
                    "label": 0
                },
                {
                    "sent": "It's a wonderful place, an exciting meeting, and it's really a good thing to be here.",
                    "label": 0
                },
                {
                    "sent": "The tutorial will be about high dimensional statistics as he Isabel already said a bit and 1st I would really like to acknowledge somehow my.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Main collaborators in this area over the last years, these are colleagues from ETS Zurich, Sarah family, here, Marlou, Smart 1000, Marcus Kalisz, and my former student, who is now in Oxford, Nikli Meinshausen.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I would like to start with just giving two brief examples about high dimensional data and then I will develop further and say a bit something about methodology and some sort of theory and go on.",
                    "label": 0
                },
                {
                    "sent": "So the first example is some sort of textbook example.",
                    "label": 0
                },
                {
                    "sent": "By now it's classification of tumor samples, whether a sample is tumors or not, or whether it's a sub sub tumor a versus a subtype B and you try to classify using high dimensional gene expression.",
                    "label": 0
                },
                {
                    "sent": "Measurements, so by now this is almost kind of a classical task, So what we have if we measure the expression of thousands of genes here, a bit more than 7000 and there are only a dozen people in the study.",
                    "label": 0
                },
                {
                    "sent": "So here it's 49 and then you can run some algorithms you can look at the misclassification error, do some cross validation to come up with these numbers, and then you publish numbers.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not suspecting that you can read this, but you can compare different methods and just go on and then just discuss about how good you can do.",
                    "label": 0
                },
                {
                    "sent": "So this is more a predictive task.",
                    "label": 0
                },
                {
                    "sent": "It's I would say almost a textbook example by now.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other example is maybe a bit more about scientific understanding.",
                    "label": 0
                },
                {
                    "sent": "This is about the riboflavin or vitamin production with Bacillus and the facility is called Bacillus subtilis.",
                    "label": 1
                },
                {
                    "sent": "This is actually a project with a company with Dutch state mining company in Switzerland and a very general goal of the companies to improve the vitamin production rate of the bacillus using clever genetic engineering.",
                    "label": 1
                },
                {
                    "sent": "If you succeed in improving that production rate, this is a huge revenue for the company.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is a huge problem and an aspect or some aspects of the problem involve also data and statistics and machine learning, and what we have here is just we measure response variables.",
                    "label": 0
                },
                {
                    "sent": "Actually how effective the bacillus is in producing the vitamin.",
                    "label": 0
                },
                {
                    "sent": "So we measure the riboflavin vitamin log production rate.",
                    "label": 1
                },
                {
                    "sent": "This is my universe, your response variable Y and the game that have fairly high dimensional covariance XI measure the expressions from all the jeans.",
                    "label": 0
                },
                {
                    "sent": "In the genome of Bacillus subtilis.",
                    "label": 0
                },
                {
                    "sent": "So this is again a microarray chip.",
                    "label": 0
                },
                {
                    "sent": "If you measure a bit more than 4000 variables, the expression of more than 4000 genes and sample size here is 115, so 115 individuals which you have measured.",
                    "label": 0
                },
                {
                    "sent": "Apparently again the number of variables is much, much larger than sample size and this is high dimensional statistics.",
                    "label": 0
                },
                {
                    "sent": "Now here's just a visualization of the gene expression data set without a wide without the response variable, and you just can visualize it a bit.",
                    "label": 0
                },
                {
                    "sent": "You don't see much, and on the right hand side we see the very exploratory way of looking at the data set.",
                    "label": 0
                },
                {
                    "sent": "You just do a couple of scatter plots and plot Y against some of these variables.",
                    "label": 0
                },
                {
                    "sent": "Some of these genes, and actually we selected here 9 reasonable jeans, and when we can just see from an exploratory POV, for example here.",
                    "label": 0
                },
                {
                    "sent": "And also there you see exploratory that there is some sort of marginal Association between the response Y and at least one of these jeans, or some of these genes.",
                    "label": 0
                },
                {
                    "sent": "So here the question is really more, if not so much about prediction.",
                    "label": 0
                },
                {
                    "sent": "It's more trying to find out which variables are actually important, which variables, which genes actually drive the production rate.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in my tutorial I will largely focus on a linear model.",
                    "label": 0
                },
                {
                    "sent": "A high dimensional linear model first of all because it's simple and Secondly I believe it's yet still very useful in many applications in high dimensional statistical inference.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking at is just a linear model of why Arjuna.",
                    "label": 0
                },
                {
                    "sent": "Varied response Yi may have an intercept and have a linear combination of the covariates.",
                    "label": 0
                },
                {
                    "sent": "XI always denote the variable J with this upper index J and.",
                    "label": 0
                },
                {
                    "sent": "Eyes the sample index and then have a stochastic error term is mean zero.",
                    "label": 0
                },
                {
                    "sent": "Everything is standard except that peed and number of covariates maybe or is typically much larger in sample size in, so this is the shorthand notation.",
                    "label": 0
                },
                {
                    "sent": "Why is Expedia plus epsilon like in all textbooks in regression?",
                    "label": 0
                },
                {
                    "sent": "And the girls are actually in the high dimensional statistical inference very similar to or in a sense the same.",
                    "label": 0
                },
                {
                    "sent": "What we have also in the classical setting.",
                    "label": 0
                },
                {
                    "sent": "We may be interested in prediction.",
                    "label": 0
                },
                {
                    "sent": "How well can we predict the future observation we may be interested in estimating just the high dimensional parameter vector beta and then from there we may want to move on and say something about feature or variable selection.",
                    "label": 0
                },
                {
                    "sent": "Saying which of these variables have corresponding regression coefficients different from zero?",
                    "label": 0
                },
                {
                    "sent": "Which of these variables?",
                    "label": 0
                },
                {
                    "sent": "Are important in that regard.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian model.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's again a bit exemplifying the outline of the tutorial.",
                    "label": 1
                },
                {
                    "sent": "Let's look again at one of these classification problems, so this is actually a binary lymph node classification problem.",
                    "label": 1
                },
                {
                    "sent": "The outcome is just a label zero or one.",
                    "label": 1
                },
                {
                    "sent": "If it's zero, it's non tumors and if it's one, it's a tumor sample.",
                    "label": 0
                },
                {
                    "sent": "And again the covariates are gene expressions.",
                    "label": 0
                },
                {
                    "sent": "A bit more than 7000 genes involved in actually a high noise problem.",
                    "label": 1
                },
                {
                    "sent": "It's a difficult problem to do the classification.",
                    "label": 0
                },
                {
                    "sent": "And So what you can do here, if you just look at this linear model and if you have the linear model in mind, you know that the target you're going for is actually the conditional mean.",
                    "label": 0
                },
                {
                    "sent": "The conditional mean of Y given X.",
                    "label": 0
                },
                {
                    "sent": "That doesn't sound like a classification, but if Y is binary, of course this is the same as just the conditional class probability.",
                    "label": 0
                },
                {
                    "sent": "So in this special case where Y is binary disease, the conditional class probability denoted by PX.",
                    "label": 0
                },
                {
                    "sent": "So if I can fit my linear model, if I can estimate my linear model.",
                    "label": 0
                },
                {
                    "sent": "I get an estimate P hat X and then I can do classification if she had X is larger than 1/2, I assign one and if it's below half I sign a 0.",
                    "label": 0
                },
                {
                    "sent": "Then you can run some cross validation.",
                    "label": 0
                },
                {
                    "sent": "You rap across validation around it.",
                    "label": 0
                },
                {
                    "sent": "He was randomly division in two third training, 1/3 test set and then you can look at a couple methods right here is the law so I will explain it here is boosting here other methods.",
                    "label": 0
                },
                {
                    "sent": "Here's an SVM.",
                    "label": 0
                },
                {
                    "sent": "The method in red here.",
                    "label": 0
                },
                {
                    "sent": "They do feature selection they have built in variable or feature selection.",
                    "label": 0
                },
                {
                    "sent": "There's the methods in black here.",
                    "label": 0
                },
                {
                    "sent": "This is a one nearest neighbor, a diagonal linear discriminate analysis.",
                    "label": 0
                },
                {
                    "sent": "And so they don't do any feature selection.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can just look at these numbers.",
                    "label": 0
                },
                {
                    "sent": "And then you can argue well, from a practical point of view, if you trust in cross validation.",
                    "label": 1
                },
                {
                    "sent": "If you just want to do prediction, you're done in a way, right.",
                    "label": 0
                },
                {
                    "sent": "You just trust your cross validation.",
                    "label": 0
                },
                {
                    "sent": "You get these numbers, you don't need any theory.",
                    "label": 0
                },
                {
                    "sent": "Maybe, and then you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say OK, I just try out which one is the best.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So again, you have to cross validation handle in your hand and you can validate how good you are.",
                    "label": 1
                },
                {
                    "sent": "It's pretty easy if you're just interested in prediction.",
                    "label": 0
                },
                {
                    "sent": "It's pretty easy to do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so however, of course we want to say something more than just stop there.",
                    "label": 0
                },
                {
                    "sent": "The first criticism which is maybe semi convincing is that be kind of no.",
                    "label": 0
                },
                {
                    "sent": "That cross validation is very variable, so actually these.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Numbers are maybe not that accurate.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe we still want to know a bit more, some sort of methodological and theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "What different methods are doing, and then if you go on beyond prediction if you go into estimation.",
                    "label": 1
                },
                {
                    "sent": "So what can I say?",
                    "label": 0
                },
                {
                    "sent": "How well my estimate beta hat approaches the true underlying regression parameter.",
                    "label": 0
                },
                {
                    "sent": "Now sometimes in my presentation I denote the true parameter by a beta 0 or a beta, not to make sure that you know this is the true underlying regression parameter.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in that kind of.",
                    "label": 0
                },
                {
                    "sent": "Norm in kind of how good we can capture the true parameter.",
                    "label": 1
                },
                {
                    "sent": "Then there is nothing like a cross validation at hand.",
                    "label": 0
                },
                {
                    "sent": "I mean there's you cannot see from cross validation how well you do in kind of estimating the true parameter, and from there you can go on and you can say OK if I'm interested in getting actually the right feature.",
                    "label": 0
                },
                {
                    "sent": "If I want to know the active set, the S 0, these are just the variables whose corresponding regression coefficients are different from zero.",
                    "label": 1
                },
                {
                    "sent": "It's hard.",
                    "label": 0
                },
                {
                    "sent": "I mean you cannot run across validation and see.",
                    "label": 0
                },
                {
                    "sent": "Which made respect for feature selection versus another one.",
                    "label": 0
                },
                {
                    "sent": "So, and this is in a way the outline what I want to do.",
                    "label": 1
                },
                {
                    "sent": "First I go on the easy task on prediction and then I move onto estimation of the parameter to variable selection.",
                    "label": 0
                },
                {
                    "sent": "I focus on the regression and classification and once we understand that we can move on the graphical models and at the end I want to say something about intervention causal analysis.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in the linear model and actually many statistical regression type classification models, the lasso DL1 penalization technique is very popular in this.",
                    "label": 0
                },
                {
                    "sent": "Also very useful, and I just go into.",
                    "label": 0
                },
                {
                    "sent": "I mean I start from scratch, so here is again what the lasso is.",
                    "label": 0
                },
                {
                    "sent": "So in my high dimensional linear model I want to estimate the unknown high dimensional regression parameter beta.",
                    "label": 0
                },
                {
                    "sent": "So what you're going to do is you minimize residual sum of squares, right?",
                    "label": 0
                },
                {
                    "sent": "And then you have to regularize because it's a high dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "So without this term here this is just order release squares, but that would heavily overfit your problem.",
                    "label": 0
                },
                {
                    "sent": "So you regularize and the regularization is this by now famous L1 norm regularization or penalization.",
                    "label": 0
                },
                {
                    "sent": "You take the L1 norm of your high dimensional coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "Just some of the absolute values.",
                    "label": 1
                },
                {
                    "sent": "And here you have a regularization parameter Lambda which you have to choose.",
                    "label": 0
                },
                {
                    "sent": "In practice, is the tuning parameter.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of doing this is this is extremely fast in computing because this is a convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "The squared error loss is a convex function in beta and a penalty term is convex in beta as well.",
                    "label": 0
                },
                {
                    "sent": "So this is beautiful.",
                    "label": 0
                },
                {
                    "sent": "You can compute that very efficiently very fast, and so it's nice.",
                    "label": 0
                },
                {
                    "sent": "So from a computational point of view this is great.",
                    "label": 0
                },
                {
                    "sent": "And now we want to understand what is it actually doing for your statistical inference problem.",
                    "label": 0
                },
                {
                    "sent": "How well can you now estimate your parameter and so on?",
                    "label": 0
                },
                {
                    "sent": "And there are two basic properties.",
                    "label": 0
                },
                {
                    "sent": "The first one is the last sort is L1 regularization technique.",
                    "label": 0
                },
                {
                    "sent": "It does variable selection or feature selection in the sense that the estimate ear beta hat say for a component J maybe exactly 0 OK depending on how you choose your Lambda and this is very different from other sorts of regularization.",
                    "label": 0
                },
                {
                    "sent": "So this kind of L1 norm regularization has what people call the sparsity principle or the sparsity property.",
                    "label": 0
                },
                {
                    "sent": "It may be exactly 0, so it puts some of the coefficients exactly to zero, and this is because of the L1 geometry.",
                    "label": 0
                },
                {
                    "sent": "I will just explain in the next slide and more generally you can think of the beta is some sort of shrunken least squares estimate.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you shrink the coefficients exactly 0 and sometimes not.",
                    "label": 0
                },
                {
                    "sent": "OK so here is this L1 geometry of the problem.",
                    "label": 0
                },
                {
                    "sent": "When you look at this, this is how in at least in statistics we typically write it down.",
                    "label": 0
                },
                {
                    "sent": "This is in the LaGrange multiplier form, so he.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is my LaGrange term and so of course I can go to the other view and just look at what I call the primal problem.",
                    "label": 0
                },
                {
                    "sent": "I could equivalently formulate the problem as OK, let's minimize residual sum of squares under the constraint that the solution has L1 norm less or equal in some value R. OK, you make constraint optimization with respect to the L1 norm and there is a one to one correspondence between the LaGrange multiplier Lambda and the parameter R. Unfortunately, this correspondence is not.",
                    "label": 0
                },
                {
                    "sent": "I mean, this depends on the data.",
                    "label": 1
                },
                {
                    "sent": "You cannot give a formula for this correspondence, but it always exists.",
                    "label": 0
                },
                {
                    "sent": "OK, and the reason is, I mean you have here a convex function, the squared error and you have a convex constraint, so he can jump back and force from the primal to dual problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what does it mean is in this picture here.",
                    "label": 0
                },
                {
                    "sent": "So here on the left hand side you see in this picture this kind of L1 regularization is L1 world.",
                    "label": 0
                },
                {
                    "sent": "So the contour lines are the residual sum of squares you squared error loss and you try to optimize that to minimize that.",
                    "label": 1
                },
                {
                    "sent": "And what you see here in the middle.",
                    "label": 0
                },
                {
                    "sent": "This is the ordinary least squares estimator.",
                    "label": 0
                },
                {
                    "sent": "So this is for P = 2, just two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And now you're saying is the L1 penalization.",
                    "label": 0
                },
                {
                    "sent": "You require that the solution must be within the L1 bowl and the L1 ball.",
                    "label": 0
                },
                {
                    "sent": "Is this diamond here so the L1 norm ball in two dimensions?",
                    "label": 0
                },
                {
                    "sent": "Is this region here, so you require your constrain your solution to be in this region, and now it's happening here in this example, what you see is that beta had one is exactly 0 because you hit the contour line at the edge of your diamond and this is really what I call L1 geometry of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's sparse because the solution in this in this constellation is forced to be 0.",
                    "label": 0
                },
                {
                    "sent": "And this has to do because of the geometry of the of the of the unit ball.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah one.",
                    "label": 0
                },
                {
                    "sent": "Now this is vastly different to L2 regularization.",
                    "label": 0
                },
                {
                    "sent": "This is called rich regression.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's called taken off regularization.",
                    "label": 0
                },
                {
                    "sent": "There you constrain your solution to live within DL 2 bowl and this is the standard Bowl and then it will never be sparse.",
                    "label": 0
                },
                {
                    "sent": "You see it from the picture.",
                    "label": 0
                },
                {
                    "sent": "I mean, it never will happen that Beta had one is exactly 0 because this has a very different geometry.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is again what I said already about the L2 world.",
                    "label": 0
                },
                {
                    "sent": "If you do the regularization Misty L2 norm, you can put it in LaGrange multiplier form like this.",
                    "label": 0
                },
                {
                    "sent": "This is called rich regression taken off regularization and it's vastly different from the L1 norm regularization because of the different geometry.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is actually this L1 norm regularization?",
                    "label": 0
                },
                {
                    "sent": "The last so doing in the simplest case, the simplest case is a linear model with orthonormal design, so here's my linear model and the design is orthonormal.",
                    "label": 0
                },
                {
                    "sent": "Molly asked, maybe this is very artificial, but it packed it, maybe not.",
                    "label": 0
                },
                {
                    "sent": "So if you use for example a wavelet expansion on the regular grid, you have an order normal design OK.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the last sodium one norm regularization has an explicit solution.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's just economix optimization problem.",
                    "label": 0
                },
                {
                    "sent": "But here it has an explicit solution and the solution is the famous soft threshold estimator.",
                    "label": 0
                },
                {
                    "sent": "So what is it so?",
                    "label": 0
                },
                {
                    "sent": "You can look at the ordinary least squares estimation in this problem and for the Jays component.",
                    "label": 0
                },
                {
                    "sent": "This is simply what I call here, ZJ.",
                    "label": 0
                },
                {
                    "sent": "This is the least squares estimator.",
                    "label": 0
                },
                {
                    "sent": "Now the last.",
                    "label": 0
                },
                {
                    "sent": "So what it is doing is a bit depicted in this figure here.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis you have disease, the least squares estimator, and on the Y axis you have the beta, the beta hand.",
                    "label": 0
                },
                {
                    "sent": "Now the last solution it may be hard to see is this fine dotted blue line.",
                    "label": 0
                },
                {
                    "sent": "So whenever the ordinary least squares estimator is less than a threshold here that Rachel is Lambda half the last, so zero.",
                    "label": 0
                },
                {
                    "sent": "And if the owner really squishy estimator in absolute value is bigger than Lambda, have you move up like this?",
                    "label": 0
                },
                {
                    "sent": "OK, this is this blue line here.",
                    "label": 0
                },
                {
                    "sent": "In contrast, the hard threshold estimator is the red dashed line which state OK if it's less than Lambda half it's zero, and if it's bigger in land 1/2 it's the order really square system either, so you just truncate urinary least squares estimator and this is kind of a hard threshold function, and what you see a bit from this picture is that the last so really has a bias problem.",
                    "label": 0
                },
                {
                    "sent": "OK, one norm regularization even if the order least squares estimate is huge, is very large in absolute value.",
                    "label": 0
                },
                {
                    "sent": "If you way out here way out there.",
                    "label": 0
                },
                {
                    "sent": "You would like to estimate this disorderly squares because it's unbiased, but the last so never approaches the red line here.",
                    "label": 0
                },
                {
                    "sent": "So the latter actually always inherits a bias of the order of Lambda, and this is something which causes troubles.",
                    "label": 0
                },
                {
                    "sent": "So already, in the simplest case, even if your signal, say for the JS component is huge, is very large, you end up with a bias Vista lasso use, one concede from this picture of it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's just use the lasso and then we want to understand a bit more about some theoretical properties about it.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "How do you use the last so well?",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "They only thing what you need to do is to choose the Lambda.",
                    "label": 0
                },
                {
                    "sent": "OK, this regularization parameter and what most people do, including myself, just run across validation and choose the best land which optimizes prediction.",
                    "label": 0
                },
                {
                    "sent": "So in practice you would choose well he can use some sort of PC criterion as well, but most people just choose Lambda via cross validation and that's fine.",
                    "label": 0
                },
                {
                    "sent": "And then you can use again some sort of cross validation to check overall how good you are.",
                    "label": 0
                },
                {
                    "sent": "This is kind of what I've shown you at the very beginning, and this is again this binary lymph node classification problem.",
                    "label": 1
                },
                {
                    "sent": "It's not that important that you see really, I mean the details here.",
                    "label": 0
                },
                {
                    "sent": "I just want to tell you the last, so does pretty well.",
                    "label": 0
                },
                {
                    "sent": "You just take this data, just stick it into this linear model.",
                    "label": 0
                },
                {
                    "sent": "It's not even a logistic model, just a linear model.",
                    "label": 0
                },
                {
                    "sent": "Do your classification just blindly and you end up with a pretty good.",
                    "label": 0
                },
                {
                    "sent": "This classification error I'm saying pretty good in comparison to others it's still not a very good classification accuracy, but that's in part due because the problem is very hard, so boosting was a bit better than last.",
                    "label": 0
                },
                {
                    "sent": "So here is a forward selection algorithm and here a couple others.",
                    "label": 0
                },
                {
                    "sent": "OK, so we lost so you can prediction.",
                    "label": 0
                },
                {
                    "sent": "It can evaluate how good your I said that already and you can also check how which variables you're choosing and now what's happening is if you do it on across valued in a cross validation setting with every new fold in the cross validation will pick a bit different variables.",
                    "label": 0
                },
                {
                    "sent": "So you see already here it's a bit in stable.",
                    "label": 0
                },
                {
                    "sent": "So depending actually what your data is you always have a bit different variables and on average what's happening here?",
                    "label": 0
                },
                {
                    "sent": "In this example you pick about 13 variables on average.",
                    "label": 1
                },
                {
                    "sent": "Out of the more than 7000 possible variables and jeans.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you can see?",
                    "label": 0
                },
                {
                    "sent": "Just very exploratory, here is the lasso yields are very sparse, model fit just a dozen of variables out of the more than 7000 variables and I will try to explain a bit what can we actually expect from a result like this?",
                    "label": 0
                },
                {
                    "sent": "What are the 13 variables?",
                    "label": 0
                },
                {
                    "sent": "Are these now the real variables?",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I will now bring couple minutes of more theory.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of you are more interested in theory and then I will move back to methodology and bring it back in.",
                    "label": 0
                },
                {
                    "sent": "A bit of theory and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so the easier part is prediction and estimation, but it's actually in particularly estimation part will be important to say something more than about the feature in variable selection problem.",
                    "label": 1
                },
                {
                    "sent": "And the first OK here is the setting.",
                    "label": 0
                },
                {
                    "sent": "I just consider a linear model for the theoreticians.",
                    "label": 1
                },
                {
                    "sent": "I assume fixed design.",
                    "label": 0
                },
                {
                    "sent": "Random design is slightly different, but it's not a major difference.",
                    "label": 0
                },
                {
                    "sent": "I've looked at a fixed design linear model and here is my true underlying regression parameter beta 0.",
                    "label": 1
                },
                {
                    "sent": "And now there is just a very very simple mathematical inequality which I call the basic inequality, which reads as follows.",
                    "label": 0
                },
                {
                    "sent": "And then we try to read off from this basic inequality what we can say.",
                    "label": 0
                },
                {
                    "sent": "OK, the basic inequality here is kind of the prediction error.",
                    "label": 0
                },
                {
                    "sent": "Expedia hat minus X beta 0.",
                    "label": 0
                },
                {
                    "sent": "How well can I predict the true underlying regression surface you scale?",
                    "label": 0
                },
                {
                    "sent": "It is 1 / N that's not important.",
                    "label": 0
                },
                {
                    "sent": "So you take Euclidean norm squared.",
                    "label": 0
                },
                {
                    "sent": "Then there is actually an additional term, is less or equal in something.",
                    "label": 0
                },
                {
                    "sent": "I will discuss this here, below and here comes this other term, Lam that times the L1 norm of the true underlying parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is kind of really the spirit of the whole last game.",
                    "label": 0
                },
                {
                    "sent": "You say, OK, if my truth is sparse.",
                    "label": 0
                },
                {
                    "sent": "If Beta zero is sparse, then it should work and what means sparse, sparse.",
                    "label": 0
                },
                {
                    "sent": "Actually, when you use the last so typically means sparse with respect to the L1 norm, because this is the norm where you do the regularization.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you have in mind?",
                    "label": 0
                },
                {
                    "sent": "Is actually beta 01.",
                    "label": 0
                },
                {
                    "sent": "Norm is not that large is sparse.",
                    "label": 0
                },
                {
                    "sent": "So this term is kind of small because you believe that your truth is sparse.",
                    "label": 0
                },
                {
                    "sent": "If it's not sparse, you're hopelessly lossed.",
                    "label": 0
                },
                {
                    "sent": "You cannot do statistical inference in a non sparse problem.",
                    "label": 1
                },
                {
                    "sent": "OK, so the basic inequality is really trivial.",
                    "label": 0
                },
                {
                    "sent": "I just want to give you an idea how trivial it is.",
                    "label": 0
                },
                {
                    "sent": "So first of all you just write down the different using the definition of the last so you know it minimizes the squared error plus DL1 norm.",
                    "label": 0
                },
                {
                    "sent": "So whatever the beta had this always smaller or equal to then use any other value of paid and now the other value of beta.",
                    "label": 0
                },
                {
                    "sent": "Just take that Rwanda Beta zero OK and then you just rewrite this term.",
                    "label": 0
                },
                {
                    "sent": "This you split up Y is X beta 0 plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "You take it apart.",
                    "label": 0
                },
                {
                    "sent": "You re write this term then you have here the prediction error.",
                    "label": 0
                },
                {
                    "sent": "Then comes the epsilon and then comes this additional cross error term.",
                    "label": 0
                },
                {
                    "sent": "It's really very easy to do and then on the right hand side you rewrite this error.",
                    "label": 0
                },
                {
                    "sent": "This is just your epsilon, just your error just cleared in two norm and then you plug it in.",
                    "label": 0
                },
                {
                    "sent": "This cancels and you end up with this basic inequality is very simple, just uses the definition of the last cell.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "If you want to do some theory about linear models, and I mean all other models, generalized linear models, additive models, multitask models, it's always the same game going on here, so we need to say something about disturbed is cross term and the other one just sparsity will do it for us.",
                    "label": 0
                },
                {
                    "sent": "So this is the main thing here.",
                    "label": 0
                },
                {
                    "sent": "OK, let's just do it in the linear model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a very simple, but actually quite sharp inequality.",
                    "label": 0
                },
                {
                    "sent": "So here is my cross term and this is a trivial bound.",
                    "label": 0
                },
                {
                    "sent": "This is just a triangle inequality, so you can bound it.",
                    "label": 0
                },
                {
                    "sent": "You just take that apart and you always want to do something with the L1 norm, because last season L1 guided methodology.",
                    "label": 0
                },
                {
                    "sent": "So here you have the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "This will be hopefully small if beta hat is doing a good job and then you have this other term two times.",
                    "label": 0
                },
                {
                    "sent": "Their Max over these variables which come from epsilon transpose takes it's very easy to write down.",
                    "label": 0
                },
                {
                    "sent": "OK now comes the trick and I mean all theoreticians at least what I see do this trick.",
                    "label": 0
                },
                {
                    "sent": "You say, OK, let's take the probabilistic part aside, let's separate the probabilistic part.",
                    "label": 0
                },
                {
                    "sent": "This is the right stuff.",
                    "label": 0
                },
                {
                    "sent": "From the more analytical part, which I will discuss.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you say OK, here this guy.",
                    "label": 0
                },
                {
                    "sent": "Let's just look on a set of events where is kind of maximum.",
                    "label": 0
                },
                {
                    "sent": "Here is small.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the set there two times.",
                    "label": 0
                },
                {
                    "sent": "The Max here of this is less or equal in a Lambda zero and Lambda Viro is sometimes called the noise level of the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the probabilistic part of the problem.",
                    "label": 1
                },
                {
                    "sent": "Then you know this part is list in Lander 0, so on tower it's just really 2 lines.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into the detail and then you can come up with this on this set tile which will have high probability which I will just argue you have this inequality here.",
                    "label": 0
                },
                {
                    "sent": "So you have here the prediction error scaled even something more less than hear something.",
                    "label": 0
                },
                {
                    "sent": "And again if beta theories spars with respect to the L1 norm, it is a small number here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in a way, this is just the trick, right?",
                    "label": 0
                },
                {
                    "sent": "They just take it apart and now I need to say something about what is the probability for this set and this is really how current theory is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Showing so I need to say something.",
                    "label": 0
                },
                {
                    "sent": "How should we choose these Lambda?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you see here for Lambda bigger than two Lambda 0, so I need to say something about how we choose the Lambda 0.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I want to say something about the probability for this set time.",
                    "label": 0
                },
                {
                    "sent": "OK, the heuristics is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choose Lambda smallest possible right.",
                    "label": 0
                },
                {
                    "sent": "Then you make this error term as small as possible, but there is a tradeoff here.",
                    "label": 0
                },
                {
                    "sent": "Land there shouldn't be too small.",
                    "label": 0
                },
                {
                    "sent": "If I take it very small, it always has to be larger than two Lambda zero.",
                    "label": 0
                },
                {
                    "sent": "If I take Lambda zero very small, then this set tile has not large probability anymore.",
                    "label": 0
                },
                {
                    "sent": "So if Lambda zero is large, the set tile at large.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ability.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is this tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we have to do is actually to look a bit the probability for this set.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's just rewrite.",
                    "label": 0
                },
                {
                    "sent": "I mean this is you have to just put it on a piece of paper one line, but you can easily.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rewrite this in a scaled version.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you denote by Vijay dies variables into Tao is just two times the Max over Vijay absolute value and then you re scale here again.",
                    "label": 0
                },
                {
                    "sent": "And honestly, I mean there's many people who tried to make a big story out of it, but I think it's in a way not all what you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have to do is really to say something about this under various assumptions on your epsilon and your ex is, and then you're done.",
                    "label": 0
                },
                {
                    "sent": "Mr problem.",
                    "label": 0
                },
                {
                    "sent": "I mean this is really is a coherent way how you can generalize to many different.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scenarios so the prime example is and it's quite a reasonable example.",
                    "label": 0
                },
                {
                    "sent": "Let's assume the epsilons are iid Gaussian errors.",
                    "label": 1
                },
                {
                    "sent": "OK, so here is the IID normal with mean zero and variance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "And we assume that you have scaled covariance.",
                    "label": 0
                },
                {
                    "sent": "That's not really a problem, it's fixed design.",
                    "label": 0
                },
                {
                    "sent": "So you scale all your covariance to equal norm.",
                    "label": 0
                },
                {
                    "sent": "OK, this is here, and then it's again trivial to see that the VJS are standard, normally distributed.",
                    "label": 0
                },
                {
                    "sent": "It's just elementary.",
                    "label": 0
                },
                {
                    "sent": "So all what you have to do in this example is to study the Max of P standard normally distributed random variables.",
                    "label": 0
                },
                {
                    "sent": "Arguably, they're not independent, they may be dependent, but nevertheless this is not a big problem.",
                    "label": 0
                },
                {
                    "sent": "You use some sort of Bonferroni Union bound and here is the solution.",
                    "label": 0
                },
                {
                    "sent": "If you take Lambda zero of this form, an important part is actually in red.",
                    "label": 0
                },
                {
                    "sent": "If Lambda Zero is of the order square root, log P over in.",
                    "label": 0
                },
                {
                    "sent": "Then the set tile has large probability.",
                    "label": 1
                },
                {
                    "sent": "OK. And once you've seen this trick, you can really do many other things.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can do it for non Gaussian errors, assuming some certainty kayson moments, you can look at the painting errors.",
                    "label": 0
                },
                {
                    "sent": "It's in my view, not a big deal anymore, you just need to look at the Max of these random variables.",
                    "label": 0
                },
                {
                    "sent": "Vijay, under slightly more technical assumptions and that's it.",
                    "label": 0
                },
                {
                    "sent": "So what we see here is Lambda zero.",
                    "label": 0
                },
                {
                    "sent": "This noise label is asymptotically or roughly.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the scale square root, log P over in, and that's also the scale which you should choose for you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Lambda should be as small as possible.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But bigger in two times land is 0, so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is kind of the Grandview if you want to do prediction with L1 norm regularization that Lambda is of the order square root log P over in.",
                    "label": 0
                },
                {
                    "sent": "That's how you should choose in a way you Lambda.",
                    "label": 0
                },
                {
                    "sent": "Now in practice it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "I mean you have a constant in front of square root log, P orange.",
                    "label": 0
                },
                {
                    "sent": "You still need some sort of cross validation, but it gives you the rough scale picture lambdas of this order.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so once you have this, choose the land of this order and this was the inequality we had before you end up with this at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the squared error loss for prediction X pada hat.",
                    "label": 0
                },
                {
                    "sent": "This is the predicted value minus X beta zero.",
                    "label": 0
                },
                {
                    "sent": "The true value, the squared error loss scaled by one over in comes out of this form.",
                    "label": 0
                },
                {
                    "sent": "Beta 01 Norm I told you again this is the sparsity of the problem.",
                    "label": 0
                },
                {
                    "sent": "You believe you don't know, but all this when you do these kind of things, you think implicitly that beta 01 norm is small.",
                    "label": 0
                },
                {
                    "sent": "OK, times this square root log P over in.",
                    "label": 0
                },
                {
                    "sent": "Which means if log P is of smaller ordered in sample size, you're in good shape.",
                    "label": 0
                },
                {
                    "sent": "OK, P can be pretty large, but log here in should be small.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you run into problems.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the right scaling here.",
                    "label": 0
                },
                {
                    "sent": "OK then from this you can derive the things.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the truth is sparse with respect to the L1 norm and kind of growth of a smaller order in square root in log P, then you can cancel it out.",
                    "label": 0
                },
                {
                    "sent": "It still goes to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the truth is sparse and as N increases, you may still get more and more variables, But then you still get a consistent estimation for prediction.",
                    "label": 0
                },
                {
                    "sent": "It converges to 0.",
                    "label": 0
                },
                {
                    "sent": "This has been a result by green standing ripped off it required.",
                    "label": 0
                },
                {
                    "sent": "I think 20 pages of proofs.",
                    "label": 0
                },
                {
                    "sent": "This requires about 15 lines of proofs now.",
                    "label": 0
                },
                {
                    "sent": "Well, this is how things develop.",
                    "label": 0
                },
                {
                    "sent": "People got more insight into the problem and it's now quite elegant to give the proof for that.",
                    "label": 0
                },
                {
                    "sent": "Now, what theoreticians argue is while this convergence rate is very bad, So what we typically expect, if something without the square root.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to famous work of Donna and Johnson in the wavelet domain, it's always the number of non zero wavelet coefficients times log P over in not just the square root, and I will come back to that in the next slides.",
                    "label": 0
                },
                {
                    "sent": "Important thing which I want to point out here is that I made no assumptions on my design.",
                    "label": 0
                },
                {
                    "sent": "There is this belief and I can understand somehow it has a reason that you need strong assumptions that the loss of works well.",
                    "label": 0
                },
                {
                    "sent": "But for prediction in this square root log P / N domain convergence rate you need no assumptions on your design.",
                    "label": 1
                },
                {
                    "sent": "It just works if it's sparse.",
                    "label": 0
                },
                {
                    "sent": "OK, so I said already a bit.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People want to have a faster convergence rate, so the aim is something to half of the order lock P over in times S 01.",
                    "label": 0
                },
                {
                    "sent": "This is actually the more theoretical papers.",
                    "label": 0
                },
                {
                    "sent": "Now they all deal with this framework, so little S 0 is just the size of your active sent.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have thousands of variables P but only 22 of them are active.",
                    "label": 0
                },
                {
                    "sent": "The little a series just 22, so it's 22 times log P over in the log pages.",
                    "label": 0
                },
                {
                    "sent": "This is the price you pay by not knowing which of the variables or the true active ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so this becomes much more technical and it's not my goal to explain you this in more detail, but I want to give you some sort of idea and claims what people are doing here.",
                    "label": 0
                },
                {
                    "sent": "So again, one starts with this basic inequality which I have put up.",
                    "label": 1
                },
                {
                    "sent": "Here again, this was exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "Then we have this trick on Tao.",
                    "label": 0
                },
                {
                    "sent": "You know, we kind of said something about this structure here, and it's really simple.",
                    "label": 1
                },
                {
                    "sent": "Rewriting an triangle inequality through transform this line into that line here, so that reads.",
                    "label": 0
                },
                {
                    "sent": "This is again the prediction error, and Sigma hat is now the gram matrix into the minus One X transpose X or the empirical covariance.",
                    "label": 0
                },
                {
                    "sent": "In Texas, so here's this quadratic form.",
                    "label": 0
                },
                {
                    "sent": "And then when I write.",
                    "label": 1
                },
                {
                    "sent": "Index S0 or S0 compliment.",
                    "label": 0
                },
                {
                    "sent": "This means these are the components which correspond to the active set and these are the components which correspond to the nonactive set OK.",
                    "label": 0
                },
                {
                    "sent": "This is just notation, so you see this inequality here without actually knowing exactly how it has arrived.",
                    "label": 0
                },
                {
                    "sent": "OK, now comes this again.",
                    "label": 0
                },
                {
                    "sent": "Some sort of trick, and I mean the more interested people have to just sit down and workout the trick, but the trick is.",
                    "label": 0
                },
                {
                    "sent": "You want to re late this term with the quadratic form.",
                    "label": 0
                },
                {
                    "sent": "This is what everybody does now.",
                    "label": 0
                },
                {
                    "sent": "OK, it's here in red.",
                    "label": 0
                },
                {
                    "sent": "Again we want to re late this one norm.",
                    "label": 0
                },
                {
                    "sent": "This kind of complicated.",
                    "label": 0
                },
                {
                    "sent": "It's more expressions with SO and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But the important point is you want to relate the L1 norm Ristic quadratic form.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is really a kind of an eigenvalue problem, and in this thing here it's restricted L1 eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "So why is this an eigenvalue problem?",
                    "label": 0
                },
                {
                    "sent": "OK, here's the reminder.",
                    "label": 0
                },
                {
                    "sent": "The classical problem, if you have to take my.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At your gram matrix, and if your minimal eigenvalue is denoted here by Lambda squared mean.",
                    "label": 0
                },
                {
                    "sent": "This is the minimal eigenvalue of your ground matrix.",
                    "label": 1
                },
                {
                    "sent": "Then by definition you can relate the two norm to the quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So for, albeit as the two norm is less or equal to the quadratic form divided by the minimal eigenvalue, and if the minimal eigenvalue is small, the bound becomes terrible, right?",
                    "label": 0
                },
                {
                    "sent": "OK, but that's you know why.",
                    "label": 0
                },
                {
                    "sent": "80 idea of the minimal eigenvalue relate the two norm, Mr.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drastic form and here I do the same game.",
                    "label": 0
                },
                {
                    "sent": "I relayed the L1 norm, not the two norm based in quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So people just.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go ahead and define what an L1 eigenvalues relating the L1 norm is.",
                    "label": 0
                },
                {
                    "sent": "The quadratic form, and you don't need it for all your parameters beta.",
                    "label": 1
                },
                {
                    "sent": "Here it's for all betas and it's actually a restricted L1 eigenvalue problem because you only have to work it out and here it becomes really technical.",
                    "label": 0
                },
                {
                    "sent": "You only have to work it out for beta which satisfy this restriction, so we require less on your gram matrix Sigma hat.",
                    "label": 0
                },
                {
                    "sent": "You only require that for, albeit a satisfying this restriction.",
                    "label": 1
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "Here is the active set.",
                    "label": 1
                },
                {
                    "sent": "You can relate it in this way.",
                    "label": 0
                },
                {
                    "sent": "It's just technical, but the idea is the same relat DL1 norm or DL2 norm to the quadratic form if you can re late it then you're essentially done.",
                    "label": 0
                },
                {
                    "sent": "If you can relay this you can pull it to the other side and get out a good inequality.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm just skipping the details now.",
                    "label": 0
                },
                {
                    "sent": "You do that.",
                    "label": 0
                },
                {
                    "sent": "You assume that you have this business.",
                    "label": 0
                },
                {
                    "sent": "Certain re stricted L1 eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "This is now the analogue of a minimal eigenvalue theory strictly on eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can come up with people what they call an Oracle inequality.",
                    "label": 0
                },
                {
                    "sent": "So for Lambda, bigger into Lambda zero, the same game as before, here is the prediction squared error.",
                    "label": 0
                },
                {
                    "sent": "Here is the estimation error.",
                    "label": 0
                },
                {
                    "sent": "One norm is less or equal to this term.",
                    "label": 0
                },
                {
                    "sent": "Here for Lambda squared F 0 / 5 zero squared.",
                    "label": 0
                },
                {
                    "sent": "So if five year the restricted L1 eigenvalue is bounded away from zero, you're in good shape, and if it's close to zero, it's a very bad inequality.",
                    "label": 0
                },
                {
                    "sent": "And then, asymptotically again, the same game choose Lambda off the ordered square root log, be over in.",
                    "label": 0
                },
                {
                    "sent": "Then you get this result for prediction.",
                    "label": 0
                },
                {
                    "sent": "So prediction is indeed the number of true underlying active variables times the rate log P over in, and it depends how small you restricted L1 eigenvalue is.",
                    "label": 0
                },
                {
                    "sent": "Now for my kind of further development here in the tutorial and also my own scientific interest.",
                    "label": 0
                },
                {
                    "sent": "The other one is in a way even more important and more interesting, so this is the one estimation error I told you at the beginning.",
                    "label": 0
                },
                {
                    "sent": "First prediction, then estimation and it can read it off.",
                    "label": 0
                },
                {
                    "sent": "Here again from this inequality I just use this part left to equal in that and forget about this so you immediately get beta hat minus through beta zero in one norm is less than this bound and here we get the square root, but that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the optimal rate actually, it's the square root because he divided by Lambda here again.",
                    "label": 0
                },
                {
                    "sent": "OK, so with this kind of approach, assuming that Sigma had big Rame, Tricaster restricted L1 eigenvalue, which is well behaved, you get out fast convergence rates and an L1 estimation error bound.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, Isabelle said I made either a Journal, and I've seen many papers and so many people prove things.",
                    "label": 0
                },
                {
                    "sent": "But what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Also, what this whole framework is?",
                    "label": 0
                },
                {
                    "sent": "Is it just make the appropriate assumptions to prove what you would like to prove?",
                    "label": 1
                },
                {
                    "sent": "I mean, it could be exaggerated, but this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not that hard.",
                    "label": 0
                },
                {
                    "sent": "I mean, you just make the right definition to restrict their one eigenvalue and then you crank it out and it's not too much work.",
                    "label": 0
                },
                {
                    "sent": "You know of course.",
                    "label": 0
                },
                {
                    "sent": "Take some time to see how this can be done.",
                    "label": 0
                },
                {
                    "sent": "But in a way, the real question for me is.",
                    "label": 1
                },
                {
                    "sent": "How restrictive are such conditions?",
                    "label": 0
                },
                {
                    "sent": "How restrictive is this?",
                    "label": 0
                },
                {
                    "sent": "Restricted L1 eigenvalue condition?",
                    "label": 0
                },
                {
                    "sent": "OK, and here is 1 answer.",
                    "label": 0
                },
                {
                    "sent": "The more technical answer.",
                    "label": 0
                },
                {
                    "sent": "Well, it's actually we crude in what most people use, at least in statistics.",
                    "label": 0
                },
                {
                    "sent": "So far this is the re strict L2 eigenvalue assumption do typically rate of sipoc off.",
                    "label": 1
                },
                {
                    "sent": "So this is what I just presented you before is slightly weaker.",
                    "label": 0
                },
                {
                    "sent": "And here is kind of the more general picture.",
                    "label": 0
                },
                {
                    "sent": "I don't want to.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pose that you understand that in detail, but the picture should give you somehow the Grandview of what's going on.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of huge literature in high dimensional statistical inference, and you see in this picture, here are various assumptions and then various implications.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's a tendency from the left to the right, so the left or the strong assumptions on the right, or the weak assumptions.",
                    "label": 0
                },
                {
                    "sent": "Just if you look at this graph right?",
                    "label": 0
                },
                {
                    "sent": "And way on the left is the restricted isometry property.",
                    "label": 0
                },
                {
                    "sent": "This is Candace Tao.",
                    "label": 0
                },
                {
                    "sent": "I mean this is great work, but it's very restrictive.",
                    "label": 0
                },
                {
                    "sent": "Assumptions.",
                    "label": 0
                },
                {
                    "sent": "OK, this isn't compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "I think many people work with the restricted isometry property, but it's just much more restrictive than what people have worked out now for the last.",
                    "label": 0
                },
                {
                    "sent": "So in linear models, here are other assumptions here representable conduct assumption and then way out here is this compatibility or restricted L1 hiking value assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so it seems to be a weak assumption, but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's not really fully convincing you.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, I mean, if it's really what's going on in practice, does it hold at least approximately in practice?",
                    "label": 0
                },
                {
                    "sent": "And the first thing is, it's not checkable, it's impossible to check it.",
                    "label": 0
                },
                {
                    "sent": "You would need to go.",
                    "label": 0
                },
                {
                    "sent": "You would need to know your true active set their zero, so you cannot check the assumption valid statistics.",
                    "label": 0
                },
                {
                    "sent": "I mean, you cannot check whether linear model is correct or not, so I mean some people is quite interesting like you did skinny Mirowski they come up with checkable assumptions.",
                    "label": 0
                },
                {
                    "sent": "This is nice, so you have a design matrix X you can check.",
                    "label": 0
                },
                {
                    "sent": "Very assumptions are fulfilled, but the checker assumptions are substantially stronger in an unshakeable assumptions.",
                    "label": 0
                },
                {
                    "sent": "So there's a tradeoff here again.",
                    "label": 0
                },
                {
                    "sent": "And here is kind of a justification why we think this restricted L1 eigenvalue assumption is kind of reasonable in practice, and it's the following scenario.",
                    "label": 0
                },
                {
                    "sent": "You say, OK, your ex is.",
                    "label": 0
                },
                {
                    "sent": "In your matrix they have been sampled as IID rose.",
                    "label": 0
                },
                {
                    "sent": "OK, so you say X1 extended arose in your matrix.",
                    "label": 0
                },
                {
                    "sent": "They're iid have mean zero?",
                    "label": 0
                },
                {
                    "sent": "That's another restriction.",
                    "label": 0
                },
                {
                    "sent": "They have a covariance structure Sigma, So Sigma is the population covariance structure of your IID sampling for the matrix.",
                    "label": 0
                },
                {
                    "sent": "And now you assume that the L1 restricted eigenvalue holds for your Sigma.",
                    "label": 0
                },
                {
                    "sent": "For the population covariance.",
                    "label": 0
                },
                {
                    "sent": "Now that's an issue, but here at the end has the end is not apparent anymore.",
                    "label": 0
                },
                {
                    "sent": "This is there's no sample size involved in this anymore, it's just the P * P matrix.",
                    "label": 0
                },
                {
                    "sent": "So we don't know whether it's high dimensional or not.",
                    "label": 0
                },
                {
                    "sent": "The relation into PES lossed it's just a PDP matrix and it just want to know whether these PP matrix is reasonable.",
                    "label": 0
                },
                {
                    "sent": "And so we assume actually it has well behaved restricted L1 eigenvalues to Sigma to population Sigma.",
                    "label": 0
                },
                {
                    "sent": "And that's OK.",
                    "label": 0
                },
                {
                    "sent": "I mean sometimes even Upnp matrix has well behaved eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If you take it triplets matrix, it has well behaved eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Even if P goes to a million.",
                    "label": 0
                },
                {
                    "sent": "If you take a quick correlation, it has well behaved eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So we kind of think it's not too unreasonable to say that the population Sigma has reasonably behaved restricted DL one eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And then you need some moment conditions on X, for example, including the Gaussian case, and then again the sparsity principle.",
                    "label": 1
                },
                {
                    "sent": "If we assume that the number of active variables in your linear model is in this regime not growing faster in square root in over log P, then you can come up with this bound with high probability.",
                    "label": 1
                },
                {
                    "sent": "So then your empirical gram matrix has also well behaved restricted L1 eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "It's just losing a factor in half.",
                    "label": 0
                },
                {
                    "sent": "And from my view, this is maybe the best justification why this restricted eigenvalue assumption kind of holds in practice.",
                    "label": 0
                },
                {
                    "sent": "If you believe this kind of scenario in this sparsity regime, it holds with high probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was kind of my theory block.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A summary for the last so so for fixed design linear models.",
                    "label": 1
                },
                {
                    "sent": "If you make no design assumptions at all, it can be as crazy as possible and you just have a mild assumption on your ear at epsilon you get kind of a slow rate of convergence for prediction, so for prediction you can do the job and you can actually get consistent prediction if it is sparse.",
                    "label": 1
                },
                {
                    "sent": "The fact 200 property two is, if this compatibility condition or is restricted L1 eigenvalue condition holds for your gram matrix and mild assumption of the error.",
                    "label": 0
                },
                {
                    "sent": "That's really not a big issue.",
                    "label": 0
                },
                {
                    "sent": "Then you get a fast rate for prediction and you get this L1 estimation error.",
                    "label": 0
                },
                {
                    "sent": "How well you can actually estimate the true underlying high dimensional regression breakthrough, and I said it already.",
                    "label": 0
                },
                {
                    "sent": "There is kind of a myth, I think that people say, yeah, the loss only works.",
                    "label": 0
                },
                {
                    "sent": "Under very restrictive design assumptions, and I think this is not quite true of first of all fact one doesn't need it at all.",
                    "label": 0
                },
                {
                    "sent": "In fact, to adjust argued a bit this this slide, at least in some regime.",
                    "label": 0
                },
                {
                    "sent": "The restricted one, eigenvalue assumption is likely to hold.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so maybe I skipped that.",
                    "label": 0
                },
                {
                    "sent": "I just want to say this kind of technique and thinking you can.",
                    "label": 0
                },
                {
                    "sent": "It's technical more technically more challenging, but you can use the same ideas for essentially any convex loss function.",
                    "label": 0
                },
                {
                    "sent": "So I mean have.",
                    "label": 0
                },
                {
                    "sent": "Minimisation of an empirical loss plus and L1 penalty for any convex loss function.",
                    "label": 0
                },
                {
                    "sent": "Essentially any you can come up with the same results Mr Kane of same type of thinking.",
                    "label": 0
                },
                {
                    "sent": "You separate the probabilistic part from the analytical part.",
                    "label": 0
                },
                {
                    "sent": "You can do additive models, multi task models.",
                    "label": 1
                },
                {
                    "sent": "I mean there are things like the damn 6 selector, also quite different at first sight, orthogonal matching, pursuit boosting.",
                    "label": 1
                },
                {
                    "sent": "But if you look actually what people have been worked out from the theory point of view.",
                    "label": 0
                },
                {
                    "sent": "Say on the rough scale it's quite similar.",
                    "label": 0
                },
                {
                    "sent": "I'm saying here on the rough scale.",
                    "label": 0
                },
                {
                    "sent": "The details are different, but this is not my task.",
                    "label": 0
                },
                {
                    "sent": "Today I just want to give you the rough picture.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The next thing is I want to move to variable selection or feature selection.",
                    "label": 0
                },
                {
                    "sent": "And here's an example.",
                    "label": 0
                },
                {
                    "sent": "It's motive regression and the task is to find the transcription factor binding sites on DNA sequence for a particular transcription factor.",
                    "label": 0
                },
                {
                    "sent": "This is the Heath 1A.",
                    "label": 0
                },
                {
                    "sent": "This is a transcription factor which is important or supposed to be important in diabetes too.",
                    "label": 0
                },
                {
                    "sent": "So there is an interest in kind of wanna know where do you save 1A where this protein actually binds exactly to the DNA sequence.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting kind of question and apparently an interesting problem, so you would like to know somehow the exact position or positions where the phone Alpha protein binds to the DNA and binding to the DNA means it binds to a word like Sgt GC.",
                    "label": 0
                },
                {
                    "sent": "So when you see these word you kind of would know the headphone Alpha binds to this world OK and now the task is what is the word on the DNA sequence maybe 5 to 15 base pairs long?",
                    "label": 0
                },
                {
                    "sent": "Where is protein binds exactly to the DNA?",
                    "label": 0
                },
                {
                    "sent": "That's kind of the question.",
                    "label": 0
                },
                {
                    "sent": "And the data we have.",
                    "label": 0
                },
                {
                    "sent": "Is I mean first you have univariate response variables Y which measure the rough binding intensity of this hip 1A protein on course DNA segments.",
                    "label": 0
                },
                {
                    "sent": "This is CHIP chip data chip, chip experiments, and I'm aware this is not the newest technology anymore.",
                    "label": 0
                },
                {
                    "sent": "People do cheap sick now.",
                    "label": 0
                },
                {
                    "sent": "So the example is still a good illustration, but I think from the biology point of view it has changed by their technologies around, so people look slightly differently at this kind of problem now.",
                    "label": 0
                },
                {
                    "sent": "OK, let's go the old fashioned way with the technology from 340 years ago.",
                    "label": 0
                },
                {
                    "sent": "So you have the rough binding intensity of this here for now.",
                    "label": 0
                },
                {
                    "sent": "For the DNA sequence on core segments, so I know in this region here for alphas binds much stronger than in another region, but that's not enough.",
                    "label": 0
                },
                {
                    "sent": "You really would like to go on defined scale.",
                    "label": 0
                },
                {
                    "sent": "And he would like to know really the exact word like 5 or 15 base pairs long.",
                    "label": 0
                },
                {
                    "sent": "Is it really binding here or there and is he cannot read off from chip chip experiments?",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to do that, what people did is you generate actually candidate words which you think they're quite good candidates.",
                    "label": 0
                },
                {
                    "sent": "Very actually, the protein would bind to these candidate words and you can generate those candidates from sequence data alone, and so these are the XI JS.",
                    "label": 0
                },
                {
                    "sent": "It can generate these candidate words, say P candidate words, and you can measure their abundance on the rough DNA segment.",
                    "label": 0
                },
                {
                    "sent": "I OK, so I have my DNA.",
                    "label": 0
                },
                {
                    "sent": "I have rough segments and in every segment I know whether a candidate word actually is highly abundant or not.",
                    "label": 0
                },
                {
                    "sent": "So I measure abundance course XIJ this is the abundant score of the candidate motive J in a DNA segment I and I create quite a few of these candidates.",
                    "label": 1
                },
                {
                    "sent": "In our example I create 195 candidate words by some sort of algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the question is, how can we relate the binding intensity, why?",
                    "label": 1
                },
                {
                    "sent": "And the abundance of these short candidate motifs.",
                    "label": 1
                },
                {
                    "sent": "At first sight, you think?",
                    "label": 0
                },
                {
                    "sent": "Well much should we do with this?",
                    "label": 0
                },
                {
                    "sent": "And quite surprisingly, I mean quite a few years ago, John Liu and colleagues at Harvard, they said, well, just related with a linear model.",
                    "label": 0
                },
                {
                    "sent": "OK re late the rough binding intensity Y&Y candidate abundant scores through a linear model.",
                    "label": 0
                },
                {
                    "sent": "It's quite so.",
                    "label": 0
                },
                {
                    "sent": "I mean certainly not mechanistic, but in terms of some sort of reasonable Association, this seems to be a simple but good enough model.",
                    "label": 0
                },
                {
                    "sent": "So this is what we did.",
                    "label": 0
                },
                {
                    "sent": "They called it motive regression, so we just re late the rough pounding intensity.",
                    "label": 0
                },
                {
                    "sent": "Is this candidate abundant scores through a linear model?",
                    "label": 0
                },
                {
                    "sent": "In our case, we have in 287 DNA segments and peas 195 candidate words.",
                    "label": 0
                },
                {
                    "sent": "So it's not super high dimensional piece listing in, but nevertheless you need to regularize to come up with a good solution here.",
                    "label": 0
                },
                {
                    "sent": "The goal is variable selection only, and I think from this point of view it's a good example.",
                    "label": 0
                },
                {
                    "sent": "I'm not interested in predicting and you why this is totally, I mean scientifically, not interesting.",
                    "label": 0
                },
                {
                    "sent": "The only thing I want to know is which of the candidate words are important for explaining why, and then hopefully these are good candidates for the biology.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "No prediction involved, it's only variable selection.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so of course you can do some sort of all subsets regression and things like this, but now the lasso technique is extremely efficient from a computational point of view.",
                    "label": 0
                },
                {
                    "sent": "All what you do is you run your convex optimization program and you get at least some selected variables, so you can select your variables just as the variables which have corresponding non zero estimated regression coefficients.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "It's very simple minded.",
                    "label": 0
                },
                {
                    "sent": "You just say if it's non zero in your estimate it should be in and if it's zero it should be out.",
                    "label": 0
                },
                {
                    "sent": "So each had Lambda is some sort of inference machinist limit for the true underlying active Set S 0, which is the one where the truth has non zero rational coefficients.",
                    "label": 0
                },
                {
                    "sent": "And the first thing which strikes, at least the statisticians, is there's no testing involved, no measure of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And well, in part because it's hard.",
                    "label": 0
                },
                {
                    "sent": "How can you come up with a test in high dimensional scenarios?",
                    "label": 0
                },
                {
                    "sent": "So this is simple minded solution.",
                    "label": 0
                },
                {
                    "sent": "It's convex optimization only and this kind of maybe simple minded solutions.",
                    "label": 1
                },
                {
                    "sent": "Sometimes you run into problems with Seymour troubles, but nevertheless it became quite popular to do it this way and I want to explain about what's going on if you do it this way.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in my modification example is again the description.",
                    "label": 0
                },
                {
                    "sent": "I'm on linear model Ann is 287 PS 195.",
                    "label": 1
                },
                {
                    "sent": "The last selects 26 variables.",
                    "label": 0
                },
                {
                    "sent": "Well, I'd chose the Lambda via cross validation.",
                    "label": 0
                },
                {
                    "sent": "So my Lambda is kind of geared towards prediction.",
                    "label": 0
                },
                {
                    "sent": "This may be a problem, but I have not another good way to do it at the moment, so the last searches selects 26 variables and while you could think of 26 interesting candidate motives.",
                    "label": 1
                },
                {
                    "sent": "No way.",
                    "label": 0
                },
                {
                    "sent": "I mean there are way too many there.",
                    "label": 0
                },
                {
                    "sent": "Just maybe one or two or three but not 26.",
                    "label": 0
                },
                {
                    "sent": "So biologically there way too many here for Mr Distichal POV would say OK 26 out of 195.",
                    "label": 0
                },
                {
                    "sent": "That may be pretty reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's again do the analog.",
                    "label": 0
                },
                {
                    "sent": "Less minutes here.",
                    "label": 0
                },
                {
                    "sent": "So what do we know from theory and how should we interpret?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Result like this OK?",
                    "label": 0
                },
                {
                    "sent": "So theory for the last Part 2 about variable selection.",
                    "label": 1
                },
                {
                    "sent": "Again, I look at a fixed design linear model because it is the easiest scenario.",
                    "label": 1
                },
                {
                    "sent": "My active set is S0 and here are two key assumptions and I will discuss them a bit.",
                    "label": 0
                },
                {
                    "sent": "The first one is what we called the neighborhood stability condition.",
                    "label": 0
                },
                {
                    "sent": "It's a condition on your design X and then later ping Zhao Bing, you reformulated it in a nicer form and it's called the representable condition for the design X.",
                    "label": 1
                },
                {
                    "sent": "So it's a design condition.",
                    "label": 0
                },
                {
                    "sent": "It's not the re strict, they want eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Another design condition and the other condition discussed this a bit and the other one is kind of plausible.",
                    "label": 0
                },
                {
                    "sent": "So you say non zero true coefficients.",
                    "label": 0
                },
                {
                    "sent": "They have to be sufficiently large if you just have very small signal strings, there's no hope you will able to detect these variables right?",
                    "label": 0
                },
                {
                    "sent": "And so actually the condition is the non zero regression coefficients, the true ones.",
                    "label": 0
                },
                {
                    "sent": "They're either 0 and then the non 0 ones have to be larger than C Times Square root log P over in CS certain constant.",
                    "label": 0
                },
                {
                    "sent": "Magic in this log P over in square root which comes up.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now both these conditions.",
                    "label": 0
                },
                {
                    "sent": "Are sufficient, that's great, but the trouble is there essentially necessary.",
                    "label": 0
                },
                {
                    "sent": "So if they fail, your lasso will not give you the right active set of variables, so these conditions are sufficient and essentially necessary in order that is simple, is had, Lambda recovers the true S0 with high probability OK.",
                    "label": 1
                },
                {
                    "sent": "So this is what he would like to have.",
                    "label": 0
                },
                {
                    "sent": "It would be a great goal.",
                    "label": 0
                },
                {
                    "sent": "It's a very ambitious goal.",
                    "label": 1
                },
                {
                    "sent": "You would need to choose the lamb that be larger than for prediction.",
                    "label": 1
                },
                {
                    "sent": "That's OK, maybe, but you have these two conditions, so we proved this quite awhile ago.",
                    "label": 0
                },
                {
                    "sent": "And to point this, both these assumptions are really restrictive.",
                    "label": 0
                },
                {
                    "sent": "OK, and I mean we have it in our paper, but people typically it's kind of nice.",
                    "label": 0
                },
                {
                    "sent": "You read it in a positive way.",
                    "label": 0
                },
                {
                    "sent": "You take the positive part of the paper and say OK, they proved under these conditions it works, but we also show if the conditions are violated it doesn't work and it's kind of a message which I want to pass here as well.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work if the conditions are violated, the conditions are sharp.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the equivalent form.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to give you an idea, this is the representable condition.",
                    "label": 0
                },
                {
                    "sent": "Here is my gram matrix and what it boils down to.",
                    "label": 0
                },
                {
                    "sent": "You partition your gram matrix into the part where you have the active through variables.",
                    "label": 0
                },
                {
                    "sent": "You just order them before the 1st is zero variables.",
                    "label": 0
                },
                {
                    "sent": "So here are the empirical covariance is among the active variables here about active and non active variables and so on and so forth and you representable condition reads like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's very compact, it's nice, it's beautiful, but of course it's non checkable, you have no chance to check this condition if somebody gives you a matrix.",
                    "label": 0
                },
                {
                    "sent": "6.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is again this graph where are we with this condition?",
                    "label": 0
                },
                {
                    "sent": "So again, here is the restricted isometry property and is very presentable.",
                    "label": 0
                },
                {
                    "sent": "Condition is somewhere here in the middle, and the restricted L1 eigenvalue condition I told you is way more on the right and in my view I would really claim the representable condition for the variable selection problem is way more restrictive than the compatibility condition order restricted L1 eigenvalue condition so.",
                    "label": 1
                },
                {
                    "sent": "If you're interested in doing the practical stuff, you should be worried about the zero, presentable condition.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I say it's not very realistic and so you can still run it.",
                    "label": 1
                },
                {
                    "sent": "I did run it, I select 26 variables in my example.",
                    "label": 1
                },
                {
                    "sent": "So what can I expect from this?",
                    "label": 0
                },
                {
                    "sent": "And here is kind of a nice story which I think is important to bring over.",
                    "label": 0
                },
                {
                    "sent": "So in my view this L1 restricted restrictor, one eigenvalue condition, or these compatibility condition is kind of reasonable in practice.",
                    "label": 0
                },
                {
                    "sent": "So I told you before under this condition we get this bound on the estimation error, and I think this is kind of reasonable in practice.",
                    "label": 0
                },
                {
                    "sent": "OK so I have this L1 norm bound.",
                    "label": 0
                },
                {
                    "sent": "Is this kind of more technical formula here?",
                    "label": 0
                },
                {
                    "sent": "But now this is really a trivial thought.",
                    "label": 0
                },
                {
                    "sent": "Now you say OK, the hope is maybe to get at least the strong variables, right?",
                    "label": 0
                },
                {
                    "sent": "So suppose some of the variables have huge regression coefficients.",
                    "label": 0
                },
                {
                    "sent": "Can we get them?",
                    "label": 0
                },
                {
                    "sent": "And so I define not.",
                    "label": 1
                },
                {
                    "sent": "This is not the S 0.",
                    "label": 0
                },
                {
                    "sent": "This is not the set of active variables.",
                    "label": 0
                },
                {
                    "sent": "These are the variables.",
                    "label": 1
                },
                {
                    "sent": "We just have sufficiently strong sufficiently large coefficients, so S relevant are the ones which have sufficiently large coefficients, and it's actually just copied from this formula, just a larger than this bound.",
                    "label": 0
                },
                {
                    "sent": "And I just say here then clearly.",
                    "label": 0
                },
                {
                    "sent": "And I mean this is really trivial.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have one of these variables.",
                    "label": 0
                },
                {
                    "sent": "And suppose your estimate would be 0.",
                    "label": 0
                },
                {
                    "sent": "OK, then this is an immediate contradiction that you have this L1 error norm bound.",
                    "label": 0
                },
                {
                    "sent": "So again, suppose you would say Beta had J is 0, but in fact it's larger than this it cannot happen because you have this bound here and that has a very interesting interpretation.",
                    "label": 0
                },
                {
                    "sent": "So clearly I know that with high probability from this stuff with high probability, the ASAT contains all the relevant variables.",
                    "label": 1
                },
                {
                    "sent": "And that's great news in a way.",
                    "label": 0
                },
                {
                    "sent": "I mean this is.",
                    "label": 0
                },
                {
                    "sent": "Only one side of the story, but in a way I don't miss them.",
                    "label": 0
                },
                {
                    "sent": "I get them and a couple of other stuff.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is screening for detecting the relevant variables is quite likely to be possible in practice.",
                    "label": 1
                },
                {
                    "sent": "OK, I selected 26 variables in my example.",
                    "label": 0
                },
                {
                    "sent": "It kind of means the true stuff is hopefully among the 26.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now you can move on.",
                    "label": 0
                },
                {
                    "sent": "If you say OK, you.",
                    "label": 0
                },
                {
                    "sent": "S0I mean this is the active state.",
                    "label": 0
                },
                {
                    "sent": "Actually all the non zero coefficients are sufficiently large.",
                    "label": 0
                },
                {
                    "sent": "Then you have screening for the active set, but that's maybe equational assumption, But if you're willing to say OK, either 0 or there is large enough, then you know the S hat from loss of will actually contain your true active variables.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. And so I didn't say it.",
                    "label": 0
                },
                {
                    "sent": "What lasso stands for.",
                    "label": 0
                },
                {
                    "sent": "So tips you ran in his crate paper.",
                    "label": 0
                },
                {
                    "sent": "I mean coined the term lasso in each stands for least absolute shrinkage and selection operator.",
                    "label": 1
                },
                {
                    "sent": "And here is my new translation of this.",
                    "label": 0
                },
                {
                    "sent": "I think it should be read at least absolute shrinkage and screening operator.",
                    "label": 1
                },
                {
                    "sent": "It really.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Screens in the sense of you catch it, but a couple of other stuff you don't select really 480.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, from a practical perspective, if you want to do that, it is still this Lambda round, right?",
                    "label": 0
                },
                {
                    "sent": "And at least I don't know well.",
                    "label": 0
                },
                {
                    "sent": "I come a bit later to this issue from another perspective, but it's hard to get a good Lambda for selection.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe some BICS would do the job, but at least nobody has proof, at least what I've seen.",
                    "label": 0
                },
                {
                    "sent": "Something like that, so you just run your cross validation for the relevant variables sorted through variables so in practice is kind of nice.",
                    "label": 0
                },
                {
                    "sent": "You have maybe two small line that you just get too many variables.",
                    "label": 0
                },
                {
                    "sent": "Now, why is it is interesting?",
                    "label": 0
                },
                {
                    "sent": "The other issue, which I should mention this, what is the size of this asset?",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have this, just all the variables, you always have the screening property.",
                    "label": 0
                },
                {
                    "sent": "That's not an interesting screening operation is hacked for any kind of Lambda in absolute values, cardinality is less or equal in meaning P. This is trivial to show or easy to show, so the last so selects at most meaning P variables.",
                    "label": 0
                },
                {
                    "sent": "Now if P is in the 10,000 or in the millions and then in the dozens or hundreds.",
                    "label": 0
                },
                {
                    "sent": "You reduced to a dozen or 100 variables.",
                    "label": 0
                },
                {
                    "sent": "OK, that's great, and so this is really.",
                    "label": 0
                },
                {
                    "sent": "This screening is a huge dimensionality reduction for your problem.",
                    "label": 0
                },
                {
                    "sent": "I mean, in the original covariance, you're not doing some sort of PCA, linear combination, or so.",
                    "label": 0
                },
                {
                    "sent": "It's just huge dimensionality reduction in the original coherence, and it is a huge gain and very useful machine.",
                    "label": 1
                },
                {
                    "sent": "Kind of in my experience.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so once you have it once we have done this reduction.",
                    "label": 0
                },
                {
                    "sent": "You're down to a couple variables, so to speak.",
                    "label": 0
                },
                {
                    "sent": "I mean, if any is not too large and then I just say now you just apply your favorite method of choice.",
                    "label": 0
                },
                {
                    "sent": "It's not very high dimensional anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, and people have done some things that typically you would do some sort of re estimation.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, you just do order release squares Re estimation.",
                    "label": 0
                },
                {
                    "sent": "You're smaller set from S 0 and you apply some sort of PC penalty there.",
                    "label": 0
                },
                {
                    "sent": "Or you do thresholding of the coefficients and OS Re estimation.",
                    "label": 1
                },
                {
                    "sent": "Or you apply the adaptive lasso and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So The thing is really you cannot do it in one stage.",
                    "label": 0
                },
                {
                    "sent": "You do the first stage for the screening, and in the second or even the third stage you need to do something else to get rid of the false positives.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK summary 2 for the lasso.",
                    "label": 0
                },
                {
                    "sent": "Variable selection, so if you really want to infer the F0 that through active state is a very ambitious goal, and it requires necessarily teeth rather restrictive assumptions, the representable condition, and that the non zero regression coefficients are sufficiently large, both of them are restrictive.",
                    "label": 1
                },
                {
                    "sent": "But as I kind of indicated, variable screening is much more realistic, it works based a much weaker restrict their one eigenvalue condition and then you get the result like this.",
                    "label": 0
                },
                {
                    "sent": "Even if you have many small nonzero regression coefficient in the truth, you would still get.",
                    "label": 0
                },
                {
                    "sent": "I mean, the ones which are relevant.",
                    "label": 0
                },
                {
                    "sent": "The large coefficients you get, and if you assume the truth is such that it's either 0 or the coefficients are sufficiently large, you actually scream for the true underlying S 0.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, I mainly focused on the last selling linear models.",
                    "label": 1
                },
                {
                    "sent": "Many extensions have been worked out.",
                    "label": 1
                },
                {
                    "sent": "Group lasso, fused lasso and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "Some people work with concave penalties.",
                    "label": 0
                },
                {
                    "sent": "They argue they have better selection properties.",
                    "label": 0
                },
                {
                    "sent": "This cat MC Plus and so on.",
                    "label": 0
                },
                {
                    "sent": "Matching pursuit boosting again on the rough scale.",
                    "label": 0
                },
                {
                    "sent": "I would say it's kind of a similar story I'm available.",
                    "label": 1
                },
                {
                    "sent": "I'm in the details are different, but in the Ralph scale I think this is really what's going on in high dimensional statistical inference and summarizing this prediction.",
                    "label": 0
                },
                {
                    "sent": "Is really kind of easy estimation of parameters.",
                    "label": 1
                },
                {
                    "sent": "Getting your parameter vector is substantially harder, but it's kind of realistic.",
                    "label": 0
                },
                {
                    "sent": "In practice, variable screening is often kind of reasonably accurate, and if you really add variable selection, getting the right is 0.",
                    "label": 0
                },
                {
                    "sent": "This is a very hard problem.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe continue with this before making it very shortly labrake, so once you know how to do the variable selection in a linear model, you almost immediately get to Gaussian graphical models, at least the undirected Gaussian graphical model.",
                    "label": 0
                },
                {
                    "sent": "It's conceptually not very different, so here is again the setting.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have observations X1, XN, IID from AP dimensional Gaussian.",
                    "label": 1
                },
                {
                    "sent": "This means zero and some covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "That the goal in Gaussian graphical modeling, the undirected graph, is to infer the zeros of Sigma inverse.",
                    "label": 0
                },
                {
                    "sent": "So Sigma inverses are very ill behaved object if P is much much larger than in.",
                    "label": 0
                },
                {
                    "sent": "If you don't observe too many too many observations relative to the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So why is sick my inverse an interesting quantity?",
                    "label": 0
                },
                {
                    "sent": "Because it encodes the conditional dependence and independence between the variables.",
                    "label": 0
                },
                {
                    "sent": "So, for example, XJ is conditionally dependent on XC given all other variables if and only if the JK entry in Sigma inverse is different from zero, and then you draw an edge in the graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So why is this almost the consequences of linear model and variable selection?",
                    "label": 0
                },
                {
                    "sent": "You can transfer this problem into linear models as follows.",
                    "label": 0
                },
                {
                    "sent": "So I say node wise regression can do the job.",
                    "label": 1
                },
                {
                    "sent": "It's quite easy to show that now you just regress the Jays covariant to all others and then you have an error term.",
                    "label": 0
                },
                {
                    "sent": "And then you look at these regression coefficients beta KJ and you can also regress the case to the Jason all others, so we can do the symmetric version.",
                    "label": 0
                },
                {
                    "sent": "And now it happens, and it's not too hard to show that beta KJ the regression coefficient is different from zero if and only if Sigma inverse JK is different from zero, because beta KJ really is the partial correlation between XJ and XK.",
                    "label": 0
                },
                {
                    "sent": "Given all others is exactly the thing like here up to scaling.",
                    "label": 0
                },
                {
                    "sent": "These are the same things, in particular, the zeros are the same, so I can read off the edge just from a regression coefficient.",
                    "label": 0
                },
                {
                    "sent": "And this is actually what we did couple.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Years ago, so you just run P different lasso regressions.",
                    "label": 1
                },
                {
                    "sent": "It's 1000 of regressions, but each of them is fast and the estimate DH if the estimated coefficient is different from zero, and sometimes because in the estimated version it's not symmetric anymore.",
                    "label": 0
                },
                {
                    "sent": "You make a rule the estimated coefficient beta had KJ and beta.",
                    "label": 0
                },
                {
                    "sent": "JK Hat is different from the Roses Symmetrize, or you don't.",
                    "label": 0
                },
                {
                    "sent": "There's an and or overrule to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "And then you can just estimate very high dimensional Gaussian graphical models with this.",
                    "label": 0
                },
                {
                    "sent": "And there's a story behind this, which is may be interesting to say.",
                    "label": 0
                },
                {
                    "sent": "So the first criticism about running these pelosso regressions, it does not use the constraint that you should actually have a positive definite matrix Sigma.",
                    "label": 1
                },
                {
                    "sent": "So you kind of lose out information on this.",
                    "label": 0
                },
                {
                    "sent": "So you think this is not really?",
                    "label": 0
                },
                {
                    "sent": "This is kind of a brute force idea.",
                    "label": 0
                },
                {
                    "sent": "Doing it this way.",
                    "label": 0
                },
                {
                    "sent": "And actually so meinshausen was my student and he first looked at what people later did.",
                    "label": 1
                },
                {
                    "sent": "Just look what people say, the G lasso approach.",
                    "label": 0
                },
                {
                    "sent": "You take the multivariate Gaussian and you estimate the Sigma inverse by L1 norm penalization.",
                    "label": 0
                },
                {
                    "sent": "And he just came to my office and said, well, I think this doesn't work well because it needs so restrictive assumptions varies if you decouple it into various regressions, you need much less restrictive assumptions.",
                    "label": 0
                },
                {
                    "sent": "So from a theory point of view, if you're interested in inferring the edge set the graph, it's substantially better to decouple it in unrelated linear regressions because the underlying representable condition based is node wise.",
                    "label": 0
                },
                {
                    "sent": "The coupled approach are much weaker than if you go to simultaneous way with the glass.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in estimating Sigma or Sigma inverse, I'm not making this claim so I just make the claim.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in the graph structure, I think just running the regression separately certainly requires less theoretical assumptions and empirically it's at least as good as the simultaneous approach.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, maybe we should take a little break here to refresh your minds.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Although maybe just a 5 minutes break and OK?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Short one, so you special screening.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you have a really short question, so on this screening property you multiple times used like those qualitative words like all should work mostly.",
                    "label": 0
                },
                {
                    "sent": "Those things seem to be really easy to simulate.",
                    "label": 0
                },
                {
                    "sent": "I wear if any work where like extensive simulations would be done and then actually show like OK, we're not missing more than 5% of something like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "I mean people and also.",
                    "label": 0
                },
                {
                    "sent": "We have simulated quite a lot.",
                    "label": 0
                },
                {
                    "sent": "We have some empirical kind of picture, at least what we see and kind of what I said.",
                    "label": 0
                },
                {
                    "sent": "It's more realistic that it works in practice is really based, maybe on my empirical observations.",
                    "label": 0
                },
                {
                    "sent": "However, in a way.",
                    "label": 0
                },
                {
                    "sent": "I agree, I mean simulation gives you a good feeling, but what the community wants to have is some sort of people call it performance guarantee, right?",
                    "label": 0
                },
                {
                    "sent": "And that's just the simulation.",
                    "label": 0
                },
                {
                    "sent": "I think it's a very good exercise for yourself or your group or your local colleagues, but it's hard to convince the colleagues by just saying, hey, I just have these great simulations.",
                    "label": 0
                },
                {
                    "sent": "It just works well and.",
                    "label": 0
                },
                {
                    "sent": "I tried to be really to give some sort of honest picture here.",
                    "label": 0
                },
                {
                    "sent": "I do think this is representable condition is really a tricky condition, a tricky assumptions and you can fail quite substantially in getting is zero.",
                    "label": 0
                },
                {
                    "sent": "However, in getting the screening property my experiences it works reasonably, so it depends alot what kind of design you're looking at.",
                    "label": 0
                },
                {
                    "sent": "I mean So what I like to do.",
                    "label": 0
                },
                {
                    "sent": "I will show it afterwards.",
                    "label": 0
                },
                {
                    "sent": "Take a real design X from real data and create maybe artificial bait as an artificial errors.",
                    "label": 0
                },
                {
                    "sent": "Then you at least you know you capture the real design X and not some sort of cooked up design from your from your mind.",
                    "label": 0
                },
                {
                    "sent": "OK, I think it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "If you use the microphone.",
                    "label": 0
                }
            ]
        }
    }
}