{
    "id": "f6lgntleyhl55zlxizwee3lfojzwxzmi",
    "title": "Deep Reinforcement Learning",
    "info": {
        "author": [
            "Pieter Abbeel, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/",
    "segmentation": [
        [
            "So this morning."
        ],
        [
            "So far we've been looking here at reinforcement learning, and this is a picture I'm borrowing from Joelle slides.",
            "There's a dynamical system which has some state your learning agent might have access to that state.",
            "Then, based on that decide to take an action might also get reward along the way, and then this goes around in the feedback loop.",
            "And there's many ways of trying to solve the problem of finding a good policy for the agent.",
            "What we're going to look at in this session is direct policy optimization."
        ],
        [
            "Where we assume there's some policy Pi Theta.",
            "This could be some kind of neural net.",
            "There could be something else to this policy Maps from state or observation.",
            "ULL inputs through some calculations to then a set of a vector of actions or potentially a distribution over actions.",
            "So take a look at what this form."
        ],
        [
            "That means we now have an optimization problem.",
            "We want to maximize expected sum of rewards accumulated overtime under policy Pi Theta, where the policy Pi Theta is what we optimize over.",
            "Typically the policy Pi Theta is chosen to be stochastic.",
            "The reason that's done is because it smooths out the optimization problem because small changes in a stochastic policy will result in a small change in expected return.",
            "And now I have a nice optimization problem to work with.",
            "Why?"
        ],
        [
            "Would you do policy optimization rather than something else?",
            "Often the policy can be simpler to learn and to represent then a value function or Q function.",
            "For example, let's say a robot needs to graph something like a bottle of water.",
            "All we need to do is get close and close its fingers and that's it.",
            "And there might be a very simple policy.",
            "Closing might be very simple to represent, but understanding exactly how much value there is associated with closing your fingers and exactly how you close them and so forth might be much harder to learn.",
            "And so by directly looking at the policy and the policy only might be easier to solve this problem.",
            "If instead you decide to learn value functions rather than a policy, one issues.",
            "That value function doesn't describe actions for you.",
            "Once you have a value function to then act, you still need to do something.",
            "For example, you need to do one step.",
            "Look at, say, if I were to take any of my set of actions, what would happen?",
            "Or what's the distribution over things that could happen?",
            "Then evaluate the value at the next state as well as reward along the way and see which action is best.",
            "You could say, well academically learn Q values throughout and values that often works well.",
            "We've seen some examples this morning where it gets tricky is if your action space is very large.",
            "For example, a continuous action space or just very high dimensional discrete action spaces.",
            "Because in the optimization argmax overall actions of qsa can be very tricky.",
            "Not saying it's impossible, it's just not super clear how to make that work yet.",
            "Another reason you might want to study policy optimizations because there's been a lot of success stories."
        ],
        [
            "So the top line represents some of the older success stories, nothing deep.",
            "And what happened there?",
            "You're showing what would be easier to choose an action if you're doing."
        ],
        [
            "Directly policy optimization.",
            "Rather than learning Qi mean you also have to pick out of many possible actions.",
            "So, so let's say, let's make this little concrete.",
            "Let's make this concrete.",
            "So if it's discrete.",
            "And you're able to output.",
            "Obviously, if you neural net can output AQ value for each possible action, then there's no issue.",
            "'cause then you can just see which one is the highest.",
            "But once you move to high dimensional, let's say continuous action spaces, you try to represent AQ value for every possible state action combination.",
            "You usually don't.",
            "It's not clear how to have an output corresponding to every possible action you might take, and so now effectively maybe becomes the input to your neural net.",
            "You have to actually propagate all the way back to infer which actions maximize this.",
            "Thinking about the high dimensional discrete space where it seems that you have the same problem in both cases, whether you learn Q.",
            "Are you learning policy using the case?",
            "Let's see if you have a high dimensional discrete case.",
            "For discrete case in general you have to.",
            "You so, so?",
            "There are a few things you can do.",
            "That might still be a little easier with policy representation, so let's say you have a discrete case and there's a bunch of actions we have to choose at any given time.",
            "Maybe you're playing multiple Atari games in parallel, or multiple players at the same time you're controlling them all, and they're all discrete actions, so a natural thing to set up, let's see, would be that if you have a policy, you would probably set it up.",
            "From state to action of the first agent, then from state and action on the 1st agent to action for the second agent, assuming they can control them all.",
            "You can think of them as a joint decision rather than simultaneously, in which case it doesn't.",
            "All doesn't matter, so would be one way to set it up for a policy for a Q function.",
            "You could.",
            "You could probably set it up so you'd have to set up your Q function to be decomposed on the output.",
            "In some way that it's easy to recover which combination of actions does best, and there are some tricks people have played.",
            "For example, there's a paper by I think it's shangu Sergei, 11, illicit skipper, Ann.",
            "One other author I'm blanking on, Tim Lillicrap.",
            "I think that looks at setting up the Q function so it is very easy to compute the argmax.",
            "So by design, the neural net is set up so that it's easy to compute the argmax, and they do it by setting up a quadratic essentially.",
            "Attic function in the actions.",
            "And since it's a quadratic function, the actions is easy to find the bottom, so there are definitely tricks people.",
            "Are playing.",
            "It's not super clear how to do it in generality, whereas if you go from state to action as a policy you can use pretty much any mapping.",
            "You're less constrained in terms of how you set it up.",
            "Another thing to think about is, I mean this will come back later as you're actually building these things is that there are things you can do to combine both.",
            "You can have multiple networks, so you can imagine that you say, well, it's hard to compute this argmax could set up another network that computes the argmax, just like you might have an inference, an encoding network, and so you do the same thing here.",
            "And if you look at something like GPG domestic policy gradient, it does something a little bit like that in general.",
            "Actor critic methods will do something a little bit like that where there is 2 networks and one is the policy network, the other one the value network and they work together."
        ],
        [
            "So.",
            "Top row here are some success stories in legged locomotion helicopter flight, another leg locomotion wanan pulling a Cup game where you have to swim football in a string and get into a Cup and they were all successfully trained on kind of very small representations.",
            "Nothing deep there, but policy optimization methods were able to succeed at these tasks where the policy was actually encoded with relatively small number of parameters and what's changed recently is that is becoming possible to run these methods with policy representations with very large numbers of parameters.",
            "And some example success stories are at the bottom.",
            "For example in Atari in Majokko, various simulated environments of dynamical systems and also even on real robots, and will look at mostly the bottom line in this presentation here.",
            "But keep in mind that there's been success stories before.",
            "Often, though constrained to small numbers of parameters that are being optimized."
        ],
        [
            "So as you all showed before, you can categorize RL methods in this way where there's policy optimization methods and there are dynamic programming methods and actually the actor critic methods bring both of them together where you have both a value function Anna policy and you train them both at the same time.",
            "In my presentation, we're going to focus on the left side of this and ending up at the actor critic methods at the end.",
            "He"
        ],
        [
            "The ideas I want to get across.",
            "There are three methods, likely ratio policy gradients, natural gradients, and trust regions.",
            "Actor critic path derivatives at different way of computing derivatives, then the likelihood ratio.",
            "A generalization of that through stochastic competition graphs.",
            "Guided policy search and inverse RL.",
            "The last two are probably for this afternoon and you'll see that there will be a lot of detail in the early ones and will dig into the math and really understand what's going on and for the later ones due to lack of time will go a little quicker and there will be referenced what the ideas are, but won't dig as deep."
        ],
        [
            "So let's start with derivative free methods.",
            "If you want to optimize the policy.",
            "Simplest thing to do?"
        ],
        [
            "To say, well, let's say I'm interested in maximizing my utility under some policy Pi Theta, it's maximizing expected sum of rewards.",
            "Well, just see this as a black box.",
            "You have a policy, you run it.",
            "You see how much reward you get.",
            "You could change the parameters, run it again, see how much you get and just not even look inside what's going on.",
            "Don't worry about the fact that this is a reinforcement learning problem, is just an optimization problem maximizing utility with respect to parameters Theta.",
            "So we're going to ignore all other information collected.",
            "This cross entropy, that which the one we're looking at here is an evolutionary algorithm means it keeps track of a distribution over possible solutions, which is represented by \u03c0 mu I sumu indexes into the type of distribution is a Gaussian.",
            "You could just be the mean or mu could be mean and variance.",
            "An I hear correspond to the iteration of our population and then Theta is the space of which the distribution lives.",
            "Here's what cross entropy looks like.",
            "You iterate, you initialize some house with some distribution over the parameters.",
            "Then you start iterating.",
            "You sample population members from your current distribution.",
            "So you sample published Member Theta E From a distribution P mu I Theta.",
            "Then you use that parameter vector to execute a bunch of times.",
            "You see how well you do collect those pairs data an utility?",
            "Once you've done that for a few parameter settings data, you check which ones achieves the achieve the highest score.",
            "So you say here are my top 10% of parameters.",
            "Those survive quantum code an.",
            "Then you maximize the log probability of those population members.",
            "And you maximize this over the choice of your parameters in your distribution you.",
            "This gives this.",
            "If it's a Gaussian, will shift your Gaussian to where the good samples were, and then you repeat gets new set of samples and keep going.",
            "No where do we look inside the box.",
            "Ugly black box optimization method."
        ],
        [
            "Actually works embarrassingly well.",
            "Here's a table from a paper from not too long ago, an essentially what it shows is that so this is on the game of Tetris, which is a standard RL benchmark.",
            "It's a game where you have blocks coming down.",
            "You need to play some of Arrow is full, it clears and you can keep letting blocks come down.",
            "It's been a benchmark NRL ever since people started doing these things, for example, typically burzik as often in their book references benchmark.",
            "And traditionally solved with value function methods.",
            "It turns out this cross entropy thing can outdo this value function methods, even though it's so simple, doesn't even look at anything underneath.",
            "Now she took till 2013 when a paper came out.",
            "Approximate dynamic programming finally performs well in the game of Tetris, where these value function methods could finally compete with cross entropy.",
            "It just took a lot of work, even though cross entropy is so simple, so it's.",
            "It's not just in Tetris that this has been the case."
        ],
        [
            "Similarly, in graphics, a lot of the graphics animations rely on a cross entropy like method CMA, and so it's a very, very powerful method.",
            "Surprisingly powerful method."
        ],
        [
            "There's a caveat, though.",
            "It tends to work best when the number of parameters is relatively small.",
            "In the touches example, the Canonical parameterisation of Tetris that was being used unlock the Atari things we've looked at is to go from the board to a predefined 20 dimensional feature vector, and then run your algorithm on how you're going to wait those different features in your value functions.",
            "They're only finding 20 parameters, and So what?",
            "You did value function methods in that literature, or cross entropy.",
            "It would always only find 20 parameters general for this kind of algorithms you need.",
            "The number of samples from your population to be roughly comperable to the number of dimensions in your parameter vector.",
            "If you can do that, then this can work pretty well if you work at a neural net million parameters that you need a lot of samples to make this practical.",
            "Still very implement and it's a great baseline to compare with you.",
            "At least know have some notion of how well your actual our algorithm works relative to some basic baseline."
        ],
        [
            "So something else you can do just completely ignoring the inside structure of the problem is just to find out differences.",
            "You could say I have a function that want to optimize it that is expected sum of rewards.",
            "Let's look at the parameter vector, perturb it, see what the local derivative is, compute the entire gradient, make an update, and repeat.",
            "Some challenges there."
        ],
        [
            "If you're doing these rollouts, typically there stochastic this can be because of the dynamics because of your policy being stochastic, both could be at play and so it could be the function that looks like this.",
            "An when you evaluate the derivative based on two samples, you actually get something that sloped exactly the wrong way.",
            "Fixes to this."
        ],
        [
            "Well, he could take a lot of samples and it will average out.",
            "There won't be a problem, but that is often what you want to avoid.",
            "You want to learn from a relatively small number of samples if possible.",
            "Not a fix you can make at times is to fix."
        ],
        [
            "The random seed.",
            "So as you execute your policy.",
            "You execute another policy.",
            "Different setting of the parameter vector.",
            "You keep the random seed the same, and so the randomness will interact with your system the same way and this will give you more consistent computation of derivatives and so forth."
        ],
        [
            "Dennis could be in policy and in dynamics in the policy you can control it when you sample actions given state, you can totally control how you do that in the dynamics.",
            "It might be harder to control, it can give issues.",
            "Let's say you're flying a helicopter.",
            "You want it to be a good policy to try a different policy.",
            "Now the wind conditions are different.",
            "If it's more wind gusts here, it's going to be harder to do well an.",
            "You can just look at the outcome of one run and decide that oh, the second parameter setting was worse.",
            "'cause it did more poorly, 'cause it might have been caused by this randomness in the environment.",
            "There's some trickiness here in getting this to work now.",
            "Some nice theoretical guarantees, though if you can fix the random seed, how does make things more efficient than if you can't fix it?",
            "Let's take a look at a result that was obtained."
        ],
        [
            "With this with UK, yeah, if you had a generative model of the wind you would be OK.",
            "Please bring me one someday.",
            "So what we'll see here is a helicopter controlled by control policy that was found through direct policy search.",
            "Running things as a black box.",
            "So there is nothing looking inside when the rewards are accumulated is just a bunch of parameters that are being optimized by repeated trial and error and looking at the total reward attained using the trick of fixing the random seed, the learning is happening in simulation.",
            "And then the test that we see here is on the real helicopter.",
            "But the reason the random seed could be fixes all the learning was in simulation.",
            "So this is helicopter reliably hovering.",
            "This actually hard problem.",
            "So stabilizing helicopter.",
            "Think of it as a balancing along broom.",
            "The palm of your hand.",
            "And then you need to constantly look at that broom to balance it and make sure it doesn't fall over.",
            "Same thing here for helicopter and as someone amazing cinematography here where we rotate the camera by 180 in the middle of this video, exposing that essentially flying upside down.",
            "So.",
            "Underneath"
        ],
        [
            "Policy here was relatively simple, just 12 parameters.",
            "If that's the number of parameters, again these kind of methods can work pretty well.",
            "You don't need necessarily much more than these basic methods.",
            "But now."
        ],
        [
            "Start digging into higher dimensional policies and see what we can do there.",
            "What's this likely ratio policy gradient?"
        ],
        [
            "So some notation.",
            "Tile will be a sequence of States and actions.",
            "OK, so stay at a time 0 action U at time zero and so forth will work with finite horizon.",
            "Just for simplicity of notation and doing the math.",
            "But these things carry over if you do more intricate math to infinite horizons and average rather than total reward.",
            "We try to optimize expected sum of rewards, which we're going now for now, right as some over possible trajectories, probability of directory under the policy Pi Theta times reward return from that trajectory.",
            "So initially we're again going to ignore the fact there's a sequence of States and actions which is going to look at it as distributions over trajectories and used by the policy later will break it up into the details of States and actions."
        ],
        [
            "OK, so this is what we want to compute.",
            "We want to compute the gradient of our utility function with respect to the parameters Theta.",
            "So gradient perspective data of some overall trajectory's probability of trajectory times reward of that trajectory.",
            "We can move that gradient inside the summation.",
            "And now we can play a trick which we play because we have some foresight of how it is going to play out.",
            "It's very easy to see that this is OK. We multiply and divide by the same thing, but it might not be obvious why we're doing this at this point.",
            "Once we do that.",
            "We remember that the gradient of a log is the grand of that thing divided by the thing, and we use that in reverse to get.",
            "Look at this get this expression over here.",
            "Now it becomes clear why we play this trick, 'cause if we look at this, we actually have an expectation.",
            "Again, we see that the gradient can be computed by taking an expectation with respect to the distribution that we're working with, distribution over trajectories.",
            "So that's why we did this and we can then evaluate this based on samples.",
            "So we can say we have our current distribution that we get from executing our policy Pi.",
            "Theta will get some samples an averaged over the samples, evaluate this quantity over here which gives us this expression over here.",
            "For interesting, the expectation of this is the exact gradient, but also from just a single sample.",
            "Actually we can get.",
            "A quantity that an expectation will be equal to the gradient.",
            "It's not going to be very accurate from 1 sample, but an expectation that will be correct.",
            "OK."
        ],
        [
            "We're going to drive.",
            "There's a different way too, which is kind of interesting.",
            "Maybe makes it look a little less like magic.",
            "Important sampling the important sampling is that you can evaluate an expectation under distribution.",
            "Theta by actually collecting samples under a different distribution parameterized by Theta old and the way you do it is you have samples under Theta old and then you re weight them by P Theta over Theta old.",
            "OK, so the first equation says this essentially infinite sample case.",
            "That utility can be evaluated as an expectation or an old data.",
            "This way.",
            "We can then take the gradient of that expression.",
            "We can then.",
            "Bring in the gradient inside the expectation.",
            "We can then evaluate this expression at Theta old.",
            "And then again, gradient of log is granted.",
            "The thing divided by that thing, and so this is what we get and we get the same expression again that we have an expectation respect to some distribution, Theta old of the grad log, probability under that distribution times reward.",
            "So here the ratio comes in because of important sampling.",
            "What's interesting is that this also allows you to see that you can do more than just computing gradients.",
            "This way you can actually evaluate.",
            "From just a few runs under your policy Theta old, you can evaluate what the policy will be like in neighboring parts of the space.",
            "Of course, the more data is different from the old, the higher the variance on this estimate.",
            "So you can go super far, but at least you have an estimate."
        ],
        [
            "What's interesting is that this is valid even if your function R is discontinuous.",
            "You have discrete state actions and so forth, so this is really interesting in that you have no relies on continuity of your reward function here.",
            "We assume that the log probabilities has derivatives, but."
        ],
        [
            "We don't assume anything on the objective.",
            "Let's look at some into it."
        ],
        [
            "I'm behind this equation here, so what's going on?",
            "We have a policy Pi Theta.",
            "We do rollouts under that policy.",
            "And then compute this Gray and estimate if we look at it says grad log probability of path I.",
            "Times reward under path I.",
            "So this grain is saying we're trying to maximize utility here is going to say we're going to increase the probability of paths with a positive R and decrease probability of paths with negative are.",
            "Important to keep in mind that it actually never tries to change the paths so it's never looking at those paths and saying if you just had budged a little bit this way or that way you would have been a better path.",
            "For that you need the kind of differentiability that exactly we're not assuming you need a different kind of derivative, will look at later.",
            "This doesn't do that, it just looks at each of the paths as they were and puts more probability mass on the good ones, lessen the bad ones.",
            "In fact, a lot like the cross entropy method, just using gradients rather than what was formulated there.",
            "Now let's break this down into pieces."
        ],
        [
            "So the probability of a path is the product of the probabilities of each of the transitions.",
            "So we have probability of initial state, which I'm ignoring here.",
            "And then there is probability of next state given current state and action times probability of action given state with dynamics and policy that participate in this and then multiplied over all times.",
            "Grad log probability log of a product.",
            "Some of the logs will get this.",
            "Then look at.",
            "The two sides, the first one actually does not have any Theta in it, so actually disappears.",
            "So it's very interesting that when we compute the grad log probability here.",
            "Dynamics disappear.",
            "Completely not present in this equation.",
            "So this is what it plays out as his final equation, and so we can evaluate this grad log probability.",
            "Without having a dynamics model.",
            "You should also be worried 'cause you said well, if you don't use your dynamics model, how informative can you gradient really be?",
            "And again, we'll get back to that later, but it's nice that you don't need it, and often that's quite helpful.",
            "'cause often you don't have a dynamics model readily available."
        ],
        [
            "OK, so.",
            "Here is a summary of what we have so far.",
            "We're going in these likely ratio policy gradient methods.",
            "We're going to compute a policy gradient estimate by the 1st equation.",
            "The grad log probabilities times reward where the ground log probabilities of passes breakdown into just some of grad log probabilities of action given state at each time.",
            "An an expectation this is correct."
        ],
        [
            "Expectation correct, but very noisy, so I'm going to look at a few tricks to make this less noisy.",
            "Reduce the variance on this estimate.",
            "Of course you can just get infinitely many samples.",
            "That will also make it a good estimate, but try to get good estimates with small number of samples.",
            "First idea is going to be baselines.",
            "Next one is going to be exploiting temporal structure, and then we'll look at trust regions.",
            "Step sizing to make this even better."
        ],
        [
            "OK, this is our equation again.",
            "Turns out I want to build some intuition.",
            "Here.",
            "We can look at what is the reward is always positive so.",
            "Reward, no matter what you do, is positive.",
            "But of course there's some things you do to reward is much more positive than for other things.",
            "This equation is saying.",
            "You want to increase the probability of everything you've experienced so far.",
            "Even the not so great things that had relatively low reward but still positive.",
            "More so for the things that were very positive, but still it's a very weird thing that whatever you did recently, you want to increase the probability of what you did.",
            "So essentially you get a lot of fighting 'cause they're not so great things will improve, increase their probability and they'll need a lot of really great things to later be experienced to kind of out.",
            "Do that.",
            "You increase those probabilities and it'll be a lot of back and forth before I get anywhere.",
            "OK, so we could consider introducing a baseline.",
            "What you do there is you subtract out from the reward some baseline B what this is saying is that maybe have some estimate of what is the average that you might achieve.",
            "And then what this becomes increase the probability of things that are better than average, decrease the probability of things that are worse than average should always do it.",
            "I mean, without a baseline really this thing will never work.",
            "Once you do a baseline at least you have that aspect in place.",
            "So this is still an unbiased estimate.",
            "It turns out if you do the math, the expected value of this thing is still equal to the actual gradient, but it will reduce the variance if you choose a good baseline.",
            "What's a good choice for B?",
            "A very simple thing would be just on average how much have you gotten from your past rollouts.",
            "A slightly more sophisticated thing would be to use a value function estimate that is estimated in a more clever way down just averaging rollouts."
        ],
        [
            "OK, now let's break it down more and look at the structure.",
            "We have.",
            "The first equation shows us what we had.",
            "We broke it down into steps.",
            "We introduced our baseline.",
            "Now look at this last part here.",
            "That's the sum of all rewards experienced, and this is multiplied in with the grad log probability of actions no matter where they are.",
            "Where they are along a trajectory, but it's clear that an action at some time T only influences the reward that comes after time T. So it's not meaningful to incorporate.",
            "Rewards from before time T into your grand lock probability of UTI given St.",
            "So we can do that.",
            "You can also do formal math for this and what you get is then this expression here where what's changed that we just sum over future times, not over the past times when we multiply with grad log probability of action given state at time T. This will also reduce variance quite a bit.",
            "OK."
        ],
        [
            "So that's the basic likely ratio policy gradient calculation.",
            "Then of course, once you have a gradient, you need to do something with it.",
            "Need to take a step.",
            "Improve your policy, hopefully thanks to that step and then repeat."
        ],
        [
            "So.",
            "The reason you need step sizing or something like that is because it's just the 1st order approximation.",
            "Not even a good one for that matter, even it was a perfect first order approximation, you still needed step sizing and this is even worse."
        ],
        [
            "Ann said it be careful that you don't go too far.",
            "In supervised learning, you actually step too far.",
            "It's not too bad.",
            "'cause then the next update will correct for it.",
            "And reinforcement is actually quite different and we earlier some questions about the destabilization and so forth, and some of these learning curves.",
            "And that's exactly the issue here.",
            "Reinforcement is step too far.",
            "You get a terrible policy.",
            "Consequences at the next mini batch you use or the next set of samples he uses collected under this terrible policy.",
            "Which is an issue 'cause now you don't get any signal in signal anymore.",
            "An best you can do is actually just go back to what you had before, which effectively means shrinking your step size.",
            "There's no automatic correction mechanism like you have in supervised learning where the data is anyway there and will correct you in the future.",
            "Here 'cause this destabilization effect once you get to about policy, the signal will disappear, and that's a problem."
        ],
        [
            "So simple step sizing, which would be I have a great interaction I.",
            "Going to decide how far step in that direction.",
            "It's simple, that's nice, but expensive because you need along that line.",
            "Evaluate how good your step is, so you might have to do rollouts.",
            "They might be stochastic, you might have too many rollouts.",
            "Understand how good that step is, an that's not great, it also.",
            "Ignores that you might be able to get more information from the rollouts.",
            "You already did.",
            "That is maybe can get more than just first order approximations out of it.",
            "And once you have a high order approximation that can tell you where your first order approximation is valid and help you restrict your step.",
            "So let's look at something called trust regions.",
            "The idea here is that."
        ],
        [
            "A trust region characterizes where your approximation is good.",
            "Once you have that, you can then within the trust region look for the best point according to maybe your first order approximation, but within the trust region.",
            "What the best spot is to be?",
            "It's a bit like a Newton method.",
            "Newton Newton's method can be seen as a trust region where you have the Hessian as telling you where you can rely your first order approximation and you need to stay within some bound around your current point, which is defined by a metric given by your Hessian.",
            "But it doesn't need to be hashing or other metrics.",
            "You can use too.",
            "For reinforcement learning, it makes sense to look at this metric here.",
            "So this metric here is saying so.",
            "This is a first order approximation.",
            "We're looking at a change Delta Theta first approximation, of course, would just say go to Infinity in the direction of the gradient.",
            "That's not what we want to do.",
            "This is saying we're going to look within a region.",
            "Of small KL divergences, from what we had before.",
            "So it's a current parameter Theta would considering change to new parameter vector Theta plus Delta Theta.",
            "An we want that the distribution over trajectories after the change is close to the distribution over trajectories before the change.",
            "If we can ensure that, then.",
            "A few things are great about that one.",
            "The 1st order approximation is.",
            "Is probably pretty good if that epsilon is small enough.",
            "The other thing is that this whole effect of.",
            "Changes in your policy resulting in complete different set of states being visited and now what you learn not being valid anymore is explicitly being avoided here, because saying you need to keep visiting the same states roughly an average.",
            "So."
        ],
        [
            "Our problem is the following.",
            "We want to solve this optimization problem at the top.",
            "Let's look at the details Khaldeh vergence.",
            "How do we compute that?",
            "Let's breakdown probability of trajectory into the components.",
            "Then let's write out the KL using that equation there.",
            "So probably objectives product of probabilities of action given state and state given previous state and action.",
            "Fill that in here.",
            "Something magical happens again.",
            "If you look at this equation.",
            "The dynamics here cancels out again.",
            "And we have an expression that only involves.",
            "The policy behind the log in front of it.",
            "There's still dynamics, but the thing in front that's an expectation that we can evaluate that based on samples, and So what we get is the following expression here that we evaluate from samples.",
            "We average over to States and actions that we visited.",
            "This thing over here, which is the conditional Cal between the distribution for the policy we had before and the one that we have after question there.",
            "The metric measure, right?",
            "So if you're defining basically involves of Delta distance away from your current parameter.",
            "OK, so it's a good question.",
            "A natural thing would maybe to say let's use.",
            "Oh, what, why?",
            "Why do we use the kelda versions given that is a asymmetric?",
            "Way of measuring distance and maybe that's not great.",
            "They measured in that asymmetric way.",
            "One way to symmetrize it would be to say we're going to sum the calibrations that work in both directions and then work with that, and that could be an interesting thing to do.",
            "What's interesting about using it this way is the way the math works out.",
            "As you see, the way the math works out is that you can evaluate this based on samples.",
            "And only need access to.",
            "Your policy you don't need access to your dynamics.",
            "Short of the samples you collected, if you were to evaluate this the other way, you'd have to also collect data under your Theta plus Delta Theta.",
            "But you're optimizing a Delta.",
            "Theta should have to re collect samples every time you're considering a different choice of Delta, Theta, and so this direction is the one that's cheap to evaluate, has downsides to it in that.",
            "Essentially, it makes it only more locally valid that asymmetry will will hurt you if you go further away.",
            "It just turned out that the 2nd order approximation of distinct is symmetric and will work with the 2nd order approximation of this, and so in that sense we actually do have a symmetric measure that we use in practice, but as expressed here, it's asymmetric indeed.",
            "The way the Cal.",
            "The way the ultimately, I mean this is future work, I don't have anybody who's actually analyzes enough detail, but the way it will hurt you that your asymmetric is that.",
            "If you were to set up the other way around.",
            "The state you visit under your new policy will play a role in how you evaluate this and here only the state you visit on your current policy, player role and so you lose a little bit there.",
            "It's not clear how much you lose, but you definitely lose something.",
            "Russia.",
            "Natural Bridge it will be on the next slide.",
            "Yes, once we make it a second order approximation rather than the full expression.",
            "I think it is already.",
            "So the question is, is it already natural gradient?",
            "They will be?",
            "Yeah, I mean.",
            "Yes.",
            "Could you say what the question is?",
            "Open observation.",
            "OK, so the observation is that locally it behaves extremely symmetrically.",
            "Only when you go further away, it becomes asymmetric.",
            "Thank you.",
            "Yes, there is an analog to what we do when we train your network with gradient clipping.",
            "So here like effectively Raquel is telling you move your parameters in some within this fall and need to create clipping which also says you are not allowed to move outside this fall except when you do pretty and clipping.",
            "Is this your estate based on the norm instead of the KL distance?",
            "I don't know if you have any intuitions how.",
            "Yeah, that's an interesting that's interesting.",
            "The point here is that gradient clipping seems quite related to this in terms of limiting.",
            "How much you are able to update things based on the current data you got.",
            "I think you're absolutely right, and in fact with NRL there's a recent result from Raymond Moonos, an collaborators that look sad, similar idea when you look at importance sampling, and you use importance reweighting in this calculations.",
            "That he shows that if you clip the importance reweighting at one and you can never go above 1.",
            "That you can get certain guarantees that are that at least nobody is able to prove otherwise yet, so I think there are very close connections absolutely."
        ],
        [
            "So here's the problem.",
            "We ended up with.",
            "We wanted to scale to be small.",
            "We saw that it can be an empirical KL on conditional action given state for States and actions that we visited who take the signature approximation.",
            "It'll look like this with the thing in the middle being the Fisher matrix.",
            "And this is easily evaluated from the gradient field already computing.",
            "So this essentially the natural gradient set up more or less."
        ],
        [
            "So this is what we have practice.",
            "We have first order approximation for the objective, then second order approximation to KL, which can be computed from what we already compute for the gradient.",
            "If the constraint is moved to the objective, you recover the natural policy gradient methods that people have proposed several years ago.",
            "Sham Kakade, Drew Bagnell and Jeff Schneider, Jan Peters and Steven shall.",
            "It turns out that it's actually more beneficial to keep it as a constraint in practice, and this has to do with the way if you change your policy, the samples you collect in the future will drastically change.",
            "Potentially, you can the destabilizing effect by putting a constraint there.",
            "It's easier to tune that it's not much more expensive to have a constraint rather than putting it in the objective by having a constraint, all we need to do is a LaGrange multiplier in front of this thing to put it into the objective, and they have one LaGrange multiplier in which you do with dual gradient descent to tune it to get your constraint satisfied.",
            "Approximately satisfied doesn't need to be perfectly satisfied, but close enough to get the right kind of stepsize.",
            "So the fact that the constraint works better is an empirical finding.",
            "Yes, the theoretical finding is actually that if you have it in the objective, you can prove certain relationships between that new objective and the actual objective, and you can prove him major addition, minor isation type setup, where it actually suggests that you want to have any objective.",
            "'cause now we have M like algorithm structure.",
            "They have a lower lower bound that you're optimizing, but in practice it's a lot easier to get it to work with a constraint than with putting it in the objective.",
            "I don't understand how it makes sense.",
            "We talk about open strain when you only have a stochastic estimator of the constraint.",
            "How do you actually constrain when you just have some random variable?",
            "So we want the expectation to be smaller than epsilon of this random variable an the random variable.",
            "The randomness is F data.",
            "Which is we estimate from samples, an Delta Theta will be.",
            "The variables we get to change.",
            "So the question here is how do we deal with is the fact that this is random in practice, should probably set this explicitly in practice.",
            "If data is computed, but also there is a holdout set.",
            "And you can.",
            "You can also keep track on the holdout set, how you're doing on that constraint, even though you're directly optimizing on training data, you can check on the holdout data that you're actually doing well, and then likely doing well on the test data.",
            "In fact, for a lot of these things, maybe it's not always totally obvious, but in RL in a lot of the updates that you do, let's say you now evaluate this optimization problem.",
            "Same is true for once you take a step you kind of want to hold out set.",
            "To evaluate how good that step actually was to help you validate that you're doing well rather than just using the robots in one batch as training data to fuel this optimization for each parameter update, you have to check the value on this holdout set is not very expensive.",
            "You already testing it on the training on the training data you collected, so you might do let's say.",
            "500 or 1000 rollouts an you keep a few of them out and don't use them and then can check on those few that you held out to efficiently check if you're indeed doing well.",
            "Only do it once in awhile, not every parameter update you can do it every parameter update essentially would just drink your step size if it seems on the whole data you're doing quite poorly.",
            "It would shrink your step size further.",
            "Question.",
            "Send people.",
            "So relative entry policy search tries to.",
            "Constrain the distribution distribution over trajectories.",
            "Which is a little less clear.",
            "How to do that exactly?",
            "Anne."
        ],
        [
            "A lot of the work with our EPS is then constraining it to looking at linear dynamical systems or constraint settings to make that work out.",
            "And it's really nice if you can explicitly look at full trajectory sprouted on what we did here.",
            "In this case, if your step size are small enough it actually it kind of works out fine 'cause the dynamics cancels in reps I believe, but don't quote me on this.",
            "That essentially you don't rely on the dynamics canceling, but you rely on other properties.",
            "Certain properties of the dynamics that still make it practical to compute this.",
            "And so when it's applicable, that's nice."
        ],
        [
            "I don't really get why it's different to put it in there."
        ],
        [
            "Functional significance constraint showing you gave the same answer, which is like the emergency share time degrading or so the question is what's different about having as a cost function versus a constraint?",
            "It's a great question, so the difference is the following.",
            "If you put it in the cost function you would have.",
            "G transpose, Delta Theta plus Lambda, the LaGrange multiplier Times Delta Theta transpose, F Theta, Delta Theta.",
            "And your choice of Lambda will determine what you end up with.",
            "Isn't that determined by epsilon?",
            "Yes, and so if you run.",
            "If you think of determining Lambda cleverly based on epsilon.",
            "That's what's dual dual gradient to send us.",
            "It'll put it in your objective.",
            "Have a Lambda there an.",
            "Tune that Lambda to match a certain epsilon so it's not just.",
            "No epsilon is fixed.",
            "There's a relationship between Epsilon and Lambda, the LaGrange multiplier and epsilon have a one to one relation.",
            "But it's not easy to know what that relation is.",
            "Ann, it's so you can add or picture Lambda.",
            "That's a number number.",
            "Or you can pick your epsilon.",
            "That's a number.",
            "It turns out in practice a lot easier to pick epsilon and pick a number for that that's stable throughout your entire optimization as a good epsilon.",
            "To use them to pick a good Lambda to use throughout your entire optimization and dual grain to send what it will do is it will compute the Lambda that matches the epsilon of your choice, so it will do what you want to do, except the Lambda will be automatically tuned to whatever epsilon you chose.",
            "But I don't understand isn't this equivalent to having this?",
            "What term up there with an epsilon?",
            "Maybe we should take this one offline, but I'm happy to talk more about it.",
            "Yes.",
            "If you do this.",
            "Have like 1 drill variable for every.",
            "No, well there's only one constraint.",
            "So the question is how many constraints this counts as one constraint.",
            "So one dual variable.",
            "Say it again.",
            "Or the interpretation of the dual variable.",
            "Not sure as how much you need to penalize for KL divergent to ensure that it stays smaller than epsilon.",
            "Yes.",
            "Please speak louder."
        ],
        [
            "OK, so the question is, are we really enforcing the Cal constraint if we're looking at the 2nd order approximation and the answer is no because it's just an approximation so not guaranteed to enforce that constraint.",
            "However, if epsilon is relatively small.",
            "Then the 2nd order approximation will be quite good, and so it'll be close to enforced.",
            "One thing to keep in mind is that we actually don't really need a very particular epsilon constraint.",
            "I mean essentially what's going on here, so we're trying to ensure that we don't step too far and now we understand where we can trust our gradient and that epsilon is a tweaking parameter, right?",
            "That's a parameter that you set.",
            "To get this to work well, and so if we're not perfectly satisfying for some particular epsilon.",
            "Well, maybe then it turns out when we treat that parameter, even though with the real Cal Epsilon should be 0.012 with this approximate scale our tweaking result will say 0.01 or something instead and so empirically is found that tweaking this epsilon is actually quite easy to do with this particular approximation, and it works quite well, but it's not the case that there's a guarantee on the Cal just because the 2nd order is enforced.",
            "See."
        ],
        [
            "Are we done or can we do more?",
            "Let's see deeper L data will be high dimensional and building an inverting that Fisher matrix like should be impractical because he might have 100,000 million parameters in neural net.",
            "So it turns out that there was a trick you can play with constant gradient to approximate that.",
            "Details in the paper, but that doesn't make it practical to do this on high dimensional policies.",
            "Can we do even better?",
            "So now we can do even better, so we just have a first order approximation of the objective.",
            "There it turns out with the calculation done to get that first order approximation, you can instead define a surrogate loss function.",
            "Which is inspired by the important sampling equations, which is a better approximation than the 1st order approximation and for the same cost you can use that and then do a better local search against that surrogate loss.",
            "Thing with the surrogate loss because it's nonlinear.",
            "You'd have to recompute grandson, and so forth, which might be annoying and time consuming and practice.",
            "What we do is we use the surrogate loss as the objective, but we evaluate, evaluate, rather than evaluate the grain.",
            "So we compute a great interaction.",
            "Search along the direction of that well, based on the Fisher Angie, we get a direction we.",
            "We find the right Lambda.",
            "We know what that direction is along that direction.",
            "Then we line search based on the surrogate loss rather than the 1st order loss."
        ],
        [
            "Here are some results that you can obtain this way, So what you see here is a simulator called majokko.",
            "Majorca was built by Emoto, drove at the University of Washington.",
            "We see here is policies learned after the learning has finished.",
            "So these are different dynamical systems running this like policy gradient with a surrogate loss with the trust region for the policy updates and stable to learn a wide range of policies, different policies for different characters.",
            "Here we see the learning in action.",
            "Initially it will just fall over because it doesn't know.",
            "What to do?",
            "It's optimizing.",
            "How far forward it gets.",
            "The further forward, the better.",
            "It's also in the reward.",
            "There's also impact with the ground.",
            "Less impact is better, and there's also a penalty for how much torque you apply to each of the joints.",
            "There's nothing in there that says what walking looks like.",
            "What kind of motion it should generate, so it's a pretty noninformative reward about the behavior that you're supposed to get out.",
            "Nevertheless, it's able to actually learn that quite well.",
            "For."
        ],
        [
            "Get some learning curves doing this the way I described actually helps the more difficult problems with Carlos relatively easy.",
            "A lot of approaches will work well once it's more difficult.",
            "Like swimmer, it stands out more as a better approach."
        ],
        [
            "Were compared with other.",
            "Policy optimization methods.",
            "Hoppers even more difficult walkers even more difficult than the difference, becomes more and more pronounced in terms of performance.",
            "Here it's interesting is that.",
            "This, for example this epsilon for kelda vergence we can get it set it to 0.0 one across all problems that we run, and so we don't even need to tweak it on a particular problem.",
            "It's a very general hyperparameter that works across a wide range of problems, which is nice 'cause often the way.",
            "You have to go about these things is do a lot of hyperparameter tuning before.",
            "Finally something works.",
            "You get a new problem.",
            "You have to redo it and so forth.",
            "And this way of formulating is.",
            "This avoids that."
        ],
        [
            "Now she also works for the Atari Games.",
            "Obviously that our games the initial results were all done with the QM, but this policy optimization works for Atari 2, where it's instead of.",
            "Continuous actions as we had further simulated robots here we have discrete actions.",
            "Works just fine.",
            "In fact, I'll get back to this later, maybe this afternoon, but the best deep mind results in that area are also actually with the policy gradient method right now, not with EQ anymore.",
            "Something called a 3C.",
            "OK."
        ],
        [
            "So.",
            "We've seen policy gradient methods.",
            "Now let's start looking at how we can bring in value function estimation to make this better."
        ],
        [
            "So this is our likelihood ratio policy gradient estimator from samples.",
            "We can evaluate this and keep in mind what's in.",
            "There is some of rewards future awards after the current time minus value of the current state.",
            "I mean doesn't have to be value, it can be any baseline, but value tends to be a really good choice.",
            "'cause then you compare what you achieved with respect to what you achieve.",
            "An average under your current policy, so it would increase probability of good actions, decrease of bad actions.",
            "Now if we look at this quantity over here, sum of future rewards, what is that?",
            "You see this quantity this morning.",
            "Essentially your Q value.",
            "If your current policy from that state.",
            "Taking that current action so it's a sample based evaluation, single sample evaluation of the Q value.",
            "So it's going to be very approximate.",
            "There's no reason we should use that single sample estimate.",
            "Probably 3.",
            "Replace it with some other estimate that doesn't suffer from the high variance you get from having just a simple single sample.",
            "We also have no generalization from rollout to roll out if you some kind of function approximator that can help, what will do is will reduce variance on this by introducing discounting and by introducing function approximation.",
            "So this is interesting.",
            "Often when people use discounting they consider it part of the definition of the MDP.",
            "We're not going to do that here, so we're interested in maximizing expected sum of rewards over Horizon H. But we're going to think of discounting as a tweaking parameter that will help us reduce variance on things we estimate for the thing we're really trying to solve."
        ],
        [
            "So original key estimate from 1 sample would look like the top line.",
            "An alternative estimate looks like this.",
            "It's not an expectation.",
            "Going to be correct.",
            "Because we're discounting future things, an that's in reality, not what we care about.",
            "But what happens is that.",
            "The longer this rollout is, the higher variance, and by discounting we counted is a little bit so that we don't have increasing variance as much with the length of the rollout.",
            "So this is just a variance reduction trick here.",
            "It's not changing the problem definition, we still want to solve.",
            "There's no problem, just reducing variance actually makes a big difference."
        ],
        [
            "Another thing that we can look at is we look at this.",
            "The first line is what we looked at so far.",
            "The second line is another way of writing this expected sum of rewards is the same as expected reward at the first time plus gamma times value, which is expected sum of rewards, then onwards.",
            "Or we can write it this way.",
            "Or we can write it this way and so forth.",
            "There's many ways of writing this expectation.",
            "If we happen to have a way of estimating a value function, we can use any one of those.",
            "OK.",
            "So in general is advantage estimation.",
            "What you use is an exponentially weighted average of those, and this of course dates back to TD, Lambda and eligibility traces.",
            "Word is also being used for value function learning, but so this is the idea here is that we can combine these WHI would you want to do this?",
            "It seems like if you had a good value function.",
            "Clearly the one R0 plus gamma times the value would be the one that you want to use.",
            "But the truth is, in practice, you don't typically have an amazing value function.",
            "Have a reasonable value function, but not amazing.",
            "Amazing an because you introduced the reasonable value function.",
            "What will happen is that this will become biased.",
            "It's not unbiased anymore.",
            "That value function uses bias and so yes, we want to use it to reduce variance.",
            "But we also want to make sure that we stay somewhat close to the unbiased estimate and hence the tradeoff between.",
            "The two and then the tradeoff can be done in a very gradual way.",
            "Doesn't have to be just the first 2 lines being average.",
            "We can average all possible lengths of rollouts capped off with value function.",
            "Here's an example."
        ],
        [
            "Experiment.",
            "Well you look at here is Lambda on the horizontal axis, gamma on the vertical axis.",
            "Remember the thing being optimized doesn't have a gamma.",
            "The gamma is just used as a variance reduction.",
            "What you see here is as a function of Lambda and gamma.",
            "How well this policy gradient method will do, and we see that there is clearly a best there in the middle where gamma is about 0.98 and Lambda is 0.92300 point 98.",
            "That's the range where this works best.",
            "So actually it's interesting to go back and see what that means so large."
        ],
        [
            "That works pretty well."
        ],
        [
            "Trashy suggested.",
            "Last time that means you're willing to look pretty far, actually, and so it's actually in this particular problem.",
            "It prefers the sum of rewards over it, bringing too much of the value function estimate, of course, is going to depend a lot on how many rollouts you do.",
            "The more rollouts you have, the better that sum of rewards will be.",
            "The less rollouts you have, the more you'll want to depend on the value function that Lambda will shift depends also where you are in training like these better, so that's a good question.",
            "Would it also depends on where you are in training."
        ],
        [
            "I think it would.",
            "I haven't seen such experiment, but I imagine that as V gets better, if you had any kind of estimate of how good your vius, you would be willing to trust it more.",
            "At the same time, once you're ready doing very well, your updates will be small, and maybe you actually don't want to use because you want your fully unbiased estimate to get the final little bit.",
            "I'm not sure how it would play out.",
            "Sure, well.",
            "OK.",
            "Provide awareness.",
            "That shows.",
            "That gamma becoming bigger overtime is better ending to the true value of the test value of gamma.",
            "As you accumulate more data and he has some theoretical analysis of theoretical characterize, this is the recent paper.",
            "Yeah, so there's an interesting paper in setting things group that looks at model based RL largely, but essentially what they look at their.",
            "They look at all.",
            "So what you can do by changing gamma and why you might want to change gamma and the idea there is the following.",
            "Let's say you do model based RL, you have an approximate model.",
            "You then plan in that approximate model.",
            "You won't get the perfect plan because the model is only approximate.",
            "An because of compounding error overtime under dynamics model the further in the future you look, the less accurate you're planning against those futures and so discounting could ensure that you don't come up with some totally amazing plans seemingly at the very last step gets you to gigantic reward, but actually the model.",
            "The model only allows you to that, but the real world doesn't let you get there and have a really nice analysis.",
            "Think.",
            "I think it's really intriguing there is.",
            "They have a theorem that says.",
            "That says we can measure if you're given.",
            "An MVP.",
            "We can.",
            "Measure.",
            "How many as a function?",
            "So if you give it an MVP and let's say you were allowed to change the reward function in the MDP.",
            "As you change the reward function, you will get different optimal policy's.",
            "Anne.",
            "Then the larger discount factor.",
            "So even in the we can change the reward function every changing reward function might result in a new optimal policy or not.",
            "The longer your horizon.",
            "Demoor optimal policies exist as you vary your reward function and so.",
            "The longer your horizon, the more likely you can overfit to something because the space of policies are going to be choosing from is larger 'cause there was more policy that potentially be optimal on this longer horizon, and so that's a really nice vertical guarantee.",
            "Thanks for pointing out well."
        ],
        [
            "Here is what happens in 3D, so without this generalized advantage estimation actually the precision policy position doesn't work in 3D or not within the amount of time corresponding to our patients of running experiments.",
            "But with all of this machinery, this actually works quite well.",
            "Initially, of course it falls over because it's a random policy, but overtime it gets better and better and better.",
            "Again, the reward function is very simple.",
            "Further North, the better, less impact with the ground, better than some penalty for amount of torque at each of the joints.",
            "Nothing in there about what motion looks like, which is often done to generate simulated motions using motion capture system and then tried to match that.",
            "That's not what's done here.",
            "You can use exact same thing for a different robot.",
            "This again in the Majokko simulator built by a motor off at University of Washington and without any change, just a different robot put in front of it.",
            "It's able to learn.",
            "How to get as far forward as possible, which is something quite interesting that happens here is that it finds a gate that is totally unrealistic for the real world.",
            "But it's very, very fast in the simulator.",
            "So Arrel could be used as a way of debugging simulators.",
            "Henry Ward is whether the head is at standing height or not, then measuring distance from standing height.",
            "I'm just going."
        ],
        [
            "Contrast this for fun with what happened at the DARPA Robotics Challenge just a year ago, by no means claiming this is the best thing you could have seen if you were there, but it's definitely some of the things you would have seen if you were there.",
            "Suggesting that this is a hard problem to get to work, even if there are some solutions out there that work at some situations."
        ],
        [
            "I want to now type this into a 3C.",
            "The async Advantage Actor critic, so not too long ago it was a paper led by Vladimir NPI at Deep Mind.",
            "Looking at asynchronous deep reinforcement learning where he wants to run it on as many machines as possible to see that can speed things up.",
            "The paper had Q learning in it distributed Q learning as well as policy gradient.",
            "That's what I'm going to see if you look at the learning curves.",
            "The yellow is the policy grand method.",
            "The other curves are the Q learning methods.",
            "You can see that actually posterior method ends up doing better than the other methods.",
            "What they used is.",
            "The generalized advantage systemation so the value function is being estimated.",
            "That in turn is being used to estimate the Q function and then that Q -- V gives you the advantage what's multiplied with your grad log probability.",
            "That's and of course run over multiple multiple machines.",
            "Here to scale it up, they don't use the trust region part.",
            "The translation part is Second order, so it's it's a little trickier to run that on multiple machines and get it to work.",
            "I'm not saying it's impossible, but.",
            "That's probably one of the reasons people haven't used a trust region over multiple machines at this point, it's a good challenge to try out for you.",
            "See here that actually works really well on a wide range of Atari games too."
        ],
        [
            "OK, after 12:30, is that right?",
            "OK, so with 20 minutes left to do the second half of our bullet points.",
            "Entirely true, because we also have half an hour this afternoon and also these are a lot shorter actually.",
            "OK, then I'll leave the last two bullet points for the afternoon and justice to these two."
        ],
        [
            "Path relatives we've been looking at gradient based policy optimization.",
            "Let's look at what this actually looks like in a graph.",
            "You have state at Time Zero state at time one state at time one is a function of state of time zero through and also have the action through some function F which is the dynamics model which in general is going to be stochastic.",
            "And.",
            "There's a reward as a function of state.",
            "It can also depend on action, but for simplicity let's assume it just depends on state here.",
            "Your state affects the action.",
            "You take computation graph corresponding to the problem we've been solving.",
            "If you look at it this way.",
            "Kind of say, well, why are we computing gradients in this kind of funny likelihood ratio way?",
            "We know when we have graphs like this you can just back property back, propagate through it, get gradients that way.",
            "Computing grains that way.",
            "That's the path derivative method of computing gradients."
        ],
        [
            "It's a different way of computing gradients.",
            "Let's look at that now, so.",
            "Can we do this?",
            "Let's say we had a current rollout.",
            "Which tells us the quantities of each of these nodes as the forward pass in this network can be followed up by a backdrop pass, well, why not?",
            "Let's see derivative of U with respect to Theta I.",
            "Each of the entries in Theta is some of their respective of each of the terms R with respect to our depends on South then as depends on Theta chain rule.",
            "There we need an answer respect to Theta which as depends on Theta through.",
            "The previous state.",
            "And the dynamics, as well as through the action and the dynamics.",
            "Again, just a chain rule and then action depends on Theta through the policy.",
            "So this is just general, not nothing special.",
            "There you can do this.",
            "You can do a backward pass compute this.",
            "This assumes that you can actually evaluate all these quantities.",
            "So let's assume for now.",
            "F is known, the dynamics is known, F is deterministic.",
            "Then we can definitely do this, and we can just back prop and optimize our policy that way.",
            "It will use the dynamics model, which is probably a good thing if we have.",
            "It will probably give you more accurate gradients than the likelihood ratio method.",
            "It does depend a little bit on these derivatives existing, so there are more assumptions here.",
            "The differences we assume are is differentiable, which we did assume before before you could have our reward.",
            "That's only good when you're at the goal and then drops to zero everywhere else.",
            "Things like that that's not going to work here.",
            "It assumes F is known as soon as it is deterministic.",
            "As you progress, here are will remain differentiable.",
            "I'm not going to be able to get rid of that, maybe somebody can, but I'm not able to in this lies at least, but we'll see if we can relax these assumptions here.",
            "So first one F is known.",
            "You're collecting data.",
            "You could use the data to build a model.",
            "You could do a supervised learning from ST&U T2ST plus one that's F, so you could in principle in addition to doing this policy optimizations, training model, plug it in here.",
            "Do this.",
            "This doesn't have to worry about the accuracy of your model in compounding errors, and you need to think about that, but F being known is not a super strong assumption necessarily.",
            "There's fixes to that.",
            "How about it being deterministic?",
            "Let's look at a simple."
        ],
        [
            "I swear it's quite intuitive how to fix this.",
            "Let's say S at time T + 1 is a function of ST&UT plus some noise WT.",
            "OK. Then if we have a simple sample trajectory, we can actually back solve for the values of W. We do the rollout.",
            "We can see what our States and actions were.",
            "Assuming we know F, we can see what the differences between S T + 1, an FSD and UTI.",
            "That's WT.",
            "We actually know that once we know WT.",
            "We actually know the dynamics.",
            "Everything is known now.",
            "It's actually not stochastic.",
            "It's slightly over fitting to the rollout you just did, but that's what it is.",
            "It's just one sample trajectory, or you can apply the exact same thing again.",
            "So that's nice even for stochastic, specially in the simple structure.",
            "You can reply to same idea, of course, with a single roll out.",
            "Be careful 'cause he will be overfitting to the sequence of noise that you experience.",
            "You need more than one roll out to do this well, but it's possible more generally."
        ],
        [
            "Call the pre parameterization trick which will do it is for you if it's applicable, original thing would be S. T + 1 is a stochastic function of ST&UT an Theta the.",
            "The parameters that you're optimizing.",
            "Re parameterized, and it's not always possible to do this, but often possible to this.",
            "You have a new function of deterministic, where there's a new variable their size stochastic.",
            "That's a new auxiliary variable you introduce that captures all the noise.",
            "OK. And I don't have time to go into the details of how to make that work in general, but let's look at a simple example.",
            "Let's say your next date is a Gaussian centered around some G function of STT and Theta with some variance in my squared.",
            "Now what this would look like is SD plus one is that G function plus Sigma times sigh and all the randomness is factored out in the Sky over here.",
            "And whenever you can factor out the randomness, you can make this work.",
            "In fact, if you look."
        ],
        [
            "But in terms of."
        ],
        [
            "This computation graph here effectively means is that you're introducing extra nodes for each side variable that are pointing into that state at the next time.",
            "Making that function F deterministic function of PSI and STNUT."
        ],
        [
            "So as I said, the later things have less detail on the earlier things.",
            "In the interest of time."
        ],
        [
            "There's actually can work quite well.",
            "You do need the continuous value.",
            "The output of after the computer.",
            "Yes, for this, so there's a lot of assumptions here that are not needed for the likelihood ratio.",
            "You need your actions to be continuous valued.",
            "Otherwise you can take derivatives through actions you need your states to be continuous valued.",
            "You need your award to be differentiable so a lot of extra assumptions need to be made.",
            "Maybe someone can be alleviated by embedding discrete things into continuous spaces.",
            "Who knows, I'm not sure, but the other thing is you use the forward more distant forward.",
            "More differentiation, which could be very expensive.",
            "Computed the dimension of data and S is large.",
            "Or you could also."
        ],
        [
            "Background yes, yes.",
            "So the question here is when we look at these equations.",
            "That's how the forward propagation, forward mode automatic differentiation works.",
            "We can just as well do it with the backdrop.",
            "That's yeah.",
            "I mean, that's also the way would be done.",
            "This is just to illustrate what derivatives are needed.",
            "An essentially also to your first point that.",
            "For these derivatives to exist, it actually S has to be continuous.",
            "Actions has to have to be continuous, and so forth.",
            "Otherwise you can't do this.",
            "But you wouldn't compute it this way.",
            "You would back prop to compute, which does dynamic programming over the things you compute."
        ],
        [
            "This idea is used in the.",
            "Stochastic value gradients paper by.",
            "Make his and collaborators so they essentially look at exactly this setup that I just described an backdrop through it to compute their gradients.",
            "You can make variants of this because when you look at this, if you back perhaps through and you have this sum of rewards that you take derivatives off, you can play the exact same idea again, some of rewards that skew values myself into Q value separately.",
            "If the Q value is differentiable with respect to state in action, I can just directly take derivatives of the Q values rather than of the future rewards.",
            "And so if you do that, you get the domestic policy grant or the deep deterministic policy grain.",
            "If you use a deep network to fit the Q values and so that those ideas are in those papers but essentially.",
            "The reparameterization trick applied to policy optimization.",
            "Including also the ideas of introducing Q value fitting to not have a sum over all future rewards, but having fits instead.",
            "An if it's applicable.",
            "This actually learns very quickly because you use a lot more information than using the likely ratio."
        ],
        [
            "This way to generalize this."
        ],
        [
            "This calculation don't want to get into the details, but essentially you can mix and match any kind of like liberation path derivative computation 1.",
            "Interesting thing is that if you have a neural network big neural network that stochastic and you have black box nodes in their nodes where you don't have access to compute derivatives on that node.",
            "By putting a stochastic node in front of it, you can they use the likelihood ratio grand on that stochastic node that sits in front of it.",
            "Exactly what we did when we want to do policy grants with no access to dynamics.",
            "That exact same idea is much more widely applicable than just to reinforcement learning.",
            "Another thing is that once you think of it this way, you don't need to re derive things in any special way.",
            "If you have recurrent policy.",
            "CEO wasn't policy grand for a recurrent policy sold the same thing you just set up your stochastic computation graph some.",
            "Knows you compute the likelihood ratio.",
            "Grandmother knows you compute the path derivative, whatever seems right for that node, and you have gradients for any kind of policy architecture that you might want to use, yes.",
            "High variance on like every system.",
            "So the question is, would you pay a price having high variance?",
            "And the answer is yes, you use the likely ratio estimated.",
            "You tend to have higher variance.",
            "So typically what you would do is you would analyze your stochastic computation graph.",
            "Add a corresponding to your RL problem or to something else.",
            "You would see where is my re parameterisation trick applicable?",
            "Whenever you can apply it, you probably apply it, but then some cases will not be applicable.",
            "Let's say you have a real dynamical system and for some reason you're not building a model.",
            "Whatever it might be only or you want to grant on this one.",
            "Run on the real system and you're not building a model then that's the only way to do it.",
            "It could be.",
            "There are some special cases that you can set up where the path relative is higher variance than likely ratio.",
            "All the examples I've seen are very artificial, but they involve things like you know, function that's like sign 1 / X, which gets like very oscillatory near 0.",
            "In fact, infinite oscillations near 0.",
            "Then if you do path derivative, it'll have the derivatives will just be going up and down, up and down, up and down.",
            "It will be really tricky to deal with.",
            "That likely ratio will have lower variance because the bounded magnitude of the actual.",
            "Function value, so there is this.",
            "Some scenarios like that, but obviously assign 1 / X is not what we typically encounter in this kind of computations.",
            "But maybe there are cases where something similar happens.",
            "Essentially, liquid ratio will do better than path derivative if there is very high variance on the derivatives, but it's less typical for that to be the case in the function.",
            "Bundle variation, yes.",
            "Bombing variation and high variance on derivatives will make likely ratio lower variance than path derivative, but.",
            "I don't know any realistic scenarios."
        ],
        [
            "OK, so last thing I want to do is say something about.",
            "Well, we've seen a lot of methods at this point an it can be very easy to get a headache with all these methods and think there's too many of them."
        ],
        [
            "So what we did is.",
            "Put up a table to clarify the headache.",
            "So we see here there is a set of methods that that's readily applicable to continuous control tasks.",
            "On the entire demand has been a lot of comparisons of methods that have discrete action spaces.",
            "This specifically for continuous control tasks.",
            "A comparison of the different methods that exist.",
            "It suggests that in this comparison, like which methods tend to work better, it seems like the natural grand idea which is made even more kind of explicit and more precise in the RPO setup tends to do best the.",
            "Deep domestic policy grant, which does the path derivative, does quite well too in many situations.",
            "Well, harder to get to work, so that's why some of these entries are empty there.",
            "I think there's some interesting questions there.",
            "If the GDP can be made more stable, what the result would be there?"
        ],
        [
            "You don't want to implement them yourself or build something called our lab, which makes it easy to experiment with them.",
            "It's not just.",
            "Here's a snippet of code that is this algorithm.",
            "It's also it will output information to allow you to make graphs more easily, have multiple runs, hyperparameter settings, and so forth.",
            "So it's really like a lab software infrastructure around each of these algorithms."
        ],
        [
            "Interfaces with the Open AI gym so you can pull in a lot of environments into it very easily and see then how well this you're going to do against other algorithms.",
            "Or you just want to compare existing algorithms.",
            "So I think we should take questions at this point."
        ],
        [
            "And then in the afternoon, I'll look at the two lost topics, guided policy search and inverse RL.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this morning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far we've been looking here at reinforcement learning, and this is a picture I'm borrowing from Joelle slides.",
                    "label": 0
                },
                {
                    "sent": "There's a dynamical system which has some state your learning agent might have access to that state.",
                    "label": 0
                },
                {
                    "sent": "Then, based on that decide to take an action might also get reward along the way, and then this goes around in the feedback loop.",
                    "label": 0
                },
                {
                    "sent": "And there's many ways of trying to solve the problem of finding a good policy for the agent.",
                    "label": 0
                },
                {
                    "sent": "What we're going to look at in this session is direct policy optimization.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we assume there's some policy Pi Theta.",
                    "label": 0
                },
                {
                    "sent": "This could be some kind of neural net.",
                    "label": 0
                },
                {
                    "sent": "There could be something else to this policy Maps from state or observation.",
                    "label": 0
                },
                {
                    "sent": "ULL inputs through some calculations to then a set of a vector of actions or potentially a distribution over actions.",
                    "label": 0
                },
                {
                    "sent": "So take a look at what this form.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That means we now have an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize expected sum of rewards accumulated overtime under policy Pi Theta, where the policy Pi Theta is what we optimize over.",
                    "label": 0
                },
                {
                    "sent": "Typically the policy Pi Theta is chosen to be stochastic.",
                    "label": 0
                },
                {
                    "sent": "The reason that's done is because it smooths out the optimization problem because small changes in a stochastic policy will result in a small change in expected return.",
                    "label": 0
                },
                {
                    "sent": "And now I have a nice optimization problem to work with.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would you do policy optimization rather than something else?",
                    "label": 0
                },
                {
                    "sent": "Often the policy can be simpler to learn and to represent then a value function or Q function.",
                    "label": 0
                },
                {
                    "sent": "For example, let's say a robot needs to graph something like a bottle of water.",
                    "label": 0
                },
                {
                    "sent": "All we need to do is get close and close its fingers and that's it.",
                    "label": 0
                },
                {
                    "sent": "And there might be a very simple policy.",
                    "label": 0
                },
                {
                    "sent": "Closing might be very simple to represent, but understanding exactly how much value there is associated with closing your fingers and exactly how you close them and so forth might be much harder to learn.",
                    "label": 0
                },
                {
                    "sent": "And so by directly looking at the policy and the policy only might be easier to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "If instead you decide to learn value functions rather than a policy, one issues.",
                    "label": 0
                },
                {
                    "sent": "That value function doesn't describe actions for you.",
                    "label": 0
                },
                {
                    "sent": "Once you have a value function to then act, you still need to do something.",
                    "label": 0
                },
                {
                    "sent": "For example, you need to do one step.",
                    "label": 0
                },
                {
                    "sent": "Look at, say, if I were to take any of my set of actions, what would happen?",
                    "label": 0
                },
                {
                    "sent": "Or what's the distribution over things that could happen?",
                    "label": 0
                },
                {
                    "sent": "Then evaluate the value at the next state as well as reward along the way and see which action is best.",
                    "label": 0
                },
                {
                    "sent": "You could say, well academically learn Q values throughout and values that often works well.",
                    "label": 0
                },
                {
                    "sent": "We've seen some examples this morning where it gets tricky is if your action space is very large.",
                    "label": 0
                },
                {
                    "sent": "For example, a continuous action space or just very high dimensional discrete action spaces.",
                    "label": 0
                },
                {
                    "sent": "Because in the optimization argmax overall actions of qsa can be very tricky.",
                    "label": 0
                },
                {
                    "sent": "Not saying it's impossible, it's just not super clear how to make that work yet.",
                    "label": 0
                },
                {
                    "sent": "Another reason you might want to study policy optimizations because there's been a lot of success stories.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the top line represents some of the older success stories, nothing deep.",
                    "label": 0
                },
                {
                    "sent": "And what happened there?",
                    "label": 0
                },
                {
                    "sent": "You're showing what would be easier to choose an action if you're doing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly policy optimization.",
                    "label": 0
                },
                {
                    "sent": "Rather than learning Qi mean you also have to pick out of many possible actions.",
                    "label": 0
                },
                {
                    "sent": "So, so let's say, let's make this little concrete.",
                    "label": 0
                },
                {
                    "sent": "Let's make this concrete.",
                    "label": 0
                },
                {
                    "sent": "So if it's discrete.",
                    "label": 0
                },
                {
                    "sent": "And you're able to output.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you neural net can output AQ value for each possible action, then there's no issue.",
                    "label": 0
                },
                {
                    "sent": "'cause then you can just see which one is the highest.",
                    "label": 0
                },
                {
                    "sent": "But once you move to high dimensional, let's say continuous action spaces, you try to represent AQ value for every possible state action combination.",
                    "label": 0
                },
                {
                    "sent": "You usually don't.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how to have an output corresponding to every possible action you might take, and so now effectively maybe becomes the input to your neural net.",
                    "label": 0
                },
                {
                    "sent": "You have to actually propagate all the way back to infer which actions maximize this.",
                    "label": 0
                },
                {
                    "sent": "Thinking about the high dimensional discrete space where it seems that you have the same problem in both cases, whether you learn Q.",
                    "label": 0
                },
                {
                    "sent": "Are you learning policy using the case?",
                    "label": 0
                },
                {
                    "sent": "Let's see if you have a high dimensional discrete case.",
                    "label": 0
                },
                {
                    "sent": "For discrete case in general you have to.",
                    "label": 0
                },
                {
                    "sent": "You so, so?",
                    "label": 0
                },
                {
                    "sent": "There are a few things you can do.",
                    "label": 0
                },
                {
                    "sent": "That might still be a little easier with policy representation, so let's say you have a discrete case and there's a bunch of actions we have to choose at any given time.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're playing multiple Atari games in parallel, or multiple players at the same time you're controlling them all, and they're all discrete actions, so a natural thing to set up, let's see, would be that if you have a policy, you would probably set it up.",
                    "label": 0
                },
                {
                    "sent": "From state to action of the first agent, then from state and action on the 1st agent to action for the second agent, assuming they can control them all.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as a joint decision rather than simultaneously, in which case it doesn't.",
                    "label": 0
                },
                {
                    "sent": "All doesn't matter, so would be one way to set it up for a policy for a Q function.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "You could probably set it up so you'd have to set up your Q function to be decomposed on the output.",
                    "label": 0
                },
                {
                    "sent": "In some way that it's easy to recover which combination of actions does best, and there are some tricks people have played.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a paper by I think it's shangu Sergei, 11, illicit skipper, Ann.",
                    "label": 0
                },
                {
                    "sent": "One other author I'm blanking on, Tim Lillicrap.",
                    "label": 0
                },
                {
                    "sent": "I think that looks at setting up the Q function so it is very easy to compute the argmax.",
                    "label": 0
                },
                {
                    "sent": "So by design, the neural net is set up so that it's easy to compute the argmax, and they do it by setting up a quadratic essentially.",
                    "label": 0
                },
                {
                    "sent": "Attic function in the actions.",
                    "label": 0
                },
                {
                    "sent": "And since it's a quadratic function, the actions is easy to find the bottom, so there are definitely tricks people.",
                    "label": 0
                },
                {
                    "sent": "Are playing.",
                    "label": 0
                },
                {
                    "sent": "It's not super clear how to do it in generality, whereas if you go from state to action as a policy you can use pretty much any mapping.",
                    "label": 0
                },
                {
                    "sent": "You're less constrained in terms of how you set it up.",
                    "label": 0
                },
                {
                    "sent": "Another thing to think about is, I mean this will come back later as you're actually building these things is that there are things you can do to combine both.",
                    "label": 0
                },
                {
                    "sent": "You can have multiple networks, so you can imagine that you say, well, it's hard to compute this argmax could set up another network that computes the argmax, just like you might have an inference, an encoding network, and so you do the same thing here.",
                    "label": 0
                },
                {
                    "sent": "And if you look at something like GPG domestic policy gradient, it does something a little bit like that in general.",
                    "label": 0
                },
                {
                    "sent": "Actor critic methods will do something a little bit like that where there is 2 networks and one is the policy network, the other one the value network and they work together.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Top row here are some success stories in legged locomotion helicopter flight, another leg locomotion wanan pulling a Cup game where you have to swim football in a string and get into a Cup and they were all successfully trained on kind of very small representations.",
                    "label": 0
                },
                {
                    "sent": "Nothing deep there, but policy optimization methods were able to succeed at these tasks where the policy was actually encoded with relatively small number of parameters and what's changed recently is that is becoming possible to run these methods with policy representations with very large numbers of parameters.",
                    "label": 0
                },
                {
                    "sent": "And some example success stories are at the bottom.",
                    "label": 0
                },
                {
                    "sent": "For example in Atari in Majokko, various simulated environments of dynamical systems and also even on real robots, and will look at mostly the bottom line in this presentation here.",
                    "label": 0
                },
                {
                    "sent": "But keep in mind that there's been success stories before.",
                    "label": 0
                },
                {
                    "sent": "Often, though constrained to small numbers of parameters that are being optimized.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you all showed before, you can categorize RL methods in this way where there's policy optimization methods and there are dynamic programming methods and actually the actor critic methods bring both of them together where you have both a value function Anna policy and you train them both at the same time.",
                    "label": 0
                },
                {
                    "sent": "In my presentation, we're going to focus on the left side of this and ending up at the actor critic methods at the end.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ideas I want to get across.",
                    "label": 0
                },
                {
                    "sent": "There are three methods, likely ratio policy gradients, natural gradients, and trust regions.",
                    "label": 0
                },
                {
                    "sent": "Actor critic path derivatives at different way of computing derivatives, then the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "A generalization of that through stochastic competition graphs.",
                    "label": 0
                },
                {
                    "sent": "Guided policy search and inverse RL.",
                    "label": 0
                },
                {
                    "sent": "The last two are probably for this afternoon and you'll see that there will be a lot of detail in the early ones and will dig into the math and really understand what's going on and for the later ones due to lack of time will go a little quicker and there will be referenced what the ideas are, but won't dig as deep.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with derivative free methods.",
                    "label": 0
                },
                {
                    "sent": "If you want to optimize the policy.",
                    "label": 0
                },
                {
                    "sent": "Simplest thing to do?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say, well, let's say I'm interested in maximizing my utility under some policy Pi Theta, it's maximizing expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "Well, just see this as a black box.",
                    "label": 0
                },
                {
                    "sent": "You have a policy, you run it.",
                    "label": 0
                },
                {
                    "sent": "You see how much reward you get.",
                    "label": 0
                },
                {
                    "sent": "You could change the parameters, run it again, see how much you get and just not even look inside what's going on.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about the fact that this is a reinforcement learning problem, is just an optimization problem maximizing utility with respect to parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "So we're going to ignore all other information collected.",
                    "label": 0
                },
                {
                    "sent": "This cross entropy, that which the one we're looking at here is an evolutionary algorithm means it keeps track of a distribution over possible solutions, which is represented by \u03c0 mu I sumu indexes into the type of distribution is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You could just be the mean or mu could be mean and variance.",
                    "label": 0
                },
                {
                    "sent": "An I hear correspond to the iteration of our population and then Theta is the space of which the distribution lives.",
                    "label": 0
                },
                {
                    "sent": "Here's what cross entropy looks like.",
                    "label": 0
                },
                {
                    "sent": "You iterate, you initialize some house with some distribution over the parameters.",
                    "label": 0
                },
                {
                    "sent": "Then you start iterating.",
                    "label": 0
                },
                {
                    "sent": "You sample population members from your current distribution.",
                    "label": 0
                },
                {
                    "sent": "So you sample published Member Theta E From a distribution P mu I Theta.",
                    "label": 0
                },
                {
                    "sent": "Then you use that parameter vector to execute a bunch of times.",
                    "label": 0
                },
                {
                    "sent": "You see how well you do collect those pairs data an utility?",
                    "label": 0
                },
                {
                    "sent": "Once you've done that for a few parameter settings data, you check which ones achieves the achieve the highest score.",
                    "label": 0
                },
                {
                    "sent": "So you say here are my top 10% of parameters.",
                    "label": 0
                },
                {
                    "sent": "Those survive quantum code an.",
                    "label": 0
                },
                {
                    "sent": "Then you maximize the log probability of those population members.",
                    "label": 0
                },
                {
                    "sent": "And you maximize this over the choice of your parameters in your distribution you.",
                    "label": 0
                },
                {
                    "sent": "This gives this.",
                    "label": 0
                },
                {
                    "sent": "If it's a Gaussian, will shift your Gaussian to where the good samples were, and then you repeat gets new set of samples and keep going.",
                    "label": 0
                },
                {
                    "sent": "No where do we look inside the box.",
                    "label": 0
                },
                {
                    "sent": "Ugly black box optimization method.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually works embarrassingly well.",
                    "label": 0
                },
                {
                    "sent": "Here's a table from a paper from not too long ago, an essentially what it shows is that so this is on the game of Tetris, which is a standard RL benchmark.",
                    "label": 0
                },
                {
                    "sent": "It's a game where you have blocks coming down.",
                    "label": 0
                },
                {
                    "sent": "You need to play some of Arrow is full, it clears and you can keep letting blocks come down.",
                    "label": 0
                },
                {
                    "sent": "It's been a benchmark NRL ever since people started doing these things, for example, typically burzik as often in their book references benchmark.",
                    "label": 0
                },
                {
                    "sent": "And traditionally solved with value function methods.",
                    "label": 0
                },
                {
                    "sent": "It turns out this cross entropy thing can outdo this value function methods, even though it's so simple, doesn't even look at anything underneath.",
                    "label": 0
                },
                {
                    "sent": "Now she took till 2013 when a paper came out.",
                    "label": 0
                },
                {
                    "sent": "Approximate dynamic programming finally performs well in the game of Tetris, where these value function methods could finally compete with cross entropy.",
                    "label": 0
                },
                {
                    "sent": "It just took a lot of work, even though cross entropy is so simple, so it's.",
                    "label": 0
                },
                {
                    "sent": "It's not just in Tetris that this has been the case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, in graphics, a lot of the graphics animations rely on a cross entropy like method CMA, and so it's a very, very powerful method.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly powerful method.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a caveat, though.",
                    "label": 0
                },
                {
                    "sent": "It tends to work best when the number of parameters is relatively small.",
                    "label": 0
                },
                {
                    "sent": "In the touches example, the Canonical parameterisation of Tetris that was being used unlock the Atari things we've looked at is to go from the board to a predefined 20 dimensional feature vector, and then run your algorithm on how you're going to wait those different features in your value functions.",
                    "label": 0
                },
                {
                    "sent": "They're only finding 20 parameters, and So what?",
                    "label": 0
                },
                {
                    "sent": "You did value function methods in that literature, or cross entropy.",
                    "label": 0
                },
                {
                    "sent": "It would always only find 20 parameters general for this kind of algorithms you need.",
                    "label": 0
                },
                {
                    "sent": "The number of samples from your population to be roughly comperable to the number of dimensions in your parameter vector.",
                    "label": 0
                },
                {
                    "sent": "If you can do that, then this can work pretty well if you work at a neural net million parameters that you need a lot of samples to make this practical.",
                    "label": 0
                },
                {
                    "sent": "Still very implement and it's a great baseline to compare with you.",
                    "label": 0
                },
                {
                    "sent": "At least know have some notion of how well your actual our algorithm works relative to some basic baseline.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So something else you can do just completely ignoring the inside structure of the problem is just to find out differences.",
                    "label": 0
                },
                {
                    "sent": "You could say I have a function that want to optimize it that is expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the parameter vector, perturb it, see what the local derivative is, compute the entire gradient, make an update, and repeat.",
                    "label": 0
                },
                {
                    "sent": "Some challenges there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're doing these rollouts, typically there stochastic this can be because of the dynamics because of your policy being stochastic, both could be at play and so it could be the function that looks like this.",
                    "label": 0
                },
                {
                    "sent": "An when you evaluate the derivative based on two samples, you actually get something that sloped exactly the wrong way.",
                    "label": 0
                },
                {
                    "sent": "Fixes to this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, he could take a lot of samples and it will average out.",
                    "label": 0
                },
                {
                    "sent": "There won't be a problem, but that is often what you want to avoid.",
                    "label": 0
                },
                {
                    "sent": "You want to learn from a relatively small number of samples if possible.",
                    "label": 0
                },
                {
                    "sent": "Not a fix you can make at times is to fix.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The random seed.",
                    "label": 0
                },
                {
                    "sent": "So as you execute your policy.",
                    "label": 0
                },
                {
                    "sent": "You execute another policy.",
                    "label": 0
                },
                {
                    "sent": "Different setting of the parameter vector.",
                    "label": 0
                },
                {
                    "sent": "You keep the random seed the same, and so the randomness will interact with your system the same way and this will give you more consistent computation of derivatives and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dennis could be in policy and in dynamics in the policy you can control it when you sample actions given state, you can totally control how you do that in the dynamics.",
                    "label": 0
                },
                {
                    "sent": "It might be harder to control, it can give issues.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're flying a helicopter.",
                    "label": 0
                },
                {
                    "sent": "You want it to be a good policy to try a different policy.",
                    "label": 0
                },
                {
                    "sent": "Now the wind conditions are different.",
                    "label": 0
                },
                {
                    "sent": "If it's more wind gusts here, it's going to be harder to do well an.",
                    "label": 0
                },
                {
                    "sent": "You can just look at the outcome of one run and decide that oh, the second parameter setting was worse.",
                    "label": 0
                },
                {
                    "sent": "'cause it did more poorly, 'cause it might have been caused by this randomness in the environment.",
                    "label": 0
                },
                {
                    "sent": "There's some trickiness here in getting this to work now.",
                    "label": 0
                },
                {
                    "sent": "Some nice theoretical guarantees, though if you can fix the random seed, how does make things more efficient than if you can't fix it?",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at a result that was obtained.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this with UK, yeah, if you had a generative model of the wind you would be OK.",
                    "label": 0
                },
                {
                    "sent": "Please bring me one someday.",
                    "label": 0
                },
                {
                    "sent": "So what we'll see here is a helicopter controlled by control policy that was found through direct policy search.",
                    "label": 1
                },
                {
                    "sent": "Running things as a black box.",
                    "label": 0
                },
                {
                    "sent": "So there is nothing looking inside when the rewards are accumulated is just a bunch of parameters that are being optimized by repeated trial and error and looking at the total reward attained using the trick of fixing the random seed, the learning is happening in simulation.",
                    "label": 0
                },
                {
                    "sent": "And then the test that we see here is on the real helicopter.",
                    "label": 0
                },
                {
                    "sent": "But the reason the random seed could be fixes all the learning was in simulation.",
                    "label": 1
                },
                {
                    "sent": "So this is helicopter reliably hovering.",
                    "label": 0
                },
                {
                    "sent": "This actually hard problem.",
                    "label": 0
                },
                {
                    "sent": "So stabilizing helicopter.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a balancing along broom.",
                    "label": 0
                },
                {
                    "sent": "The palm of your hand.",
                    "label": 0
                },
                {
                    "sent": "And then you need to constantly look at that broom to balance it and make sure it doesn't fall over.",
                    "label": 0
                },
                {
                    "sent": "Same thing here for helicopter and as someone amazing cinematography here where we rotate the camera by 180 in the middle of this video, exposing that essentially flying upside down.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Underneath",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Policy here was relatively simple, just 12 parameters.",
                    "label": 0
                },
                {
                    "sent": "If that's the number of parameters, again these kind of methods can work pretty well.",
                    "label": 0
                },
                {
                    "sent": "You don't need necessarily much more than these basic methods.",
                    "label": 0
                },
                {
                    "sent": "But now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start digging into higher dimensional policies and see what we can do there.",
                    "label": 0
                },
                {
                    "sent": "What's this likely ratio policy gradient?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some notation.",
                    "label": 0
                },
                {
                    "sent": "Tile will be a sequence of States and actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so stay at a time 0 action U at time zero and so forth will work with finite horizon.",
                    "label": 0
                },
                {
                    "sent": "Just for simplicity of notation and doing the math.",
                    "label": 0
                },
                {
                    "sent": "But these things carry over if you do more intricate math to infinite horizons and average rather than total reward.",
                    "label": 0
                },
                {
                    "sent": "We try to optimize expected sum of rewards, which we're going now for now, right as some over possible trajectories, probability of directory under the policy Pi Theta times reward return from that trajectory.",
                    "label": 0
                },
                {
                    "sent": "So initially we're again going to ignore the fact there's a sequence of States and actions which is going to look at it as distributions over trajectories and used by the policy later will break it up into the details of States and actions.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what we want to compute.",
                    "label": 0
                },
                {
                    "sent": "We want to compute the gradient of our utility function with respect to the parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "So gradient perspective data of some overall trajectory's probability of trajectory times reward of that trajectory.",
                    "label": 0
                },
                {
                    "sent": "We can move that gradient inside the summation.",
                    "label": 0
                },
                {
                    "sent": "And now we can play a trick which we play because we have some foresight of how it is going to play out.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to see that this is OK. We multiply and divide by the same thing, but it might not be obvious why we're doing this at this point.",
                    "label": 0
                },
                {
                    "sent": "Once we do that.",
                    "label": 0
                },
                {
                    "sent": "We remember that the gradient of a log is the grand of that thing divided by the thing, and we use that in reverse to get.",
                    "label": 0
                },
                {
                    "sent": "Look at this get this expression over here.",
                    "label": 0
                },
                {
                    "sent": "Now it becomes clear why we play this trick, 'cause if we look at this, we actually have an expectation.",
                    "label": 0
                },
                {
                    "sent": "Again, we see that the gradient can be computed by taking an expectation with respect to the distribution that we're working with, distribution over trajectories.",
                    "label": 0
                },
                {
                    "sent": "So that's why we did this and we can then evaluate this based on samples.",
                    "label": 0
                },
                {
                    "sent": "So we can say we have our current distribution that we get from executing our policy Pi.",
                    "label": 0
                },
                {
                    "sent": "Theta will get some samples an averaged over the samples, evaluate this quantity over here which gives us this expression over here.",
                    "label": 0
                },
                {
                    "sent": "For interesting, the expectation of this is the exact gradient, but also from just a single sample.",
                    "label": 0
                },
                {
                    "sent": "Actually we can get.",
                    "label": 0
                },
                {
                    "sent": "A quantity that an expectation will be equal to the gradient.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be very accurate from 1 sample, but an expectation that will be correct.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to drive.",
                    "label": 0
                },
                {
                    "sent": "There's a different way too, which is kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "Maybe makes it look a little less like magic.",
                    "label": 0
                },
                {
                    "sent": "Important sampling the important sampling is that you can evaluate an expectation under distribution.",
                    "label": 0
                },
                {
                    "sent": "Theta by actually collecting samples under a different distribution parameterized by Theta old and the way you do it is you have samples under Theta old and then you re weight them by P Theta over Theta old.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first equation says this essentially infinite sample case.",
                    "label": 0
                },
                {
                    "sent": "That utility can be evaluated as an expectation or an old data.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "We can then take the gradient of that expression.",
                    "label": 0
                },
                {
                    "sent": "We can then.",
                    "label": 0
                },
                {
                    "sent": "Bring in the gradient inside the expectation.",
                    "label": 0
                },
                {
                    "sent": "We can then evaluate this expression at Theta old.",
                    "label": 0
                },
                {
                    "sent": "And then again, gradient of log is granted.",
                    "label": 0
                },
                {
                    "sent": "The thing divided by that thing, and so this is what we get and we get the same expression again that we have an expectation respect to some distribution, Theta old of the grad log, probability under that distribution times reward.",
                    "label": 0
                },
                {
                    "sent": "So here the ratio comes in because of important sampling.",
                    "label": 0
                },
                {
                    "sent": "What's interesting is that this also allows you to see that you can do more than just computing gradients.",
                    "label": 0
                },
                {
                    "sent": "This way you can actually evaluate.",
                    "label": 0
                },
                {
                    "sent": "From just a few runs under your policy Theta old, you can evaluate what the policy will be like in neighboring parts of the space.",
                    "label": 0
                },
                {
                    "sent": "Of course, the more data is different from the old, the higher the variance on this estimate.",
                    "label": 0
                },
                {
                    "sent": "So you can go super far, but at least you have an estimate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's interesting is that this is valid even if your function R is discontinuous.",
                    "label": 0
                },
                {
                    "sent": "You have discrete state actions and so forth, so this is really interesting in that you have no relies on continuity of your reward function here.",
                    "label": 0
                },
                {
                    "sent": "We assume that the log probabilities has derivatives, but.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't assume anything on the objective.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some into it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm behind this equation here, so what's going on?",
                    "label": 0
                },
                {
                    "sent": "We have a policy Pi Theta.",
                    "label": 0
                },
                {
                    "sent": "We do rollouts under that policy.",
                    "label": 0
                },
                {
                    "sent": "And then compute this Gray and estimate if we look at it says grad log probability of path I.",
                    "label": 0
                },
                {
                    "sent": "Times reward under path I.",
                    "label": 0
                },
                {
                    "sent": "So this grain is saying we're trying to maximize utility here is going to say we're going to increase the probability of paths with a positive R and decrease probability of paths with negative are.",
                    "label": 0
                },
                {
                    "sent": "Important to keep in mind that it actually never tries to change the paths so it's never looking at those paths and saying if you just had budged a little bit this way or that way you would have been a better path.",
                    "label": 0
                },
                {
                    "sent": "For that you need the kind of differentiability that exactly we're not assuming you need a different kind of derivative, will look at later.",
                    "label": 0
                },
                {
                    "sent": "This doesn't do that, it just looks at each of the paths as they were and puts more probability mass on the good ones, lessen the bad ones.",
                    "label": 0
                },
                {
                    "sent": "In fact, a lot like the cross entropy method, just using gradients rather than what was formulated there.",
                    "label": 0
                },
                {
                    "sent": "Now let's break this down into pieces.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the probability of a path is the product of the probabilities of each of the transitions.",
                    "label": 0
                },
                {
                    "sent": "So we have probability of initial state, which I'm ignoring here.",
                    "label": 0
                },
                {
                    "sent": "And then there is probability of next state given current state and action times probability of action given state with dynamics and policy that participate in this and then multiplied over all times.",
                    "label": 0
                },
                {
                    "sent": "Grad log probability log of a product.",
                    "label": 0
                },
                {
                    "sent": "Some of the logs will get this.",
                    "label": 0
                },
                {
                    "sent": "Then look at.",
                    "label": 0
                },
                {
                    "sent": "The two sides, the first one actually does not have any Theta in it, so actually disappears.",
                    "label": 0
                },
                {
                    "sent": "So it's very interesting that when we compute the grad log probability here.",
                    "label": 0
                },
                {
                    "sent": "Dynamics disappear.",
                    "label": 0
                },
                {
                    "sent": "Completely not present in this equation.",
                    "label": 0
                },
                {
                    "sent": "So this is what it plays out as his final equation, and so we can evaluate this grad log probability.",
                    "label": 0
                },
                {
                    "sent": "Without having a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "You should also be worried 'cause you said well, if you don't use your dynamics model, how informative can you gradient really be?",
                    "label": 0
                },
                {
                    "sent": "And again, we'll get back to that later, but it's nice that you don't need it, and often that's quite helpful.",
                    "label": 0
                },
                {
                    "sent": "'cause often you don't have a dynamics model readily available.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here is a summary of what we have so far.",
                    "label": 0
                },
                {
                    "sent": "We're going in these likely ratio policy gradient methods.",
                    "label": 0
                },
                {
                    "sent": "We're going to compute a policy gradient estimate by the 1st equation.",
                    "label": 0
                },
                {
                    "sent": "The grad log probabilities times reward where the ground log probabilities of passes breakdown into just some of grad log probabilities of action given state at each time.",
                    "label": 0
                },
                {
                    "sent": "An an expectation this is correct.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expectation correct, but very noisy, so I'm going to look at a few tricks to make this less noisy.",
                    "label": 0
                },
                {
                    "sent": "Reduce the variance on this estimate.",
                    "label": 0
                },
                {
                    "sent": "Of course you can just get infinitely many samples.",
                    "label": 0
                },
                {
                    "sent": "That will also make it a good estimate, but try to get good estimates with small number of samples.",
                    "label": 0
                },
                {
                    "sent": "First idea is going to be baselines.",
                    "label": 0
                },
                {
                    "sent": "Next one is going to be exploiting temporal structure, and then we'll look at trust regions.",
                    "label": 0
                },
                {
                    "sent": "Step sizing to make this even better.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is our equation again.",
                    "label": 0
                },
                {
                    "sent": "Turns out I want to build some intuition.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We can look at what is the reward is always positive so.",
                    "label": 0
                },
                {
                    "sent": "Reward, no matter what you do, is positive.",
                    "label": 0
                },
                {
                    "sent": "But of course there's some things you do to reward is much more positive than for other things.",
                    "label": 0
                },
                {
                    "sent": "This equation is saying.",
                    "label": 0
                },
                {
                    "sent": "You want to increase the probability of everything you've experienced so far.",
                    "label": 0
                },
                {
                    "sent": "Even the not so great things that had relatively low reward but still positive.",
                    "label": 0
                },
                {
                    "sent": "More so for the things that were very positive, but still it's a very weird thing that whatever you did recently, you want to increase the probability of what you did.",
                    "label": 0
                },
                {
                    "sent": "So essentially you get a lot of fighting 'cause they're not so great things will improve, increase their probability and they'll need a lot of really great things to later be experienced to kind of out.",
                    "label": 0
                },
                {
                    "sent": "Do that.",
                    "label": 0
                },
                {
                    "sent": "You increase those probabilities and it'll be a lot of back and forth before I get anywhere.",
                    "label": 0
                },
                {
                    "sent": "OK, so we could consider introducing a baseline.",
                    "label": 0
                },
                {
                    "sent": "What you do there is you subtract out from the reward some baseline B what this is saying is that maybe have some estimate of what is the average that you might achieve.",
                    "label": 0
                },
                {
                    "sent": "And then what this becomes increase the probability of things that are better than average, decrease the probability of things that are worse than average should always do it.",
                    "label": 0
                },
                {
                    "sent": "I mean, without a baseline really this thing will never work.",
                    "label": 0
                },
                {
                    "sent": "Once you do a baseline at least you have that aspect in place.",
                    "label": 0
                },
                {
                    "sent": "So this is still an unbiased estimate.",
                    "label": 0
                },
                {
                    "sent": "It turns out if you do the math, the expected value of this thing is still equal to the actual gradient, but it will reduce the variance if you choose a good baseline.",
                    "label": 0
                },
                {
                    "sent": "What's a good choice for B?",
                    "label": 0
                },
                {
                    "sent": "A very simple thing would be just on average how much have you gotten from your past rollouts.",
                    "label": 0
                },
                {
                    "sent": "A slightly more sophisticated thing would be to use a value function estimate that is estimated in a more clever way down just averaging rollouts.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now let's break it down more and look at the structure.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "The first equation shows us what we had.",
                    "label": 0
                },
                {
                    "sent": "We broke it down into steps.",
                    "label": 0
                },
                {
                    "sent": "We introduced our baseline.",
                    "label": 0
                },
                {
                    "sent": "Now look at this last part here.",
                    "label": 0
                },
                {
                    "sent": "That's the sum of all rewards experienced, and this is multiplied in with the grad log probability of actions no matter where they are.",
                    "label": 0
                },
                {
                    "sent": "Where they are along a trajectory, but it's clear that an action at some time T only influences the reward that comes after time T. So it's not meaningful to incorporate.",
                    "label": 0
                },
                {
                    "sent": "Rewards from before time T into your grand lock probability of UTI given St.",
                    "label": 0
                },
                {
                    "sent": "So we can do that.",
                    "label": 0
                },
                {
                    "sent": "You can also do formal math for this and what you get is then this expression here where what's changed that we just sum over future times, not over the past times when we multiply with grad log probability of action given state at time T. This will also reduce variance quite a bit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the basic likely ratio policy gradient calculation.",
                    "label": 0
                },
                {
                    "sent": "Then of course, once you have a gradient, you need to do something with it.",
                    "label": 0
                },
                {
                    "sent": "Need to take a step.",
                    "label": 0
                },
                {
                    "sent": "Improve your policy, hopefully thanks to that step and then repeat.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The reason you need step sizing or something like that is because it's just the 1st order approximation.",
                    "label": 0
                },
                {
                    "sent": "Not even a good one for that matter, even it was a perfect first order approximation, you still needed step sizing and this is even worse.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann said it be careful that you don't go too far.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning, you actually step too far.",
                    "label": 0
                },
                {
                    "sent": "It's not too bad.",
                    "label": 0
                },
                {
                    "sent": "'cause then the next update will correct for it.",
                    "label": 0
                },
                {
                    "sent": "And reinforcement is actually quite different and we earlier some questions about the destabilization and so forth, and some of these learning curves.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the issue here.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement is step too far.",
                    "label": 0
                },
                {
                    "sent": "You get a terrible policy.",
                    "label": 0
                },
                {
                    "sent": "Consequences at the next mini batch you use or the next set of samples he uses collected under this terrible policy.",
                    "label": 0
                },
                {
                    "sent": "Which is an issue 'cause now you don't get any signal in signal anymore.",
                    "label": 0
                },
                {
                    "sent": "An best you can do is actually just go back to what you had before, which effectively means shrinking your step size.",
                    "label": 0
                },
                {
                    "sent": "There's no automatic correction mechanism like you have in supervised learning where the data is anyway there and will correct you in the future.",
                    "label": 0
                },
                {
                    "sent": "Here 'cause this destabilization effect once you get to about policy, the signal will disappear, and that's a problem.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So simple step sizing, which would be I have a great interaction I.",
                    "label": 0
                },
                {
                    "sent": "Going to decide how far step in that direction.",
                    "label": 0
                },
                {
                    "sent": "It's simple, that's nice, but expensive because you need along that line.",
                    "label": 0
                },
                {
                    "sent": "Evaluate how good your step is, so you might have to do rollouts.",
                    "label": 0
                },
                {
                    "sent": "They might be stochastic, you might have too many rollouts.",
                    "label": 0
                },
                {
                    "sent": "Understand how good that step is, an that's not great, it also.",
                    "label": 0
                },
                {
                    "sent": "Ignores that you might be able to get more information from the rollouts.",
                    "label": 0
                },
                {
                    "sent": "You already did.",
                    "label": 0
                },
                {
                    "sent": "That is maybe can get more than just first order approximations out of it.",
                    "label": 0
                },
                {
                    "sent": "And once you have a high order approximation that can tell you where your first order approximation is valid and help you restrict your step.",
                    "label": 0
                },
                {
                    "sent": "So let's look at something called trust regions.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A trust region characterizes where your approximation is good.",
                    "label": 0
                },
                {
                    "sent": "Once you have that, you can then within the trust region look for the best point according to maybe your first order approximation, but within the trust region.",
                    "label": 0
                },
                {
                    "sent": "What the best spot is to be?",
                    "label": 0
                },
                {
                    "sent": "It's a bit like a Newton method.",
                    "label": 0
                },
                {
                    "sent": "Newton Newton's method can be seen as a trust region where you have the Hessian as telling you where you can rely your first order approximation and you need to stay within some bound around your current point, which is defined by a metric given by your Hessian.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't need to be hashing or other metrics.",
                    "label": 0
                },
                {
                    "sent": "You can use too.",
                    "label": 0
                },
                {
                    "sent": "For reinforcement learning, it makes sense to look at this metric here.",
                    "label": 0
                },
                {
                    "sent": "So this metric here is saying so.",
                    "label": 0
                },
                {
                    "sent": "This is a first order approximation.",
                    "label": 0
                },
                {
                    "sent": "We're looking at a change Delta Theta first approximation, of course, would just say go to Infinity in the direction of the gradient.",
                    "label": 0
                },
                {
                    "sent": "That's not what we want to do.",
                    "label": 0
                },
                {
                    "sent": "This is saying we're going to look within a region.",
                    "label": 0
                },
                {
                    "sent": "Of small KL divergences, from what we had before.",
                    "label": 0
                },
                {
                    "sent": "So it's a current parameter Theta would considering change to new parameter vector Theta plus Delta Theta.",
                    "label": 0
                },
                {
                    "sent": "An we want that the distribution over trajectories after the change is close to the distribution over trajectories before the change.",
                    "label": 0
                },
                {
                    "sent": "If we can ensure that, then.",
                    "label": 0
                },
                {
                    "sent": "A few things are great about that one.",
                    "label": 0
                },
                {
                    "sent": "The 1st order approximation is.",
                    "label": 0
                },
                {
                    "sent": "Is probably pretty good if that epsilon is small enough.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that this whole effect of.",
                    "label": 0
                },
                {
                    "sent": "Changes in your policy resulting in complete different set of states being visited and now what you learn not being valid anymore is explicitly being avoided here, because saying you need to keep visiting the same states roughly an average.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our problem is the following.",
                    "label": 0
                },
                {
                    "sent": "We want to solve this optimization problem at the top.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the details Khaldeh vergence.",
                    "label": 0
                },
                {
                    "sent": "How do we compute that?",
                    "label": 0
                },
                {
                    "sent": "Let's breakdown probability of trajectory into the components.",
                    "label": 0
                },
                {
                    "sent": "Then let's write out the KL using that equation there.",
                    "label": 0
                },
                {
                    "sent": "So probably objectives product of probabilities of action given state and state given previous state and action.",
                    "label": 0
                },
                {
                    "sent": "Fill that in here.",
                    "label": 0
                },
                {
                    "sent": "Something magical happens again.",
                    "label": 0
                },
                {
                    "sent": "If you look at this equation.",
                    "label": 0
                },
                {
                    "sent": "The dynamics here cancels out again.",
                    "label": 0
                },
                {
                    "sent": "And we have an expression that only involves.",
                    "label": 0
                },
                {
                    "sent": "The policy behind the log in front of it.",
                    "label": 0
                },
                {
                    "sent": "There's still dynamics, but the thing in front that's an expectation that we can evaluate that based on samples, and So what we get is the following expression here that we evaluate from samples.",
                    "label": 0
                },
                {
                    "sent": "We average over to States and actions that we visited.",
                    "label": 0
                },
                {
                    "sent": "This thing over here, which is the conditional Cal between the distribution for the policy we had before and the one that we have after question there.",
                    "label": 0
                },
                {
                    "sent": "The metric measure, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're defining basically involves of Delta distance away from your current parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a good question.",
                    "label": 0
                },
                {
                    "sent": "A natural thing would maybe to say let's use.",
                    "label": 0
                },
                {
                    "sent": "Oh, what, why?",
                    "label": 0
                },
                {
                    "sent": "Why do we use the kelda versions given that is a asymmetric?",
                    "label": 0
                },
                {
                    "sent": "Way of measuring distance and maybe that's not great.",
                    "label": 0
                },
                {
                    "sent": "They measured in that asymmetric way.",
                    "label": 0
                },
                {
                    "sent": "One way to symmetrize it would be to say we're going to sum the calibrations that work in both directions and then work with that, and that could be an interesting thing to do.",
                    "label": 0
                },
                {
                    "sent": "What's interesting about using it this way is the way the math works out.",
                    "label": 0
                },
                {
                    "sent": "As you see, the way the math works out is that you can evaluate this based on samples.",
                    "label": 0
                },
                {
                    "sent": "And only need access to.",
                    "label": 0
                },
                {
                    "sent": "Your policy you don't need access to your dynamics.",
                    "label": 0
                },
                {
                    "sent": "Short of the samples you collected, if you were to evaluate this the other way, you'd have to also collect data under your Theta plus Delta Theta.",
                    "label": 0
                },
                {
                    "sent": "But you're optimizing a Delta.",
                    "label": 0
                },
                {
                    "sent": "Theta should have to re collect samples every time you're considering a different choice of Delta, Theta, and so this direction is the one that's cheap to evaluate, has downsides to it in that.",
                    "label": 0
                },
                {
                    "sent": "Essentially, it makes it only more locally valid that asymmetry will will hurt you if you go further away.",
                    "label": 0
                },
                {
                    "sent": "It just turned out that the 2nd order approximation of distinct is symmetric and will work with the 2nd order approximation of this, and so in that sense we actually do have a symmetric measure that we use in practice, but as expressed here, it's asymmetric indeed.",
                    "label": 0
                },
                {
                    "sent": "The way the Cal.",
                    "label": 0
                },
                {
                    "sent": "The way the ultimately, I mean this is future work, I don't have anybody who's actually analyzes enough detail, but the way it will hurt you that your asymmetric is that.",
                    "label": 0
                },
                {
                    "sent": "If you were to set up the other way around.",
                    "label": 0
                },
                {
                    "sent": "The state you visit under your new policy will play a role in how you evaluate this and here only the state you visit on your current policy, player role and so you lose a little bit there.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how much you lose, but you definitely lose something.",
                    "label": 0
                },
                {
                    "sent": "Russia.",
                    "label": 0
                },
                {
                    "sent": "Natural Bridge it will be on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Yes, once we make it a second order approximation rather than the full expression.",
                    "label": 0
                },
                {
                    "sent": "I think it is already.",
                    "label": 0
                },
                {
                    "sent": "So the question is, is it already natural gradient?",
                    "label": 0
                },
                {
                    "sent": "They will be?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Could you say what the question is?",
                    "label": 0
                },
                {
                    "sent": "Open observation.",
                    "label": 0
                },
                {
                    "sent": "OK, so the observation is that locally it behaves extremely symmetrically.",
                    "label": 0
                },
                {
                    "sent": "Only when you go further away, it becomes asymmetric.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is an analog to what we do when we train your network with gradient clipping.",
                    "label": 0
                },
                {
                    "sent": "So here like effectively Raquel is telling you move your parameters in some within this fall and need to create clipping which also says you are not allowed to move outside this fall except when you do pretty and clipping.",
                    "label": 0
                },
                {
                    "sent": "Is this your estate based on the norm instead of the KL distance?",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have any intuitions how.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's an interesting that's interesting.",
                    "label": 0
                },
                {
                    "sent": "The point here is that gradient clipping seems quite related to this in terms of limiting.",
                    "label": 0
                },
                {
                    "sent": "How much you are able to update things based on the current data you got.",
                    "label": 0
                },
                {
                    "sent": "I think you're absolutely right, and in fact with NRL there's a recent result from Raymond Moonos, an collaborators that look sad, similar idea when you look at importance sampling, and you use importance reweighting in this calculations.",
                    "label": 0
                },
                {
                    "sent": "That he shows that if you clip the importance reweighting at one and you can never go above 1.",
                    "label": 0
                },
                {
                    "sent": "That you can get certain guarantees that are that at least nobody is able to prove otherwise yet, so I think there are very close connections absolutely.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the problem.",
                    "label": 0
                },
                {
                    "sent": "We ended up with.",
                    "label": 0
                },
                {
                    "sent": "We wanted to scale to be small.",
                    "label": 0
                },
                {
                    "sent": "We saw that it can be an empirical KL on conditional action given state for States and actions that we visited who take the signature approximation.",
                    "label": 0
                },
                {
                    "sent": "It'll look like this with the thing in the middle being the Fisher matrix.",
                    "label": 0
                },
                {
                    "sent": "And this is easily evaluated from the gradient field already computing.",
                    "label": 0
                },
                {
                    "sent": "So this essentially the natural gradient set up more or less.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what we have practice.",
                    "label": 0
                },
                {
                    "sent": "We have first order approximation for the objective, then second order approximation to KL, which can be computed from what we already compute for the gradient.",
                    "label": 0
                },
                {
                    "sent": "If the constraint is moved to the objective, you recover the natural policy gradient methods that people have proposed several years ago.",
                    "label": 0
                },
                {
                    "sent": "Sham Kakade, Drew Bagnell and Jeff Schneider, Jan Peters and Steven shall.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's actually more beneficial to keep it as a constraint in practice, and this has to do with the way if you change your policy, the samples you collect in the future will drastically change.",
                    "label": 0
                },
                {
                    "sent": "Potentially, you can the destabilizing effect by putting a constraint there.",
                    "label": 0
                },
                {
                    "sent": "It's easier to tune that it's not much more expensive to have a constraint rather than putting it in the objective by having a constraint, all we need to do is a LaGrange multiplier in front of this thing to put it into the objective, and they have one LaGrange multiplier in which you do with dual gradient descent to tune it to get your constraint satisfied.",
                    "label": 0
                },
                {
                    "sent": "Approximately satisfied doesn't need to be perfectly satisfied, but close enough to get the right kind of stepsize.",
                    "label": 0
                },
                {
                    "sent": "So the fact that the constraint works better is an empirical finding.",
                    "label": 0
                },
                {
                    "sent": "Yes, the theoretical finding is actually that if you have it in the objective, you can prove certain relationships between that new objective and the actual objective, and you can prove him major addition, minor isation type setup, where it actually suggests that you want to have any objective.",
                    "label": 0
                },
                {
                    "sent": "'cause now we have M like algorithm structure.",
                    "label": 0
                },
                {
                    "sent": "They have a lower lower bound that you're optimizing, but in practice it's a lot easier to get it to work with a constraint than with putting it in the objective.",
                    "label": 0
                },
                {
                    "sent": "I don't understand how it makes sense.",
                    "label": 0
                },
                {
                    "sent": "We talk about open strain when you only have a stochastic estimator of the constraint.",
                    "label": 0
                },
                {
                    "sent": "How do you actually constrain when you just have some random variable?",
                    "label": 0
                },
                {
                    "sent": "So we want the expectation to be smaller than epsilon of this random variable an the random variable.",
                    "label": 0
                },
                {
                    "sent": "The randomness is F data.",
                    "label": 0
                },
                {
                    "sent": "Which is we estimate from samples, an Delta Theta will be.",
                    "label": 0
                },
                {
                    "sent": "The variables we get to change.",
                    "label": 0
                },
                {
                    "sent": "So the question here is how do we deal with is the fact that this is random in practice, should probably set this explicitly in practice.",
                    "label": 0
                },
                {
                    "sent": "If data is computed, but also there is a holdout set.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "You can also keep track on the holdout set, how you're doing on that constraint, even though you're directly optimizing on training data, you can check on the holdout data that you're actually doing well, and then likely doing well on the test data.",
                    "label": 0
                },
                {
                    "sent": "In fact, for a lot of these things, maybe it's not always totally obvious, but in RL in a lot of the updates that you do, let's say you now evaluate this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Same is true for once you take a step you kind of want to hold out set.",
                    "label": 0
                },
                {
                    "sent": "To evaluate how good that step actually was to help you validate that you're doing well rather than just using the robots in one batch as training data to fuel this optimization for each parameter update, you have to check the value on this holdout set is not very expensive.",
                    "label": 0
                },
                {
                    "sent": "You already testing it on the training on the training data you collected, so you might do let's say.",
                    "label": 0
                },
                {
                    "sent": "500 or 1000 rollouts an you keep a few of them out and don't use them and then can check on those few that you held out to efficiently check if you're indeed doing well.",
                    "label": 0
                },
                {
                    "sent": "Only do it once in awhile, not every parameter update you can do it every parameter update essentially would just drink your step size if it seems on the whole data you're doing quite poorly.",
                    "label": 0
                },
                {
                    "sent": "It would shrink your step size further.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Send people.",
                    "label": 0
                },
                {
                    "sent": "So relative entry policy search tries to.",
                    "label": 0
                },
                {
                    "sent": "Constrain the distribution distribution over trajectories.",
                    "label": 0
                },
                {
                    "sent": "Which is a little less clear.",
                    "label": 0
                },
                {
                    "sent": "How to do that exactly?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of the work with our EPS is then constraining it to looking at linear dynamical systems or constraint settings to make that work out.",
                    "label": 0
                },
                {
                    "sent": "And it's really nice if you can explicitly look at full trajectory sprouted on what we did here.",
                    "label": 0
                },
                {
                    "sent": "In this case, if your step size are small enough it actually it kind of works out fine 'cause the dynamics cancels in reps I believe, but don't quote me on this.",
                    "label": 0
                },
                {
                    "sent": "That essentially you don't rely on the dynamics canceling, but you rely on other properties.",
                    "label": 0
                },
                {
                    "sent": "Certain properties of the dynamics that still make it practical to compute this.",
                    "label": 0
                },
                {
                    "sent": "And so when it's applicable, that's nice.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't really get why it's different to put it in there.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functional significance constraint showing you gave the same answer, which is like the emergency share time degrading or so the question is what's different about having as a cost function versus a constraint?",
                    "label": 0
                },
                {
                    "sent": "It's a great question, so the difference is the following.",
                    "label": 0
                },
                {
                    "sent": "If you put it in the cost function you would have.",
                    "label": 0
                },
                {
                    "sent": "G transpose, Delta Theta plus Lambda, the LaGrange multiplier Times Delta Theta transpose, F Theta, Delta Theta.",
                    "label": 0
                },
                {
                    "sent": "And your choice of Lambda will determine what you end up with.",
                    "label": 0
                },
                {
                    "sent": "Isn't that determined by epsilon?",
                    "label": 0
                },
                {
                    "sent": "Yes, and so if you run.",
                    "label": 0
                },
                {
                    "sent": "If you think of determining Lambda cleverly based on epsilon.",
                    "label": 0
                },
                {
                    "sent": "That's what's dual dual gradient to send us.",
                    "label": 0
                },
                {
                    "sent": "It'll put it in your objective.",
                    "label": 0
                },
                {
                    "sent": "Have a Lambda there an.",
                    "label": 0
                },
                {
                    "sent": "Tune that Lambda to match a certain epsilon so it's not just.",
                    "label": 0
                },
                {
                    "sent": "No epsilon is fixed.",
                    "label": 0
                },
                {
                    "sent": "There's a relationship between Epsilon and Lambda, the LaGrange multiplier and epsilon have a one to one relation.",
                    "label": 0
                },
                {
                    "sent": "But it's not easy to know what that relation is.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's so you can add or picture Lambda.",
                    "label": 0
                },
                {
                    "sent": "That's a number number.",
                    "label": 0
                },
                {
                    "sent": "Or you can pick your epsilon.",
                    "label": 0
                },
                {
                    "sent": "That's a number.",
                    "label": 0
                },
                {
                    "sent": "It turns out in practice a lot easier to pick epsilon and pick a number for that that's stable throughout your entire optimization as a good epsilon.",
                    "label": 0
                },
                {
                    "sent": "To use them to pick a good Lambda to use throughout your entire optimization and dual grain to send what it will do is it will compute the Lambda that matches the epsilon of your choice, so it will do what you want to do, except the Lambda will be automatically tuned to whatever epsilon you chose.",
                    "label": 0
                },
                {
                    "sent": "But I don't understand isn't this equivalent to having this?",
                    "label": 0
                },
                {
                    "sent": "What term up there with an epsilon?",
                    "label": 0
                },
                {
                    "sent": "Maybe we should take this one offline, but I'm happy to talk more about it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "If you do this.",
                    "label": 0
                },
                {
                    "sent": "Have like 1 drill variable for every.",
                    "label": 0
                },
                {
                    "sent": "No, well there's only one constraint.",
                    "label": 0
                },
                {
                    "sent": "So the question is how many constraints this counts as one constraint.",
                    "label": 0
                },
                {
                    "sent": "So one dual variable.",
                    "label": 0
                },
                {
                    "sent": "Say it again.",
                    "label": 0
                },
                {
                    "sent": "Or the interpretation of the dual variable.",
                    "label": 0
                },
                {
                    "sent": "Not sure as how much you need to penalize for KL divergent to ensure that it stays smaller than epsilon.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Please speak louder.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the question is, are we really enforcing the Cal constraint if we're looking at the 2nd order approximation and the answer is no because it's just an approximation so not guaranteed to enforce that constraint.",
                    "label": 0
                },
                {
                    "sent": "However, if epsilon is relatively small.",
                    "label": 0
                },
                {
                    "sent": "Then the 2nd order approximation will be quite good, and so it'll be close to enforced.",
                    "label": 0
                },
                {
                    "sent": "One thing to keep in mind is that we actually don't really need a very particular epsilon constraint.",
                    "label": 0
                },
                {
                    "sent": "I mean essentially what's going on here, so we're trying to ensure that we don't step too far and now we understand where we can trust our gradient and that epsilon is a tweaking parameter, right?",
                    "label": 0
                },
                {
                    "sent": "That's a parameter that you set.",
                    "label": 0
                },
                {
                    "sent": "To get this to work well, and so if we're not perfectly satisfying for some particular epsilon.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe then it turns out when we treat that parameter, even though with the real Cal Epsilon should be 0.012 with this approximate scale our tweaking result will say 0.01 or something instead and so empirically is found that tweaking this epsilon is actually quite easy to do with this particular approximation, and it works quite well, but it's not the case that there's a guarantee on the Cal just because the 2nd order is enforced.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are we done or can we do more?",
                    "label": 0
                },
                {
                    "sent": "Let's see deeper L data will be high dimensional and building an inverting that Fisher matrix like should be impractical because he might have 100,000 million parameters in neural net.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that there was a trick you can play with constant gradient to approximate that.",
                    "label": 0
                },
                {
                    "sent": "Details in the paper, but that doesn't make it practical to do this on high dimensional policies.",
                    "label": 0
                },
                {
                    "sent": "Can we do even better?",
                    "label": 0
                },
                {
                    "sent": "So now we can do even better, so we just have a first order approximation of the objective.",
                    "label": 0
                },
                {
                    "sent": "There it turns out with the calculation done to get that first order approximation, you can instead define a surrogate loss function.",
                    "label": 0
                },
                {
                    "sent": "Which is inspired by the important sampling equations, which is a better approximation than the 1st order approximation and for the same cost you can use that and then do a better local search against that surrogate loss.",
                    "label": 0
                },
                {
                    "sent": "Thing with the surrogate loss because it's nonlinear.",
                    "label": 0
                },
                {
                    "sent": "You'd have to recompute grandson, and so forth, which might be annoying and time consuming and practice.",
                    "label": 0
                },
                {
                    "sent": "What we do is we use the surrogate loss as the objective, but we evaluate, evaluate, rather than evaluate the grain.",
                    "label": 0
                },
                {
                    "sent": "So we compute a great interaction.",
                    "label": 0
                },
                {
                    "sent": "Search along the direction of that well, based on the Fisher Angie, we get a direction we.",
                    "label": 0
                },
                {
                    "sent": "We find the right Lambda.",
                    "label": 0
                },
                {
                    "sent": "We know what that direction is along that direction.",
                    "label": 0
                },
                {
                    "sent": "Then we line search based on the surrogate loss rather than the 1st order loss.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some results that you can obtain this way, So what you see here is a simulator called majokko.",
                    "label": 0
                },
                {
                    "sent": "Majorca was built by Emoto, drove at the University of Washington.",
                    "label": 0
                },
                {
                    "sent": "We see here is policies learned after the learning has finished.",
                    "label": 0
                },
                {
                    "sent": "So these are different dynamical systems running this like policy gradient with a surrogate loss with the trust region for the policy updates and stable to learn a wide range of policies, different policies for different characters.",
                    "label": 0
                },
                {
                    "sent": "Here we see the learning in action.",
                    "label": 0
                },
                {
                    "sent": "Initially it will just fall over because it doesn't know.",
                    "label": 0
                },
                {
                    "sent": "What to do?",
                    "label": 0
                },
                {
                    "sent": "It's optimizing.",
                    "label": 0
                },
                {
                    "sent": "How far forward it gets.",
                    "label": 0
                },
                {
                    "sent": "The further forward, the better.",
                    "label": 0
                },
                {
                    "sent": "It's also in the reward.",
                    "label": 0
                },
                {
                    "sent": "There's also impact with the ground.",
                    "label": 0
                },
                {
                    "sent": "Less impact is better, and there's also a penalty for how much torque you apply to each of the joints.",
                    "label": 0
                },
                {
                    "sent": "There's nothing in there that says what walking looks like.",
                    "label": 0
                },
                {
                    "sent": "What kind of motion it should generate, so it's a pretty noninformative reward about the behavior that you're supposed to get out.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, it's able to actually learn that quite well.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get some learning curves doing this the way I described actually helps the more difficult problems with Carlos relatively easy.",
                    "label": 0
                },
                {
                    "sent": "A lot of approaches will work well once it's more difficult.",
                    "label": 0
                },
                {
                    "sent": "Like swimmer, it stands out more as a better approach.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Were compared with other.",
                    "label": 0
                },
                {
                    "sent": "Policy optimization methods.",
                    "label": 0
                },
                {
                    "sent": "Hoppers even more difficult walkers even more difficult than the difference, becomes more and more pronounced in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "Here it's interesting is that.",
                    "label": 0
                },
                {
                    "sent": "This, for example this epsilon for kelda vergence we can get it set it to 0.0 one across all problems that we run, and so we don't even need to tweak it on a particular problem.",
                    "label": 0
                },
                {
                    "sent": "It's a very general hyperparameter that works across a wide range of problems, which is nice 'cause often the way.",
                    "label": 0
                },
                {
                    "sent": "You have to go about these things is do a lot of hyperparameter tuning before.",
                    "label": 0
                },
                {
                    "sent": "Finally something works.",
                    "label": 0
                },
                {
                    "sent": "You get a new problem.",
                    "label": 0
                },
                {
                    "sent": "You have to redo it and so forth.",
                    "label": 0
                },
                {
                    "sent": "And this way of formulating is.",
                    "label": 0
                },
                {
                    "sent": "This avoids that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now she also works for the Atari Games.",
                    "label": 0
                },
                {
                    "sent": "Obviously that our games the initial results were all done with the QM, but this policy optimization works for Atari 2, where it's instead of.",
                    "label": 0
                },
                {
                    "sent": "Continuous actions as we had further simulated robots here we have discrete actions.",
                    "label": 0
                },
                {
                    "sent": "Works just fine.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'll get back to this later, maybe this afternoon, but the best deep mind results in that area are also actually with the policy gradient method right now, not with EQ anymore.",
                    "label": 0
                },
                {
                    "sent": "Something called a 3C.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We've seen policy gradient methods.",
                    "label": 0
                },
                {
                    "sent": "Now let's start looking at how we can bring in value function estimation to make this better.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is our likelihood ratio policy gradient estimator from samples.",
                    "label": 0
                },
                {
                    "sent": "We can evaluate this and keep in mind what's in.",
                    "label": 0
                },
                {
                    "sent": "There is some of rewards future awards after the current time minus value of the current state.",
                    "label": 0
                },
                {
                    "sent": "I mean doesn't have to be value, it can be any baseline, but value tends to be a really good choice.",
                    "label": 0
                },
                {
                    "sent": "'cause then you compare what you achieved with respect to what you achieve.",
                    "label": 0
                },
                {
                    "sent": "An average under your current policy, so it would increase probability of good actions, decrease of bad actions.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at this quantity over here, sum of future rewards, what is that?",
                    "label": 0
                },
                {
                    "sent": "You see this quantity this morning.",
                    "label": 0
                },
                {
                    "sent": "Essentially your Q value.",
                    "label": 0
                },
                {
                    "sent": "If your current policy from that state.",
                    "label": 0
                },
                {
                    "sent": "Taking that current action so it's a sample based evaluation, single sample evaluation of the Q value.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be very approximate.",
                    "label": 0
                },
                {
                    "sent": "There's no reason we should use that single sample estimate.",
                    "label": 0
                },
                {
                    "sent": "Probably 3.",
                    "label": 0
                },
                {
                    "sent": "Replace it with some other estimate that doesn't suffer from the high variance you get from having just a simple single sample.",
                    "label": 0
                },
                {
                    "sent": "We also have no generalization from rollout to roll out if you some kind of function approximator that can help, what will do is will reduce variance on this by introducing discounting and by introducing function approximation.",
                    "label": 0
                },
                {
                    "sent": "So this is interesting.",
                    "label": 0
                },
                {
                    "sent": "Often when people use discounting they consider it part of the definition of the MDP.",
                    "label": 0
                },
                {
                    "sent": "We're not going to do that here, so we're interested in maximizing expected sum of rewards over Horizon H. But we're going to think of discounting as a tweaking parameter that will help us reduce variance on things we estimate for the thing we're really trying to solve.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So original key estimate from 1 sample would look like the top line.",
                    "label": 0
                },
                {
                    "sent": "An alternative estimate looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's not an expectation.",
                    "label": 0
                },
                {
                    "sent": "Going to be correct.",
                    "label": 0
                },
                {
                    "sent": "Because we're discounting future things, an that's in reality, not what we care about.",
                    "label": 0
                },
                {
                    "sent": "But what happens is that.",
                    "label": 0
                },
                {
                    "sent": "The longer this rollout is, the higher variance, and by discounting we counted is a little bit so that we don't have increasing variance as much with the length of the rollout.",
                    "label": 0
                },
                {
                    "sent": "So this is just a variance reduction trick here.",
                    "label": 0
                },
                {
                    "sent": "It's not changing the problem definition, we still want to solve.",
                    "label": 0
                },
                {
                    "sent": "There's no problem, just reducing variance actually makes a big difference.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing that we can look at is we look at this.",
                    "label": 0
                },
                {
                    "sent": "The first line is what we looked at so far.",
                    "label": 0
                },
                {
                    "sent": "The second line is another way of writing this expected sum of rewards is the same as expected reward at the first time plus gamma times value, which is expected sum of rewards, then onwards.",
                    "label": 0
                },
                {
                    "sent": "Or we can write it this way.",
                    "label": 0
                },
                {
                    "sent": "Or we can write it this way and so forth.",
                    "label": 0
                },
                {
                    "sent": "There's many ways of writing this expectation.",
                    "label": 0
                },
                {
                    "sent": "If we happen to have a way of estimating a value function, we can use any one of those.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in general is advantage estimation.",
                    "label": 0
                },
                {
                    "sent": "What you use is an exponentially weighted average of those, and this of course dates back to TD, Lambda and eligibility traces.",
                    "label": 0
                },
                {
                    "sent": "Word is also being used for value function learning, but so this is the idea here is that we can combine these WHI would you want to do this?",
                    "label": 0
                },
                {
                    "sent": "It seems like if you had a good value function.",
                    "label": 0
                },
                {
                    "sent": "Clearly the one R0 plus gamma times the value would be the one that you want to use.",
                    "label": 0
                },
                {
                    "sent": "But the truth is, in practice, you don't typically have an amazing value function.",
                    "label": 0
                },
                {
                    "sent": "Have a reasonable value function, but not amazing.",
                    "label": 0
                },
                {
                    "sent": "Amazing an because you introduced the reasonable value function.",
                    "label": 0
                },
                {
                    "sent": "What will happen is that this will become biased.",
                    "label": 0
                },
                {
                    "sent": "It's not unbiased anymore.",
                    "label": 0
                },
                {
                    "sent": "That value function uses bias and so yes, we want to use it to reduce variance.",
                    "label": 0
                },
                {
                    "sent": "But we also want to make sure that we stay somewhat close to the unbiased estimate and hence the tradeoff between.",
                    "label": 0
                },
                {
                    "sent": "The two and then the tradeoff can be done in a very gradual way.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be just the first 2 lines being average.",
                    "label": 0
                },
                {
                    "sent": "We can average all possible lengths of rollouts capped off with value function.",
                    "label": 0
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiment.",
                    "label": 0
                },
                {
                    "sent": "Well you look at here is Lambda on the horizontal axis, gamma on the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "Remember the thing being optimized doesn't have a gamma.",
                    "label": 0
                },
                {
                    "sent": "The gamma is just used as a variance reduction.",
                    "label": 0
                },
                {
                    "sent": "What you see here is as a function of Lambda and gamma.",
                    "label": 0
                },
                {
                    "sent": "How well this policy gradient method will do, and we see that there is clearly a best there in the middle where gamma is about 0.98 and Lambda is 0.92300 point 98.",
                    "label": 0
                },
                {
                    "sent": "That's the range where this works best.",
                    "label": 0
                },
                {
                    "sent": "So actually it's interesting to go back and see what that means so large.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That works pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trashy suggested.",
                    "label": 0
                },
                {
                    "sent": "Last time that means you're willing to look pretty far, actually, and so it's actually in this particular problem.",
                    "label": 0
                },
                {
                    "sent": "It prefers the sum of rewards over it, bringing too much of the value function estimate, of course, is going to depend a lot on how many rollouts you do.",
                    "label": 0
                },
                {
                    "sent": "The more rollouts you have, the better that sum of rewards will be.",
                    "label": 0
                },
                {
                    "sent": "The less rollouts you have, the more you'll want to depend on the value function that Lambda will shift depends also where you are in training like these better, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Would it also depends on where you are in training.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think it would.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen such experiment, but I imagine that as V gets better, if you had any kind of estimate of how good your vius, you would be willing to trust it more.",
                    "label": 0
                },
                {
                    "sent": "At the same time, once you're ready doing very well, your updates will be small, and maybe you actually don't want to use because you want your fully unbiased estimate to get the final little bit.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how it would play out.",
                    "label": 0
                },
                {
                    "sent": "Sure, well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Provide awareness.",
                    "label": 0
                },
                {
                    "sent": "That shows.",
                    "label": 0
                },
                {
                    "sent": "That gamma becoming bigger overtime is better ending to the true value of the test value of gamma.",
                    "label": 0
                },
                {
                    "sent": "As you accumulate more data and he has some theoretical analysis of theoretical characterize, this is the recent paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's an interesting paper in setting things group that looks at model based RL largely, but essentially what they look at their.",
                    "label": 0
                },
                {
                    "sent": "They look at all.",
                    "label": 0
                },
                {
                    "sent": "So what you can do by changing gamma and why you might want to change gamma and the idea there is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's say you do model based RL, you have an approximate model.",
                    "label": 0
                },
                {
                    "sent": "You then plan in that approximate model.",
                    "label": 0
                },
                {
                    "sent": "You won't get the perfect plan because the model is only approximate.",
                    "label": 0
                },
                {
                    "sent": "An because of compounding error overtime under dynamics model the further in the future you look, the less accurate you're planning against those futures and so discounting could ensure that you don't come up with some totally amazing plans seemingly at the very last step gets you to gigantic reward, but actually the model.",
                    "label": 0
                },
                {
                    "sent": "The model only allows you to that, but the real world doesn't let you get there and have a really nice analysis.",
                    "label": 0
                },
                {
                    "sent": "Think.",
                    "label": 0
                },
                {
                    "sent": "I think it's really intriguing there is.",
                    "label": 0
                },
                {
                    "sent": "They have a theorem that says.",
                    "label": 0
                },
                {
                    "sent": "That says we can measure if you're given.",
                    "label": 0
                },
                {
                    "sent": "An MVP.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Measure.",
                    "label": 0
                },
                {
                    "sent": "How many as a function?",
                    "label": 0
                },
                {
                    "sent": "So if you give it an MVP and let's say you were allowed to change the reward function in the MDP.",
                    "label": 0
                },
                {
                    "sent": "As you change the reward function, you will get different optimal policy's.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Then the larger discount factor.",
                    "label": 0
                },
                {
                    "sent": "So even in the we can change the reward function every changing reward function might result in a new optimal policy or not.",
                    "label": 0
                },
                {
                    "sent": "The longer your horizon.",
                    "label": 0
                },
                {
                    "sent": "Demoor optimal policies exist as you vary your reward function and so.",
                    "label": 0
                },
                {
                    "sent": "The longer your horizon, the more likely you can overfit to something because the space of policies are going to be choosing from is larger 'cause there was more policy that potentially be optimal on this longer horizon, and so that's a really nice vertical guarantee.",
                    "label": 0
                },
                {
                    "sent": "Thanks for pointing out well.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is what happens in 3D, so without this generalized advantage estimation actually the precision policy position doesn't work in 3D or not within the amount of time corresponding to our patients of running experiments.",
                    "label": 0
                },
                {
                    "sent": "But with all of this machinery, this actually works quite well.",
                    "label": 0
                },
                {
                    "sent": "Initially, of course it falls over because it's a random policy, but overtime it gets better and better and better.",
                    "label": 0
                },
                {
                    "sent": "Again, the reward function is very simple.",
                    "label": 0
                },
                {
                    "sent": "Further North, the better, less impact with the ground, better than some penalty for amount of torque at each of the joints.",
                    "label": 0
                },
                {
                    "sent": "Nothing in there about what motion looks like, which is often done to generate simulated motions using motion capture system and then tried to match that.",
                    "label": 0
                },
                {
                    "sent": "That's not what's done here.",
                    "label": 0
                },
                {
                    "sent": "You can use exact same thing for a different robot.",
                    "label": 0
                },
                {
                    "sent": "This again in the Majokko simulator built by a motor off at University of Washington and without any change, just a different robot put in front of it.",
                    "label": 0
                },
                {
                    "sent": "It's able to learn.",
                    "label": 0
                },
                {
                    "sent": "How to get as far forward as possible, which is something quite interesting that happens here is that it finds a gate that is totally unrealistic for the real world.",
                    "label": 0
                },
                {
                    "sent": "But it's very, very fast in the simulator.",
                    "label": 0
                },
                {
                    "sent": "So Arrel could be used as a way of debugging simulators.",
                    "label": 0
                },
                {
                    "sent": "Henry Ward is whether the head is at standing height or not, then measuring distance from standing height.",
                    "label": 0
                },
                {
                    "sent": "I'm just going.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contrast this for fun with what happened at the DARPA Robotics Challenge just a year ago, by no means claiming this is the best thing you could have seen if you were there, but it's definitely some of the things you would have seen if you were there.",
                    "label": 0
                },
                {
                    "sent": "Suggesting that this is a hard problem to get to work, even if there are some solutions out there that work at some situations.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to now type this into a 3C.",
                    "label": 0
                },
                {
                    "sent": "The async Advantage Actor critic, so not too long ago it was a paper led by Vladimir NPI at Deep Mind.",
                    "label": 0
                },
                {
                    "sent": "Looking at asynchronous deep reinforcement learning where he wants to run it on as many machines as possible to see that can speed things up.",
                    "label": 0
                },
                {
                    "sent": "The paper had Q learning in it distributed Q learning as well as policy gradient.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm going to see if you look at the learning curves.",
                    "label": 0
                },
                {
                    "sent": "The yellow is the policy grand method.",
                    "label": 0
                },
                {
                    "sent": "The other curves are the Q learning methods.",
                    "label": 0
                },
                {
                    "sent": "You can see that actually posterior method ends up doing better than the other methods.",
                    "label": 0
                },
                {
                    "sent": "What they used is.",
                    "label": 0
                },
                {
                    "sent": "The generalized advantage systemation so the value function is being estimated.",
                    "label": 0
                },
                {
                    "sent": "That in turn is being used to estimate the Q function and then that Q -- V gives you the advantage what's multiplied with your grad log probability.",
                    "label": 0
                },
                {
                    "sent": "That's and of course run over multiple multiple machines.",
                    "label": 0
                },
                {
                    "sent": "Here to scale it up, they don't use the trust region part.",
                    "label": 0
                },
                {
                    "sent": "The translation part is Second order, so it's it's a little trickier to run that on multiple machines and get it to work.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying it's impossible, but.",
                    "label": 0
                },
                {
                    "sent": "That's probably one of the reasons people haven't used a trust region over multiple machines at this point, it's a good challenge to try out for you.",
                    "label": 0
                },
                {
                    "sent": "See here that actually works really well on a wide range of Atari games too.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, after 12:30, is that right?",
                    "label": 0
                },
                {
                    "sent": "OK, so with 20 minutes left to do the second half of our bullet points.",
                    "label": 0
                },
                {
                    "sent": "Entirely true, because we also have half an hour this afternoon and also these are a lot shorter actually.",
                    "label": 0
                },
                {
                    "sent": "OK, then I'll leave the last two bullet points for the afternoon and justice to these two.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Path relatives we've been looking at gradient based policy optimization.",
                    "label": 0
                },
                {
                    "sent": "Let's look at what this actually looks like in a graph.",
                    "label": 0
                },
                {
                    "sent": "You have state at Time Zero state at time one state at time one is a function of state of time zero through and also have the action through some function F which is the dynamics model which in general is going to be stochastic.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There's a reward as a function of state.",
                    "label": 0
                },
                {
                    "sent": "It can also depend on action, but for simplicity let's assume it just depends on state here.",
                    "label": 0
                },
                {
                    "sent": "Your state affects the action.",
                    "label": 0
                },
                {
                    "sent": "You take computation graph corresponding to the problem we've been solving.",
                    "label": 0
                },
                {
                    "sent": "If you look at it this way.",
                    "label": 0
                },
                {
                    "sent": "Kind of say, well, why are we computing gradients in this kind of funny likelihood ratio way?",
                    "label": 0
                },
                {
                    "sent": "We know when we have graphs like this you can just back property back, propagate through it, get gradients that way.",
                    "label": 0
                },
                {
                    "sent": "Computing grains that way.",
                    "label": 0
                },
                {
                    "sent": "That's the path derivative method of computing gradients.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a different way of computing gradients.",
                    "label": 0
                },
                {
                    "sent": "Let's look at that now, so.",
                    "label": 0
                },
                {
                    "sent": "Can we do this?",
                    "label": 0
                },
                {
                    "sent": "Let's say we had a current rollout.",
                    "label": 0
                },
                {
                    "sent": "Which tells us the quantities of each of these nodes as the forward pass in this network can be followed up by a backdrop pass, well, why not?",
                    "label": 0
                },
                {
                    "sent": "Let's see derivative of U with respect to Theta I.",
                    "label": 0
                },
                {
                    "sent": "Each of the entries in Theta is some of their respective of each of the terms R with respect to our depends on South then as depends on Theta chain rule.",
                    "label": 0
                },
                {
                    "sent": "There we need an answer respect to Theta which as depends on Theta through.",
                    "label": 0
                },
                {
                    "sent": "The previous state.",
                    "label": 0
                },
                {
                    "sent": "And the dynamics, as well as through the action and the dynamics.",
                    "label": 0
                },
                {
                    "sent": "Again, just a chain rule and then action depends on Theta through the policy.",
                    "label": 0
                },
                {
                    "sent": "So this is just general, not nothing special.",
                    "label": 0
                },
                {
                    "sent": "There you can do this.",
                    "label": 0
                },
                {
                    "sent": "You can do a backward pass compute this.",
                    "label": 0
                },
                {
                    "sent": "This assumes that you can actually evaluate all these quantities.",
                    "label": 0
                },
                {
                    "sent": "So let's assume for now.",
                    "label": 0
                },
                {
                    "sent": "F is known, the dynamics is known, F is deterministic.",
                    "label": 0
                },
                {
                    "sent": "Then we can definitely do this, and we can just back prop and optimize our policy that way.",
                    "label": 0
                },
                {
                    "sent": "It will use the dynamics model, which is probably a good thing if we have.",
                    "label": 0
                },
                {
                    "sent": "It will probably give you more accurate gradients than the likelihood ratio method.",
                    "label": 0
                },
                {
                    "sent": "It does depend a little bit on these derivatives existing, so there are more assumptions here.",
                    "label": 0
                },
                {
                    "sent": "The differences we assume are is differentiable, which we did assume before before you could have our reward.",
                    "label": 0
                },
                {
                    "sent": "That's only good when you're at the goal and then drops to zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "Things like that that's not going to work here.",
                    "label": 0
                },
                {
                    "sent": "It assumes F is known as soon as it is deterministic.",
                    "label": 0
                },
                {
                    "sent": "As you progress, here are will remain differentiable.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be able to get rid of that, maybe somebody can, but I'm not able to in this lies at least, but we'll see if we can relax these assumptions here.",
                    "label": 0
                },
                {
                    "sent": "So first one F is known.",
                    "label": 0
                },
                {
                    "sent": "You're collecting data.",
                    "label": 0
                },
                {
                    "sent": "You could use the data to build a model.",
                    "label": 0
                },
                {
                    "sent": "You could do a supervised learning from ST&U T2ST plus one that's F, so you could in principle in addition to doing this policy optimizations, training model, plug it in here.",
                    "label": 0
                },
                {
                    "sent": "Do this.",
                    "label": 0
                },
                {
                    "sent": "This doesn't have to worry about the accuracy of your model in compounding errors, and you need to think about that, but F being known is not a super strong assumption necessarily.",
                    "label": 0
                },
                {
                    "sent": "There's fixes to that.",
                    "label": 0
                },
                {
                    "sent": "How about it being deterministic?",
                    "label": 0
                },
                {
                    "sent": "Let's look at a simple.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I swear it's quite intuitive how to fix this.",
                    "label": 0
                },
                {
                    "sent": "Let's say S at time T + 1 is a function of ST&UT plus some noise WT.",
                    "label": 0
                },
                {
                    "sent": "OK. Then if we have a simple sample trajectory, we can actually back solve for the values of W. We do the rollout.",
                    "label": 0
                },
                {
                    "sent": "We can see what our States and actions were.",
                    "label": 0
                },
                {
                    "sent": "Assuming we know F, we can see what the differences between S T + 1, an FSD and UTI.",
                    "label": 0
                },
                {
                    "sent": "That's WT.",
                    "label": 0
                },
                {
                    "sent": "We actually know that once we know WT.",
                    "label": 0
                },
                {
                    "sent": "We actually know the dynamics.",
                    "label": 0
                },
                {
                    "sent": "Everything is known now.",
                    "label": 0
                },
                {
                    "sent": "It's actually not stochastic.",
                    "label": 0
                },
                {
                    "sent": "It's slightly over fitting to the rollout you just did, but that's what it is.",
                    "label": 0
                },
                {
                    "sent": "It's just one sample trajectory, or you can apply the exact same thing again.",
                    "label": 0
                },
                {
                    "sent": "So that's nice even for stochastic, specially in the simple structure.",
                    "label": 0
                },
                {
                    "sent": "You can reply to same idea, of course, with a single roll out.",
                    "label": 0
                },
                {
                    "sent": "Be careful 'cause he will be overfitting to the sequence of noise that you experience.",
                    "label": 0
                },
                {
                    "sent": "You need more than one roll out to do this well, but it's possible more generally.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call the pre parameterization trick which will do it is for you if it's applicable, original thing would be S. T + 1 is a stochastic function of ST&UT an Theta the.",
                    "label": 0
                },
                {
                    "sent": "The parameters that you're optimizing.",
                    "label": 0
                },
                {
                    "sent": "Re parameterized, and it's not always possible to do this, but often possible to this.",
                    "label": 0
                },
                {
                    "sent": "You have a new function of deterministic, where there's a new variable their size stochastic.",
                    "label": 0
                },
                {
                    "sent": "That's a new auxiliary variable you introduce that captures all the noise.",
                    "label": 0
                },
                {
                    "sent": "OK. And I don't have time to go into the details of how to make that work in general, but let's look at a simple example.",
                    "label": 0
                },
                {
                    "sent": "Let's say your next date is a Gaussian centered around some G function of STT and Theta with some variance in my squared.",
                    "label": 0
                },
                {
                    "sent": "Now what this would look like is SD plus one is that G function plus Sigma times sigh and all the randomness is factored out in the Sky over here.",
                    "label": 0
                },
                {
                    "sent": "And whenever you can factor out the randomness, you can make this work.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you look.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This computation graph here effectively means is that you're introducing extra nodes for each side variable that are pointing into that state at the next time.",
                    "label": 0
                },
                {
                    "sent": "Making that function F deterministic function of PSI and STNUT.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, the later things have less detail on the earlier things.",
                    "label": 0
                },
                {
                    "sent": "In the interest of time.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's actually can work quite well.",
                    "label": 0
                },
                {
                    "sent": "You do need the continuous value.",
                    "label": 0
                },
                {
                    "sent": "The output of after the computer.",
                    "label": 0
                },
                {
                    "sent": "Yes, for this, so there's a lot of assumptions here that are not needed for the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "You need your actions to be continuous valued.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can take derivatives through actions you need your states to be continuous valued.",
                    "label": 0
                },
                {
                    "sent": "You need your award to be differentiable so a lot of extra assumptions need to be made.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone can be alleviated by embedding discrete things into continuous spaces.",
                    "label": 0
                },
                {
                    "sent": "Who knows, I'm not sure, but the other thing is you use the forward more distant forward.",
                    "label": 0
                },
                {
                    "sent": "More differentiation, which could be very expensive.",
                    "label": 0
                },
                {
                    "sent": "Computed the dimension of data and S is large.",
                    "label": 0
                },
                {
                    "sent": "Or you could also.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Background yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So the question here is when we look at these equations.",
                    "label": 0
                },
                {
                    "sent": "That's how the forward propagation, forward mode automatic differentiation works.",
                    "label": 0
                },
                {
                    "sent": "We can just as well do it with the backdrop.",
                    "label": 0
                },
                {
                    "sent": "That's yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's also the way would be done.",
                    "label": 0
                },
                {
                    "sent": "This is just to illustrate what derivatives are needed.",
                    "label": 0
                },
                {
                    "sent": "An essentially also to your first point that.",
                    "label": 0
                },
                {
                    "sent": "For these derivatives to exist, it actually S has to be continuous.",
                    "label": 0
                },
                {
                    "sent": "Actions has to have to be continuous, and so forth.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can't do this.",
                    "label": 0
                },
                {
                    "sent": "But you wouldn't compute it this way.",
                    "label": 0
                },
                {
                    "sent": "You would back prop to compute, which does dynamic programming over the things you compute.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This idea is used in the.",
                    "label": 0
                },
                {
                    "sent": "Stochastic value gradients paper by.",
                    "label": 0
                },
                {
                    "sent": "Make his and collaborators so they essentially look at exactly this setup that I just described an backdrop through it to compute their gradients.",
                    "label": 0
                },
                {
                    "sent": "You can make variants of this because when you look at this, if you back perhaps through and you have this sum of rewards that you take derivatives off, you can play the exact same idea again, some of rewards that skew values myself into Q value separately.",
                    "label": 0
                },
                {
                    "sent": "If the Q value is differentiable with respect to state in action, I can just directly take derivatives of the Q values rather than of the future rewards.",
                    "label": 0
                },
                {
                    "sent": "And so if you do that, you get the domestic policy grant or the deep deterministic policy grain.",
                    "label": 0
                },
                {
                    "sent": "If you use a deep network to fit the Q values and so that those ideas are in those papers but essentially.",
                    "label": 0
                },
                {
                    "sent": "The reparameterization trick applied to policy optimization.",
                    "label": 0
                },
                {
                    "sent": "Including also the ideas of introducing Q value fitting to not have a sum over all future rewards, but having fits instead.",
                    "label": 0
                },
                {
                    "sent": "An if it's applicable.",
                    "label": 0
                },
                {
                    "sent": "This actually learns very quickly because you use a lot more information than using the likely ratio.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This way to generalize this.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This calculation don't want to get into the details, but essentially you can mix and match any kind of like liberation path derivative computation 1.",
                    "label": 0
                },
                {
                    "sent": "Interesting thing is that if you have a neural network big neural network that stochastic and you have black box nodes in their nodes where you don't have access to compute derivatives on that node.",
                    "label": 0
                },
                {
                    "sent": "By putting a stochastic node in front of it, you can they use the likelihood ratio grand on that stochastic node that sits in front of it.",
                    "label": 0
                },
                {
                    "sent": "Exactly what we did when we want to do policy grants with no access to dynamics.",
                    "label": 0
                },
                {
                    "sent": "That exact same idea is much more widely applicable than just to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that once you think of it this way, you don't need to re derive things in any special way.",
                    "label": 0
                },
                {
                    "sent": "If you have recurrent policy.",
                    "label": 0
                },
                {
                    "sent": "CEO wasn't policy grand for a recurrent policy sold the same thing you just set up your stochastic computation graph some.",
                    "label": 0
                },
                {
                    "sent": "Knows you compute the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "Grandmother knows you compute the path derivative, whatever seems right for that node, and you have gradients for any kind of policy architecture that you might want to use, yes.",
                    "label": 0
                },
                {
                    "sent": "High variance on like every system.",
                    "label": 0
                },
                {
                    "sent": "So the question is, would you pay a price having high variance?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes, you use the likely ratio estimated.",
                    "label": 0
                },
                {
                    "sent": "You tend to have higher variance.",
                    "label": 0
                },
                {
                    "sent": "So typically what you would do is you would analyze your stochastic computation graph.",
                    "label": 0
                },
                {
                    "sent": "Add a corresponding to your RL problem or to something else.",
                    "label": 0
                },
                {
                    "sent": "You would see where is my re parameterisation trick applicable?",
                    "label": 0
                },
                {
                    "sent": "Whenever you can apply it, you probably apply it, but then some cases will not be applicable.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a real dynamical system and for some reason you're not building a model.",
                    "label": 0
                },
                {
                    "sent": "Whatever it might be only or you want to grant on this one.",
                    "label": 0
                },
                {
                    "sent": "Run on the real system and you're not building a model then that's the only way to do it.",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "There are some special cases that you can set up where the path relative is higher variance than likely ratio.",
                    "label": 0
                },
                {
                    "sent": "All the examples I've seen are very artificial, but they involve things like you know, function that's like sign 1 / X, which gets like very oscillatory near 0.",
                    "label": 0
                },
                {
                    "sent": "In fact, infinite oscillations near 0.",
                    "label": 0
                },
                {
                    "sent": "Then if you do path derivative, it'll have the derivatives will just be going up and down, up and down, up and down.",
                    "label": 0
                },
                {
                    "sent": "It will be really tricky to deal with.",
                    "label": 0
                },
                {
                    "sent": "That likely ratio will have lower variance because the bounded magnitude of the actual.",
                    "label": 0
                },
                {
                    "sent": "Function value, so there is this.",
                    "label": 0
                },
                {
                    "sent": "Some scenarios like that, but obviously assign 1 / X is not what we typically encounter in this kind of computations.",
                    "label": 0
                },
                {
                    "sent": "But maybe there are cases where something similar happens.",
                    "label": 0
                },
                {
                    "sent": "Essentially, liquid ratio will do better than path derivative if there is very high variance on the derivatives, but it's less typical for that to be the case in the function.",
                    "label": 0
                },
                {
                    "sent": "Bundle variation, yes.",
                    "label": 0
                },
                {
                    "sent": "Bombing variation and high variance on derivatives will make likely ratio lower variance than path derivative, but.",
                    "label": 0
                },
                {
                    "sent": "I don't know any realistic scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so last thing I want to do is say something about.",
                    "label": 0
                },
                {
                    "sent": "Well, we've seen a lot of methods at this point an it can be very easy to get a headache with all these methods and think there's too many of them.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we did is.",
                    "label": 0
                },
                {
                    "sent": "Put up a table to clarify the headache.",
                    "label": 0
                },
                {
                    "sent": "So we see here there is a set of methods that that's readily applicable to continuous control tasks.",
                    "label": 0
                },
                {
                    "sent": "On the entire demand has been a lot of comparisons of methods that have discrete action spaces.",
                    "label": 0
                },
                {
                    "sent": "This specifically for continuous control tasks.",
                    "label": 0
                },
                {
                    "sent": "A comparison of the different methods that exist.",
                    "label": 0
                },
                {
                    "sent": "It suggests that in this comparison, like which methods tend to work better, it seems like the natural grand idea which is made even more kind of explicit and more precise in the RPO setup tends to do best the.",
                    "label": 0
                },
                {
                    "sent": "Deep domestic policy grant, which does the path derivative, does quite well too in many situations.",
                    "label": 0
                },
                {
                    "sent": "Well, harder to get to work, so that's why some of these entries are empty there.",
                    "label": 0
                },
                {
                    "sent": "I think there's some interesting questions there.",
                    "label": 0
                },
                {
                    "sent": "If the GDP can be made more stable, what the result would be there?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't want to implement them yourself or build something called our lab, which makes it easy to experiment with them.",
                    "label": 0
                },
                {
                    "sent": "It's not just.",
                    "label": 0
                },
                {
                    "sent": "Here's a snippet of code that is this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's also it will output information to allow you to make graphs more easily, have multiple runs, hyperparameter settings, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So it's really like a lab software infrastructure around each of these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interfaces with the Open AI gym so you can pull in a lot of environments into it very easily and see then how well this you're going to do against other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Or you just want to compare existing algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I think we should take questions at this point.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the afternoon, I'll look at the two lost topics, guided policy search and inverse RL.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}