{
    "id": "wygifd3jlmictezfm5ecc2a6ptpnk6co",
    "title": "Supervised Learning on Matrices with the Dual Spectral Regularization",
    "info": {
        "author": [
            "Ryota Tomioka, Fraunhofer Institute for Intelligent Analysis and Information Systems"
        ],
        "published": "Nov. 9, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss07_tomioka_slm/",
    "segmentation": [
        [
            "Right, so let me introduce me out that dummy, Oka, he's going to tell you about supervised learning over matrices with general spectral regularization.",
            "So thank you for the introduction.",
            "I'm doing some yoga.",
            "I'm from Fraunhofer Institute in Berlin.",
            "I work on Brain Computer interface an I'm quite excited about this study because I work on easy data every day and I come up with this idea and it could be applied for many different problems.",
            "So let's."
        ],
        [
            "See where the matrix data come from, for example, in EG, we often encounter multivariate time series like.",
            "We have many sensors and many time points and also we often take the example 2nd order statistics.",
            "Then we get the cross term of all sensors and then this is the input of our system and what we want to do here is simple simply supervised learning.",
            "OK I have to click yes."
        ],
        [
            "Yeah, so I have my input matrix X and my outfit Y is given and we want to learn, for example simply a linear function WX and the dot product of WX plus Biotherm.",
            "So W here is we call this weight matrix which is our time.",
            "See Matrix which is the same size as the input.",
            "So we want to do a regularize empirical risk minimization as standard S you know with the sum of losses for each sample.",
            "An regularization term, but the question here is what regularization is appropriate here for the matrices?"
        ],
        [
            "And for example, we can do laugh though, which is the absolute sum of the entries of the matrix, which is just taking all the elements.",
            "Or Alternatively, you can do rich regression, which is the squared sum of old entries in the matrix.",
            "And actually this leads to the squared sum of the singular value of the matrix.",
            "And what we do here is called deal spectral regularization, which is the absolute sum of singular values.",
            "And the intuition is that difference between the Ridge regression we have the squared sum of singular values, which is like having a Gaussian prior here and on the other hand, deal spectrum regularization is absolute sum of singular values of a given matrix W. So which is like having a Laplacian prior on the singular values?"
        ],
        [
            "So the intuition about this model is that if we do the singular value decomposition of W, like having you and singular values in V transposed, then by just plugging in the definition of this into the classifier we get a nice decomposition where."
        ],
        [
            "Here we have features, which is the input X which is projected from left by U vector yuan from right by vector V and we have a linear combination of these features where this linear combination coefficient is the singular value of the matrix W. So now here we are putting absolute sum penalty on the linear combination coefficient.",
            "That means that we expect this.",
            "Linear thumb to be a few components and not like real whole spectrum.",
            "So we assume some sparseness here.",
            "So the difference between having traditional L1 loss so kind of penalty is that we are doing the shrinkage and the feature learning at the same time.",
            "And surprisingly this leads to a convex optimization problem an.",
            "Basically because this.",
            "Sum of singular value is a norm which is the dual norm of the spectral norm, which is the maximum singular."
        ],
        [
            "Lee so for many different loss functions we can do the optimization an.",
            "For example, for general, twice differentiable lot function like logistic loss we can do interior point method.",
            "I have a very crude barrier method implementation an for his loss.",
            "The story is quite easier.",
            "We can do semidefinite programming an for particularly quadratic loss which actually belongs to this class.",
            "There is a better implementation based on the second owner.",
            "Conan."
        ],
        [
            "Gramming and let's go through the application directly.",
            "An here we have problem of classifying imaginative movement of people and the underlying Physiology is known as localized modulation of rhythmic oscillatory behaviors like in the left motor cortex or in the right motor cortex.",
            "So because we're interested in these rhythmic activities, we take the 2nd order statistics of the signal and say this is my input matrix X.",
            "Which looks like this, and this is simply binary classification to classify between imagine left hand movement or right hand movement."
        ],
        [
            "And we have the results which I compare with three classical methods which I don't explain, so don't expect me too much.",
            "But it has a nice improvement compared to the kind of state of the art method.",
            "And this part we show the comparison between the regularization of our absolute some regularization and the squared.",
            "Some regularization which prefers our low rank pollution.",
            "And the third comparison is that if we fix the rank before hand manually, what we get is actually suboptimal.",
            "Then having this.",
            "Sparseness panel."
        ],
        [
            "Lee So what we can see here is that we do cross validation and select the regularization constant Lambda here and we see that the more we regularize, the more bars the singular value spectrum becomes.",
            "So at the selected value of Lambda we only have two Eigen components, one positive and one negative and all the other eigenvalues are numerically there."
        ],
        [
            "So what we see here is a projection of the weight, the eigenvectors of the weight matrix.",
            "So in this case we only have two eigenvalues, so the weight matrix it decomposed into outer products and we have plotted the eigenvectors corresponding to one negative eigenvalue and one positive eigen value which has nice interpretation of having one spot in the left hemisphere and one spot in the right hemisphere.",
            "Which says that the discriminative information is concentrated in the two dimensional subspace band by these two vectors."
        ],
        [
            "We have another example which is kind of feller system.",
            "You have to focus on one of the letters in a matrix which is displayed and which kind of flashes all the time, and we measure the potential in the brain and when you when the letter that you're focusing on flash if you have them, surprise signal which called P300 an we're interested in detecting this surprise signal.",
            "And the input is simply roll times three, like having many channels in many time points."
        ],
        [
            "So this what the Liminary result tells is that this.",
            "The matrix is moderately far sparse, which have only 15 singular values, and I show here is the first eigen vector which is in this case the the left eigenvector correspond to the spatial filter which has not focus on the central area of the brain.",
            "An one the right one, right singular vector which corresponds to the temporal filter.",
            "Which have a knife drop here where we exactly expect something some discriminative."
        ],
        [
            "For May shun.",
            "So to summarize, this is a discarded model that factorizes automatically using so called deal spectral regularization.",
            "So what's left through a penalized empirical loss minimization problem, which is a nicely convex and has a good interpretation because of the sparseness that the method gives?",
            "And we have talked about two applications.",
            "One was about modern mystery easy classification which use the 2nd order statistics of the data and the second application was based on the 1st order statistics of data which is spatial temporal data.",
            "So they still."
        ],
        [
            "Anne, thank you very much and also my colleague.",
            "James hey, sometimes a in law, so in in usual not matrix but vector dimensional space cases you do regularization, it's usually combination of AL, one regularization and for example a spectral regularization.",
            "Did you compare your method for?",
            "You, know, two together the spectral the signal square signal, singular values and the sparsity.",
            "Organization together.",
            "Yeah, first of all, I don't have the comparison so I cannot tell that the result an I'm interested in these kind of switch and when the regularization is not exactly a norm, which makes the story a bit more complicated.",
            "But the good thing about this is that there is nice duality between the primary problem and the dual problem and the deal problem has.",
            "Then I form like.",
            "So the the deal spectral regularization in the primal corresponds to bounding the.",
            "The ratio of the fit using the dual norm so which has nice kind of integration.",
            "So I like this, but of course the using other regularization is definitely we have to try.",
            "Yeah, let's make it just the last question.",
            "So with this, regularization will be able to.",
            "Kernel is the method.",
            "Sorry with the regularization that you have chosen, will it be able to?",
            "Kernel is the method.",
            "I don't know whether you want kernelized.",
            "But yet that great question Anne.",
            "How do I go back?"
        ],
        [
            "So it is known so.",
            "There are three types of problems known, so the case I have talked here is prediction over matrices an we want to predict this color value.",
            "For example, you can imagine different problem when you have input X, which is a vector and want to predict an output which is a vector and it is known I thermal this year.",
            "People amid another.",
            "Considered this gate and they showed that they can kernelized the input space and other people consider in this situation whether they can kernel eyes the input space in output space simultaneously.",
            "But so far there seems to be no result positive result about simultaneously caramelizing.",
            "Right, I mean usually OK.",
            "I mean here.",
            "Probably the objective can be kernelized.",
            "Maybe it can be, but usually I mean you take the Dark Ages norm of the function, right?",
            "In the case of, you're just dealing with the functions functions which map to are.",
            "Here you have, like regularizer is something like the absolute similar values.",
            "Yeah, so did you know with the regularizer?",
            "I'm wondering whether some or you will be able to accomplish your method.",
            "Maybe maybe you can do it for objective, but what happens to the regularizer?",
            "Sorry, I didn't exactly get the last.",
            "I mean maybe you can map these matrices into high dimensional space and probably you can define some kernel and kernel is your objective right?",
            "Yeah, but what happens to your regularizer, I mean?",
            "Your your regularizer which you mentioned as these absolute sum of singular values, yes, so that there is of course another approach, but it using a different regularization.",
            "So yeah, so the problem is could we kernelized this directly in this formulation an we don't know yet?",
            "OK, so let's thank speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let me introduce me out that dummy, Oka, he's going to tell you about supervised learning over matrices with general spectral regularization.",
                    "label": 0
                },
                {
                    "sent": "So thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "I'm doing some yoga.",
                    "label": 0
                },
                {
                    "sent": "I'm from Fraunhofer Institute in Berlin.",
                    "label": 0
                },
                {
                    "sent": "I work on Brain Computer interface an I'm quite excited about this study because I work on easy data every day and I come up with this idea and it could be applied for many different problems.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See where the matrix data come from, for example, in EG, we often encounter multivariate time series like.",
                    "label": 1
                },
                {
                    "sent": "We have many sensors and many time points and also we often take the example 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "Then we get the cross term of all sensors and then this is the input of our system and what we want to do here is simple simply supervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK I have to click yes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so I have my input matrix X and my outfit Y is given and we want to learn, for example simply a linear function WX and the dot product of WX plus Biotherm.",
                    "label": 0
                },
                {
                    "sent": "So W here is we call this weight matrix which is our time.",
                    "label": 0
                },
                {
                    "sent": "See Matrix which is the same size as the input.",
                    "label": 0
                },
                {
                    "sent": "So we want to do a regularize empirical risk minimization as standard S you know with the sum of losses for each sample.",
                    "label": 1
                },
                {
                    "sent": "An regularization term, but the question here is what regularization is appropriate here for the matrices?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for example, we can do laugh though, which is the absolute sum of the entries of the matrix, which is just taking all the elements.",
                    "label": 0
                },
                {
                    "sent": "Or Alternatively, you can do rich regression, which is the squared sum of old entries in the matrix.",
                    "label": 0
                },
                {
                    "sent": "And actually this leads to the squared sum of the singular value of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And what we do here is called deal spectral regularization, which is the absolute sum of singular values.",
                    "label": 1
                },
                {
                    "sent": "And the intuition is that difference between the Ridge regression we have the squared sum of singular values, which is like having a Gaussian prior here and on the other hand, deal spectrum regularization is absolute sum of singular values of a given matrix W. So which is like having a Laplacian prior on the singular values?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the intuition about this model is that if we do the singular value decomposition of W, like having you and singular values in V transposed, then by just plugging in the definition of this into the classifier we get a nice decomposition where.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have features, which is the input X which is projected from left by U vector yuan from right by vector V and we have a linear combination of these features where this linear combination coefficient is the singular value of the matrix W. So now here we are putting absolute sum penalty on the linear combination coefficient.",
                    "label": 0
                },
                {
                    "sent": "That means that we expect this.",
                    "label": 0
                },
                {
                    "sent": "Linear thumb to be a few components and not like real whole spectrum.",
                    "label": 0
                },
                {
                    "sent": "So we assume some sparseness here.",
                    "label": 0
                },
                {
                    "sent": "So the difference between having traditional L1 loss so kind of penalty is that we are doing the shrinkage and the feature learning at the same time.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly this leads to a convex optimization problem an.",
                    "label": 0
                },
                {
                    "sent": "Basically because this.",
                    "label": 0
                },
                {
                    "sent": "Sum of singular value is a norm which is the dual norm of the spectral norm, which is the maximum singular.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lee so for many different loss functions we can do the optimization an.",
                    "label": 0
                },
                {
                    "sent": "For example, for general, twice differentiable lot function like logistic loss we can do interior point method.",
                    "label": 1
                },
                {
                    "sent": "I have a very crude barrier method implementation an for his loss.",
                    "label": 0
                },
                {
                    "sent": "The story is quite easier.",
                    "label": 0
                },
                {
                    "sent": "We can do semidefinite programming an for particularly quadratic loss which actually belongs to this class.",
                    "label": 0
                },
                {
                    "sent": "There is a better implementation based on the second owner.",
                    "label": 0
                },
                {
                    "sent": "Conan.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gramming and let's go through the application directly.",
                    "label": 0
                },
                {
                    "sent": "An here we have problem of classifying imaginative movement of people and the underlying Physiology is known as localized modulation of rhythmic oscillatory behaviors like in the left motor cortex or in the right motor cortex.",
                    "label": 1
                },
                {
                    "sent": "So because we're interested in these rhythmic activities, we take the 2nd order statistics of the signal and say this is my input matrix X.",
                    "label": 0
                },
                {
                    "sent": "Which looks like this, and this is simply binary classification to classify between imagine left hand movement or right hand movement.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have the results which I compare with three classical methods which I don't explain, so don't expect me too much.",
                    "label": 0
                },
                {
                    "sent": "But it has a nice improvement compared to the kind of state of the art method.",
                    "label": 0
                },
                {
                    "sent": "And this part we show the comparison between the regularization of our absolute some regularization and the squared.",
                    "label": 0
                },
                {
                    "sent": "Some regularization which prefers our low rank pollution.",
                    "label": 0
                },
                {
                    "sent": "And the third comparison is that if we fix the rank before hand manually, what we get is actually suboptimal.",
                    "label": 0
                },
                {
                    "sent": "Then having this.",
                    "label": 0
                },
                {
                    "sent": "Sparseness panel.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee So what we can see here is that we do cross validation and select the regularization constant Lambda here and we see that the more we regularize, the more bars the singular value spectrum becomes.",
                    "label": 0
                },
                {
                    "sent": "So at the selected value of Lambda we only have two Eigen components, one positive and one negative and all the other eigenvalues are numerically there.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we see here is a projection of the weight, the eigenvectors of the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "So in this case we only have two eigenvalues, so the weight matrix it decomposed into outer products and we have plotted the eigenvectors corresponding to one negative eigenvalue and one positive eigen value which has nice interpretation of having one spot in the left hemisphere and one spot in the right hemisphere.",
                    "label": 0
                },
                {
                    "sent": "Which says that the discriminative information is concentrated in the two dimensional subspace band by these two vectors.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have another example which is kind of feller system.",
                    "label": 0
                },
                {
                    "sent": "You have to focus on one of the letters in a matrix which is displayed and which kind of flashes all the time, and we measure the potential in the brain and when you when the letter that you're focusing on flash if you have them, surprise signal which called P300 an we're interested in detecting this surprise signal.",
                    "label": 0
                },
                {
                    "sent": "And the input is simply roll times three, like having many channels in many time points.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this what the Liminary result tells is that this.",
                    "label": 0
                },
                {
                    "sent": "The matrix is moderately far sparse, which have only 15 singular values, and I show here is the first eigen vector which is in this case the the left eigenvector correspond to the spatial filter which has not focus on the central area of the brain.",
                    "label": 0
                },
                {
                    "sent": "An one the right one, right singular vector which corresponds to the temporal filter.",
                    "label": 0
                },
                {
                    "sent": "Which have a knife drop here where we exactly expect something some discriminative.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For May shun.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, this is a discarded model that factorizes automatically using so called deal spectral regularization.",
                    "label": 1
                },
                {
                    "sent": "So what's left through a penalized empirical loss minimization problem, which is a nicely convex and has a good interpretation because of the sparseness that the method gives?",
                    "label": 0
                },
                {
                    "sent": "And we have talked about two applications.",
                    "label": 0
                },
                {
                    "sent": "One was about modern mystery easy classification which use the 2nd order statistics of the data and the second application was based on the 1st order statistics of data which is spatial temporal data.",
                    "label": 0
                },
                {
                    "sent": "So they still.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne, thank you very much and also my colleague.",
                    "label": 0
                },
                {
                    "sent": "James hey, sometimes a in law, so in in usual not matrix but vector dimensional space cases you do regularization, it's usually combination of AL, one regularization and for example a spectral regularization.",
                    "label": 0
                },
                {
                    "sent": "Did you compare your method for?",
                    "label": 0
                },
                {
                    "sent": "You, know, two together the spectral the signal square signal, singular values and the sparsity.",
                    "label": 0
                },
                {
                    "sent": "Organization together.",
                    "label": 0
                },
                {
                    "sent": "Yeah, first of all, I don't have the comparison so I cannot tell that the result an I'm interested in these kind of switch and when the regularization is not exactly a norm, which makes the story a bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "But the good thing about this is that there is nice duality between the primary problem and the dual problem and the deal problem has.",
                    "label": 0
                },
                {
                    "sent": "Then I form like.",
                    "label": 0
                },
                {
                    "sent": "So the the deal spectral regularization in the primal corresponds to bounding the.",
                    "label": 0
                },
                {
                    "sent": "The ratio of the fit using the dual norm so which has nice kind of integration.",
                    "label": 0
                },
                {
                    "sent": "So I like this, but of course the using other regularization is definitely we have to try.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's make it just the last question.",
                    "label": 0
                },
                {
                    "sent": "So with this, regularization will be able to.",
                    "label": 0
                },
                {
                    "sent": "Kernel is the method.",
                    "label": 0
                },
                {
                    "sent": "Sorry with the regularization that you have chosen, will it be able to?",
                    "label": 0
                },
                {
                    "sent": "Kernel is the method.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you want kernelized.",
                    "label": 0
                },
                {
                    "sent": "But yet that great question Anne.",
                    "label": 0
                },
                {
                    "sent": "How do I go back?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is known so.",
                    "label": 0
                },
                {
                    "sent": "There are three types of problems known, so the case I have talked here is prediction over matrices an we want to predict this color value.",
                    "label": 1
                },
                {
                    "sent": "For example, you can imagine different problem when you have input X, which is a vector and want to predict an output which is a vector and it is known I thermal this year.",
                    "label": 0
                },
                {
                    "sent": "People amid another.",
                    "label": 0
                },
                {
                    "sent": "Considered this gate and they showed that they can kernelized the input space and other people consider in this situation whether they can kernel eyes the input space in output space simultaneously.",
                    "label": 0
                },
                {
                    "sent": "But so far there seems to be no result positive result about simultaneously caramelizing.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean usually OK.",
                    "label": 0
                },
                {
                    "sent": "I mean here.",
                    "label": 0
                },
                {
                    "sent": "Probably the objective can be kernelized.",
                    "label": 0
                },
                {
                    "sent": "Maybe it can be, but usually I mean you take the Dark Ages norm of the function, right?",
                    "label": 0
                },
                {
                    "sent": "In the case of, you're just dealing with the functions functions which map to are.",
                    "label": 0
                },
                {
                    "sent": "Here you have, like regularizer is something like the absolute similar values.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so did you know with the regularizer?",
                    "label": 0
                },
                {
                    "sent": "I'm wondering whether some or you will be able to accomplish your method.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe you can do it for objective, but what happens to the regularizer?",
                    "label": 0
                },
                {
                    "sent": "Sorry, I didn't exactly get the last.",
                    "label": 0
                },
                {
                    "sent": "I mean maybe you can map these matrices into high dimensional space and probably you can define some kernel and kernel is your objective right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but what happens to your regularizer, I mean?",
                    "label": 0
                },
                {
                    "sent": "Your your regularizer which you mentioned as these absolute sum of singular values, yes, so that there is of course another approach, but it using a different regularization.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so the problem is could we kernelized this directly in this formulation an we don't know yet?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank speaker.",
                    "label": 0
                }
            ]
        }
    }
}