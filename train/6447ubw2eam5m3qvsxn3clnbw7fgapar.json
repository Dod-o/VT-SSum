{
    "id": "6447ubw2eam5m3qvsxn3clnbw7fgapar",
    "title": "Computational Limits for Matrix Completion",
    "info": {
        "author": [
            "Benjamin Weitz, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_weitz_computational/",
    "segmentation": [
        [
            "Hello, my name is Ben Weitz and this is joint work with Moritz Hardt who's in the audience.",
            "Romika and Prasad Raghavendra and we're going to talk about computational limits for matrix completion."
        ],
        [
            "Alright, so First off, what is matrix completion?",
            "Well, we heard about heard about it a little bit less talk.",
            "So the problem is that nature or nature has some rank K matrix.",
            "And here you should think of K as being much much smaller than NK is some constant, so there's some hidden ranking matrix out there in the world and rather than getting to see the whole matrix, you only get to see a partial matrix.",
            "So a lot of the entries have been erased or have question marks or something and you don't get to see what they are.",
            "And then the question.",
            "Is can you recover the original matrix M?",
            "Seeing only this partial matrix, or more generally what you want to do is you want given any partial matrix, you just want to find some matrix, some completion that has low rank.",
            "And so this is the problem that we're going to be talking about an actually before we go on.",
            "I want to just give a little bit of an alternative viewpoint like a geometric viewpoint of this problem."
        ],
        [
            "So if you have some completion of it, some some low rank completion of a partial matrix.",
            "Just by virtue of being low rank, that completion has some factorization.",
            "M equals UV transpose.",
            "And then how the entries of the original matrix are related to this factorization, while the entries are going to be dot products of the row and column vectors of this factorization.",
            "So really, an alternative way of think about the matrix completion problem is you can think of the entries of your partial matrix P as being dot product constraints on the vectors in the factorization services and then finding a lower rank completion of this partial matrix is exactly finding a set of vectors that satisfy these dot product constraints in a low dimensional space.",
            "So this is going to be the sort of geometric viewpoint we're going to take while looking at this problem, because it's going to sort of help with the proof intuition later on.",
            "And so then our goal in this work is to understand when this computational problem is tractable.",
            "So when are we going to be able to?",
            "So given some partial matrix, when is it going to be reasonable for us to expect to be able to compute some low rank factorization of it?",
            "Get guaranteed that some low rank factorization exists."
        ],
        [
            "Right, so there's been a lot of previous work into this problem, a huge amount.",
            "We saw some of it last talk.",
            "I'm not going to go through all of it.",
            "You can see you know some some work by Candis wrecked in town just for some particularly beautiful results an you know their works shows that you can solve this problem with sort of a natural convex, really solving some convex relaxation to find a completion.",
            "But you can only do this under two assumptions on the underlying.",
            "The hidden matrix, the Hidden Laurent matrix M. So the first of those assumptions is that the.",
            "The matrix is incoherent, and here incoherent means that the singular vectors of the matrix are not too correlated with any basis vector.",
            "So like for the for Netflix, this is like saying asking that your feature vectors be like fairly spread out and not be too small, like not be a too small of a class.",
            "The second assumption that they make is randomness, so that the entries of the matrix really are revealed, sort of in a random manner, so that you you get some good covering.",
            "Of the matrix and one of the questions we want to address in this work is, are these assumptions really required for tractability?",
            "So do you really need?",
            "If you have only a few of these, have only one of these assumptions.",
            "For example, is the problem harder?",
            "Is are there going to be algorithms to solve it?",
            "Um?"
        ],
        [
            "So that's sort of previous algorithm results.",
            "On the flip side, for previous hardness results were not aware of of that many, but the main.",
            "So the main one where I am aware of is in 1996 or in a Peters proved that for some constant K. Up Rank completion is NP hard, So what that means is that if someone hands you a partial matrix and they promise you that this matrix comes from some rank three, they actually took a rank three matrix and they deleted some of the entries and they gave you the completion.",
            "Computing that completion is going to is NP hard.",
            "It's it's as hard as three coloring a graph.",
            "And similarly, you know you learn involving theosis in 2013 proved that rank, rank, PSD completion is NP hard.",
            "So PSD completion is just you Additionally require that the completion that you output be positive semidefinite.",
            "So this similar, this problem is also NP hard for some constant for all constants K greater than equal to.",
            "What these results don't really address is that they say nothing about the approximation hardness of the problem, so these results are for exact completion.",
            "When you're exactly completing the partial matrix, they don't say anything about what.",
            "If you only want to output a matrix that's close to the observed entries that you've seen?"
        ],
        [
            "And so, so there are sort of two 2 natural ways you can talk about approximating the matrix completion problem.",
            "One I already alluded to, which is entry approximation.",
            "So rather than.",
            "Requiring that your matrix requiring the completion that you compute be an exact completion of the partial matrix, you asked only that its entries are close to the entries of the partial matrix.",
            "Work close can mean.",
            "I mean often means like squared error and you want that to be small.",
            "And then another sort of approximation concept that you can take when looking at matrix completion is rank approximation.",
            "So if someone gives you this partial matrix and they guarantee to you that there is a rank completion, now you want your algorithm not to just output a rank a completion, but maybe like a rank 10K completion, or like a rank 100K completion or something like that.",
            "So another question, we wanted justice.",
            "Does matrix completion remain hard even when you allow your algorithm some leeway in these in these two senses?",
            "So if you allow it to only approximate the partially reveal the partial matrix and you allow it to output a matrix of higher rank?"
        ],
        [
            "Right, so our contribution to the to these questions is that under a reasonable complexity assumption, which I'll get into later.",
            "We have the following two answers, so one is that even if the underlying matrix is incoherent, it's going to be NP hard to sorry, not NPR, but it's going to be hard to find any low rank completion when the entries are not revealed randomly, so they are revealed adversarially like in a in a reduction.",
            "And so when that's the case, it's going to be hard to compute a completion.",
            "And Secondly, even if you allow your algorithm some leeway in the two senses that I talked about on the last slide, it's going to be hard to find any completion even when you allow leeway in the rank and leeway in the entry approximation.",
            "So and to our knowledge these these hardness results are among the very first approximation hardness results for matrix completion.",
            "Right, so I said under a reasonable complexity assumption.",
            "What do I mean when I say reasonable complexity assumption?",
            "I don't.",
            "Actually, unfortunately, I don't actually mean P does not equal NP.",
            "We're going to have to rely on the conjectured hardness of a different computational problem.",
            "The graph coloring problem, which I'll talk about now."
        ],
        [
            "So let's so recall that a graph coloring is just an assignment of colors to vertices such that no edges monochromatic, so every edge touches the vertices of different colors.",
            "Then you can define the QR graph coloring problem to be given AK colorable graph.",
            "Just find an independent set of fractional size 1 / R. So this this problem is closely related to the problem of our coloring AK colorable graph because if you have an R coloring of a graph, since the color classes are independent sets, at least one of them has to be a fractional size, at least 1 / R. So this problem is somewhat easier than our coloring K colorable graph, but Even so it's conjectured to be quite hard because sort of despite 30 years of a lot of smart people thinking about it, it's still very little progress has been made, so even even if you have a three colorable graph, sort of the best, the best results known are like super constant, so you can't even find a constant size.",
            "Independent set in a three colorable graph.",
            "An I should mention that there are also some hardness results.",
            "None of them are NP hardness, but under something like the unique Games conjecture, it's known that this is this is hard under that conjecture.",
            "So sorry for any constant constant R and also for slightly super constant R. Um?",
            "Right, so this is sort of the problem.",
            "We're going to be reducing from this is the problem that we conjectured to be hard, and if this problem there has been conjectured to be hard, and if it is, then the matrix completion problem is similarly hard."
        ],
        [
            "OK, so this is our main theorem, so if you assume that the QR graph coloring problem is intractable for any two constants, K&R, then for constants CCR there's going to be no polynomial time algorithm such that given a partial matrix that has a completion with the following properties.",
            "So the completion is bounded coefficients, so off its coefficients are bounded by C. It has low rank in the sense that the rank is at most K and it has constant coherence.",
            "So not only can you not find this matrix, but you can't even find a matrix approximating this matrix in the following sense is so one has bounded coefficients.",
            "Two, it's a good rank approximation, so its rank is at most some constant R, which in principle could be much larger than K. Maybe like 10K or something.",
            "And Secondly, if you can't find one that has good entry approximation, so this is some bound on the squared, the squared error of the.",
            "Of the matrix with respect to the revealed entries of the partial matrix.",
            "So this term here is the two norm of the revealed entries, which is sort of the natural error to be looking at, because if you were allowed to output more like asymptotically more than the two norm of the revealed entries, you could just output all zeros matrix and then that would trivially be matrix of low rank.",
            "Alright, so the sort of the rest of the talk I'm going to try to give a little bit of an outline into how this theorem is proved, so I'll try to give you a sense of what the reduction is like and how it works so."
        ],
        [
            "Let's just recall the geometric interpretation of matrix completion that I talked about, so I want you to think of an instance of matrix completion is just being a list of dot product constraints, and I want to think of finding a completion is finding a set of vectors satisfying these constraints in a low dimensional space, and So what I want to do with my reduction is I want to set it up so that.",
            "The solution to these dot product constraints solve some NP hard problem, so in particular it finds an independent set in the K colorable graph.",
            "Right, so this is what the reduction is going to be.",
            "It's not terribly complicated, so you have some graph.",
            "This one and what you're going to do is all the diagonal entries of the matrix are going to be ones.",
            "An every IJ entry where there's an edge in the graph you're going to set to 0, and all of the other.",
            "All the other entries in the Matrix are going to be not filled in, so they're going to be question marks.",
            "So what this means is that.",
            "When you find the vectors so every vertex of the graph is associated with two vectors in a completion and those vectors when there's an edge, they need to be perpendicular, so they need to be pointing in like very, very different directions.",
            "And so there's sort of two things we need to prove.",
            "We need to prove that some coloring of this graph gives a good completion of this partial matrix, and also that a good completion of this partial matrix lets us find an independent set in the graph."
        ],
        [
            "So in the first case, I don't know how well you can see the colors, but do we have a three colorable graph here?",
            "An what we're going to do is we're just going to set it up so that you put.",
            "So there's K colors, so I'm going to make a K dimensional solution where you put a vector.",
            "Just pick an orthonormal basis of RK, and then every vertex that's the earth color.",
            "Just make just make its vector point in the right direction.",
            "So here you can see like the blue vectors, the blue vertex is a vector pointing in the Z direction.",
            "So does this one and then the green one is pointing in the X and the red ones pointing in the Y.",
            "And then since the graph is colorable, like every edge, there's going to be they're going into different basis directions, so they're going to orthonormal, so this is going to satisfy the constraints of the partial matrix.",
            "Why does this have constant coherence?",
            "Well, it turns out that the coherence of the solution is proportional to.",
            "Sort of, the is inversely proportional to the smallest color class.",
            "But dude like standard tricks and coloring, you can sort of assume that the color classes are all the same size.",
            "So you just copy the graph a bunch of times and then it has a balanced coloring.",
            "So now for the op."
        ],
        [
            "Direction the soundness case if you have some completion and let's recall that a completion is a set of vectors in R to R, so you have some rank.",
            "Our completion this set of vectors that satisfies these constraints and you want to find a large independent set.",
            "So ideally you would do it just like the opposite of incompleteness case, and you would hope that these vectors just all lie in.",
            "Are different directions or K different directions and you could just decode a color for every direction.",
            "Unfortunately this is not going to be true, so instead we're going to do is we're going to find an independent set by just finding a direction that many vectors are close to so many vectors are correlated with this direction and then that direction is going to be an independent set.",
            "So how do we do that?",
            "We do it the same way computer scientists do.",
            "Anything.",
            "We do it randomly.",
            "So what you do is you just pick a random direction.",
            "And then you're going to pick all of the vectors that lie in a cone around that direction.",
            "So for example here.",
            "Sorry, you're going to pick your going to pick all the vertices, both of whose vectors UI and UI and VI lie in this cone.",
            "Um?",
            "Why do you want to do that?",
            "Well, you want to do that, because this cone is going to be because."
        ],
        [
            "It's the cone has angular radius like \u03c0 / 2 angular diameter \u03c0 / 2.",
            "Any two vectors that lie inside the cone can't be perpendicular, and if two vectors in the cone can't be perpendicular, that means there can't be an edge between those two indices because any two indices that have an edge have to be perpendicular.",
            "So these vertices that are chosen by this cone are going to be an independent set, so that's good.",
            "But now we have to argue that this independence is large and the way we argue that is basically I don't have alot of time, but I'll just say that.",
            "You have to argue it because this is a constant dimensional space.",
            "You have to argue that the that both UI and VI lie in the set with some constant probability, and if you get that then you know that in expectation this is going to have a constant number of the vertices.",
            "And so to get that you envy both line it with constant probability.",
            "You have to argue about the angle between UI and VI.",
            "You have to show that it's less than \u03c0 / 2, and sort of.",
            "That's implied by this constraint here.",
            "And the fact that it's that low dimensional matrices, sorry, low rank matrices have factorizations that are that have bounds on the norms.",
            "They're like.",
            "Well, they have factorizations that are well behaved, so they have.",
            "Bounded norms, and then that sort of implies that the angle is bounded and that will imply that the there's a constant dimension that there's a constant number of them in the independent set.",
            "Oak."
        ],
        [
            "So that's sort of it.",
            "That's as much of an outline as as I can give, so this work is sort of established.",
            "The sum of the first approximation hardness results for matrix completion, so in particular we showed that if entries are revealed adversarially the underlying matrix being incoherent doesn't really help you.",
            "If you're an algorithm designer, it's still going to be NP hard.",
            "You really need if you want to, you can do incoherence plus randomness, but you can't just do incoherence.",
            "Scuse me and then Secondly, even when there is a low rank completion, not only is it hard to find that lowering completion, but it's also hard to find any completion that approximates that completion in either entry in either the entry sense or the rank sense.",
            "And then."
        ],
        [
            "Some open problems, so there's still a huge amount of work to be done on this on this sort of frontier, and there's so much we don't know.",
            "So we showed that incoherence without randomness is hard.",
            "Is it true that randomness without incoherence is hard?",
            "Or maybe randomness alone is easy?",
            "We know that randomness plus incoherence is easy, and so another open question would be to remove the coloring hardness and just rely on only NP hardness.",
            "Or Alternatively, we show the hardness for any pair of constants RNC?",
            "What about if you have our grow slightly with N?",
            "Like maybe log log log, log in or something?",
            "Is it still hard?",
            "And finally, we know that coloring is easy for K = 3 and are some Poly some epsilon for this epsilon is completion.",
            "Also easy for these parameters, regardless of incoherence or randomness?",
            "OK, and that's it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, my name is Ben Weitz and this is joint work with Moritz Hardt who's in the audience.",
                    "label": 0
                },
                {
                    "sent": "Romika and Prasad Raghavendra and we're going to talk about computational limits for matrix completion.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so First off, what is matrix completion?",
                    "label": 1
                },
                {
                    "sent": "Well, we heard about heard about it a little bit less talk.",
                    "label": 1
                },
                {
                    "sent": "So the problem is that nature or nature has some rank K matrix.",
                    "label": 0
                },
                {
                    "sent": "And here you should think of K as being much much smaller than NK is some constant, so there's some hidden ranking matrix out there in the world and rather than getting to see the whole matrix, you only get to see a partial matrix.",
                    "label": 0
                },
                {
                    "sent": "So a lot of the entries have been erased or have question marks or something and you don't get to see what they are.",
                    "label": 1
                },
                {
                    "sent": "And then the question.",
                    "label": 0
                },
                {
                    "sent": "Is can you recover the original matrix M?",
                    "label": 0
                },
                {
                    "sent": "Seeing only this partial matrix, or more generally what you want to do is you want given any partial matrix, you just want to find some matrix, some completion that has low rank.",
                    "label": 0
                },
                {
                    "sent": "And so this is the problem that we're going to be talking about an actually before we go on.",
                    "label": 0
                },
                {
                    "sent": "I want to just give a little bit of an alternative viewpoint like a geometric viewpoint of this problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you have some completion of it, some some low rank completion of a partial matrix.",
                    "label": 0
                },
                {
                    "sent": "Just by virtue of being low rank, that completion has some factorization.",
                    "label": 0
                },
                {
                    "sent": "M equals UV transpose.",
                    "label": 0
                },
                {
                    "sent": "And then how the entries of the original matrix are related to this factorization, while the entries are going to be dot products of the row and column vectors of this factorization.",
                    "label": 0
                },
                {
                    "sent": "So really, an alternative way of think about the matrix completion problem is you can think of the entries of your partial matrix P as being dot product constraints on the vectors in the factorization services and then finding a lower rank completion of this partial matrix is exactly finding a set of vectors that satisfy these dot product constraints in a low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be the sort of geometric viewpoint we're going to take while looking at this problem, because it's going to sort of help with the proof intuition later on.",
                    "label": 1
                },
                {
                    "sent": "And so then our goal in this work is to understand when this computational problem is tractable.",
                    "label": 0
                },
                {
                    "sent": "So when are we going to be able to?",
                    "label": 0
                },
                {
                    "sent": "So given some partial matrix, when is it going to be reasonable for us to expect to be able to compute some low rank factorization of it?",
                    "label": 0
                },
                {
                    "sent": "Get guaranteed that some low rank factorization exists.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so there's been a lot of previous work into this problem, a huge amount.",
                    "label": 0
                },
                {
                    "sent": "We saw some of it last talk.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through all of it.",
                    "label": 0
                },
                {
                    "sent": "You can see you know some some work by Candis wrecked in town just for some particularly beautiful results an you know their works shows that you can solve this problem with sort of a natural convex, really solving some convex relaxation to find a completion.",
                    "label": 0
                },
                {
                    "sent": "But you can only do this under two assumptions on the underlying.",
                    "label": 1
                },
                {
                    "sent": "The hidden matrix, the Hidden Laurent matrix M. So the first of those assumptions is that the.",
                    "label": 0
                },
                {
                    "sent": "The matrix is incoherent, and here incoherent means that the singular vectors of the matrix are not too correlated with any basis vector.",
                    "label": 1
                },
                {
                    "sent": "So like for the for Netflix, this is like saying asking that your feature vectors be like fairly spread out and not be too small, like not be a too small of a class.",
                    "label": 0
                },
                {
                    "sent": "The second assumption that they make is randomness, so that the entries of the matrix really are revealed, sort of in a random manner, so that you you get some good covering.",
                    "label": 1
                },
                {
                    "sent": "Of the matrix and one of the questions we want to address in this work is, are these assumptions really required for tractability?",
                    "label": 1
                },
                {
                    "sent": "So do you really need?",
                    "label": 0
                },
                {
                    "sent": "If you have only a few of these, have only one of these assumptions.",
                    "label": 0
                },
                {
                    "sent": "For example, is the problem harder?",
                    "label": 0
                },
                {
                    "sent": "Is are there going to be algorithms to solve it?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's sort of previous algorithm results.",
                    "label": 0
                },
                {
                    "sent": "On the flip side, for previous hardness results were not aware of of that many, but the main.",
                    "label": 0
                },
                {
                    "sent": "So the main one where I am aware of is in 1996 or in a Peters proved that for some constant K. Up Rank completion is NP hard, So what that means is that if someone hands you a partial matrix and they promise you that this matrix comes from some rank three, they actually took a rank three matrix and they deleted some of the entries and they gave you the completion.",
                    "label": 0
                },
                {
                    "sent": "Computing that completion is going to is NP hard.",
                    "label": 0
                },
                {
                    "sent": "It's it's as hard as three coloring a graph.",
                    "label": 0
                },
                {
                    "sent": "And similarly, you know you learn involving theosis in 2013 proved that rank, rank, PSD completion is NP hard.",
                    "label": 0
                },
                {
                    "sent": "So PSD completion is just you Additionally require that the completion that you output be positive semidefinite.",
                    "label": 1
                },
                {
                    "sent": "So this similar, this problem is also NP hard for some constant for all constants K greater than equal to.",
                    "label": 0
                },
                {
                    "sent": "What these results don't really address is that they say nothing about the approximation hardness of the problem, so these results are for exact completion.",
                    "label": 1
                },
                {
                    "sent": "When you're exactly completing the partial matrix, they don't say anything about what.",
                    "label": 0
                },
                {
                    "sent": "If you only want to output a matrix that's close to the observed entries that you've seen?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, so there are sort of two 2 natural ways you can talk about approximating the matrix completion problem.",
                    "label": 0
                },
                {
                    "sent": "One I already alluded to, which is entry approximation.",
                    "label": 1
                },
                {
                    "sent": "So rather than.",
                    "label": 0
                },
                {
                    "sent": "Requiring that your matrix requiring the completion that you compute be an exact completion of the partial matrix, you asked only that its entries are close to the entries of the partial matrix.",
                    "label": 1
                },
                {
                    "sent": "Work close can mean.",
                    "label": 0
                },
                {
                    "sent": "I mean often means like squared error and you want that to be small.",
                    "label": 1
                },
                {
                    "sent": "And then another sort of approximation concept that you can take when looking at matrix completion is rank approximation.",
                    "label": 0
                },
                {
                    "sent": "So if someone gives you this partial matrix and they guarantee to you that there is a rank completion, now you want your algorithm not to just output a rank a completion, but maybe like a rank 10K completion, or like a rank 100K completion or something like that.",
                    "label": 0
                },
                {
                    "sent": "So another question, we wanted justice.",
                    "label": 0
                },
                {
                    "sent": "Does matrix completion remain hard even when you allow your algorithm some leeway in these in these two senses?",
                    "label": 1
                },
                {
                    "sent": "So if you allow it to only approximate the partially reveal the partial matrix and you allow it to output a matrix of higher rank?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so our contribution to the to these questions is that under a reasonable complexity assumption, which I'll get into later.",
                    "label": 1
                },
                {
                    "sent": "We have the following two answers, so one is that even if the underlying matrix is incoherent, it's going to be NP hard to sorry, not NPR, but it's going to be hard to find any low rank completion when the entries are not revealed randomly, so they are revealed adversarially like in a in a reduction.",
                    "label": 0
                },
                {
                    "sent": "And so when that's the case, it's going to be hard to compute a completion.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, even if you allow your algorithm some leeway in the two senses that I talked about on the last slide, it's going to be hard to find any completion even when you allow leeway in the rank and leeway in the entry approximation.",
                    "label": 0
                },
                {
                    "sent": "So and to our knowledge these these hardness results are among the very first approximation hardness results for matrix completion.",
                    "label": 1
                },
                {
                    "sent": "Right, so I said under a reasonable complexity assumption.",
                    "label": 0
                },
                {
                    "sent": "What do I mean when I say reasonable complexity assumption?",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "Actually, unfortunately, I don't actually mean P does not equal NP.",
                    "label": 0
                },
                {
                    "sent": "We're going to have to rely on the conjectured hardness of a different computational problem.",
                    "label": 0
                },
                {
                    "sent": "The graph coloring problem, which I'll talk about now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's so recall that a graph coloring is just an assignment of colors to vertices such that no edges monochromatic, so every edge touches the vertices of different colors.",
                    "label": 1
                },
                {
                    "sent": "Then you can define the QR graph coloring problem to be given AK colorable graph.",
                    "label": 0
                },
                {
                    "sent": "Just find an independent set of fractional size 1 / R. So this this problem is closely related to the problem of our coloring AK colorable graph because if you have an R coloring of a graph, since the color classes are independent sets, at least one of them has to be a fractional size, at least 1 / R. So this problem is somewhat easier than our coloring K colorable graph, but Even so it's conjectured to be quite hard because sort of despite 30 years of a lot of smart people thinking about it, it's still very little progress has been made, so even even if you have a three colorable graph, sort of the best, the best results known are like super constant, so you can't even find a constant size.",
                    "label": 1
                },
                {
                    "sent": "Independent set in a three colorable graph.",
                    "label": 0
                },
                {
                    "sent": "An I should mention that there are also some hardness results.",
                    "label": 1
                },
                {
                    "sent": "None of them are NP hardness, but under something like the unique Games conjecture, it's known that this is this is hard under that conjecture.",
                    "label": 0
                },
                {
                    "sent": "So sorry for any constant constant R and also for slightly super constant R. Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so this is sort of the problem.",
                    "label": 0
                },
                {
                    "sent": "We're going to be reducing from this is the problem that we conjectured to be hard, and if this problem there has been conjectured to be hard, and if it is, then the matrix completion problem is similarly hard.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is our main theorem, so if you assume that the QR graph coloring problem is intractable for any two constants, K&R, then for constants CCR there's going to be no polynomial time algorithm such that given a partial matrix that has a completion with the following properties.",
                    "label": 1
                },
                {
                    "sent": "So the completion is bounded coefficients, so off its coefficients are bounded by C. It has low rank in the sense that the rank is at most K and it has constant coherence.",
                    "label": 0
                },
                {
                    "sent": "So not only can you not find this matrix, but you can't even find a matrix approximating this matrix in the following sense is so one has bounded coefficients.",
                    "label": 1
                },
                {
                    "sent": "Two, it's a good rank approximation, so its rank is at most some constant R, which in principle could be much larger than K. Maybe like 10K or something.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, if you can't find one that has good entry approximation, so this is some bound on the squared, the squared error of the.",
                    "label": 0
                },
                {
                    "sent": "Of the matrix with respect to the revealed entries of the partial matrix.",
                    "label": 0
                },
                {
                    "sent": "So this term here is the two norm of the revealed entries, which is sort of the natural error to be looking at, because if you were allowed to output more like asymptotically more than the two norm of the revealed entries, you could just output all zeros matrix and then that would trivially be matrix of low rank.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the sort of the rest of the talk I'm going to try to give a little bit of an outline into how this theorem is proved, so I'll try to give you a sense of what the reduction is like and how it works so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's just recall the geometric interpretation of matrix completion that I talked about, so I want you to think of an instance of matrix completion is just being a list of dot product constraints, and I want to think of finding a completion is finding a set of vectors satisfying these constraints in a low dimensional space, and So what I want to do with my reduction is I want to set it up so that.",
                    "label": 0
                },
                {
                    "sent": "The solution to these dot product constraints solve some NP hard problem, so in particular it finds an independent set in the K colorable graph.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is what the reduction is going to be.",
                    "label": 0
                },
                {
                    "sent": "It's not terribly complicated, so you have some graph.",
                    "label": 0
                },
                {
                    "sent": "This one and what you're going to do is all the diagonal entries of the matrix are going to be ones.",
                    "label": 0
                },
                {
                    "sent": "An every IJ entry where there's an edge in the graph you're going to set to 0, and all of the other.",
                    "label": 0
                },
                {
                    "sent": "All the other entries in the Matrix are going to be not filled in, so they're going to be question marks.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that.",
                    "label": 0
                },
                {
                    "sent": "When you find the vectors so every vertex of the graph is associated with two vectors in a completion and those vectors when there's an edge, they need to be perpendicular, so they need to be pointing in like very, very different directions.",
                    "label": 0
                },
                {
                    "sent": "And so there's sort of two things we need to prove.",
                    "label": 0
                },
                {
                    "sent": "We need to prove that some coloring of this graph gives a good completion of this partial matrix, and also that a good completion of this partial matrix lets us find an independent set in the graph.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the first case, I don't know how well you can see the colors, but do we have a three colorable graph here?",
                    "label": 0
                },
                {
                    "sent": "An what we're going to do is we're just going to set it up so that you put.",
                    "label": 0
                },
                {
                    "sent": "So there's K colors, so I'm going to make a K dimensional solution where you put a vector.",
                    "label": 0
                },
                {
                    "sent": "Just pick an orthonormal basis of RK, and then every vertex that's the earth color.",
                    "label": 0
                },
                {
                    "sent": "Just make just make its vector point in the right direction.",
                    "label": 0
                },
                {
                    "sent": "So here you can see like the blue vectors, the blue vertex is a vector pointing in the Z direction.",
                    "label": 0
                },
                {
                    "sent": "So does this one and then the green one is pointing in the X and the red ones pointing in the Y.",
                    "label": 0
                },
                {
                    "sent": "And then since the graph is colorable, like every edge, there's going to be they're going into different basis directions, so they're going to orthonormal, so this is going to satisfy the constraints of the partial matrix.",
                    "label": 0
                },
                {
                    "sent": "Why does this have constant coherence?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that the coherence of the solution is proportional to.",
                    "label": 0
                },
                {
                    "sent": "Sort of, the is inversely proportional to the smallest color class.",
                    "label": 0
                },
                {
                    "sent": "But dude like standard tricks and coloring, you can sort of assume that the color classes are all the same size.",
                    "label": 0
                },
                {
                    "sent": "So you just copy the graph a bunch of times and then it has a balanced coloring.",
                    "label": 0
                },
                {
                    "sent": "So now for the op.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Direction the soundness case if you have some completion and let's recall that a completion is a set of vectors in R to R, so you have some rank.",
                    "label": 0
                },
                {
                    "sent": "Our completion this set of vectors that satisfies these constraints and you want to find a large independent set.",
                    "label": 1
                },
                {
                    "sent": "So ideally you would do it just like the opposite of incompleteness case, and you would hope that these vectors just all lie in.",
                    "label": 1
                },
                {
                    "sent": "Are different directions or K different directions and you could just decode a color for every direction.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately this is not going to be true, so instead we're going to do is we're going to find an independent set by just finding a direction that many vectors are close to so many vectors are correlated with this direction and then that direction is going to be an independent set.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "We do it the same way computer scientists do.",
                    "label": 0
                },
                {
                    "sent": "Anything.",
                    "label": 0
                },
                {
                    "sent": "We do it randomly.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you just pick a random direction.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to pick all of the vectors that lie in a cone around that direction.",
                    "label": 0
                },
                {
                    "sent": "So for example here.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you're going to pick your going to pick all the vertices, both of whose vectors UI and UI and VI lie in this cone.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Why do you want to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, you want to do that, because this cone is going to be because.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the cone has angular radius like \u03c0 / 2 angular diameter \u03c0 / 2.",
                    "label": 0
                },
                {
                    "sent": "Any two vectors that lie inside the cone can't be perpendicular, and if two vectors in the cone can't be perpendicular, that means there can't be an edge between those two indices because any two indices that have an edge have to be perpendicular.",
                    "label": 0
                },
                {
                    "sent": "So these vertices that are chosen by this cone are going to be an independent set, so that's good.",
                    "label": 0
                },
                {
                    "sent": "But now we have to argue that this independence is large and the way we argue that is basically I don't have alot of time, but I'll just say that.",
                    "label": 0
                },
                {
                    "sent": "You have to argue it because this is a constant dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You have to argue that the that both UI and VI lie in the set with some constant probability, and if you get that then you know that in expectation this is going to have a constant number of the vertices.",
                    "label": 1
                },
                {
                    "sent": "And so to get that you envy both line it with constant probability.",
                    "label": 0
                },
                {
                    "sent": "You have to argue about the angle between UI and VI.",
                    "label": 0
                },
                {
                    "sent": "You have to show that it's less than \u03c0 / 2, and sort of.",
                    "label": 0
                },
                {
                    "sent": "That's implied by this constraint here.",
                    "label": 0
                },
                {
                    "sent": "And the fact that it's that low dimensional matrices, sorry, low rank matrices have factorizations that are that have bounds on the norms.",
                    "label": 0
                },
                {
                    "sent": "They're like.",
                    "label": 0
                },
                {
                    "sent": "Well, they have factorizations that are well behaved, so they have.",
                    "label": 0
                },
                {
                    "sent": "Bounded norms, and then that sort of implies that the angle is bounded and that will imply that the there's a constant dimension that there's a constant number of them in the independent set.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's sort of it.",
                    "label": 0
                },
                {
                    "sent": "That's as much of an outline as as I can give, so this work is sort of established.",
                    "label": 0
                },
                {
                    "sent": "The sum of the first approximation hardness results for matrix completion, so in particular we showed that if entries are revealed adversarially the underlying matrix being incoherent doesn't really help you.",
                    "label": 1
                },
                {
                    "sent": "If you're an algorithm designer, it's still going to be NP hard.",
                    "label": 0
                },
                {
                    "sent": "You really need if you want to, you can do incoherence plus randomness, but you can't just do incoherence.",
                    "label": 0
                },
                {
                    "sent": "Scuse me and then Secondly, even when there is a low rank completion, not only is it hard to find that lowering completion, but it's also hard to find any completion that approximates that completion in either entry in either the entry sense or the rank sense.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some open problems, so there's still a huge amount of work to be done on this on this sort of frontier, and there's so much we don't know.",
                    "label": 1
                },
                {
                    "sent": "So we showed that incoherence without randomness is hard.",
                    "label": 0
                },
                {
                    "sent": "Is it true that randomness without incoherence is hard?",
                    "label": 1
                },
                {
                    "sent": "Or maybe randomness alone is easy?",
                    "label": 0
                },
                {
                    "sent": "We know that randomness plus incoherence is easy, and so another open question would be to remove the coloring hardness and just rely on only NP hardness.",
                    "label": 1
                },
                {
                    "sent": "Or Alternatively, we show the hardness for any pair of constants RNC?",
                    "label": 0
                },
                {
                    "sent": "What about if you have our grow slightly with N?",
                    "label": 0
                },
                {
                    "sent": "Like maybe log log log, log in or something?",
                    "label": 1
                },
                {
                    "sent": "Is it still hard?",
                    "label": 0
                },
                {
                    "sent": "And finally, we know that coloring is easy for K = 3 and are some Poly some epsilon for this epsilon is completion.",
                    "label": 1
                },
                {
                    "sent": "Also easy for these parameters, regardless of incoherence or randomness?",
                    "label": 0
                },
                {
                    "sent": "OK, and that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}