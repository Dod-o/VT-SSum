{
    "id": "mt2zr2tfg77vxjuxw65cghxqmmjoxz3q",
    "title": "Primal Sparse Max-Margin Markov Networks",
    "info": {
        "author": [
            "Jun Zhu, Department of Computer Science and Technology, Tsinghua University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_zhu_smmmn/",
    "segmentation": [
        [
            "OK, so so this talk is about.",
            "Primus past Max Mark Mark Network.",
            "It basically is so hard to do feature selection in the next market market networks so this is joint work with directly at the same you and my otherwise support."
        ],
        [
            "So here is the content in this talk first, which tells the structure prediction problem and there's models that we choose the best story learning algorithm for this primer smart model and we choose some impairment."
        ],
        [
            "Results OK, so to compare with, we start with the classification problem.",
            "So the linear basic attacker set of ideation is training data.",
            "Diagram output predicted function.",
            "Which is mapping from input space to output space.",
            "For testing example you assign or best class level.",
            "So here is 1 example of OCR OCR inputs, human written characters and output English character.",
            "So competition is one of the most important supervised learning tasks.",
            "There have been many methods proposed to the classifiers, so here's one popular example with support vector machine.",
            "So this is a multi multi class classification problem formulated by quorum and single in 2001, so this year constrained optimization problem.",
            "It can be equivalent rating as a recognized.",
            "Causation problem so here the.",
            "Be sure is a pound of the empirical loss of this model."
        ],
        [
            "So, but the problem with cloud cache model is that it ignores the dependence among among data.",
            "So for example in the OCR, if we treat the characters independently.",
            "The cupcake model you are difficult to distinguish this character and this one.",
            "So to improve the predicted predicted accuracy, we want to employ more information about the data.",
            "So first.",
            "At the input side, windows that they put a character it's you know word sequence.",
            "So we can check some content dependently feature features.",
            "So discriminative model like SM and the models average dose can incorporate this structure.",
            "Information structure input feature so.",
            "But at the output side the prediction also we also have some structure, for example in OCR.",
            "The prediction is also in sequence.",
            "So structured prediction is learning from work that that tried to incorporate the structural dependency in the output side.",
            "So to produce global optimal prediction.",
            "So the best setup is that it also needs a set of ID training samples.",
            "The output structure predict function for each input output, a structure prediction so, but here it is.",
            "It input each sample is a pair of.",
            "Zion, why I both have some structures.",
            "For example, you know sociology puts the output sequence.",
            "Here's another example.",
            "Or imagination.",
            "Your inputs.",
            "Maybe it's a metric.",
            "Essentially the feature worked, so I put it.",
            "Also a metric of."
        ],
        [
            "Class level.",
            "So, so this is supposed to let pizza true distribution.",
            "The risk of a particular model is the expectation of data error data error is a non negative loss function which measures how accurate is your prediction is compared to the truth.",
            "For example harmless Meyers actually basically counts the number of mismatches in the prediction so but.",
            "You are way to we do not know the true distribution, so we use empirical risk.",
            "Empirical risk based on the training data to approximate the risk.",
            "So for finite number of training data to avoid overfitting, which you had already used the regularizer to.",
            "To Pharos no simple model, so minimize this records imperious case noise records and imperious communalization from work.",
            "So is this general framework we can define define different actual model and use different regularizers we can get."
        ],
        [
            "Model so is this talk.",
            "I will focus on the mixed Mark Mark Network, so the best seller hub is is that follows.",
            "So suppose that for its discriminant function, which is a mapping from the product space of input output to the real number space.",
            "So here we consider the linear form.",
            "Basically this.",
            "Function is a linear combination of a set of feature functions and W is the upper middle weight.",
            "So keeping a define a particular rule and also defines the margin which is the difference between the refs called difference between the truth and the best run up production.",
            "So for this model we can prove that this this actually is a pound of the empirical loss.",
            "So maximizing Mac network.",
            "Optimal model by solving this minimalization problem, that's really it's regulated pendulum azatian.",
            "So again like swim, this problem can be written as a constraint optimization problem.",
            "So here we use the L1 norm instead of two norm.",
            "We get one answer which is the primary sparse model orange juice."
        ],
        [
            "So let's see the difference between L1 norm and L2 norm.",
            "So we know that this regular minimalization problem can be written as this content optimization problem.",
            "So so basically this constraint for different normal define different subset subset of visible physical weight.",
            "So it basically defined can be illustrated by the projection.",
            "So for our turn on the Facebook.",
            "In the face of subspace subset, is error two ball and for their right now it's it's it's share.",
            "So if we know model for into this in this region there no projection is needed.",
            "So so here we just consider the points outside this physical subset.",
            "So let's consider first, so all sent so we can partition it into surgery.",
            "3 different regions.",
            "So first point in this series, like Pyp, the projection to the airline bar to bar.",
            "Do not have zero coordinate.",
            "So however, the point is this region, for example P2.",
            "The second coordinate is very small.",
            "The projection to their rainbow.",
            "We have 4 zero coordinate, so but the projection to their two or two, but that doesn't have zero coordinate.",
            "If if the this pointer doesn't contain zero code in it.",
            "So this is the difference.",
            "So so it suggests for everyone normal regular, so we can.",
            "Get a sparse at Med so.",
            "We call this the type of spicy.",
            "Does primal specific.",
            "Let's give the earlier feel.",
            "Input features have non zero weights.",
            "So there are two normal condition can shrink shrink model weights by do this by doing this projection.",
            "However, it cannot implicitly discard some feature setting there with 203 calls.",
            "This kind of sparsity as shooter promise positive.",
            "The third kind of sparsity is it too spicy body do sparsity for field.",
            "Lagrangian multipliers are non 0.",
            "So here's why.",
            "Example for linear makes money market network.",
            "By the component complementary slackness condition, we can we can get this equation.",
            "So basically only for the active constraints we have non zero lag or multiplier multipliers.",
            "So so both Erlander 2M3 and do sparse.",
            "However L1 norm, I'm sorry is the primary sparse next market network.",
            "So now we will see how to solve the how to learn this at one M3."
        ],
        [
            "So the first method is cutting parameter.",
            "So this this method has been successfully used to the election or three.",
            "So the basic procedure is that it actively construct a subset of constraint and the optimized with respect to the contracting subset.",
            "So here's the algorithm outlined for the airwaves.",
            "Basically, for the first able to construct the the select the subset of constraints it actually solved for a loss argument augmented prediction problem.",
            "To find the white Whitehead.",
            "So this problem this problem can be efficiently solved for loss loss function data.",
            "Can be decomposed into small parts.",
            "So basically we can use Max product algorithm.",
            "So for the sex that we choose optimize to solve a subproblem to optimize with respect to the current working set.",
            "So this problem can be formulated as a linear programming problem.",
            "So here is the key insight.",
            "So is this is this constraint of one day?",
            "Probably we can lead to W equal to mu minus.",
            "We both knew and they are nonnegative vectors.",
            "So without loss of generality, we can assume that at most one of four Mckelvey case non 0.",
            "Otherwise we can stop check both by a smaller way without changing the value of W. So based on this observation this problem can be equivalently taxes for the one norm is equal to mu plus we.",
            "So now we can use the linear linear programming solver to solve this problem.",
            "So since the constraint I territory added so this solution will be more and more accurate, so algorithm terminates if no change in the working set.",
            "Bezier no constraints selected."
        ],
        [
            "So the sector second method is the projects upgraded.",
            "Based on this constraint optimization formulation.",
            "So here is the definition of the subgradient.",
            "So basically for for differentiable function.",
            "The subgradient is actually the gradient.",
            "So far why I'm sorry and the algorithm authorized is this one, so it's very simple.",
            "Basically is the greatest distance method, but with the additional step to protect the.",
            "Moderate to the to the physical subset.",
            "Physical substance subset.",
            "So for the first step to compute the subgradient windows that.",
            "For each example, this.",
            "This this part is piecewise linear.",
            "It can be returned this form.",
            "So this is this graph shows the basic shape of this function.",
            "So for this point there not added the turning point that actually this function function is differentiable, so so the subgradient actually is is according to for example, for this point is gradient operator Y 2.",
            "So, but for the turning point.",
            "The subcode it can be any vector in between all of these two little guardians, so, but in general we can.",
            "We can choose this.",
            "GI is is a one of the subgradients of hi.",
            "So two computers this upgrading.",
            "We also need to solve for the loss augmented prediction problem.",
            "We can be efficient now.",
            "So for the second step, to do the projection, we also have efficient algorithm which is actually scale linearly with in terms of the number of features.",
            "If the Cardinal sparse we can actually get a log scale capacity.",
            "So so about two methods on North and not an over because there have been both plans.",
            "Upgrading has been successfully to apply to solve their genomes.",
            "So what we did just what we did is to make some necessary modifications to get them a pliable for their normal regular."
        ],
        [
            "So, so the last algorithm we want to propose, the ZM stagger.",
            "It's completely different from about two, so this method is based on equivalence between the LM three and an adaptive hamster.",
            "Definition of this model is this one so so basically associated each feature with Visa.",
            "Or scaling factor talk here.",
            "So for different dimension features, this this scaling parameter coupled by this constraint.",
            "So this definition is.",
            "So here we optimize over the scaling parameters.",
            "So, so this is, this model can adaptively change the scaling parameter for different features so.",
            "Intuitively, for the year and features, the scatterometer can be cheering for 220.",
            "0 means the feature will be discarded.",
            "So also certificate shows that the adaptive, sorry, actually equivalent to that one, I'm sorry, and basically they have their both their primary sparse.",
            "So based on this equivalence theorem we can develop a Yammer algorithm."
        ],
        [
            "By solving the adaptive I'm certain problems, so this is Zachary outside, so it alternates between the two steps.",
            "The first step.",
            "Actually it's creepy problem which is identical chooser two answering optimization problem.",
            "The things that we have closed form solution to update the scaling parameter.",
            "So to compare with their mother model called Laplace sensor and which were proposed in 2008.",
            "There it has a version version algorithm which has same structure but the differences in updating those getting parameter.",
            "So, so for this model this talk it can be 00 means the WDR and the feature will be discarded.",
            "However, for this model, this get parameter is.",
            "Can we shoot it's positive and finite, so no feature implicit discarded, so that's the fundamental difference between the two model.",
            "So that means these multi select features, but this one cannot just do some sharing feature."
        ],
        [
            "OK, so we have machines 3 Max market models.",
            "Let's see their connection so we have show these two are equivalent and these two have same structure GM style algorithm for this for this to actually have some sort of analysis results based location.",
            "If the regularization regularization parameter is going to Infinity, actually, I am sorry that extreme case of lap times.",
            "So in their rewards directions in this world can be seen as a smoother relaxation of the airline.",
            "I'm sorry.",
            "So, so most details can be found in our ICML paper about this analysis."
        ],
        [
            "OK, so let me show you some impairment results are going to compare different models and also to compare the three learning algorithm for LM.",
            "Sorry to hear the model will compare that is conditional random fields or proposed by John Lafferty and his colleagues in 2001.",
            "It's based on maximum likelihood automation.",
            "If we use our two or Aaron normal regulars.",
            "When doing the parameter learning, we get our Church Alpha no STI.",
            "So all the CR phone models are not do sparse, but airlines.",
            "Geography is the primary sparse which can be used to select feature."
        ],
        [
            "So the 1st results are on the synthetic data data set.",
            "We just simulate a sparse learning problem.",
            "So here we assume the datasets have.",
            "I have a 100 dimensional input feature.",
            "Of which is First Circuit relevant time.",
            "There's a rest.",
            "17 are irrelevant features, so we're generating data sets and digital setup as well, and samples.",
            "We assign the two levels just given similar.",
            "So here's the original error rate over the data set.",
            "Here is the model weights.",
            "So we can still the primary sponsor L once they are finalized.",
            "Answering achieve the best problems.",
            "So by using by using a good regular parameters both models can select actually discard all the relevant feature.",
            "So for the last web server will have sure close connection to the right answer.",
            "It has shrinked effect which kind of makes the the.",
            "You're a feature hardware small weights so that so that the popularity also comparable to the prom."
        ],
        [
            "This past model.",
            "So here's the result.",
            "Sounds also OCR data set, so we with our sample subset to do 10 fold cross validation.",
            "So we build actually 3 four data set.",
            "So here's the results again.",
            "We can see the.",
            "Airline CR M3 and lobstering always get the best result."
        ],
        [
            "So for data set.",
            "So we hear the same results.",
            "Compare the different algorithm for.",
            "I'm sorry, so this ones on the synthetic data set and this one is on the OCR data set for OCR does at the cutting plane.",
            "Calimesa is very efficient, so we do not report it does here.",
            "So we can see the project subgradient player sensitive.",
            "Choose a regular regularization parameter, but the camera is very robust in terms of both area and running time."
        ],
        [
            "So the last three appearances on web data extraction, the task is to identify the name, image, price and description of product items on the web.",
            "So the data set contained most of my thousand training records and the test content.",
            "Moses resulting the testing records so they were evaluation criteria error Jeff FY the block instance accuracy.",
            "The block intense accuracy is the percentage of records whose name, image, and price are all correct level.",
            "So here's the results.",
            "This is the F1.",
            "This is blocking things accuracy so again we can see the airlines M3 and closed related lab answering get the best results compared to other models."
        ],
        [
            "OK, so to conclude, that's good.",
            "We did two things in this talk we proposed presented US primary sparse mass market, Malcolm at work and the developers three learning algorithm for for the so called errors M3 and also we.",
            "Represent a novel adaptive maximum network and issues that's good.",
            "It's equivalent to the airlines, so we also do some impairment, so empirical results shows that error.",
            "I'm sorry can affect your selects.",
            "The new significant features and also show the simultaneous primer and two sparse.",
            "It can give you some.",
            "The improvements on the predicting results.",
            "Also, we show that GM style algorithm is model."
        ],
        [
            "OK, that's all.",
            "Thank you for attention.",
            "2 minutes.",
            "So loud.",
            "Single objectively Lucky announced his comeback, so last year converged global solution.",
            "Yeah, we can get look optimal.",
            "We just.",
            "It's not a distributed convex, but we can do some parameter changes to get its convex convex problem.",
            "So because here we have."
        ],
        [
            "So in the original formulation, this is one over talk talk so.",
            "With respect to so, but the toys parameter is not convex with respect to its diagonal, but it is 1 / 1 / 12.",
            "It's one over talking.",
            "Paul.",
            "It's OK, so anyway, yeah, we gotta change the parameters to get it.",
            "So yeah, OK then formulated.",
            "OK, can you go to the slash 14?"
        ],
        [
            "14 OK. OK, yeah is that number of selected features for L1M N from 40 to 5040.",
            "Two 60 is the number of features.",
            "Let it.",
            "So here is the index of the features.",
            "So simple actually there about.",
            "20 30 party ground teachers.",
            "Yeah, that feature from 40 to 60 or select as significant feature so other features are reduced.",
            "Yeah, because we that we use the.",
            "Set to generator to level so so the features are not actually the true true set of the significant feature.",
            "So we have some noise.",
            "Or at least I'm speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so this talk is about.",
                    "label": 0
                },
                {
                    "sent": "Primus past Max Mark Mark Network.",
                    "label": 0
                },
                {
                    "sent": "It basically is so hard to do feature selection in the next market market networks so this is joint work with directly at the same you and my otherwise support.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the content in this talk first, which tells the structure prediction problem and there's models that we choose the best story learning algorithm for this primer smart model and we choose some impairment.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results OK, so to compare with, we start with the classification problem.",
                    "label": 0
                },
                {
                    "sent": "So the linear basic attacker set of ideation is training data.",
                    "label": 1
                },
                {
                    "sent": "Diagram output predicted function.",
                    "label": 0
                },
                {
                    "sent": "Which is mapping from input space to output space.",
                    "label": 0
                },
                {
                    "sent": "For testing example you assign or best class level.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 example of OCR OCR inputs, human written characters and output English character.",
                    "label": 0
                },
                {
                    "sent": "So competition is one of the most important supervised learning tasks.",
                    "label": 1
                },
                {
                    "sent": "There have been many methods proposed to the classifiers, so here's one popular example with support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So this is a multi multi class classification problem formulated by quorum and single in 2001, so this year constrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It can be equivalent rating as a recognized.",
                    "label": 0
                },
                {
                    "sent": "Causation problem so here the.",
                    "label": 0
                },
                {
                    "sent": "Be sure is a pound of the empirical loss of this model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but the problem with cloud cache model is that it ignores the dependence among among data.",
                    "label": 1
                },
                {
                    "sent": "So for example in the OCR, if we treat the characters independently.",
                    "label": 0
                },
                {
                    "sent": "The cupcake model you are difficult to distinguish this character and this one.",
                    "label": 0
                },
                {
                    "sent": "So to improve the predicted predicted accuracy, we want to employ more information about the data.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "At the input side, windows that they put a character it's you know word sequence.",
                    "label": 0
                },
                {
                    "sent": "So we can check some content dependently feature features.",
                    "label": 0
                },
                {
                    "sent": "So discriminative model like SM and the models average dose can incorporate this structure.",
                    "label": 0
                },
                {
                    "sent": "Information structure input feature so.",
                    "label": 0
                },
                {
                    "sent": "But at the output side the prediction also we also have some structure, for example in OCR.",
                    "label": 0
                },
                {
                    "sent": "The prediction is also in sequence.",
                    "label": 0
                },
                {
                    "sent": "So structured prediction is learning from work that that tried to incorporate the structural dependency in the output side.",
                    "label": 0
                },
                {
                    "sent": "So to produce global optimal prediction.",
                    "label": 1
                },
                {
                    "sent": "So the best setup is that it also needs a set of ID training samples.",
                    "label": 1
                },
                {
                    "sent": "The output structure predict function for each input output, a structure prediction so, but here it is.",
                    "label": 0
                },
                {
                    "sent": "It input each sample is a pair of.",
                    "label": 0
                },
                {
                    "sent": "Zion, why I both have some structures.",
                    "label": 0
                },
                {
                    "sent": "For example, you know sociology puts the output sequence.",
                    "label": 0
                },
                {
                    "sent": "Here's another example.",
                    "label": 0
                },
                {
                    "sent": "Or imagination.",
                    "label": 0
                },
                {
                    "sent": "Your inputs.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a metric.",
                    "label": 0
                },
                {
                    "sent": "Essentially the feature worked, so I put it.",
                    "label": 0
                },
                {
                    "sent": "Also a metric of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Class level.",
                    "label": 0
                },
                {
                    "sent": "So, so this is supposed to let pizza true distribution.",
                    "label": 0
                },
                {
                    "sent": "The risk of a particular model is the expectation of data error data error is a non negative loss function which measures how accurate is your prediction is compared to the truth.",
                    "label": 1
                },
                {
                    "sent": "For example harmless Meyers actually basically counts the number of mismatches in the prediction so but.",
                    "label": 0
                },
                {
                    "sent": "You are way to we do not know the true distribution, so we use empirical risk.",
                    "label": 1
                },
                {
                    "sent": "Empirical risk based on the training data to approximate the risk.",
                    "label": 0
                },
                {
                    "sent": "So for finite number of training data to avoid overfitting, which you had already used the regularizer to.",
                    "label": 0
                },
                {
                    "sent": "To Pharos no simple model, so minimize this records imperious case noise records and imperious communalization from work.",
                    "label": 0
                },
                {
                    "sent": "So is this general framework we can define define different actual model and use different regularizers we can get.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model so is this talk.",
                    "label": 0
                },
                {
                    "sent": "I will focus on the mixed Mark Mark Network, so the best seller hub is is that follows.",
                    "label": 0
                },
                {
                    "sent": "So suppose that for its discriminant function, which is a mapping from the product space of input output to the real number space.",
                    "label": 0
                },
                {
                    "sent": "So here we consider the linear form.",
                    "label": 1
                },
                {
                    "sent": "Basically this.",
                    "label": 0
                },
                {
                    "sent": "Function is a linear combination of a set of feature functions and W is the upper middle weight.",
                    "label": 0
                },
                {
                    "sent": "So keeping a define a particular rule and also defines the margin which is the difference between the refs called difference between the truth and the best run up production.",
                    "label": 0
                },
                {
                    "sent": "So for this model we can prove that this this actually is a pound of the empirical loss.",
                    "label": 0
                },
                {
                    "sent": "So maximizing Mac network.",
                    "label": 0
                },
                {
                    "sent": "Optimal model by solving this minimalization problem, that's really it's regulated pendulum azatian.",
                    "label": 0
                },
                {
                    "sent": "So again like swim, this problem can be written as a constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So here we use the L1 norm instead of two norm.",
                    "label": 0
                },
                {
                    "sent": "We get one answer which is the primary sparse model orange juice.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see the difference between L1 norm and L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So we know that this regular minimalization problem can be written as this content optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So so basically this constraint for different normal define different subset subset of visible physical weight.",
                    "label": 0
                },
                {
                    "sent": "So it basically defined can be illustrated by the projection.",
                    "label": 0
                },
                {
                    "sent": "So for our turn on the Facebook.",
                    "label": 0
                },
                {
                    "sent": "In the face of subspace subset, is error two ball and for their right now it's it's it's share.",
                    "label": 1
                },
                {
                    "sent": "So if we know model for into this in this region there no projection is needed.",
                    "label": 0
                },
                {
                    "sent": "So so here we just consider the points outside this physical subset.",
                    "label": 0
                },
                {
                    "sent": "So let's consider first, so all sent so we can partition it into surgery.",
                    "label": 0
                },
                {
                    "sent": "3 different regions.",
                    "label": 0
                },
                {
                    "sent": "So first point in this series, like Pyp, the projection to the airline bar to bar.",
                    "label": 0
                },
                {
                    "sent": "Do not have zero coordinate.",
                    "label": 0
                },
                {
                    "sent": "So however, the point is this region, for example P2.",
                    "label": 0
                },
                {
                    "sent": "The second coordinate is very small.",
                    "label": 0
                },
                {
                    "sent": "The projection to their rainbow.",
                    "label": 0
                },
                {
                    "sent": "We have 4 zero coordinate, so but the projection to their two or two, but that doesn't have zero coordinate.",
                    "label": 0
                },
                {
                    "sent": "If if the this pointer doesn't contain zero code in it.",
                    "label": 0
                },
                {
                    "sent": "So this is the difference.",
                    "label": 1
                },
                {
                    "sent": "So so it suggests for everyone normal regular, so we can.",
                    "label": 0
                },
                {
                    "sent": "Get a sparse at Med so.",
                    "label": 0
                },
                {
                    "sent": "We call this the type of spicy.",
                    "label": 0
                },
                {
                    "sent": "Does primal specific.",
                    "label": 0
                },
                {
                    "sent": "Let's give the earlier feel.",
                    "label": 0
                },
                {
                    "sent": "Input features have non zero weights.",
                    "label": 1
                },
                {
                    "sent": "So there are two normal condition can shrink shrink model weights by do this by doing this projection.",
                    "label": 0
                },
                {
                    "sent": "However, it cannot implicitly discard some feature setting there with 203 calls.",
                    "label": 0
                },
                {
                    "sent": "This kind of sparsity as shooter promise positive.",
                    "label": 1
                },
                {
                    "sent": "The third kind of sparsity is it too spicy body do sparsity for field.",
                    "label": 0
                },
                {
                    "sent": "Lagrangian multipliers are non 0.",
                    "label": 0
                },
                {
                    "sent": "So here's why.",
                    "label": 0
                },
                {
                    "sent": "Example for linear makes money market network.",
                    "label": 1
                },
                {
                    "sent": "By the component complementary slackness condition, we can we can get this equation.",
                    "label": 0
                },
                {
                    "sent": "So basically only for the active constraints we have non zero lag or multiplier multipliers.",
                    "label": 0
                },
                {
                    "sent": "So so both Erlander 2M3 and do sparse.",
                    "label": 0
                },
                {
                    "sent": "However L1 norm, I'm sorry is the primary sparse next market network.",
                    "label": 0
                },
                {
                    "sent": "So now we will see how to solve the how to learn this at one M3.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first method is cutting parameter.",
                    "label": 0
                },
                {
                    "sent": "So this this method has been successfully used to the election or three.",
                    "label": 1
                },
                {
                    "sent": "So the basic procedure is that it actively construct a subset of constraint and the optimized with respect to the contracting subset.",
                    "label": 1
                },
                {
                    "sent": "So here's the algorithm outlined for the airwaves.",
                    "label": 1
                },
                {
                    "sent": "Basically, for the first able to construct the the select the subset of constraints it actually solved for a loss argument augmented prediction problem.",
                    "label": 0
                },
                {
                    "sent": "To find the white Whitehead.",
                    "label": 1
                },
                {
                    "sent": "So this problem this problem can be efficiently solved for loss loss function data.",
                    "label": 0
                },
                {
                    "sent": "Can be decomposed into small parts.",
                    "label": 0
                },
                {
                    "sent": "So basically we can use Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for the sex that we choose optimize to solve a subproblem to optimize with respect to the current working set.",
                    "label": 0
                },
                {
                    "sent": "So this problem can be formulated as a linear programming problem.",
                    "label": 0
                },
                {
                    "sent": "So here is the key insight.",
                    "label": 0
                },
                {
                    "sent": "So is this is this constraint of one day?",
                    "label": 0
                },
                {
                    "sent": "Probably we can lead to W equal to mu minus.",
                    "label": 1
                },
                {
                    "sent": "We both knew and they are nonnegative vectors.",
                    "label": 0
                },
                {
                    "sent": "So without loss of generality, we can assume that at most one of four Mckelvey case non 0.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we can stop check both by a smaller way without changing the value of W. So based on this observation this problem can be equivalently taxes for the one norm is equal to mu plus we.",
                    "label": 0
                },
                {
                    "sent": "So now we can use the linear linear programming solver to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So since the constraint I territory added so this solution will be more and more accurate, so algorithm terminates if no change in the working set.",
                    "label": 0
                },
                {
                    "sent": "Bezier no constraints selected.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the sector second method is the projects upgraded.",
                    "label": 0
                },
                {
                    "sent": "Based on this constraint optimization formulation.",
                    "label": 0
                },
                {
                    "sent": "So here is the definition of the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So basically for for differentiable function.",
                    "label": 0
                },
                {
                    "sent": "The subgradient is actually the gradient.",
                    "label": 0
                },
                {
                    "sent": "So far why I'm sorry and the algorithm authorized is this one, so it's very simple.",
                    "label": 0
                },
                {
                    "sent": "Basically is the greatest distance method, but with the additional step to protect the.",
                    "label": 0
                },
                {
                    "sent": "Moderate to the to the physical subset.",
                    "label": 0
                },
                {
                    "sent": "Physical substance subset.",
                    "label": 0
                },
                {
                    "sent": "So for the first step to compute the subgradient windows that.",
                    "label": 0
                },
                {
                    "sent": "For each example, this.",
                    "label": 0
                },
                {
                    "sent": "This this part is piecewise linear.",
                    "label": 1
                },
                {
                    "sent": "It can be returned this form.",
                    "label": 0
                },
                {
                    "sent": "So this is this graph shows the basic shape of this function.",
                    "label": 0
                },
                {
                    "sent": "So for this point there not added the turning point that actually this function function is differentiable, so so the subgradient actually is is according to for example, for this point is gradient operator Y 2.",
                    "label": 0
                },
                {
                    "sent": "So, but for the turning point.",
                    "label": 0
                },
                {
                    "sent": "The subcode it can be any vector in between all of these two little guardians, so, but in general we can.",
                    "label": 0
                },
                {
                    "sent": "We can choose this.",
                    "label": 0
                },
                {
                    "sent": "GI is is a one of the subgradients of hi.",
                    "label": 0
                },
                {
                    "sent": "So two computers this upgrading.",
                    "label": 1
                },
                {
                    "sent": "We also need to solve for the loss augmented prediction problem.",
                    "label": 0
                },
                {
                    "sent": "We can be efficient now.",
                    "label": 0
                },
                {
                    "sent": "So for the second step, to do the projection, we also have efficient algorithm which is actually scale linearly with in terms of the number of features.",
                    "label": 0
                },
                {
                    "sent": "If the Cardinal sparse we can actually get a log scale capacity.",
                    "label": 0
                },
                {
                    "sent": "So so about two methods on North and not an over because there have been both plans.",
                    "label": 0
                },
                {
                    "sent": "Upgrading has been successfully to apply to solve their genomes.",
                    "label": 0
                },
                {
                    "sent": "So what we did just what we did is to make some necessary modifications to get them a pliable for their normal regular.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the last algorithm we want to propose, the ZM stagger.",
                    "label": 0
                },
                {
                    "sent": "It's completely different from about two, so this method is based on equivalence between the LM three and an adaptive hamster.",
                    "label": 0
                },
                {
                    "sent": "Definition of this model is this one so so basically associated each feature with Visa.",
                    "label": 0
                },
                {
                    "sent": "Or scaling factor talk here.",
                    "label": 0
                },
                {
                    "sent": "So for different dimension features, this this scaling parameter coupled by this constraint.",
                    "label": 0
                },
                {
                    "sent": "So this definition is.",
                    "label": 0
                },
                {
                    "sent": "So here we optimize over the scaling parameters.",
                    "label": 0
                },
                {
                    "sent": "So, so this is, this model can adaptively change the scaling parameter for different features so.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, for the year and features, the scatterometer can be cheering for 220.",
                    "label": 0
                },
                {
                    "sent": "0 means the feature will be discarded.",
                    "label": 0
                },
                {
                    "sent": "So also certificate shows that the adaptive, sorry, actually equivalent to that one, I'm sorry, and basically they have their both their primary sparse.",
                    "label": 0
                },
                {
                    "sent": "So based on this equivalence theorem we can develop a Yammer algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By solving the adaptive I'm certain problems, so this is Zachary outside, so it alternates between the two steps.",
                    "label": 0
                },
                {
                    "sent": "The first step.",
                    "label": 0
                },
                {
                    "sent": "Actually it's creepy problem which is identical chooser two answering optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The things that we have closed form solution to update the scaling parameter.",
                    "label": 0
                },
                {
                    "sent": "So to compare with their mother model called Laplace sensor and which were proposed in 2008.",
                    "label": 0
                },
                {
                    "sent": "There it has a version version algorithm which has same structure but the differences in updating those getting parameter.",
                    "label": 0
                },
                {
                    "sent": "So, so for this model this talk it can be 00 means the WDR and the feature will be discarded.",
                    "label": 0
                },
                {
                    "sent": "However, for this model, this get parameter is.",
                    "label": 0
                },
                {
                    "sent": "Can we shoot it's positive and finite, so no feature implicit discarded, so that's the fundamental difference between the two model.",
                    "label": 0
                },
                {
                    "sent": "So that means these multi select features, but this one cannot just do some sharing feature.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have machines 3 Max market models.",
                    "label": 0
                },
                {
                    "sent": "Let's see their connection so we have show these two are equivalent and these two have same structure GM style algorithm for this for this to actually have some sort of analysis results based location.",
                    "label": 0
                },
                {
                    "sent": "If the regularization regularization parameter is going to Infinity, actually, I am sorry that extreme case of lap times.",
                    "label": 0
                },
                {
                    "sent": "So in their rewards directions in this world can be seen as a smoother relaxation of the airline.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So, so most details can be found in our ICML paper about this analysis.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me show you some impairment results are going to compare different models and also to compare the three learning algorithm for LM.",
                    "label": 1
                },
                {
                    "sent": "Sorry to hear the model will compare that is conditional random fields or proposed by John Lafferty and his colleagues in 2001.",
                    "label": 0
                },
                {
                    "sent": "It's based on maximum likelihood automation.",
                    "label": 0
                },
                {
                    "sent": "If we use our two or Aaron normal regulars.",
                    "label": 0
                },
                {
                    "sent": "When doing the parameter learning, we get our Church Alpha no STI.",
                    "label": 0
                },
                {
                    "sent": "So all the CR phone models are not do sparse, but airlines.",
                    "label": 0
                },
                {
                    "sent": "Geography is the primary sparse which can be used to select feature.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st results are on the synthetic data data set.",
                    "label": 1
                },
                {
                    "sent": "We just simulate a sparse learning problem.",
                    "label": 0
                },
                {
                    "sent": "So here we assume the datasets have.",
                    "label": 0
                },
                {
                    "sent": "I have a 100 dimensional input feature.",
                    "label": 0
                },
                {
                    "sent": "Of which is First Circuit relevant time.",
                    "label": 0
                },
                {
                    "sent": "There's a rest.",
                    "label": 1
                },
                {
                    "sent": "17 are irrelevant features, so we're generating data sets and digital setup as well, and samples.",
                    "label": 0
                },
                {
                    "sent": "We assign the two levels just given similar.",
                    "label": 0
                },
                {
                    "sent": "So here's the original error rate over the data set.",
                    "label": 0
                },
                {
                    "sent": "Here is the model weights.",
                    "label": 0
                },
                {
                    "sent": "So we can still the primary sponsor L once they are finalized.",
                    "label": 0
                },
                {
                    "sent": "Answering achieve the best problems.",
                    "label": 0
                },
                {
                    "sent": "So by using by using a good regular parameters both models can select actually discard all the relevant feature.",
                    "label": 0
                },
                {
                    "sent": "So for the last web server will have sure close connection to the right answer.",
                    "label": 0
                },
                {
                    "sent": "It has shrinked effect which kind of makes the the.",
                    "label": 0
                },
                {
                    "sent": "You're a feature hardware small weights so that so that the popularity also comparable to the prom.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This past model.",
                    "label": 0
                },
                {
                    "sent": "So here's the result.",
                    "label": 0
                },
                {
                    "sent": "Sounds also OCR data set, so we with our sample subset to do 10 fold cross validation.",
                    "label": 1
                },
                {
                    "sent": "So we build actually 3 four data set.",
                    "label": 0
                },
                {
                    "sent": "So here's the results again.",
                    "label": 0
                },
                {
                    "sent": "We can see the.",
                    "label": 0
                },
                {
                    "sent": "Airline CR M3 and lobstering always get the best result.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for data set.",
                    "label": 0
                },
                {
                    "sent": "So we hear the same results.",
                    "label": 0
                },
                {
                    "sent": "Compare the different algorithm for.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, so this ones on the synthetic data set and this one is on the OCR data set for OCR does at the cutting plane.",
                    "label": 1
                },
                {
                    "sent": "Calimesa is very efficient, so we do not report it does here.",
                    "label": 0
                },
                {
                    "sent": "So we can see the project subgradient player sensitive.",
                    "label": 0
                },
                {
                    "sent": "Choose a regular regularization parameter, but the camera is very robust in terms of both area and running time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the last three appearances on web data extraction, the task is to identify the name, image, price and description of product items on the web.",
                    "label": 1
                },
                {
                    "sent": "So the data set contained most of my thousand training records and the test content.",
                    "label": 1
                },
                {
                    "sent": "Moses resulting the testing records so they were evaluation criteria error Jeff FY the block instance accuracy.",
                    "label": 0
                },
                {
                    "sent": "The block intense accuracy is the percentage of records whose name, image, and price are all correct level.",
                    "label": 1
                },
                {
                    "sent": "So here's the results.",
                    "label": 0
                },
                {
                    "sent": "This is the F1.",
                    "label": 0
                },
                {
                    "sent": "This is blocking things accuracy so again we can see the airlines M3 and closed related lab answering get the best results compared to other models.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude, that's good.",
                    "label": 0
                },
                {
                    "sent": "We did two things in this talk we proposed presented US primary sparse mass market, Malcolm at work and the developers three learning algorithm for for the so called errors M3 and also we.",
                    "label": 0
                },
                {
                    "sent": "Represent a novel adaptive maximum network and issues that's good.",
                    "label": 1
                },
                {
                    "sent": "It's equivalent to the airlines, so we also do some impairment, so empirical results shows that error.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry can affect your selects.",
                    "label": 1
                },
                {
                    "sent": "The new significant features and also show the simultaneous primer and two sparse.",
                    "label": 0
                },
                {
                    "sent": "It can give you some.",
                    "label": 1
                },
                {
                    "sent": "The improvements on the predicting results.",
                    "label": 0
                },
                {
                    "sent": "Also, we show that GM style algorithm is model.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's all.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "2 minutes.",
                    "label": 0
                },
                {
                    "sent": "So loud.",
                    "label": 0
                },
                {
                    "sent": "Single objectively Lucky announced his comeback, so last year converged global solution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can get look optimal.",
                    "label": 0
                },
                {
                    "sent": "We just.",
                    "label": 0
                },
                {
                    "sent": "It's not a distributed convex, but we can do some parameter changes to get its convex convex problem.",
                    "label": 0
                },
                {
                    "sent": "So because here we have.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the original formulation, this is one over talk talk so.",
                    "label": 0
                },
                {
                    "sent": "With respect to so, but the toys parameter is not convex with respect to its diagonal, but it is 1 / 1 / 12.",
                    "label": 0
                },
                {
                    "sent": "It's one over talking.",
                    "label": 0
                },
                {
                    "sent": "Paul.",
                    "label": 0
                },
                {
                    "sent": "It's OK, so anyway, yeah, we gotta change the parameters to get it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, OK then formulated.",
                    "label": 0
                },
                {
                    "sent": "OK, can you go to the slash 14?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "14 OK. OK, yeah is that number of selected features for L1M N from 40 to 5040.",
                    "label": 0
                },
                {
                    "sent": "Two 60 is the number of features.",
                    "label": 0
                },
                {
                    "sent": "Let it.",
                    "label": 0
                },
                {
                    "sent": "So here is the index of the features.",
                    "label": 0
                },
                {
                    "sent": "So simple actually there about.",
                    "label": 0
                },
                {
                    "sent": "20 30 party ground teachers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that feature from 40 to 60 or select as significant feature so other features are reduced.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because we that we use the.",
                    "label": 0
                },
                {
                    "sent": "Set to generator to level so so the features are not actually the true true set of the significant feature.",
                    "label": 0
                },
                {
                    "sent": "So we have some noise.",
                    "label": 0
                },
                {
                    "sent": "Or at least I'm speaker.",
                    "label": 0
                }
            ]
        }
    }
}