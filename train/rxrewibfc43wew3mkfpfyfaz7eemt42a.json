{
    "id": "rxrewibfc43wew3mkfpfyfaz7eemt42a",
    "title": "Recent Advances in Bayesian Methods",
    "info": {
        "author": [
            "Jun Zhu, Department of Computer Science and Technology, Tsinghua University"
        ],
        "published": "March 27, 2014",
        "recorded": "November 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/acml2013_zhu_bayesian_methods/",
    "segmentation": [
        [
            "OK yeah thanks.",
            "Also, first of all, thanks to organizers invited me here.",
            "So it's my great honor to speak here and to share with you some of the most.",
            "Obviously they talk about some recent work in my group about basic methods.",
            "R."
        ],
        [
            "So here is the plan of this tutorial.",
            "So little early naturally split in two parts because they have the morning session and also have the optimization right.",
            "So for the morning session I speak for one hour about one hour and basically let's give you some quick idea about the white keyboard based methods and also our particular focus on the recent advanced on the nonparametric Bayesian methods.",
            "Then we have for lunch break for one hour and after that we are continuing this afternoon and particularly we show you some recent work on the record constraint drug testing methods and I show you some interesting applications to topic modeling link prediction.",
            "So recommendation and and some other cases OK."
        ],
        [
            "So just a quick reminder you about some basic rules of probability.",
            "I believe for most of your maybe no better than me, and so here we have some better concept.",
            "We have probability of some verbal and we have conditional probability.",
            "It's basically some notation here.",
            "We hope to scrap the joint probability and we have some basic rules in the in the probability theory.",
            "So the product rule and some rule and also the conditional probability defined by the.",
            "Divide of P joint over the marginal OK."
        ],
        [
            "So you could combine combine this definition of control and also applies the product and some rules we get recalled basing rules.",
            "It was first proposed in it's it's part 250 years ago.",
            "So actually this year is the it's marking here and celebrate the 250th anniversary of the so called best theorem.",
            "OK, so."
        ],
        [
            "First overalls, they they basically maybe the problem.",
            "We care about the machine learning.",
            "When we apply the best rule.",
            "OK so so here I I sweet speeches and notation a little bit.",
            "Let D denote the given data set and M is a particular model.",
            "We care about knowing the best rules say that we have this party redistribution and depressed of the prior and likelihood and divided by the marginal likelihood.",
            "All the evidence.",
            "And to recap, we can apply this best best rules to maybe.",
            "Scenarios that we can compare different kind of models.",
            "For example, if we may have different family of models like linear model or nonlinear models and we can impress the posterior over the whole whole family.",
            "I'm the party real giving some data and use this marginal likelihood basically integral over all the all the possible instance in the in the model family.",
            "OK, we can use these factors as scoring criteria to compare the different families and do model selection for example.",
            "And also we care about the prediction like if we have a new incoming instance we make computers that predicted probability.",
            "Giving these are his historical data and the particular model family, and they also process the integral over the material given a particular model and over the whole family of the model, right?",
            "So on the most of the common assumptions for them.",
            "So basically for all the model based approaches, we assume that the Betta they better distributed independently continue on that particular model, right?",
            "And simplify this this first.",
            "Use this one if we know the particular model instance they support.",
            "This conditional is simplified to this one and this is independent of the historical data, right?",
            "So this is very common assumption in the model based approaches compared in contrast to some memory based approach in memory.",
            "Best we basically remember all the data we do the computation in some complicated way.",
            "OK."
        ],
        [
            "So here's some common questions when we apply when we do billing method.",
            "So firstly I will.",
            "People would like I would like to ask maybe why we use Bayesian and the 2nd is so where does the prior come from?",
            "And the third one is how do we do the integral?",
            "So because remember we have a lot of have a different scenario but to do the integral computation.",
            "So how do we do this?",
            "OK, so here is a general questions, so let me give you some brief answers to each officer."
        ],
        [
            "So why be basing their their many possible answers like basic methods are very flexible and very intuitive.",
            "It's a I mean so easy to interpret the different parts of the model and also there are some some theoretical arguments.",
            "So here it's a very result.",
            "It called Death Infinity theorem.",
            "It's suppose we have a set of points and there's some some definition of card if interchangeability.",
            "For for any finite subset.",
            "For example, a number of points and for any permutation of the order.",
            "And this distribution doesn't change.",
            "So that means the order doesn't matter in the discard this data.",
            "So there are a lot of scenarios we we can apply this kind of property, for example for the most, for the very common feature, like bigger word representation of documents, it's it satisfies these kind of property, so the theorem says that if we.",
            "Better satisfies this kind of property.",
            "Can be the joint distribution can be described as a as a marginal over another by over some augmented variable like M, it follows some distribution and this part.",
            "Continuing this M the data points will be independent so.",
            "So it can be visualized as in this way.",
            "This is a marginal joint district over the object we care about, and it can be integral as this use the additional variable M and continue this verbal M with this observed data points are independent, so this very basic idea is basically only in Beijing methods OK."
        ],
        [
            "So the next question is how to choose priors.",
            "They are basically in the pacing of community.",
            "There are people care about difficult of priors.",
            "So first, why is the objective prior it try to be non informative?",
            "Maybe you don't have any belief.",
            "You can use some of qnective priors, so we're going to have some some.",
            "This property is some nice properties, then another is another family, so we use subjective priors.",
            "We we try to use the prior to capture some of our belief prior belief before seeing the data.",
            "So for this objective, prior to the one I people would like to ask how to choose, maybe how to choose the.",
            "The hub parameters, if any right so so one way is to weaken the influence of the hub parameters did to you some something we called hierarchical priors so we can use multiple layers to represent a prior.",
            "For example, this is the.",
            "Maybe this is target will target prior over the model.",
            "We can we can introduce another layer, maybe put some hyper prior over the over this one and we continually hierarchy attention.",
            "To Infinity in theory.",
            "So the best belief is that if they.",
            "If the prize is higher from the model you care about, the influence of week.",
            "So this is the basic beliefs we when we apply the heart of fire.",
            "And also there's another thing we called the empirical price.",
            "It give you some method to ultimate the hyperparameters of your prior.",
            "OK.",
            "So for example, we would like to impose a prior over the model an giving some.",
            "Some parameter we can ultimate admitedly the hyperparameters by maximizing the marginal life hold.",
            "This is a given data, right?",
            "So there are some other wanted your disability, obviously empirical approach, so it can be reversed because the.",
            "It's capture system.",
            "It's all of your data, so we can all come something amiss.",
            "The specification of the prior, so also it's also has some disadvantages, double counting on your data.",
            "So because you're so for prior we.",
            "The goal is to capture some some prior belief before you are seeing your data but but here we do consider the data when we estimated the prior.",
            "So it's kind of a double counting, so it may be limited to some.",
            "Resulting in some overfitting OK?"
        ],
        [
            "Also, when we choose were prior computationally, we have some trade off.",
            "It's called conjugate or non conjugated shut off.",
            "So far can you get priors?",
            "It can be.",
            "It's relatively easier because the if the prices contributed to your like hood, we can.",
            "We know that the form of the posterior right it has the same same family as a prior so.",
            "But the main question is how to update the parameter.",
            "So here a lot of examples like golfing golfing.",
            "If you use a Gaussian prior, causing like code is there we conjugate its conjugate pair.",
            "Also we have different multinomial conjugate pair.",
            "So, but another families of non conjugate priors, so for non conjugated it's a more flexible but completion is harder to do the inference.",
            "So I give you a particular example so we know the.",
            "So this is a prior over over some multinomial space.",
            "OK so for do should we we can write to the patient here so you can see that the different dimensions kind of for independent, but they're not truly independent because.",
            "They have a normalization constraint, right?",
            "So besides that, so we don't have any any correlations between these different dimension but another non computer cases that we could use the so-called largest normal prior.",
            "We first sample a parameter from a normal distribution multivariate normal and then we do a modest transformation to normalize the and make this vector.",
            "Supply over them at normal distribution.",
            "OK so you can see that for this largest novel, because we have a chorus magic, it can encode the correlation structures between different dimension, But for this kind of price, so much harder to do the inference I will show you.",
            "Some results in this afternoon about how to do.",
            "Maybe large scale computing for this kind of non conjugate priors?",
            "OK."
        ],
        [
            "So then this other patches, how do we compute integrals?",
            "Just to remind you about this impression, this marginally code given the model family we do the integral over all the possible instance in this family, and this integral can be very high dimension.",
            "We could maybe even information.",
            "Default nonparametric models and so by Americans is for better method.",
            "We may consider some some later marbles to discover data.",
            "In this case you will get even harder integral problem.",
            "So maybe you have mixed integral have discovered variables you have continued variables right?",
            "So for discrete integral will be assigned over all possible values.",
            "So how do we do the universe?"
        ],
        [
            "So I imbedding community, we do, so we have.",
            "We have no exact solution.",
            "We made you just resort to some approximation methods.",
            "This is approximate Bayesian inference time.",
            "So there are a lot of common examples for subversion approximations, Markov chain mode, colored methods and application propagation, last approximation and also lot of others.",
            "So the code is to develop some.",
            "Relatively accurate and scalable algorithm for to do the integral and to do the prediction.",
            "So this is a very active area in the immersion learning study to to develop better algorithms.",
            "So I give you, I just give you some basic ideas about the 1st two.",
            "They are basically represent most of the.",
            "The methods we use nowadays."
        ],
        [
            "So here's the white slides.",
            "The better software version approximation.",
            "So because of this are marginally code, it's hard to compute, but we can.",
            "We can approximate it by using some bound.",
            "So for example, we if we take the logarithm of the marginal marginal likelihood, we can.",
            "This is just the dependa repression of this.",
            "Modern, likely using that marbles and models and we can introduce another distribution called version of Distribution Q and its joint over head verbals and the model.",
            "And we much product here and divide here so it doesn't change anything.",
            "So then we comply with the Jason inequality and get a lower bound right?",
            "We move the logarithm into the integral.",
            "We change the order of the integrand, logarithm, and then we can.",
            "We can do in some cases.",
            "We can do this kind of.",
            "Any glory in some way easier than this one, OK?",
            "So note that if we don't make any assumptions, it's the boundary tight so that basically means that we can get the.",
            "Equality here, so just by setting this Q equal to the party real, the true posterior divide by derived from the model, right?",
            "But the but this still hard to do, and literally so we most common case we use them.",
            "We need to do some additional assumption like midfield assumptions.",
            "So basically make it affects assumption that for example for this one maybe assume the hidden marble independent of from the model variables.",
            "So this is very simple assumption.",
            "So then we can optimize this bound with this assumption.",
            "For example, user quantities and are assigned to optimize it.",
            "We get some local optimal.",
            "This is reduced.",
            "The solution here as approximation to the true posterior, and then we also we can do the integral of the lower bound right?",
            "So this is the basic idea of version approximation."
        ],
        [
            "And again, another slides about the basics of the McCall McAuliffe methods.",
            "So for Microchip mccullar methods through, the goal is to we want to draw some samples from the posterior from the target posterior and we use our samples V to approximate the integral example we.",
            "We have multiple samples from the.",
            "The distribution and we can use the empirical.",
            "Maybe the test user empirical method.",
            "That really integral and so the best idea is that we define verifying a Markov chain and M-02M1 and Gen 2 and so on.",
            "So at this time we will draw a sample from the distribution PT.",
            "Over there, the model and the PT is defined as a because there is a Markov chain.",
            "It's dependent from the previous step.",
            "Palmer's team right away by using a transition metric.",
            "This is this a microphone change, consider probability.",
            "It's from it that is the probability from the mprime TM.",
            "So, so we call that the target distribution environment.",
            "Or is it just a stationary distribution of the Markov chain before it satisfies this this equality?",
            "So basically you do the integral, you get the true posterior, the true target.",
            "OK, so how do?"
        ],
        [
            "Cheers of wine.",
            "Youthful condition that implies the universe that guarantee that we can get this taken this week as our target.",
            "So it's so called detailed balance.",
            "So we multiply.",
            "So from the impromptu M we multiply the distribution of prime and equal to so.",
            "So it's from one way to.",
            "So if you from the employment M which multiplies the, we can view as.",
            "Prior, maybe an alternative way we can see from MTN prime and use this as a scaffold prior their equivalent.",
            "We call it the detailed balance.",
            "And see in practice with design design some micro chamber, methods that satisfies this.",
            "This kind of condition is conduct detailed, balanced and guarantee that if the card we get the true posterior the true target.",
            "So this is the basis of our Markov chain color methods.",
            "OK so I just give you work picture or we offer the basic problem we captured in the basic methods.",
            "So if you have any questions feel free to interpret interrupted me.",
            "But I'm OK."
        ],
        [
            "OK, so for the best measures we can roughly categorize in two families.",
            "The first one is called Parametric Bayesian methods, so in this case our model is represented as a finite set of parameters that called Theatre and we impose some prior divine light code.",
            "We can do the building inference to the best rule.",
            "Everything is worse than so a lot of popular examples.",
            "As I said this contradicting pairs.",
            "Housing price goes in like code and you should try and normalize code they give you.",
            "The analytical criteria and also investing family there are.",
            "There is a larger district of research on these so called sparse Bayesian methods.",
            "They use some sparsity inducing prior.",
            "Plus them like old and they can get some machine could affecting the posterior and do the bathing spots.",
            "Building inference OK."
        ],
        [
            "So another family is a condom primary building inference.",
            "So in this case the model is much richer it can.",
            "You can have infinite number of parameters.",
            "So to emphasize, for not property with it doesn't mean the model don't have funding parameters.",
            "It means with the number of parameters can be infinite.",
            "So this is the.",
            "Point, which we should.",
            "Should we care about so for this nonparametric models?",
            "We can still do the basing inference, use the best rule we post some prior over the infinite number of parameters space and we we use the like hood.",
            "I don't do that.",
            "The posterior inference."
        ],
        [
            "So in the rest of talk, I will quickly go over the three popular examples.",
            "So first one is we can define the nonparametric priors over the the probability measure space.",
            "It's called user prior.",
            "So by the way, for all these kind of because we."
        ],
        [
            "We will have an even number of parameters that they prior.",
            "We just started process.",
            "It defined on the number of variables."
        ],
        [
            "OK, so another example is just process defined on the on the metrics.",
            "So here we care about binary metrics, but it can be attended to integer or real valued metrics.",
            "So here emphasize that for this metric you can have an even number of columns.",
            "So as our assure you, this kind of scenario very useful for to learn features for for different applications and the last one is you can define start process on the function space for them.",
            "For this most functions you can divide your Gaussian process.",
            "Overseas the whole thing or Hostess.",
            "For all these are nonparametric process, so we use them as.",
            "About priors over over some model and in order to do the best inference, we also need another part is the car like hood.",
            "So here for like oh we just use some common choices like for discretely maybe use multinomial or post on record for real value of data.",
            "We may use Gaussian like OK so this is our standard."
        ],
        [
            "OK, so the first question before I go into the details about the each of them.",
            "The first question we may ask is why be basing number metrics?",
            "Why we care about this?",
            "So the basic principle is that we want to let the data speak for itself.",
            "So that means we we want to automatically infer the model complexity from the data.",
            "For example, when you do a clustering, you may often ask question how many costs we need for describer particular data set.",
            "So here if you.",
            "Using nonparametric Bayesian methods like usual process mixture model, you can automatically infer number of components from your data.",
            "So this is a automatic model selection solution.",
            "So also if your data changes like if you have a data size screws, you can your model kind of debt has ability to adapt because it's a describers carpets over even number of possibles.",
            "If your data become more complicated, you may need more.",
            "Opponents to discover data."
        ],
        [
            "So zero for this our topic on basing methods.",
            "There are lots of nice tutorials in last three years and including XML needs an on the basic basic methods.",
            "Also, basing nonparametric methods.",
            "Also, there's a nice article on I think it just published last year.",
            "It's to view the basic nonparametric field."
        ],
        [
            "So, OK, let's move on to the first case, but how?",
            "How do we do the clustering using non primary ballot measures?",
            "OK."
        ],
        [
            "So I guess the most of your.",
            "I know this for capturing the goal is to partition the data into different groups like before Mark.",
            "Here you different colors and for example for this better maybe we put into four groups.",
            "OK."
        ],
        [
            "So if we want to apply basing method to do clustering, so we basically need to specify two things.",
            "The first wise we need a likelihood model to discover data OK. Another is a prior distribution over the model parameters."
        ],
        [
            "So here's a particular examples particular sample on the primary approaches to do clustering.",
            "It's called Gaussian mixture model, so if you have a real valued inputs, we define the light code as.",
            "Like or as a major over multiple components, multiple causing components and each pound component maybe follow some prior distribution.",
            "We denoted by \u03c0 Pi K, so here are considered key components.",
            "OK, so for this basic model for the basic properties to model, we can use in GM algorithm the position maximum maximization algorithm to maximize the deadline code and to optimize to activate this kind of parameters the dosing parameters.",
            "With me and the course right?",
            "So this is very standard.",
            "You can find this from any machine learning textbook."
        ],
        [
            "So we can use that power cable representation for this basic model.",
            "So we divide.",
            "This is G as a mixture over multiple Atom distribution.",
            "This is drag Delta function.",
            "It's a it's a point function at a particular location 5 and this has zero mass of other other points and but they also guarantee that the integral equal to 1, so it's a it's a distribution point distribution, so we kind of mixture.",
            "Parameter here then.",
            "This is the process of the generated data can be described this query it this way for each data point we sample with sample a distance from this this discussion it's a discrete distribution, right?",
            "It's just describable over finite number of items, so each each particular sample is just one item from this distribution.",
            "Then we use this sample parameter to generalize their data like we using.",
            "For example, using Gaussian model to discovery weather data.",
            "So this reversed and adjust alternative definition of the Gaussian mixture.",
            "They gotta make model with K components OK."
        ],
        [
            "So we so far before 2K.",
            "Then we need another party.",
            "So it's a prior over the parameters.",
            "So we need a prior overlay.",
            "The mixture mixing proportion pie.",
            "So for example we use the one common choices.",
            "Use the contract duration prior, like this one.",
            "We have a parameter 5 divided by the number of components and before the crossing components the parameter can follow some.",
            "Again, it's a complicated pricing for them.",
            "More invoice we showed OK.",
            "So like always the same, it's a this is causing mental model."
        ],
        [
            "So for the.",
            "We apply there until definition.",
            "Alternative representation of the Gaussian mixture model and we can write to the building.",
            "Gotta make model in this way an so for this atoms.",
            "This job from some best estimator based distribution and these are mixing proportion is sample from some jewelry prior.",
            "And this is our because this is random variables.",
            "So this measures random layer.",
            "It's a discrete over the 10 atoms and for sharing things with your particular sample from this discrete distribution, and we generate the data so that everything is very standard from the reputation of the Gaussian mixture model OK.",
            "So perfectly with visualize this process in this way, this is the best measure.",
            "It could be continuous and delete.",
            "This is a discrete random energy and we for each data point withdraw sample from GK."
        ],
        [
            "So for bathing methods we do the posterior inference over the order, letting rebels.",
            "And of course, for as I said before, for this Bayesian inference, it's generally hard, so we need some more approximation like we can do version approximation.",
            "We can do Markov chain color, you can, you can choose anyone you prefer, OK."
        ],
        [
            "So then the question is, for this kind of question probably you may ask how many casts is good to discover public data for them.",
            "For this data point for this data set, is it better to use four or two classes to describe the data?",
            "So how do we solve this problem?",
            "Maybe to automatically identifies a number of components?"
        ],
        [
            "So here is another magical solution, not embedding solution.",
            "So again, we can we define like older.",
            "We can use a prior.",
            "So here we choose unformatted process.",
            "Its usual process to discover the if possible the even number of possible components."
        ],
        [
            "And just give you some quick ideas about the judicial process.",
            "It's a flexible, not permitted prior defined over even number of components.",
            "And we basically still.",
            "Here is the notation we use.",
            "So if she follows a future prior process we write in this form and here we have two parameters, Alpha is the concentration parameter.",
            "Let's get.",
            "It controls the number of components you will have in in error, and they zero is the best measure is the best distribution OK?",
            "It's it's basically the pricing, but just think about it as a prior over the cluster specific parameters."
        ],
        [
            "So here's a mathematical rigorous way to define the judicial process.",
            "So we say so G is a random measure.",
            "It's a defined.",
            "Also may have space exists, ample space and algebra, and have a parameter Alpha and the judicial process is discussion.",
            "Mayor says that for any final position over the summer space and we this kind of variables.",
            "So because this is random partition for each part this will be a random variable induced from the partition, right?",
            "So we've put all these variable together.",
            "Note that we just care about the finance, the final position, so this is a.",
            "This is a much more random variable and justice of multiple random variable follows that usually distribution is motivated for discussion and the parameters defined by from the best measure right?",
            "So this is a.",
            "The basic definition.",
            "So notice that I just define.",
            "Are there any final position so you actually tend to infinitely need some theorem congruent consistent theorem to attend to make sure that you have a consistence to corpus data discovery within normal components.",
            "So I just skip the details about the theorem, just give you some basic ideas about this."
        ],
        [
            "So for the for the judicial process, there's where nice representing.",
            "Met with property one this particular sample from the judicial process that can be written in this way.",
            "It's a countable infinite number of atoms and use some mixture weights.",
            "Pie right?",
            "So note that this is a.",
            "This is very similar to the.",
            "Mixture mixing representation office.",
            "They they got a mixture with cake with finance K components right but here we have in the number of components.",
            "So this is a discrete right.",
            "We say that if we draw a sample from this mayor, we have another probability too.",
            "So sorry if we have multiple, maybe we have two samples from this mayor.",
            "We get to have another probability to hit.",
            "Do you have the identical samples?",
            "So this is a basic idea why this process can be used as clustering.",
            "'cause suppose we have a data point side job.",
            "Multiple samples from this, this mayor this Gmail is this one.",
            "I have a probability, not their probability to just sign the data point to the same, the same components.",
            "If we have the same parameter right?",
            "So this is a way we can part in the the.",
            "The density into different components.",
            "OK.",
            "There are some static guarantees at C4.",
            "You have adjustable finite number of data points.",
            "The number of components in it is also finance to discover data."
        ],
        [
            "So they."
        ],
        [
            "Question is, are hard to set this up mixing weights?"
        ],
        [
            "There is 1 processor called Stick Breaking process to define this this rates.",
            "So basically I have another variable better, better wearable follow from a better discussion with parameter 1A and then I can construct the mixing weights pie for the first one just as the first sample from a better description for the next one it's constructed in this way so I can visualize this process.",
            "In the following way, so suppose this is the stick with the Union lines.",
            "I first joined a sample from a better discussion, like a better one.",
            "I do a cutting of this Dick and put the this part as a pipeline.",
            "And then I I do another cutting for the rest.",
            "OK, so I joined sample.",
            "The cut is from the sample, another sample from that distribution, like a bad two and I put it this part, that's Pi 2 and continue this process and I can define the all these these weights you can see from this visualization that.",
            "There are some destination.",
            "Overall these pipe parameters will be some 21 right if you continue, continue the partition to Infinity.",
            "Some of the length will be.",
            "You could choose a unit length of the steep.",
            "Yeah."
        ],
        [
            "So another useful reporting update direction process is called Chinese restaurant process.",
            "It's uh, it's local process on the parting of data, so it's your describers the metaphor to metaphor of.",
            "Maybe in customers sit down in your Chinese restaurant with even number of tables.",
            "OK, so the metaphor is described as follows.",
            "The first customer sees as the first table with probability one and for the next thing coming.",
            "For example, the end customer.",
            "They choose a table with a probability according to in two scenarios.",
            "If a table has some previous customers have sit down, it will be choosing according to the popularity of the table.",
            "Proportion to the popularity and case number of customers that have seen her sit down in the in particular table K. And also you have another probability to sit in on you table.",
            "A new table means there are no customer have sit on the table.",
            "So again, it can be resized in this way.",
            "Suppose I have even number of tables for the first category has probably went to sit down in the first table and for the for the next second customer that has a probability 1 / 4 + 1.",
            "So according to the definition right to sit in the first table and also another probability of our press 1 to sit down and sit on another table.",
            "OK, so for the subculture has this probability?",
            "Here and probably sit here and also we have another, probably due to a new table, right?",
            "So you can continue on this process to assign.",
            "Suppose you have a.",
            "Each customer represented at point.",
            "You can in this way you can assign the dead pointing into different tables and different table can be think about these different components and so is this where you put in the data points into different components.",
            "You do the Castle right so?",
            "OK."
        ],
        [
            "So you may want to the relationship between the generation process and and you have process in fact that the process is definitely mixture mixing distribution for the channel process.",
            "So in other words, in other ways you can.",
            "So it means we integral over the random mayor T and we get to the general election process over the all these samples.",
            "So it's a quarter here.",
            "If you integrate out.",
            "If traps out, this random random layer, you will die rectifier channel process over over the parameters Theta and you can still combine with likely model discover data right?",
            "So this is the basic idea.",
            "So why do we care about the general process?",
            "Because you give you a natural way to gel samples so we can do.",
            "We can develop a sampling algorithm to do the inference for the usual process mixture models."
        ],
        [
            "So here is the best girl procedure of the Gibbs sampler for which we can do give sample for for the usual mixture model using the channel process repetition so.",
            "For the components with a with a non zero points have been assigned to that component withdraw or is this equation so you can see that in nature divided into 2 parts this part is the prior, the prior it's defined from the IT can be derived from the general process recognition and the 2nd is the likelihood the lack of a particular data points and four new component.",
            "So I represent a new component in this way, basically different from all the other components will have seen.",
            "OK, so for the new component I need to do an integral because I don't know the the parameter of the of this component is totally new to me.",
            "OK, so I do the integral over all possible or possible parameters Theta for this component and I can compute it this way again is proposing to.",
            "I2 parts the first part is the prior derived from the Chinese restaurant process, the sacrifice, the margin, like hold of assigning this step point to this component to this new company.",
            "So very intuitive from these equations."
        ],
        [
            "So for the process we K time in multiple layers.",
            "For example questionnaire is that if we want to model for example M. That sets from maybe from different sources about the same event.",
            "We can build multiple miniature models over the different data sets, and in some in some applications maybe.",
            "This test sets are correlated with.",
            "We want to share some components here.",
            "Some component between this multiple mission model.",
            "So what we have to do that you build another day of judicial process?",
            "So it's basically can see that this is at the top layer.",
            "This is our best player, so we're under jaw sample from judicial process and this is a discrete right.",
            "So there are four different datasets.",
            "I draw a particular instance from this this discriminator, so because there is discord is guaranteed that the multiple jobs will share some atoms.",
            "They basically have the same support in the probability term, so so they.",
            "Right, so they share the same set of atoms but with different mixing proportions.",
            "So in this sense the the multiple datasets we share the same mixture components OK, so one particular application for this so called hierarchical judicial process is to build a Top Model."
        ],
        [
            "I give you are basically over over the top models.",
            "So in this afternoon I will show you some some work we did in this topic.",
            "So the topic model is it's very popular basic mixture model.",
            "Fix it, it can be visualized in this way.",
            "Suppose I have two topics.",
            "Each topic is a mathematically discussion over the all the terms in Additionally.",
            "So for example, for this topic will have a probability point .0105.",
            "Of all the Tom images and .05 for the JPG.",
            "You can roughly get the idea of this topic right.",
            "It's about the image.",
            "So for this same topic, it's maybe about some power of or current or circuit.",
            "OK, so then for particular documents it's a mixture of over this multiple topics, like for them.",
            "For this document it's a point edge.",
            "From this from Capital One and .2 from the public two.",
            "So for each particular word it's assumed to be.",
            "From particular that particular topic, as I indicated by the colors for the query, green color from topic one and blue on blue awards general from topic to write, so you can get a better idea for not documenting will have a different mixing proportion and different kind of popular elements for dinner words.",
            "And if we put all these things in the best in term, for example, we put a prior over the mixing component over the topics and over the mixed proportions we get.",
            "Of also reduce the contractor prior will get the very popular model collecting social location or LDA in short.",
            "So this is a very basic idea about the topic models."
        ],
        [
            "So we are graphically we can visualize in this way, so we have all the data.",
            "Is the mixing proposal for particular particular document and this is the top examiner political world and this is observed world.",
            "OK so this file our topics there there discussion over the words in a dictionary and can the joint Distribution Committee this way, according to the semantics of the graph model, everything is very standard so.",
            "So here I put to emphasize that for Standard Model maybe we reduce sweeper, specified number of topics and we can do the mixture model.",
            "Always rigorously, this is Adam mixed model.",
            "It's a mixture of mixture because two layer GT Cabrio documents and awards right.",
            "Sold."
        ],
        [
            "But for the regulatory process, we can apply to fly here to develop a nonparametric model.",
            "Schedule set up for the best measure we can rectify as I imagine marginal distribution for example, of dimensional ducher dispute with the number of words in a dictionary and.",
            "So here's the Clock model.",
            "OK, so they she O it's.",
            "It's accountable, you can measure just collection of of this.",
            "Because our best measure is the discussion over the marginal probability vectors.",
            "So G is even a collection of this of this workers right?",
            "So each Atom is a McNamee distribution overview words.",
            "OK so so basically chat Tommy is a topic, so then for each document I draw a sample from this discrete TTL mayor.",
            "So it's also also a mixture over the topics.",
            "So it's just selected for a particular document.",
            "Then for award I just sample from this mayor again.",
            "This is discrete, discreet are coming to the theory of judicial process.",
            "So CIJ is a particular topic particular topic to discover the the observed world.",
            "So in this sense we have a nonparametric process to discover the Top Model.",
            "So we can infer the number of topics from the data."
        ],
        [
            "I guess sure some examples.",
            "So this is the results from the paper.",
            "8 feet to the GP topic.",
            "Model to biological abstract data set and you can see that this publicity is a common measure to measure how well your model fit the data.",
            "That's going to test on its.",
            "Calculated on testing data set OK so you can see that for the standard Top Model LDA, you have the parameter K to tune.",
            "So if we change the number of topics you get different performance.",
            "So you can see that roughly in this area that get the best performance.",
            "But if you use the two large billets overfitting for some some.",
            "Some risk it over or video data, but if you run the nonparametric ATP make model you can just have.",
            "One point, because the number of topic is is different from your data, right?",
            "So also you can see the problem is making the best points by tuning the key parameter."
        ],
        [
            "So also this finger show you if you draw samples from the posterior, you roughly you can get.",
            "You can see that most of the most of the documents are has about 65 number of topics.",
            "So because we have a material over all possible number of K so this is.",
            "For each sample, you may have different values, so here is hugely histogram over multiple samples."
        ],
        [
            "OK, so so here is the same for the clustering, so next I will move to the another processor to describe the feature learning.",
            "So any questions so far?",
            "I just finished the first part."
        ],
        [
            "So for future learning, because the basic celebrate this way.",
            "So suppose we have objects and at points we want to learn a little feature feature vector for each data points.",
            "So these are are we can formulate this problem as a metric learning problem.",
            "So I put the feature vector of each each object in true and connecting to a metric letting feature metric and for example for for the object line.",
            "At this kind of features for black black entries there, that means that.",
            "At that point, has not zero value for this feature.",
            "For the wax interest, they better means the object doesn't have this feature.",
            "So suppose if we want to learn real value official metric so it can be decomposed as a elementwise product of the binary metric and the real value value.",
            "So here we are.",
            "Sorry.",
            "Yes, I think this is an integer, but it just simplifies as a binary, so it's just the indicator whether object has a particular feature.",
            "The feature value is determined by another real one real value metric, OK.",
            "So for the building setting, we can put a prior over the over the future metrics using a productive.",
            "Over the prior biometric and the real value metric.",
            "So."
        ],
        [
            "We just consider the binary binary parts because it basically gives you an idea of how how sparse your feature will be, how whether you did also determine whether a particular data point as political feature, right?",
            "So here's a let's start from the word simple setting.",
            "If we know the number of features like a, K, the K dimensional features, we can use a very simple bed Bernoulli process to discover the latter figure magic.",
            "So here's the idea.",
            "So each column carbon 2 particular feature I. I assume that the feature of the features of multiple objects are drawn from the same Bernoulli distribution with parameter Pi KIK associated with feature K. So I say indicated here and in the Bayesian term.",
            "I also assume that the all the Pi K following the common prior.",
            "So the natural choice is a better is better prior.",
            "That is correct.",
            "It's a conjugate to the point you can think about this as like all of the features and the sale price, so they are conjugate.",
            "So if we know the features, if we draw a sample from the this process, we have the basically cover the feature of each death points.",
            "So we can connect to the observation for them we use a crossing like model discover data, so this is worse than.",
            "OK."
        ],
        [
            "So some analysis of these are finding the Latin feature model, so you can compute the marginal probability of a particular feature metrics.",
            "You just do some regular.",
            "It's very simple to do this are using this.",
            "This is equation, so basically it's normalization term of the better, better distribution.",
            "So by the point here on the inside, their features can see that the marginal.",
            "So the probability joint probability of the metric is composed in 2K terms.",
            "So that basically means that the features are independent.",
            "So this is a obvious formal definition, so we assume the different features followed a different varieties distribution OK."
        ],
        [
            "So you can also compute if we would develop a sampler, you need to.",
            "Maybe you need this kind of conditional distribution.",
            "You can compute this against using the marginal distribution within the previous slide and then finally we get the conditional this term and you have the M is the number of number of data points that has has chosen this feature K except the data point.",
            "Hi, because Ice is a current of their point that we focus on.",
            "So this is the formula prior so everything is worse than that.",
            "They just apply the qualities.",
            "And Rob this one OK."
        ],
        [
            "So, so we can generalize so we can some.",
            "You may want how to generalize this kind of finding letter feature model to Infinity.",
            "So if we just put care to Infinity, there will be some trouble.",
            "So it basically means if we just simply Simply put Keita invented, each particular metric letting figure metric, we have a zero probability from the previous definition, so we need some some technique or some tickets.",
            "To derive a non zero probability of limit.",
            "So this is.",
            "Big achieved by defining a current class using letter order form of these metrics.",
            "So basically you order the columns the features according to the number of appearance of in the points in the left order sense you will get your developer equivalent class of multiple metrics in the in the same class and in that way we can define probability over the current cost and this is not their probability.",
            "You can keep pressing this form.",
            "So KK passes the number of active active features, not 0 features, OK?",
            "So basically, not do columns in the latency in magic, there's some other.",
            "Numbers here by.",
            "Yeah.",
            "OK, so the this is called the Indian buffet process."
        ],
        [
            "There are also a metaphor to describe this up.",
            "This process of generating a biometric.",
            "So think about the number of customers coming into a Indian Indian restaurant they have can have even number of pitches so this process.",
            "Discover how the customer choose the dishes, so therefore the first customer it's choose Karen dishes from a person distribution with parameter Alpha.",
            "So if you remember, this is the finding the backbone process archive the perimeter of the bed prior.",
            "So here for the interview process.",
            "We still have this parameter.",
            "I'm there for the next customer.",
            "For that I would choose it.",
            "Also consider two scenarios to choose dishes so it choose the older dishes for the existing digits.",
            "That means it has been chosen by previous customer according to the.",
            "Again, according to the popularity of the dishes.",
            "And I MK is the number of customers that have chosen dishes K also you have an audio or distribute distribution to choose some additional dishes will have haven't chosen by previous customer.",
            "Again following a person distribution.",
            "So far, for this particular instance, for example, the customer one, choose the dishes 123 and customer two choose the oldest 1 three and also we choose our new dishes four to five other games for the customer.",
            "Three, it has ordered dishes here, and also new dishes, so you can continue this process to generate a binary metric."
        ],
        [
            "So against therefore, the II preferred process, there is a sticker bragging construction, so these are the these as a particular entry of the binary market will follow up on redistricting this very intuitive from our our duration of the process, right?",
            "So for this parameter pie it's a driver again, definer users debugging process.",
            "It can be visualizing this way.",
            "So again, we have for a unit length disk and can we do the do the break OK?",
            "So the first sample, maybe I draw from better disappear and I get the first sample like the point edge and.",
            "Then I I definition in the pipeline you could to the better variable.",
            "OK.",
            "So then I join another sample.",
            "From a better district, maybe .25, and then multiply this number to the to the previous length .8 I get to the number of .4, right?",
            "So for the eyes this number is the 2nd second dimension.",
            "So I join another sample from better prior for them.",
            ".4 multiplied from the previous length .4 from this one right so I get .16 and resigned to a submission.",
            "So I continued process and can generate the probability of.",
            "Over the particular feature so you can see that if you continue on this breaking process, the probability of.",
            "Appear feature to be active get decreased, right?",
            "It's kind of a potential decrease, so this is this sparsity property is very important to guarantee that you can have fun #4 features for particular data set, so this is very important property of this independent process."
        ],
        [
            "So again, I can do sampling.",
            "So according to that definition.",
            "So this Remember Remember you for the final, better quality electric model?",
            "I have this condition alright.",
            "I drive this one for the if we set K to Infinity we need to consider 2 cases.",
            "So for the features that have been taken by by some data points we the conditional probability is proportional to the number of data points that have taken this feature.",
            "I'm just following new features withdraw from some portion distribution we have.",
            "We have an audio probability to sample some new features."
        ],
        [
            "And we can combine this this process with likely model.",
            "For example using Gaussian likelihood discover the real weather data."
        ],
        [
            "And we can still do the do the sampling like combine by considering the data.",
            "So, so we have this posterior so we can get this condition also.",
            "It's very intuitively one party formed the process at the prior and otherwise get like hood under.",
            "Right, so for the if we are like a model is the simple.",
            "For example for linear causing the deadlock that can be computed in analytical form maybe, but for."
        ],
        [
            "In general we have maybe have some harder inference problem.",
            "For example, our if we likely complicated, we need to deal with some non conjugated models.",
            "So we do particle sample in or other sampling methods.",
            "Also we can give up variational inference.",
            "For example, you can use a stick breaking construction and to have some variation approximation methods and also there are some research about.",
            "To apply."
        ],
        [
            "So the last part I think I have maybe 7 minutes, but I quickly go over another process is.",
            "It's a Gaussian process for regression."
        ],
        [
            "Again, we start from simple.",
            "OK, so we consider simple noisy observation model.",
            "So this is our response and we assume that it's generated from some function plus some noise like we consider Gaussian noise.",
            "Very simple case if we if the for linear regression we consider this linear function.",
            "For example every is.",
            "Dot product of weights and our feature vector is very simple.",
            "And in this way from this observation model we can we have this crossing like code.",
            "The forbidden we put it for them for the conjugated prior accounting prior here for the.",
            "For the model parameter and we can do the inference using Bayes rule.",
            "Everything 1st and so because the crossings.",
            "Is a nice it's conjugate, so we can write down the posterior Gaussian right?",
            "So we just updated the parameters.",
            "And for prediction for four new Fairpoint, we predicted the function.",
            "We can do this integral gains analytically computed.",
            "Little checkable, so it's a game of dosing distribution.",
            "OK, so everything is worse than that."
        ],
        [
            "So, but you may think about the linear regression model can be too restrictive, so we may simply want to do nonlinear.",
            "OK, so so one trick is I think everybody knows this device just called kernel trick.",
            "We map the data points into another space using a feature mapping function."
        ],
        [
            "Just give you some quick samples.",
            "So our goal is to find a mapping function and then we can still if we have this mapping function, we can still do the linear regression using the product in the in the map fish found feature space and if you put Gaussian prior over here over the parameter, we can still do the do the dosing inference.",
            "Everything, store standards just likely live very simple linear case."
        ],
        [
            "And here are some examples you can choose to for the for the mapping function like, you can use this.",
            "It's I think a Colonel in the kernel field we call it causing it's called RBF kernel, or we can use the polynomial term to define this metric function."
        ],
        [
            "And also we can parameterise using for someone using a neural network to paralyze these making function 5.",
            "Using some another parameters to to discover this this mapping function."
        ],
        [
            "So so we may want to have a complete, completely nonparametric, a way to do this.",
            "And suppose we don't have.",
            "We had to remove the implicit parameterization of the function.",
            "Can we do this OK?",
            "So in fact we can do.",
            "We can use in causing process to manage a uncountable even number of functions."
        ],
        [
            "So so before I just can't describe the process, just to remind you of the basic concept of the random function, random random variable, you may want hard to represent a function random function.",
            "So here is one way and function basically can be represented by even even a vector with the index set.",
            "For example for this.",
            "For this function we may if suppose we have an even number of index.",
            "In this axis we can, using the function values to represent the function right and for particular data points, and so if I fix the deadline here, I draw a vertical line and because the function is random and the function value at this point will be a random variable, right?",
            "So this is the way we describe.",
            "We discovered the random function using.",
            "If you give me a set of index index is I use the function values as a random variable to discover the function.",
            "So for example, maybe forgotten process.",
            "I use it maybe across industry to describe this particular variable."
        ],
        [
            "So here it says the definition of Gaussian process.",
            "OK, so you still process or if I every finite set of variables.",
            "For example, if the deck set is financially can discover the joint stream over the function values as it's multivariate Gaussian random variable.",
            "So Gaussian distribution.",
            "So remember that concentration is parameterized sesame and current functioning.",
            "So awesome process is specified by the main function of the current function.",
            "Basically, taking over all your random function."
        ],
        [
            "I'm just so, for example, for the simple Bayesian linear regression is it's a Gaussian process in this way.",
            "For example, we can derive that the main function is zero and query functions is the fine.",
            "This is occurrence of noise of sorry, the prior.",
            "This should be transposed.",
            "OK."
        ],
        [
            "So, so the code using called process it to do to do prediction for the noisy free setting.",
            "We know the true function value according to the definition for the new testing points and Windows from the definition that for this finite vector it follow across industry right from the definition process.",
            "So we can divide.",
            "We know that from the joint distribution.",
            "The condition or so for this condition is the prediction predicted this week about so it's still got it from the property of Gaussian distribution.",
            "So this reversed and."
        ],
        [
            "For the, for the noisy observations which we observe the response and from some noisy model, and in this case of the queries will be changed, the plus some noise variance.",
            "And in this case we can still according to the definition this is joint multivariate browsing and this conditional is also causing right?",
            "But this has been shifted by the by the noise words, so this reverse standard."
        ],
        [
            "But other issues for Gaussian process, for example for clarification or some other scenarios, so we may have remember if you remember for the for the recurring cast they response model is causing is contributed from the.",
            "Opposing process, but for the clarification or other scenarios you may have an unconjugated models.",
            "So they alot of research has been done on deriving some approximate approximate algorithm to deal with the.",
            "This kind of a scenario for them apply the closing process for cascading predicted control or relational learning.",
            "OK, so there is so very nice website to close all this material about Gaussian process, including theory applications.",
            "You can check out this website is if you're interested in this top."
        ],
        [
            "OK, so.",
            "I'm just alone times OK so just the RO quick summary.",
            "So for the first part I give you were basically solve the basic methods and introduced the three popular non permit databases.",
            "So then we have a launch correct?",
            "And for the afternoon session of issue, some very recent work may be conducted in microphone, but how to do the constraint based inference with in particular with mixed market constraints to improve the discriminative ability of business model and also I'll show you some interesting applications to clarification much learning link prediction and also recommendation."
        ],
        [
            "OK, I think it's time to break.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK yeah thanks.",
                    "label": 0
                },
                {
                    "sent": "Also, first of all, thanks to organizers invited me here.",
                    "label": 0
                },
                {
                    "sent": "So it's my great honor to speak here and to share with you some of the most.",
                    "label": 0
                },
                {
                    "sent": "Obviously they talk about some recent work in my group about basic methods.",
                    "label": 0
                },
                {
                    "sent": "R.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the plan of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So little early naturally split in two parts because they have the morning session and also have the optimization right.",
                    "label": 0
                },
                {
                    "sent": "So for the morning session I speak for one hour about one hour and basically let's give you some quick idea about the white keyboard based methods and also our particular focus on the recent advanced on the nonparametric Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "Then we have for lunch break for one hour and after that we are continuing this afternoon and particularly we show you some recent work on the record constraint drug testing methods and I show you some interesting applications to topic modeling link prediction.",
                    "label": 0
                },
                {
                    "sent": "So recommendation and and some other cases OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a quick reminder you about some basic rules of probability.",
                    "label": 1
                },
                {
                    "sent": "I believe for most of your maybe no better than me, and so here we have some better concept.",
                    "label": 1
                },
                {
                    "sent": "We have probability of some verbal and we have conditional probability.",
                    "label": 0
                },
                {
                    "sent": "It's basically some notation here.",
                    "label": 1
                },
                {
                    "sent": "We hope to scrap the joint probability and we have some basic rules in the in the probability theory.",
                    "label": 0
                },
                {
                    "sent": "So the product rule and some rule and also the conditional probability defined by the.",
                    "label": 0
                },
                {
                    "sent": "Divide of P joint over the marginal OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you could combine combine this definition of control and also applies the product and some rules we get recalled basing rules.",
                    "label": 1
                },
                {
                    "sent": "It was first proposed in it's it's part 250 years ago.",
                    "label": 1
                },
                {
                    "sent": "So actually this year is the it's marking here and celebrate the 250th anniversary of the so called best theorem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First overalls, they they basically maybe the problem.",
                    "label": 0
                },
                {
                    "sent": "We care about the machine learning.",
                    "label": 1
                },
                {
                    "sent": "When we apply the best rule.",
                    "label": 0
                },
                {
                    "sent": "OK so so here I I sweet speeches and notation a little bit.",
                    "label": 0
                },
                {
                    "sent": "Let D denote the given data set and M is a particular model.",
                    "label": 1
                },
                {
                    "sent": "We care about knowing the best rules say that we have this party redistribution and depressed of the prior and likelihood and divided by the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "All the evidence.",
                    "label": 0
                },
                {
                    "sent": "And to recap, we can apply this best best rules to maybe.",
                    "label": 0
                },
                {
                    "sent": "Scenarios that we can compare different kind of models.",
                    "label": 0
                },
                {
                    "sent": "For example, if we may have different family of models like linear model or nonlinear models and we can impress the posterior over the whole whole family.",
                    "label": 0
                },
                {
                    "sent": "I'm the party real giving some data and use this marginal likelihood basically integral over all the all the possible instance in the in the model family.",
                    "label": 0
                },
                {
                    "sent": "OK, we can use these factors as scoring criteria to compare the different families and do model selection for example.",
                    "label": 0
                },
                {
                    "sent": "And also we care about the prediction like if we have a new incoming instance we make computers that predicted probability.",
                    "label": 1
                },
                {
                    "sent": "Giving these are his historical data and the particular model family, and they also process the integral over the material given a particular model and over the whole family of the model, right?",
                    "label": 0
                },
                {
                    "sent": "So on the most of the common assumptions for them.",
                    "label": 0
                },
                {
                    "sent": "So basically for all the model based approaches, we assume that the Betta they better distributed independently continue on that particular model, right?",
                    "label": 0
                },
                {
                    "sent": "And simplify this this first.",
                    "label": 0
                },
                {
                    "sent": "Use this one if we know the particular model instance they support.",
                    "label": 0
                },
                {
                    "sent": "This conditional is simplified to this one and this is independent of the historical data, right?",
                    "label": 0
                },
                {
                    "sent": "So this is very common assumption in the model based approaches compared in contrast to some memory based approach in memory.",
                    "label": 0
                },
                {
                    "sent": "Best we basically remember all the data we do the computation in some complicated way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's some common questions when we apply when we do billing method.",
                    "label": 0
                },
                {
                    "sent": "So firstly I will.",
                    "label": 0
                },
                {
                    "sent": "People would like I would like to ask maybe why we use Bayesian and the 2nd is so where does the prior come from?",
                    "label": 1
                },
                {
                    "sent": "And the third one is how do we do the integral?",
                    "label": 0
                },
                {
                    "sent": "So because remember we have a lot of have a different scenario but to do the integral computation.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 1
                },
                {
                    "sent": "OK, so here is a general questions, so let me give you some brief answers to each officer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why be basing their their many possible answers like basic methods are very flexible and very intuitive.",
                    "label": 1
                },
                {
                    "sent": "It's a I mean so easy to interpret the different parts of the model and also there are some some theoretical arguments.",
                    "label": 0
                },
                {
                    "sent": "So here it's a very result.",
                    "label": 0
                },
                {
                    "sent": "It called Death Infinity theorem.",
                    "label": 0
                },
                {
                    "sent": "It's suppose we have a set of points and there's some some definition of card if interchangeability.",
                    "label": 0
                },
                {
                    "sent": "For for any finite subset.",
                    "label": 0
                },
                {
                    "sent": "For example, a number of points and for any permutation of the order.",
                    "label": 0
                },
                {
                    "sent": "And this distribution doesn't change.",
                    "label": 0
                },
                {
                    "sent": "So that means the order doesn't matter in the discard this data.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of scenarios we we can apply this kind of property, for example for the most, for the very common feature, like bigger word representation of documents, it's it satisfies these kind of property, so the theorem says that if we.",
                    "label": 0
                },
                {
                    "sent": "Better satisfies this kind of property.",
                    "label": 0
                },
                {
                    "sent": "Can be the joint distribution can be described as a as a marginal over another by over some augmented variable like M, it follows some distribution and this part.",
                    "label": 0
                },
                {
                    "sent": "Continuing this M the data points will be independent so.",
                    "label": 0
                },
                {
                    "sent": "So it can be visualized as in this way.",
                    "label": 0
                },
                {
                    "sent": "This is a marginal joint district over the object we care about, and it can be integral as this use the additional variable M and continue this verbal M with this observed data points are independent, so this very basic idea is basically only in Beijing methods OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next question is how to choose priors.",
                    "label": 1
                },
                {
                    "sent": "They are basically in the pacing of community.",
                    "label": 0
                },
                {
                    "sent": "There are people care about difficult of priors.",
                    "label": 0
                },
                {
                    "sent": "So first, why is the objective prior it try to be non informative?",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't have any belief.",
                    "label": 0
                },
                {
                    "sent": "You can use some of qnective priors, so we're going to have some some.",
                    "label": 0
                },
                {
                    "sent": "This property is some nice properties, then another is another family, so we use subjective priors.",
                    "label": 0
                },
                {
                    "sent": "We we try to use the prior to capture some of our belief prior belief before seeing the data.",
                    "label": 1
                },
                {
                    "sent": "So for this objective, prior to the one I people would like to ask how to choose, maybe how to choose the.",
                    "label": 0
                },
                {
                    "sent": "The hub parameters, if any right so so one way is to weaken the influence of the hub parameters did to you some something we called hierarchical priors so we can use multiple layers to represent a prior.",
                    "label": 0
                },
                {
                    "sent": "For example, this is the.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is target will target prior over the model.",
                    "label": 0
                },
                {
                    "sent": "We can we can introduce another layer, maybe put some hyper prior over the over this one and we continually hierarchy attention.",
                    "label": 0
                },
                {
                    "sent": "To Infinity in theory.",
                    "label": 0
                },
                {
                    "sent": "So the best belief is that if they.",
                    "label": 0
                },
                {
                    "sent": "If the prize is higher from the model you care about, the influence of week.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic beliefs we when we apply the heart of fire.",
                    "label": 0
                },
                {
                    "sent": "And also there's another thing we called the empirical price.",
                    "label": 0
                },
                {
                    "sent": "It give you some method to ultimate the hyperparameters of your prior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So for example, we would like to impose a prior over the model an giving some.",
                    "label": 0
                },
                {
                    "sent": "Some parameter we can ultimate admitedly the hyperparameters by maximizing the marginal life hold.",
                    "label": 0
                },
                {
                    "sent": "This is a given data, right?",
                    "label": 0
                },
                {
                    "sent": "So there are some other wanted your disability, obviously empirical approach, so it can be reversed because the.",
                    "label": 0
                },
                {
                    "sent": "It's capture system.",
                    "label": 1
                },
                {
                    "sent": "It's all of your data, so we can all come something amiss.",
                    "label": 0
                },
                {
                    "sent": "The specification of the prior, so also it's also has some disadvantages, double counting on your data.",
                    "label": 0
                },
                {
                    "sent": "So because you're so for prior we.",
                    "label": 0
                },
                {
                    "sent": "The goal is to capture some some prior belief before you are seeing your data but but here we do consider the data when we estimated the prior.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a double counting, so it may be limited to some.",
                    "label": 0
                },
                {
                    "sent": "Resulting in some overfitting OK?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, when we choose were prior computationally, we have some trade off.",
                    "label": 0
                },
                {
                    "sent": "It's called conjugate or non conjugated shut off.",
                    "label": 0
                },
                {
                    "sent": "So far can you get priors?",
                    "label": 0
                },
                {
                    "sent": "It can be.",
                    "label": 0
                },
                {
                    "sent": "It's relatively easier because the if the prices contributed to your like hood, we can.",
                    "label": 1
                },
                {
                    "sent": "We know that the form of the posterior right it has the same same family as a prior so.",
                    "label": 0
                },
                {
                    "sent": "But the main question is how to update the parameter.",
                    "label": 0
                },
                {
                    "sent": "So here a lot of examples like golfing golfing.",
                    "label": 0
                },
                {
                    "sent": "If you use a Gaussian prior, causing like code is there we conjugate its conjugate pair.",
                    "label": 0
                },
                {
                    "sent": "Also we have different multinomial conjugate pair.",
                    "label": 0
                },
                {
                    "sent": "So, but another families of non conjugate priors, so for non conjugated it's a more flexible but completion is harder to do the inference.",
                    "label": 1
                },
                {
                    "sent": "So I give you a particular example so we know the.",
                    "label": 0
                },
                {
                    "sent": "So this is a prior over over some multinomial space.",
                    "label": 0
                },
                {
                    "sent": "OK so for do should we we can write to the patient here so you can see that the different dimensions kind of for independent, but they're not truly independent because.",
                    "label": 0
                },
                {
                    "sent": "They have a normalization constraint, right?",
                    "label": 0
                },
                {
                    "sent": "So besides that, so we don't have any any correlations between these different dimension but another non computer cases that we could use the so-called largest normal prior.",
                    "label": 0
                },
                {
                    "sent": "We first sample a parameter from a normal distribution multivariate normal and then we do a modest transformation to normalize the and make this vector.",
                    "label": 0
                },
                {
                    "sent": "Supply over them at normal distribution.",
                    "label": 0
                },
                {
                    "sent": "OK so you can see that for this largest novel, because we have a chorus magic, it can encode the correlation structures between different dimension, But for this kind of price, so much harder to do the inference I will show you.",
                    "label": 1
                },
                {
                    "sent": "Some results in this afternoon about how to do.",
                    "label": 0
                },
                {
                    "sent": "Maybe large scale computing for this kind of non conjugate priors?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then this other patches, how do we compute integrals?",
                    "label": 1
                },
                {
                    "sent": "Just to remind you about this impression, this marginally code given the model family we do the integral over all the possible instance in this family, and this integral can be very high dimension.",
                    "label": 0
                },
                {
                    "sent": "We could maybe even information.",
                    "label": 0
                },
                {
                    "sent": "Default nonparametric models and so by Americans is for better method.",
                    "label": 0
                },
                {
                    "sent": "We may consider some some later marbles to discover data.",
                    "label": 0
                },
                {
                    "sent": "In this case you will get even harder integral problem.",
                    "label": 0
                },
                {
                    "sent": "So maybe you have mixed integral have discovered variables you have continued variables right?",
                    "label": 0
                },
                {
                    "sent": "So for discrete integral will be assigned over all possible values.",
                    "label": 0
                },
                {
                    "sent": "So how do we do the universe?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I imbedding community, we do, so we have.",
                    "label": 0
                },
                {
                    "sent": "We have no exact solution.",
                    "label": 0
                },
                {
                    "sent": "We made you just resort to some approximation methods.",
                    "label": 0
                },
                {
                    "sent": "This is approximate Bayesian inference time.",
                    "label": 1
                },
                {
                    "sent": "So there are a lot of common examples for subversion approximations, Markov chain mode, colored methods and application propagation, last approximation and also lot of others.",
                    "label": 0
                },
                {
                    "sent": "So the code is to develop some.",
                    "label": 0
                },
                {
                    "sent": "Relatively accurate and scalable algorithm for to do the integral and to do the prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is a very active area in the immersion learning study to to develop better algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I give you, I just give you some basic ideas about the 1st two.",
                    "label": 0
                },
                {
                    "sent": "They are basically represent most of the.",
                    "label": 0
                },
                {
                    "sent": "The methods we use nowadays.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the white slides.",
                    "label": 0
                },
                {
                    "sent": "The better software version approximation.",
                    "label": 0
                },
                {
                    "sent": "So because of this are marginally code, it's hard to compute, but we can.",
                    "label": 0
                },
                {
                    "sent": "We can approximate it by using some bound.",
                    "label": 0
                },
                {
                    "sent": "So for example, we if we take the logarithm of the marginal marginal likelihood, we can.",
                    "label": 1
                },
                {
                    "sent": "This is just the dependa repression of this.",
                    "label": 0
                },
                {
                    "sent": "Modern, likely using that marbles and models and we can introduce another distribution called version of Distribution Q and its joint over head verbals and the model.",
                    "label": 0
                },
                {
                    "sent": "And we much product here and divide here so it doesn't change anything.",
                    "label": 1
                },
                {
                    "sent": "So then we comply with the Jason inequality and get a lower bound right?",
                    "label": 0
                },
                {
                    "sent": "We move the logarithm into the integral.",
                    "label": 0
                },
                {
                    "sent": "We change the order of the integrand, logarithm, and then we can.",
                    "label": 0
                },
                {
                    "sent": "We can do in some cases.",
                    "label": 0
                },
                {
                    "sent": "We can do this kind of.",
                    "label": 0
                },
                {
                    "sent": "Any glory in some way easier than this one, OK?",
                    "label": 0
                },
                {
                    "sent": "So note that if we don't make any assumptions, it's the boundary tight so that basically means that we can get the.",
                    "label": 0
                },
                {
                    "sent": "Equality here, so just by setting this Q equal to the party real, the true posterior divide by derived from the model, right?",
                    "label": 0
                },
                {
                    "sent": "But the but this still hard to do, and literally so we most common case we use them.",
                    "label": 0
                },
                {
                    "sent": "We need to do some additional assumption like midfield assumptions.",
                    "label": 0
                },
                {
                    "sent": "So basically make it affects assumption that for example for this one maybe assume the hidden marble independent of from the model variables.",
                    "label": 1
                },
                {
                    "sent": "So this is very simple assumption.",
                    "label": 0
                },
                {
                    "sent": "So then we can optimize this bound with this assumption.",
                    "label": 0
                },
                {
                    "sent": "For example, user quantities and are assigned to optimize it.",
                    "label": 1
                },
                {
                    "sent": "We get some local optimal.",
                    "label": 0
                },
                {
                    "sent": "This is reduced.",
                    "label": 0
                },
                {
                    "sent": "The solution here as approximation to the true posterior, and then we also we can do the integral of the lower bound right?",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea of version approximation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, another slides about the basics of the McCall McAuliffe methods.",
                    "label": 0
                },
                {
                    "sent": "So for Microchip mccullar methods through, the goal is to we want to draw some samples from the posterior from the target posterior and we use our samples V to approximate the integral example we.",
                    "label": 0
                },
                {
                    "sent": "We have multiple samples from the.",
                    "label": 0
                },
                {
                    "sent": "The distribution and we can use the empirical.",
                    "label": 0
                },
                {
                    "sent": "Maybe the test user empirical method.",
                    "label": 0
                },
                {
                    "sent": "That really integral and so the best idea is that we define verifying a Markov chain and M-02M1 and Gen 2 and so on.",
                    "label": 0
                },
                {
                    "sent": "So at this time we will draw a sample from the distribution PT.",
                    "label": 0
                },
                {
                    "sent": "Over there, the model and the PT is defined as a because there is a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "It's dependent from the previous step.",
                    "label": 0
                },
                {
                    "sent": "Palmer's team right away by using a transition metric.",
                    "label": 0
                },
                {
                    "sent": "This is this a microphone change, consider probability.",
                    "label": 0
                },
                {
                    "sent": "It's from it that is the probability from the mprime TM.",
                    "label": 0
                },
                {
                    "sent": "So, so we call that the target distribution environment.",
                    "label": 0
                },
                {
                    "sent": "Or is it just a stationary distribution of the Markov chain before it satisfies this this equality?",
                    "label": 1
                },
                {
                    "sent": "So basically you do the integral, you get the true posterior, the true target.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cheers of wine.",
                    "label": 0
                },
                {
                    "sent": "Youthful condition that implies the universe that guarantee that we can get this taken this week as our target.",
                    "label": 1
                },
                {
                    "sent": "So it's so called detailed balance.",
                    "label": 0
                },
                {
                    "sent": "So we multiply.",
                    "label": 0
                },
                {
                    "sent": "So from the impromptu M we multiply the distribution of prime and equal to so.",
                    "label": 0
                },
                {
                    "sent": "So it's from one way to.",
                    "label": 0
                },
                {
                    "sent": "So if you from the employment M which multiplies the, we can view as.",
                    "label": 0
                },
                {
                    "sent": "Prior, maybe an alternative way we can see from MTN prime and use this as a scaffold prior their equivalent.",
                    "label": 0
                },
                {
                    "sent": "We call it the detailed balance.",
                    "label": 0
                },
                {
                    "sent": "And see in practice with design design some micro chamber, methods that satisfies this.",
                    "label": 0
                },
                {
                    "sent": "This kind of condition is conduct detailed, balanced and guarantee that if the card we get the true posterior the true target.",
                    "label": 0
                },
                {
                    "sent": "So this is the basis of our Markov chain color methods.",
                    "label": 0
                },
                {
                    "sent": "OK so I just give you work picture or we offer the basic problem we captured in the basic methods.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions feel free to interpret interrupted me.",
                    "label": 0
                },
                {
                    "sent": "But I'm OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for the best measures we can roughly categorize in two families.",
                    "label": 0
                },
                {
                    "sent": "The first one is called Parametric Bayesian methods, so in this case our model is represented as a finite set of parameters that called Theatre and we impose some prior divine light code.",
                    "label": 1
                },
                {
                    "sent": "We can do the building inference to the best rule.",
                    "label": 0
                },
                {
                    "sent": "Everything is worse than so a lot of popular examples.",
                    "label": 0
                },
                {
                    "sent": "As I said this contradicting pairs.",
                    "label": 0
                },
                {
                    "sent": "Housing price goes in like code and you should try and normalize code they give you.",
                    "label": 0
                },
                {
                    "sent": "The analytical criteria and also investing family there are.",
                    "label": 0
                },
                {
                    "sent": "There is a larger district of research on these so called sparse Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "They use some sparsity inducing prior.",
                    "label": 0
                },
                {
                    "sent": "Plus them like old and they can get some machine could affecting the posterior and do the bathing spots.",
                    "label": 0
                },
                {
                    "sent": "Building inference OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another family is a condom primary building inference.",
                    "label": 1
                },
                {
                    "sent": "So in this case the model is much richer it can.",
                    "label": 0
                },
                {
                    "sent": "You can have infinite number of parameters.",
                    "label": 1
                },
                {
                    "sent": "So to emphasize, for not property with it doesn't mean the model don't have funding parameters.",
                    "label": 0
                },
                {
                    "sent": "It means with the number of parameters can be infinite.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Point, which we should.",
                    "label": 0
                },
                {
                    "sent": "Should we care about so for this nonparametric models?",
                    "label": 0
                },
                {
                    "sent": "We can still do the basing inference, use the best rule we post some prior over the infinite number of parameters space and we we use the like hood.",
                    "label": 0
                },
                {
                    "sent": "I don't do that.",
                    "label": 0
                },
                {
                    "sent": "The posterior inference.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the rest of talk, I will quickly go over the three popular examples.",
                    "label": 0
                },
                {
                    "sent": "So first one is we can define the nonparametric priors over the the probability measure space.",
                    "label": 0
                },
                {
                    "sent": "It's called user prior.",
                    "label": 0
                },
                {
                    "sent": "So by the way, for all these kind of because we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will have an even number of parameters that they prior.",
                    "label": 1
                },
                {
                    "sent": "We just started process.",
                    "label": 0
                },
                {
                    "sent": "It defined on the number of variables.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another example is just process defined on the on the metrics.",
                    "label": 0
                },
                {
                    "sent": "So here we care about binary metrics, but it can be attended to integer or real valued metrics.",
                    "label": 0
                },
                {
                    "sent": "So here emphasize that for this metric you can have an even number of columns.",
                    "label": 0
                },
                {
                    "sent": "So as our assure you, this kind of scenario very useful for to learn features for for different applications and the last one is you can define start process on the function space for them.",
                    "label": 0
                },
                {
                    "sent": "For this most functions you can divide your Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Overseas the whole thing or Hostess.",
                    "label": 0
                },
                {
                    "sent": "For all these are nonparametric process, so we use them as.",
                    "label": 0
                },
                {
                    "sent": "About priors over over some model and in order to do the best inference, we also need another part is the car like hood.",
                    "label": 0
                },
                {
                    "sent": "So here for like oh we just use some common choices like for discretely maybe use multinomial or post on record for real value of data.",
                    "label": 0
                },
                {
                    "sent": "We may use Gaussian like OK so this is our standard.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first question before I go into the details about the each of them.",
                    "label": 0
                },
                {
                    "sent": "The first question we may ask is why be basing number metrics?",
                    "label": 0
                },
                {
                    "sent": "Why we care about this?",
                    "label": 0
                },
                {
                    "sent": "So the basic principle is that we want to let the data speak for itself.",
                    "label": 1
                },
                {
                    "sent": "So that means we we want to automatically infer the model complexity from the data.",
                    "label": 0
                },
                {
                    "sent": "For example, when you do a clustering, you may often ask question how many costs we need for describer particular data set.",
                    "label": 1
                },
                {
                    "sent": "So here if you.",
                    "label": 1
                },
                {
                    "sent": "Using nonparametric Bayesian methods like usual process mixture model, you can automatically infer number of components from your data.",
                    "label": 0
                },
                {
                    "sent": "So this is a automatic model selection solution.",
                    "label": 0
                },
                {
                    "sent": "So also if your data changes like if you have a data size screws, you can your model kind of debt has ability to adapt because it's a describers carpets over even number of possibles.",
                    "label": 0
                },
                {
                    "sent": "If your data become more complicated, you may need more.",
                    "label": 0
                },
                {
                    "sent": "Opponents to discover data.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So zero for this our topic on basing methods.",
                    "label": 0
                },
                {
                    "sent": "There are lots of nice tutorials in last three years and including XML needs an on the basic basic methods.",
                    "label": 0
                },
                {
                    "sent": "Also, basing nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "Also, there's a nice article on I think it just published last year.",
                    "label": 0
                },
                {
                    "sent": "It's to view the basic nonparametric field.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, OK, let's move on to the first case, but how?",
                    "label": 0
                },
                {
                    "sent": "How do we do the clustering using non primary ballot measures?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I guess the most of your.",
                    "label": 0
                },
                {
                    "sent": "I know this for capturing the goal is to partition the data into different groups like before Mark.",
                    "label": 0
                },
                {
                    "sent": "Here you different colors and for example for this better maybe we put into four groups.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we want to apply basing method to do clustering, so we basically need to specify two things.",
                    "label": 0
                },
                {
                    "sent": "The first wise we need a likelihood model to discover data OK. Another is a prior distribution over the model parameters.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a particular examples particular sample on the primary approaches to do clustering.",
                    "label": 0
                },
                {
                    "sent": "It's called Gaussian mixture model, so if you have a real valued inputs, we define the light code as.",
                    "label": 0
                },
                {
                    "sent": "Like or as a major over multiple components, multiple causing components and each pound component maybe follow some prior distribution.",
                    "label": 0
                },
                {
                    "sent": "We denoted by \u03c0 Pi K, so here are considered key components.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this basic model for the basic properties to model, we can use in GM algorithm the position maximum maximization algorithm to maximize the deadline code and to optimize to activate this kind of parameters the dosing parameters.",
                    "label": 1
                },
                {
                    "sent": "With me and the course right?",
                    "label": 0
                },
                {
                    "sent": "So this is very standard.",
                    "label": 0
                },
                {
                    "sent": "You can find this from any machine learning textbook.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can use that power cable representation for this basic model.",
                    "label": 0
                },
                {
                    "sent": "So we divide.",
                    "label": 0
                },
                {
                    "sent": "This is G as a mixture over multiple Atom distribution.",
                    "label": 0
                },
                {
                    "sent": "This is drag Delta function.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a point function at a particular location 5 and this has zero mass of other other points and but they also guarantee that the integral equal to 1, so it's a it's a distribution point distribution, so we kind of mixture.",
                    "label": 0
                },
                {
                    "sent": "Parameter here then.",
                    "label": 0
                },
                {
                    "sent": "This is the process of the generated data can be described this query it this way for each data point we sample with sample a distance from this this discussion it's a discrete distribution, right?",
                    "label": 0
                },
                {
                    "sent": "It's just describable over finite number of items, so each each particular sample is just one item from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we use this sample parameter to generalize their data like we using.",
                    "label": 0
                },
                {
                    "sent": "For example, using Gaussian model to discovery weather data.",
                    "label": 0
                },
                {
                    "sent": "So this reversed and adjust alternative definition of the Gaussian mixture.",
                    "label": 0
                },
                {
                    "sent": "They gotta make model with K components OK.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we so far before 2K.",
                    "label": 0
                },
                {
                    "sent": "Then we need another party.",
                    "label": 0
                },
                {
                    "sent": "So it's a prior over the parameters.",
                    "label": 0
                },
                {
                    "sent": "So we need a prior overlay.",
                    "label": 0
                },
                {
                    "sent": "The mixture mixing proportion pie.",
                    "label": 0
                },
                {
                    "sent": "So for example we use the one common choices.",
                    "label": 0
                },
                {
                    "sent": "Use the contract duration prior, like this one.",
                    "label": 0
                },
                {
                    "sent": "We have a parameter 5 divided by the number of components and before the crossing components the parameter can follow some.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a complicated pricing for them.",
                    "label": 0
                },
                {
                    "sent": "More invoice we showed OK.",
                    "label": 0
                },
                {
                    "sent": "So like always the same, it's a this is causing mental model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "We apply there until definition.",
                    "label": 0
                },
                {
                    "sent": "Alternative representation of the Gaussian mixture model and we can write to the building.",
                    "label": 1
                },
                {
                    "sent": "Gotta make model in this way an so for this atoms.",
                    "label": 0
                },
                {
                    "sent": "This job from some best estimator based distribution and these are mixing proportion is sample from some jewelry prior.",
                    "label": 0
                },
                {
                    "sent": "And this is our because this is random variables.",
                    "label": 0
                },
                {
                    "sent": "So this measures random layer.",
                    "label": 0
                },
                {
                    "sent": "It's a discrete over the 10 atoms and for sharing things with your particular sample from this discrete distribution, and we generate the data so that everything is very standard from the reputation of the Gaussian mixture model OK.",
                    "label": 0
                },
                {
                    "sent": "So perfectly with visualize this process in this way, this is the best measure.",
                    "label": 0
                },
                {
                    "sent": "It could be continuous and delete.",
                    "label": 0
                },
                {
                    "sent": "This is a discrete random energy and we for each data point withdraw sample from GK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for bathing methods we do the posterior inference over the order, letting rebels.",
                    "label": 0
                },
                {
                    "sent": "And of course, for as I said before, for this Bayesian inference, it's generally hard, so we need some more approximation like we can do version approximation.",
                    "label": 0
                },
                {
                    "sent": "We can do Markov chain color, you can, you can choose anyone you prefer, OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then the question is, for this kind of question probably you may ask how many casts is good to discover public data for them.",
                    "label": 1
                },
                {
                    "sent": "For this data point for this data set, is it better to use four or two classes to describe the data?",
                    "label": 0
                },
                {
                    "sent": "So how do we solve this problem?",
                    "label": 0
                },
                {
                    "sent": "Maybe to automatically identifies a number of components?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is another magical solution, not embedding solution.",
                    "label": 0
                },
                {
                    "sent": "So again, we can we define like older.",
                    "label": 0
                },
                {
                    "sent": "We can use a prior.",
                    "label": 0
                },
                {
                    "sent": "So here we choose unformatted process.",
                    "label": 0
                },
                {
                    "sent": "Its usual process to discover the if possible the even number of possible components.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just give you some quick ideas about the judicial process.",
                    "label": 0
                },
                {
                    "sent": "It's a flexible, not permitted prior defined over even number of components.",
                    "label": 1
                },
                {
                    "sent": "And we basically still.",
                    "label": 0
                },
                {
                    "sent": "Here is the notation we use.",
                    "label": 0
                },
                {
                    "sent": "So if she follows a future prior process we write in this form and here we have two parameters, Alpha is the concentration parameter.",
                    "label": 1
                },
                {
                    "sent": "Let's get.",
                    "label": 1
                },
                {
                    "sent": "It controls the number of components you will have in in error, and they zero is the best measure is the best distribution OK?",
                    "label": 0
                },
                {
                    "sent": "It's it's basically the pricing, but just think about it as a prior over the cluster specific parameters.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a mathematical rigorous way to define the judicial process.",
                    "label": 0
                },
                {
                    "sent": "So we say so G is a random measure.",
                    "label": 0
                },
                {
                    "sent": "It's a defined.",
                    "label": 0
                },
                {
                    "sent": "Also may have space exists, ample space and algebra, and have a parameter Alpha and the judicial process is discussion.",
                    "label": 0
                },
                {
                    "sent": "Mayor says that for any final position over the summer space and we this kind of variables.",
                    "label": 1
                },
                {
                    "sent": "So because this is random partition for each part this will be a random variable induced from the partition, right?",
                    "label": 0
                },
                {
                    "sent": "So we've put all these variable together.",
                    "label": 0
                },
                {
                    "sent": "Note that we just care about the finance, the final position, so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a much more random variable and justice of multiple random variable follows that usually distribution is motivated for discussion and the parameters defined by from the best measure right?",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "The basic definition.",
                    "label": 0
                },
                {
                    "sent": "So notice that I just define.",
                    "label": 0
                },
                {
                    "sent": "Are there any final position so you actually tend to infinitely need some theorem congruent consistent theorem to attend to make sure that you have a consistence to corpus data discovery within normal components.",
                    "label": 0
                },
                {
                    "sent": "So I just skip the details about the theorem, just give you some basic ideas about this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the for the judicial process, there's where nice representing.",
                    "label": 0
                },
                {
                    "sent": "Met with property one this particular sample from the judicial process that can be written in this way.",
                    "label": 0
                },
                {
                    "sent": "It's a countable infinite number of atoms and use some mixture weights.",
                    "label": 0
                },
                {
                    "sent": "Pie right?",
                    "label": 0
                },
                {
                    "sent": "So note that this is a.",
                    "label": 1
                },
                {
                    "sent": "This is very similar to the.",
                    "label": 0
                },
                {
                    "sent": "Mixture mixing representation office.",
                    "label": 0
                },
                {
                    "sent": "They they got a mixture with cake with finance K components right but here we have in the number of components.",
                    "label": 0
                },
                {
                    "sent": "So this is a discrete right.",
                    "label": 1
                },
                {
                    "sent": "We say that if we draw a sample from this mayor, we have another probability too.",
                    "label": 0
                },
                {
                    "sent": "So sorry if we have multiple, maybe we have two samples from this mayor.",
                    "label": 0
                },
                {
                    "sent": "We get to have another probability to hit.",
                    "label": 0
                },
                {
                    "sent": "Do you have the identical samples?",
                    "label": 0
                },
                {
                    "sent": "So this is a basic idea why this process can be used as clustering.",
                    "label": 0
                },
                {
                    "sent": "'cause suppose we have a data point side job.",
                    "label": 0
                },
                {
                    "sent": "Multiple samples from this, this mayor this Gmail is this one.",
                    "label": 0
                },
                {
                    "sent": "I have a probability, not their probability to just sign the data point to the same, the same components.",
                    "label": 0
                },
                {
                    "sent": "If we have the same parameter right?",
                    "label": 0
                },
                {
                    "sent": "So this is a way we can part in the the.",
                    "label": 0
                },
                {
                    "sent": "The density into different components.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There are some static guarantees at C4.",
                    "label": 0
                },
                {
                    "sent": "You have adjustable finite number of data points.",
                    "label": 0
                },
                {
                    "sent": "The number of components in it is also finance to discover data.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question is, are hard to set this up mixing weights?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is 1 processor called Stick Breaking process to define this this rates.",
                    "label": 0
                },
                {
                    "sent": "So basically I have another variable better, better wearable follow from a better discussion with parameter 1A and then I can construct the mixing weights pie for the first one just as the first sample from a better description for the next one it's constructed in this way so I can visualize this process.",
                    "label": 0
                },
                {
                    "sent": "In the following way, so suppose this is the stick with the Union lines.",
                    "label": 0
                },
                {
                    "sent": "I first joined a sample from a better discussion, like a better one.",
                    "label": 0
                },
                {
                    "sent": "I do a cutting of this Dick and put the this part as a pipeline.",
                    "label": 0
                },
                {
                    "sent": "And then I I do another cutting for the rest.",
                    "label": 0
                },
                {
                    "sent": "OK, so I joined sample.",
                    "label": 0
                },
                {
                    "sent": "The cut is from the sample, another sample from that distribution, like a bad two and I put it this part, that's Pi 2 and continue this process and I can define the all these these weights you can see from this visualization that.",
                    "label": 0
                },
                {
                    "sent": "There are some destination.",
                    "label": 0
                },
                {
                    "sent": "Overall these pipe parameters will be some 21 right if you continue, continue the partition to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Some of the length will be.",
                    "label": 0
                },
                {
                    "sent": "You could choose a unit length of the steep.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another useful reporting update direction process is called Chinese restaurant process.",
                    "label": 1
                },
                {
                    "sent": "It's uh, it's local process on the parting of data, so it's your describers the metaphor to metaphor of.",
                    "label": 0
                },
                {
                    "sent": "Maybe in customers sit down in your Chinese restaurant with even number of tables.",
                    "label": 1
                },
                {
                    "sent": "OK, so the metaphor is described as follows.",
                    "label": 1
                },
                {
                    "sent": "The first customer sees as the first table with probability one and for the next thing coming.",
                    "label": 0
                },
                {
                    "sent": "For example, the end customer.",
                    "label": 0
                },
                {
                    "sent": "They choose a table with a probability according to in two scenarios.",
                    "label": 0
                },
                {
                    "sent": "If a table has some previous customers have sit down, it will be choosing according to the popularity of the table.",
                    "label": 0
                },
                {
                    "sent": "Proportion to the popularity and case number of customers that have seen her sit down in the in particular table K. And also you have another probability to sit in on you table.",
                    "label": 0
                },
                {
                    "sent": "A new table means there are no customer have sit on the table.",
                    "label": 0
                },
                {
                    "sent": "So again, it can be resized in this way.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have even number of tables for the first category has probably went to sit down in the first table and for the for the next second customer that has a probability 1 / 4 + 1.",
                    "label": 0
                },
                {
                    "sent": "So according to the definition right to sit in the first table and also another probability of our press 1 to sit down and sit on another table.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the subculture has this probability?",
                    "label": 0
                },
                {
                    "sent": "Here and probably sit here and also we have another, probably due to a new table, right?",
                    "label": 0
                },
                {
                    "sent": "So you can continue on this process to assign.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a.",
                    "label": 0
                },
                {
                    "sent": "Each customer represented at point.",
                    "label": 0
                },
                {
                    "sent": "You can in this way you can assign the dead pointing into different tables and different table can be think about these different components and so is this where you put in the data points into different components.",
                    "label": 0
                },
                {
                    "sent": "You do the Castle right so?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you may want to the relationship between the generation process and and you have process in fact that the process is definitely mixture mixing distribution for the channel process.",
                    "label": 1
                },
                {
                    "sent": "So in other words, in other ways you can.",
                    "label": 0
                },
                {
                    "sent": "So it means we integral over the random mayor T and we get to the general election process over the all these samples.",
                    "label": 0
                },
                {
                    "sent": "So it's a quarter here.",
                    "label": 0
                },
                {
                    "sent": "If you integrate out.",
                    "label": 0
                },
                {
                    "sent": "If traps out, this random random layer, you will die rectifier channel process over over the parameters Theta and you can still combine with likely model discover data right?",
                    "label": 1
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So why do we care about the general process?",
                    "label": 0
                },
                {
                    "sent": "Because you give you a natural way to gel samples so we can do.",
                    "label": 0
                },
                {
                    "sent": "We can develop a sampling algorithm to do the inference for the usual process mixture models.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the best girl procedure of the Gibbs sampler for which we can do give sample for for the usual mixture model using the channel process repetition so.",
                    "label": 1
                },
                {
                    "sent": "For the components with a with a non zero points have been assigned to that component withdraw or is this equation so you can see that in nature divided into 2 parts this part is the prior, the prior it's defined from the IT can be derived from the general process recognition and the 2nd is the likelihood the lack of a particular data points and four new component.",
                    "label": 0
                },
                {
                    "sent": "So I represent a new component in this way, basically different from all the other components will have seen.",
                    "label": 1
                },
                {
                    "sent": "OK, so for the new component I need to do an integral because I don't know the the parameter of the of this component is totally new to me.",
                    "label": 0
                },
                {
                    "sent": "OK, so I do the integral over all possible or possible parameters Theta for this component and I can compute it this way again is proposing to.",
                    "label": 0
                },
                {
                    "sent": "I2 parts the first part is the prior derived from the Chinese restaurant process, the sacrifice, the margin, like hold of assigning this step point to this component to this new company.",
                    "label": 0
                },
                {
                    "sent": "So very intuitive from these equations.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the process we K time in multiple layers.",
                    "label": 0
                },
                {
                    "sent": "For example questionnaire is that if we want to model for example M. That sets from maybe from different sources about the same event.",
                    "label": 1
                },
                {
                    "sent": "We can build multiple miniature models over the different data sets, and in some in some applications maybe.",
                    "label": 0
                },
                {
                    "sent": "This test sets are correlated with.",
                    "label": 0
                },
                {
                    "sent": "We want to share some components here.",
                    "label": 0
                },
                {
                    "sent": "Some component between this multiple mission model.",
                    "label": 0
                },
                {
                    "sent": "So what we have to do that you build another day of judicial process?",
                    "label": 0
                },
                {
                    "sent": "So it's basically can see that this is at the top layer.",
                    "label": 0
                },
                {
                    "sent": "This is our best player, so we're under jaw sample from judicial process and this is a discrete right.",
                    "label": 0
                },
                {
                    "sent": "So there are four different datasets.",
                    "label": 0
                },
                {
                    "sent": "I draw a particular instance from this this discriminator, so because there is discord is guaranteed that the multiple jobs will share some atoms.",
                    "label": 0
                },
                {
                    "sent": "They basically have the same support in the probability term, so so they.",
                    "label": 0
                },
                {
                    "sent": "Right, so they share the same set of atoms but with different mixing proportions.",
                    "label": 0
                },
                {
                    "sent": "So in this sense the the multiple datasets we share the same mixture components OK, so one particular application for this so called hierarchical judicial process is to build a Top Model.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I give you are basically over over the top models.",
                    "label": 0
                },
                {
                    "sent": "So in this afternoon I will show you some some work we did in this topic.",
                    "label": 0
                },
                {
                    "sent": "So the topic model is it's very popular basic mixture model.",
                    "label": 1
                },
                {
                    "sent": "Fix it, it can be visualized in this way.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have two topics.",
                    "label": 0
                },
                {
                    "sent": "Each topic is a mathematically discussion over the all the terms in Additionally.",
                    "label": 0
                },
                {
                    "sent": "So for example, for this topic will have a probability point .0105.",
                    "label": 0
                },
                {
                    "sent": "Of all the Tom images and .05 for the JPG.",
                    "label": 0
                },
                {
                    "sent": "You can roughly get the idea of this topic right.",
                    "label": 0
                },
                {
                    "sent": "It's about the image.",
                    "label": 0
                },
                {
                    "sent": "So for this same topic, it's maybe about some power of or current or circuit.",
                    "label": 1
                },
                {
                    "sent": "OK, so then for particular documents it's a mixture of over this multiple topics, like for them.",
                    "label": 0
                },
                {
                    "sent": "For this document it's a point edge.",
                    "label": 0
                },
                {
                    "sent": "From this from Capital One and .2 from the public two.",
                    "label": 0
                },
                {
                    "sent": "So for each particular word it's assumed to be.",
                    "label": 0
                },
                {
                    "sent": "From particular that particular topic, as I indicated by the colors for the query, green color from topic one and blue on blue awards general from topic to write, so you can get a better idea for not documenting will have a different mixing proportion and different kind of popular elements for dinner words.",
                    "label": 0
                },
                {
                    "sent": "And if we put all these things in the best in term, for example, we put a prior over the mixing component over the topics and over the mixed proportions we get.",
                    "label": 0
                },
                {
                    "sent": "Of also reduce the contractor prior will get the very popular model collecting social location or LDA in short.",
                    "label": 0
                },
                {
                    "sent": "So this is a very basic idea about the topic models.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are graphically we can visualize in this way, so we have all the data.",
                    "label": 0
                },
                {
                    "sent": "Is the mixing proposal for particular particular document and this is the top examiner political world and this is observed world.",
                    "label": 0
                },
                {
                    "sent": "OK so this file our topics there there discussion over the words in a dictionary and can the joint Distribution Committee this way, according to the semantics of the graph model, everything is very standard so.",
                    "label": 0
                },
                {
                    "sent": "So here I put to emphasize that for Standard Model maybe we reduce sweeper, specified number of topics and we can do the mixture model.",
                    "label": 0
                },
                {
                    "sent": "Always rigorously, this is Adam mixed model.",
                    "label": 0
                },
                {
                    "sent": "It's a mixture of mixture because two layer GT Cabrio documents and awards right.",
                    "label": 0
                },
                {
                    "sent": "Sold.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for the regulatory process, we can apply to fly here to develop a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "Schedule set up for the best measure we can rectify as I imagine marginal distribution for example, of dimensional ducher dispute with the number of words in a dictionary and.",
                    "label": 0
                },
                {
                    "sent": "So here's the Clock model.",
                    "label": 0
                },
                {
                    "sent": "OK, so they she O it's.",
                    "label": 0
                },
                {
                    "sent": "It's accountable, you can measure just collection of of this.",
                    "label": 1
                },
                {
                    "sent": "Because our best measure is the discussion over the marginal probability vectors.",
                    "label": 0
                },
                {
                    "sent": "So G is even a collection of this of this workers right?",
                    "label": 0
                },
                {
                    "sent": "So each Atom is a McNamee distribution overview words.",
                    "label": 0
                },
                {
                    "sent": "OK so so basically chat Tommy is a topic, so then for each document I draw a sample from this discrete TTL mayor.",
                    "label": 0
                },
                {
                    "sent": "So it's also also a mixture over the topics.",
                    "label": 0
                },
                {
                    "sent": "So it's just selected for a particular document.",
                    "label": 0
                },
                {
                    "sent": "Then for award I just sample from this mayor again.",
                    "label": 0
                },
                {
                    "sent": "This is discrete, discreet are coming to the theory of judicial process.",
                    "label": 0
                },
                {
                    "sent": "So CIJ is a particular topic particular topic to discover the the observed world.",
                    "label": 1
                },
                {
                    "sent": "So in this sense we have a nonparametric process to discover the Top Model.",
                    "label": 0
                },
                {
                    "sent": "So we can infer the number of topics from the data.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess sure some examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the results from the paper.",
                    "label": 0
                },
                {
                    "sent": "8 feet to the GP topic.",
                    "label": 0
                },
                {
                    "sent": "Model to biological abstract data set and you can see that this publicity is a common measure to measure how well your model fit the data.",
                    "label": 0
                },
                {
                    "sent": "That's going to test on its.",
                    "label": 0
                },
                {
                    "sent": "Calculated on testing data set OK so you can see that for the standard Top Model LDA, you have the parameter K to tune.",
                    "label": 0
                },
                {
                    "sent": "So if we change the number of topics you get different performance.",
                    "label": 0
                },
                {
                    "sent": "So you can see that roughly in this area that get the best performance.",
                    "label": 0
                },
                {
                    "sent": "But if you use the two large billets overfitting for some some.",
                    "label": 0
                },
                {
                    "sent": "Some risk it over or video data, but if you run the nonparametric ATP make model you can just have.",
                    "label": 0
                },
                {
                    "sent": "One point, because the number of topic is is different from your data, right?",
                    "label": 0
                },
                {
                    "sent": "So also you can see the problem is making the best points by tuning the key parameter.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So also this finger show you if you draw samples from the posterior, you roughly you can get.",
                    "label": 0
                },
                {
                    "sent": "You can see that most of the most of the documents are has about 65 number of topics.",
                    "label": 0
                },
                {
                    "sent": "So because we have a material over all possible number of K so this is.",
                    "label": 0
                },
                {
                    "sent": "For each sample, you may have different values, so here is hugely histogram over multiple samples.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so here is the same for the clustering, so next I will move to the another processor to describe the feature learning.",
                    "label": 0
                },
                {
                    "sent": "So any questions so far?",
                    "label": 0
                },
                {
                    "sent": "I just finished the first part.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for future learning, because the basic celebrate this way.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have objects and at points we want to learn a little feature feature vector for each data points.",
                    "label": 0
                },
                {
                    "sent": "So these are are we can formulate this problem as a metric learning problem.",
                    "label": 0
                },
                {
                    "sent": "So I put the feature vector of each each object in true and connecting to a metric letting feature metric and for example for for the object line.",
                    "label": 1
                },
                {
                    "sent": "At this kind of features for black black entries there, that means that.",
                    "label": 0
                },
                {
                    "sent": "At that point, has not zero value for this feature.",
                    "label": 0
                },
                {
                    "sent": "For the wax interest, they better means the object doesn't have this feature.",
                    "label": 1
                },
                {
                    "sent": "So suppose if we want to learn real value official metric so it can be decomposed as a elementwise product of the binary metric and the real value value.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think this is an integer, but it just simplifies as a binary, so it's just the indicator whether object has a particular feature.",
                    "label": 0
                },
                {
                    "sent": "The feature value is determined by another real one real value metric, OK.",
                    "label": 1
                },
                {
                    "sent": "So for the building setting, we can put a prior over the over the future metrics using a productive.",
                    "label": 0
                },
                {
                    "sent": "Over the prior biometric and the real value metric.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We just consider the binary binary parts because it basically gives you an idea of how how sparse your feature will be, how whether you did also determine whether a particular data point as political feature, right?",
                    "label": 0
                },
                {
                    "sent": "So here's a let's start from the word simple setting.",
                    "label": 0
                },
                {
                    "sent": "If we know the number of features like a, K, the K dimensional features, we can use a very simple bed Bernoulli process to discover the latter figure magic.",
                    "label": 0
                },
                {
                    "sent": "So here's the idea.",
                    "label": 0
                },
                {
                    "sent": "So each column carbon 2 particular feature I. I assume that the feature of the features of multiple objects are drawn from the same Bernoulli distribution with parameter Pi KIK associated with feature K. So I say indicated here and in the Bayesian term.",
                    "label": 0
                },
                {
                    "sent": "I also assume that the all the Pi K following the common prior.",
                    "label": 0
                },
                {
                    "sent": "So the natural choice is a better is better prior.",
                    "label": 0
                },
                {
                    "sent": "That is correct.",
                    "label": 0
                },
                {
                    "sent": "It's a conjugate to the point you can think about this as like all of the features and the sale price, so they are conjugate.",
                    "label": 0
                },
                {
                    "sent": "So if we know the features, if we draw a sample from the this process, we have the basically cover the feature of each death points.",
                    "label": 0
                },
                {
                    "sent": "So we can connect to the observation for them we use a crossing like model discover data, so this is worse than.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some analysis of these are finding the Latin feature model, so you can compute the marginal probability of a particular feature metrics.",
                    "label": 1
                },
                {
                    "sent": "You just do some regular.",
                    "label": 0
                },
                {
                    "sent": "It's very simple to do this are using this.",
                    "label": 0
                },
                {
                    "sent": "This is equation, so basically it's normalization term of the better, better distribution.",
                    "label": 0
                },
                {
                    "sent": "So by the point here on the inside, their features can see that the marginal.",
                    "label": 0
                },
                {
                    "sent": "So the probability joint probability of the metric is composed in 2K terms.",
                    "label": 1
                },
                {
                    "sent": "So that basically means that the features are independent.",
                    "label": 0
                },
                {
                    "sent": "So this is a obvious formal definition, so we assume the different features followed a different varieties distribution OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can also compute if we would develop a sampler, you need to.",
                    "label": 0
                },
                {
                    "sent": "Maybe you need this kind of conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "You can compute this against using the marginal distribution within the previous slide and then finally we get the conditional this term and you have the M is the number of number of data points that has has chosen this feature K except the data point.",
                    "label": 0
                },
                {
                    "sent": "Hi, because Ice is a current of their point that we focus on.",
                    "label": 0
                },
                {
                    "sent": "So this is the formula prior so everything is worse than that.",
                    "label": 0
                },
                {
                    "sent": "They just apply the qualities.",
                    "label": 0
                },
                {
                    "sent": "And Rob this one OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we can generalize so we can some.",
                    "label": 0
                },
                {
                    "sent": "You may want how to generalize this kind of finding letter feature model to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if we just put care to Infinity, there will be some trouble.",
                    "label": 1
                },
                {
                    "sent": "So it basically means if we just simply Simply put Keita invented, each particular metric letting figure metric, we have a zero probability from the previous definition, so we need some some technique or some tickets.",
                    "label": 0
                },
                {
                    "sent": "To derive a non zero probability of limit.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Big achieved by defining a current class using letter order form of these metrics.",
                    "label": 0
                },
                {
                    "sent": "So basically you order the columns the features according to the number of appearance of in the points in the left order sense you will get your developer equivalent class of multiple metrics in the in the same class and in that way we can define probability over the current cost and this is not their probability.",
                    "label": 1
                },
                {
                    "sent": "You can keep pressing this form.",
                    "label": 0
                },
                {
                    "sent": "So KK passes the number of active active features, not 0 features, OK?",
                    "label": 1
                },
                {
                    "sent": "So basically, not do columns in the latency in magic, there's some other.",
                    "label": 0
                },
                {
                    "sent": "Numbers here by.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so the this is called the Indian buffet process.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are also a metaphor to describe this up.",
                    "label": 0
                },
                {
                    "sent": "This process of generating a biometric.",
                    "label": 0
                },
                {
                    "sent": "So think about the number of customers coming into a Indian Indian restaurant they have can have even number of pitches so this process.",
                    "label": 0
                },
                {
                    "sent": "Discover how the customer choose the dishes, so therefore the first customer it's choose Karen dishes from a person distribution with parameter Alpha.",
                    "label": 1
                },
                {
                    "sent": "So if you remember, this is the finding the backbone process archive the perimeter of the bed prior.",
                    "label": 0
                },
                {
                    "sent": "So here for the interview process.",
                    "label": 0
                },
                {
                    "sent": "We still have this parameter.",
                    "label": 0
                },
                {
                    "sent": "I'm there for the next customer.",
                    "label": 0
                },
                {
                    "sent": "For that I would choose it.",
                    "label": 0
                },
                {
                    "sent": "Also consider two scenarios to choose dishes so it choose the older dishes for the existing digits.",
                    "label": 1
                },
                {
                    "sent": "That means it has been chosen by previous customer according to the.",
                    "label": 0
                },
                {
                    "sent": "Again, according to the popularity of the dishes.",
                    "label": 1
                },
                {
                    "sent": "And I MK is the number of customers that have chosen dishes K also you have an audio or distribute distribution to choose some additional dishes will have haven't chosen by previous customer.",
                    "label": 0
                },
                {
                    "sent": "Again following a person distribution.",
                    "label": 1
                },
                {
                    "sent": "So far, for this particular instance, for example, the customer one, choose the dishes 123 and customer two choose the oldest 1 three and also we choose our new dishes four to five other games for the customer.",
                    "label": 0
                },
                {
                    "sent": "Three, it has ordered dishes here, and also new dishes, so you can continue this process to generate a binary metric.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So against therefore, the II preferred process, there is a sticker bragging construction, so these are the these as a particular entry of the binary market will follow up on redistricting this very intuitive from our our duration of the process, right?",
                    "label": 0
                },
                {
                    "sent": "So for this parameter pie it's a driver again, definer users debugging process.",
                    "label": 0
                },
                {
                    "sent": "It can be visualizing this way.",
                    "label": 0
                },
                {
                    "sent": "So again, we have for a unit length disk and can we do the do the break OK?",
                    "label": 0
                },
                {
                    "sent": "So the first sample, maybe I draw from better disappear and I get the first sample like the point edge and.",
                    "label": 0
                },
                {
                    "sent": "Then I I definition in the pipeline you could to the better variable.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then I join another sample.",
                    "label": 0
                },
                {
                    "sent": "From a better district, maybe .25, and then multiply this number to the to the previous length .8 I get to the number of .4, right?",
                    "label": 0
                },
                {
                    "sent": "So for the eyes this number is the 2nd second dimension.",
                    "label": 0
                },
                {
                    "sent": "So I join another sample from better prior for them.",
                    "label": 0
                },
                {
                    "sent": ".4 multiplied from the previous length .4 from this one right so I get .16 and resigned to a submission.",
                    "label": 0
                },
                {
                    "sent": "So I continued process and can generate the probability of.",
                    "label": 0
                },
                {
                    "sent": "Over the particular feature so you can see that if you continue on this breaking process, the probability of.",
                    "label": 0
                },
                {
                    "sent": "Appear feature to be active get decreased, right?",
                    "label": 0
                },
                {
                    "sent": "It's kind of a potential decrease, so this is this sparsity property is very important to guarantee that you can have fun #4 features for particular data set, so this is very important property of this independent process.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, I can do sampling.",
                    "label": 0
                },
                {
                    "sent": "So according to that definition.",
                    "label": 0
                },
                {
                    "sent": "So this Remember Remember you for the final, better quality electric model?",
                    "label": 0
                },
                {
                    "sent": "I have this condition alright.",
                    "label": 0
                },
                {
                    "sent": "I drive this one for the if we set K to Infinity we need to consider 2 cases.",
                    "label": 0
                },
                {
                    "sent": "So for the features that have been taken by by some data points we the conditional probability is proportional to the number of data points that have taken this feature.",
                    "label": 1
                },
                {
                    "sent": "I'm just following new features withdraw from some portion distribution we have.",
                    "label": 1
                },
                {
                    "sent": "We have an audio probability to sample some new features.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can combine this this process with likely model.",
                    "label": 0
                },
                {
                    "sent": "For example using Gaussian likelihood discover the real weather data.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can still do the do the sampling like combine by considering the data.",
                    "label": 0
                },
                {
                    "sent": "So, so we have this posterior so we can get this condition also.",
                    "label": 0
                },
                {
                    "sent": "It's very intuitively one party formed the process at the prior and otherwise get like hood under.",
                    "label": 0
                },
                {
                    "sent": "Right, so for the if we are like a model is the simple.",
                    "label": 1
                },
                {
                    "sent": "For example for linear causing the deadlock that can be computed in analytical form maybe, but for.",
                    "label": 1
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general we have maybe have some harder inference problem.",
                    "label": 0
                },
                {
                    "sent": "For example, our if we likely complicated, we need to deal with some non conjugated models.",
                    "label": 0
                },
                {
                    "sent": "So we do particle sample in or other sampling methods.",
                    "label": 0
                },
                {
                    "sent": "Also we can give up variational inference.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use a stick breaking construction and to have some variation approximation methods and also there are some research about.",
                    "label": 0
                },
                {
                    "sent": "To apply.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the last part I think I have maybe 7 minutes, but I quickly go over another process is.",
                    "label": 0
                },
                {
                    "sent": "It's a Gaussian process for regression.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, we start from simple.",
                    "label": 0
                },
                {
                    "sent": "OK, so we consider simple noisy observation model.",
                    "label": 1
                },
                {
                    "sent": "So this is our response and we assume that it's generated from some function plus some noise like we consider Gaussian noise.",
                    "label": 1
                },
                {
                    "sent": "Very simple case if we if the for linear regression we consider this linear function.",
                    "label": 0
                },
                {
                    "sent": "For example every is.",
                    "label": 0
                },
                {
                    "sent": "Dot product of weights and our feature vector is very simple.",
                    "label": 0
                },
                {
                    "sent": "And in this way from this observation model we can we have this crossing like code.",
                    "label": 1
                },
                {
                    "sent": "The forbidden we put it for them for the conjugated prior accounting prior here for the.",
                    "label": 0
                },
                {
                    "sent": "For the model parameter and we can do the inference using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Everything 1st and so because the crossings.",
                    "label": 0
                },
                {
                    "sent": "Is a nice it's conjugate, so we can write down the posterior Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "So we just updated the parameters.",
                    "label": 0
                },
                {
                    "sent": "And for prediction for four new Fairpoint, we predicted the function.",
                    "label": 0
                },
                {
                    "sent": "We can do this integral gains analytically computed.",
                    "label": 0
                },
                {
                    "sent": "Little checkable, so it's a game of dosing distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything is worse than that.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but you may think about the linear regression model can be too restrictive, so we may simply want to do nonlinear.",
                    "label": 1
                },
                {
                    "sent": "OK, so so one trick is I think everybody knows this device just called kernel trick.",
                    "label": 0
                },
                {
                    "sent": "We map the data points into another space using a feature mapping function.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just give you some quick samples.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to find a mapping function and then we can still if we have this mapping function, we can still do the linear regression using the product in the in the map fish found feature space and if you put Gaussian prior over here over the parameter, we can still do the do the dosing inference.",
                    "label": 1
                },
                {
                    "sent": "Everything, store standards just likely live very simple linear case.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are some examples you can choose to for the for the mapping function like, you can use this.",
                    "label": 0
                },
                {
                    "sent": "It's I think a Colonel in the kernel field we call it causing it's called RBF kernel, or we can use the polynomial term to define this metric function.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also we can parameterise using for someone using a neural network to paralyze these making function 5.",
                    "label": 0
                },
                {
                    "sent": "Using some another parameters to to discover this this mapping function.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so we may want to have a complete, completely nonparametric, a way to do this.",
                    "label": 0
                },
                {
                    "sent": "And suppose we don't have.",
                    "label": 0
                },
                {
                    "sent": "We had to remove the implicit parameterization of the function.",
                    "label": 1
                },
                {
                    "sent": "Can we do this OK?",
                    "label": 0
                },
                {
                    "sent": "So in fact we can do.",
                    "label": 1
                },
                {
                    "sent": "We can use in causing process to manage a uncountable even number of functions.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so before I just can't describe the process, just to remind you of the basic concept of the random function, random random variable, you may want hard to represent a function random function.",
                    "label": 1
                },
                {
                    "sent": "So here is one way and function basically can be represented by even even a vector with the index set.",
                    "label": 0
                },
                {
                    "sent": "For example for this.",
                    "label": 0
                },
                {
                    "sent": "For this function we may if suppose we have an even number of index.",
                    "label": 0
                },
                {
                    "sent": "In this axis we can, using the function values to represent the function right and for particular data points, and so if I fix the deadline here, I draw a vertical line and because the function is random and the function value at this point will be a random variable, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the way we describe.",
                    "label": 1
                },
                {
                    "sent": "We discovered the random function using.",
                    "label": 0
                },
                {
                    "sent": "If you give me a set of index index is I use the function values as a random variable to discover the function.",
                    "label": 0
                },
                {
                    "sent": "So for example, maybe forgotten process.",
                    "label": 0
                },
                {
                    "sent": "I use it maybe across industry to describe this particular variable.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here it says the definition of Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "OK, so you still process or if I every finite set of variables.",
                    "label": 1
                },
                {
                    "sent": "For example, if the deck set is financially can discover the joint stream over the function values as it's multivariate Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So remember that concentration is parameterized sesame and current functioning.",
                    "label": 1
                },
                {
                    "sent": "So awesome process is specified by the main function of the current function.",
                    "label": 0
                },
                {
                    "sent": "Basically, taking over all your random function.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm just so, for example, for the simple Bayesian linear regression is it's a Gaussian process in this way.",
                    "label": 1
                },
                {
                    "sent": "For example, we can derive that the main function is zero and query functions is the fine.",
                    "label": 0
                },
                {
                    "sent": "This is occurrence of noise of sorry, the prior.",
                    "label": 0
                },
                {
                    "sent": "This should be transposed.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the code using called process it to do to do prediction for the noisy free setting.",
                    "label": 0
                },
                {
                    "sent": "We know the true function value according to the definition for the new testing points and Windows from the definition that for this finite vector it follow across industry right from the definition process.",
                    "label": 1
                },
                {
                    "sent": "So we can divide.",
                    "label": 0
                },
                {
                    "sent": "We know that from the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "The condition or so for this condition is the prediction predicted this week about so it's still got it from the property of Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So this reversed and.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the, for the noisy observations which we observe the response and from some noisy model, and in this case of the queries will be changed, the plus some noise variance.",
                    "label": 0
                },
                {
                    "sent": "And in this case we can still according to the definition this is joint multivariate browsing and this conditional is also causing right?",
                    "label": 0
                },
                {
                    "sent": "But this has been shifted by the by the noise words, so this reverse standard.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But other issues for Gaussian process, for example for clarification or some other scenarios, so we may have remember if you remember for the for the recurring cast they response model is causing is contributed from the.",
                    "label": 0
                },
                {
                    "sent": "Opposing process, but for the clarification or other scenarios you may have an unconjugated models.",
                    "label": 0
                },
                {
                    "sent": "So they alot of research has been done on deriving some approximate approximate algorithm to deal with the.",
                    "label": 0
                },
                {
                    "sent": "This kind of a scenario for them apply the closing process for cascading predicted control or relational learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is so very nice website to close all this material about Gaussian process, including theory applications.",
                    "label": 0
                },
                {
                    "sent": "You can check out this website is if you're interested in this top.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm just alone times OK so just the RO quick summary.",
                    "label": 0
                },
                {
                    "sent": "So for the first part I give you were basically solve the basic methods and introduced the three popular non permit databases.",
                    "label": 0
                },
                {
                    "sent": "So then we have a launch correct?",
                    "label": 0
                },
                {
                    "sent": "And for the afternoon session of issue, some very recent work may be conducted in microphone, but how to do the constraint based inference with in particular with mixed market constraints to improve the discriminative ability of business model and also I'll show you some interesting applications to clarification much learning link prediction and also recommendation.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think it's time to break.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}