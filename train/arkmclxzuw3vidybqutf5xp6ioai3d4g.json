{
    "id": "arkmclxzuw3vidybqutf5xp6ioai3d4g",
    "title": "Exponential Families",
    "info": {
        "author": [
            "Alex Smola, Amazon"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/mlss06au_smola_ef/",
    "segmentation": [
        [
            "Alex has been at the end you on and off now for about five years and before then works extensively with people like Klaus, Alex.",
            "Alex is a kind of person is on the cutting edge of machine learning and have been successively proving that there are more and more general machine learning projects out there that we can utilize for for our benefit.",
            "Thank you, Alex.",
            "So ask me to play little bit more.",
            "Wanna talk about this?",
            "And.",
            "Because.",
            "Both ways stairway to dim down the front left front row a little bit.",
            "'cause otherwise I guess it's a little bit hard.",
            "Yeah.",
            "It takes awhile.",
            "K. Maybe it's improving here.",
            "Yeah OK alright.",
            "Up maybe that was a bit too much.",
            "Yeah.",
            "OK, I hope you won't be falling asleep.",
            "So.",
            "Who did that now?",
            "Thanks so one of the reasons why I think exponential families are really useful and powerful.",
            "Is that well?",
            "They actually allow us to unify a couple of existing tools into one common framework.",
            "So that actually, I guess, is probably one of the main attractions.",
            "So it turns out there slightly more general objects out there that you could use.",
            "And yes, you can prove wonderful theorems about them, but in that case the math gets a little bit ugly when you have to talk about bank space rather than Hilbert spaces.",
            "But I mean basically I'll be trying to keep things reasonably simple and start with a couple of things that.",
            "Well, if you were assisted, you will probably know forwards and backwards, but quite often in machine learning people haven't really seen that much before.",
            "OK, let's just get started.",
            "So."
        ],
        [
            "What I'll be talking about is exponential families.",
            "First of all, just to define what they look like.",
            "Well, we look at maximum likelihood estimation Fisher information.",
            "Also, because I mean this is the Fisher information, at least for me.",
            "It was always a little bit shrouded in some mystery as to why this is this weird beast from statistics that people haven't really.",
            "Well, they do something with it.",
            "They talk about information geometry and nobody really understands.",
            "It actually turns out to be something rather simple.",
            "And we'll be talking about price.",
            "And I guess this is probably the main thing that you should be remembering from this course.",
            "Then the next thing is will be looking into conditioning and feature spaces.",
            "And so we talk about conditional distributions and inner product.",
            "This is basically making the switch from.",
            "Exponential families seem to kernels, and this is, I think the main point of departure from where your typical statistics book would cover the subject.",
            "So they would always worry very much about specific parametric solutions.",
            "And then the Hammersley Clifford decomposition, which allows us to map certain structures of graphical models into corresponding objects in terms of kernels.",
            "And then we'll apply to a few things.",
            "Classification, novelty detection, regression, conditional random fields.",
            "Probably I'll be only very, very briefly mentioning intractable models.",
            "Just give you an idea why they are such an issue and how to deal with them, but I won't be talking much about semidefinite approximations.",
            "One reason also being that as it's turned out, this is a wonderful theory, but it.",
            "Well, if you want to implement it, good luck.",
            "Um?"
        ],
        [
            "Nature one.",
            "So the first thing we'll look at just the model will get used to a little bit of what log partition function is expectations.",
            "How to compute derivatives and the maximum entropy, and so their entire conferences on that subject, and they're basically something like, well, I know a few things about my system.",
            "Random variables that measure some things and I would like to find a distribution which matches all those observations.",
            "And usually there are lots of distributions which will do that.",
            "So you need an additional.",
            "Inference principle.",
            "To put on top of it, in order to allow you to choose which of the distributions you might want.",
            "And one of those principles that have actually proven to be quite powerful is maximum entropy.",
            "So people would say, well, I want to have a distribution with given mean in a given variance that has maximum entropy.",
            "And it turns out that a this problem is actually nicely solvable.",
            "I mean, it's not completely cleared the first slide, because I mean, you know, there's an infinite number of distributions out there.",
            "Well, how can I pick?",
            "Well, the maximum of this, and Secondly that actually has a very nice and clean solution.",
            "And we've recently my progress on this area too, in the sense that, well, if you don't want to exact maximum trivett approximate.",
            "It actually covers a much larger range of.",
            "Well, systems, then what you would typically expect.",
            "So there are a few surprises here.",
            "And.",
            "Probably the main part of this first leg tries to get familiar with exponential families.",
            "And each of you probably have at some point in his life seen a normal distribution and well estimated coefficients, probably coded up in math level pricing or whatever.",
            "You will most definitely have also looked into distribution of a discrete event if you ever have to model a Dicer coin flip or something.",
            "Laplace Poisson beta distributions are probably something that you might consider slightly more exotic, and I want to convince you that they actually aren't.",
            "And then I'll show you how all those things which often in undergraduate course of statistics lead to very different recipes on how to get the parameters and all that actually can be seen as well following one common thread how to actually get those parameters?",
            "I'll talk a little bit about the Fisher information matrix in the Chroma Row theorem.",
            "Which is actually quite an amazing theorem because it says, well, maximum likelihood is optimal under certain conditions.",
            "It's optimal in terms of the minimum variance estimator for among unbiased estimators.",
            "Unbiased estimators are those which don't introduce a certain bar stored well, OK by its very definition, but a certain distortion of the solution towards a specific answer.",
            "They are not necessarily optimal.",
            "If you look at all estimators but among a certain class there pretty good.",
            "And I'll be mentioning normal product and conjugate priors a little bit and again will encounter things that you might have seen before in a different context.",
            "OK."
        ],
        [
            "So what's the beast will be talking about well?",
            "So this expression here.",
            "So familiar of probability distributions which satisfy this condition, I will be referring to as.",
            "Members of the exponential family.",
            "So this is P of X parameterized by some parameter Theta, which I will be calling the natural parameter.",
            "It's given for E to the inner product between fall fix and Theta minus G of Theta.",
            "Quite often in a stats book you would see some P0X showing up front here.",
            "But I'm basically doing is on kind of hiding this away.",
            "The measure over the domain indexes.",
            "So for all practical purposes, we can just sweep that under the rug, but.",
            "If you need to do.",
            "A specific example, well, you might have to take this into account, but it just makes the notation may seem big and heavy, so whenever you feel like it, put the PC ravex at Frontier.",
            "OK, now let's have a look at the various terms of it.",
            "This expression here.",
            "Ensures that the probability distribution integrates out to one.",
            "So in other words, at this beast here D X = 1, which also gives us the formula how to compute G of Theta?",
            "So just take the integral over.",
            "This equals one, then solve for GF data and we have presto that expression.",
            "OK. X is this domain of which those little extras are drawn.",
            "And the fall of X called the sufficient statistics of X.",
            "So those are really the ones which map X into that specific set of features.",
            "Which then define our distribution.",
            "So vastly different choices of X.",
            "Will lead to vastly different solutions of distribution and will be encountering them in awhile.",
            "And basically for any file fix for which people analytically could compute this expression here.",
            "People have essentially looked at in the end of the 19th beginning of the 20th century.",
            "So your chances of finding some distribution here that were the integral can be solved, and it doesn't have a name yet are essentially 0.",
            "But it also means that, well, you know there's this big laundry list of problems that people have resolved.",
            "So now if you need to study a new problem, all you have to do is go through this laundry list, see whether one of those kind of matches the problem that you want to solve, and you might be lucky.",
            "OK."
        ],
        [
            "Now let's have a look at the simplest such case, and in this case it almost looks like I'm making things considerably more messy than they should be.",
            "So let's toss coins and here with probability P. Well, we can have all the heads or tails occurring.",
            "So let's say with we've got withdrawal EP.",
            "We've got hit with probability 1 -- P we see tails.",
            "MPs, of course between zero and one.",
            "Now that's actually quite convenient, because we can immediately see that those two probabilities sum up to one.",
            "And I know that well P of X, where X is, well, just all the 01 is P to the X * 1 -- P to the 116.",
            "OK. That's a really, really complicated way of writing an if then else statement.",
            "Basically, if X = 1, then P if X = 0 then 1 -- P. Let's make things even more complicated with at the end of the day we want to have this.",
            "Financial form so I can write P of X as being E to the log of P of X. Alright.",
            "Now I go and plug this expression in to the log of P of X.",
            "And big surprise, we actually get something that looks.",
            "Well, so a little bit like what you would have from the exponential family.",
            "And what you get?",
            "Is.",
            "That so I can pull out X and 1 -- X.",
            "Put this together into a vector.",
            "I put together log P and log 1 -- P. Now I have the inner product between the rate and the black vector.",
            "Azita something so it looks pretty close to our exponential family.",
            "Is 1 difference?"
        ],
        [
            "We look at the previous slide.",
            "We have this normalization here."
        ],
        [
            "Now, why don't we need a normalization in this case?",
            "Because I've chosen P. Well too.",
            "Just such that P and 1 -- P sum up to one.",
            "And another thing that you would notice is this vector Theta as a function of P. Is a 1 dimensional curve in the two dimensional space.",
            "So I'm actually not really exploring the full space that I could be exploring.",
            "OK.",
            "So how to make things even more messy?",
            "But I could simply say well rather than state of being in this, let me draw it.",
            "If this is Theta 2.",
            "And this is Theta one.",
            "Well, you might get some.",
            "Curve I don't know which might look like this.",
            "And only on this curve you will have a normalized distribution.",
            "So how can I turn this into a thesis that covers the entire space well quite easily?",
            "I just need to take this normalization explicitly into account.",
            "So.",
            "I could just say, well, let's leave this parameter free and then I need to normalize by G of Theta being the log of E to the theater 1 + T to this data 2.",
            "OK.",
            "So what I've done is, I've turned this perfectly nice and simple expression here into a big monster, which.",
            "Uses a sufficient statistics X and 1 -- X.",
            "And.",
            "As G of Theta, that will be just in some of those expressions.",
            "OK. One thing to notice is.",
            "This function here is convex.",
            "Now this will be extremely useful later on.",
            "But it will allow us to find the optimal parameters, Theta one and Theta 2.",
            "In some inference scheme.",
            "By running a convex solver.",
            "Now Bernard's already explained to you why convexity is really, really great.",
            "Why this is really the quantity that you want to have for inference.",
            "And you know why, while I'm doing is, I'm setting everything up.",
            "Two then throw a convex overrated.",
            "Because, well, convex solvers are nicely nice to use.",
            "They have nice convergence guarantees in all that they don't get stuck in local minima by construction, all that.",
            "OK. Who does not know?",
            "That a convex function?",
            "Sorry, who knows that a convex function has only one global minimum?",
            "OK. Um?",
            "Who would know how to prove it?",
            "Not many of you for not.",
            "Did you show it to them?",
            "OK, let me quickly all the picture because.",
            "So quite a useful piece of intuition.",
            "OK.",
            "So first of all.",
            "A convex function.",
            "The function and that may not be just in one dimension, but in several dimensions, where if I pick two points on its graph.",
            "I draw a line.",
            "Then all the function values underneath the line like.",
            "Under none of the points underneath this line on the graph can be found above the line.",
            "So in other words.",
            "This would not be a convex function.",
            "OK. Now, how do I prove that there is only one local, one global minimum?",
            "Well, let's assume.",
            "That I have to local mode.",
            "So at this point in that point.",
            "Pinpoint convexity.",
            "I know that the function values between those two points.",
            "If I just make the line will be less equal in those two values.",
            "Now either those two values are the same.",
            "Then, well, this must all be one global minimum, so I get a function that looks like a troll.",
            "If one of them is less than well, quite obviously I can go from here to there.",
            "For the sake of the argument.",
            "And I've decreased my my function value so I get the contradiction.",
            "And this also works out.",
            "This reasoning in more than one dimension.",
            "And that's one of the.",
            "Reasons why we all like convexity so much.",
            "Their entire courses that you can teach on convex analysis is a wonderful book by Rockefeller.",
            "On convex analysis, and I mean basically it's really a treasure trove, alot of optimization.",
            "Yep.",
            "What?",
            "Face optimization gives us OK, let's actually solve this problem.",
            "OK, So what I'm going to try at some point is I'm going to try and find the optimal parameters, Theta one and Theta 2.",
            "They might want to infer the probability that I'm seeing hits or tails.",
            "So therefore, at some point I will need to optimize over those parameters, Theta one and Theta 2.",
            "So one of the expressions that we might want to optimize is the negative log likelihood of the negative log posterior.",
            "So in this case I would take the negative log.",
            "Of that well."
        ],
        [
            "Let's go up here.",
            "We're basically jumping a few slides ahead.",
            "The negative log of P of X parameterized by Theta will then be G of Theta minus some linear function in Theta.",
            "Now that overall expression is convex.",
            "And so it turns out that it can do things quite easily.",
            "We'll be going through the specific equations in a few slides, so don't worry.",
            "But that's the main reason very good question.",
            "After all, you gotta answer why you're doing something.",
            "So this is the main reason why convexity is really, really handy here.",
            "And we look at maximum likelihood maximum cost."
        ],
        [
            "Estimators.",
            "OK, good, so let's look an example for binomial distribution."
        ],
        [
            "OK.",
            "Doesn't look.",
            "But great deal?",
            "Well, that's just one with point 4.6 probabilities.",
            "OK. Now let's look at.",
            "Another one Laplace distribution.",
            "So this is maybe something that.",
            "Well, you know, collectors familiar with."
        ],
        [
            "So that one.",
            "Well, physics for instance uses this quite a bit, so it's like atomic decay.",
            "Well, at anytime with probability Theta DX, an Atom will decay.",
            "And time interval, if it still exists so we can look at the distribution of the cavins overtime.",
            "And then if you consult your physics book then you will see that well P of X is given by Theta times E to the sea to X.",
            "For X is positive both and it actually turns out that it has to be.",
            "So there should be a minus here, otherwise it doesn't make so much sense.",
            "OK. After we want to have a decay, well, the atoms don't grow overtime.",
            "Um?",
            "So how do we now get this back into an exponential families notation?",
            "Well, we just do our standard log X trick.",
            "So this is E to the now this is a weird inner product which is just the product between 2 numbers.",
            "Namely, minus XC is the minus again and Theta.",
            "Minus the negative log of data.",
            "That's a really simple, sufficient statistic.",
            "Namely far fixes minus X.",
            "And the normalization is also quite simple.",
            "You can see that this function is clearly convex in Theta.",
            "And.",
            "Well, here's an example of 1."
        ],
        [
            "Yeah OK, everybody seen an exponential decay before.",
            "But it's also an exponential families distribution."
        ],
        [
            "Engineers favorite normal distribution everybody stand that one before and This is why you would learn it.",
            "If it's just the univariate one.",
            "Multivariate ones look a little bit more messy, but not so much.",
            "OK, now that doesn't really look like an inner product between some 5X and something else at all.",
            "OK, well let's see how we can reshape this.",
            "Well, P of XI can just write it out.",
            "So the first thing is I'm going to push this into the exponential.",
            "The second thing is going to expand.",
            "This query expression.",
            "So you get minus 1 / 2 Sigma squared X squared plus mu over Sigma squared X minus mu squared over two Sigma squared minus 1/2 log of 2\u03c0 Sigma squared.",
            "Ah, that's good.",
            "See.",
            "Here, get some linear factor front X ^2.",
            "I get some other linear factor front X.",
            "And it gets some junk here, which ensures that everything integrates out to one.",
            "OK.",
            "So now let's rewrite it in terms of, well, inner product between something in the axis and a parameter Theta.",
            "And then something else.",
            "OK.",
            "So we get it easily enough between X&X squared on one hand and Theta, which now captures those two parameters.",
            "Note, this integral doesn't always exist.",
            "In particular, doesn't exist if this term is not negative.",
            "Within the integral with diverge.",
            "So we've seen an interesting phenomenon occurring that the domain of the status need not be all of our.",
            "It can be a subset of R. In fact, it's a convex subset of R. Why so?",
            "Because if we've got a convex function, we want to ensure that this is less equal then it.",
            "That's less that it doesn't diverge.",
            "So basically.",
            "It's less than Infinity then that domain will be a convex domain.",
            "Now the last bit that you have to do is you have to rewrite this in terms of Theta one and Theta 2.",
            "That's.",
            "Quite tedious, but you can work it out so data one would be mu Sigma square, mu Sigma squared, Theta two would be minus 1/2 Sigma squared.",
            "I would single square, then G of Theta.",
            "Is this expression here?",
            "OK, it looks like we've gained absolutely nothing.",
            "Because I've turned a perfectly good and well known expression, which is this one here?",
            "Into something absolutely atrocious down there just will cycle algebraic reformulations.",
            "Well, actually have gained something because if I later on.",
            "Want to make this expression here dependent on the location or something similar like what you would do for heteroskedastic regression.",
            "Then this function will be extremely handy because it will be convex and will actually allow me nice optimization and dreaming up that function.",
            "Well, I couldn't dream it up in my worst nightmares.",
            "So you will in this way, in a systematic way stumble across nice convex optimization problems that otherwise you wouldn't find.",
            "And while some others statisticians before have tried designing such estimators, and since they didn't follow this track, they dreamed up the wrong functions and the problems were convex.",
            "So this is actually quite convenient in a way we have to pay a price, but afterwards also the implementation gets nicer.",
            "OK, everybody seen it."
        ],
        [
            "Distribution.",
            "OK mom."
        ],
        [
            "Normal distributions.",
            "Well, you've seen that those two, so that's basically.",
            "For instance, if we have disjoint events like in different ones, like if I toss a Dyson in would be 6.",
            "Which all may occur with a certain probability.",
            "Now I could go through this entire expression expression like what we had for the binomial distribution and then re derive it.",
            "Or I could just tell you the answer.",
            "So I'll save you the pain.",
            "And what I'm going to tell you is that this map here, which Maps X into the unit vector X.",
            "Is the right one to give us multinomial distribution?",
            "Well, what does this mean?",
            "Well, the inner product between X and Theta just picks out the corresponding entry in Theta.",
            "So for instance E3.",
            "In a product with Theta, would pick up data 3.",
            "So in other words, I would get E to the theater 3 minus the normalization.",
            "And that's exactly what I have here GF data.",
            "Is the log of the sum over E to the Theta eyes?",
            "OK. And that's again.",
            "Is an exponential families distribution.",
            "The advantage of doing it this way is that we've now covered a rather wide range of distributions which all can be dealt with with the same machinery afterwards.",
            "So for instance, if I have some convex solvers, some optimizer, it will not care so much about which particular distribution I'm putting into it.",
            "Well, it's just different.",
            "Feature map and all that, and afterwards once I tell my solver, but the form of Geocities, it'll just happily optimize away at it.",
            "So I can basically build one optimizer that can deal with multiclass with regression with normal distributions with.",
            "Well.",
            "Just.",
            "Two different events and will see a lot more others in the same framework.",
            "This is great if you're writing software.",
            "OK."
        ],
        [
            "Distribution of a 10 different events yeah.",
            "Unless you have some switch instructing program.",
            "Well, it depends.",
            "I mean in certain case."
        ],
        [
            "This is even for a normal distribution.",
            "You want this G of data to be in this ugly form.",
            "Yeah, when you're writing a solver, you do.",
            "You do want to take care of this.",
            "I guess most of your time management in this computing normalization because you're integrating automated.",
            "Not always.",
            "I mean it.",
            "It really depends very much on which problem you're trying to solve.",
            "So what you need to take care of is that in certain cases, well, your status are not well before I mean do not lead to well defined normalizations, G of Theta over the entire domain, like for instance normal distributions with negative variance don't make sense.",
            "Or so that's the type of thing that you want to avoid.",
            "Well, plus distributions where the number of atoms keeps on increasing doesn't make sense.",
            "But other than that it."
        ],
        [
            "Just fine."
        ],
        [
            "OK. Now let's look at some others.",
            "Let's look at the personal distribution.",
            "So once all distributions look very much like Laplace distribution in fact.",
            "Well, the sufficient statistics of those are exactly the same ones as what we had for the plus distribution.",
            "Let's just go back."
        ],
        [
            "So our far fix here was minus X."
        ],
        [
            "And here are far fix was expected, just also call it minor fix and flip the sign of data.",
            "But there's one key difference between the points on the Laplace distribution.",
            "With the partial distribution I have a discrete domain.",
            "So it's not just the positive half axis.",
            "It's just the set of all integers, greater, equal and zero.",
            "And what this does is it leads to.",
            "And in addition to that, I also have a different measure here on the domain, which I impose.",
            "Let me do this one here.",
            "So I don't count every observation the same accounting, well, count the small terms higher than the larger terms.",
            "So this is exactly where I told you initially that will you can hide avoid a PC of X in any way that you want, as long as you can solve the integral.",
            "That's one of those cases.",
            "And it just leads thin to quite a different normalization.",
            "Here the normalization is each of the theater.",
            "Remember, before we have log of Theta here we have minus log of data.",
            "Here is each of the Theta.",
            "So depending on how you.",
            "Define your domain and how you play with your normalization.",
            "I mean, so how you play with the measure, you will get quite different normalizations.",
            "That's probably the most.",
            "Relevant thing in this context.",
            "So this point here.",
            "We play around with the measure.",
            "So what does it look like?"
        ],
        [
            "Prince, this one here.",
            "Almost looks like a normal distribution.",
            "Century the equivalent for normal distribution for the discrete case.",
            "OK beta distribution."
        ],
        [
            "So now we're going to into more and more exact exotic territory of distributions that you might encounter.",
            "Well, actually not quite.",
            "Do you remember before?",
            "We had log it.",
            "We had X and 1 -- X is are sufficient statistics.",
            "For the binomial distribution.",
            "Here we have the logs of those.",
            "And then I just take the inner product with some parameters Theta one and Theta two and I again restricted domain of the axis.",
            "Now to be the interval between zero and one.",
            "Before that we had just events zero and one.",
            "Well, that's a beta function, so it's called the beta distribution.",
            "And.",
            "Well."
        ],
        [
            "You can essentially model a lot of.",
            "Skewed and picked distributions on the unit interval with this.",
            "They don't have to be symmetric, but I've just drawn you three symmetric ones.",
            "So for instance, the uniform distribution over the unit interval is a member of it, one that's peaked at the tails at the ends one.",
            "That's peaked in the middle.",
            "Or you could also have some which are skewed.",
            "So.",
            "You can actually model a lot of distributions of the unit interval.",
            "By beta distribution."
        ],
        [
            "Likewise, you could take a gamma distribution, which.",
            "Well, now rather than taking the unit interval takes interval between zero and Infinity.",
            "And we get slightly different sufficient statistics.",
            "You get a slightly different integral here and off you go again.",
            "The one thing to notice is that here this parameter Theta.",
            "Is actually in rather funny.",
            "Space C. Theta one, so the first coefficient here can take any value between zero in less equal to Infinity, whereas the second parameter is within the open interval of mobile screens.",
            "So the overall domain is convex, but there's no guarantee.",
            "That the interval that domains are actually closed.",
            "Makes sense, so for instance when you have a normal distribution.",
            "Well, you can have.",
            "And in an inverse variance, which goes as close to zero as you want.",
            "In other words, the variance can be as large as you want, but it never will be Infinity.",
            "So this means that the inverse variance will never be 0.",
            "And so therefore we always have an open interval in that parameter.",
            "So there's no guarantee that the intervals on which you're operating on are closed.",
            "And actually, sometimes you get really funny effects happening at the boundaries, and if you want to know more about that, you should talk to me.",
            "She's written entire papers on that."
        ],
        [
            "Some gamma distributions.",
            "OK, and basically you can get things which are really picked at zero or something which is.",
            "Little bit where else, but essentially there recently heavy tailed.",
            "At least you can make them this way.",
            "He"
        ],
        [
            "And toss aladji 'cause I'm an echo.",
            "Go on and talk about more so you can have dish lay distribution so wish heart and we've covered all those in avoid.",
            "They're all just different in terms of their choice of 5X and their domain in the measure.",
            "So as soon as you pick any combination of some form of X.",
            "A domain in which you define it.",
            "And the measure with respect to which you want to measure this domain.",
            "You have everything that you need for exponential families distribution.",
            "In quite a few cases you can solve the integral analytically, that's great.",
            "And those are the cases that most likely would have already been studied before in some stats book.",
            "And.",
            "Well then you just put things together and you can build larger inference models through that.",
            "For instance, if you use this as a conjugate prior, in certain cases, you can.",
            "Produce an entire stream of papers based on exchange ability and so on.",
            "So Michael Jordan's been.",
            "Using this.",
            "Distribution here quite extensively.",
            "And for instance, if you have normal distributions, well, there's a large body of work in statistics literature.",
            "Just about what you can do with a normal distribution.",
            "For instance, you could have Gaussian Markov random fields.",
            "People have written books on that actually even very recently.",
            "And it has a lot to do then with, for instance, sparse matrices and so on.",
            "So we'll get to this in a bit more detail later on.",
            "But so basically, don't underestimate the power of even just a single one of those distributions.",
            "You can build a large body of work of statistical inference devices on top of that.",
            "But what I'm saying is you know, rather than.",
            "Zooming in on a specific one, we can play with all of them and do it efficiently.",
            "All we gotta do is we pick one.",
            "We read off the corresponding normalization.",
            "And, well, we've made our life much easier."
        ],
        [
            "K. Now, why is this useful?",
            "Well, let's just I mean, after all at the end of the day, we want to perform inference.",
            "So.",
            "Let's just look at again what we started out with, so I said, well, let's consider all distributions which look like E to the inner product between some far fix and Theta normalization.",
            "And these were the sufficient statistics.",
            "Will get to that little bit later.",
            "There is a normalization."
        ],
        [
            "OK. One of the really good things is that this function G of data is a nice function.",
            "Because it generates a cumulants.",
            "So sometimes people actually use a mechanism very similar to this, as cumulant generating mechanism, just to get those for any arbitrary distribution.",
            "And the math actually does look quite similar.",
            "So let's recall this is our.",
            "G of data.",
            "Well, let's take the derivative with respect to Theta of G of Theta.",
            "And, well, ignoring issues of convergence and all that, well, all we do is we just apply the chain rule and push the derivative through the integral.",
            "So the log of an integral.",
            "Is one of the integral times then?",
            "The relative pushed into the integral.",
            "Now ITA, the something well dictator.",
            "I just pulled my file fix out of here.",
            "OK. Now what do I get?",
            "And get 5X times E to this expression here, minus the log of that expression.",
            "So basically, if I wanted to squeeze them in here, this in here I would have to take Monster log.",
            "Now that."
        ],
        [
            "If you recall, is exactly GF data."
        ],
        [
            "In other words, what I've got sitting here, this entire expression there is P of X parameterized point Theta.",
            "Now the integral of our fix.",
            "DP of X parameterized class data is just the expected value of four fix.",
            "OK, well that's great is what I've done is I've now once I've done the hard work of looking up in some statistics book book about G of Theta is.",
            "I can compute the expected values.",
            "A for fix.",
            "Just by taking derivatives.",
            "So rather than me having to actually solve this integral here, I just take Maple, tell it, take the derivative with respect to those parameters and all that comes with an answer.",
            "So now this means that they have an automatic way of generating all those expectations.",
            "So this makes my life really really much easier.",
            "OK. Now let's get ambitious.",
            "Let's take the second derivative of G of Theta.",
            "Well.",
            "You could now go through all the details and I encourage you to do so at you later.",
            "At some point, you'll probably want to do that exactly once in your life, and then never again.",
            "You'll find, but you really should do it once.",
            "It's really, really worth it.",
            "I mean, don't believe just everything that I'm saying.",
            "It might have typos on my slides.",
            "You will get the covariance of our fix.",
            "And it turns out that if you.",
            "Take further derivatives.",
            "You'll get higher order Cumulants.",
            "OK. That's great, it's basically I can generate all my cumulants of this distribution.",
            "And it's a slightly deeper effect.",
            "By just taking derivatives of this function G of data.",
            "Now, why is this good?",
            "Well, first of all, yeah, OK, I could get all the cumulants fine Secondly.",
            "Well, what does it mean that the second derivative of G of Theta is the covariance?",
            "Well, does it?",
            "Does anybody have an idea how the eigenvalues of a covariance matrix have to behave?",
            "Any takers?",
            "They have to be, well wish you know it, but they have to be positive.",
            "Well actually at least non negative.",
            "'cause yeah, variance can never be less than 0.",
            "Now what does that mean?",
            "Well, this means that this function here is actually.",
            "At least twice differentiable.",
            "And Furthermore, this function G of Theta is convex.",
            "So.",
            "Is exactly what I was talking about before that.",
            "Well, we want to get a nice optimization problem and this is already the first indication.",
            "That this will be the case.",
            "OK.",
            "Yes, and that comes from because.",
            "Well.",
            "And showing that takes more than a lie, but yes, so in fact this function is really, really nice as we indicated.",
            "So you can do pretty much anything you want with it and it will behave nicely.",
            "Now.",
            "Let's actually use."
        ],
        [
            "This was something practical.",
            "So say we want to estimate the decay constant of an Atom.",
            "Back to our physics.",
            "Idea?",
            "So well, we just use this exponential family notation.",
            "OK. Now all I want is well.",
            "That well, if I.",
            "Know what mu is?",
            "So if I know what the expected lifetime of an Atom is, I want to find the corresponding parameter Theta.",
            "Well, I could just actually, you know, go and take expectations and all that, but I can just make my life really easy.",
            "'cause I know that file fix is minus 6.",
            "So I know that the expected.",
            "Life of X.",
            "So that's the expected value of X.",
            "That's the lifetime has to be mute.",
            "But I also know that this expected value of this expression here is D, Theta of G of Theta.",
            "OK.",
            "So it's one of those data.",
            "So therefore I can just solve for Theta and I get that this is minus one over mu.",
            "OK.",
            "So basically what I've done is I've solved an optimization problem by just solving it.",
            "Analytically here.",
            "That's a lucky.",
            "Situation and it will usually not occur.",
            "Usually you'll have to use.",
            "The optimizer gets an answer.",
            "But in some cases you can just completely analytically right out the expression and model.",
            "Of course, that's nice.",
            "No.",
            "And this is Zach."
        ],
        [
            "We caught.",
            "I mean, sometimes this is one of the reasons that people introduce exponential family distributions.",
            "Namely, maximum entropy estimation.",
            "So.",
            "I mean, really, really.",
            "Loosely speaking, entropy is basically the number of bits that you need to encode a random variable.",
            "And well, we can define it as the expected value of the negative log of P of X.",
            "And we just said zero log zero X 0.",
            "OK.",
            "So it turns out that the density P of X.",
            "Which satisfies the condition that expected value for fixes create equal Anita with maximum entropy.",
            "Is of this form?",
            "Where?",
            "Then the theater of G of Theta equals eaten.",
            "OK. Now.",
            "Who has seen this fact before?",
            "OK. For five people.",
            "Um?",
            "Well, who wants to see the proof?",
            "OK, so basically in the break the five minutes break when other people can go to the bathroom, you will get to see the proof.",
            "OK.",
            "So.",
            "Now.",
            "One of the corollaries is at the maximum entropy distribution with a given variance.",
            "Is the Gaussian distribution.",
            "Well, why?",
            "Because the.",
            "Now I could go through this entire expect special like what we have for the binomial distribution, and then we derive correct.",
            "Just tell me the answer.",
            "South.",
            "So I will get a distribution which looks like P of X is proportional to equals.",
            "E to the sum parameter Theta times X ^2.",
            "Minus G updater.",
            "Because this is the same guy here.",
            "So now I have a normal distribution.",
            "If I want to play around with the mean two, I might want to say something like the expected value of X is created equal, then say maybe 2.",
            "So I would get Theta two X squared plus Theta 1 * X.",
            "And off we go again.",
            "Or if I just want to play around with the mean so.",
            "Just required that the expected value X is greater equal then say two.",
            "I would get P of X being each other.",
            "Theta Times X -- G of Theta.",
            "I could then say, well, yeah, that should be the case and.",
            "X to be in the domain of integers.",
            "Well then I will get a different function G, But that's the only thing that would change.",
            "So I have a fairly powerful mechanism.",
            "How to turn conditions on expectations of random variables?",
            "Into distributions.",
            "And the entire conference is on.",
            "Well, how to do this efficiently?",
            "So conference called Max int.",
            "Now in fact.",
            "As it will turn out, this is not topic of this talk, but it's quite.",
            "An amazing thing that.",
            "Basically you asked me out and I prove.",
            "Is.",
            "Well, if you don't just know.",
            "Exactly that, the expected value of some form of X.",
            "Not equals mu.",
            "So some mean.",
            "But it's just minus mu at this norm.",
            "Is less equal then some epsilon.",
            "And you want to find the maximum entropy distribution of that.",
            "Then you will get an estimate.",
            "Which will not be a maximum likelihood estimate.",
            "But a maximum posterior estimate.",
            "Basically, you get Bayesian estimation out of this.",
            "So.",
            "That's all I wanted to say about that specific thing.",
            "Um?",
            "So.",
            "That sack."
        ],
        [
            "I put it to practical use.",
            "I mean, these are all really nice things to know in order to know that, well, yeah, the world is all sane and all the estimation algorithms lead to something quite similar.",
            "But now let's actually put it to practical use.",
            "So let's say we observe some data X1 through exam.",
            "Which are drawn from some distribution P of X parameterized by Theta.",
            "Well, I could go and compute the likelihood.",
            "So get PF X parameterized by Theta.",
            "Which is a big product of roll.",
            "My inbox ovations of.",
            "The individual probabilities.",
            "OK.",
            "Fat this is always the case if all these eyes are drawn in dependently.",
            "If they were to depend on each other, I couldn't just write the joint probability as the product of the individual probabilities, but.",
            "Since they're all drawn independently, as I assume that's the case.",
            "OK. Now we know.",
            "That well, this is an exponential, so I can push this product inside.",
            "And well, what I get is.",
            "Each other and then the sum of the falx size in here.",
            "And I get M * 3 of data.",
            "Is.",
            "Product of in terms of this expression, here is just M * G of data.",
            "OK. Now let me put those intermediate calculations up on the whiteboard.",
            "So.",
            "PFX parameterized by Theatre.",
            "Is if I push this product inside?",
            "Is each other?",
            "Son, I going from one to M. Find of XI.",
            "In a product with data.",
            "Minus.",
            "M Times GF later.",
            "OK everybody happy with this situation?",
            "Who's unhappy?",
            "Nobody good now if I want to find that Theta which is the most likely Theta, explaining the data.",
            "So explain X.",
            "Well, I would go and compute.",
            "Arc Max.",
            "Over theater of payoff data so.",
            "PFX parameterized by Theta.",
            "Which is of course the same thing as the arcmin.",
            "Over Theta of the negative log likelihood.",
            "The reason for taking this term is that while people usually don't like taking products, they prefer taking sums.",
            "Just.",
            "Algebraic convenience.",
            "OK. Now that expression here.",
            "I can write down immediately as.",
            "M * G of data.",
            "Minus.",
            "The inner product between some over I going from one to M. Phi of XI.",
            "With data.",
            "OK. Just taking this out here.",
            "I could also.",
            "Bracket out the M. So get the one over him in here.",
            "Ah, now this is interesting.",
            "Becaused what's happening is that this expression in here.",
            "It's basically an empirical average over my file fix.",
            "OK. And in fact, this is the only thing I really need to remember about my data.",
            "I don't need to remember the individual excise, just we need to remember this average.",
            "This may be hard to compute, but that's the only thing I need to remember.",
            "That's fully equivalent to what I had before.",
            "That's why this is called the sufficient statistics.",
            "OK. And now what I'm going to do is, I'm going to say well now argument of Theta.",
            "Well, OK, well how do we get the minimum?",
            "Well, we take the first relative set it to 0.",
            "And this is now a convex function.",
            "With these are convex function that's linear.",
            "So I get that these data just drop the M because it doesn't really matter.",
            "Equals mu hat.",
            "Now we know.",
            "That this expression here by the Cumulant generating property was also the expected value of firefix.",
            "OK.",
            "So what I'm doing, in a way, is I'm saying I want to find the distribution.",
            "For which the empirical mean equals the actual mean.",
            "Sorry.",
            "3 foot one.",
            "Slide back.",
            "Yeah, sorry.",
            "So if I had a normal distribution.",
            "I would just observe my empirically mean empirical variance.",
            "And then I will construct a normal distribution for which the empirically mean and empirical variance will fit through mean and variance.",
            "Now obviously nobody of us here believes that the empirically mean is always spot on.",
            "And This is why I said before.",
            "If we find an estimator for which well, the true mean difference by no more than epsilon, and this epsilon is chosen chosen suitably.",
            "I will actually get an estimator which is quite powerful in practice.",
            "OK.",
            "But the good news is that this expression here.",
            "Can be solved.",
            "Well, OK, if I cannot solve it analytically, I can solve it easily by Newton's method.",
            "So it's something that you can do quite efficiently."
        ],
        [
            "Now.",
            "Let's look at an application.",
            "So just create events.",
            "Sorry, I've got some random variables from tossing a dice 6 possible outcomes.",
            "These would be the counts, maybe imperfectly.",
            "And these are the probabilities.",
            "Now the maximum.",
            "Now this is something you would do intuitively anyway.",
            "If I tell you.",
            "You have the actual counts and I would ask.",
            "You will tell me the probabilities, then even improbably.",
            "Yeah, high school you would say, well OK, therefore the probability of seeing a two would be .3.",
            "So what you've done is, without knowing all this machinery, you've done a maximum likelihood estimation.",
            "But what you can also see is that this is somewhat fraught with problems.",
            "Well, I mean, if we toss a dice and we get those numbers, well, you would say that's quite normal.",
            "But he would also say, well, it doesn't mean that the probability of seeing a four is actually .04.",
            "It just happens that you've been a bit unlucky that time because you didn't toss the dice often enough.",
            "So if you repeat it a million times times and you still see really small County would probably think that the dice is broken.",
            "But if you just toss it like 20 times or so, there's no reason for you to believe that.",
            "OK, So what do we have to do?",
            "We have to.",
            "In a way, smooth out those numbers we need.",
            "I mean, you would immediately feel like you'd want to make them closer to uniform here, because your prior assumption on the dice is that ISIS well behaved.",
            "The dice has all the events evenly distributed uniformly.",
            "And things get even worse if we have continuous random variables.",
            "OK.",
            "So This is why at some point we'll need to introduce a prior.",
            "Just to show you again, little bit what happens, let's pertain to."
        ],
        [
            "1500 observations and you can see well it smooths out more closely to point to 1 / 6.",
            "And this is not a real dice.",
            "I didn't sit down.",
            "OK. Well, I guess probably a good time to have a break just for a moment."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alex has been at the end you on and off now for about five years and before then works extensively with people like Klaus, Alex.",
                    "label": 0
                },
                {
                    "sent": "Alex is a kind of person is on the cutting edge of machine learning and have been successively proving that there are more and more general machine learning projects out there that we can utilize for for our benefit.",
                    "label": 0
                },
                {
                    "sent": "Thank you, Alex.",
                    "label": 0
                },
                {
                    "sent": "So ask me to play little bit more.",
                    "label": 0
                },
                {
                    "sent": "Wanna talk about this?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Both ways stairway to dim down the front left front row a little bit.",
                    "label": 0
                },
                {
                    "sent": "'cause otherwise I guess it's a little bit hard.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It takes awhile.",
                    "label": 0
                },
                {
                    "sent": "K. Maybe it's improving here.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK alright.",
                    "label": 0
                },
                {
                    "sent": "Up maybe that was a bit too much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, I hope you won't be falling asleep.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Who did that now?",
                    "label": 0
                },
                {
                    "sent": "Thanks so one of the reasons why I think exponential families are really useful and powerful.",
                    "label": 1
                },
                {
                    "sent": "Is that well?",
                    "label": 0
                },
                {
                    "sent": "They actually allow us to unify a couple of existing tools into one common framework.",
                    "label": 0
                },
                {
                    "sent": "So that actually, I guess, is probably one of the main attractions.",
                    "label": 0
                },
                {
                    "sent": "So it turns out there slightly more general objects out there that you could use.",
                    "label": 0
                },
                {
                    "sent": "And yes, you can prove wonderful theorems about them, but in that case the math gets a little bit ugly when you have to talk about bank space rather than Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "But I mean basically I'll be trying to keep things reasonably simple and start with a couple of things that.",
                    "label": 1
                },
                {
                    "sent": "Well, if you were assisted, you will probably know forwards and backwards, but quite often in machine learning people haven't really seen that much before.",
                    "label": 0
                },
                {
                    "sent": "OK, let's just get started.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'll be talking about is exponential families.",
                    "label": 0
                },
                {
                    "sent": "First of all, just to define what they look like.",
                    "label": 0
                },
                {
                    "sent": "Well, we look at maximum likelihood estimation Fisher information.",
                    "label": 1
                },
                {
                    "sent": "Also, because I mean this is the Fisher information, at least for me.",
                    "label": 0
                },
                {
                    "sent": "It was always a little bit shrouded in some mystery as to why this is this weird beast from statistics that people haven't really.",
                    "label": 0
                },
                {
                    "sent": "Well, they do something with it.",
                    "label": 0
                },
                {
                    "sent": "They talk about information geometry and nobody really understands.",
                    "label": 0
                },
                {
                    "sent": "It actually turns out to be something rather simple.",
                    "label": 0
                },
                {
                    "sent": "And we'll be talking about price.",
                    "label": 0
                },
                {
                    "sent": "And I guess this is probably the main thing that you should be remembering from this course.",
                    "label": 0
                },
                {
                    "sent": "Then the next thing is will be looking into conditioning and feature spaces.",
                    "label": 1
                },
                {
                    "sent": "And so we talk about conditional distributions and inner product.",
                    "label": 1
                },
                {
                    "sent": "This is basically making the switch from.",
                    "label": 0
                },
                {
                    "sent": "Exponential families seem to kernels, and this is, I think the main point of departure from where your typical statistics book would cover the subject.",
                    "label": 0
                },
                {
                    "sent": "So they would always worry very much about specific parametric solutions.",
                    "label": 0
                },
                {
                    "sent": "And then the Hammersley Clifford decomposition, which allows us to map certain structures of graphical models into corresponding objects in terms of kernels.",
                    "label": 0
                },
                {
                    "sent": "And then we'll apply to a few things.",
                    "label": 0
                },
                {
                    "sent": "Classification, novelty detection, regression, conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "Probably I'll be only very, very briefly mentioning intractable models.",
                    "label": 0
                },
                {
                    "sent": "Just give you an idea why they are such an issue and how to deal with them, but I won't be talking much about semidefinite approximations.",
                    "label": 0
                },
                {
                    "sent": "One reason also being that as it's turned out, this is a wonderful theory, but it.",
                    "label": 0
                },
                {
                    "sent": "Well, if you want to implement it, good luck.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nature one.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we'll look at just the model will get used to a little bit of what log partition function is expectations.",
                    "label": 1
                },
                {
                    "sent": "How to compute derivatives and the maximum entropy, and so their entire conferences on that subject, and they're basically something like, well, I know a few things about my system.",
                    "label": 0
                },
                {
                    "sent": "Random variables that measure some things and I would like to find a distribution which matches all those observations.",
                    "label": 0
                },
                {
                    "sent": "And usually there are lots of distributions which will do that.",
                    "label": 0
                },
                {
                    "sent": "So you need an additional.",
                    "label": 0
                },
                {
                    "sent": "Inference principle.",
                    "label": 0
                },
                {
                    "sent": "To put on top of it, in order to allow you to choose which of the distributions you might want.",
                    "label": 0
                },
                {
                    "sent": "And one of those principles that have actually proven to be quite powerful is maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "So people would say, well, I want to have a distribution with given mean in a given variance that has maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that a this problem is actually nicely solvable.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not completely cleared the first slide, because I mean, you know, there's an infinite number of distributions out there.",
                    "label": 0
                },
                {
                    "sent": "Well, how can I pick?",
                    "label": 0
                },
                {
                    "sent": "Well, the maximum of this, and Secondly that actually has a very nice and clean solution.",
                    "label": 0
                },
                {
                    "sent": "And we've recently my progress on this area too, in the sense that, well, if you don't want to exact maximum trivett approximate.",
                    "label": 0
                },
                {
                    "sent": "It actually covers a much larger range of.",
                    "label": 0
                },
                {
                    "sent": "Well, systems, then what you would typically expect.",
                    "label": 0
                },
                {
                    "sent": "So there are a few surprises here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Probably the main part of this first leg tries to get familiar with exponential families.",
                    "label": 0
                },
                {
                    "sent": "And each of you probably have at some point in his life seen a normal distribution and well estimated coefficients, probably coded up in math level pricing or whatever.",
                    "label": 0
                },
                {
                    "sent": "You will most definitely have also looked into distribution of a discrete event if you ever have to model a Dicer coin flip or something.",
                    "label": 0
                },
                {
                    "sent": "Laplace Poisson beta distributions are probably something that you might consider slightly more exotic, and I want to convince you that they actually aren't.",
                    "label": 0
                },
                {
                    "sent": "And then I'll show you how all those things which often in undergraduate course of statistics lead to very different recipes on how to get the parameters and all that actually can be seen as well following one common thread how to actually get those parameters?",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit about the Fisher information matrix in the Chroma Row theorem.",
                    "label": 1
                },
                {
                    "sent": "Which is actually quite an amazing theorem because it says, well, maximum likelihood is optimal under certain conditions.",
                    "label": 0
                },
                {
                    "sent": "It's optimal in terms of the minimum variance estimator for among unbiased estimators.",
                    "label": 0
                },
                {
                    "sent": "Unbiased estimators are those which don't introduce a certain bar stored well, OK by its very definition, but a certain distortion of the solution towards a specific answer.",
                    "label": 0
                },
                {
                    "sent": "They are not necessarily optimal.",
                    "label": 1
                },
                {
                    "sent": "If you look at all estimators but among a certain class there pretty good.",
                    "label": 0
                },
                {
                    "sent": "And I'll be mentioning normal product and conjugate priors a little bit and again will encounter things that you might have seen before in a different context.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the beast will be talking about well?",
                    "label": 0
                },
                {
                    "sent": "So this expression here.",
                    "label": 0
                },
                {
                    "sent": "So familiar of probability distributions which satisfy this condition, I will be referring to as.",
                    "label": 1
                },
                {
                    "sent": "Members of the exponential family.",
                    "label": 0
                },
                {
                    "sent": "So this is P of X parameterized by some parameter Theta, which I will be calling the natural parameter.",
                    "label": 0
                },
                {
                    "sent": "It's given for E to the inner product between fall fix and Theta minus G of Theta.",
                    "label": 0
                },
                {
                    "sent": "Quite often in a stats book you would see some P0X showing up front here.",
                    "label": 0
                },
                {
                    "sent": "But I'm basically doing is on kind of hiding this away.",
                    "label": 0
                },
                {
                    "sent": "The measure over the domain indexes.",
                    "label": 0
                },
                {
                    "sent": "So for all practical purposes, we can just sweep that under the rug, but.",
                    "label": 0
                },
                {
                    "sent": "If you need to do.",
                    "label": 0
                },
                {
                    "sent": "A specific example, well, you might have to take this into account, but it just makes the notation may seem big and heavy, so whenever you feel like it, put the PC ravex at Frontier.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's have a look at the various terms of it.",
                    "label": 0
                },
                {
                    "sent": "This expression here.",
                    "label": 0
                },
                {
                    "sent": "Ensures that the probability distribution integrates out to one.",
                    "label": 1
                },
                {
                    "sent": "So in other words, at this beast here D X = 1, which also gives us the formula how to compute G of Theta?",
                    "label": 1
                },
                {
                    "sent": "So just take the integral over.",
                    "label": 0
                },
                {
                    "sent": "This equals one, then solve for GF data and we have presto that expression.",
                    "label": 0
                },
                {
                    "sent": "OK. X is this domain of which those little extras are drawn.",
                    "label": 1
                },
                {
                    "sent": "And the fall of X called the sufficient statistics of X.",
                    "label": 0
                },
                {
                    "sent": "So those are really the ones which map X into that specific set of features.",
                    "label": 0
                },
                {
                    "sent": "Which then define our distribution.",
                    "label": 0
                },
                {
                    "sent": "So vastly different choices of X.",
                    "label": 0
                },
                {
                    "sent": "Will lead to vastly different solutions of distribution and will be encountering them in awhile.",
                    "label": 0
                },
                {
                    "sent": "And basically for any file fix for which people analytically could compute this expression here.",
                    "label": 0
                },
                {
                    "sent": "People have essentially looked at in the end of the 19th beginning of the 20th century.",
                    "label": 0
                },
                {
                    "sent": "So your chances of finding some distribution here that were the integral can be solved, and it doesn't have a name yet are essentially 0.",
                    "label": 0
                },
                {
                    "sent": "But it also means that, well, you know there's this big laundry list of problems that people have resolved.",
                    "label": 0
                },
                {
                    "sent": "So now if you need to study a new problem, all you have to do is go through this laundry list, see whether one of those kind of matches the problem that you want to solve, and you might be lucky.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's have a look at the simplest such case, and in this case it almost looks like I'm making things considerably more messy than they should be.",
                    "label": 0
                },
                {
                    "sent": "So let's toss coins and here with probability P. Well, we can have all the heads or tails occurring.",
                    "label": 0
                },
                {
                    "sent": "So let's say with we've got withdrawal EP.",
                    "label": 0
                },
                {
                    "sent": "We've got hit with probability 1 -- P we see tails.",
                    "label": 0
                },
                {
                    "sent": "MPs, of course between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Now that's actually quite convenient, because we can immediately see that those two probabilities sum up to one.",
                    "label": 0
                },
                {
                    "sent": "And I know that well P of X, where X is, well, just all the 01 is P to the X * 1 -- P to the 116.",
                    "label": 0
                },
                {
                    "sent": "OK. That's a really, really complicated way of writing an if then else statement.",
                    "label": 0
                },
                {
                    "sent": "Basically, if X = 1, then P if X = 0 then 1 -- P. Let's make things even more complicated with at the end of the day we want to have this.",
                    "label": 0
                },
                {
                    "sent": "Financial form so I can write P of X as being E to the log of P of X. Alright.",
                    "label": 0
                },
                {
                    "sent": "Now I go and plug this expression in to the log of P of X.",
                    "label": 0
                },
                {
                    "sent": "And big surprise, we actually get something that looks.",
                    "label": 0
                },
                {
                    "sent": "Well, so a little bit like what you would have from the exponential family.",
                    "label": 0
                },
                {
                    "sent": "And what you get?",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "That so I can pull out X and 1 -- X.",
                    "label": 0
                },
                {
                    "sent": "Put this together into a vector.",
                    "label": 0
                },
                {
                    "sent": "I put together log P and log 1 -- P. Now I have the inner product between the rate and the black vector.",
                    "label": 0
                },
                {
                    "sent": "Azita something so it looks pretty close to our exponential family.",
                    "label": 0
                },
                {
                    "sent": "Is 1 difference?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at the previous slide.",
                    "label": 0
                },
                {
                    "sent": "We have this normalization here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, why don't we need a normalization in this case?",
                    "label": 0
                },
                {
                    "sent": "Because I've chosen P. Well too.",
                    "label": 0
                },
                {
                    "sent": "Just such that P and 1 -- P sum up to one.",
                    "label": 0
                },
                {
                    "sent": "And another thing that you would notice is this vector Theta as a function of P. Is a 1 dimensional curve in the two dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So I'm actually not really exploring the full space that I could be exploring.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how to make things even more messy?",
                    "label": 0
                },
                {
                    "sent": "But I could simply say well rather than state of being in this, let me draw it.",
                    "label": 0
                },
                {
                    "sent": "If this is Theta 2.",
                    "label": 0
                },
                {
                    "sent": "And this is Theta one.",
                    "label": 0
                },
                {
                    "sent": "Well, you might get some.",
                    "label": 0
                },
                {
                    "sent": "Curve I don't know which might look like this.",
                    "label": 0
                },
                {
                    "sent": "And only on this curve you will have a normalized distribution.",
                    "label": 0
                },
                {
                    "sent": "So how can I turn this into a thesis that covers the entire space well quite easily?",
                    "label": 0
                },
                {
                    "sent": "I just need to take this normalization explicitly into account.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I could just say, well, let's leave this parameter free and then I need to normalize by G of Theta being the log of E to the theater 1 + T to this data 2.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what I've done is, I've turned this perfectly nice and simple expression here into a big monster, which.",
                    "label": 0
                },
                {
                    "sent": "Uses a sufficient statistics X and 1 -- X.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As G of Theta, that will be just in some of those expressions.",
                    "label": 0
                },
                {
                    "sent": "OK. One thing to notice is.",
                    "label": 0
                },
                {
                    "sent": "This function here is convex.",
                    "label": 0
                },
                {
                    "sent": "Now this will be extremely useful later on.",
                    "label": 0
                },
                {
                    "sent": "But it will allow us to find the optimal parameters, Theta one and Theta 2.",
                    "label": 0
                },
                {
                    "sent": "In some inference scheme.",
                    "label": 0
                },
                {
                    "sent": "By running a convex solver.",
                    "label": 0
                },
                {
                    "sent": "Now Bernard's already explained to you why convexity is really, really great.",
                    "label": 0
                },
                {
                    "sent": "Why this is really the quantity that you want to have for inference.",
                    "label": 0
                },
                {
                    "sent": "And you know why, while I'm doing is, I'm setting everything up.",
                    "label": 0
                },
                {
                    "sent": "Two then throw a convex overrated.",
                    "label": 0
                },
                {
                    "sent": "Because, well, convex solvers are nicely nice to use.",
                    "label": 0
                },
                {
                    "sent": "They have nice convergence guarantees in all that they don't get stuck in local minima by construction, all that.",
                    "label": 0
                },
                {
                    "sent": "OK. Who does not know?",
                    "label": 0
                },
                {
                    "sent": "That a convex function?",
                    "label": 0
                },
                {
                    "sent": "Sorry, who knows that a convex function has only one global minimum?",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Who would know how to prove it?",
                    "label": 0
                },
                {
                    "sent": "Not many of you for not.",
                    "label": 0
                },
                {
                    "sent": "Did you show it to them?",
                    "label": 0
                },
                {
                    "sent": "OK, let me quickly all the picture because.",
                    "label": 0
                },
                {
                    "sent": "So quite a useful piece of intuition.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "A convex function.",
                    "label": 0
                },
                {
                    "sent": "The function and that may not be just in one dimension, but in several dimensions, where if I pick two points on its graph.",
                    "label": 0
                },
                {
                    "sent": "I draw a line.",
                    "label": 0
                },
                {
                    "sent": "Then all the function values underneath the line like.",
                    "label": 0
                },
                {
                    "sent": "Under none of the points underneath this line on the graph can be found above the line.",
                    "label": 0
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "This would not be a convex function.",
                    "label": 0
                },
                {
                    "sent": "OK. Now, how do I prove that there is only one local, one global minimum?",
                    "label": 0
                },
                {
                    "sent": "Well, let's assume.",
                    "label": 0
                },
                {
                    "sent": "That I have to local mode.",
                    "label": 0
                },
                {
                    "sent": "So at this point in that point.",
                    "label": 0
                },
                {
                    "sent": "Pinpoint convexity.",
                    "label": 0
                },
                {
                    "sent": "I know that the function values between those two points.",
                    "label": 0
                },
                {
                    "sent": "If I just make the line will be less equal in those two values.",
                    "label": 0
                },
                {
                    "sent": "Now either those two values are the same.",
                    "label": 0
                },
                {
                    "sent": "Then, well, this must all be one global minimum, so I get a function that looks like a troll.",
                    "label": 0
                },
                {
                    "sent": "If one of them is less than well, quite obviously I can go from here to there.",
                    "label": 0
                },
                {
                    "sent": "For the sake of the argument.",
                    "label": 0
                },
                {
                    "sent": "And I've decreased my my function value so I get the contradiction.",
                    "label": 0
                },
                {
                    "sent": "And this also works out.",
                    "label": 0
                },
                {
                    "sent": "This reasoning in more than one dimension.",
                    "label": 0
                },
                {
                    "sent": "And that's one of the.",
                    "label": 0
                },
                {
                    "sent": "Reasons why we all like convexity so much.",
                    "label": 0
                },
                {
                    "sent": "Their entire courses that you can teach on convex analysis is a wonderful book by Rockefeller.",
                    "label": 0
                },
                {
                    "sent": "On convex analysis, and I mean basically it's really a treasure trove, alot of optimization.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Face optimization gives us OK, let's actually solve this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to try at some point is I'm going to try and find the optimal parameters, Theta one and Theta 2.",
                    "label": 0
                },
                {
                    "sent": "They might want to infer the probability that I'm seeing hits or tails.",
                    "label": 0
                },
                {
                    "sent": "So therefore, at some point I will need to optimize over those parameters, Theta one and Theta 2.",
                    "label": 0
                },
                {
                    "sent": "So one of the expressions that we might want to optimize is the negative log likelihood of the negative log posterior.",
                    "label": 0
                },
                {
                    "sent": "So in this case I would take the negative log.",
                    "label": 0
                },
                {
                    "sent": "Of that well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's go up here.",
                    "label": 0
                },
                {
                    "sent": "We're basically jumping a few slides ahead.",
                    "label": 0
                },
                {
                    "sent": "The negative log of P of X parameterized by Theta will then be G of Theta minus some linear function in Theta.",
                    "label": 0
                },
                {
                    "sent": "Now that overall expression is convex.",
                    "label": 0
                },
                {
                    "sent": "And so it turns out that it can do things quite easily.",
                    "label": 0
                },
                {
                    "sent": "We'll be going through the specific equations in a few slides, so don't worry.",
                    "label": 0
                },
                {
                    "sent": "But that's the main reason very good question.",
                    "label": 0
                },
                {
                    "sent": "After all, you gotta answer why you're doing something.",
                    "label": 0
                },
                {
                    "sent": "So this is the main reason why convexity is really, really handy here.",
                    "label": 0
                },
                {
                    "sent": "And we look at maximum likelihood maximum cost.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimators.",
                    "label": 0
                },
                {
                    "sent": "OK, good, so let's look an example for binomial distribution.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Doesn't look.",
                    "label": 0
                },
                {
                    "sent": "But great deal?",
                    "label": 0
                },
                {
                    "sent": "Well, that's just one with point 4.6 probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let's look at.",
                    "label": 0
                },
                {
                    "sent": "Another one Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe something that.",
                    "label": 0
                },
                {
                    "sent": "Well, you know, collectors familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that one.",
                    "label": 0
                },
                {
                    "sent": "Well, physics for instance uses this quite a bit, so it's like atomic decay.",
                    "label": 1
                },
                {
                    "sent": "Well, at anytime with probability Theta DX, an Atom will decay.",
                    "label": 1
                },
                {
                    "sent": "And time interval, if it still exists so we can look at the distribution of the cavins overtime.",
                    "label": 0
                },
                {
                    "sent": "And then if you consult your physics book then you will see that well P of X is given by Theta times E to the sea to X.",
                    "label": 0
                },
                {
                    "sent": "For X is positive both and it actually turns out that it has to be.",
                    "label": 0
                },
                {
                    "sent": "So there should be a minus here, otherwise it doesn't make so much sense.",
                    "label": 0
                },
                {
                    "sent": "OK. After we want to have a decay, well, the atoms don't grow overtime.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So how do we now get this back into an exponential families notation?",
                    "label": 0
                },
                {
                    "sent": "Well, we just do our standard log X trick.",
                    "label": 0
                },
                {
                    "sent": "So this is E to the now this is a weird inner product which is just the product between 2 numbers.",
                    "label": 0
                },
                {
                    "sent": "Namely, minus XC is the minus again and Theta.",
                    "label": 0
                },
                {
                    "sent": "Minus the negative log of data.",
                    "label": 0
                },
                {
                    "sent": "That's a really simple, sufficient statistic.",
                    "label": 0
                },
                {
                    "sent": "Namely far fixes minus X.",
                    "label": 1
                },
                {
                    "sent": "And the normalization is also quite simple.",
                    "label": 0
                },
                {
                    "sent": "You can see that this function is clearly convex in Theta.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, here's an example of 1.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah OK, everybody seen an exponential decay before.",
                    "label": 0
                },
                {
                    "sent": "But it's also an exponential families distribution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Engineers favorite normal distribution everybody stand that one before and This is why you would learn it.",
                    "label": 1
                },
                {
                    "sent": "If it's just the univariate one.",
                    "label": 0
                },
                {
                    "sent": "Multivariate ones look a little bit more messy, but not so much.",
                    "label": 0
                },
                {
                    "sent": "OK, now that doesn't really look like an inner product between some 5X and something else at all.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's see how we can reshape this.",
                    "label": 0
                },
                {
                    "sent": "Well, P of XI can just write it out.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is I'm going to push this into the exponential.",
                    "label": 0
                },
                {
                    "sent": "The second thing is going to expand.",
                    "label": 0
                },
                {
                    "sent": "This query expression.",
                    "label": 0
                },
                {
                    "sent": "So you get minus 1 / 2 Sigma squared X squared plus mu over Sigma squared X minus mu squared over two Sigma squared minus 1/2 log of 2\u03c0 Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "Ah, that's good.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Here, get some linear factor front X ^2.",
                    "label": 0
                },
                {
                    "sent": "I get some other linear factor front X.",
                    "label": 0
                },
                {
                    "sent": "And it gets some junk here, which ensures that everything integrates out to one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now let's rewrite it in terms of, well, inner product between something in the axis and a parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "And then something else.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we get it easily enough between X&X squared on one hand and Theta, which now captures those two parameters.",
                    "label": 0
                },
                {
                    "sent": "Note, this integral doesn't always exist.",
                    "label": 0
                },
                {
                    "sent": "In particular, doesn't exist if this term is not negative.",
                    "label": 0
                },
                {
                    "sent": "Within the integral with diverge.",
                    "label": 0
                },
                {
                    "sent": "So we've seen an interesting phenomenon occurring that the domain of the status need not be all of our.",
                    "label": 0
                },
                {
                    "sent": "It can be a subset of R. In fact, it's a convex subset of R. Why so?",
                    "label": 0
                },
                {
                    "sent": "Because if we've got a convex function, we want to ensure that this is less equal then it.",
                    "label": 0
                },
                {
                    "sent": "That's less that it doesn't diverge.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "It's less than Infinity then that domain will be a convex domain.",
                    "label": 0
                },
                {
                    "sent": "Now the last bit that you have to do is you have to rewrite this in terms of Theta one and Theta 2.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "Quite tedious, but you can work it out so data one would be mu Sigma square, mu Sigma squared, Theta two would be minus 1/2 Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "I would single square, then G of Theta.",
                    "label": 0
                },
                {
                    "sent": "Is this expression here?",
                    "label": 0
                },
                {
                    "sent": "OK, it looks like we've gained absolutely nothing.",
                    "label": 0
                },
                {
                    "sent": "Because I've turned a perfectly good and well known expression, which is this one here?",
                    "label": 0
                },
                {
                    "sent": "Into something absolutely atrocious down there just will cycle algebraic reformulations.",
                    "label": 0
                },
                {
                    "sent": "Well, actually have gained something because if I later on.",
                    "label": 0
                },
                {
                    "sent": "Want to make this expression here dependent on the location or something similar like what you would do for heteroskedastic regression.",
                    "label": 0
                },
                {
                    "sent": "Then this function will be extremely handy because it will be convex and will actually allow me nice optimization and dreaming up that function.",
                    "label": 0
                },
                {
                    "sent": "Well, I couldn't dream it up in my worst nightmares.",
                    "label": 0
                },
                {
                    "sent": "So you will in this way, in a systematic way stumble across nice convex optimization problems that otherwise you wouldn't find.",
                    "label": 0
                },
                {
                    "sent": "And while some others statisticians before have tried designing such estimators, and since they didn't follow this track, they dreamed up the wrong functions and the problems were convex.",
                    "label": 0
                },
                {
                    "sent": "So this is actually quite convenient in a way we have to pay a price, but afterwards also the implementation gets nicer.",
                    "label": 0
                },
                {
                    "sent": "OK, everybody seen it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "OK mom.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Normal distributions.",
                    "label": 0
                },
                {
                    "sent": "Well, you've seen that those two, so that's basically.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we have disjoint events like in different ones, like if I toss a Dyson in would be 6.",
                    "label": 0
                },
                {
                    "sent": "Which all may occur with a certain probability.",
                    "label": 0
                },
                {
                    "sent": "Now I could go through this entire expression expression like what we had for the binomial distribution and then re derive it.",
                    "label": 0
                },
                {
                    "sent": "Or I could just tell you the answer.",
                    "label": 0
                },
                {
                    "sent": "So I'll save you the pain.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to tell you is that this map here, which Maps X into the unit vector X.",
                    "label": 0
                },
                {
                    "sent": "Is the right one to give us multinomial distribution?",
                    "label": 1
                },
                {
                    "sent": "Well, what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Well, the inner product between X and Theta just picks out the corresponding entry in Theta.",
                    "label": 0
                },
                {
                    "sent": "So for instance E3.",
                    "label": 0
                },
                {
                    "sent": "In a product with Theta, would pick up data 3.",
                    "label": 0
                },
                {
                    "sent": "So in other words, I would get E to the theater 3 minus the normalization.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what I have here GF data.",
                    "label": 0
                },
                {
                    "sent": "Is the log of the sum over E to the Theta eyes?",
                    "label": 0
                },
                {
                    "sent": "OK. And that's again.",
                    "label": 0
                },
                {
                    "sent": "Is an exponential families distribution.",
                    "label": 1
                },
                {
                    "sent": "The advantage of doing it this way is that we've now covered a rather wide range of distributions which all can be dealt with with the same machinery afterwards.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if I have some convex solvers, some optimizer, it will not care so much about which particular distribution I'm putting into it.",
                    "label": 0
                },
                {
                    "sent": "Well, it's just different.",
                    "label": 0
                },
                {
                    "sent": "Feature map and all that, and afterwards once I tell my solver, but the form of Geocities, it'll just happily optimize away at it.",
                    "label": 0
                },
                {
                    "sent": "So I can basically build one optimizer that can deal with multiclass with regression with normal distributions with.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Two different events and will see a lot more others in the same framework.",
                    "label": 0
                },
                {
                    "sent": "This is great if you're writing software.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution of a 10 different events yeah.",
                    "label": 0
                },
                {
                    "sent": "Unless you have some switch instructing program.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends.",
                    "label": 0
                },
                {
                    "sent": "I mean in certain case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is even for a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "You want this G of data to be in this ugly form.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when you're writing a solver, you do.",
                    "label": 0
                },
                {
                    "sent": "You do want to take care of this.",
                    "label": 0
                },
                {
                    "sent": "I guess most of your time management in this computing normalization because you're integrating automated.",
                    "label": 0
                },
                {
                    "sent": "Not always.",
                    "label": 0
                },
                {
                    "sent": "I mean it.",
                    "label": 0
                },
                {
                    "sent": "It really depends very much on which problem you're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So what you need to take care of is that in certain cases, well, your status are not well before I mean do not lead to well defined normalizations, G of Theta over the entire domain, like for instance normal distributions with negative variance don't make sense.",
                    "label": 0
                },
                {
                    "sent": "Or so that's the type of thing that you want to avoid.",
                    "label": 0
                },
                {
                    "sent": "Well, plus distributions where the number of atoms keeps on increasing doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "But other than that it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just fine.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now let's look at some others.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the personal distribution.",
                    "label": 0
                },
                {
                    "sent": "So once all distributions look very much like Laplace distribution in fact.",
                    "label": 0
                },
                {
                    "sent": "Well, the sufficient statistics of those are exactly the same ones as what we had for the plus distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's just go back.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our far fix here was minus X.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are far fix was expected, just also call it minor fix and flip the sign of data.",
                    "label": 0
                },
                {
                    "sent": "But there's one key difference between the points on the Laplace distribution.",
                    "label": 0
                },
                {
                    "sent": "With the partial distribution I have a discrete domain.",
                    "label": 0
                },
                {
                    "sent": "So it's not just the positive half axis.",
                    "label": 0
                },
                {
                    "sent": "It's just the set of all integers, greater, equal and zero.",
                    "label": 0
                },
                {
                    "sent": "And what this does is it leads to.",
                    "label": 0
                },
                {
                    "sent": "And in addition to that, I also have a different measure here on the domain, which I impose.",
                    "label": 0
                },
                {
                    "sent": "Let me do this one here.",
                    "label": 0
                },
                {
                    "sent": "So I don't count every observation the same accounting, well, count the small terms higher than the larger terms.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly where I told you initially that will you can hide avoid a PC of X in any way that you want, as long as you can solve the integral.",
                    "label": 0
                },
                {
                    "sent": "That's one of those cases.",
                    "label": 0
                },
                {
                    "sent": "And it just leads thin to quite a different normalization.",
                    "label": 0
                },
                {
                    "sent": "Here the normalization is each of the theater.",
                    "label": 0
                },
                {
                    "sent": "Remember, before we have log of Theta here we have minus log of data.",
                    "label": 0
                },
                {
                    "sent": "Here is each of the Theta.",
                    "label": 0
                },
                {
                    "sent": "So depending on how you.",
                    "label": 0
                },
                {
                    "sent": "Define your domain and how you play with your normalization.",
                    "label": 0
                },
                {
                    "sent": "I mean, so how you play with the measure, you will get quite different normalizations.",
                    "label": 0
                },
                {
                    "sent": "That's probably the most.",
                    "label": 0
                },
                {
                    "sent": "Relevant thing in this context.",
                    "label": 0
                },
                {
                    "sent": "So this point here.",
                    "label": 0
                },
                {
                    "sent": "We play around with the measure.",
                    "label": 0
                },
                {
                    "sent": "So what does it look like?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prince, this one here.",
                    "label": 0
                },
                {
                    "sent": "Almost looks like a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Century the equivalent for normal distribution for the discrete case.",
                    "label": 0
                },
                {
                    "sent": "OK beta distribution.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we're going to into more and more exact exotic territory of distributions that you might encounter.",
                    "label": 0
                },
                {
                    "sent": "Well, actually not quite.",
                    "label": 0
                },
                {
                    "sent": "Do you remember before?",
                    "label": 0
                },
                {
                    "sent": "We had log it.",
                    "label": 0
                },
                {
                    "sent": "We had X and 1 -- X is are sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "For the binomial distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we have the logs of those.",
                    "label": 0
                },
                {
                    "sent": "And then I just take the inner product with some parameters Theta one and Theta two and I again restricted domain of the axis.",
                    "label": 0
                },
                {
                    "sent": "Now to be the interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Before that we had just events zero and one.",
                    "label": 0
                },
                {
                    "sent": "Well, that's a beta function, so it's called the beta distribution.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can essentially model a lot of.",
                    "label": 0
                },
                {
                    "sent": "Skewed and picked distributions on the unit interval with this.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be symmetric, but I've just drawn you three symmetric ones.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the uniform distribution over the unit interval is a member of it, one that's peaked at the tails at the ends one.",
                    "label": 0
                },
                {
                    "sent": "That's peaked in the middle.",
                    "label": 0
                },
                {
                    "sent": "Or you could also have some which are skewed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can actually model a lot of distributions of the unit interval.",
                    "label": 0
                },
                {
                    "sent": "By beta distribution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likewise, you could take a gamma distribution, which.",
                    "label": 1
                },
                {
                    "sent": "Well, now rather than taking the unit interval takes interval between zero and Infinity.",
                    "label": 0
                },
                {
                    "sent": "And we get slightly different sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "You get a slightly different integral here and off you go again.",
                    "label": 0
                },
                {
                    "sent": "The one thing to notice is that here this parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "Is actually in rather funny.",
                    "label": 0
                },
                {
                    "sent": "Space C. Theta one, so the first coefficient here can take any value between zero in less equal to Infinity, whereas the second parameter is within the open interval of mobile screens.",
                    "label": 0
                },
                {
                    "sent": "So the overall domain is convex, but there's no guarantee.",
                    "label": 0
                },
                {
                    "sent": "That the interval that domains are actually closed.",
                    "label": 0
                },
                {
                    "sent": "Makes sense, so for instance when you have a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, you can have.",
                    "label": 0
                },
                {
                    "sent": "And in an inverse variance, which goes as close to zero as you want.",
                    "label": 0
                },
                {
                    "sent": "In other words, the variance can be as large as you want, but it never will be Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this means that the inverse variance will never be 0.",
                    "label": 0
                },
                {
                    "sent": "And so therefore we always have an open interval in that parameter.",
                    "label": 0
                },
                {
                    "sent": "So there's no guarantee that the intervals on which you're operating on are closed.",
                    "label": 0
                },
                {
                    "sent": "And actually, sometimes you get really funny effects happening at the boundaries, and if you want to know more about that, you should talk to me.",
                    "label": 0
                },
                {
                    "sent": "She's written entire papers on that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some gamma distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, and basically you can get things which are really picked at zero or something which is.",
                    "label": 0
                },
                {
                    "sent": "Little bit where else, but essentially there recently heavy tailed.",
                    "label": 0
                },
                {
                    "sent": "At least you can make them this way.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And toss aladji 'cause I'm an echo.",
                    "label": 0
                },
                {
                    "sent": "Go on and talk about more so you can have dish lay distribution so wish heart and we've covered all those in avoid.",
                    "label": 0
                },
                {
                    "sent": "They're all just different in terms of their choice of 5X and their domain in the measure.",
                    "label": 0
                },
                {
                    "sent": "So as soon as you pick any combination of some form of X.",
                    "label": 0
                },
                {
                    "sent": "A domain in which you define it.",
                    "label": 0
                },
                {
                    "sent": "And the measure with respect to which you want to measure this domain.",
                    "label": 0
                },
                {
                    "sent": "You have everything that you need for exponential families distribution.",
                    "label": 0
                },
                {
                    "sent": "In quite a few cases you can solve the integral analytically, that's great.",
                    "label": 0
                },
                {
                    "sent": "And those are the cases that most likely would have already been studied before in some stats book.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well then you just put things together and you can build larger inference models through that.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you use this as a conjugate prior, in certain cases, you can.",
                    "label": 0
                },
                {
                    "sent": "Produce an entire stream of papers based on exchange ability and so on.",
                    "label": 0
                },
                {
                    "sent": "So Michael Jordan's been.",
                    "label": 0
                },
                {
                    "sent": "Using this.",
                    "label": 0
                },
                {
                    "sent": "Distribution here quite extensively.",
                    "label": 0
                },
                {
                    "sent": "And for instance, if you have normal distributions, well, there's a large body of work in statistics literature.",
                    "label": 0
                },
                {
                    "sent": "Just about what you can do with a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could have Gaussian Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "People have written books on that actually even very recently.",
                    "label": 0
                },
                {
                    "sent": "And it has a lot to do then with, for instance, sparse matrices and so on.",
                    "label": 0
                },
                {
                    "sent": "So we'll get to this in a bit more detail later on.",
                    "label": 0
                },
                {
                    "sent": "But so basically, don't underestimate the power of even just a single one of those distributions.",
                    "label": 0
                },
                {
                    "sent": "You can build a large body of work of statistical inference devices on top of that.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying is you know, rather than.",
                    "label": 0
                },
                {
                    "sent": "Zooming in on a specific one, we can play with all of them and do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "All we gotta do is we pick one.",
                    "label": 0
                },
                {
                    "sent": "We read off the corresponding normalization.",
                    "label": 0
                },
                {
                    "sent": "And, well, we've made our life much easier.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. Now, why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Well, let's just I mean, after all at the end of the day, we want to perform inference.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's just look at again what we started out with, so I said, well, let's consider all distributions which look like E to the inner product between some far fix and Theta normalization.",
                    "label": 1
                },
                {
                    "sent": "And these were the sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "Will get to that little bit later.",
                    "label": 0
                },
                {
                    "sent": "There is a normalization.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. One of the really good things is that this function G of data is a nice function.",
                    "label": 0
                },
                {
                    "sent": "Because it generates a cumulants.",
                    "label": 0
                },
                {
                    "sent": "So sometimes people actually use a mechanism very similar to this, as cumulant generating mechanism, just to get those for any arbitrary distribution.",
                    "label": 0
                },
                {
                    "sent": "And the math actually does look quite similar.",
                    "label": 0
                },
                {
                    "sent": "So let's recall this is our.",
                    "label": 0
                },
                {
                    "sent": "G of data.",
                    "label": 0
                },
                {
                    "sent": "Well, let's take the derivative with respect to Theta of G of Theta.",
                    "label": 0
                },
                {
                    "sent": "And, well, ignoring issues of convergence and all that, well, all we do is we just apply the chain rule and push the derivative through the integral.",
                    "label": 0
                },
                {
                    "sent": "So the log of an integral.",
                    "label": 0
                },
                {
                    "sent": "Is one of the integral times then?",
                    "label": 0
                },
                {
                    "sent": "The relative pushed into the integral.",
                    "label": 0
                },
                {
                    "sent": "Now ITA, the something well dictator.",
                    "label": 0
                },
                {
                    "sent": "I just pulled my file fix out of here.",
                    "label": 0
                },
                {
                    "sent": "OK. Now what do I get?",
                    "label": 0
                },
                {
                    "sent": "And get 5X times E to this expression here, minus the log of that expression.",
                    "label": 0
                },
                {
                    "sent": "So basically, if I wanted to squeeze them in here, this in here I would have to take Monster log.",
                    "label": 0
                },
                {
                    "sent": "Now that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you recall, is exactly GF data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In other words, what I've got sitting here, this entire expression there is P of X parameterized point Theta.",
                    "label": 0
                },
                {
                    "sent": "Now the integral of our fix.",
                    "label": 0
                },
                {
                    "sent": "DP of X parameterized class data is just the expected value of four fix.",
                    "label": 0
                },
                {
                    "sent": "OK, well that's great is what I've done is I've now once I've done the hard work of looking up in some statistics book book about G of Theta is.",
                    "label": 0
                },
                {
                    "sent": "I can compute the expected values.",
                    "label": 0
                },
                {
                    "sent": "A for fix.",
                    "label": 0
                },
                {
                    "sent": "Just by taking derivatives.",
                    "label": 0
                },
                {
                    "sent": "So rather than me having to actually solve this integral here, I just take Maple, tell it, take the derivative with respect to those parameters and all that comes with an answer.",
                    "label": 0
                },
                {
                    "sent": "So now this means that they have an automatic way of generating all those expectations.",
                    "label": 0
                },
                {
                    "sent": "So this makes my life really really much easier.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let's get ambitious.",
                    "label": 0
                },
                {
                    "sent": "Let's take the second derivative of G of Theta.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "You could now go through all the details and I encourage you to do so at you later.",
                    "label": 0
                },
                {
                    "sent": "At some point, you'll probably want to do that exactly once in your life, and then never again.",
                    "label": 0
                },
                {
                    "sent": "You'll find, but you really should do it once.",
                    "label": 0
                },
                {
                    "sent": "It's really, really worth it.",
                    "label": 0
                },
                {
                    "sent": "I mean, don't believe just everything that I'm saying.",
                    "label": 0
                },
                {
                    "sent": "It might have typos on my slides.",
                    "label": 0
                },
                {
                    "sent": "You will get the covariance of our fix.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you.",
                    "label": 0
                },
                {
                    "sent": "Take further derivatives.",
                    "label": 0
                },
                {
                    "sent": "You'll get higher order Cumulants.",
                    "label": 1
                },
                {
                    "sent": "OK. That's great, it's basically I can generate all my cumulants of this distribution.",
                    "label": 0
                },
                {
                    "sent": "And it's a slightly deeper effect.",
                    "label": 0
                },
                {
                    "sent": "By just taking derivatives of this function G of data.",
                    "label": 0
                },
                {
                    "sent": "Now, why is this good?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, yeah, OK, I could get all the cumulants fine Secondly.",
                    "label": 0
                },
                {
                    "sent": "Well, what does it mean that the second derivative of G of Theta is the covariance?",
                    "label": 0
                },
                {
                    "sent": "Well, does it?",
                    "label": 0
                },
                {
                    "sent": "Does anybody have an idea how the eigenvalues of a covariance matrix have to behave?",
                    "label": 0
                },
                {
                    "sent": "Any takers?",
                    "label": 0
                },
                {
                    "sent": "They have to be, well wish you know it, but they have to be positive.",
                    "label": 0
                },
                {
                    "sent": "Well actually at least non negative.",
                    "label": 0
                },
                {
                    "sent": "'cause yeah, variance can never be less than 0.",
                    "label": 0
                },
                {
                    "sent": "Now what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Well, this means that this function here is actually.",
                    "label": 0
                },
                {
                    "sent": "At least twice differentiable.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, this function G of Theta is convex.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Is exactly what I was talking about before that.",
                    "label": 0
                },
                {
                    "sent": "Well, we want to get a nice optimization problem and this is already the first indication.",
                    "label": 0
                },
                {
                    "sent": "That this will be the case.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, and that comes from because.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "And showing that takes more than a lie, but yes, so in fact this function is really, really nice as we indicated.",
                    "label": 0
                },
                {
                    "sent": "So you can do pretty much anything you want with it and it will behave nicely.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's actually use.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was something practical.",
                    "label": 0
                },
                {
                    "sent": "So say we want to estimate the decay constant of an Atom.",
                    "label": 1
                },
                {
                    "sent": "Back to our physics.",
                    "label": 0
                },
                {
                    "sent": "Idea?",
                    "label": 1
                },
                {
                    "sent": "So well, we just use this exponential family notation.",
                    "label": 0
                },
                {
                    "sent": "OK. Now all I want is well.",
                    "label": 0
                },
                {
                    "sent": "That well, if I.",
                    "label": 0
                },
                {
                    "sent": "Know what mu is?",
                    "label": 0
                },
                {
                    "sent": "So if I know what the expected lifetime of an Atom is, I want to find the corresponding parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "Well, I could just actually, you know, go and take expectations and all that, but I can just make my life really easy.",
                    "label": 0
                },
                {
                    "sent": "'cause I know that file fix is minus 6.",
                    "label": 0
                },
                {
                    "sent": "So I know that the expected.",
                    "label": 0
                },
                {
                    "sent": "Life of X.",
                    "label": 0
                },
                {
                    "sent": "So that's the expected value of X.",
                    "label": 0
                },
                {
                    "sent": "That's the lifetime has to be mute.",
                    "label": 0
                },
                {
                    "sent": "But I also know that this expected value of this expression here is D, Theta of G of Theta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it's one of those data.",
                    "label": 0
                },
                {
                    "sent": "So therefore I can just solve for Theta and I get that this is minus one over mu.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically what I've done is I've solved an optimization problem by just solving it.",
                    "label": 0
                },
                {
                    "sent": "Analytically here.",
                    "label": 0
                },
                {
                    "sent": "That's a lucky.",
                    "label": 0
                },
                {
                    "sent": "Situation and it will usually not occur.",
                    "label": 0
                },
                {
                    "sent": "Usually you'll have to use.",
                    "label": 0
                },
                {
                    "sent": "The optimizer gets an answer.",
                    "label": 0
                },
                {
                    "sent": "But in some cases you can just completely analytically right out the expression and model.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's nice.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "And this is Zach.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We caught.",
                    "label": 0
                },
                {
                    "sent": "I mean, sometimes this is one of the reasons that people introduce exponential family distributions.",
                    "label": 0
                },
                {
                    "sent": "Namely, maximum entropy estimation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, really, really.",
                    "label": 0
                },
                {
                    "sent": "Loosely speaking, entropy is basically the number of bits that you need to encode a random variable.",
                    "label": 1
                },
                {
                    "sent": "And well, we can define it as the expected value of the negative log of P of X.",
                    "label": 0
                },
                {
                    "sent": "And we just said zero log zero X 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that the density P of X.",
                    "label": 1
                },
                {
                    "sent": "Which satisfies the condition that expected value for fixes create equal Anita with maximum entropy.",
                    "label": 0
                },
                {
                    "sent": "Is of this form?",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "Then the theater of G of Theta equals eaten.",
                    "label": 0
                },
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "Who has seen this fact before?",
                    "label": 0
                },
                {
                    "sent": "OK. For five people.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, who wants to see the proof?",
                    "label": 0
                },
                {
                    "sent": "OK, so basically in the break the five minutes break when other people can go to the bathroom, you will get to see the proof.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 1
                },
                {
                    "sent": "One of the corollaries is at the maximum entropy distribution with a given variance.",
                    "label": 0
                },
                {
                    "sent": "Is the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, why?",
                    "label": 0
                },
                {
                    "sent": "Because the.",
                    "label": 0
                },
                {
                    "sent": "Now I could go through this entire expect special like what we have for the binomial distribution, and then we derive correct.",
                    "label": 0
                },
                {
                    "sent": "Just tell me the answer.",
                    "label": 0
                },
                {
                    "sent": "South.",
                    "label": 0
                },
                {
                    "sent": "So I will get a distribution which looks like P of X is proportional to equals.",
                    "label": 1
                },
                {
                    "sent": "E to the sum parameter Theta times X ^2.",
                    "label": 0
                },
                {
                    "sent": "Minus G updater.",
                    "label": 0
                },
                {
                    "sent": "Because this is the same guy here.",
                    "label": 0
                },
                {
                    "sent": "So now I have a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "If I want to play around with the mean two, I might want to say something like the expected value of X is created equal, then say maybe 2.",
                    "label": 0
                },
                {
                    "sent": "So I would get Theta two X squared plus Theta 1 * X.",
                    "label": 0
                },
                {
                    "sent": "And off we go again.",
                    "label": 0
                },
                {
                    "sent": "Or if I just want to play around with the mean so.",
                    "label": 0
                },
                {
                    "sent": "Just required that the expected value X is greater equal then say two.",
                    "label": 0
                },
                {
                    "sent": "I would get P of X being each other.",
                    "label": 0
                },
                {
                    "sent": "Theta Times X -- G of Theta.",
                    "label": 0
                },
                {
                    "sent": "I could then say, well, yeah, that should be the case and.",
                    "label": 0
                },
                {
                    "sent": "X to be in the domain of integers.",
                    "label": 0
                },
                {
                    "sent": "Well then I will get a different function G, But that's the only thing that would change.",
                    "label": 0
                },
                {
                    "sent": "So I have a fairly powerful mechanism.",
                    "label": 0
                },
                {
                    "sent": "How to turn conditions on expectations of random variables?",
                    "label": 0
                },
                {
                    "sent": "Into distributions.",
                    "label": 0
                },
                {
                    "sent": "And the entire conference is on.",
                    "label": 0
                },
                {
                    "sent": "Well, how to do this efficiently?",
                    "label": 0
                },
                {
                    "sent": "So conference called Max int.",
                    "label": 0
                },
                {
                    "sent": "Now in fact.",
                    "label": 0
                },
                {
                    "sent": "As it will turn out, this is not topic of this talk, but it's quite.",
                    "label": 0
                },
                {
                    "sent": "An amazing thing that.",
                    "label": 0
                },
                {
                    "sent": "Basically you asked me out and I prove.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't just know.",
                    "label": 0
                },
                {
                    "sent": "Exactly that, the expected value of some form of X.",
                    "label": 0
                },
                {
                    "sent": "Not equals mu.",
                    "label": 0
                },
                {
                    "sent": "So some mean.",
                    "label": 0
                },
                {
                    "sent": "But it's just minus mu at this norm.",
                    "label": 0
                },
                {
                    "sent": "Is less equal then some epsilon.",
                    "label": 0
                },
                {
                    "sent": "And you want to find the maximum entropy distribution of that.",
                    "label": 0
                },
                {
                    "sent": "Then you will get an estimate.",
                    "label": 0
                },
                {
                    "sent": "Which will not be a maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "But a maximum posterior estimate.",
                    "label": 0
                },
                {
                    "sent": "Basically, you get Bayesian estimation out of this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's all I wanted to say about that specific thing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That sack.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I put it to practical use.",
                    "label": 0
                },
                {
                    "sent": "I mean, these are all really nice things to know in order to know that, well, yeah, the world is all sane and all the estimation algorithms lead to something quite similar.",
                    "label": 0
                },
                {
                    "sent": "But now let's actually put it to practical use.",
                    "label": 0
                },
                {
                    "sent": "So let's say we observe some data X1 through exam.",
                    "label": 0
                },
                {
                    "sent": "Which are drawn from some distribution P of X parameterized by Theta.",
                    "label": 0
                },
                {
                    "sent": "Well, I could go and compute the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So get PF X parameterized by Theta.",
                    "label": 0
                },
                {
                    "sent": "Which is a big product of roll.",
                    "label": 0
                },
                {
                    "sent": "My inbox ovations of.",
                    "label": 0
                },
                {
                    "sent": "The individual probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Fat this is always the case if all these eyes are drawn in dependently.",
                    "label": 0
                },
                {
                    "sent": "If they were to depend on each other, I couldn't just write the joint probability as the product of the individual probabilities, but.",
                    "label": 0
                },
                {
                    "sent": "Since they're all drawn independently, as I assume that's the case.",
                    "label": 0
                },
                {
                    "sent": "OK. Now we know.",
                    "label": 0
                },
                {
                    "sent": "That well, this is an exponential, so I can push this product inside.",
                    "label": 0
                },
                {
                    "sent": "And well, what I get is.",
                    "label": 0
                },
                {
                    "sent": "Each other and then the sum of the falx size in here.",
                    "label": 0
                },
                {
                    "sent": "And I get M * 3 of data.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Product of in terms of this expression, here is just M * G of data.",
                    "label": 0
                },
                {
                    "sent": "OK. Now let me put those intermediate calculations up on the whiteboard.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "PFX parameterized by Theatre.",
                    "label": 0
                },
                {
                    "sent": "Is if I push this product inside?",
                    "label": 0
                },
                {
                    "sent": "Is each other?",
                    "label": 0
                },
                {
                    "sent": "Son, I going from one to M. Find of XI.",
                    "label": 0
                },
                {
                    "sent": "In a product with data.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "M Times GF later.",
                    "label": 0
                },
                {
                    "sent": "OK everybody happy with this situation?",
                    "label": 0
                },
                {
                    "sent": "Who's unhappy?",
                    "label": 0
                },
                {
                    "sent": "Nobody good now if I want to find that Theta which is the most likely Theta, explaining the data.",
                    "label": 0
                },
                {
                    "sent": "So explain X.",
                    "label": 0
                },
                {
                    "sent": "Well, I would go and compute.",
                    "label": 0
                },
                {
                    "sent": "Arc Max.",
                    "label": 0
                },
                {
                    "sent": "Over theater of payoff data so.",
                    "label": 0
                },
                {
                    "sent": "PFX parameterized by Theta.",
                    "label": 0
                },
                {
                    "sent": "Which is of course the same thing as the arcmin.",
                    "label": 0
                },
                {
                    "sent": "Over Theta of the negative log likelihood.",
                    "label": 1
                },
                {
                    "sent": "The reason for taking this term is that while people usually don't like taking products, they prefer taking sums.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Algebraic convenience.",
                    "label": 0
                },
                {
                    "sent": "OK. Now that expression here.",
                    "label": 0
                },
                {
                    "sent": "I can write down immediately as.",
                    "label": 0
                },
                {
                    "sent": "M * G of data.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "The inner product between some over I going from one to M. Phi of XI.",
                    "label": 0
                },
                {
                    "sent": "With data.",
                    "label": 0
                },
                {
                    "sent": "OK. Just taking this out here.",
                    "label": 0
                },
                {
                    "sent": "I could also.",
                    "label": 0
                },
                {
                    "sent": "Bracket out the M. So get the one over him in here.",
                    "label": 0
                },
                {
                    "sent": "Ah, now this is interesting.",
                    "label": 0
                },
                {
                    "sent": "Becaused what's happening is that this expression in here.",
                    "label": 0
                },
                {
                    "sent": "It's basically an empirical average over my file fix.",
                    "label": 0
                },
                {
                    "sent": "OK. And in fact, this is the only thing I really need to remember about my data.",
                    "label": 0
                },
                {
                    "sent": "I don't need to remember the individual excise, just we need to remember this average.",
                    "label": 0
                },
                {
                    "sent": "This may be hard to compute, but that's the only thing I need to remember.",
                    "label": 0
                },
                {
                    "sent": "That's fully equivalent to what I had before.",
                    "label": 0
                },
                {
                    "sent": "That's why this is called the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "OK. And now what I'm going to do is, I'm going to say well now argument of Theta.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, well how do we get the minimum?",
                    "label": 0
                },
                {
                    "sent": "Well, we take the first relative set it to 0.",
                    "label": 0
                },
                {
                    "sent": "And this is now a convex function.",
                    "label": 0
                },
                {
                    "sent": "With these are convex function that's linear.",
                    "label": 0
                },
                {
                    "sent": "So I get that these data just drop the M because it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "Equals mu hat.",
                    "label": 0
                },
                {
                    "sent": "Now we know.",
                    "label": 0
                },
                {
                    "sent": "That this expression here by the Cumulant generating property was also the expected value of firefix.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what I'm doing, in a way, is I'm saying I want to find the distribution.",
                    "label": 0
                },
                {
                    "sent": "For which the empirical mean equals the actual mean.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "3 foot one.",
                    "label": 0
                },
                {
                    "sent": "Slide back.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "So if I had a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "I would just observe my empirically mean empirical variance.",
                    "label": 0
                },
                {
                    "sent": "And then I will construct a normal distribution for which the empirically mean and empirical variance will fit through mean and variance.",
                    "label": 0
                },
                {
                    "sent": "Now obviously nobody of us here believes that the empirically mean is always spot on.",
                    "label": 0
                },
                {
                    "sent": "And This is why I said before.",
                    "label": 0
                },
                {
                    "sent": "If we find an estimator for which well, the true mean difference by no more than epsilon, and this epsilon is chosen chosen suitably.",
                    "label": 0
                },
                {
                    "sent": "I will actually get an estimator which is quite powerful in practice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But the good news is that this expression here.",
                    "label": 0
                },
                {
                    "sent": "Can be solved.",
                    "label": 1
                },
                {
                    "sent": "Well, OK, if I cannot solve it analytically, I can solve it easily by Newton's method.",
                    "label": 0
                },
                {
                    "sent": "So it's something that you can do quite efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's look at an application.",
                    "label": 0
                },
                {
                    "sent": "So just create events.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I've got some random variables from tossing a dice 6 possible outcomes.",
                    "label": 1
                },
                {
                    "sent": "These would be the counts, maybe imperfectly.",
                    "label": 1
                },
                {
                    "sent": "And these are the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now the maximum.",
                    "label": 0
                },
                {
                    "sent": "Now this is something you would do intuitively anyway.",
                    "label": 0
                },
                {
                    "sent": "If I tell you.",
                    "label": 0
                },
                {
                    "sent": "You have the actual counts and I would ask.",
                    "label": 0
                },
                {
                    "sent": "You will tell me the probabilities, then even improbably.",
                    "label": 0
                },
                {
                    "sent": "Yeah, high school you would say, well OK, therefore the probability of seeing a two would be .3.",
                    "label": 0
                },
                {
                    "sent": "So what you've done is, without knowing all this machinery, you've done a maximum likelihood estimation.",
                    "label": 0
                },
                {
                    "sent": "But what you can also see is that this is somewhat fraught with problems.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, if we toss a dice and we get those numbers, well, you would say that's quite normal.",
                    "label": 0
                },
                {
                    "sent": "But he would also say, well, it doesn't mean that the probability of seeing a four is actually .04.",
                    "label": 0
                },
                {
                    "sent": "It just happens that you've been a bit unlucky that time because you didn't toss the dice often enough.",
                    "label": 0
                },
                {
                    "sent": "So if you repeat it a million times times and you still see really small County would probably think that the dice is broken.",
                    "label": 0
                },
                {
                    "sent": "But if you just toss it like 20 times or so, there's no reason for you to believe that.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do we have to do?",
                    "label": 0
                },
                {
                    "sent": "We have to.",
                    "label": 0
                },
                {
                    "sent": "In a way, smooth out those numbers we need.",
                    "label": 0
                },
                {
                    "sent": "I mean, you would immediately feel like you'd want to make them closer to uniform here, because your prior assumption on the dice is that ISIS well behaved.",
                    "label": 0
                },
                {
                    "sent": "The dice has all the events evenly distributed uniformly.",
                    "label": 0
                },
                {
                    "sent": "And things get even worse if we have continuous random variables.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So This is why at some point we'll need to introduce a prior.",
                    "label": 0
                },
                {
                    "sent": "Just to show you again, little bit what happens, let's pertain to.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1500 observations and you can see well it smooths out more closely to point to 1 / 6.",
                    "label": 0
                },
                {
                    "sent": "And this is not a real dice.",
                    "label": 0
                },
                {
                    "sent": "I didn't sit down.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, I guess probably a good time to have a break just for a moment.",
                    "label": 0
                }
            ]
        }
    }
}