{
    "id": "44bosxm5p5hpqnx7ig4bbo4ckjflheoz",
    "title": "Introduction to Theano",
    "info": {
        "author": [
            "Pascal Lamblin, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_lamblin_theano/",
    "segmentation": [
        [
            "But I'm going to present is mostly an introduction to TNO what it is, what you can do with it.",
            "And this is work with lots of people like Miller.",
            "Previously, Lisa in particular.",
            "Credit investor.",
            "And so this is the first."
        ],
        [
            "But the first part, so today is going to be just the theoretical part, so I'm going to present you slides and a couple of code snippets and so on.",
            "Tomorrow there's going to be practical session at the same time it's going to be split between Theano and Torch.",
            "One room is going to be faulty.",
            "I know another one for torch, I don't remember which one is going to be where, but it's going to be hands-on exercises.",
            "Where you access machine with the GPU in the cloud through an ipython or I torch notebook.",
            "So today you don't need a laptop to follow anything, even though the code snippets are going to be available online.",
            "If you want to go back later and play with them a little bit, but for tomorrow you should bring a laptop an you just need the browser.",
            "You don't need the local installation because it's going to be hosted on the cloud.",
            "So all the material is online on GitHub.",
            "If you want to come back later."
        ],
        [
            "So first of all, I'm going to give you an overview as a quick overview of Theano.",
            "Then explain more in detail the graph definition, the graph structure that TNO is using.",
            "Then, which graph transformations and manipulation you can do with piano.",
            "Then I'm going to present what optimizations exist and how we make runtime fast, and then we're going to talk quickly about more advanced topics.",
            "And if you have questions about these advanced topics.",
            "Hands-on session of Tomorrow would be a good place to look for more in depth answers and existing code examples demonstrating those features."
        ],
        [
            "So.",
            "To begin, Theano is a compiler of mathematical expressions and what that means is that it makes it possible to define mathematical expressions using an easy syntax which is really close to the syntax of NUM PY.",
            "So if you know Python, if you've already used NUM PY, then it's not that difficult to adapt, then it makes it possible to.",
            "Change those expressions, manipulate them to substitute sub expression for others to derive gradients automatically.",
            "And by that I mean derive mathematical expression of gradients that you can then further manipulate and then introducing optimizations for performance for numerical stability and so on.",
            "And then.",
            "It generates code optimized codes or uses already optimized backends like optimized blast like CUDA, an also custom C code so that the execution as well is fast.",
            "And then I'm going to talk a little bit about the diagnostic tools and.",
            "Checks and so on that are built in Indiana."
        ],
        [
            "So do you know is currently?",
            "Metro project it started as a student project eight years ago, but now it's quite stable.",
            "An is being used all over the world in universities, small companies, large companies, it's driven.",
            "Lots of research papers.",
            "We have contributors for from all over the world and.",
            "Yeah so.",
            "You can find well the official documentation online and as well as tutorials about deep learning for."
        ],
        [
            "Piano.",
            "No is also the basic the base for different.",
            "Software projects, most of them are machine learning libraries, but.",
            "Not only for instance, by MC Three is actually probabilistic programming programming software using using Theano for automatic gradient starvation and fast execution.",
            "There's Splatoon, which is a framework built on top of piano that enables training the same model on multiple GPU's on the same machine, exploiting data.",
            "Realism, there's no MPI that exploits model parallelism to train on multiple machines, and there are many more."
        ],
        [
            "So now let me walk you briefly through how.",
            "You typically would use Theano, so the defines a language, a compiler and library at the same time, so the language close to NUM Py makes it possible to define symbolic mathematical expression.",
            "That you then compile as a Python callable.",
            "That will be able to compute values output values given input values.",
            "And then you can execute that function that callable on whatever data you have."
        ],
        [
            "So to define an expression, you start from symbolic inputs.",
            "The inputs are strongly typed, so you need to define if it's like vector matrix and so on.",
            "By default they're Floating Points, which can also define integer values, and so on, and then from those symbolic inputs that don't hold any any value.",
            "Yet you can define abstract operations well.",
            "For instance, in that case, a DOT product addition, sigmoid, sigmoid and so on.",
            "So one bed."
        ],
        [
            "You can visualize what it means to do that, so we have a couple of debugging function to help visualize graphs.",
            "Here we have debug print that prints the structure of the graph as a tree.",
            "So if you look at the dot variable is well, it's the output of a dot operation on variables named X&W.",
            "If you see what the output is, then you see it's sigmoid of Alan Wise addition.",
            "And that addition is of two elements, the dots that we had earlier, and the bias that's here."
        ],
        [
            "Then once we have that expression, we can compile it into a Python callable.",
            "So to do that you called piano function and you specify the inputs and the output.",
            "So basically you have that whole computation graph and you say I want a function that given these variables, computes that one or these ones.",
            "So for instance, if he has two outputs, only one input, which is an intermediate variable.",
            "Or you can also go all the way through the final variable, define new ones up with more than one variable, and so on."
        ],
        [
            "Here again, we can visualize the graph.",
            "After the compilation, we actually call it debug print again on the compiled objects.",
            "And here we see it's not exactly the same as before.",
            "We have, like Jenn Viane Alok and those kind of things.",
            "And this is because what happened when we call Tiano function is that the competition graph that we define all the part of the computation graph that we actually care about in each of the function has been written in parts or optimized.",
            "For instance, instead of calling the non Pi dot product.",
            "We since we accumulate on top of the bias, we can use the blast function generally, so which is the generalized matrix vector product?",
            "We have, like transposition, that happens in place instead of copying the whole.",
            "The whole weight matrix, doing the transposition, and so on, so.",
            "This is the."
        ],
        [
            "Nothing that happens.",
            "And if we visualize it using another tool, we can see well in green like the inputs here.",
            "Take awhile, transposition and so on.",
            "We have the dot product here and the output in dark blue."
        ],
        [
            "Here, so here we have only the output of the dot product.",
            "Here we have the output after the sigmoid and we see that here if it's after the sigmoid sigmoid can work in place and so This is why you have a right arrow here."
        ],
        [
            "You can visualize here.",
            "You have two outputs.",
            "This is an intermediate result output of the dot product, which is also an output of the function, which is why it's called Blue, an not overwritten by the sigmoid and so."
        ],
        [
            "We have a new tool for visualization that's called.",
            "This revisits been integrated in piano last summer.",
            "An IT outputs an HTML page where you can you have JavaScript and so on where you can interact with those."
        ],
        [
            "Kind of nodes and you can move them around and inspect them more easily than than just that."
        ],
        [
            "Ann, once yeah, sorry.",
            "OK."
        ],
        [
            "So the color coding that is here, I think is explained on the online documentation.",
            "If it's not, please open the ticket on GitHub and we'll try to correct that."
        ],
        [
            "Yeah, so once you have that compiled object then you can provide numerical values for all the inputs that it expects.",
            "For instance, here we sample a couple of random values an if we call F which only computed the dot product we have it, it's after the sigmoid.",
            "You have that.",
            "If you compute more than one at the same time while you have.",
            "The right values."
        ],
        [
            "So now we're going to dig a little bit deeper in what actually happens during that time and how does it work, and motherhood, which kind of data structure we're using, and so on, so."
        ],
        [
            "First of all, I've talked about the computation graph, which represents the mathematical expression or expressions that we want to use so.",
            "How it's done internally is that we have a graph that has two kinds of nodes.",
            "One kind of those we called variable nodes or variables and they represent value.",
            "They are going to have to hold values where we compute when we actually execute the function.",
            "And we have apply nodes that represent mathematical operations being applied on these values, and so in practice, for instance, the input and output of the graph, those are variables they will hold.",
            "You will provide values for them for the inputs and you will.",
            "Get output values as a return.",
            "An for intermediate variables, well, they will hold the intermediate values during execution and the apply nodes can have.",
            "Several inputs, for instance, the addition will have like an arbitrary number of inputs and can have one or more outputs.",
            "For instance, we have like 1 operation to compute the Max and argmax of 1 tensor, where it's going to be only one operation.",
            "But you'll have two output variables that will be computed at the same time.",
            "An intermediate variable can be used by several.",
            "By several operations next, but one variable can only be the output of 1.",
            "Apply nodes, or none, or none at all.",
            "If it's an input variable."
        ],
        [
            "So here is how it works.",
            "So you have for instance in that case.",
            "Addition of two matrices.",
            "So this is an apply node that has two inputs, two matrices X&Y.",
            "It's of the type.",
            "I mean it has an op that is the addition.",
            "And it has one output which is also a matrix."
        ],
        [
            "If so, calling Python prints again the same tool we introduced before, but with compact equals false.",
            "It shows all the nodes, all the variable nodes and the apply nodes and the variable nodes are those square boxes and the apply nodes are these Oval boxes.",
            "And so here, unlike the previous previous figure, you see all the intermediate variables like.",
            "You see that for instance, here we take their shape.",
            "Of the weights and it's going to be used to allocate some output of the right size and so on.",
            "And here you can see all these intermediate variables that are not named that we usually don't interact with, but you see, this is the whole.",
            "This is the whole graph.",
            "Whereas if you see earlier.",
            "They're not here, here, and here.",
            "So."
        ],
        [
            "Ann, those variables have a strong type, meaning that unlike in Python, where you can have the same variable being.",
            "Like any type, in order to handle needs to know what the type of the input variables is and that way you can infer types for the intermediate variables and the outputs and so on.",
            "And everything has to be consistent.",
            "It also helps debugging things.",
            "It also helps avoiding to suddenly have an increase in precision that makes everything slow.",
            "So what is so we have a different?",
            "Different types and the most used or tensor types for NUM PY arrays on the main host or CPU memory and GPU array for arrays on the GPU using CUDA.",
            "We also have a sparse type which is not as well supported as we would like.",
            "And basically what is in the type is the number of dimensions.",
            "So you have to know if something is like a scalar or vector or matrix and so on in advance the data type is.",
            "It's like float, double integer of whatever precision and so on.",
            "And the road festival pattern.",
            "And we're going to come back to the broadcastable pattern later.",
            "But basically it means knowing which shapes I mean on which dimension the shape is always going to be exactly 1.",
            "Or not one.",
            "The shape itself is not part of the type, so that means that you can have an input you declare it's a matrix.",
            "It's not a problem at all if during different calls of the same compiled function you pass it matrices of different shapes.",
            "For instance, if you have a batch size that varies, it's usually not an issue.",
            "And the memory layout is not part of the type either, so you can have like a matrix, A transpose matrix, slices and so on.",
            "And it's not going to be an issue."
        ],
        [
            "So broadcasting is it using NUM PY and it's used in other languages as well.",
            "It's the implicit replication of a tensor along an axis that exists, but that's of length one.",
            "So for instance, if you have, say, a row vector.",
            "And you add it to a matrix.",
            "Then what you expect is that that vector is going to be replicated.",
            "End times, and then this implicitly and then this is going to be added to the matrix if you have a column vector, then you expect it to be replicated.",
            "On the columns and added them.",
            "But if you have like a two by two matrix and the two by four matrix, you don't expect that two by two matrix to be replicated.",
            "You expect the dimension error or size mismatch.",
            "Something like that.",
            "So Theano needs to know in advance whether it will have to replicate the tensor implicitly or not.",
            "And This is why you have that broadcastable pattern.",
            "That's part of the type.",
            "So for instance, here you can define like a row, which is a one by North 2D matrix and a column that's an end by 1 to the denser.",
            "And if you add them then both are going to be replicated.",
            "That way, if you have a vector, it's going to be treated as a row in that case, and broadcastable dimensions are always added implicitly to the left.",
            "This this part is exactly like NUM Py."
        ],
        [
            "Another difference between piano and NUM py.",
            "Is.",
            "That we cannot override everything from Python, so that means we have a couple of limitations.",
            "If you do the like tiano since it has a competition graph that needs to be done in advance.",
            "It cannot.",
            "You cannot have side effects that happen when you create new expression or when you build expression.",
            "The only thing you can do in Theano, all the expressions.",
            "What they do is that they add new nodes to the existing competition graph.",
            "They build them with, add inputs, existing variables, an that gets added to the computation graph you're working on, so you can do things like if you have a variable A, you can do things like.",
            "A + = 1 because Python will just interpret that as redefined a as a plus one and you still have the old a value that exists somewhere in the graph, and if it was the input to other construction.",
            "Other expressions, then those expressions are still going to use the old A and the new ones that you build are going to use the new A.",
            "However, you cannot use the syntax to assign things to slices of arrays.",
            "So it's because you cannot return a new object.",
            "Python requires that you return the same object and so you have to use things like ink, sub tensor and sets of tensor, which creates a new variable and then you would have to use that new variable to in your new expression.",
            "There are a couple of specific OPS or the specific operations that have side effects like print assert.",
            "There's also a PDB breakpoint, one an how that works is that it's.",
            "Inodes that exists in the computation graph, but that does not just copies over its inputs to its output, so it's not computing anything, but it has to be somewhere in the graph an if the output is used then.",
            "Then the node is going to be executed and the print statement is going to appear or the assertion is going to be checked and so on.",
            "However, if you do not reuse.",
            "Of that, or if that output is not actually part of the computation that you want to execute or not.",
            "Part of the sub graph that you're that you're compiling a function of, then it's going to be pruned, and it's not going to be executed."
        ],
        [
            "Also, we cannot really redefine Python keywords like if an for an Lang and so on.",
            "So we have the other constructs for that.",
            "If you do, if some variable, then Python is going to cast that into a Boolean an.",
            "It's not like a symbolic variable that represents the condition that the value of that variable evaluates to true or something.",
            "So you have to do something like if else.",
            "Or switch like in NUM py?",
            "If you do.",
            "A for loop in Python.",
            "It will just execute the four loop once when you build the graph, so sometimes that's what you want to do.",
            "If you have something to execute like 3 times, maybe you want to have nodes in the computation graph that compute the expression one time and then the second time, and then the third time.",
            "This is basically unrolling the loop.",
            "And sometimes that's what you want, and sometimes it's not really appropriate, because if you don't know like how many elements there will be in advance, then it will not work.",
            "If the number of nodes that you create is too big, then it's going to create further complications during the optimization graph optimization phase.",
            "There's Daniel scan.",
            "Basically, encapsulates the whole loop in only one node that I'm going to talk about that later.",
            "The length function in Python only returns Python integers.",
            "Or maybe long, but so you cannot use it to get the symbolic shape available and the print statement.",
            "Well, it's going to be executed once.",
            "Ocean.",
            "Internally, if BAR evaluates data true, there's expression to still get devaluated.",
            "In this example.",
            "OK, so the question is whether eyfells executive expression one an expression to only One South.",
            "We support lazy evaluation in some in some parts, or what's going to happen in that case is that it's going to evaluate the condition first.",
            "And then, depending on the evaluation of that, the runtime will ask for devaluation, either of expression one or expression two there is.",
            "I mean there are a couple of issues that we're aware of, and in some cases devaluation of the other one is forced.",
            "It happens when you have in place computation and basically the runtime.",
            "For instance, something in expression two will overwrite an intermediate.",
            "Value or used in expression one.",
            "Then it will try to evaluate it first.",
            "To prevent some values from being overwritten and currently it's an issue we are aware of and will try to find some time to fix that, but in general it works fine and it only executes the branch that you that you need."
        ],
        [
            "So once you build that graph.",
            "You can, I mean there are functions too.",
            "To modify it or to.",
            "I mean it's it's only a starting point, so there are different things you can do.",
            "You can do substitution.",
            "You can clone part of the graph.",
            "You can rebuild part of the graph.",
            "You can take gradients, and then I'm going to talk about shared variables for optimization."
        ],
        [
            "So the first thing you can do.",
            "When you compile, a function is in addition to the inputs and outputs of the callable that you that you have.",
            "You can also provide substitutions.",
            "So for instance, if you define a graph that.",
            "I know that computes some.",
            "Well, the same function that we had earlier, like the dot product and sigmoid that bias and so on.",
            "But you actually want to apply that.",
            "Both maybe 2X and 2A normalized version of X.",
            "Then you can create another another expression from a new input X_ and then say, well, this is my normalized X.",
            "And then instead of feeding X to my expression here, I want to feed the normalized X.",
            "And so you can do that with Givens.",
            "You just say, well.",
            "Let's compute dots, but instead of computing it from X computed from XN an.",
            "Now since you need since you need the X_as a starting point, well, use X_as the input instead of X.",
            "And then you can call FN the same way."
        ],
        [
            "Another way of achieving pretty much the same thing.",
            "Is to create to clone part of the graph.",
            "So you have a new dot and then you output variables and but where X was replaced by the normalized version of X.",
            "And if you do that, the advantage of cloning that way is that you can do it over and over if you have several variable substitutions to make.",
            "If you want to substitute something 1st and then you have your valuables in there that you want to do another substitution, or it's a bit more flexible.",
            "But it creates a whole new sub graph.",
            "And it used the same results."
        ],
        [
            "Something else that's really useful and one of the reason TNO was developed in the 1st place is to be able to derive gradients automatically from arbitrary expressions.",
            "So.",
            "The way it works, so the is that we use the backpropagation algorithm.",
            "And so if you have, for instance, a function compound function from RN to R with only like a single scalar as an output, that's the composition of like F&G for instance.",
            "Then you can express the gradients so the partial derivative of serial respect to X at X.",
            "As the DOT product between two Jacobian matrices, which represent the whole.",
            "The whole partial derivative of each element of the output with respect to each element of the inputs.",
            "So here you have.",
            "Well, the Jacobian for the output, which is basically just just a vector, because this is a function, I mean this one is a function from a vector to a scalar, but this one is a huge matrix.",
            "Fortunately, we don't need to express that matrix because it would take a huge amount of memory, and usually it's structured in some way that we can take advantage of what we want.",
            "We don't want that whole.",
            "Gradient that whole Jacobian matrix.",
            "What we want is an expression that for every vector enables us to compute the dot products between that vector and the Jacobian.",
            "And that's that's the backpropagation algorithm.",
            "And this is what we."
        ],
        [
            "Two in piano in piano.",
            "Each basic operation like the addition that product the Sigma made.",
            "Everything that is defined or almost.",
            "Also defines a symbolic expression.",
            "For that product.",
            "Given this symbolic vector that represents the output gradients.",
            "So how do you use that?",
            "Well, in the earlier case that we had, so the simple dot product sigmoid and so on, let's define a vector of targets.",
            "And a scalar cost, that's just.",
            "The design of the squared error.",
            "Then we can use the high level function tiano dot grants you provide it with the costs and the variable you want to take the gradient with respect to.",
            "And what will happen is that it will start from the output of the graph, which is the cost and go back an each time calling.",
            "The quivalent of that operation.",
            "On the symbolic, granite starts from one because the gradient of the code was back to, the cost is 1, and from there it back propagates.",
            "Building knew expressions that represent the gradient with respect to intermediate variables, and if you have an intermediate variable that has two clients that that's being used for two different operations, then the gradients are going to be summed and so.",
            "If you have non differentiable operations, well it depends on the case.",
            "And the important thing is that.",
            "And it goes all the way through the variables that you define the with respect to.",
            "Hear WNBA.",
            "And the important thing is that it creates actually a symbolic mathematical expression of the gradients.",
            "So this is still part of the computation graph.",
            "You don't, and you build that in advance.",
            "You have no numerical values yet."
        ],
        [
            "And then one you have these expressions for the gradients.",
            "Then you'd want to use them.",
            "So how you do that?",
            "Well, as we as we saw earlier at piano function.",
            "Can have more than one output, so in that case we can compile at the end of function that takes all the necessary inputs and then computes the cost and the gradients with respect to WND.",
            "If we gave random values for the targets.",
            "And then we can.",
            "Execute that and print the cause.",
            "I don't have the values here, but it's just random values for the moment.",
            "And since these expressions are symbolic, we can continue building on new expressions as part of the computation graph.",
            "For instance, if you want to do gradient descent, then you might want to compute an update and update expression from these gradients.",
            "So, for instance, you know you can multiply that by a fixed learning rate and then compute the update expression and say, well, here's symbolic expressions for mine, UW, and for my newbie.",
            "And then you can compile a channel function that outputs that instead."
        ],
        [
            "And here is how it looks.",
            "So you have lots of where you have the usual inputs plus constants.",
            "Some computation and then.",
            "The date expressions for W 4 W&B and the costs."
        ],
        [
            "So if you want to update values, you can do it.",
            "I mean this gives you like two different ways of updating values.",
            "One thing you can do is output the gradients.",
            "So called the cost and graph function that we defined."
        ],
        [
            "Here, that's what cost in Grad, so it outputs The CW and DC DB."
        ],
        [
            "And you call that on the values and then you perform gradient descent in the regular Numpy.",
            "You take this output value and multiply it by the running rate and perform the outputs.",
            "Sometimes that's what you want to do if you want to use some.",
            "Some complex optimization techniques like FGS, all those kind of things rather than code that in TNO you just have a function that computes the gradients and then plug that into the Scipy optimize routine.",
            "Another option is to use Wella Coast, an update and that outputs directly the new value of W&B, but it's not really efficient and it can be cumbersome to have to do that by hand and keep track of that.",
            "An always handle explicitly these updated values on top of that.",
            "The input and output variables you define are usually defined as CPU arrays.",
            "I mean arrays in CPU memory because that's usually what you handle.",
            "An if you only need these arrays to hold the parameters of your models and update them and not really interact with them, then it will lead to useless transfers between CPU and GPU memory."
        ],
        [
            "So to avoid that an.",
            "We introduce shared variables so shared variables.",
            "They are symbolic variables first of all, but they have a persistent value associated to them.",
            "It's persistent, but it's mutable, so it's persistent across different function calls, and it's also shared between different compiled handle functions.",
            "So for instance, if you have the parameters of your models in in shared variables, then the function for training and for evaluating the performance of your model will use the same value.",
            "You don't need to do anything special, it will just use the value for that variable.",
            "An it has to be an input variable.",
            "Because it has already a value, it's not the output of some computation, so it needs to be an input variable.",
            "An it's going to be an implicit input to every function that needs it.",
            "So that means you don't have to provide it explicitly as an input variable to your training or validation function.",
            "Each time that you are using an expression that depends on the shared variable, it's going to be added as an implicit inputs an.",
            "Its value is going to be retrieved each time you call the function itself."
        ],
        [
            "So here's an example of how you might want to use it.",
            "So still the same basic computation you have X&Y, which represent your data, so you don't want them as a shared variable, usually because it's going to change each time you call the function.",
            "But that time we define WNB as shared variable.",
            "And since the shared variable has a value associated to it, we need to create it with an existing value.",
            "So here we start from the default value that we sample.",
            "Last time like just random weights and zero bias.",
            "And then you build your computation graph exactly the same way as as before.",
            "We created a product output and so on.",
            "And when it's done, when you create the function here, you see that we don't have to provide WNBA anymore.",
            "I mean W here and the value and B here because their implicit inputs.",
            "And if you call F on only exe file Angiomax while you get the same values as before.",
            "If you need to change the value of a shared variable in between calls to.",
            "To compile channel functions, you can call get value and set value.",
            "Shared variables do not need to be of fixed shape either, so you can have calls to get values and set value that add rows to a matrix or something like that."
        ],
        [
            "So if you want to update shared variables, well, one of the ways is to use get value into use getvalue and setvalue.",
            "But it doesn't really bring anything more than your way of handling those values explicitly.",
            "But what we can have is a function that also computes update value or new value to update shared variables with.",
            "So for instance, again still doing gradient descent on the same function will compute the costs, compute tiano grad of C with respect to W&B.",
            "By the way, it's more efficient to have only one call to Theano grad, because it will.",
            "Do only one traversal of the graph nodes, like one for the value and then one for B.",
            "And you compute the update values.",
            "The symbolic expression for the updates.",
            "And then call channel function with inputs, outputs an updates, so the updates are pairs of shared variables and symbolic expression for the updates.",
            "So here we have W an here.",
            "This amounts to W minus learning rates, times, gradients and same for me.",
            "So again, W. NBR, implicit inputs, and in that case update W. An update B.",
            "Are treated as implicit outputs of the Theano function, so they will not be returned when you call.",
            "Here cost and performance updates in perform updates, but they will be computed.",
            "At the same time as C. And then after everything, every output is computed.",
            "Then the update is going to be performed and the value of the shared variable is actually going to change.",
            "So first we compute all the outputs and then we perform the updates."
        ],
        [
            "An if we visualize the graph, this is how it looks like.",
            "So you have in pale blue the shared variables.",
            "Which are treated are kind of like inputs because their inputs to like this operation and this operation and so on.",
            "And here we have the output.",
            "That the function computes the cost itself, and here the update expression for the bias and for the wait.",
            "And you can see that.",
            "I mean you cannot really read it here, I guess, but here is written updates.",
            "So it means that, well, this is really like the update expression for that and the blue dark blue arrow here shows that this operation is a view and the right one shows that this is an inplace operation.",
            "And it's really nice because it means that this operation.",
            "Happens in place on one of its inputs, which is this one, and that means that we have our shared variable in GPU memory, for instance, and the update expression.",
            "When it's computed, overwrites the memory in place and so you don't have a copy of memory.",
            "You don't have the duplicate amount of memory usage and so on, and you can see that in that case it's pretty efficient."
        ],
        [
            "So now I'm going to talk about the different features that we have that enabled fast runtime performance for 40 and compile."
        ],
        [
            "The functions.",
            "So.",
            "Part of it comes from graph optimizations.",
            "And I've talked briefly about it at the beginning when I showed you the updated expression with the C generally and so on.",
            "But basically we have a graph rewriting framework in piano an that takes the graph that you created.",
            "And then perform some replacements in it.",
            "That still have the same meaning that still correspond to the same mathematical expression.",
            "And so in particular, the values should not change or not match.",
            "And the type has to be the same and at runtime the shape has to be the same and so.",
            "So we have different goals for optimization.",
            "Some of them merge equivalent computation.",
            "If you define those same operation twice or more, then you don't want it to be computed that many times.",
            "Just want the system to realize that it's really like a + B.",
            "Here an A+B.",
            "He ran the nature of a plus B defines the same A+B.",
            "You want to simplify expressions like something.",
            "Time Zero will be zero X /, X will be one and so on.",
            "In some cases it optimizes things that.",
            "Give surprising results, like for instance if you add a matrix full of zeros to a matrix of a different shape.",
            "Well, maybe after optimization it will say, oh, we're only adding zeros.",
            "It does not matter and you'll get no no error in the default full optimized mode.",
            "But you can always deactivate that and.",
            "If you want to debug, we have numerical stability optimizations.",
            "For instance to call like log 1P instead of log of 1 + X.",
            "If you have like the log of a softmax or something, then.",
            "Depending of what something is, sometimes you can.",
            "You can simplify it that.",
            "That way if you have a log of sigmoid then can be replaced with a soft plus if you so you have optimizations like that.",
            "It's also the optimization framework that will insert in place and destructive operations.",
            "So you can have operations that our view of their inputs and just read just a DS output memory to the memory of some inputs.",
            "Annual have actual destructive operation that will overwrite some variables and basically The thing is that you can overwrite memory, but only if you're the last operation that needs that value.",
            "So we have some analysis, some static analysis of the computation graph that basically defines, well, OK. Who is going to be executive 1st and then the last one when no one needs that value can work in place and district in disruptive fashion?",
            "Or if it's not possible, then make a copy and then work in place on the copy.",
            "We have optimized routines, for instance, from glass, that will combine operations like for instance JMV.",
            "Anjem will do metrics or matrix vector multiplication, or that product and scaling an accumulation into a buffer.",
            "So it's really several operations in the original computation graph that will get put together in only one operation.",
            "Sometimes it's one operation.",
            "In the initial computation graph that gets splits into more than one, like for instance, take the shape of this and then allocate memory of that shape and then do something with it rather than copy that valuable like, compute some variable and then compute something else and then.",
            "Just multiply it with zero so that we have something with the right shape.",
            "So we have some shape inference.",
            "An we track basically some shapes and shape equivalents, so we're able to say OK. Well, this is like the same as like the shape of like the shape of the first dimension of this.",
            "So it's like the batch size and we're using a different places and it can enable some simplifications.",
            "We have constant folding, so if you define complex expressions and then substitute constants.",
            "For their inputs, then we can precompute things during the the phase, and we also have the transfer to the GPU they transferred the computation to.",
            "The GPU happens during the graph optimization phase, because basically the dot product of A2 CPU matrices is 1 operation.",
            "But we also have like a separate GPU dot product that takes GPU arrays as inputs.",
            "And so on.",
            "So we have to manage that transfer, but this enables us to have like part of the computation kept on CPU.",
            "If there's no GPU equivalent, and still some other parts transfer to GPU."
        ],
        [
            "You have a couple of predefined modes that define how the runtime is going to be executed and also which optimizations are going to be included.",
            "For instance, should I use?",
            "Like the first run is the default.",
            "It will include most of the optimization.",
            "It will include CPU and GPU code generation and compilation and so on.",
            "We have fast compile like mode fast compile which minimize completely the the compilation times so it does not generate any CPU code, it just uses the Python backends.",
            "Sometimes when you want to provide some too.",
            "To prototype something really quick, it can be useful.",
            "We have optimized or fast compile, which enables the cogeneration for CPU and GPU but does not apply all the graph optimizations.",
            "In particular, the memory usage can be higher overall using optimizer fast compiler makes the compilation phase, so the graph optimization phase in particular much faster, but end costs between like 2750% of performance which sometimes can be worth it.",
            "An we have things like debug mode which is extremely slow and just checks everything.",
            "Checks that every optimizing that every optimization, for instance gives the same numerical results as before the optimization, which is useful mostly for developers.",
            "And then you can also enable and disable specific optimizations like if you if you.",
            "I suspect that some optimization is causing some trouble.",
            "You can disable it.",
            "You can also manually include unsafe optimizations that are not there by default.",
            "For instance, we have optimization that insert couple of assert nodes to check that, for instance, their shapes are still consistent after the simplification.",
            "But if you know that it's working fine and you want that extra speed up, you can explicitly specify optimizer including unsafe, which is just like the first thing.",
            "And that can be done globally.",
            "That can be done individually for different compiled funk."
        ],
        [
            "This.",
            "So something.",
            "Important that we do is have C code for the operations.",
            "And basically it means that you define.",
            "Possibly optimized C code that interacts with the Python object, the Python arrays or GPU arrays, and so on that you have in memory.",
            "And that's provide the.",
            "The actual C code followed computation.",
            "It can call external libraries, for instance.",
            "That's how we use the different blast function and so on.",
            "And so you can have a look at a couple of examples, But basically you write code to extract, slide the right memory regions from the NUM PY arrays.",
            "It's using the NUM py C API to access values.",
            "Do the computation, puts the values in the right container.",
            "And.",
            "When you have that code during the compilation phase, if it's needed, then it's going to stitch together like boilerplate code plus the one you wrote and generate it as a Python module.",
            "Basically, it's defining a new Python modules written in C++.",
            "It's compiled by G+ Plus and imported and loaded back into Python so you can interact with it from Python.",
            "And for GPU codes, most of the time it's the same process but using CUDA and MVC instead of C++ and plus plus."
        ],
        [
            "Then we also have a runtime environment that will be in charge of executing all of these operations, so it's going to.",
            "Take the input values, check what is needed to compute the output.",
            "In particular, if you have like lazy operations like the FLS and it will say OK. Well, let's compute this and that like.",
            "These outputs are always need an this condition.",
            "I'll always needs an compute the thing in the right order.",
            "And.",
            "And then check well which additional things it needs to compute.",
            "Put everything in NUM PY arrays for the output, and return that.",
            "And we have Python version of that, and we have also a version written in C, which means that if all the the OPS in your compiled graph have C code, then you can basically.",
            "Leave the Python runtime for awhile and just execute callbacks and C functions, and you can do that without the overhead of running Python from C or C from Python And doesn't look like much, but it actually can make a difference in the."
        ],
        [
            "In the measured runtime.",
            "So.",
            "How does it work?",
            "If you want to use the GPU?",
            "Well, we wanted to make it as transparent as possible so.",
            "You define your graph the same way as on CPU.",
            "And we have a switch.",
            "Basically you have a configuration flag that enables you to say, well, I'd like to use.",
            "That's GPU an.",
            "What this does is that all the inputs.",
            "I mean it means that the shared variables that creates will be created on GPU by default.",
            "And all the inputs are going to be transferred to GPU.",
            "All the operation.",
            "Things that have a GPU implementation will get transferred to the GPU as well and applied on the GPU version of your variables.",
            "And the GPU CUDA code will be computed will be generated and compiled and loaded.",
            "And in the end.",
            "Everything will work transparently.",
            "There's still a couple of things to keep in mind.",
            "We have a legacy back end which is still the one used in the last release of Theano that only supports one GPU at the time, and that only supports float 32 as a D type, but the new one, which is completely functional an is available in the development branch and we hope to have a new release soon featuring it.",
            "This supports most of the types.",
            "Then the issue is whether the hardware supports them and support them well and will it be fast.",
            "So usually you still want to use 432 as much as you can, at least for floating point computation.",
            "We have limited support for float 16 for storage, so the computation is still going to be done in float 32 at the moment, but then it's going to be down casted as float 16 on GPU's that support it and it's useful too.",
            "To spare bandwidth and storage space on the GPU.",
            "However, you have to be careful about the precision, because depending on your algorithm.",
            "Rounding errors can become can.",
            "Can make the difference between like due to rounding error.",
            "Float 16 might not train at all even if the code is actually technically correct.",
            "So if you want to make sure that you use flow 32 or then.",
            "To easily switch between precisions, we have especially type called float X.",
            "And which is governed by a global flag.",
            "So if no taxes for 32, then by default like all the all the sensors that created with like matrix or vector and so on are going to be flat X.",
            "So if it's 432 into 2 if it's not 64 zero 64.",
            "But if you need some part of the graph to like always be double precision.",
            "Or if you need some part of the graph to always be single precision, then you can always explicitly use like F matrix for float or D matrix for double or matrix D types equal explicitly what you want, it's just syntactic sugar to help you have the same model work on.",
            "On different precision."
        ],
        [
            "So Speaking of configuration flags, we have a config module that lets you configure how you want to have you on piano to behave like you can specify which default device you want to use, like for instance, the default is CPU.",
            "If you're using the CUDA back end and you won't find probably like CUDA CUDA 0 if you want to change float X, you can specify like that and you can also specify like the default compiler compilation mode.",
            "You can specify which optimization to add or remove.",
            "You can define lots of things including like behavior during some kind of errors, and so there are three ways to set set those kind of configuration flags.",
            "The first which.",
            "I mean, yeah, the first is by changing and the environment variable channel flags.",
            "The second one is directly in Python.",
            "We just set things to tiano config.",
            "Whatever he wants, this does not work for all the options because some of them need to not change and value needs to be constant throughout the whole execution of the process.",
            "In that case, if you try that, you get an error message and you have the.",
            "The configuration file, just like configuration file with sections an you so the in order of presidents like the first thing that's going to be checked for is the configuration file.",
            "Then what's in the configuration file can get overwritten by the flags.",
            "An then at runtime you can even change the value several times between some things by playing directly in the other config.",
            "Then four.",
            "Yeah.",
            "So the question was if you can write yes or I have to repeat for the recording.",
            "So the question was if you already have C code and it's working fast, can you?",
            "Have an additional speedup by using the GPU, that's right.",
            "So if you have GPU codes in Python, maybe maybe I should answer your question later.",
            "OK, so if you want to use the GPU.",
            "To get additional speedup, you need to write CUDA codes I guess for that specific operation, but this is.",
            "I mean, This is why we have piano is that you can, if you can express your model in Mumbai or something close to NUM py, you can use Theano to express.",
            "You can use the arrow to express the model that you need.",
            "Then Jenna will automatically.",
            "Translates that as.",
            "CUDA codes that will execute on the GPU if I hope it answers your questions.",
            "If it did not, please let's, let's take that offline afterwards."
        ],
        [
            "So.",
            "For the, for the last part, I'm going to brush over more advanced topics that you'll have the occasion to delve deeper into tomorrow.",
            "So."
        ],
        [
            "First of all, loops in Theano and symbolic loops.",
            "So the approach we've taken for that is basically to encapsulate the whole loop.",
            "As one operation.",
            "And this enables us to have like variable number of iterations to have things like early exits or of the loop.",
            "To have the number of steps be a symbolic variable to have yeah stopping condition.",
            "Symbolic stopping condition that gets evaluated at each iteration to know how many iterations will be executed in total.",
            "And the way it works is so you have one symbolic node in the graph.",
            "It's up is scan.",
            "It's all scan an inside that node.",
            "That's basically a whole separate Theano sub graph.",
            "And compiled gender function and what that sub graph represents is the computation that has to be done at each iteration of the loop.",
            "So if of the loop.",
            "All you have is like an addition.",
            "Then you'll have a really small channel graph in there.",
            "We'd like 2 inputs and one output an an addition.",
            "And this is going to be compiled as an optimized general function, potentially transfer to GPU if it's if it's feasible.",
            "And then when the execution of that node begins, then the scan node itself will iterate through its inputs.",
            "If there are sequences or just take always the same value if you have constants and call the compiled inner function.",
            "On the appropriate slice.",
            "Inputs and compute the output and feed that either to the output that you're required or to the next iteration in like a temporary buffer or something and call it over and over again an if you have a stopping condition, then it will also compute the stopping condition and check whether it needs to stop and exit at that point.",
            "And the fact that we have this symbolic node.",
            "Enables us easily to take gradients through it as well, so the scan operation also defines a graph function like any regular node in the graph.",
            "And that scan is basically a loop in reverse.",
            "Over well, the same number of iterations, reverting the sequences, feeding it like regions with respect to outputs and accumulating the right thing at the right place, and so on.",
            "An basically you type A scan node for the forward.",
            "Computation of your loop and when you call grad and so on, you'll get another scan computation on like the reverse.",
            "Sequences and so on.",
            "And this implements regular backdrops, full time with a full accumulation of gradients.",
            "And it has a GPU implementation optimizations that move.",
            "The symbolic scan onto GPU and in a graph on GPU and so on exists.",
            "And there are also additional optimizations that make it possible, for instance to move invariant parts out of the loop, like if you define something that only involves things that are not sequences and that you.",
            "Just recompute at each timestep.",
            "Then you want that to be moved out of the loop, or in some cases if you have sequential things that are not really sequential, like for instance, if you compute the DOT product.",
            "Of a different vector at each timestep and always the same matrix.",
            "It's basically just a matrix matrix product, so you can move that part out of the scan and have only one matrix matrix product instead of North vector matrix products and so on.",
            "So we have those things also."
        ],
        [
            "If we only need the last.",
            "Iteration.",
            "Of the loop.",
            "Then in some cases we can.",
            "We can forget about the intermediate values for that state, so we have optimizations to reduce memory usage for that.",
            "However, sometimes you need if you plan to take a gradient through that loop.",
            "Sometimes you'll need all those intermediate variables to be able to do the backdrop through time an in that case, well, I still need that amount of memory.",
            "Yeah.",
            "Does it make a difference if you scan or reported?",
            "So the question was, apart from the conditional ending, does it make a difference to you scan all the for loop so well you have the symbolic number of iterations that is known before executing the loop, but not at compilation time.",
            "So that makes a difference in that case as well in practice.",
            "The graph will not be the same at all.",
            "So in one case you have like 1 node with the loop implicitly inside and in the other case if you use the follow up then you'll have many instances of what happens in the loop, so it can have an impact on the memory usage.",
            "It can have an impact on the computation time, but it's really depends on your problem and it's hard to predict which one is going to be faster at runtime and which one is going to be faster.",
            "At execution time.",
            "Like for instance, the bunch of vector matrix operations into a matrix matrix operation.",
            "This will not happen if you want all the loop.",
            "Just because currently we don't have an optimization for that.",
            "We could add 1.",
            "There's nothing in principle that prevents stats, but.",
            "So yeah, if you have a doubt, usually the only actual answer is try both and see which one works best for your case.",
            "So here's a small example of how you would.",
            "Define those small case of a loop that actually needs to be sequential.",
            "It's just loop with accumulation.",
            "Into does need to be so yeah.",
            "So.",
            "Here we start by defining, well, the inputs that we have.",
            "So we have one scalar that's an integer, one vector that's of floating point type it's protector.",
            "You call.",
            "That Lambda function FN is basically the function that needs to be executed at each iteration.",
            "And in that case, it's a multiplication of the prior result by.",
            "The current vector.",
            "So.",
            "That we provide, we have to provide the initial values, which in that case is just once.",
            "We say that A is a non sequence, so we use the same A over and over again.",
            "And so the other one, this one for your result is the recurrent States and we provide the number of steps.",
            "In the end we just say we want only the final results, so it will override the intermediate results and then we call Tina function with these inputs and outputs.",
            "The updates here are mainly if there are random variables or if you sample random numbers inside the loop, because if you sample random numbers inside the loop you want to update the random random number generator with the right amount.",
            "In the end, so this is basically just why there's update here.",
            "It's always good practice to use it even if you don't sample.",
            "And yeah, and then if you have.",
            "You just call that compiled function with, well, a.",
            "This range from zero to 9 K is too, and you see that.",
            "It computes the square of that vector by just multiplying with itself.",
            "K times."
        ],
        [
            "So the next section is about the tools we have to help.",
            "Investigating what happens and debugging and diagnosing.",
            "The reasons we need that is because since we separate the definition of the computation and the computation graph from the actual execution.",
            "And since we define the graph only one but once, but we can executed lots of time, then we need some tools to help reconcile both and help make the link between.",
            "Why do I have that error at runtime when it didn't complain at compile time, for instance?",
            "Or if you're training models or if you're calling the same function over and over again, and at some point something bad happens then?",
            "We want to have tools to help investigate that.",
            "So.",
            "We will dig into that deeper.",
            "Tomorrow for those who are interested right now, I can just show you small demo of.",
            "A notebook showing well.",
            "Regular examples of.",
            "Like the kind of error message you might expect and what that means and how to interpret that.",
            "So let's say I. I executes that.",
            "Statement.",
            "Let's not worry too much about what's happening in there yet.",
            "The error will pop out.",
            "But basically there are four parts in their message that stereo.",
            "First of all, there's.",
            "The stack trace itself that comes from Python or Ipython, or whatever interpreter you're using that tells you where during the execution.",
            "Things went wrong.",
            "Then you'll have the error message itself with a small description of like what's wrong.",
            "Sometimes you can have additional details, and sometimes you can have more information and I'll just see.",
            "K. Oh, here it is.",
            "Screen this.",
            "So first you have the stack trace from my Python.",
            "The screen is kind of narrow, so it doesn't.",
            "It's not very pretty, but usually you have it.",
            "You have the full lines, it tells it's a value error.",
            "It shows what happens during the electrician, but that's not really helpful.",
            "Then you have the actual error message that says input dimension mismatch.",
            "So is well, the shape of 1 input is 2.",
            "An another input shape is 3.",
            "And this happened in some automized node.",
            "That's composite operation of like something and this like.",
            "In addition here and the product here.",
            "Control and then so this shows like what the inputs are.",
            "It's additional information on the operation and its inputs.",
            "In that case, you see that the shape of the input well one is 2 and one is 3, and the values because these are short values, we can show them one is 11111 an you say OK. Maybe?",
            "There's a shape mismatch somewhere.",
            "Seems obvious, but you don't really know where an you don't know.",
            "Is it like this operation?",
            "Is it the second one?",
            "Is there something else what's happening?",
            "So.",
            "To know that you have a hint here.",
            "And that says running with optimization disabled can give a backtrace of when the.",
            "Problematic operation was actually created.",
            "And it says you can do that with the flag optimizer fast compile and if you.",
            "Can also use optimizer none an if you want more information you can use exception for both the high.",
            "So.",
            "If you do that again.",
            "But in that case.",
            "Using.",
            "Mode equals tiano mode.",
            "Optimizerx equals, let's say fast, compact.",
            "And try to execute that.",
            "You'll see well the same informative stack trace that yeah, it happens during the execution.",
            "The same error input dimension mismatch 2, three and so on.",
            "But then it tells you backtrace when the node is created.",
            "And so it's somewhere in the Ipython notebook obviously, and so on and so on.",
            "And here it says in the end.",
            "Oh, it's on line 11.",
            "And it said equals that times Y.",
            "So you can.",
            "You can actually go back here and you say it's not that line.",
            "It's actually that line that has an issue.",
            "And then well, of course.",
            "The case is simple enough that you know that, well, you assign something of size 22X and something of size 3 to Y.",
            "Well, X + X will work, but set times Y will not.",
            "So there are other useful tools like well, debug mode that I mentioned that double checks and triple checks everything.",
            "You can use the Python debugger to have an interactive Python debugging session in the middle of your model or during execution.",
            "If you want to check things, you can have breakpoints during execution where you define conditions to check an if.",
            "Those conditions are met, then Tina will drop into my DB prompt where you can look around at what the values currently are from the select from some selected Bibles.",
            "You can print things during execution using the print up.",
            "You can print such tributes or values are things to help you detect Nan and Infinity and large values.",
            "These values are useful when you want to compute at the same time as you build the graph, so it lets you specify test values for all the input variables that you have.",
            "It will use the initial value of shared variables as the default value for the shared variables, and each time you create a new symbolic node like.",
            "In addition, the dot donations on right at the same time as creating the symbolic variable.",
            "It will try to compute value for that variable.",
            "Given the test value for its inputs and so on, so that means it's especially useful if you have like shape mismatch issues or things like that, because it will try to execute that the computation at least once and so right away you'll have your shape mismatch error when building your graph, and you don't have to wait until the whole graph is built and the gradient is compiled and the graph is optimized and the code is generated to then.",
            "Plug in your input values and realize that.",
            "You're missing your transpose somewhere in the middle of a graph.",
            "And plus couple of other things that that exists, but let's let's go back to.",
            "The presentation itself."
        ],
        [
            "Jenna has been designed so that it's easy to extend it and to add new functionality's new operations, especially if you have specialized or optimized libraries or third party libraries that exists and that you want to easily wrap in Python, for instance, then it's a couple of lines of codes to just wrap an existing operation that has Python bindings and that works on Empire Azor.",
            "Or could I raise or things like that.",
            "To just.",
            "Depart of Theano an it's just a couple of more lines if you have an expression for the gradient and 1:25 as well.",
            "It has the disadvantage that there's a context switch between C and Python during the execution, but if it's a large operation and that's highly optimized, it might really be worth it.",
            "For instance, people having implemented through the convolutions using FFT on GPU using that kind of Python bindings, which was much faster than having to write CUDA code for that."
        ],
        [
            "The harder way an sometimes more optimized, so usually a bit more optimized is to write C code.",
            "For that you need to understand the API of Python And PY to interact directly with the data that we have.",
            "Usually we want our code to handle arbitrary strides, but we have not.",
            "That's called GPU contiguous.",
            "That basically makes a copy of the inputs if they're not contiguous, and ensures that everything is fine.",
            "You need to manage your accounts for Python And so on.",
            "But you don't have any overhead from the Python interpreter for calling from going from C to Python And back.",
            "An external contributors have used that to implement better and more optimized and more complete versions of convolutions on CD on CPU and GPU."
        ],
        [
            "It's just a list of couple of new things that we have in Theano.",
            "The new GPU backends which I mentioned earlier which supports multiple GPU's in the same function for model parallelism that supports additional detect, including the half precision float float 16 experimental support for this basic operation for open CL, but it's really really experimental.",
            "Four months improvements for.",
            "Many critical parts like for scan for convolutions without using codeine, an allocation and the allocation of GPU memory which used to be sometimes a bottleneck.",
            "We're not using CNN or something similar to avoid synchronizing the GPU each time we allocate allocate memory, and we have a mini framework called platoon which is built on top of piano and which uses Theano to train the same model in I mean.",
            "To train multiple instances of the same model using data parallelism.",
            "So we have.",
            "Done for style, asynchronous SGD and we have elastic synchronous SGD as well and someone is working on Synchronoss and multi machine version of Splatoon as well.",
            "It's not there yet, but it's.",
            "It's in the future.",
            "We have made a lot of progress in the compilation time as well.",
            "An also in ways of compiling the optimizing the graph only once an having different functions.",
            "Different functions use the same compile graph or compile the graph once and then execute only part of it.",
            "Compute only some outputs or don't apply the updates or swap shared variables and so on.",
            "So for instance if you have.",
            "You can compile one function to do some training and then just say oh I'd like to swap like this.",
            "Variables for this short variables for like this previous weights or these weights averaged by.",
            "I don't know some kind of aggregation or whatever and don't perform the updates and just apply that on some new data set and you don't have to re optimize the graph and.",
            "So this is this is something quite recent.",
            "We have new dynamic diagnostic tools like the interactive visualization that has been.",
            "Incorporated into, you know, I think that's September or something like that.",
            "The PDB breakpoint and the we are trying to keep the stack trace of when nodes were created, even across optimizations.",
            "So right now for some cases does not work yet like the one I showed you earlier with the Illinois Fusion.",
            "This one did not preserve the stack trace, but we have some that do and.",
            "We have a couple of people working on that and this should."
        ],
        [
            "Coming soon so.",
            "This is, well, the next features that are coming soon, so Latin across nodes.",
            "Someone's working on that, but faster optimization or graph optimization are coming.",
            "Wrapping off more quickly and optimization is also something that's come in the following month and."
        ],
        [
            "Yeah.",
            "So.",
            "This is almost the end.",
            "I'd like to thank well lot of people who supported the lab who supported the development of piano, competition resources and so on, and the CRM and see for for for organization of the summer school."
        ],
        [
            "If you have questions we don't have much time."
        ],
        [
            "But you can always ask them offline and I'll be there and I'll be there around and other developers will be there as well and.",
            "The hands-on practical session tomorrow is also a good time."
        ],
        [
            "To ask us some questions if you want to go further, these slides and the notebooks with the code examples that I showed in the slides are available on GitHub and if you want more resources on piano while the documentation is on deeplearning.net, the code is on GitHub and a couple of months ago we published a new article.",
            "Expanding well, the current state of piano and couple of benchmarks and so on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm going to present is mostly an introduction to TNO what it is, what you can do with it.",
                    "label": 0
                },
                {
                    "sent": "And this is work with lots of people like Miller.",
                    "label": 0
                },
                {
                    "sent": "Previously, Lisa in particular.",
                    "label": 0
                },
                {
                    "sent": "Credit investor.",
                    "label": 0
                },
                {
                    "sent": "And so this is the first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the first part, so today is going to be just the theoretical part, so I'm going to present you slides and a couple of code snippets and so on.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow there's going to be practical session at the same time it's going to be split between Theano and Torch.",
                    "label": 0
                },
                {
                    "sent": "One room is going to be faulty.",
                    "label": 0
                },
                {
                    "sent": "I know another one for torch, I don't remember which one is going to be where, but it's going to be hands-on exercises.",
                    "label": 0
                },
                {
                    "sent": "Where you access machine with the GPU in the cloud through an ipython or I torch notebook.",
                    "label": 0
                },
                {
                    "sent": "So today you don't need a laptop to follow anything, even though the code snippets are going to be available online.",
                    "label": 0
                },
                {
                    "sent": "If you want to go back later and play with them a little bit, but for tomorrow you should bring a laptop an you just need the browser.",
                    "label": 1
                },
                {
                    "sent": "You don't need the local installation because it's going to be hosted on the cloud.",
                    "label": 0
                },
                {
                    "sent": "So all the material is online on GitHub.",
                    "label": 1
                },
                {
                    "sent": "If you want to come back later.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, I'm going to give you an overview as a quick overview of Theano.",
                    "label": 0
                },
                {
                    "sent": "Then explain more in detail the graph definition, the graph structure that TNO is using.",
                    "label": 1
                },
                {
                    "sent": "Then, which graph transformations and manipulation you can do with piano.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to present what optimizations exist and how we make runtime fast, and then we're going to talk quickly about more advanced topics.",
                    "label": 1
                },
                {
                    "sent": "And if you have questions about these advanced topics.",
                    "label": 0
                },
                {
                    "sent": "Hands-on session of Tomorrow would be a good place to look for more in depth answers and existing code examples demonstrating those features.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To begin, Theano is a compiler of mathematical expressions and what that means is that it makes it possible to define mathematical expressions using an easy syntax which is really close to the syntax of NUM PY.",
                    "label": 0
                },
                {
                    "sent": "So if you know Python, if you've already used NUM PY, then it's not that difficult to adapt, then it makes it possible to.",
                    "label": 0
                },
                {
                    "sent": "Change those expressions, manipulate them to substitute sub expression for others to derive gradients automatically.",
                    "label": 0
                },
                {
                    "sent": "And by that I mean derive mathematical expression of gradients that you can then further manipulate and then introducing optimizations for performance for numerical stability and so on.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "It generates code optimized codes or uses already optimized backends like optimized blast like CUDA, an also custom C code so that the execution as well is fast.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk a little bit about the diagnostic tools and.",
                    "label": 0
                },
                {
                    "sent": "Checks and so on that are built in Indiana.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do you know is currently?",
                    "label": 0
                },
                {
                    "sent": "Metro project it started as a student project eight years ago, but now it's quite stable.",
                    "label": 0
                },
                {
                    "sent": "An is being used all over the world in universities, small companies, large companies, it's driven.",
                    "label": 0
                },
                {
                    "sent": "Lots of research papers.",
                    "label": 0
                },
                {
                    "sent": "We have contributors for from all over the world and.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "You can find well the official documentation online and as well as tutorials about deep learning for.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Piano.",
                    "label": 0
                },
                {
                    "sent": "No is also the basic the base for different.",
                    "label": 0
                },
                {
                    "sent": "Software projects, most of them are machine learning libraries, but.",
                    "label": 1
                },
                {
                    "sent": "Not only for instance, by MC Three is actually probabilistic programming programming software using using Theano for automatic gradient starvation and fast execution.",
                    "label": 0
                },
                {
                    "sent": "There's Splatoon, which is a framework built on top of piano that enables training the same model on multiple GPU's on the same machine, exploiting data.",
                    "label": 1
                },
                {
                    "sent": "Realism, there's no MPI that exploits model parallelism to train on multiple machines, and there are many more.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me walk you briefly through how.",
                    "label": 0
                },
                {
                    "sent": "You typically would use Theano, so the defines a language, a compiler and library at the same time, so the language close to NUM Py makes it possible to define symbolic mathematical expression.",
                    "label": 1
                },
                {
                    "sent": "That you then compile as a Python callable.",
                    "label": 0
                },
                {
                    "sent": "That will be able to compute values output values given input values.",
                    "label": 1
                },
                {
                    "sent": "And then you can execute that function that callable on whatever data you have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to define an expression, you start from symbolic inputs.",
                    "label": 0
                },
                {
                    "sent": "The inputs are strongly typed, so you need to define if it's like vector matrix and so on.",
                    "label": 0
                },
                {
                    "sent": "By default they're Floating Points, which can also define integer values, and so on, and then from those symbolic inputs that don't hold any any value.",
                    "label": 0
                },
                {
                    "sent": "Yet you can define abstract operations well.",
                    "label": 0
                },
                {
                    "sent": "For instance, in that case, a DOT product addition, sigmoid, sigmoid and so on.",
                    "label": 0
                },
                {
                    "sent": "So one bed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can visualize what it means to do that, so we have a couple of debugging function to help visualize graphs.",
                    "label": 0
                },
                {
                    "sent": "Here we have debug print that prints the structure of the graph as a tree.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the dot variable is well, it's the output of a dot operation on variables named X&W.",
                    "label": 0
                },
                {
                    "sent": "If you see what the output is, then you see it's sigmoid of Alan Wise addition.",
                    "label": 0
                },
                {
                    "sent": "And that addition is of two elements, the dots that we had earlier, and the bias that's here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then once we have that expression, we can compile it into a Python callable.",
                    "label": 0
                },
                {
                    "sent": "So to do that you called piano function and you specify the inputs and the output.",
                    "label": 0
                },
                {
                    "sent": "So basically you have that whole computation graph and you say I want a function that given these variables, computes that one or these ones.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if he has two outputs, only one input, which is an intermediate variable.",
                    "label": 0
                },
                {
                    "sent": "Or you can also go all the way through the final variable, define new ones up with more than one variable, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here again, we can visualize the graph.",
                    "label": 0
                },
                {
                    "sent": "After the compilation, we actually call it debug print again on the compiled objects.",
                    "label": 0
                },
                {
                    "sent": "And here we see it's not exactly the same as before.",
                    "label": 0
                },
                {
                    "sent": "We have, like Jenn Viane Alok and those kind of things.",
                    "label": 0
                },
                {
                    "sent": "And this is because what happened when we call Tiano function is that the competition graph that we define all the part of the computation graph that we actually care about in each of the function has been written in parts or optimized.",
                    "label": 0
                },
                {
                    "sent": "For instance, instead of calling the non Pi dot product.",
                    "label": 0
                },
                {
                    "sent": "We since we accumulate on top of the bias, we can use the blast function generally, so which is the generalized matrix vector product?",
                    "label": 0
                },
                {
                    "sent": "We have, like transposition, that happens in place instead of copying the whole.",
                    "label": 0
                },
                {
                    "sent": "The whole weight matrix, doing the transposition, and so on, so.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing that happens.",
                    "label": 0
                },
                {
                    "sent": "And if we visualize it using another tool, we can see well in green like the inputs here.",
                    "label": 0
                },
                {
                    "sent": "Take awhile, transposition and so on.",
                    "label": 0
                },
                {
                    "sent": "We have the dot product here and the output in dark blue.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, so here we have only the output of the dot product.",
                    "label": 0
                },
                {
                    "sent": "Here we have the output after the sigmoid and we see that here if it's after the sigmoid sigmoid can work in place and so This is why you have a right arrow here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can visualize here.",
                    "label": 0
                },
                {
                    "sent": "You have two outputs.",
                    "label": 0
                },
                {
                    "sent": "This is an intermediate result output of the dot product, which is also an output of the function, which is why it's called Blue, an not overwritten by the sigmoid and so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a new tool for visualization that's called.",
                    "label": 0
                },
                {
                    "sent": "This revisits been integrated in piano last summer.",
                    "label": 0
                },
                {
                    "sent": "An IT outputs an HTML page where you can you have JavaScript and so on where you can interact with those.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of nodes and you can move them around and inspect them more easily than than just that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann, once yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the color coding that is here, I think is explained on the online documentation.",
                    "label": 0
                },
                {
                    "sent": "If it's not, please open the ticket on GitHub and we'll try to correct that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so once you have that compiled object then you can provide numerical values for all the inputs that it expects.",
                    "label": 0
                },
                {
                    "sent": "For instance, here we sample a couple of random values an if we call F which only computed the dot product we have it, it's after the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "You have that.",
                    "label": 0
                },
                {
                    "sent": "If you compute more than one at the same time while you have.",
                    "label": 0
                },
                {
                    "sent": "The right values.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to dig a little bit deeper in what actually happens during that time and how does it work, and motherhood, which kind of data structure we're using, and so on, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I've talked about the computation graph, which represents the mathematical expression or expressions that we want to use so.",
                    "label": 0
                },
                {
                    "sent": "How it's done internally is that we have a graph that has two kinds of nodes.",
                    "label": 0
                },
                {
                    "sent": "One kind of those we called variable nodes or variables and they represent value.",
                    "label": 0
                },
                {
                    "sent": "They are going to have to hold values where we compute when we actually execute the function.",
                    "label": 0
                },
                {
                    "sent": "And we have apply nodes that represent mathematical operations being applied on these values, and so in practice, for instance, the input and output of the graph, those are variables they will hold.",
                    "label": 1
                },
                {
                    "sent": "You will provide values for them for the inputs and you will.",
                    "label": 0
                },
                {
                    "sent": "Get output values as a return.",
                    "label": 0
                },
                {
                    "sent": "An for intermediate variables, well, they will hold the intermediate values during execution and the apply nodes can have.",
                    "label": 0
                },
                {
                    "sent": "Several inputs, for instance, the addition will have like an arbitrary number of inputs and can have one or more outputs.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have like 1 operation to compute the Max and argmax of 1 tensor, where it's going to be only one operation.",
                    "label": 0
                },
                {
                    "sent": "But you'll have two output variables that will be computed at the same time.",
                    "label": 1
                },
                {
                    "sent": "An intermediate variable can be used by several.",
                    "label": 1
                },
                {
                    "sent": "By several operations next, but one variable can only be the output of 1.",
                    "label": 0
                },
                {
                    "sent": "Apply nodes, or none, or none at all.",
                    "label": 0
                },
                {
                    "sent": "If it's an input variable.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is how it works.",
                    "label": 0
                },
                {
                    "sent": "So you have for instance in that case.",
                    "label": 0
                },
                {
                    "sent": "Addition of two matrices.",
                    "label": 0
                },
                {
                    "sent": "So this is an apply node that has two inputs, two matrices X&Y.",
                    "label": 0
                },
                {
                    "sent": "It's of the type.",
                    "label": 0
                },
                {
                    "sent": "I mean it has an op that is the addition.",
                    "label": 0
                },
                {
                    "sent": "And it has one output which is also a matrix.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If so, calling Python prints again the same tool we introduced before, but with compact equals false.",
                    "label": 0
                },
                {
                    "sent": "It shows all the nodes, all the variable nodes and the apply nodes and the variable nodes are those square boxes and the apply nodes are these Oval boxes.",
                    "label": 0
                },
                {
                    "sent": "And so here, unlike the previous previous figure, you see all the intermediate variables like.",
                    "label": 0
                },
                {
                    "sent": "You see that for instance, here we take their shape.",
                    "label": 0
                },
                {
                    "sent": "Of the weights and it's going to be used to allocate some output of the right size and so on.",
                    "label": 0
                },
                {
                    "sent": "And here you can see all these intermediate variables that are not named that we usually don't interact with, but you see, this is the whole.",
                    "label": 0
                },
                {
                    "sent": "This is the whole graph.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you see earlier.",
                    "label": 0
                },
                {
                    "sent": "They're not here, here, and here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ann, those variables have a strong type, meaning that unlike in Python, where you can have the same variable being.",
                    "label": 1
                },
                {
                    "sent": "Like any type, in order to handle needs to know what the type of the input variables is and that way you can infer types for the intermediate variables and the outputs and so on.",
                    "label": 0
                },
                {
                    "sent": "And everything has to be consistent.",
                    "label": 0
                },
                {
                    "sent": "It also helps debugging things.",
                    "label": 0
                },
                {
                    "sent": "It also helps avoiding to suddenly have an increase in precision that makes everything slow.",
                    "label": 0
                },
                {
                    "sent": "So what is so we have a different?",
                    "label": 0
                },
                {
                    "sent": "Different types and the most used or tensor types for NUM PY arrays on the main host or CPU memory and GPU array for arrays on the GPU using CUDA.",
                    "label": 0
                },
                {
                    "sent": "We also have a sparse type which is not as well supported as we would like.",
                    "label": 1
                },
                {
                    "sent": "And basically what is in the type is the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "So you have to know if something is like a scalar or vector or matrix and so on in advance the data type is.",
                    "label": 0
                },
                {
                    "sent": "It's like float, double integer of whatever precision and so on.",
                    "label": 0
                },
                {
                    "sent": "And the road festival pattern.",
                    "label": 1
                },
                {
                    "sent": "And we're going to come back to the broadcastable pattern later.",
                    "label": 0
                },
                {
                    "sent": "But basically it means knowing which shapes I mean on which dimension the shape is always going to be exactly 1.",
                    "label": 0
                },
                {
                    "sent": "Or not one.",
                    "label": 0
                },
                {
                    "sent": "The shape itself is not part of the type, so that means that you can have an input you declare it's a matrix.",
                    "label": 0
                },
                {
                    "sent": "It's not a problem at all if during different calls of the same compiled function you pass it matrices of different shapes.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have a batch size that varies, it's usually not an issue.",
                    "label": 0
                },
                {
                    "sent": "And the memory layout is not part of the type either, so you can have like a matrix, A transpose matrix, slices and so on.",
                    "label": 1
                },
                {
                    "sent": "And it's not going to be an issue.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So broadcasting is it using NUM PY and it's used in other languages as well.",
                    "label": 0
                },
                {
                    "sent": "It's the implicit replication of a tensor along an axis that exists, but that's of length one.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if you have, say, a row vector.",
                    "label": 0
                },
                {
                    "sent": "And you add it to a matrix.",
                    "label": 0
                },
                {
                    "sent": "Then what you expect is that that vector is going to be replicated.",
                    "label": 1
                },
                {
                    "sent": "End times, and then this implicitly and then this is going to be added to the matrix if you have a column vector, then you expect it to be replicated.",
                    "label": 0
                },
                {
                    "sent": "On the columns and added them.",
                    "label": 0
                },
                {
                    "sent": "But if you have like a two by two matrix and the two by four matrix, you don't expect that two by two matrix to be replicated.",
                    "label": 0
                },
                {
                    "sent": "You expect the dimension error or size mismatch.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "So Theano needs to know in advance whether it will have to replicate the tensor implicitly or not.",
                    "label": 0
                },
                {
                    "sent": "And This is why you have that broadcastable pattern.",
                    "label": 0
                },
                {
                    "sent": "That's part of the type.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here you can define like a row, which is a one by North 2D matrix and a column that's an end by 1 to the denser.",
                    "label": 0
                },
                {
                    "sent": "And if you add them then both are going to be replicated.",
                    "label": 0
                },
                {
                    "sent": "That way, if you have a vector, it's going to be treated as a row in that case, and broadcastable dimensions are always added implicitly to the left.",
                    "label": 1
                },
                {
                    "sent": "This this part is exactly like NUM Py.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another difference between piano and NUM py.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "That we cannot override everything from Python, so that means we have a couple of limitations.",
                    "label": 0
                },
                {
                    "sent": "If you do the like tiano since it has a competition graph that needs to be done in advance.",
                    "label": 0
                },
                {
                    "sent": "It cannot.",
                    "label": 0
                },
                {
                    "sent": "You cannot have side effects that happen when you create new expression or when you build expression.",
                    "label": 1
                },
                {
                    "sent": "The only thing you can do in Theano, all the expressions.",
                    "label": 0
                },
                {
                    "sent": "What they do is that they add new nodes to the existing competition graph.",
                    "label": 0
                },
                {
                    "sent": "They build them with, add inputs, existing variables, an that gets added to the computation graph you're working on, so you can do things like if you have a variable A, you can do things like.",
                    "label": 0
                },
                {
                    "sent": "A + = 1 because Python will just interpret that as redefined a as a plus one and you still have the old a value that exists somewhere in the graph, and if it was the input to other construction.",
                    "label": 0
                },
                {
                    "sent": "Other expressions, then those expressions are still going to use the old A and the new ones that you build are going to use the new A.",
                    "label": 1
                },
                {
                    "sent": "However, you cannot use the syntax to assign things to slices of arrays.",
                    "label": 0
                },
                {
                    "sent": "So it's because you cannot return a new object.",
                    "label": 1
                },
                {
                    "sent": "Python requires that you return the same object and so you have to use things like ink, sub tensor and sets of tensor, which creates a new variable and then you would have to use that new variable to in your new expression.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of specific OPS or the specific operations that have side effects like print assert.",
                    "label": 0
                },
                {
                    "sent": "There's also a PDB breakpoint, one an how that works is that it's.",
                    "label": 1
                },
                {
                    "sent": "Inodes that exists in the computation graph, but that does not just copies over its inputs to its output, so it's not computing anything, but it has to be somewhere in the graph an if the output is used then.",
                    "label": 0
                },
                {
                    "sent": "Then the node is going to be executed and the print statement is going to appear or the assertion is going to be checked and so on.",
                    "label": 0
                },
                {
                    "sent": "However, if you do not reuse.",
                    "label": 0
                },
                {
                    "sent": "Of that, or if that output is not actually part of the computation that you want to execute or not.",
                    "label": 0
                },
                {
                    "sent": "Part of the sub graph that you're that you're compiling a function of, then it's going to be pruned, and it's not going to be executed.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, we cannot really redefine Python keywords like if an for an Lang and so on.",
                    "label": 1
                },
                {
                    "sent": "So we have the other constructs for that.",
                    "label": 1
                },
                {
                    "sent": "If you do, if some variable, then Python is going to cast that into a Boolean an.",
                    "label": 0
                },
                {
                    "sent": "It's not like a symbolic variable that represents the condition that the value of that variable evaluates to true or something.",
                    "label": 1
                },
                {
                    "sent": "So you have to do something like if else.",
                    "label": 0
                },
                {
                    "sent": "Or switch like in NUM py?",
                    "label": 0
                },
                {
                    "sent": "If you do.",
                    "label": 0
                },
                {
                    "sent": "A for loop in Python.",
                    "label": 0
                },
                {
                    "sent": "It will just execute the four loop once when you build the graph, so sometimes that's what you want to do.",
                    "label": 0
                },
                {
                    "sent": "If you have something to execute like 3 times, maybe you want to have nodes in the computation graph that compute the expression one time and then the second time, and then the third time.",
                    "label": 1
                },
                {
                    "sent": "This is basically unrolling the loop.",
                    "label": 0
                },
                {
                    "sent": "And sometimes that's what you want, and sometimes it's not really appropriate, because if you don't know like how many elements there will be in advance, then it will not work.",
                    "label": 0
                },
                {
                    "sent": "If the number of nodes that you create is too big, then it's going to create further complications during the optimization graph optimization phase.",
                    "label": 0
                },
                {
                    "sent": "There's Daniel scan.",
                    "label": 0
                },
                {
                    "sent": "Basically, encapsulates the whole loop in only one node that I'm going to talk about that later.",
                    "label": 1
                },
                {
                    "sent": "The length function in Python only returns Python integers.",
                    "label": 0
                },
                {
                    "sent": "Or maybe long, but so you cannot use it to get the symbolic shape available and the print statement.",
                    "label": 0
                },
                {
                    "sent": "Well, it's going to be executed once.",
                    "label": 0
                },
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "Internally, if BAR evaluates data true, there's expression to still get devaluated.",
                    "label": 0
                },
                {
                    "sent": "In this example.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is whether eyfells executive expression one an expression to only One South.",
                    "label": 0
                },
                {
                    "sent": "We support lazy evaluation in some in some parts, or what's going to happen in that case is that it's going to evaluate the condition first.",
                    "label": 0
                },
                {
                    "sent": "And then, depending on the evaluation of that, the runtime will ask for devaluation, either of expression one or expression two there is.",
                    "label": 0
                },
                {
                    "sent": "I mean there are a couple of issues that we're aware of, and in some cases devaluation of the other one is forced.",
                    "label": 0
                },
                {
                    "sent": "It happens when you have in place computation and basically the runtime.",
                    "label": 0
                },
                {
                    "sent": "For instance, something in expression two will overwrite an intermediate.",
                    "label": 0
                },
                {
                    "sent": "Value or used in expression one.",
                    "label": 0
                },
                {
                    "sent": "Then it will try to evaluate it first.",
                    "label": 0
                },
                {
                    "sent": "To prevent some values from being overwritten and currently it's an issue we are aware of and will try to find some time to fix that, but in general it works fine and it only executes the branch that you that you need.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once you build that graph.",
                    "label": 0
                },
                {
                    "sent": "You can, I mean there are functions too.",
                    "label": 0
                },
                {
                    "sent": "To modify it or to.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's only a starting point, so there are different things you can do.",
                    "label": 0
                },
                {
                    "sent": "You can do substitution.",
                    "label": 0
                },
                {
                    "sent": "You can clone part of the graph.",
                    "label": 0
                },
                {
                    "sent": "You can rebuild part of the graph.",
                    "label": 0
                },
                {
                    "sent": "You can take gradients, and then I'm going to talk about shared variables for optimization.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing you can do.",
                    "label": 0
                },
                {
                    "sent": "When you compile, a function is in addition to the inputs and outputs of the callable that you that you have.",
                    "label": 0
                },
                {
                    "sent": "You can also provide substitutions.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you define a graph that.",
                    "label": 0
                },
                {
                    "sent": "I know that computes some.",
                    "label": 0
                },
                {
                    "sent": "Well, the same function that we had earlier, like the dot product and sigmoid that bias and so on.",
                    "label": 0
                },
                {
                    "sent": "But you actually want to apply that.",
                    "label": 0
                },
                {
                    "sent": "Both maybe 2X and 2A normalized version of X.",
                    "label": 0
                },
                {
                    "sent": "Then you can create another another expression from a new input X_ and then say, well, this is my normalized X.",
                    "label": 0
                },
                {
                    "sent": "And then instead of feeding X to my expression here, I want to feed the normalized X.",
                    "label": 0
                },
                {
                    "sent": "And so you can do that with Givens.",
                    "label": 0
                },
                {
                    "sent": "You just say, well.",
                    "label": 0
                },
                {
                    "sent": "Let's compute dots, but instead of computing it from X computed from XN an.",
                    "label": 0
                },
                {
                    "sent": "Now since you need since you need the X_as a starting point, well, use X_as the input instead of X.",
                    "label": 0
                },
                {
                    "sent": "And then you can call FN the same way.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way of achieving pretty much the same thing.",
                    "label": 0
                },
                {
                    "sent": "Is to create to clone part of the graph.",
                    "label": 0
                },
                {
                    "sent": "So you have a new dot and then you output variables and but where X was replaced by the normalized version of X.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, the advantage of cloning that way is that you can do it over and over if you have several variable substitutions to make.",
                    "label": 0
                },
                {
                    "sent": "If you want to substitute something 1st and then you have your valuables in there that you want to do another substitution, or it's a bit more flexible.",
                    "label": 0
                },
                {
                    "sent": "But it creates a whole new sub graph.",
                    "label": 0
                },
                {
                    "sent": "And it used the same results.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something else that's really useful and one of the reason TNO was developed in the 1st place is to be able to derive gradients automatically from arbitrary expressions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The way it works, so the is that we use the backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so if you have, for instance, a function compound function from RN to R with only like a single scalar as an output, that's the composition of like F&G for instance.",
                    "label": 1
                },
                {
                    "sent": "Then you can express the gradients so the partial derivative of serial respect to X at X.",
                    "label": 0
                },
                {
                    "sent": "As the DOT product between two Jacobian matrices, which represent the whole.",
                    "label": 0
                },
                {
                    "sent": "The whole partial derivative of each element of the output with respect to each element of the inputs.",
                    "label": 0
                },
                {
                    "sent": "So here you have.",
                    "label": 0
                },
                {
                    "sent": "Well, the Jacobian for the output, which is basically just just a vector, because this is a function, I mean this one is a function from a vector to a scalar, but this one is a huge matrix.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, we don't need to express that matrix because it would take a huge amount of memory, and usually it's structured in some way that we can take advantage of what we want.",
                    "label": 0
                },
                {
                    "sent": "We don't want that whole.",
                    "label": 0
                },
                {
                    "sent": "Gradient that whole Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "What we want is an expression that for every vector enables us to compute the dot products between that vector and the Jacobian.",
                    "label": 0
                },
                {
                    "sent": "And that's that's the backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "And this is what we.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two in piano in piano.",
                    "label": 0
                },
                {
                    "sent": "Each basic operation like the addition that product the Sigma made.",
                    "label": 0
                },
                {
                    "sent": "Everything that is defined or almost.",
                    "label": 0
                },
                {
                    "sent": "Also defines a symbolic expression.",
                    "label": 0
                },
                {
                    "sent": "For that product.",
                    "label": 0
                },
                {
                    "sent": "Given this symbolic vector that represents the output gradients.",
                    "label": 0
                },
                {
                    "sent": "So how do you use that?",
                    "label": 0
                },
                {
                    "sent": "Well, in the earlier case that we had, so the simple dot product sigmoid and so on, let's define a vector of targets.",
                    "label": 0
                },
                {
                    "sent": "And a scalar cost, that's just.",
                    "label": 0
                },
                {
                    "sent": "The design of the squared error.",
                    "label": 0
                },
                {
                    "sent": "Then we can use the high level function tiano dot grants you provide it with the costs and the variable you want to take the gradient with respect to.",
                    "label": 0
                },
                {
                    "sent": "And what will happen is that it will start from the output of the graph, which is the cost and go back an each time calling.",
                    "label": 0
                },
                {
                    "sent": "The quivalent of that operation.",
                    "label": 0
                },
                {
                    "sent": "On the symbolic, granite starts from one because the gradient of the code was back to, the cost is 1, and from there it back propagates.",
                    "label": 0
                },
                {
                    "sent": "Building knew expressions that represent the gradient with respect to intermediate variables, and if you have an intermediate variable that has two clients that that's being used for two different operations, then the gradients are going to be summed and so.",
                    "label": 0
                },
                {
                    "sent": "If you have non differentiable operations, well it depends on the case.",
                    "label": 0
                },
                {
                    "sent": "And the important thing is that.",
                    "label": 0
                },
                {
                    "sent": "And it goes all the way through the variables that you define the with respect to.",
                    "label": 0
                },
                {
                    "sent": "Hear WNBA.",
                    "label": 0
                },
                {
                    "sent": "And the important thing is that it creates actually a symbolic mathematical expression of the gradients.",
                    "label": 0
                },
                {
                    "sent": "So this is still part of the computation graph.",
                    "label": 0
                },
                {
                    "sent": "You don't, and you build that in advance.",
                    "label": 0
                },
                {
                    "sent": "You have no numerical values yet.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then one you have these expressions for the gradients.",
                    "label": 0
                },
                {
                    "sent": "Then you'd want to use them.",
                    "label": 0
                },
                {
                    "sent": "So how you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, as we as we saw earlier at piano function.",
                    "label": 0
                },
                {
                    "sent": "Can have more than one output, so in that case we can compile at the end of function that takes all the necessary inputs and then computes the cost and the gradients with respect to WND.",
                    "label": 0
                },
                {
                    "sent": "If we gave random values for the targets.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "Execute that and print the cause.",
                    "label": 0
                },
                {
                    "sent": "I don't have the values here, but it's just random values for the moment.",
                    "label": 0
                },
                {
                    "sent": "And since these expressions are symbolic, we can continue building on new expressions as part of the computation graph.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you want to do gradient descent, then you might want to compute an update and update expression from these gradients.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, you know you can multiply that by a fixed learning rate and then compute the update expression and say, well, here's symbolic expressions for mine, UW, and for my newbie.",
                    "label": 0
                },
                {
                    "sent": "And then you can compile a channel function that outputs that instead.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is how it looks.",
                    "label": 0
                },
                {
                    "sent": "So you have lots of where you have the usual inputs plus constants.",
                    "label": 0
                },
                {
                    "sent": "Some computation and then.",
                    "label": 0
                },
                {
                    "sent": "The date expressions for W 4 W&B and the costs.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to update values, you can do it.",
                    "label": 1
                },
                {
                    "sent": "I mean this gives you like two different ways of updating values.",
                    "label": 0
                },
                {
                    "sent": "One thing you can do is output the gradients.",
                    "label": 0
                },
                {
                    "sent": "So called the cost and graph function that we defined.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, that's what cost in Grad, so it outputs The CW and DC DB.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you call that on the values and then you perform gradient descent in the regular Numpy.",
                    "label": 0
                },
                {
                    "sent": "You take this output value and multiply it by the running rate and perform the outputs.",
                    "label": 0
                },
                {
                    "sent": "Sometimes that's what you want to do if you want to use some.",
                    "label": 0
                },
                {
                    "sent": "Some complex optimization techniques like FGS, all those kind of things rather than code that in TNO you just have a function that computes the gradients and then plug that into the Scipy optimize routine.",
                    "label": 0
                },
                {
                    "sent": "Another option is to use Wella Coast, an update and that outputs directly the new value of W&B, but it's not really efficient and it can be cumbersome to have to do that by hand and keep track of that.",
                    "label": 0
                },
                {
                    "sent": "An always handle explicitly these updated values on top of that.",
                    "label": 0
                },
                {
                    "sent": "The input and output variables you define are usually defined as CPU arrays.",
                    "label": 0
                },
                {
                    "sent": "I mean arrays in CPU memory because that's usually what you handle.",
                    "label": 0
                },
                {
                    "sent": "An if you only need these arrays to hold the parameters of your models and update them and not really interact with them, then it will lead to useless transfers between CPU and GPU memory.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to avoid that an.",
                    "label": 0
                },
                {
                    "sent": "We introduce shared variables so shared variables.",
                    "label": 0
                },
                {
                    "sent": "They are symbolic variables first of all, but they have a persistent value associated to them.",
                    "label": 0
                },
                {
                    "sent": "It's persistent, but it's mutable, so it's persistent across different function calls, and it's also shared between different compiled handle functions.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have the parameters of your models in in shared variables, then the function for training and for evaluating the performance of your model will use the same value.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do anything special, it will just use the value for that variable.",
                    "label": 0
                },
                {
                    "sent": "An it has to be an input variable.",
                    "label": 0
                },
                {
                    "sent": "Because it has already a value, it's not the output of some computation, so it needs to be an input variable.",
                    "label": 0
                },
                {
                    "sent": "An it's going to be an implicit input to every function that needs it.",
                    "label": 0
                },
                {
                    "sent": "So that means you don't have to provide it explicitly as an input variable to your training or validation function.",
                    "label": 0
                },
                {
                    "sent": "Each time that you are using an expression that depends on the shared variable, it's going to be added as an implicit inputs an.",
                    "label": 0
                },
                {
                    "sent": "Its value is going to be retrieved each time you call the function itself.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example of how you might want to use it.",
                    "label": 0
                },
                {
                    "sent": "So still the same basic computation you have X&Y, which represent your data, so you don't want them as a shared variable, usually because it's going to change each time you call the function.",
                    "label": 0
                },
                {
                    "sent": "But that time we define WNB as shared variable.",
                    "label": 0
                },
                {
                    "sent": "And since the shared variable has a value associated to it, we need to create it with an existing value.",
                    "label": 1
                },
                {
                    "sent": "So here we start from the default value that we sample.",
                    "label": 0
                },
                {
                    "sent": "Last time like just random weights and zero bias.",
                    "label": 0
                },
                {
                    "sent": "And then you build your computation graph exactly the same way as as before.",
                    "label": 0
                },
                {
                    "sent": "We created a product output and so on.",
                    "label": 0
                },
                {
                    "sent": "And when it's done, when you create the function here, you see that we don't have to provide WNBA anymore.",
                    "label": 0
                },
                {
                    "sent": "I mean W here and the value and B here because their implicit inputs.",
                    "label": 0
                },
                {
                    "sent": "And if you call F on only exe file Angiomax while you get the same values as before.",
                    "label": 0
                },
                {
                    "sent": "If you need to change the value of a shared variable in between calls to.",
                    "label": 0
                },
                {
                    "sent": "To compile channel functions, you can call get value and set value.",
                    "label": 1
                },
                {
                    "sent": "Shared variables do not need to be of fixed shape either, so you can have calls to get values and set value that add rows to a matrix or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to update shared variables, well, one of the ways is to use get value into use getvalue and setvalue.",
                    "label": 1
                },
                {
                    "sent": "But it doesn't really bring anything more than your way of handling those values explicitly.",
                    "label": 0
                },
                {
                    "sent": "But what we can have is a function that also computes update value or new value to update shared variables with.",
                    "label": 1
                },
                {
                    "sent": "So for instance, again still doing gradient descent on the same function will compute the costs, compute tiano grad of C with respect to W&B.",
                    "label": 0
                },
                {
                    "sent": "By the way, it's more efficient to have only one call to Theano grad, because it will.",
                    "label": 0
                },
                {
                    "sent": "Do only one traversal of the graph nodes, like one for the value and then one for B.",
                    "label": 1
                },
                {
                    "sent": "And you compute the update values.",
                    "label": 0
                },
                {
                    "sent": "The symbolic expression for the updates.",
                    "label": 0
                },
                {
                    "sent": "And then call channel function with inputs, outputs an updates, so the updates are pairs of shared variables and symbolic expression for the updates.",
                    "label": 0
                },
                {
                    "sent": "So here we have W an here.",
                    "label": 0
                },
                {
                    "sent": "This amounts to W minus learning rates, times, gradients and same for me.",
                    "label": 0
                },
                {
                    "sent": "So again, W. NBR, implicit inputs, and in that case update W. An update B.",
                    "label": 1
                },
                {
                    "sent": "Are treated as implicit outputs of the Theano function, so they will not be returned when you call.",
                    "label": 0
                },
                {
                    "sent": "Here cost and performance updates in perform updates, but they will be computed.",
                    "label": 0
                },
                {
                    "sent": "At the same time as C. And then after everything, every output is computed.",
                    "label": 0
                },
                {
                    "sent": "Then the update is going to be performed and the value of the shared variable is actually going to change.",
                    "label": 0
                },
                {
                    "sent": "So first we compute all the outputs and then we perform the updates.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An if we visualize the graph, this is how it looks like.",
                    "label": 0
                },
                {
                    "sent": "So you have in pale blue the shared variables.",
                    "label": 0
                },
                {
                    "sent": "Which are treated are kind of like inputs because their inputs to like this operation and this operation and so on.",
                    "label": 0
                },
                {
                    "sent": "And here we have the output.",
                    "label": 0
                },
                {
                    "sent": "That the function computes the cost itself, and here the update expression for the bias and for the wait.",
                    "label": 0
                },
                {
                    "sent": "And you can see that.",
                    "label": 0
                },
                {
                    "sent": "I mean you cannot really read it here, I guess, but here is written updates.",
                    "label": 0
                },
                {
                    "sent": "So it means that, well, this is really like the update expression for that and the blue dark blue arrow here shows that this operation is a view and the right one shows that this is an inplace operation.",
                    "label": 0
                },
                {
                    "sent": "And it's really nice because it means that this operation.",
                    "label": 0
                },
                {
                    "sent": "Happens in place on one of its inputs, which is this one, and that means that we have our shared variable in GPU memory, for instance, and the update expression.",
                    "label": 0
                },
                {
                    "sent": "When it's computed, overwrites the memory in place and so you don't have a copy of memory.",
                    "label": 0
                },
                {
                    "sent": "You don't have the duplicate amount of memory usage and so on, and you can see that in that case it's pretty efficient.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about the different features that we have that enabled fast runtime performance for 40 and compile.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Part of it comes from graph optimizations.",
                    "label": 0
                },
                {
                    "sent": "And I've talked briefly about it at the beginning when I showed you the updated expression with the C generally and so on.",
                    "label": 0
                },
                {
                    "sent": "But basically we have a graph rewriting framework in piano an that takes the graph that you created.",
                    "label": 0
                },
                {
                    "sent": "And then perform some replacements in it.",
                    "label": 0
                },
                {
                    "sent": "That still have the same meaning that still correspond to the same mathematical expression.",
                    "label": 0
                },
                {
                    "sent": "And so in particular, the values should not change or not match.",
                    "label": 0
                },
                {
                    "sent": "And the type has to be the same and at runtime the shape has to be the same and so.",
                    "label": 0
                },
                {
                    "sent": "So we have different goals for optimization.",
                    "label": 0
                },
                {
                    "sent": "Some of them merge equivalent computation.",
                    "label": 0
                },
                {
                    "sent": "If you define those same operation twice or more, then you don't want it to be computed that many times.",
                    "label": 0
                },
                {
                    "sent": "Just want the system to realize that it's really like a + B.",
                    "label": 0
                },
                {
                    "sent": "Here an A+B.",
                    "label": 0
                },
                {
                    "sent": "He ran the nature of a plus B defines the same A+B.",
                    "label": 0
                },
                {
                    "sent": "You want to simplify expressions like something.",
                    "label": 0
                },
                {
                    "sent": "Time Zero will be zero X /, X will be one and so on.",
                    "label": 0
                },
                {
                    "sent": "In some cases it optimizes things that.",
                    "label": 0
                },
                {
                    "sent": "Give surprising results, like for instance if you add a matrix full of zeros to a matrix of a different shape.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe after optimization it will say, oh, we're only adding zeros.",
                    "label": 0
                },
                {
                    "sent": "It does not matter and you'll get no no error in the default full optimized mode.",
                    "label": 0
                },
                {
                    "sent": "But you can always deactivate that and.",
                    "label": 0
                },
                {
                    "sent": "If you want to debug, we have numerical stability optimizations.",
                    "label": 0
                },
                {
                    "sent": "For instance to call like log 1P instead of log of 1 + X.",
                    "label": 0
                },
                {
                    "sent": "If you have like the log of a softmax or something, then.",
                    "label": 0
                },
                {
                    "sent": "Depending of what something is, sometimes you can.",
                    "label": 0
                },
                {
                    "sent": "You can simplify it that.",
                    "label": 0
                },
                {
                    "sent": "That way if you have a log of sigmoid then can be replaced with a soft plus if you so you have optimizations like that.",
                    "label": 0
                },
                {
                    "sent": "It's also the optimization framework that will insert in place and destructive operations.",
                    "label": 0
                },
                {
                    "sent": "So you can have operations that our view of their inputs and just read just a DS output memory to the memory of some inputs.",
                    "label": 0
                },
                {
                    "sent": "Annual have actual destructive operation that will overwrite some variables and basically The thing is that you can overwrite memory, but only if you're the last operation that needs that value.",
                    "label": 0
                },
                {
                    "sent": "So we have some analysis, some static analysis of the computation graph that basically defines, well, OK. Who is going to be executive 1st and then the last one when no one needs that value can work in place and district in disruptive fashion?",
                    "label": 0
                },
                {
                    "sent": "Or if it's not possible, then make a copy and then work in place on the copy.",
                    "label": 0
                },
                {
                    "sent": "We have optimized routines, for instance, from glass, that will combine operations like for instance JMV.",
                    "label": 0
                },
                {
                    "sent": "Anjem will do metrics or matrix vector multiplication, or that product and scaling an accumulation into a buffer.",
                    "label": 0
                },
                {
                    "sent": "So it's really several operations in the original computation graph that will get put together in only one operation.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's one operation.",
                    "label": 0
                },
                {
                    "sent": "In the initial computation graph that gets splits into more than one, like for instance, take the shape of this and then allocate memory of that shape and then do something with it rather than copy that valuable like, compute some variable and then compute something else and then.",
                    "label": 0
                },
                {
                    "sent": "Just multiply it with zero so that we have something with the right shape.",
                    "label": 0
                },
                {
                    "sent": "So we have some shape inference.",
                    "label": 0
                },
                {
                    "sent": "An we track basically some shapes and shape equivalents, so we're able to say OK. Well, this is like the same as like the shape of like the shape of the first dimension of this.",
                    "label": 0
                },
                {
                    "sent": "So it's like the batch size and we're using a different places and it can enable some simplifications.",
                    "label": 0
                },
                {
                    "sent": "We have constant folding, so if you define complex expressions and then substitute constants.",
                    "label": 0
                },
                {
                    "sent": "For their inputs, then we can precompute things during the the phase, and we also have the transfer to the GPU they transferred the computation to.",
                    "label": 0
                },
                {
                    "sent": "The GPU happens during the graph optimization phase, because basically the dot product of A2 CPU matrices is 1 operation.",
                    "label": 0
                },
                {
                    "sent": "But we also have like a separate GPU dot product that takes GPU arrays as inputs.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So we have to manage that transfer, but this enables us to have like part of the computation kept on CPU.",
                    "label": 0
                },
                {
                    "sent": "If there's no GPU equivalent, and still some other parts transfer to GPU.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have a couple of predefined modes that define how the runtime is going to be executed and also which optimizations are going to be included.",
                    "label": 0
                },
                {
                    "sent": "For instance, should I use?",
                    "label": 0
                },
                {
                    "sent": "Like the first run is the default.",
                    "label": 0
                },
                {
                    "sent": "It will include most of the optimization.",
                    "label": 1
                },
                {
                    "sent": "It will include CPU and GPU code generation and compilation and so on.",
                    "label": 0
                },
                {
                    "sent": "We have fast compile like mode fast compile which minimize completely the the compilation times so it does not generate any CPU code, it just uses the Python backends.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when you want to provide some too.",
                    "label": 0
                },
                {
                    "sent": "To prototype something really quick, it can be useful.",
                    "label": 0
                },
                {
                    "sent": "We have optimized or fast compile, which enables the cogeneration for CPU and GPU but does not apply all the graph optimizations.",
                    "label": 1
                },
                {
                    "sent": "In particular, the memory usage can be higher overall using optimizer fast compiler makes the compilation phase, so the graph optimization phase in particular much faster, but end costs between like 2750% of performance which sometimes can be worth it.",
                    "label": 0
                },
                {
                    "sent": "An we have things like debug mode which is extremely slow and just checks everything.",
                    "label": 0
                },
                {
                    "sent": "Checks that every optimizing that every optimization, for instance gives the same numerical results as before the optimization, which is useful mostly for developers.",
                    "label": 0
                },
                {
                    "sent": "And then you can also enable and disable specific optimizations like if you if you.",
                    "label": 0
                },
                {
                    "sent": "I suspect that some optimization is causing some trouble.",
                    "label": 0
                },
                {
                    "sent": "You can disable it.",
                    "label": 0
                },
                {
                    "sent": "You can also manually include unsafe optimizations that are not there by default.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have optimization that insert couple of assert nodes to check that, for instance, their shapes are still consistent after the simplification.",
                    "label": 0
                },
                {
                    "sent": "But if you know that it's working fine and you want that extra speed up, you can explicitly specify optimizer including unsafe, which is just like the first thing.",
                    "label": 0
                },
                {
                    "sent": "And that can be done globally.",
                    "label": 0
                },
                {
                    "sent": "That can be done individually for different compiled funk.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So something.",
                    "label": 0
                },
                {
                    "sent": "Important that we do is have C code for the operations.",
                    "label": 0
                },
                {
                    "sent": "And basically it means that you define.",
                    "label": 0
                },
                {
                    "sent": "Possibly optimized C code that interacts with the Python object, the Python arrays or GPU arrays, and so on that you have in memory.",
                    "label": 0
                },
                {
                    "sent": "And that's provide the.",
                    "label": 0
                },
                {
                    "sent": "The actual C code followed computation.",
                    "label": 0
                },
                {
                    "sent": "It can call external libraries, for instance.",
                    "label": 0
                },
                {
                    "sent": "That's how we use the different blast function and so on.",
                    "label": 0
                },
                {
                    "sent": "And so you can have a look at a couple of examples, But basically you write code to extract, slide the right memory regions from the NUM PY arrays.",
                    "label": 0
                },
                {
                    "sent": "It's using the NUM py C API to access values.",
                    "label": 0
                },
                {
                    "sent": "Do the computation, puts the values in the right container.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "When you have that code during the compilation phase, if it's needed, then it's going to stitch together like boilerplate code plus the one you wrote and generate it as a Python module.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's defining a new Python modules written in C++.",
                    "label": 0
                },
                {
                    "sent": "It's compiled by G+ Plus and imported and loaded back into Python so you can interact with it from Python.",
                    "label": 0
                },
                {
                    "sent": "And for GPU codes, most of the time it's the same process but using CUDA and MVC instead of C++ and plus plus.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we also have a runtime environment that will be in charge of executing all of these operations, so it's going to.",
                    "label": 0
                },
                {
                    "sent": "Take the input values, check what is needed to compute the output.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you have like lazy operations like the FLS and it will say OK. Well, let's compute this and that like.",
                    "label": 0
                },
                {
                    "sent": "These outputs are always need an this condition.",
                    "label": 0
                },
                {
                    "sent": "I'll always needs an compute the thing in the right order.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And then check well which additional things it needs to compute.",
                    "label": 0
                },
                {
                    "sent": "Put everything in NUM PY arrays for the output, and return that.",
                    "label": 0
                },
                {
                    "sent": "And we have Python version of that, and we have also a version written in C, which means that if all the the OPS in your compiled graph have C code, then you can basically.",
                    "label": 0
                },
                {
                    "sent": "Leave the Python runtime for awhile and just execute callbacks and C functions, and you can do that without the overhead of running Python from C or C from Python And doesn't look like much, but it actually can make a difference in the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the measured runtime.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "If you want to use the GPU?",
                    "label": 0
                },
                {
                    "sent": "Well, we wanted to make it as transparent as possible so.",
                    "label": 1
                },
                {
                    "sent": "You define your graph the same way as on CPU.",
                    "label": 0
                },
                {
                    "sent": "And we have a switch.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a configuration flag that enables you to say, well, I'd like to use.",
                    "label": 0
                },
                {
                    "sent": "That's GPU an.",
                    "label": 0
                },
                {
                    "sent": "What this does is that all the inputs.",
                    "label": 0
                },
                {
                    "sent": "I mean it means that the shared variables that creates will be created on GPU by default.",
                    "label": 1
                },
                {
                    "sent": "And all the inputs are going to be transferred to GPU.",
                    "label": 0
                },
                {
                    "sent": "All the operation.",
                    "label": 0
                },
                {
                    "sent": "Things that have a GPU implementation will get transferred to the GPU as well and applied on the GPU version of your variables.",
                    "label": 0
                },
                {
                    "sent": "And the GPU CUDA code will be computed will be generated and compiled and loaded.",
                    "label": 1
                },
                {
                    "sent": "And in the end.",
                    "label": 0
                },
                {
                    "sent": "Everything will work transparently.",
                    "label": 1
                },
                {
                    "sent": "There's still a couple of things to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "We have a legacy back end which is still the one used in the last release of Theano that only supports one GPU at the time, and that only supports float 32 as a D type, but the new one, which is completely functional an is available in the development branch and we hope to have a new release soon featuring it.",
                    "label": 1
                },
                {
                    "sent": "This supports most of the types.",
                    "label": 0
                },
                {
                    "sent": "Then the issue is whether the hardware supports them and support them well and will it be fast.",
                    "label": 0
                },
                {
                    "sent": "So usually you still want to use 432 as much as you can, at least for floating point computation.",
                    "label": 0
                },
                {
                    "sent": "We have limited support for float 16 for storage, so the computation is still going to be done in float 32 at the moment, but then it's going to be down casted as float 16 on GPU's that support it and it's useful too.",
                    "label": 0
                },
                {
                    "sent": "To spare bandwidth and storage space on the GPU.",
                    "label": 0
                },
                {
                    "sent": "However, you have to be careful about the precision, because depending on your algorithm.",
                    "label": 0
                },
                {
                    "sent": "Rounding errors can become can.",
                    "label": 0
                },
                {
                    "sent": "Can make the difference between like due to rounding error.",
                    "label": 0
                },
                {
                    "sent": "Float 16 might not train at all even if the code is actually technically correct.",
                    "label": 0
                },
                {
                    "sent": "So if you want to make sure that you use flow 32 or then.",
                    "label": 0
                },
                {
                    "sent": "To easily switch between precisions, we have especially type called float X.",
                    "label": 0
                },
                {
                    "sent": "And which is governed by a global flag.",
                    "label": 0
                },
                {
                    "sent": "So if no taxes for 32, then by default like all the all the sensors that created with like matrix or vector and so on are going to be flat X.",
                    "label": 0
                },
                {
                    "sent": "So if it's 432 into 2 if it's not 64 zero 64.",
                    "label": 0
                },
                {
                    "sent": "But if you need some part of the graph to like always be double precision.",
                    "label": 0
                },
                {
                    "sent": "Or if you need some part of the graph to always be single precision, then you can always explicitly use like F matrix for float or D matrix for double or matrix D types equal explicitly what you want, it's just syntactic sugar to help you have the same model work on.",
                    "label": 0
                },
                {
                    "sent": "On different precision.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Speaking of configuration flags, we have a config module that lets you configure how you want to have you on piano to behave like you can specify which default device you want to use, like for instance, the default is CPU.",
                    "label": 1
                },
                {
                    "sent": "If you're using the CUDA back end and you won't find probably like CUDA CUDA 0 if you want to change float X, you can specify like that and you can also specify like the default compiler compilation mode.",
                    "label": 0
                },
                {
                    "sent": "You can specify which optimization to add or remove.",
                    "label": 0
                },
                {
                    "sent": "You can define lots of things including like behavior during some kind of errors, and so there are three ways to set set those kind of configuration flags.",
                    "label": 0
                },
                {
                    "sent": "The first which.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, the first is by changing and the environment variable channel flags.",
                    "label": 0
                },
                {
                    "sent": "The second one is directly in Python.",
                    "label": 0
                },
                {
                    "sent": "We just set things to tiano config.",
                    "label": 0
                },
                {
                    "sent": "Whatever he wants, this does not work for all the options because some of them need to not change and value needs to be constant throughout the whole execution of the process.",
                    "label": 0
                },
                {
                    "sent": "In that case, if you try that, you get an error message and you have the.",
                    "label": 0
                },
                {
                    "sent": "The configuration file, just like configuration file with sections an you so the in order of presidents like the first thing that's going to be checked for is the configuration file.",
                    "label": 0
                },
                {
                    "sent": "Then what's in the configuration file can get overwritten by the flags.",
                    "label": 0
                },
                {
                    "sent": "An then at runtime you can even change the value several times between some things by playing directly in the other config.",
                    "label": 0
                },
                {
                    "sent": "Then four.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the question was if you can write yes or I have to repeat for the recording.",
                    "label": 0
                },
                {
                    "sent": "So the question was if you already have C code and it's working fast, can you?",
                    "label": 1
                },
                {
                    "sent": "Have an additional speedup by using the GPU, that's right.",
                    "label": 0
                },
                {
                    "sent": "So if you have GPU codes in Python, maybe maybe I should answer your question later.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to use the GPU.",
                    "label": 0
                },
                {
                    "sent": "To get additional speedup, you need to write CUDA codes I guess for that specific operation, but this is.",
                    "label": 0
                },
                {
                    "sent": "I mean, This is why we have piano is that you can, if you can express your model in Mumbai or something close to NUM py, you can use Theano to express.",
                    "label": 0
                },
                {
                    "sent": "You can use the arrow to express the model that you need.",
                    "label": 0
                },
                {
                    "sent": "Then Jenna will automatically.",
                    "label": 0
                },
                {
                    "sent": "Translates that as.",
                    "label": 0
                },
                {
                    "sent": "CUDA codes that will execute on the GPU if I hope it answers your questions.",
                    "label": 0
                },
                {
                    "sent": "If it did not, please let's, let's take that offline afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For the, for the last part, I'm going to brush over more advanced topics that you'll have the occasion to delve deeper into tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, loops in Theano and symbolic loops.",
                    "label": 0
                },
                {
                    "sent": "So the approach we've taken for that is basically to encapsulate the whole loop.",
                    "label": 0
                },
                {
                    "sent": "As one operation.",
                    "label": 0
                },
                {
                    "sent": "And this enables us to have like variable number of iterations to have things like early exits or of the loop.",
                    "label": 0
                },
                {
                    "sent": "To have the number of steps be a symbolic variable to have yeah stopping condition.",
                    "label": 0
                },
                {
                    "sent": "Symbolic stopping condition that gets evaluated at each iteration to know how many iterations will be executed in total.",
                    "label": 0
                },
                {
                    "sent": "And the way it works is so you have one symbolic node in the graph.",
                    "label": 0
                },
                {
                    "sent": "It's up is scan.",
                    "label": 0
                },
                {
                    "sent": "It's all scan an inside that node.",
                    "label": 0
                },
                {
                    "sent": "That's basically a whole separate Theano sub graph.",
                    "label": 0
                },
                {
                    "sent": "And compiled gender function and what that sub graph represents is the computation that has to be done at each iteration of the loop.",
                    "label": 0
                },
                {
                    "sent": "So if of the loop.",
                    "label": 0
                },
                {
                    "sent": "All you have is like an addition.",
                    "label": 0
                },
                {
                    "sent": "Then you'll have a really small channel graph in there.",
                    "label": 0
                },
                {
                    "sent": "We'd like 2 inputs and one output an an addition.",
                    "label": 0
                },
                {
                    "sent": "And this is going to be compiled as an optimized general function, potentially transfer to GPU if it's if it's feasible.",
                    "label": 0
                },
                {
                    "sent": "And then when the execution of that node begins, then the scan node itself will iterate through its inputs.",
                    "label": 1
                },
                {
                    "sent": "If there are sequences or just take always the same value if you have constants and call the compiled inner function.",
                    "label": 0
                },
                {
                    "sent": "On the appropriate slice.",
                    "label": 0
                },
                {
                    "sent": "Inputs and compute the output and feed that either to the output that you're required or to the next iteration in like a temporary buffer or something and call it over and over again an if you have a stopping condition, then it will also compute the stopping condition and check whether it needs to stop and exit at that point.",
                    "label": 0
                },
                {
                    "sent": "And the fact that we have this symbolic node.",
                    "label": 0
                },
                {
                    "sent": "Enables us easily to take gradients through it as well, so the scan operation also defines a graph function like any regular node in the graph.",
                    "label": 1
                },
                {
                    "sent": "And that scan is basically a loop in reverse.",
                    "label": 0
                },
                {
                    "sent": "Over well, the same number of iterations, reverting the sequences, feeding it like regions with respect to outputs and accumulating the right thing at the right place, and so on.",
                    "label": 0
                },
                {
                    "sent": "An basically you type A scan node for the forward.",
                    "label": 0
                },
                {
                    "sent": "Computation of your loop and when you call grad and so on, you'll get another scan computation on like the reverse.",
                    "label": 0
                },
                {
                    "sent": "Sequences and so on.",
                    "label": 0
                },
                {
                    "sent": "And this implements regular backdrops, full time with a full accumulation of gradients.",
                    "label": 0
                },
                {
                    "sent": "And it has a GPU implementation optimizations that move.",
                    "label": 0
                },
                {
                    "sent": "The symbolic scan onto GPU and in a graph on GPU and so on exists.",
                    "label": 0
                },
                {
                    "sent": "And there are also additional optimizations that make it possible, for instance to move invariant parts out of the loop, like if you define something that only involves things that are not sequences and that you.",
                    "label": 0
                },
                {
                    "sent": "Just recompute at each timestep.",
                    "label": 0
                },
                {
                    "sent": "Then you want that to be moved out of the loop, or in some cases if you have sequential things that are not really sequential, like for instance, if you compute the DOT product.",
                    "label": 0
                },
                {
                    "sent": "Of a different vector at each timestep and always the same matrix.",
                    "label": 0
                },
                {
                    "sent": "It's basically just a matrix matrix product, so you can move that part out of the scan and have only one matrix matrix product instead of North vector matrix products and so on.",
                    "label": 0
                },
                {
                    "sent": "So we have those things also.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we only need the last.",
                    "label": 0
                },
                {
                    "sent": "Iteration.",
                    "label": 0
                },
                {
                    "sent": "Of the loop.",
                    "label": 0
                },
                {
                    "sent": "Then in some cases we can.",
                    "label": 0
                },
                {
                    "sent": "We can forget about the intermediate values for that state, so we have optimizations to reduce memory usage for that.",
                    "label": 0
                },
                {
                    "sent": "However, sometimes you need if you plan to take a gradient through that loop.",
                    "label": 1
                },
                {
                    "sent": "Sometimes you'll need all those intermediate variables to be able to do the backdrop through time an in that case, well, I still need that amount of memory.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Does it make a difference if you scan or reported?",
                    "label": 0
                },
                {
                    "sent": "So the question was, apart from the conditional ending, does it make a difference to you scan all the for loop so well you have the symbolic number of iterations that is known before executing the loop, but not at compilation time.",
                    "label": 0
                },
                {
                    "sent": "So that makes a difference in that case as well in practice.",
                    "label": 0
                },
                {
                    "sent": "The graph will not be the same at all.",
                    "label": 0
                },
                {
                    "sent": "So in one case you have like 1 node with the loop implicitly inside and in the other case if you use the follow up then you'll have many instances of what happens in the loop, so it can have an impact on the memory usage.",
                    "label": 0
                },
                {
                    "sent": "It can have an impact on the computation time, but it's really depends on your problem and it's hard to predict which one is going to be faster at runtime and which one is going to be faster.",
                    "label": 0
                },
                {
                    "sent": "At execution time.",
                    "label": 0
                },
                {
                    "sent": "Like for instance, the bunch of vector matrix operations into a matrix matrix operation.",
                    "label": 0
                },
                {
                    "sent": "This will not happen if you want all the loop.",
                    "label": 0
                },
                {
                    "sent": "Just because currently we don't have an optimization for that.",
                    "label": 0
                },
                {
                    "sent": "We could add 1.",
                    "label": 0
                },
                {
                    "sent": "There's nothing in principle that prevents stats, but.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you have a doubt, usually the only actual answer is try both and see which one works best for your case.",
                    "label": 0
                },
                {
                    "sent": "So here's a small example of how you would.",
                    "label": 1
                },
                {
                    "sent": "Define those small case of a loop that actually needs to be sequential.",
                    "label": 0
                },
                {
                    "sent": "It's just loop with accumulation.",
                    "label": 0
                },
                {
                    "sent": "Into does need to be so yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we start by defining, well, the inputs that we have.",
                    "label": 0
                },
                {
                    "sent": "So we have one scalar that's an integer, one vector that's of floating point type it's protector.",
                    "label": 0
                },
                {
                    "sent": "You call.",
                    "label": 0
                },
                {
                    "sent": "That Lambda function FN is basically the function that needs to be executed at each iteration.",
                    "label": 0
                },
                {
                    "sent": "And in that case, it's a multiplication of the prior result by.",
                    "label": 0
                },
                {
                    "sent": "The current vector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That we provide, we have to provide the initial values, which in that case is just once.",
                    "label": 0
                },
                {
                    "sent": "We say that A is a non sequence, so we use the same A over and over again.",
                    "label": 0
                },
                {
                    "sent": "And so the other one, this one for your result is the recurrent States and we provide the number of steps.",
                    "label": 1
                },
                {
                    "sent": "In the end we just say we want only the final results, so it will override the intermediate results and then we call Tina function with these inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "The updates here are mainly if there are random variables or if you sample random numbers inside the loop, because if you sample random numbers inside the loop you want to update the random random number generator with the right amount.",
                    "label": 0
                },
                {
                    "sent": "In the end, so this is basically just why there's update here.",
                    "label": 0
                },
                {
                    "sent": "It's always good practice to use it even if you don't sample.",
                    "label": 0
                },
                {
                    "sent": "And yeah, and then if you have.",
                    "label": 0
                },
                {
                    "sent": "You just call that compiled function with, well, a.",
                    "label": 0
                },
                {
                    "sent": "This range from zero to 9 K is too, and you see that.",
                    "label": 0
                },
                {
                    "sent": "It computes the square of that vector by just multiplying with itself.",
                    "label": 0
                },
                {
                    "sent": "K times.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next section is about the tools we have to help.",
                    "label": 0
                },
                {
                    "sent": "Investigating what happens and debugging and diagnosing.",
                    "label": 0
                },
                {
                    "sent": "The reasons we need that is because since we separate the definition of the computation and the computation graph from the actual execution.",
                    "label": 0
                },
                {
                    "sent": "And since we define the graph only one but once, but we can executed lots of time, then we need some tools to help reconcile both and help make the link between.",
                    "label": 0
                },
                {
                    "sent": "Why do I have that error at runtime when it didn't complain at compile time, for instance?",
                    "label": 0
                },
                {
                    "sent": "Or if you're training models or if you're calling the same function over and over again, and at some point something bad happens then?",
                    "label": 0
                },
                {
                    "sent": "We want to have tools to help investigate that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We will dig into that deeper.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow for those who are interested right now, I can just show you small demo of.",
                    "label": 0
                },
                {
                    "sent": "A notebook showing well.",
                    "label": 0
                },
                {
                    "sent": "Regular examples of.",
                    "label": 0
                },
                {
                    "sent": "Like the kind of error message you might expect and what that means and how to interpret that.",
                    "label": 0
                },
                {
                    "sent": "So let's say I. I executes that.",
                    "label": 0
                },
                {
                    "sent": "Statement.",
                    "label": 0
                },
                {
                    "sent": "Let's not worry too much about what's happening in there yet.",
                    "label": 0
                },
                {
                    "sent": "The error will pop out.",
                    "label": 0
                },
                {
                    "sent": "But basically there are four parts in their message that stereo.",
                    "label": 0
                },
                {
                    "sent": "First of all, there's.",
                    "label": 0
                },
                {
                    "sent": "The stack trace itself that comes from Python or Ipython, or whatever interpreter you're using that tells you where during the execution.",
                    "label": 0
                },
                {
                    "sent": "Things went wrong.",
                    "label": 0
                },
                {
                    "sent": "Then you'll have the error message itself with a small description of like what's wrong.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you can have additional details, and sometimes you can have more information and I'll just see.",
                    "label": 0
                },
                {
                    "sent": "K. Oh, here it is.",
                    "label": 0
                },
                {
                    "sent": "Screen this.",
                    "label": 0
                },
                {
                    "sent": "So first you have the stack trace from my Python.",
                    "label": 0
                },
                {
                    "sent": "The screen is kind of narrow, so it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not very pretty, but usually you have it.",
                    "label": 0
                },
                {
                    "sent": "You have the full lines, it tells it's a value error.",
                    "label": 0
                },
                {
                    "sent": "It shows what happens during the electrician, but that's not really helpful.",
                    "label": 0
                },
                {
                    "sent": "Then you have the actual error message that says input dimension mismatch.",
                    "label": 0
                },
                {
                    "sent": "So is well, the shape of 1 input is 2.",
                    "label": 0
                },
                {
                    "sent": "An another input shape is 3.",
                    "label": 0
                },
                {
                    "sent": "And this happened in some automized node.",
                    "label": 0
                },
                {
                    "sent": "That's composite operation of like something and this like.",
                    "label": 0
                },
                {
                    "sent": "In addition here and the product here.",
                    "label": 0
                },
                {
                    "sent": "Control and then so this shows like what the inputs are.",
                    "label": 0
                },
                {
                    "sent": "It's additional information on the operation and its inputs.",
                    "label": 0
                },
                {
                    "sent": "In that case, you see that the shape of the input well one is 2 and one is 3, and the values because these are short values, we can show them one is 11111 an you say OK. Maybe?",
                    "label": 0
                },
                {
                    "sent": "There's a shape mismatch somewhere.",
                    "label": 0
                },
                {
                    "sent": "Seems obvious, but you don't really know where an you don't know.",
                    "label": 0
                },
                {
                    "sent": "Is it like this operation?",
                    "label": 0
                },
                {
                    "sent": "Is it the second one?",
                    "label": 0
                },
                {
                    "sent": "Is there something else what's happening?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To know that you have a hint here.",
                    "label": 0
                },
                {
                    "sent": "And that says running with optimization disabled can give a backtrace of when the.",
                    "label": 0
                },
                {
                    "sent": "Problematic operation was actually created.",
                    "label": 0
                },
                {
                    "sent": "And it says you can do that with the flag optimizer fast compile and if you.",
                    "label": 0
                },
                {
                    "sent": "Can also use optimizer none an if you want more information you can use exception for both the high.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you do that again.",
                    "label": 0
                },
                {
                    "sent": "But in that case.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Mode equals tiano mode.",
                    "label": 0
                },
                {
                    "sent": "Optimizerx equals, let's say fast, compact.",
                    "label": 0
                },
                {
                    "sent": "And try to execute that.",
                    "label": 0
                },
                {
                    "sent": "You'll see well the same informative stack trace that yeah, it happens during the execution.",
                    "label": 0
                },
                {
                    "sent": "The same error input dimension mismatch 2, three and so on.",
                    "label": 0
                },
                {
                    "sent": "But then it tells you backtrace when the node is created.",
                    "label": 0
                },
                {
                    "sent": "And so it's somewhere in the Ipython notebook obviously, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And here it says in the end.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's on line 11.",
                    "label": 0
                },
                {
                    "sent": "And it said equals that times Y.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually go back here and you say it's not that line.",
                    "label": 0
                },
                {
                    "sent": "It's actually that line that has an issue.",
                    "label": 0
                },
                {
                    "sent": "And then well, of course.",
                    "label": 0
                },
                {
                    "sent": "The case is simple enough that you know that, well, you assign something of size 22X and something of size 3 to Y.",
                    "label": 0
                },
                {
                    "sent": "Well, X + X will work, but set times Y will not.",
                    "label": 0
                },
                {
                    "sent": "So there are other useful tools like well, debug mode that I mentioned that double checks and triple checks everything.",
                    "label": 0
                },
                {
                    "sent": "You can use the Python debugger to have an interactive Python debugging session in the middle of your model or during execution.",
                    "label": 0
                },
                {
                    "sent": "If you want to check things, you can have breakpoints during execution where you define conditions to check an if.",
                    "label": 0
                },
                {
                    "sent": "Those conditions are met, then Tina will drop into my DB prompt where you can look around at what the values currently are from the select from some selected Bibles.",
                    "label": 0
                },
                {
                    "sent": "You can print things during execution using the print up.",
                    "label": 0
                },
                {
                    "sent": "You can print such tributes or values are things to help you detect Nan and Infinity and large values.",
                    "label": 0
                },
                {
                    "sent": "These values are useful when you want to compute at the same time as you build the graph, so it lets you specify test values for all the input variables that you have.",
                    "label": 0
                },
                {
                    "sent": "It will use the initial value of shared variables as the default value for the shared variables, and each time you create a new symbolic node like.",
                    "label": 0
                },
                {
                    "sent": "In addition, the dot donations on right at the same time as creating the symbolic variable.",
                    "label": 0
                },
                {
                    "sent": "It will try to compute value for that variable.",
                    "label": 0
                },
                {
                    "sent": "Given the test value for its inputs and so on, so that means it's especially useful if you have like shape mismatch issues or things like that, because it will try to execute that the computation at least once and so right away you'll have your shape mismatch error when building your graph, and you don't have to wait until the whole graph is built and the gradient is compiled and the graph is optimized and the code is generated to then.",
                    "label": 0
                },
                {
                    "sent": "Plug in your input values and realize that.",
                    "label": 0
                },
                {
                    "sent": "You're missing your transpose somewhere in the middle of a graph.",
                    "label": 0
                },
                {
                    "sent": "And plus couple of other things that that exists, but let's let's go back to.",
                    "label": 0
                },
                {
                    "sent": "The presentation itself.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jenna has been designed so that it's easy to extend it and to add new functionality's new operations, especially if you have specialized or optimized libraries or third party libraries that exists and that you want to easily wrap in Python, for instance, then it's a couple of lines of codes to just wrap an existing operation that has Python bindings and that works on Empire Azor.",
                    "label": 0
                },
                {
                    "sent": "Or could I raise or things like that.",
                    "label": 0
                },
                {
                    "sent": "To just.",
                    "label": 0
                },
                {
                    "sent": "Depart of Theano an it's just a couple of more lines if you have an expression for the gradient and 1:25 as well.",
                    "label": 0
                },
                {
                    "sent": "It has the disadvantage that there's a context switch between C and Python during the execution, but if it's a large operation and that's highly optimized, it might really be worth it.",
                    "label": 0
                },
                {
                    "sent": "For instance, people having implemented through the convolutions using FFT on GPU using that kind of Python bindings, which was much faster than having to write CUDA code for that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The harder way an sometimes more optimized, so usually a bit more optimized is to write C code.",
                    "label": 0
                },
                {
                    "sent": "For that you need to understand the API of Python And PY to interact directly with the data that we have.",
                    "label": 0
                },
                {
                    "sent": "Usually we want our code to handle arbitrary strides, but we have not.",
                    "label": 0
                },
                {
                    "sent": "That's called GPU contiguous.",
                    "label": 0
                },
                {
                    "sent": "That basically makes a copy of the inputs if they're not contiguous, and ensures that everything is fine.",
                    "label": 0
                },
                {
                    "sent": "You need to manage your accounts for Python And so on.",
                    "label": 0
                },
                {
                    "sent": "But you don't have any overhead from the Python interpreter for calling from going from C to Python And back.",
                    "label": 0
                },
                {
                    "sent": "An external contributors have used that to implement better and more optimized and more complete versions of convolutions on CD on CPU and GPU.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just a list of couple of new things that we have in Theano.",
                    "label": 0
                },
                {
                    "sent": "The new GPU backends which I mentioned earlier which supports multiple GPU's in the same function for model parallelism that supports additional detect, including the half precision float float 16 experimental support for this basic operation for open CL, but it's really really experimental.",
                    "label": 0
                },
                {
                    "sent": "Four months improvements for.",
                    "label": 0
                },
                {
                    "sent": "Many critical parts like for scan for convolutions without using codeine, an allocation and the allocation of GPU memory which used to be sometimes a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "We're not using CNN or something similar to avoid synchronizing the GPU each time we allocate allocate memory, and we have a mini framework called platoon which is built on top of piano and which uses Theano to train the same model in I mean.",
                    "label": 0
                },
                {
                    "sent": "To train multiple instances of the same model using data parallelism.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Done for style, asynchronous SGD and we have elastic synchronous SGD as well and someone is working on Synchronoss and multi machine version of Splatoon as well.",
                    "label": 0
                },
                {
                    "sent": "It's not there yet, but it's.",
                    "label": 0
                },
                {
                    "sent": "It's in the future.",
                    "label": 0
                },
                {
                    "sent": "We have made a lot of progress in the compilation time as well.",
                    "label": 0
                },
                {
                    "sent": "An also in ways of compiling the optimizing the graph only once an having different functions.",
                    "label": 0
                },
                {
                    "sent": "Different functions use the same compile graph or compile the graph once and then execute only part of it.",
                    "label": 0
                },
                {
                    "sent": "Compute only some outputs or don't apply the updates or swap shared variables and so on.",
                    "label": 0
                },
                {
                    "sent": "So for instance if you have.",
                    "label": 0
                },
                {
                    "sent": "You can compile one function to do some training and then just say oh I'd like to swap like this.",
                    "label": 0
                },
                {
                    "sent": "Variables for this short variables for like this previous weights or these weights averaged by.",
                    "label": 0
                },
                {
                    "sent": "I don't know some kind of aggregation or whatever and don't perform the updates and just apply that on some new data set and you don't have to re optimize the graph and.",
                    "label": 0
                },
                {
                    "sent": "So this is this is something quite recent.",
                    "label": 0
                },
                {
                    "sent": "We have new dynamic diagnostic tools like the interactive visualization that has been.",
                    "label": 0
                },
                {
                    "sent": "Incorporated into, you know, I think that's September or something like that.",
                    "label": 0
                },
                {
                    "sent": "The PDB breakpoint and the we are trying to keep the stack trace of when nodes were created, even across optimizations.",
                    "label": 0
                },
                {
                    "sent": "So right now for some cases does not work yet like the one I showed you earlier with the Illinois Fusion.",
                    "label": 0
                },
                {
                    "sent": "This one did not preserve the stack trace, but we have some that do and.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of people working on that and this should.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming soon so.",
                    "label": 0
                },
                {
                    "sent": "This is, well, the next features that are coming soon, so Latin across nodes.",
                    "label": 0
                },
                {
                    "sent": "Someone's working on that, but faster optimization or graph optimization are coming.",
                    "label": 0
                },
                {
                    "sent": "Wrapping off more quickly and optimization is also something that's come in the following month and.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is almost the end.",
                    "label": 0
                },
                {
                    "sent": "I'd like to thank well lot of people who supported the lab who supported the development of piano, competition resources and so on, and the CRM and see for for for organization of the summer school.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have questions we don't have much time.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can always ask them offline and I'll be there and I'll be there around and other developers will be there as well and.",
                    "label": 0
                },
                {
                    "sent": "The hands-on practical session tomorrow is also a good time.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To ask us some questions if you want to go further, these slides and the notebooks with the code examples that I showed in the slides are available on GitHub and if you want more resources on piano while the documentation is on deeplearning.net, the code is on GitHub and a couple of months ago we published a new article.",
                    "label": 0
                },
                {
                    "sent": "Expanding well, the current state of piano and couple of benchmarks and so on.",
                    "label": 0
                }
            ]
        }
    }
}