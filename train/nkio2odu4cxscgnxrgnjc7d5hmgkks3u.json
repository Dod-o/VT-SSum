{
    "id": "nkio2odu4cxscgnxrgnjc7d5hmgkks3u",
    "title": "Human Pose Search using Deep Poselets",
    "info": {
        "presenter": [
            "Nataraj Jammalamadak, International Institute of Information Technology Hyderabad"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_jammalamadaka_pose_search/",
    "segmentation": [
        [
            "Thank you.",
            "In this work, we focus on human pose, a query modality for image and video retrieval task, and to encode the posts.",
            "We propose our deposit based deposit method.",
            "I'm not rich, appears distant from reply to Heather about India.",
            "This work is in collaboration with my advisors Andrew Surman.",
            "See Java.",
            "So first I will introduce several terms, so I will introduce what is human pose, what is human?",
            "Both search and water disposal, it's."
        ],
        [
            "So here we have three people and their body parts are marked with these green sticks.",
            "This spatial arrangement of body parts is what we mean by oppose, so I want to emphasize here that there is no depth here at all.",
            "It's just RGB image and you are supposed to detect this pose.",
            "And if you closely observe the underlying, the pose is indicative of the underlying action and gesture here.",
            "So those are very important for both action and gesture.",
            "Does he?"
        ],
        [
            "Suppose now what is human posted so you have a.",
            "And why is somebody interested in human pose as a search, let's say?",
            "We have a sports database.",
            "And we are interested in viewing all the video clips where a particular shortest play.",
            "Or let's say we have a dance database and we're interested in all the videos where a particular dance step is performed.",
            "So in this situations, human pose search system is very useful.",
            "So this is a pipeline of the system.",
            "So you take a query as an image."
        ],
        [
            "And then you build up a sensitive feature representation.",
            "Then you search through the database and then you return all the video clips where the people have the same pose as that of the query."
        ],
        [
            "So first I'll talk about this is a worry of the presentation.",
            "1st, I'll talk about the depots.",
            "Let's.",
            "Because it is simply a body part detector which is sensitive to pose.",
            "1st, I'll talk about the polar discovery.",
            "In this we will cluster the post base to get various postlets.",
            "Then we train using convolution neural networks, hence the name deposits and then we finally detect these pose.",
            "Let's in a test image.",
            "So the second part of the presentations about retrieval where we use these depots, let's.",
            "So given a query, we first build a bag of deposits descriptor.",
            "Then we using this we do the retrieval."
        ],
        [
            "So these are the kinds of data with which we are working.",
            "These are a bunch of images that we collected from the publicly available datasets.",
            "These the images are collected from TV series, movies, Flickr images and so on.",
            "As you can see, they're totally unconstrained and in very, very hard settings."
        ],
        [
            "So there is no constraint on pose that is arms have full degrees of freedom.",
            "There is no constraint on the appearance of the person, so people can wear any clothes, can be of any ethnicity, doesn't matter, and they can appear in.",
            "There is no constraint on the background so they can appear in any scene whether it is indoor, outdoor, whatever."
        ],
        [
            "So overall we have about twenty 1400 samples.",
            "We divide it into train validation and test such that there is no movie overlap.",
            "So what I mean by that is that.",
            "All the frames belonging to one movie are either in train validation or test.",
            "They are not distributed, so this helps in better understanding the generalization performance of our method."
        ],
        [
            "So first I'll talk about the positives."
        ],
        [
            "So what is suppose let?"
        ],
        [
            "In this example.",
            "We have two left arms making the same angle, so when I say left arm left ARM is the leftmost arm in the image does not necessarily correspond to the anatomical left arm.",
            "So this can potentially be one post late like this way.",
            "In this."
        ],
        [
            "Example again, you have the same left arm but in a different post, so this will be.",
            "Another puzzle it the same case here."
        ],
        [
            "So we have the same left arm but three different poses.",
            "Hence three different posits."
        ],
        [
            "So clearly there are several posits possible and we would like to discover them from the data straight away.",
            "So for this user training data, which of these ground to stick my annotated?",
            "We then cluster this in different contexts, like just the left arm, just the right arm or all the body parts together.",
            "And this clustering is done on the body but angles.",
            "We use K means clustering and we're we obtain several pose.",
            "Let's these are the positive average images that some of some of the positive regions that we obtain.",
            "So now given."
        ],
        [
            "That we have obtained the pose let's we would like to train the posits.",
            "So we use convolution neural networks.",
            "A convolutional neural network is simply standard neural network with some additional modifications.",
            "It has sort of sort of three types of layers of convolution layer, fully connected layer and softmax layer.",
            "A convolution layer doesn't perform the convolution operation on the output of the previous layer.",
            "It is typically followed by a pooling operation.",
            "Here, for example, we show Max pooling and this pulling is done in a small area or something like that across 3.",
            "Then we have the fully connected layer.",
            "In this all the neurons of the current layer are connected to all the neurons of the previous layer in a bipartite fashion.",
            "And we have softmax layer which is essentially fully connected layer except that its outputs on chapter one.",
            "CNN's have a non linearity within them.",
            "So we use electrified linear unit rectified rectified linear unit returns the input if the input is greater than 0 otherwise it returns 0.",
            "So we use the architecture of preserve skittle.",
            "So our model takes an input image, is an input image Patch and then it classifieds into one of the.",
            "And bullets.",
            "The model has output layer and the output has N neurons, each corresponding to one pose let.",
            "And the.",
            "Best.",
            "Best scoring Neuron is the output level.",
            "So this is the notation, so input level is represented by X model parameters by W ground truth by G and the output by Y.",
            "So Y is a function of input and the model parameters.",
            "So to train."
        ],
        [
            "Network, you need a loss function.",
            "We use cross entropy loss function.",
            "So briefly cause cross entropy loss function assigns a higher cost when the score of the true posnet is low and vice versa.",
            "And we minimize this loss over the training data using stochastic gradient descent.",
            "So I think the efficient version of this stochastic gradient descent for the neural networks is the famous backpropagation algorithm."
        ],
        [
            "So the challenges are following our network has 40,000,000 parameters.",
            "You really require about 1 to 2 million samples if not more.",
            "And we only have 50,000 samples, so the solution is using fine tuning.",
            "So the idea is that you train your network on a task which has enough data present.",
            "Then you adapt your network to our current task.",
            "And this procedure operation is called is fine tuning.",
            "So the procedure is as follows.",
            "So first we first train the network on image classification task using the image net data, which is about 1.2 million samples.",
            "And then we replace the softmax layer.",
            "With inappropriate fun for our case and we randomly initialize the weights and then we proceed with the gradient descent using for the using the.",
            "Quizlet data"
        ],
        [
            "Right, so now we have obtained all the pose.",
            "Let's now given a test image we would like to get the positive actions.",
            "So first, given a test image, you run all the depots let's but we make one observation here.",
            "If we are given the upper body detection bounding box, we see that pose.",
            "Let's occur occur in a very localized area.",
            "For example, if you have opposed it corresponding to her left arm, maybe it will occur only in the left half of your image.",
            "So we run the classifier only in this localized area.",
            "This localized area can be learned from the training data.",
            "This improves both our speed and accuracy.",
            "So one of the problems of running multiple classifiers.",
            "Is."
        ],
        [
            "That several of them fire in the same location.",
            "To the obj."
        ],
        [
            "Issue is that we would like to restore these detections such that the correct ones will get higher score and then correct once we get lower score.",
            "So what we do is we learn a regression function for each of deposits and this regression function takes the scores of other positive detections and it gives a new score."
        ],
        [
            "So to understand the performance of the model, we run the postlets on the test data.",
            "So these are the two examples, let's so the pose let's are defined by the body part and their post.",
            "So these are depicted by these images.",
            "And the performance is plotted on a precision recall curve.",
            "So for those of you who are unfamiliar, so the desert would be here.",
            "That means the higher the better.",
            "So ours is the green one and the second one is red one second best performance.",
            "So we compare our method against those puzzles which are trained using Hall and handcrafted feature.",
            "As you can see, our method simply outperforms the hog based methods.",
            "So here we we take an average across several postlets and we get the mean average precision.",
            "So with the hog waste bullets we get an API for about 32.6, but when you CNN just with before even fine tuning, we gotta jump about 16%.",
            "And when you do fine tuning it, the jump is further 8%.",
            "Overall we get about 24% improvement.",
            "Using the deposits combination your networks."
        ],
        [
            "So these are the some of the examples, so there are two postlets and their top detections.",
            "As you can see, all the top detections are correct."
        ],
        [
            "So now we know how to train and test depots.",
            "Let's now I'll talk about how to use these deposits to do retrieval."
        ],
        [
            "So, given a video collection first, we would like to index.",
            "So let's say we are given a set of videos.",
            "We extract all the frames.",
            "On each frame we run the upper body detection.",
            "Then we run all the pose let's and then we do the spatial reasoning.",
            "So at end of this stage we are and we end up with a set of mostly detections.",
            "Viewmax pull all these positive detections to obtain a bag of depots.",
            "Let descriptor this is 1 dimensional vector and then we index it into standard databases like approximate nearest neighbor structures like randomized KD tree and so on."
        ],
        [
            "Now given a query image.",
            "We first build the bag of deposits and then we search through the database using.",
            "In our case we are using cosine distance.",
            "And then we retrieve all the images or video clips.",
            "Where their frames are most similar to the query."
        ],
        [
            "Here to understand the performance of our method, we use the test data which contains about 5400 samples.",
            "Samples we use.",
            "We use each sample as a query.",
            "And we retrieve within the test data itself, so you get 5400.",
            "Average positions we take average of them.",
            "It's a mean average position is the evaluation metric that we use.",
            "So we compare our method against three standard retrieval methods.",
            "The first one is a bag of visual words, the second one is using the Berkeley puzzle.",
            "It's where we, the descriptor, would be back of part descriptor and 3rd one is using human pose estimation algorithms.",
            "So we use this human position algorithm, which is one of the state of the art.",
            "Here we build the descriptor based on the angle of the body parts.",
            "So these are the results that we get.",
            "So the second best performing method is the human pose estimation, one that the average average mean average precision is about 17.5.",
            "Our method simply doubles it to 34.6."
        ],
        [
            "So to take a closer look at the performance of all these queries.",
            "We plot the performance.",
            "Hear that on the X axis it is average precision on the Y axis.",
            "It is a percentage of queries, so this is a performance of human pose estimation.",
            "So just to be clear, so this bar is saying that 30% of the queries have.",
            "Average position of around 10 percent.",
            "35% of the queries have an average position of around 10%.",
            "That means this performance is really bad and there in fact beyond 60% there there are no bars at all, so this is really performing terribly.",
            "Over my"
        ],
        [
            "It is doing very well, so here there are.",
            "The bars are low.",
            "That means few queries have low average position and there are quite a few bars present here.",
            "So there are many many queries which have high average precision.",
            "So to be more specific, in the HPF."
        ],
        [
            "With HP method, 75% of the queries have an average position less than 20%.",
            "And only little over than 5% of the queries have average position about 50%.",
            "But in our case, only 45% of queries on average less than 20%, but 25% of the queries are have an average position greater than 50%."
        ],
        [
            "So.",
            "To understand our.",
            "To understand our method deeply and to get an insight, we analyze the second best performing method that is human position method and our method.",
            "So human pose estimation based method.",
            "Human protection method often commits.",
            "Do a wrong posts and making recoverable mistake.",
            "In this example, this is the ground truth post, the one marked by dotted ones and this is the our detection, so clearly they're wrong.",
            "And naturally, the Postal system, which is based on this pose detection algorithms, will perform very poorly.",
            "But on the other hand, our method in quotes multiple proposals.",
            "But but waited on their likelihood.",
            "So even if some of the detections are wrong, it can recover well."
        ],
        [
            "So these are finalised are some of the example queries.",
            "So this is a query of a person in a one particular post, so this is a performance of the query and these are the top retrievals.",
            "As you can see, all the top retrievals have similar to that of the query.",
            "So."
        ],
        [
            "This is another query.",
            "So here again, several of the.",
            "Top detections have their poses similar to that of the query."
        ],
        [
            "One more example.",
            "Here again, several of them have a.",
            "There was a similar to that query."
        ],
        [
            "So in summary.",
            "We focus on human pose as a query modality for image and video retrieval.",
            "Pause is a very difficult task.",
            "Just RGB image.",
            "So despite despite these challenges.",
            "Our deposit method significantly outperforms hog based method and our retrieval method again significantly outperforms the state of the art method by 17%.",
            "It in fact doubles the performance.",
            "Thank you."
        ],
        [
            "So here are the results."
        ],
        [
            "Video of the results that just play along.",
            "Thank you for the last presentation."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "In this work, we focus on human pose, a query modality for image and video retrieval task, and to encode the posts.",
                    "label": 0
                },
                {
                    "sent": "We propose our deposit based deposit method.",
                    "label": 0
                },
                {
                    "sent": "I'm not rich, appears distant from reply to Heather about India.",
                    "label": 0
                },
                {
                    "sent": "This work is in collaboration with my advisors Andrew Surman.",
                    "label": 0
                },
                {
                    "sent": "See Java.",
                    "label": 0
                },
                {
                    "sent": "So first I will introduce several terms, so I will introduce what is human pose, what is human?",
                    "label": 0
                },
                {
                    "sent": "Both search and water disposal, it's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we have three people and their body parts are marked with these green sticks.",
                    "label": 0
                },
                {
                    "sent": "This spatial arrangement of body parts is what we mean by oppose, so I want to emphasize here that there is no depth here at all.",
                    "label": 0
                },
                {
                    "sent": "It's just RGB image and you are supposed to detect this pose.",
                    "label": 0
                },
                {
                    "sent": "And if you closely observe the underlying, the pose is indicative of the underlying action and gesture here.",
                    "label": 1
                },
                {
                    "sent": "So those are very important for both action and gesture.",
                    "label": 1
                },
                {
                    "sent": "Does he?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suppose now what is human posted so you have a.",
                    "label": 0
                },
                {
                    "sent": "And why is somebody interested in human pose as a search, let's say?",
                    "label": 0
                },
                {
                    "sent": "We have a sports database.",
                    "label": 0
                },
                {
                    "sent": "And we are interested in viewing all the video clips where a particular shortest play.",
                    "label": 0
                },
                {
                    "sent": "Or let's say we have a dance database and we're interested in all the videos where a particular dance step is performed.",
                    "label": 0
                },
                {
                    "sent": "So in this situations, human pose search system is very useful.",
                    "label": 1
                },
                {
                    "sent": "So this is a pipeline of the system.",
                    "label": 0
                },
                {
                    "sent": "So you take a query as an image.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you build up a sensitive feature representation.",
                    "label": 0
                },
                {
                    "sent": "Then you search through the database and then you return all the video clips where the people have the same pose as that of the query.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'll talk about this is a worry of the presentation.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll talk about the depots.",
                    "label": 0
                },
                {
                    "sent": "Let's.",
                    "label": 0
                },
                {
                    "sent": "Because it is simply a body part detector which is sensitive to pose.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll talk about the polar discovery.",
                    "label": 0
                },
                {
                    "sent": "In this we will cluster the post base to get various postlets.",
                    "label": 0
                },
                {
                    "sent": "Then we train using convolution neural networks, hence the name deposits and then we finally detect these pose.",
                    "label": 0
                },
                {
                    "sent": "Let's in a test image.",
                    "label": 0
                },
                {
                    "sent": "So the second part of the presentations about retrieval where we use these depots, let's.",
                    "label": 0
                },
                {
                    "sent": "So given a query, we first build a bag of deposits descriptor.",
                    "label": 1
                },
                {
                    "sent": "Then we using this we do the retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the kinds of data with which we are working.",
                    "label": 0
                },
                {
                    "sent": "These are a bunch of images that we collected from the publicly available datasets.",
                    "label": 0
                },
                {
                    "sent": "These the images are collected from TV series, movies, Flickr images and so on.",
                    "label": 1
                },
                {
                    "sent": "As you can see, they're totally unconstrained and in very, very hard settings.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is no constraint on pose that is arms have full degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "There is no constraint on the appearance of the person, so people can wear any clothes, can be of any ethnicity, doesn't matter, and they can appear in.",
                    "label": 0
                },
                {
                    "sent": "There is no constraint on the background so they can appear in any scene whether it is indoor, outdoor, whatever.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So overall we have about twenty 1400 samples.",
                    "label": 0
                },
                {
                    "sent": "We divide it into train validation and test such that there is no movie overlap.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that is that.",
                    "label": 0
                },
                {
                    "sent": "All the frames belonging to one movie are either in train validation or test.",
                    "label": 0
                },
                {
                    "sent": "They are not distributed, so this helps in better understanding the generalization performance of our method.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'll talk about the positives.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is suppose let?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this example.",
                    "label": 0
                },
                {
                    "sent": "We have two left arms making the same angle, so when I say left arm left ARM is the leftmost arm in the image does not necessarily correspond to the anatomical left arm.",
                    "label": 0
                },
                {
                    "sent": "So this can potentially be one post late like this way.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example again, you have the same left arm but in a different post, so this will be.",
                    "label": 0
                },
                {
                    "sent": "Another puzzle it the same case here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the same left arm but three different poses.",
                    "label": 0
                },
                {
                    "sent": "Hence three different posits.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So clearly there are several posits possible and we would like to discover them from the data straight away.",
                    "label": 0
                },
                {
                    "sent": "So for this user training data, which of these ground to stick my annotated?",
                    "label": 1
                },
                {
                    "sent": "We then cluster this in different contexts, like just the left arm, just the right arm or all the body parts together.",
                    "label": 1
                },
                {
                    "sent": "And this clustering is done on the body but angles.",
                    "label": 0
                },
                {
                    "sent": "We use K means clustering and we're we obtain several pose.",
                    "label": 0
                },
                {
                    "sent": "Let's these are the positive average images that some of some of the positive regions that we obtain.",
                    "label": 0
                },
                {
                    "sent": "So now given.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we have obtained the pose let's we would like to train the posits.",
                    "label": 0
                },
                {
                    "sent": "So we use convolution neural networks.",
                    "label": 0
                },
                {
                    "sent": "A convolutional neural network is simply standard neural network with some additional modifications.",
                    "label": 0
                },
                {
                    "sent": "It has sort of sort of three types of layers of convolution layer, fully connected layer and softmax layer.",
                    "label": 1
                },
                {
                    "sent": "A convolution layer doesn't perform the convolution operation on the output of the previous layer.",
                    "label": 1
                },
                {
                    "sent": "It is typically followed by a pooling operation.",
                    "label": 1
                },
                {
                    "sent": "Here, for example, we show Max pooling and this pulling is done in a small area or something like that across 3.",
                    "label": 0
                },
                {
                    "sent": "Then we have the fully connected layer.",
                    "label": 0
                },
                {
                    "sent": "In this all the neurons of the current layer are connected to all the neurons of the previous layer in a bipartite fashion.",
                    "label": 0
                },
                {
                    "sent": "And we have softmax layer which is essentially fully connected layer except that its outputs on chapter one.",
                    "label": 1
                },
                {
                    "sent": "CNN's have a non linearity within them.",
                    "label": 0
                },
                {
                    "sent": "So we use electrified linear unit rectified rectified linear unit returns the input if the input is greater than 0 otherwise it returns 0.",
                    "label": 0
                },
                {
                    "sent": "So we use the architecture of preserve skittle.",
                    "label": 0
                },
                {
                    "sent": "So our model takes an input image, is an input image Patch and then it classifieds into one of the.",
                    "label": 0
                },
                {
                    "sent": "And bullets.",
                    "label": 0
                },
                {
                    "sent": "The model has output layer and the output has N neurons, each corresponding to one pose let.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Best.",
                    "label": 0
                },
                {
                    "sent": "Best scoring Neuron is the output level.",
                    "label": 0
                },
                {
                    "sent": "So this is the notation, so input level is represented by X model parameters by W ground truth by G and the output by Y.",
                    "label": 0
                },
                {
                    "sent": "So Y is a function of input and the model parameters.",
                    "label": 0
                },
                {
                    "sent": "So to train.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Network, you need a loss function.",
                    "label": 1
                },
                {
                    "sent": "We use cross entropy loss function.",
                    "label": 0
                },
                {
                    "sent": "So briefly cause cross entropy loss function assigns a higher cost when the score of the true posnet is low and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And we minimize this loss over the training data using stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So I think the efficient version of this stochastic gradient descent for the neural networks is the famous backpropagation algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the challenges are following our network has 40,000,000 parameters.",
                    "label": 0
                },
                {
                    "sent": "You really require about 1 to 2 million samples if not more.",
                    "label": 0
                },
                {
                    "sent": "And we only have 50,000 samples, so the solution is using fine tuning.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you train your network on a task which has enough data present.",
                    "label": 1
                },
                {
                    "sent": "Then you adapt your network to our current task.",
                    "label": 0
                },
                {
                    "sent": "And this procedure operation is called is fine tuning.",
                    "label": 1
                },
                {
                    "sent": "So the procedure is as follows.",
                    "label": 1
                },
                {
                    "sent": "So first we first train the network on image classification task using the image net data, which is about 1.2 million samples.",
                    "label": 1
                },
                {
                    "sent": "And then we replace the softmax layer.",
                    "label": 0
                },
                {
                    "sent": "With inappropriate fun for our case and we randomly initialize the weights and then we proceed with the gradient descent using for the using the.",
                    "label": 0
                },
                {
                    "sent": "Quizlet data",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so now we have obtained all the pose.",
                    "label": 0
                },
                {
                    "sent": "Let's now given a test image we would like to get the positive actions.",
                    "label": 0
                },
                {
                    "sent": "So first, given a test image, you run all the depots let's but we make one observation here.",
                    "label": 1
                },
                {
                    "sent": "If we are given the upper body detection bounding box, we see that pose.",
                    "label": 0
                },
                {
                    "sent": "Let's occur occur in a very localized area.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have opposed it corresponding to her left arm, maybe it will occur only in the left half of your image.",
                    "label": 0
                },
                {
                    "sent": "So we run the classifier only in this localized area.",
                    "label": 0
                },
                {
                    "sent": "This localized area can be learned from the training data.",
                    "label": 1
                },
                {
                    "sent": "This improves both our speed and accuracy.",
                    "label": 0
                },
                {
                    "sent": "So one of the problems of running multiple classifiers.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That several of them fire in the same location.",
                    "label": 0
                },
                {
                    "sent": "To the obj.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Issue is that we would like to restore these detections such that the correct ones will get higher score and then correct once we get lower score.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we learn a regression function for each of deposits and this regression function takes the scores of other positive detections and it gives a new score.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to understand the performance of the model, we run the postlets on the test data.",
                    "label": 0
                },
                {
                    "sent": "So these are the two examples, let's so the pose let's are defined by the body part and their post.",
                    "label": 0
                },
                {
                    "sent": "So these are depicted by these images.",
                    "label": 0
                },
                {
                    "sent": "And the performance is plotted on a precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who are unfamiliar, so the desert would be here.",
                    "label": 0
                },
                {
                    "sent": "That means the higher the better.",
                    "label": 0
                },
                {
                    "sent": "So ours is the green one and the second one is red one second best performance.",
                    "label": 0
                },
                {
                    "sent": "So we compare our method against those puzzles which are trained using Hall and handcrafted feature.",
                    "label": 1
                },
                {
                    "sent": "As you can see, our method simply outperforms the hog based methods.",
                    "label": 1
                },
                {
                    "sent": "So here we we take an average across several postlets and we get the mean average precision.",
                    "label": 0
                },
                {
                    "sent": "So with the hog waste bullets we get an API for about 32.6, but when you CNN just with before even fine tuning, we gotta jump about 16%.",
                    "label": 0
                },
                {
                    "sent": "And when you do fine tuning it, the jump is further 8%.",
                    "label": 0
                },
                {
                    "sent": "Overall we get about 24% improvement.",
                    "label": 0
                },
                {
                    "sent": "Using the deposits combination your networks.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the some of the examples, so there are two postlets and their top detections.",
                    "label": 0
                },
                {
                    "sent": "As you can see, all the top detections are correct.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we know how to train and test depots.",
                    "label": 0
                },
                {
                    "sent": "Let's now I'll talk about how to use these deposits to do retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given a video collection first, we would like to index.",
                    "label": 0
                },
                {
                    "sent": "So let's say we are given a set of videos.",
                    "label": 0
                },
                {
                    "sent": "We extract all the frames.",
                    "label": 0
                },
                {
                    "sent": "On each frame we run the upper body detection.",
                    "label": 1
                },
                {
                    "sent": "Then we run all the pose let's and then we do the spatial reasoning.",
                    "label": 0
                },
                {
                    "sent": "So at end of this stage we are and we end up with a set of mostly detections.",
                    "label": 0
                },
                {
                    "sent": "Viewmax pull all these positive detections to obtain a bag of depots.",
                    "label": 0
                },
                {
                    "sent": "Let descriptor this is 1 dimensional vector and then we index it into standard databases like approximate nearest neighbor structures like randomized KD tree and so on.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now given a query image.",
                    "label": 1
                },
                {
                    "sent": "We first build the bag of deposits and then we search through the database using.",
                    "label": 0
                },
                {
                    "sent": "In our case we are using cosine distance.",
                    "label": 0
                },
                {
                    "sent": "And then we retrieve all the images or video clips.",
                    "label": 0
                },
                {
                    "sent": "Where their frames are most similar to the query.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here to understand the performance of our method, we use the test data which contains about 5400 samples.",
                    "label": 0
                },
                {
                    "sent": "Samples we use.",
                    "label": 0
                },
                {
                    "sent": "We use each sample as a query.",
                    "label": 0
                },
                {
                    "sent": "And we retrieve within the test data itself, so you get 5400.",
                    "label": 1
                },
                {
                    "sent": "Average positions we take average of them.",
                    "label": 1
                },
                {
                    "sent": "It's a mean average position is the evaluation metric that we use.",
                    "label": 0
                },
                {
                    "sent": "So we compare our method against three standard retrieval methods.",
                    "label": 1
                },
                {
                    "sent": "The first one is a bag of visual words, the second one is using the Berkeley puzzle.",
                    "label": 1
                },
                {
                    "sent": "It's where we, the descriptor, would be back of part descriptor and 3rd one is using human pose estimation algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we use this human position algorithm, which is one of the state of the art.",
                    "label": 0
                },
                {
                    "sent": "Here we build the descriptor based on the angle of the body parts.",
                    "label": 0
                },
                {
                    "sent": "So these are the results that we get.",
                    "label": 0
                },
                {
                    "sent": "So the second best performing method is the human pose estimation, one that the average average mean average precision is about 17.5.",
                    "label": 1
                },
                {
                    "sent": "Our method simply doubles it to 34.6.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to take a closer look at the performance of all these queries.",
                    "label": 0
                },
                {
                    "sent": "We plot the performance.",
                    "label": 0
                },
                {
                    "sent": "Hear that on the X axis it is average precision on the Y axis.",
                    "label": 1
                },
                {
                    "sent": "It is a percentage of queries, so this is a performance of human pose estimation.",
                    "label": 1
                },
                {
                    "sent": "So just to be clear, so this bar is saying that 30% of the queries have.",
                    "label": 0
                },
                {
                    "sent": "Average position of around 10 percent.",
                    "label": 0
                },
                {
                    "sent": "35% of the queries have an average position of around 10%.",
                    "label": 0
                },
                {
                    "sent": "That means this performance is really bad and there in fact beyond 60% there there are no bars at all, so this is really performing terribly.",
                    "label": 0
                },
                {
                    "sent": "Over my",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is doing very well, so here there are.",
                    "label": 0
                },
                {
                    "sent": "The bars are low.",
                    "label": 0
                },
                {
                    "sent": "That means few queries have low average position and there are quite a few bars present here.",
                    "label": 0
                },
                {
                    "sent": "So there are many many queries which have high average precision.",
                    "label": 0
                },
                {
                    "sent": "So to be more specific, in the HPF.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With HP method, 75% of the queries have an average position less than 20%.",
                    "label": 0
                },
                {
                    "sent": "And only little over than 5% of the queries have average position about 50%.",
                    "label": 0
                },
                {
                    "sent": "But in our case, only 45% of queries on average less than 20%, but 25% of the queries are have an average position greater than 50%.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To understand our.",
                    "label": 0
                },
                {
                    "sent": "To understand our method deeply and to get an insight, we analyze the second best performing method that is human position method and our method.",
                    "label": 0
                },
                {
                    "sent": "So human pose estimation based method.",
                    "label": 0
                },
                {
                    "sent": "Human protection method often commits.",
                    "label": 0
                },
                {
                    "sent": "Do a wrong posts and making recoverable mistake.",
                    "label": 0
                },
                {
                    "sent": "In this example, this is the ground truth post, the one marked by dotted ones and this is the our detection, so clearly they're wrong.",
                    "label": 0
                },
                {
                    "sent": "And naturally, the Postal system, which is based on this pose detection algorithms, will perform very poorly.",
                    "label": 1
                },
                {
                    "sent": "But on the other hand, our method in quotes multiple proposals.",
                    "label": 1
                },
                {
                    "sent": "But but waited on their likelihood.",
                    "label": 0
                },
                {
                    "sent": "So even if some of the detections are wrong, it can recover well.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are finalised are some of the example queries.",
                    "label": 0
                },
                {
                    "sent": "So this is a query of a person in a one particular post, so this is a performance of the query and these are the top retrievals.",
                    "label": 0
                },
                {
                    "sent": "As you can see, all the top retrievals have similar to that of the query.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another query.",
                    "label": 0
                },
                {
                    "sent": "So here again, several of the.",
                    "label": 0
                },
                {
                    "sent": "Top detections have their poses similar to that of the query.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One more example.",
                    "label": 0
                },
                {
                    "sent": "Here again, several of them have a.",
                    "label": 0
                },
                {
                    "sent": "There was a similar to that query.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "We focus on human pose as a query modality for image and video retrieval.",
                    "label": 0
                },
                {
                    "sent": "Pause is a very difficult task.",
                    "label": 0
                },
                {
                    "sent": "Just RGB image.",
                    "label": 0
                },
                {
                    "sent": "So despite despite these challenges.",
                    "label": 0
                },
                {
                    "sent": "Our deposit method significantly outperforms hog based method and our retrieval method again significantly outperforms the state of the art method by 17%.",
                    "label": 1
                },
                {
                    "sent": "It in fact doubles the performance.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video of the results that just play along.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the last presentation.",
                    "label": 0
                }
            ]
        }
    }
}