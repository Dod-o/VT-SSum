{
    "id": "2ndomm2gygpnsrd7wowz2bhrgjtspq7b",
    "title": "Diefficiency Metrics: Measuring the Continuous Efficiency of Query Processing Approaches",
    "info": {
        "author": [
            "Maribel Acosta, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_acosta_diefficiency_metrics/",
    "segmentation": [
        [
            "So good afternoon, my name is Maribel Acosta and I'm from KIT and this is a joint work with Media stupid Al Ain York should have better and the title of our work is the efficiency metrics.",
            "Measuring the continuous efficiency of preprocessing approaches sofa."
        ],
        [
            "First, let me start with a motivation, so let's consider that we want to execute this sparkle query against an RDF data set.",
            "So typically we take this query and we submitted to a query engine and then we get some results and for example in this case we got all the answers at the same time.",
            "So this is the definition of a blocking approach which produces all the results at the exact same time, typically at the end.",
            "Off the query execution, however, there are other types of."
        ],
        [
            "Boring ending, so for example, and use that produce the answer overtime this is the definition of incremental approaches which are able to produce results as soon as the data is ready and in the semantic web community there have been proposals for query engines like the ones I'm showing on the slides that are able to produce the results incrementally.",
            "So we took one of these engines NLD or network.",
            "Kathleen Data Ed is.",
            "We run this query.",
            "Against the pedia.",
            "And then we measure the performance of the ending."
        ],
        [
            "Using three different configurations, so we use energy, not adaptive, selective and random.",
            "And then we took some traditional metrics like time for the first answer, execution time, throughput or completeness.",
            "And then we measure the performance which is reported on the table.",
            "Now if we take a look at execution time and throughput, we can conclude that the variant in LTE random outperforms the other approaches.",
            "However, these metrics do not consider that the ending was producing the results incrementally.",
            "So when we look at the continuous performance of the engine by looking at the number of answers produced overtime, we can actually see that in the first 7.5 seconds of query execution.",
            "The approach not adaptive outperformed the other approaches.",
            "So by looking at the continuous performance we can actually get further insights about the behavior of the endings that were not possible to be seen with traditional metrics.",
            "However, in this example we're doing."
        ],
        [
            "Qualitative analysis.",
            "We're just looking at a plot and drawing this conclusion, but in order to properly benchmark our engines, we need."
        ],
        [
            "Quantitative methods to measure the continuous efficiency of incremental approaches.",
            "So we."
        ],
        [
            "Take a look at the related work.",
            "Can we look at benchmarks proposed for query processing as?"
        ],
        [
            "Well, as metrics reported in experimental studies of research papers and we collected metrics and classify them into dimensions according of what they measure and we have, for example, metrics that measure effectiveness such as answer completeness or answer correctness and other metrics that focus on efficiency like execution time or queries per second.",
            "There also metrics that combine these two dimensions.",
            "Like that combined metric, but all these metrics have a commonality.",
            "None of these metrics consider their continuous performance of the ending, so they are not tailored to benchmark the performance of incremental approaches.",
            "So based on this."
        ],
        [
            "Observations, we propose an approach to measure the continuous efficiency of approaches an we propose."
        ],
        [
            "The efficiency metrics and the efficiency in this context is defined as continuous efficiency, and we saw in the motivating example that the continuous performance of the approaches is actually recorded in the answer traces of the endings or in the query logs.",
            "So for example, here we have for each of the answers that timestamp where the answer was produced.",
            "So based on the answer Metro or the answer traces.",
            "Our metrics quantify the efficiency of incremental approaches."
        ],
        [
            "To do so, first we build the answer distribution function, which is a function that is defined in the interval from zero to TN, where zero is where the execution of the query starts.",
            "Anti N is the point in time where the last answer was produced by the engine and the value at the time X of this function indicates the number of answers produced until that point in time.",
            "Then the answer distribution function can be built from the answer tracers or the query logs by applying linear interpolations between the timestamps recorded in the answer traces.",
            "So from the answer traces from the endings we can build discontinuous functions that we see on the right side."
        ],
        [
            "And then we propose our first metric, which is the FAT which quantifies the efficiency during the first tee time units of execution time, and to do so the metric measures the area under the curve in the interval from zero to T. And we can do this by computing the integral from zero to T of the answer distribution function, and in this example I'm highlighting the area under the curve for the approach random.",
            "Until the point in time 7.5 and important thing with the metric is that we actually get some numerical values, then now we can compare to each other to determine the most performant approach.",
            "So in this particular example, we computed the FAT for the three approaches until the point in time 7.5, and these are the results reported by the metric.",
            "So we can observe that the approach not adaptive is the most performant one, because it's the one that was.",
            "Able to produce more answers until the point in time T. So in this case, the interpretation of the metric is higher is better, the higher the value of the metric, the better the performance of the engine."
        ],
        [
            "Then the second metric that we propose is the F at K, and this metric quantifies the efficiency while producing the first K answers of the query and to do so, the metric measures the area under the curve in the interval from zero to TK of the answer distribution function, where K is the point in time where the Kate answer is produced.",
            "So in this case we just have to compute the integral from zero to TK of the answer distribution function.",
            "And in this example, we want to compute the efficiency of producing the 1st 2000 answers and again with the at K. We can have this numerical values which are the area under the curve and the approach that performs the best is the one with the smallest value for this metric, becauses the one that produces the answers at a faster rate.",
            "So the interpretation of this metric in this case is lower is better."
        ],
        [
            "We also propose extensions of the metrics.",
            "The first extension is to measure the efficiency at any time interval, and this is possible to do with the FAT.",
            "For example, to measure the efficiency from a point in time TA to a point in time TV by just computing the deficiency with the FAT at the point in time TV and then we compute the FAT to the point in time, TA, and then we compute the difference between these two.",
            "To get the area under the curve in that exact interval in time.",
            "So in this example, we're computing that the efficiency from the time of five seconds to 7.5 seconds, and these are the results reported by the metric."
        ],
        [
            "The second extension is a very similar extension, but for the F at K and here we want to measure the efficiency between the K-8 answer and ECB answer.",
            "So this is using the scenarios.",
            "For example when we want to measure the deficiency between producing the answer #1000 and they answer #3000 and we apply also the difference between computing the F at K at different values.",
            "In this case at the value KV and the value key A."
        ],
        [
            "And now we also formally demonstrated the properties of our metrics.",
            "First, we stab Lish, the analytical relationship between the two metrics.",
            "If we consider that TK is the point in time when the Kate answer was produced, then the following equality holds.",
            "So the value of the FAT at the point in time TK is the same as the value of the F at K. We also demonstrated two theorems.",
            "The first one is that the deficiency of blocking approaches or approaches that produce all the answers at the same time is always zero, and in the second theorem we demonstrated that in queries where the number of answers is higher than one than the total, the efficiency of the incremental approaches is also higher than 0.",
            "Now to illustrate."
        ],
        [
            "The applications of our metrics were conducted, an empirical study, and we used the query engine an LDE with three different configurations, not adaptive, selective, an random, and we use queries from the LDA benchmark.",
            "One we chose 16 non selective queries and we executed them against the DB pedia data set and these are the technical specifications of the machine where we executed the experiments."
        ],
        [
            "In our first study, we compare the metric DfT with other metrics, and here I'm just showing the results for query Q 17 of the benchmark and I'm plotting that results for the metrics time for the first tuple, execution time, completeness, throughput, and the FAT at the point in time with ending the fastest engine and the execution.",
            "Now in this plot, the further away from the center, the better the performance of the approach and we can observe that in metrics such as execution time, completeness and throughput, the approach is not adaptive and random behave competitive.",
            "However, when we look at our metric, we can observe that random the approach random outperforms nonadaptive and this is actually corroborated.",
            "If we look at the answer distribution function.",
            "Where we can see that the Gray line always over passes the blue line, which indicates that energy random actually outperforms the other approach.",
            "So we conducted this type of study over all the queries of the benchmark."
        ],
        [
            "And with."
        ],
        [
            "Detected that for eight out of the 16 queries, our metrics allow us to uncover unknown patterns that could not be observed with the other metrics.",
            "Then in the second."
        ],
        [
            "In a study we measure the answer rate with the metric DF at K. And here I'm showing the results for query Q Two from the benchmark an in this study were measuring the rate at which the endings produce the first 25% of the answer.",
            "The first 50% of the answer, and so on and so forth.",
            "And these in this case the closer to the center of the plot, the better the performance of the ending.",
            "So we can observe that according to our metrics, they approach selective produces.",
            "The first 25% of the answer's slower than, for example, the random approach.",
            "And when we look at the answer distribution function, we can actually corroborate this, because the approach selected was a bit slower than the other approach.",
            "However, as the execution goes on, the approach selective produces the last portions of the answer.",
            "At a faster rate, and this is actually corroborated here.",
            "When we looked at the light blue line over past the green line."
        ],
        [
            "So with these types of analysis we can observe that the behavior of the engine changes over time an when we look at all the queries in the benchmark, we could observe that this is was actually the case, so the engines will not perform uniformly overtime, but they might change their behavior.",
            "So we need actually to have metrics to measure how they change overtime.",
            "So actually only in two queries of the benchmark, the ones that I'm highlighting here at the only queries for which the approaches behave.",
            "In uniform way."
        ],
        [
            "Now to conclude.",
            "I have presented two metrics, the FAT&EFK which allow us to measure the efficiency of incremental approaches.",
            "In our work we demonstrated the theoretical soundness of the proposed metrics and we also conducted an empirical study which indicates that our metrics allows for uncovering performance, particularity in particular for when comparing the continuous behavior of the approaches.",
            "Now, as a final remark, our metrics can measure the performance of any incremental approach.",
            "In this scenario we used the query endings as an exemplar of an application, but there might be many other contexts in which these metrics can be applied.",
            "And Lastly."
        ],
        [
            "We encourage the community to use this resource.",
            "Therefore, we have made available the following resources.",
            "We published the DFR package to compute these metrics which is openly available in GitHub and we also provide a Jupiter notebook that explains step by step how to use the package and Lastly, we have an online demo that reproduces all the results that are reported in this paper.",
            "So these are the references that I used throughout my presentation, and that's all from my side.",
            "Thank you very much for your attention.",
            "So I was wondering how much work would it be to apply this to some other query processor than the one you have already prepared it for?",
            "So how heavy is it on producing the logs and how heavy is it on development work needed to produce the logs you are expecting?",
            "So just to make sure the questions how hard it is to produce to to develop.",
            "How hard is the development needed to change query processor so that it produces appropriate logs and how heavy an overhead does it impose?",
            "OK, thanks for the question.",
            "Very interesting.",
            "So actually to adapt the Indians to produce the information needed for the metric is very simple.",
            "The only thing that you need to keep is a track of the time stamp where each of the.",
            "Answers was produced, so just make a timestamp at the beginning of correct Siku Shun and then every time the answer is produced then you measure the time and compute the difference between the initial time and the time where the answer was produced and then as a recommendation to avoid overhead while producing the answer.",
            "Just keep all these let's say in main memory and then when the query has finished the execution then to flush all this information to avoid having bottlenecks while writing this information into disk.",
            "To not to not affect the experimental results.",
            "You said using your metrics to compare stream processing engines and usually the metrics that instant processing are used are.",
            "Latency and throughput.",
            "Did you have any?",
            "Did you think about any relation between your metrics on these two?",
            "Yes, thanks for the question.",
            "That's actually very interesting because throughput also combines the number of answers and the execution time.",
            "And maybe I can go back to.",
            "One of my first slides.",
            "Um?",
            "So right here.",
            "So for example, if we use throughput in this particular scenario, we will see up to the 7.5 seconds the throughput will be almost exactly the same for both approaches.",
            "For example, for the one with the dark blue and the one for the grey blue, because the number of answers is the same at the point in time is the same.",
            "However, we can observe that the blue one is outperforming the other one, and this is not really capturing throughput because the limitation of throughput is that it assumes.",
            "That the answers are produced at a uniform rate, an as we can observe here.",
            "The this is not the case.",
            "So for example, here we have a bunch of answers that are produced and the ending stops producing the need continues and so on and so forth.",
            "So we believe that our metric actually captures more information than what is reported by throughput.",
            "But of course all these metrics are complementary, so we was just suggest that traditional metrics should still be reported, but.",
            "In addition, our metrics can provide further insights that are not capturing this traditional metrics."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good afternoon, my name is Maribel Acosta and I'm from KIT and this is a joint work with Media stupid Al Ain York should have better and the title of our work is the efficiency metrics.",
                    "label": 0
                },
                {
                    "sent": "Measuring the continuous efficiency of preprocessing approaches sofa.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, let me start with a motivation, so let's consider that we want to execute this sparkle query against an RDF data set.",
                    "label": 0
                },
                {
                    "sent": "So typically we take this query and we submitted to a query engine and then we get some results and for example in this case we got all the answers at the same time.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of a blocking approach which produces all the results at the exact same time, typically at the end.",
                    "label": 1
                },
                {
                    "sent": "Off the query execution, however, there are other types of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boring ending, so for example, and use that produce the answer overtime this is the definition of incremental approaches which are able to produce results as soon as the data is ready and in the semantic web community there have been proposals for query engines like the ones I'm showing on the slides that are able to produce the results incrementally.",
                    "label": 0
                },
                {
                    "sent": "So we took one of these engines NLD or network.",
                    "label": 0
                },
                {
                    "sent": "Kathleen Data Ed is.",
                    "label": 0
                },
                {
                    "sent": "We run this query.",
                    "label": 0
                },
                {
                    "sent": "Against the pedia.",
                    "label": 0
                },
                {
                    "sent": "And then we measure the performance of the ending.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using three different configurations, so we use energy, not adaptive, selective and random.",
                    "label": 0
                },
                {
                    "sent": "And then we took some traditional metrics like time for the first answer, execution time, throughput or completeness.",
                    "label": 1
                },
                {
                    "sent": "And then we measure the performance which is reported on the table.",
                    "label": 0
                },
                {
                    "sent": "Now if we take a look at execution time and throughput, we can conclude that the variant in LTE random outperforms the other approaches.",
                    "label": 1
                },
                {
                    "sent": "However, these metrics do not consider that the ending was producing the results incrementally.",
                    "label": 0
                },
                {
                    "sent": "So when we look at the continuous performance of the engine by looking at the number of answers produced overtime, we can actually see that in the first 7.5 seconds of query execution.",
                    "label": 1
                },
                {
                    "sent": "The approach not adaptive outperformed the other approaches.",
                    "label": 0
                },
                {
                    "sent": "So by looking at the continuous performance we can actually get further insights about the behavior of the endings that were not possible to be seen with traditional metrics.",
                    "label": 0
                },
                {
                    "sent": "However, in this example we're doing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Qualitative analysis.",
                    "label": 0
                },
                {
                    "sent": "We're just looking at a plot and drawing this conclusion, but in order to properly benchmark our engines, we need.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quantitative methods to measure the continuous efficiency of incremental approaches.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look at the related work.",
                    "label": 0
                },
                {
                    "sent": "Can we look at benchmarks proposed for query processing as?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, as metrics reported in experimental studies of research papers and we collected metrics and classify them into dimensions according of what they measure and we have, for example, metrics that measure effectiveness such as answer completeness or answer correctness and other metrics that focus on efficiency like execution time or queries per second.",
                    "label": 0
                },
                {
                    "sent": "There also metrics that combine these two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Like that combined metric, but all these metrics have a commonality.",
                    "label": 0
                },
                {
                    "sent": "None of these metrics consider their continuous performance of the ending, so they are not tailored to benchmark the performance of incremental approaches.",
                    "label": 1
                },
                {
                    "sent": "So based on this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observations, we propose an approach to measure the continuous efficiency of approaches an we propose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The efficiency metrics and the efficiency in this context is defined as continuous efficiency, and we saw in the motivating example that the continuous performance of the approaches is actually recorded in the answer traces of the endings or in the query logs.",
                    "label": 1
                },
                {
                    "sent": "So for example, here we have for each of the answers that timestamp where the answer was produced.",
                    "label": 0
                },
                {
                    "sent": "So based on the answer Metro or the answer traces.",
                    "label": 0
                },
                {
                    "sent": "Our metrics quantify the efficiency of incremental approaches.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do so, first we build the answer distribution function, which is a function that is defined in the interval from zero to TN, where zero is where the execution of the query starts.",
                    "label": 0
                },
                {
                    "sent": "Anti N is the point in time where the last answer was produced by the engine and the value at the time X of this function indicates the number of answers produced until that point in time.",
                    "label": 1
                },
                {
                    "sent": "Then the answer distribution function can be built from the answer tracers or the query logs by applying linear interpolations between the timestamps recorded in the answer traces.",
                    "label": 0
                },
                {
                    "sent": "So from the answer traces from the endings we can build discontinuous functions that we see on the right side.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we propose our first metric, which is the FAT which quantifies the efficiency during the first tee time units of execution time, and to do so the metric measures the area under the curve in the interval from zero to T. And we can do this by computing the integral from zero to T of the answer distribution function, and in this example I'm highlighting the area under the curve for the approach random.",
                    "label": 1
                },
                {
                    "sent": "Until the point in time 7.5 and important thing with the metric is that we actually get some numerical values, then now we can compare to each other to determine the most performant approach.",
                    "label": 0
                },
                {
                    "sent": "So in this particular example, we computed the FAT for the three approaches until the point in time 7.5, and these are the results reported by the metric.",
                    "label": 0
                },
                {
                    "sent": "So we can observe that the approach not adaptive is the most performant one, because it's the one that was.",
                    "label": 0
                },
                {
                    "sent": "Able to produce more answers until the point in time T. So in this case, the interpretation of the metric is higher is better, the higher the value of the metric, the better the performance of the engine.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the second metric that we propose is the F at K, and this metric quantifies the efficiency while producing the first K answers of the query and to do so, the metric measures the area under the curve in the interval from zero to TK of the answer distribution function, where K is the point in time where the Kate answer is produced.",
                    "label": 1
                },
                {
                    "sent": "So in this case we just have to compute the integral from zero to TK of the answer distribution function.",
                    "label": 0
                },
                {
                    "sent": "And in this example, we want to compute the efficiency of producing the 1st 2000 answers and again with the at K. We can have this numerical values which are the area under the curve and the approach that performs the best is the one with the smallest value for this metric, becauses the one that produces the answers at a faster rate.",
                    "label": 0
                },
                {
                    "sent": "So the interpretation of this metric in this case is lower is better.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also propose extensions of the metrics.",
                    "label": 1
                },
                {
                    "sent": "The first extension is to measure the efficiency at any time interval, and this is possible to do with the FAT.",
                    "label": 1
                },
                {
                    "sent": "For example, to measure the efficiency from a point in time TA to a point in time TV by just computing the deficiency with the FAT at the point in time TV and then we compute the FAT to the point in time, TA, and then we compute the difference between these two.",
                    "label": 0
                },
                {
                    "sent": "To get the area under the curve in that exact interval in time.",
                    "label": 0
                },
                {
                    "sent": "So in this example, we're computing that the efficiency from the time of five seconds to 7.5 seconds, and these are the results reported by the metric.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second extension is a very similar extension, but for the F at K and here we want to measure the efficiency between the K-8 answer and ECB answer.",
                    "label": 1
                },
                {
                    "sent": "So this is using the scenarios.",
                    "label": 0
                },
                {
                    "sent": "For example when we want to measure the deficiency between producing the answer #1000 and they answer #3000 and we apply also the difference between computing the F at K at different values.",
                    "label": 0
                },
                {
                    "sent": "In this case at the value KV and the value key A.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we also formally demonstrated the properties of our metrics.",
                    "label": 0
                },
                {
                    "sent": "First, we stab Lish, the analytical relationship between the two metrics.",
                    "label": 1
                },
                {
                    "sent": "If we consider that TK is the point in time when the Kate answer was produced, then the following equality holds.",
                    "label": 1
                },
                {
                    "sent": "So the value of the FAT at the point in time TK is the same as the value of the F at K. We also demonstrated two theorems.",
                    "label": 0
                },
                {
                    "sent": "The first one is that the deficiency of blocking approaches or approaches that produce all the answers at the same time is always zero, and in the second theorem we demonstrated that in queries where the number of answers is higher than one than the total, the efficiency of the incremental approaches is also higher than 0.",
                    "label": 1
                },
                {
                    "sent": "Now to illustrate.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The applications of our metrics were conducted, an empirical study, and we used the query engine an LDE with three different configurations, not adaptive, selective, an random, and we use queries from the LDA benchmark.",
                    "label": 0
                },
                {
                    "sent": "One we chose 16 non selective queries and we executed them against the DB pedia data set and these are the technical specifications of the machine where we executed the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our first study, we compare the metric DfT with other metrics, and here I'm just showing the results for query Q 17 of the benchmark and I'm plotting that results for the metrics time for the first tuple, execution time, completeness, throughput, and the FAT at the point in time with ending the fastest engine and the execution.",
                    "label": 1
                },
                {
                    "sent": "Now in this plot, the further away from the center, the better the performance of the approach and we can observe that in metrics such as execution time, completeness and throughput, the approach is not adaptive and random behave competitive.",
                    "label": 0
                },
                {
                    "sent": "However, when we look at our metric, we can observe that random the approach random outperforms nonadaptive and this is actually corroborated.",
                    "label": 0
                },
                {
                    "sent": "If we look at the answer distribution function.",
                    "label": 0
                },
                {
                    "sent": "Where we can see that the Gray line always over passes the blue line, which indicates that energy random actually outperforms the other approach.",
                    "label": 0
                },
                {
                    "sent": "So we conducted this type of study over all the queries of the benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detected that for eight out of the 16 queries, our metrics allow us to uncover unknown patterns that could not be observed with the other metrics.",
                    "label": 0
                },
                {
                    "sent": "Then in the second.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a study we measure the answer rate with the metric DF at K. And here I'm showing the results for query Q Two from the benchmark an in this study were measuring the rate at which the endings produce the first 25% of the answer.",
                    "label": 0
                },
                {
                    "sent": "The first 50% of the answer, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And these in this case the closer to the center of the plot, the better the performance of the ending.",
                    "label": 0
                },
                {
                    "sent": "So we can observe that according to our metrics, they approach selective produces.",
                    "label": 0
                },
                {
                    "sent": "The first 25% of the answer's slower than, for example, the random approach.",
                    "label": 0
                },
                {
                    "sent": "And when we look at the answer distribution function, we can actually corroborate this, because the approach selected was a bit slower than the other approach.",
                    "label": 0
                },
                {
                    "sent": "However, as the execution goes on, the approach selective produces the last portions of the answer.",
                    "label": 1
                },
                {
                    "sent": "At a faster rate, and this is actually corroborated here.",
                    "label": 0
                },
                {
                    "sent": "When we looked at the light blue line over past the green line.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with these types of analysis we can observe that the behavior of the engine changes over time an when we look at all the queries in the benchmark, we could observe that this is was actually the case, so the engines will not perform uniformly overtime, but they might change their behavior.",
                    "label": 0
                },
                {
                    "sent": "So we need actually to have metrics to measure how they change overtime.",
                    "label": 0
                },
                {
                    "sent": "So actually only in two queries of the benchmark, the ones that I'm highlighting here at the only queries for which the approaches behave.",
                    "label": 1
                },
                {
                    "sent": "In uniform way.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to conclude.",
                    "label": 0
                },
                {
                    "sent": "I have presented two metrics, the FAT&EFK which allow us to measure the efficiency of incremental approaches.",
                    "label": 1
                },
                {
                    "sent": "In our work we demonstrated the theoretical soundness of the proposed metrics and we also conducted an empirical study which indicates that our metrics allows for uncovering performance, particularity in particular for when comparing the continuous behavior of the approaches.",
                    "label": 1
                },
                {
                    "sent": "Now, as a final remark, our metrics can measure the performance of any incremental approach.",
                    "label": 1
                },
                {
                    "sent": "In this scenario we used the query endings as an exemplar of an application, but there might be many other contexts in which these metrics can be applied.",
                    "label": 0
                },
                {
                    "sent": "And Lastly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We encourage the community to use this resource.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we have made available the following resources.",
                    "label": 0
                },
                {
                    "sent": "We published the DFR package to compute these metrics which is openly available in GitHub and we also provide a Jupiter notebook that explains step by step how to use the package and Lastly, we have an online demo that reproduces all the results that are reported in this paper.",
                    "label": 1
                },
                {
                    "sent": "So these are the references that I used throughout my presentation, and that's all from my side.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering how much work would it be to apply this to some other query processor than the one you have already prepared it for?",
                    "label": 0
                },
                {
                    "sent": "So how heavy is it on producing the logs and how heavy is it on development work needed to produce the logs you are expecting?",
                    "label": 0
                },
                {
                    "sent": "So just to make sure the questions how hard it is to produce to to develop.",
                    "label": 0
                },
                {
                    "sent": "How hard is the development needed to change query processor so that it produces appropriate logs and how heavy an overhead does it impose?",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for the question.",
                    "label": 0
                },
                {
                    "sent": "Very interesting.",
                    "label": 0
                },
                {
                    "sent": "So actually to adapt the Indians to produce the information needed for the metric is very simple.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you need to keep is a track of the time stamp where each of the.",
                    "label": 0
                },
                {
                    "sent": "Answers was produced, so just make a timestamp at the beginning of correct Siku Shun and then every time the answer is produced then you measure the time and compute the difference between the initial time and the time where the answer was produced and then as a recommendation to avoid overhead while producing the answer.",
                    "label": 0
                },
                {
                    "sent": "Just keep all these let's say in main memory and then when the query has finished the execution then to flush all this information to avoid having bottlenecks while writing this information into disk.",
                    "label": 0
                },
                {
                    "sent": "To not to not affect the experimental results.",
                    "label": 0
                },
                {
                    "sent": "You said using your metrics to compare stream processing engines and usually the metrics that instant processing are used are.",
                    "label": 0
                },
                {
                    "sent": "Latency and throughput.",
                    "label": 0
                },
                {
                    "sent": "Did you have any?",
                    "label": 0
                },
                {
                    "sent": "Did you think about any relation between your metrics on these two?",
                    "label": 0
                },
                {
                    "sent": "Yes, thanks for the question.",
                    "label": 0
                },
                {
                    "sent": "That's actually very interesting because throughput also combines the number of answers and the execution time.",
                    "label": 0
                },
                {
                    "sent": "And maybe I can go back to.",
                    "label": 0
                },
                {
                    "sent": "One of my first slides.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So right here.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we use throughput in this particular scenario, we will see up to the 7.5 seconds the throughput will be almost exactly the same for both approaches.",
                    "label": 0
                },
                {
                    "sent": "For example, for the one with the dark blue and the one for the grey blue, because the number of answers is the same at the point in time is the same.",
                    "label": 0
                },
                {
                    "sent": "However, we can observe that the blue one is outperforming the other one, and this is not really capturing throughput because the limitation of throughput is that it assumes.",
                    "label": 0
                },
                {
                    "sent": "That the answers are produced at a uniform rate, an as we can observe here.",
                    "label": 0
                },
                {
                    "sent": "The this is not the case.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we have a bunch of answers that are produced and the ending stops producing the need continues and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we believe that our metric actually captures more information than what is reported by throughput.",
                    "label": 0
                },
                {
                    "sent": "But of course all these metrics are complementary, so we was just suggest that traditional metrics should still be reported, but.",
                    "label": 0
                },
                {
                    "sent": "In addition, our metrics can provide further insights that are not capturing this traditional metrics.",
                    "label": 0
                }
            ]
        }
    }
}