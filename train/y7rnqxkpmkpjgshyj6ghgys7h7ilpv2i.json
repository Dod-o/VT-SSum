{
    "id": "y7rnqxkpmkpjgshyj6ghgys7h7ilpv2i",
    "title": "Learning with Gaussian Processes",
    "info": {
        "author": [
            "Carl Edward Rasmussen, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_rasmussen_lgp/",
    "segmentation": [
        [
            "Call we're lucky to have car Rasmussen College.",
            "He's a PhD with Jeff Hinton and then after that I believe post opting the Gatsby.",
            "To prove it spent some time in Denmark and then moved to Bingham where he led the Central European Bayesians for a number of years.",
            "Night in the middle of darkness there.",
            "Since then, he's being recruited to Cambridge, where he's now a lecturer.",
            "His research career as a. I think since the start when I first came across works is 1996, sixty pieces would really like the most.",
            "The launch Gaussian process is on the machine learning community in it was very much written well in the context of work he done with Chris Williams, previous speaker and the pair of them sort of letters.",
            "Small community of.",
            "Intrepid Gaussian process operators over the next eight years until they finally gave away all the secrets of the field by writing an excellent book on it's called Gaussian Process Machine Learning, which I highly recommend you purchase, so he's an ideal choice to give us a talk on water, because partially due to their book and partially due to their service over the years, become one of the most active areas of research in machine learning today.",
            "And that's counting process is so over to can't.",
            "Thanks, Neil.",
            "Yeah, so today I'm going to try and tell you about Gaussian processes and I have this.",
            "I have this idea that Gaussian process is sort of the simplest way that you can.",
            "You can solve regression problems or learning about a function.",
            "So one of the one of the very basic things that you can try and think about in machine learning.",
            "So you might have to ask you again in 2 hours time whether you agree with me.",
            "This is simplest way you could possibly do this right?",
            "So if you don't say yes then I've failed.",
            "So I started thinking about these things about 15 years ago in Toronto, when I would when I was doing my PhD and there was a guy in what was known as the neuron lab that was machine learning was called neural networks back then called Radford Neal and he was he goes into doing Bayesian inference on very large neural networks and had these complicated Monte Carlo based methods for doing inference and he had a line in his thesis.",
            "It said somewhere you know, actually in the limit of the neural network growing to being infinitely large.",
            "This actually converges to something called a Gaussian process, and it might be easier doing inference if you look at it that way.",
            "So actually that got me thinking about what does that actually mean?",
            "What does this Gaussian crossword Gaussian process mean?",
            "And Chris Williams, who just spoke before me and myself.",
            "We were sort of talking about this and trying to figure out what did that mean.",
            "Could you actually, how could you do inference there?",
            "And I remember we were talking about this for a few months and after a little while, Chris sent me a draft paper where he'd written down some equations.",
            "I found that all very confusing, but I could type them into Matlab, right?",
            "So I did that.",
            "So I typed in equations and then I ran it on my data set and work really well.",
            "Actually work better than anything else I had.",
            "Right, so this is the opposite situation that you're normally in in research, right?",
            "Normally you have these great ideas that just happened not to work right, and this was the opposite, right?",
            "You had no idea what was really going on, but it really worked.",
            "So I'm going to try to.",
            "I'm going to try to persuade you that it's possible to give you some of the abstract notion of what is the Gaussian process, right?",
            "But actually I find that that's not very helpful, so I'm going to try to do it sort of backwards to try and try and pull things apart, and really to look under the hood and sort of think about the basic problem of what are we doing when we're trying to do inference about a function where we're learning about a function.",
            "OK, and it's going to end up being based on these stochastic processes, and if you don't know anything about sarcastic posts, is that OK?",
            "Value the things that you need to know about it right?",
            "And I'm not very good at mathematics.",
            "And actually you don't need to be very good enough.",
            "Value.",
            "Some very pedestrian notions to try and get at things that you can also find in very thick books."
        ],
        [
            "OK.",
            "So so so.",
            "So I'm going to try and set out here some of the some of the problems which I'll try and persuade you that Gaussian process actually the answer too, right?",
            "So if you're trying to learn about, let's say regression, you're trying to learn about function, then you want to answer a bunch of questions more or less always right?",
            "So you want to want to know well, how do I access how to actually fit this model to the data?",
            "How do model selection if I have two different types of models?",
            "How do I know which one to use over?",
            "Another one, and how do I interpret what's going on can actually understand what the model is doing.",
            "And things about you know, to what extent can I can I trust the predictions?",
            "Things like that, and so I'll try to persuade you that Gaussian processes can help you solve all these problems insofar as it is solution, right?",
            "Some of these problems there are sort of good reasons why there isn't a clear cut solution to the problem, right?",
            "And the Gaussian process will also help understand why that's the case and why there are some sort of an inherent problem."
        ],
        [
            "An OK, so we'll have a break in the middle, so hopefully I'll cover most of these sort of foundations part and I'd like you to ask questions when we start.",
            "When we start going right, there will be some notions that are maybe a little bit mysterious, like if you don't break me, then I won't notice that you're that you're not with me anymore, right?",
            "So to ask questions, if we only manage the first part, that's fine, right?",
            "This second part is sort of more, more special ways of using this."
        ],
        [
            "There's some colorful pictures in there, but apart from that.",
            "OK, so let's let's get started.",
            "So Gaussian process are based on Gaussian distribution.",
            "So I'll just refresh for you what a Gaussian distribution is.",
            "I'm going to say this.",
            "Station here, so I'm going to say P. Of X, so X is a is a multivariate thing here P of X given mu and Sigma.",
            "So MU is the mean and Sigma is the variance or covariance.",
            "In the multivariate case.",
            "So here I have two Gaussian distributions with different means.",
            "So the mean is for the for the black distributions.",
            "Here the mean for blue distributions over here and I've marked also the standard deviation.",
            "So this is a bit a bit wider distribution because the distribution normalized to one.",
            "It's also going to have a smaller magnitude than this one over here.",
            "So this is 1 dimensional Gaussian distribution which is easy to draw, so I can also more or less draw 2 dimensional distribution.",
            "But it's very hard to draw a higher dimensional distributions and it turns out that Gaussian processes are just very high dimensional Gaussian distributions like, so we have to imagine a high dimensional Gaussian.",
            "So here's a 2 dimensional Gaussian.",
            "So instead of instead of sort of showing you the the the actual value of the probability density.",
            "I've just shown you here the mean.",
            "I've shown you the two... here that correspond to the one standard deviation and the two stand deviations.",
            "And the and the variance here is encoded in the covariance matrix of the covariance matrix.",
            "In this case, has two eigenvectors eigenvectors, one that goes in this direction and one that goes in this direction and the eigenvalue going in this direction.",
            "So the variance in that direction corresponds to the eigenvalue of the covariance.",
            "And here I've written out the actual distribution and it's gory detail."
        ],
        [
            "OK, so the two properties of these Gaussians that are going to be of central importance, so one is called the conditional distribution and one is called the marginal distribution.",
            "So conditional distribution.",
            "So here I've got my 2 dimensional Gaussian from before and the conditional distribution of 1 variable given the other variable.",
            "I've tried to illustrate that here, so let's say so this is the joint distribution over the two dimensional space here and now.",
            "Let's say that I observe that the value of this variable.",
            "And this variable has this.",
            "At this variable, sorry, had this particular value OK, and now I say OK.",
            "If this variable has this particular value, then what does that imply about the other variable?",
            "Alright, So what I'm doing here is sort of I'm slicing through the distribution and seeing what's leftover as a function of the other variable, right?",
            "And you can see over here if I know that this is the value of this variable, then the probability that the this variable takes on some negative value of some value over here is very, very small, right?",
            "Because there isn't any mass down here, so the masses is over here, so the conditional distribution here looks something like this and.",
            "And there's an interesting fact about Gaussians is that if you condition a Gaussian, then you get another Gaussian, right?",
            "So the conditional distribution here is also Gaussian.",
            "And the last thing we need to know about is marginal distribution.",
            "So marginal distributions is just that you like to some out or integrate out one of the variables and look at what is now the marginal distribution for the other variable.",
            "So I'm just collect collapsing or projecting down this joint distribution onto one particular axis.",
            "And again the marginal distribution of when you marginalized the Gaussian, you get another Gaussian."
        ],
        [
            "OK, so now what is a Gaussian process?",
            "Well, I got some process is just generalization of a multivariate Gaussian to infinitely many variables.",
            "So just like so it's sort of a natural extension.",
            "When you go from a from a univariate Gaussian to a multivariate Gaussian.",
            "And now we just go to a larger multivariate Gaussian right?",
            "So a Gaussian that contains infinitely very very many variables.",
            "So now.",
            "One can ask, well, why are we interested in infinitely many variables?",
            "That sounds that sounds to be a sort of odd thing to be interested in, but the reason for this is that we can think of functions and functions are sort of natural things to try and sort of infer properties of.",
            "You can think of a function as just doing an infinitely long vector.",
            "Right, so because you can simply specify what is the value of the function for any value of the argument X. OK, so this would be an infinitely long vector, so this is.",
            "It seems like a very primitive notion, but it conveys exactly what we need.",
            "Right, So what I'm getting at here is that a Gaussian process can be used to specify a distribution over functions, right?",
            "And if we want to make probabilistic inference about functions, then we better have a way to write down distributions over these objects, right?",
            "Because if you can't do that, then we can't apply probability theory.",
            "OK, and the Gaussian process is exactly the object that we use to handle distribution or function to specify them and to manipulate them.",
            "OK, so the formal definition here is a Gaussian processes collection of random variables.",
            "Any finite number of which have Gaussian distributions.",
            "So mathematically there are number of technicalities involved in having infinite collections of things, right?",
            "So if you're strict mathematician, then you might wonder, is this this really well defined?",
            "I'm not a strict mathematician, I don't care about these things.",
            "But that's why.",
            "So that's the mathematicians definition here, right?",
            "So they try to avoid saying anything about infinities, right?",
            "So they just say, well, it's an infinite collection, but only that any finite number of which have consistent Gaussian distributions, right?",
            "So they haven't said anything about things that might be.",
            "Unpleasant.",
            "Alright, but we don't.",
            "We don't.",
            "Actually, we don't need to worry about that thing.",
            "Nothing will nothing bad will happen here.",
            "OK, so just like a Gaussian distribution here is now specified fully specified by a mean vector and covariance matrix.",
            "A Gaussian process is just specified in terms of the infinitely long versions of that right.",
            "So instead of having a mean vector, I now have the Gaussian process here hasn't had an infinitely long mean vector right?",
            "And infinitely long vectors are functions, right?",
            "OK, so the Gaussian process has a mean function, and similarly it doesn't have a covariance matrix, but it has a covariance matrix here, which is an infinite by infinite dimensional matrix.",
            "Right, so that's that's a function with two arguments.",
            "OK, so so formally at least we can write down something like this.",
            "Alright."
        ],
        [
            "So now.",
            "At this point, you might be a little bit worried, right?",
            "Because you can't actually write down an infinitely long vector anywhere, right?",
            "So if this machinery that I'm going to develop will require you to write that down in your computer, for example, then you know already that you know you can't do it exactly, at least, and it might.",
            "Things might get very hairy, but it turns out that things don't get hairy, right?",
            "And the reason why things will turn out OK is what's known as the what I call the marginalization property.",
            "Alright, so basically it has to do with the fact that marginal distribution of Gaussian of a joint Gaussian is again a Gaussian, right?",
            "So if we have something which is jointly jointly has a Gaussian distribution, and if we now marginalized out the distribution of Y then we get back the distribution of X and if this joint distribution was Gaussian, then the distribution of X will also be Gaussian, right?",
            "And both X&Y could be vectors in this case.",
            "OK, so now.",
            "So let's pretend that we now have a full Gaussian process, right?",
            "So we have we have an infinite and infinitely large long vector here, right?",
            "So now the joint distribution over, let's say we had.",
            "Let's say we had a number of points that we were actually interested in, and then we had all the other points.",
            "Why over here?",
            "Right now, if you write down the joint distribution of this, that will have a mean where the mean of the X vector will be a in the mean of the Y vector will be, and the covariances between the access will be A and they cross covariances will begin by B, right?",
            "So where this thing here?",
            "If this is a Gaussian process and this thing here will be will be an infinitely large.",
            "Matrix C matrix.",
            "But the nice thing here is that the marginal distribution for X is just going to be a Gaussian distribution with mean A.",
            "An covariance matrix capital A, right?",
            "So it means if you ask finite dimensional questions about this infinite dimensional objects, then you get you get simply get a finite dimensional answer right?",
            "And the finite dimensional answer doesn't depend on all these things that were going on all the other coordinates of this very very long vector.",
            "Right, so this sort of motivates that maybe there's hope that you can do inference using these objects without writing down everything."
        ],
        [
            "OK, so.",
            "I told you that.",
            "The way I'm going to use Gaussian process is to specify distributions of our functions, right?",
            "So now let me try to persuade you that that Gaussian process really is a distribution or functions, and the way I'm going to do that is I'm going to draw some random samples from that distribution.",
            "So let's write down some Gaussian process, so we have a Gaussian process here, which will have a mean of zero.",
            "And actually I think for the rest of the talk today, my Gaussian process will always have means of 0.",
            "This seems like this seems like a big restriction, but it actually turns out that it's not terribly important, so I always have zero mean Gaussian process.",
            "I'm more interested in what happens in the covariance function here, so here's an example of a covariance function.",
            "Equivalent function is E to the minus distance between.",
            "X&X prime squared.",
            "OK, so this is also sort of has a Gaussian look to it right?",
            "But this is not a Gaussian process because this looks Gaussian, it's already a Gaussian process.",
            "If this was some other function, then it could also still be a Gaussian process.",
            "Now it's going to be a Gaussian process of the inputs, right?",
            "So the index set to the Gaussian process are going to be my input.",
            "So if I'm going to use this to model a function and the function Maps inputs to outputs like, then the X is are going to be the the index set to the random variables, right?",
            "So this is important that a lot of people have maybe heard about stochastic process in the time domain.",
            "But I'm not going to talk about time here.",
            "There is no time.",
            "So the index set, which is usually given by time in this case, is given by the inputs.",
            "OK.",
            "So now if I have specified now this specifies a Gaussian process, right?",
            "I specify the mean function and the covariance function.",
            "I'm going to tell you a lot more about covariance functions later on.",
            "For now, let's just let's just choose this one.",
            "So once I've done that.",
            "I can now say, well.",
            "OK, how do I look at a function?",
            "Well I look at a function by plotting the function value at some points.",
            "OK, so let me just do that.",
            "So I invent a bunch of X points.",
            "Let's say the numbers between 1:02 hundred or something like that.",
            "I just choose some X is.",
            "So once I've chosen the X is.",
            "I can then worry about well what are the corresponding values of the function evaluated at those points, right?",
            "So I'm interested in F at X1, the function values X1 and FX-2 and so on, right lump all those together in a vector which I call BF XF here, right?",
            "And now use the marginalization property.",
            "Now I say OK if the function was drawn from this Gaussian process then I had this big infinite dimensional thing.",
            "But I'm only interested in a subset of the values right?",
            "Namely these ones right?",
            "And the distribution of these ones will just be just as we had before, the marginal distribution of the X variable was just Gaussian with mean a, an covariance matrix capital A, so that will just be the mean vector will be.",
            "Zero in this case, in the covariance matrix will just be the covariance function evaluated at all pairs of pairs of X is right, so the top diagonal element will be K evaluated at X1 and X1.",
            "And so on.",
            "OK, so this defines now a Gaussian distribution, so this is no longer a Gaussian process.",
            "But this is now a Gaussian distribution.",
            "So saying that the Gaussian distribution implies that the joint distribution of F follow a Gaussian and follows the Gaussian that has this particular covariance matrix, OK?",
            "So let me try to actually do this."
        ],
        [
            "OK, so here I picked a bunch of of of X is here.",
            "I think I picked maybe 20 or something like that.",
            "I just picked them randomly in this interval.",
            "And then I wrote down this joint Gaussian distribution, which would now be a 20 by 20, have a 20 by 20 covariance matrix, and then I draw a random sample from that distribution, right?",
            "So a random sample from a 20 dimensional Gaussian is a vector with 20 dimensions.",
            "And then I just plot the Y values of corresponding to those X values.",
            "And you can see what happens.",
            "What happens here is that you can sort of see that there might be an underlying underlying function here, right?",
            "These things are not independent of each other, definitely right?",
            "And actually, and so we can sort of wonder, you know what are the properties of this function?",
            "Why do the samples actually look like this right?",
            "And if we go back?"
        ],
        [
            "To the previous slide, here we can say, well, the properties of functions actually depend on the properties of the Korean function and the covariance function.",
            "Here said that well, if X&X prime.",
            "If the two axes are very close to each other, then the covariance will be E to the minus, something close to 0 is 1, right?",
            "So then they will have covariance if the X is are very far apart, then you will have eaten the minus some large positive number, which is almost zero right?",
            "So say that the targets corresponding to the.",
            "To these inputs will then not covary very much right, and that's."
        ],
        [
            "Exactly what we sort of see happening in the picture that what the function does over here doesn't really seem to.",
            "Is not doesn't seem to be particularly influenced about what the what the function does over here, right?",
            "But what the function does at this point?",
            "At this point is somehow heavily correlated."
        ],
        [
            "Now this this this already seems maybe a little bit mysterious, right?",
            "Because you can sort of say, well, where did that function come from?",
            "Right, it seems a little odd that I didn't.",
            "I didn't write down.",
            "A parametric function for the for this thing like this function, just sort of sort of came from.",
            "Yeah, where did they come from?",
            "So I have a little illustration here where you can sort of see.",
            "Sort of where where the function is in this in this system, and so one way of thinking about this high dimensional Gaussian is you can try to fax about factorize it, right?",
            "So you can have the high dimensional Gaussian, is it?",
            "It's a joint distribution of all the all the F values that we were interested in conditioned on all the input variables.",
            "An and you can always factorize a distribution in this form here.",
            "So you can say well, the distribution here is the probability of of the one variable given the corresponding axis.",
            "Always course probability of the first variable times the probability of the second variable given the previous one.",
            "Times the probability of the third variable given the two previous ones.",
            "OK, you can always factorize a distribution like that.",
            "And the nice thing about thinking about a joint distribution as a product of factors of this form is that all of these factors are just one dimensional right?",
            "So they are easier to think about.",
            "It is very hard to make a picture or think about high dimensional Gaussians.",
            "But now we can.",
            "We can we can make factorize this joint distribution in terms of these conditionals that are each owner distribution for a single variable.",
            "So that means that now now we can use this to generate the samples right before I just draw, I just draw a random sample from this high dimensional Gaussian.",
            "Now let's try to do this using this formula.",
            "So now we can draw them sequentially.",
            "Can first draw the first Gaussian for the first random variable, and then I can draw the second one given the first one, and so on.",
            "And then we can see how things develop.",
            "And if you do that, then you will need the rule for how you draw.",
            "What are conditional distributions for Gaussians, right?",
            "So if you have a joint Gaussian here, I've just written this down for reference.",
            "So there's a particular formula for how to find the Gaussian, which is the conditional distribution of 1 variable given another one.",
            "OK, so just written down this for for reference.",
            "So in the coffee break or instead of the coffee break you might try to do this on your laptop or you can do it maybe at home tonight.",
            "Or if you want to skip dinner or something.",
            "OK so I give you the formula here.",
            "Um?",
            "Alright, so now let me try to walk you through how this works.",
            "So in this case I have the input values here and how the output values here.",
            "OK, so now I'm just going to draw these samples from the function, so now.",
            "So the first thing I do is I pick an X value, right?",
            "I say I'm interested in what the function does right here, so I've picked X value and I pick this value here.",
            "Now when I pick this value I need now to draw a random sample from the conditional distribution of the value corresponding to that X value.",
            "Given the other ones, but there aren't any at the moment, right?",
            "So so it's just drawn from that distribution.",
            "OK, so now I draw the Gaussian distribution is illustrated by the width of the Gaussian is illustrated by this by the grey zone here.",
            "Become a little bit clearer as we go along, and the mean function here initially is just zero, just as we had before.",
            "OK, so I picked something.",
            "Actually this is a random sample.",
            "It looks very close to zero, it's just a random sample from that.",
            "OK, and haven't fixed the seed, so I don't know what's going to happen now.",
            "OK so I picked this thing OK so now the grey zone says well.",
            "If you pick random samples that are close to the sample like, then it has to covary with this value, right?",
            "So the conditional distribution.",
            "Is now changed from before, right?",
            "Because you're conditioning on the observation that you already made OK, Now I pick another X at random.",
            "OK, picked a value over here.",
            "So now the distribution over this for this variable happens to be not very influenced by this value, right?",
            "So the conditional distribution of this one, given this one would be the same as just the distribution of this variable.",
            "OK so I picked a random sample, so here it was a little bit below the mean.",
            "I do the same thing, so now the the distribution for the next variable given these two variables look like this, right?",
            "So if I'm close to this point then the function will have to agree with this value over here, but out here for example, things are not very influenced by what we've already observed.",
            "So if I just keep going doing this.",
            "You can see so now at this point I'm now picking a random variable which is heavily influenced by this point and also to some extent influenced by this point.",
            "So you can sort of see that the underlying function is sort of.",
            "It's characterized by the other data points, right?",
            "So it's the data points themselves which are characterizing the function like I don't have any explicit formula for the function.",
            "Inside my machine, right?",
            "OK, this question.",
            "What what's on the X axis here is just the input to the function.",
            "Alright, so there's going to be some.",
            "There's some functional relationship between the output and the input of this function, right?",
            "And currently I don't know what that function is right?",
            "But having made some observations about the function has told me something about that function, so it hasn't told me anything about what the function is doing in this area, but it has told me what's doing what is doing around this area and around this area.",
            "So if I continue doing this.",
            "Then after.",
            "Seeing more and more observations.",
            "You will find that things will collapse towards.",
            "And you can see it now.",
            "You can sort of visualize the underlying function right?",
            "And if I sample more and more samples here then they will lie exactly on that function right?",
            "Because the function is already been pinned down by these things over here over here, it hasn't been pinned down exactly yet.",
            "Yep.",
            "That's right, so it's it's.",
            "It's the mean.",
            "This greystone is mean plus and minus two times the standard deviation of the of the conditional distribution.",
            "Get a point.",
            "Different shapes like Johnson.",
            "Yeah.",
            "OK, so the question is, you know why was it that that you had these funny shapes happening as we went along with the sampling right?",
            "And so I only compute the conditional distribution, right?",
            "So I only use that formula on the previous slide, right?",
            "And it just happens that that's the way.",
            "So I just plug into this to this formula here right?",
            "So for for every one of these I just look at, well, what's the probability at a new F given any possible point on the on the on the X axis, right?",
            "So I haven't sort of injected anything into the system other than the covariance function.",
            "I specify the covariance function, but everything followed from that.",
            "So it's actually really is actually really interesting to try and do this on your computer, right?",
            "Because it's a it's a, it's not a very usual way to think about functions, right?"
        ],
        [
            "So here's a picture of doing this in a 2 dimensional case.",
            "So here I have an input which is which is of two dimensions.",
            "And I have a, so I've drawn.",
            "Now my ex is our.",
            "So there's 100 points along this axis and 100 points along this axis, so my input space contains 10,000 points.",
            "Right?",
            "That means met my my covariance function.",
            "So I have an entry in the covariance function for each pair of points, right?",
            "So that means my covariance matrix is a 10,000 by 10,000 matrix.",
            "Like and I draw a point from that.",
            "10,000 dimensional Gaussian is at 10,000 dimensional vector, right?",
            "And I can plot the values of that vector as a function of those corresponding X values, right?",
            "And this is the function that I draw this random function from that distribution, yes?",
            "Yeah, so so the question is, you know if I'm if I'm making inference about functions that I don't know, then how can I know the covariances?",
            "So until now I've only been trying to characterize the notion of a Gaussian process, right?",
            "So we haven't.",
            "So this is all about random functions, right?",
            "So I haven't actually told you about you know how would you actually.",
            "Usually you're not interested in random functions, right?",
            "You're interested in functions that are useful for predicting whatever it is you're interested in, right?",
            "So we haven't gotten that far yet, so but the distribution or functions specify that there's a certain covariance and and the covariance depends in a certain way upon the upon the inputs.",
            "And this somehow implies something about the functions, right?",
            "And I'll tell you more about that.",
            "Exactly how that works, yeah?",
            "OK, so the Gaussian process defines the distribution of functions right?",
            "And this is 1 function drawn from that distribution or functions.",
            "Right and I can only show you.",
            "A finite number of function values, right?",
            "So it's only 10,000 values here.",
            "Well, in the limit the picture would look the same, right?",
            "Because all the points that would be in between would also be in between, because this particular Gaussian process.",
            "With this, with this covariance function.",
            "Actually has the property that generates smooth functions.",
            "Alright, so if I drew this on an even finer grid then it will look the same.",
            "Yeah, so the function is a function of X1 and X2 and this is a function value F over here and this is just to illustrate that in my simple example I had a function which was from 1 dimensional X2A1 dimensional F of X but that need not be the case right?",
            "You can also have the X is can live in any space.",
            "Yep.",
            "What happens when you specify the general way how different data points must?",
            "And then you choose.",
            "That's right, so the covariance structure.",
            "The covariance structure tells you something about how the what the dependencies between cases, right?",
            "So if I say well conditioned on that, I have this point that tells me something about what's going on elsewhere, right?",
            "And that's enforced exactly what we're trying to utilize when trying to learn about function, were saying, oh, making this observation actually tells me something about the function, not just at that point, but also at nearby points in some sense.",
            "Right and we have to get closer to exactly what is that sense and try to characterize that yeah?",
            "Choose your ex is in there.",
            "No, so that the ordering of the ex is completely irrelevant.",
            "OK, so this is this is a random sample from a Bell shaped curve, right?",
            "So if you have a Bell shaped curve in one dimension, then if you draw a random sample from that then it's a number.",
            "OK, now I have a high dimensional one so when I draw a sample from that distribution.",
            "I can't show you a picture of the high dimensional Gaussian, but if you draw a sample from it, it's a vector right?",
            "And I'm showing you a picture of that vector.",
            "Exactly, yeah."
        ],
        [
            "OK, now.",
            "As somebody have already mentioned, you know this was only about random functions, right?",
            "So how do we actually?",
            "How do we use this to learn something about our functions?",
            "So now I'm going to.",
            "I'm going to tell you how to do that in in a three step procedure.",
            "1st I'm going to talk about how to do maximum likelihood and what I call a parametric method parametric model.",
            "Then I'm going to show you how to do Bayesian inference in a parametric model, and then I'm going to show you how to do parameter based on inference in what in this thing which is known as a nonparametric model.",
            "The reason why is known as a nonparametric model as opposed to a parametric model is in the parametric model we usually specify.",
            "A functional form of the thing we're trying to fit, and it might have a bunch of free parameters like and now the inference task is to find out what are the most likely values or what are likely values or typical values, or some notion of typicality.",
            "What are good values for those parameters?",
            "But that's not what's going on in our model, like we haven't explicitly written down a functional form, right?",
            "We're just treating it in this nonparametric way.",
            "And actually.",
            "It turns out that when you are used to thinking about this, the whole process of doing inference is much simpler, because when you're doing when you're using a parametric model.",
            "Then the inference procedure.",
            "The inference step is quite complicated, right?",
            "You have first of all, you have some notion about the functions that you're interested in.",
            "Then you formalize that in terms of a parametric model.",
            "Then you have to say then you have to have your assumptions about the parameters in the parametric model.",
            "Then you do inference about the parameters and that implies something about what's going on in the function.",
            "That was quite a lot of steps, right?",
            "What we're going to do here is just say what we have some notion about the weather functions are.",
            "Then we condition on the data and then we're done.",
            "Like there's no moving between.",
            "As a parametric family of functions and moving back to the function space, we're just doing everything directly in the class of functions.",
            "But since that's a little bit unusual to do things that way, I'm going to tell you how to do this inference in a slightly different way.",
            "OK, so supervised learning, so this prove supervised parametric learning is what you're used to thinking about as learning.",
            "So we have a data set so collections of X is and wise and we have a model that says that the wise are related to the FS here or corrupted.",
            "Maybe buy some small amount of noise right?",
            "And the F value here the function value the function is parameterized by a vector of parameters W and it's the WS that I don't know.",
            "Right, so my task is now to figure out what are good values in the WS.",
            "So one way of doing that is based on what's known as.",
            "Maximum likelihood, so we can write down first, the likelihood the likelihood function is the probability of the data given the model given the model parameters probability of the data given the parameters.",
            "OK, so here I've written the probability of the output given the inputs and the parameters, but that's because I'm interested in doing supervised learning and supervised learning.",
            "You just learn the the outputs given the inputs.",
            "It's a purely conditional model, right?",
            "We're not interested in actually modeling where those excess came from.",
            "The X is, the inputs are somehow given.",
            "OK, so the probability of the data is P of Y given X and the likelihood function is P of Y given X&W.",
            "And this M here is just if you had different models there might be there might be different models.",
            "You also have to condition on which model you're using, right?",
            "In this case, if the noise here was Gaussian, then the likelihood function is Gaussian is basically saying that the discrepancy between Y&F so y -- F. Here is what distributors as epsilon here.",
            "That's a Gaussian distribution, and in the maximum likelihood framework, what you do is you say, well, I don't know what the parameters of WR, but maybe I could try to find the parameters that maximize the likelihood if I make the probability of the outputs as large as possible, so that would sort of seem to be the most likely values of W, right?",
            "So what we do here is you do the arc Max of the likelihood function over W and that's your maximum likelihood estimate of the parameters.",
            "And then, once you've done that, you've trained your model.",
            "An what do you want to do?",
            "Your model you want to make predictions, so you plug in to make predictions.",
            "You plug in the best possible estimate your head of your parameters.",
            "You plug that into the likelihood function, and that gives you.",
            "Now they predict if distribution will be the same as this distribution up here, except you're now plugging in W maximum likelihood, and I use the star notation here to indicate that I'm looking at Test quantities.",
            "OK, so that's what you're probably most used used when you when you're fitting data right?",
            "So in this case, doing maximum likelihood.",
            "This is the same as doing least squares, right?",
            "Because you can look at the you can minimize minus the log likelihood instead.",
            "That corresponds to doing these squares."
        ],
        [
            "Now what we want to do is we want to do probabilistic inference.",
            "We want to do Bayesian inference.",
            "On this thing.",
            "So how do you do?",
            "How do you do Bayesian inference?",
            "Well, you start by specifying the model in exactly the same way you have exactly the same likelihood function.",
            "But in this case we are interested in the posterior distribution over the parameters.",
            "Anne.",
            "So the posterior distribution over the parameters to use, you compute that using Bayes rule.",
            "So base rules tells you how to get from the likelihood, which is the probability of Y given X&W to the probability of W given X&Y, right?",
            "So the Bayes rule is used to swap.",
            "What you are taking the probability over and what you're conditioning over right?",
            "So and if we want to write down based rule then we have to have here the probability we have to multiply this by the probability of W. OK, and everything is conditioned on on on EM here and essentially this should also be conditional X.",
            "Normally you see base rule written as.",
            "PFA given be.",
            "Times, Peter B.",
            "Over a.",
            "Recipe of be given a.",
            "Right, but I'm allowed to Bayes rule.",
            "I'm allowed to extend that to condition everywhere on some other variables, right?",
            "So in this form of Bayes rule, I've Additionally conditioned everywhere on MI and I've conditioned everywhere on X also, except that this thing here, which also ought to be conditioned on X, is actually independent of X, so I don't need to explicitly write that conditioning.",
            "So what is this thing where this thing is our?",
            "It's called the prior on the parameters.",
            "It's what you know about the parameters before you see the data.",
            "Right?",
            "And so this thing of course implies something about what possible values of the.",
            "Of the parameters could be and.",
            "The prior also tells you something about what the what.",
            "Values of WR.",
            "Right, and this is a.",
            "At the same time, the.",
            "So at the moment you could sort of say, well, this looks like a sort of a weakness of the procedure that you that you would have to specify something about those parameters.",
            "Maybe you would just want to learn them from the data, but it turns out that it's actually not possible to do inference if you're not willing to assume anything.",
            "If you're not willing to assume anything about your functions, then the function could do anything between two data points.",
            "Right, so so it's not really possible to do inference in a meaningful way, right?",
            "So this is somehow specifying the knowledge about the functions.",
            "And it turns out that it's crucial that we understand what that knowledge or over the over the functions are, right?",
            "And I'll come back to exactly that.",
            "That actually turns out to be a strength of the Gaussian process framework, so my posterior distribution will look like this, and it's just notice that it's just the product of the likelihood and the prior, and then normalized with the with the with the term here that doesn't depend on W. This just ensures that this product, when it's normalized this way, ends up being at.",
            "Properly normalized distribution over W. But essentially, the posterior is just the product of the.",
            "And of the knowledge from the prior compared combined with the knowledge from the likelihood."
        ],
        [
            "OK, and now when I've computed the posterior, then when I want to make predictions.",
            "My predictions are going to be given by this equation here, so this equation is saying So what we're interested in is the probability of and you have a test output given a test input, and given that we've already observed a particular data set right?",
            "But notice here we don't want to condition condition on a on a particular value of W. Right, because we don't know exactly what values what the right value of W is.",
            "So what we do instead is we average the predictions for particular values, weighted by how likely the different WSR, how probable they are under the posterior distribution.",
            "That's sort of the interpretation of what's going on here, But actually this is just a rule of probability theory.",
            "Right probability of Y given W times the probability of W is a joint distribution of Y star and W. And then I'm marginalized out West.",
            "That gives me the probability of Y star.",
            "OK. Anne.",
            "Right, so let's try to.",
            "Anne.",
            "Yeah, so that's so.",
            "There's another thing one can also ask about and that is.",
            "The what's known as the marginal likelihood.",
            "So one thing that I need to do is that if I trained a bunch of different models, then I want to know which of these models is actually best, right?",
            "And what we can do is just we can ask for what's the probability of the different models given the observed data.",
            "Again, I can just use Bayes rule to swap this around, right?",
            "So it's going to be equal to the prior over the models times this object here, which is called the marginal likelihood.",
            "So it's the probability of Y given X&M but not given W. It's called the marginal likelihood because the parameter has been marginalized out.",
            "So.",
            "So the marginal likelihood here is just the normalization constant that we got from the posterior, right?",
            "So it's the integral of the prior times likelihood, and again it's just a rule of probability, right?",
            "It's the probability of Y given W times the probability of W is a joint of Y&W, and now integrate out West.",
            "And that gives me now the probability of Y.",
            "Everything conditioned on X&M.",
            "So the marginal likelihood is important because it tells us it helps us to figure out how good are the models.",
            "What's the probability of the model, right?",
            "Actually, the probability of the model is proportional to the marginal likelihood for the model.",
            "Times the prior on the model, but usually we don't have strong priors on the models, right?",
            "Usually we would only.",
            "We only try a model if we think it's likely that it's that it's a good fit, right?",
            "And this is now again a normalization constant that doesn't depend on anything that we are modeling like this only depends on the data, so this is a sort of a constant.",
            "So now the problem with this kind of inference.",
            "So this is very nice that you can just drive these rules from probability theory.",
            "The problems are these two integrals, right?",
            "So normally if you have a complicated model then the posterior distribution will be very complicated because it's proportional to the prior to the likelihood.",
            "So then you would have and if you have lots of parameters then you have a high dimensional nonlinear integral to do right and normally for many interesting models this is intractable.",
            "So that's the problem with Bayesian inference.",
            "But Luckily for Gaussian process you can actually just do this.",
            "Alright, this turns out just to be Gaussian integrals and you just write down the solution in closed form.",
            "So I'll show you that."
        ],
        [
            "And then I'll let you go too.",
            "Coffee.",
            "So now the clue here is to say, well.",
            "We're just going to do the same analysis as in the previous slide except.",
            "Now what in the in the terms of a Gaussian process?",
            "What are the parameters?",
            "We didn't have any parameters in the Gaussian process, right?",
            "In the Gaussian process, it turns out that the parameters is the function itself.",
            "OK, and this is the thing that's a little bit hard to think about, right, but let's just do the same analysis as we had in the previous slide, and every time we see parameters, we stick in the function OK and see what happens.",
            "OK, so now the first thing I need to do is to write down the likelihood function.",
            "OK now likelihood function is OK in this.",
            "In this slide I've used a notation where I don't write P of Y given X, but it's the same thing.",
            "I just skip the P. So the probability of Y given X and the function now is this used to say the parameter W, right?",
            "So I have to substitute in the function.",
            "It's a Gaussian distribution.",
            "Which is centered on the F values that correspond to the X is right, right?",
            "So if we look back on the previous slide, notice that the likelihood function it's a Gaussian where the which is centered on the predicted values.",
            "Here, right?",
            "So it's a sentence on the FS evaluated at the corresponding axes.",
            "So in my case, that will just be for the first case, it will be F1 and for the second case it will be F2.",
            "And remember I call these the collection of of F values.",
            "I call them BF X right?",
            "So that means that the likelihood is just going to be a Gaussian distribution with a mean of BF X and whatever noise variance we had.",
            "OK, so it means that the likelihood function.",
            "Doesn't depend on all the parameters, right?",
            "All the parameters would be the function everywhere.",
            "The likelihood actually only depends on the value of the function at the points.",
            "We're interested in.",
            "That makes sense, right?",
            "If we if we want to evaluate whether a function is a good fit to the data or not, and then we look at is it predicting the values well at those points, right?",
            "We're not look it doesn't.",
            "Actually, we don't care what the function does over here, right?",
            "We care about whether it actually predicts the observed target.",
            "OK, so now we have to so before.",
            "I didn't specify what the what the prior was, but here we could have used the Gaussian prior.",
            "In this case, we would now extend the Gaussian prior instead of being joint Gaussian distribution over those function values.",
            "It will now be a Gaussian distribution over all the function values, right?",
            "So this will be the Gaussian process.",
            "OK, so we say what we know about the function before the data arrives is that it comes from a Gaussian process.",
            "OK, so now we have to compute the posterior as before, so the posterior is the product of the likelihood and the prior and both of them have Gaussian shapes, although this one is a little bit funny, right?",
            "Is this infinite dimensional Gaussian?",
            "But still, if you multiply, you multiply 2 Gaussians, you get another Gaussian right?",
            "Although unnormalized, right?",
            "But the posterior here is a distribution over F. So we want the normalized version of that.",
            "So if you multiply together 2 Gaussians, you get another Gaussian.",
            "If you want to apply together 2 infinite dimensional Gaussians, you get an infinite dimensional Gaussian.",
            "Alright, so here we are multiplying together an infinite dimensional Gaussian with well with something which is, which is finite dimensional.",
            "You get an infinite dimensional Gaussian.",
            "So the posterior will now be a Gaussian process.",
            "OK, that makes sense right?",
            "The posterior is a distribution of functions.",
            "Is the distribution of functions that give high probability to the functions that agree with the data and agree with our prior assumptions.",
            "And so now the posterior process here is has a mean function and has a covariance function and the covariance function in the mean function in various function are just simple functions of the likelihood and prior.",
            "Right, so the so this thing here.",
            "So the mean here is a vector.",
            "It depends on the observation.",
            "It depends on the on the values of the X is here and it depends on the covariance function.",
            "So the K matrix here is now the matrix of covariances evaluated all pairs of training points.",
            "OK, and now we have to do the last integral we have to now.",
            "Anne.",
            "Compute the predictive distribution here using this integral.",
            "So in this case this integral is also going to be tractable.",
            "It's also going to be Gaussian, because this thing here was a Gaussian.",
            "It was an infant dimensional one, but it was a Gaussian and this thing here is just the likelihood function written down for the test case.",
            "So again, we have a product of two Gaussian terms, which is a Gaussian and an integral part of that is another Gaussian, so they predicted distribution.",
            "So what is now the predicted distribution?",
            "The distribution of of a test case given the training set is also going to be just a Gaussian distribution.",
            "It's going to have a mean and variance.",
            "OK, so now these are a little bit long.",
            "Equations will spend the next couple of slides actually."
        ],
        [
            "Taking them apart, so I'll show you before coffee.",
            "I'll just show you a pictorial version of this, right?",
            "So this thing here depicts a distribution the prior distribution, the distribution or functions.",
            "Before we see the data like so.",
            "And there are three different draws, so I've drawn three times from this Gaussian Gaussian process distribution.",
            "So in blue, I've actually done things properly.",
            "I've actually written down 50 values of X just equidistantly spaced out on this axis here, and then I've drawn from that high dimensional Gaussian those 50 values.",
            "And then I plotted those 50 values as a function of X is now in the.",
            "In red and green here, I've cheated a little bit.",
            "I've actually drawn these 50 samples and then just connected up the lines right so you can see the function.",
            "But technically I can only evaluate the function that this finite number of points.",
            "OK, so the function sort of looked like this and you can see there they somehow have a characteristic look to them right?",
            "They have the same sort of magnitude here in the same two they seem to fluctuate somehow in the same way, and they're all smooth right there?",
            "Definitely some properties of these functions.",
            "And then so this was a prior.",
            "Right now I make some observations and make here make 5 observations.",
            "So I observed that the function value here is this value here and I've said this value in this value.",
            "This value.",
            "And now I say OK. Now what is the posterior process?",
            "That's what we computed on the previous slide.",
            "I repeated it down here like the posterior process I've drawn again 3 sample functions from that posterior process and you can see that these functions always agree with the data, right?",
            "Because we're conditioning on the fact that they have to agree with the data, but there's still some places where we don't know what the function is doing.",
            "So for example, here it's a little bit uncertain about what the function is doing it out here.",
            "It still is as uncertain about what's going on as it was under the prior.",
            "Alright, so one way of thinking about this would be to say, well.",
            "One way of doing this kind of inference would be to say, well, I make my 5 observations OK, so I write them down.",
            "Now I draw random functions and I keep drawing random functions.",
            "Until I get one that agrees with all the five points.",
            "OK.",
            "So this might take awhile, right?",
            "But if you if you did it then you would get exactly this distribution right?",
            "So it just happens that you can compute what that distribution is using linear algebra, basically right?",
            "You just manipulate these Gaussian variables, but it's equivalent to saying I'm interested in this distribution of random function condition on the fact that they agree with my training data.",
            "And that gives you this distribution or functions.",
            "OK, let's stop for a break for.",
            "How long?",
            "Half past OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call we're lucky to have car Rasmussen College.",
                    "label": 0
                },
                {
                    "sent": "He's a PhD with Jeff Hinton and then after that I believe post opting the Gatsby.",
                    "label": 0
                },
                {
                    "sent": "To prove it spent some time in Denmark and then moved to Bingham where he led the Central European Bayesians for a number of years.",
                    "label": 0
                },
                {
                    "sent": "Night in the middle of darkness there.",
                    "label": 0
                },
                {
                    "sent": "Since then, he's being recruited to Cambridge, where he's now a lecturer.",
                    "label": 0
                },
                {
                    "sent": "His research career as a. I think since the start when I first came across works is 1996, sixty pieces would really like the most.",
                    "label": 0
                },
                {
                    "sent": "The launch Gaussian process is on the machine learning community in it was very much written well in the context of work he done with Chris Williams, previous speaker and the pair of them sort of letters.",
                    "label": 0
                },
                {
                    "sent": "Small community of.",
                    "label": 0
                },
                {
                    "sent": "Intrepid Gaussian process operators over the next eight years until they finally gave away all the secrets of the field by writing an excellent book on it's called Gaussian Process Machine Learning, which I highly recommend you purchase, so he's an ideal choice to give us a talk on water, because partially due to their book and partially due to their service over the years, become one of the most active areas of research in machine learning today.",
                    "label": 0
                },
                {
                    "sent": "And that's counting process is so over to can't.",
                    "label": 0
                },
                {
                    "sent": "Thanks, Neil.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so today I'm going to try and tell you about Gaussian processes and I have this.",
                    "label": 0
                },
                {
                    "sent": "I have this idea that Gaussian process is sort of the simplest way that you can.",
                    "label": 0
                },
                {
                    "sent": "You can solve regression problems or learning about a function.",
                    "label": 0
                },
                {
                    "sent": "So one of the one of the very basic things that you can try and think about in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So you might have to ask you again in 2 hours time whether you agree with me.",
                    "label": 0
                },
                {
                    "sent": "This is simplest way you could possibly do this right?",
                    "label": 0
                },
                {
                    "sent": "So if you don't say yes then I've failed.",
                    "label": 0
                },
                {
                    "sent": "So I started thinking about these things about 15 years ago in Toronto, when I would when I was doing my PhD and there was a guy in what was known as the neuron lab that was machine learning was called neural networks back then called Radford Neal and he was he goes into doing Bayesian inference on very large neural networks and had these complicated Monte Carlo based methods for doing inference and he had a line in his thesis.",
                    "label": 0
                },
                {
                    "sent": "It said somewhere you know, actually in the limit of the neural network growing to being infinitely large.",
                    "label": 0
                },
                {
                    "sent": "This actually converges to something called a Gaussian process, and it might be easier doing inference if you look at it that way.",
                    "label": 0
                },
                {
                    "sent": "So actually that got me thinking about what does that actually mean?",
                    "label": 0
                },
                {
                    "sent": "What does this Gaussian crossword Gaussian process mean?",
                    "label": 0
                },
                {
                    "sent": "And Chris Williams, who just spoke before me and myself.",
                    "label": 0
                },
                {
                    "sent": "We were sort of talking about this and trying to figure out what did that mean.",
                    "label": 0
                },
                {
                    "sent": "Could you actually, how could you do inference there?",
                    "label": 0
                },
                {
                    "sent": "And I remember we were talking about this for a few months and after a little while, Chris sent me a draft paper where he'd written down some equations.",
                    "label": 0
                },
                {
                    "sent": "I found that all very confusing, but I could type them into Matlab, right?",
                    "label": 0
                },
                {
                    "sent": "So I did that.",
                    "label": 0
                },
                {
                    "sent": "So I typed in equations and then I ran it on my data set and work really well.",
                    "label": 0
                },
                {
                    "sent": "Actually work better than anything else I had.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is the opposite situation that you're normally in in research, right?",
                    "label": 0
                },
                {
                    "sent": "Normally you have these great ideas that just happened not to work right, and this was the opposite, right?",
                    "label": 0
                },
                {
                    "sent": "You had no idea what was really going on, but it really worked.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to persuade you that it's possible to give you some of the abstract notion of what is the Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "But actually I find that that's not very helpful, so I'm going to try to do it sort of backwards to try and try and pull things apart, and really to look under the hood and sort of think about the basic problem of what are we doing when we're trying to do inference about a function where we're learning about a function.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's going to end up being based on these stochastic processes, and if you don't know anything about sarcastic posts, is that OK?",
                    "label": 0
                },
                {
                    "sent": "Value the things that you need to know about it right?",
                    "label": 0
                },
                {
                    "sent": "And I'm not very good at mathematics.",
                    "label": 0
                },
                {
                    "sent": "And actually you don't need to be very good enough.",
                    "label": 0
                },
                {
                    "sent": "Value.",
                    "label": 0
                },
                {
                    "sent": "Some very pedestrian notions to try and get at things that you can also find in very thick books.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try and set out here some of the some of the problems which I'll try and persuade you that Gaussian process actually the answer too, right?",
                    "label": 1
                },
                {
                    "sent": "So if you're trying to learn about, let's say regression, you're trying to learn about function, then you want to answer a bunch of questions more or less always right?",
                    "label": 1
                },
                {
                    "sent": "So you want to want to know well, how do I access how to actually fit this model to the data?",
                    "label": 1
                },
                {
                    "sent": "How do model selection if I have two different types of models?",
                    "label": 1
                },
                {
                    "sent": "How do I know which one to use over?",
                    "label": 0
                },
                {
                    "sent": "Another one, and how do I interpret what's going on can actually understand what the model is doing.",
                    "label": 0
                },
                {
                    "sent": "And things about you know, to what extent can I can I trust the predictions?",
                    "label": 1
                },
                {
                    "sent": "Things like that, and so I'll try to persuade you that Gaussian processes can help you solve all these problems insofar as it is solution, right?",
                    "label": 0
                },
                {
                    "sent": "Some of these problems there are sort of good reasons why there isn't a clear cut solution to the problem, right?",
                    "label": 0
                },
                {
                    "sent": "And the Gaussian process will also help understand why that's the case and why there are some sort of an inherent problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An OK, so we'll have a break in the middle, so hopefully I'll cover most of these sort of foundations part and I'd like you to ask questions when we start.",
                    "label": 0
                },
                {
                    "sent": "When we start going right, there will be some notions that are maybe a little bit mysterious, like if you don't break me, then I won't notice that you're that you're not with me anymore, right?",
                    "label": 0
                },
                {
                    "sent": "So to ask questions, if we only manage the first part, that's fine, right?",
                    "label": 0
                },
                {
                    "sent": "This second part is sort of more, more special ways of using this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's some colorful pictures in there, but apart from that.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's get started.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process are based on Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "So I'll just refresh for you what a Gaussian distribution is.",
                    "label": 1
                },
                {
                    "sent": "I'm going to say this.",
                    "label": 0
                },
                {
                    "sent": "Station here, so I'm going to say P. Of X, so X is a is a multivariate thing here P of X given mu and Sigma.",
                    "label": 1
                },
                {
                    "sent": "So MU is the mean and Sigma is the variance or covariance.",
                    "label": 0
                },
                {
                    "sent": "In the multivariate case.",
                    "label": 0
                },
                {
                    "sent": "So here I have two Gaussian distributions with different means.",
                    "label": 0
                },
                {
                    "sent": "So the mean is for the for the black distributions.",
                    "label": 0
                },
                {
                    "sent": "Here the mean for blue distributions over here and I've marked also the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit a bit wider distribution because the distribution normalized to one.",
                    "label": 0
                },
                {
                    "sent": "It's also going to have a smaller magnitude than this one over here.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 dimensional Gaussian distribution which is easy to draw, so I can also more or less draw 2 dimensional distribution.",
                    "label": 0
                },
                {
                    "sent": "But it's very hard to draw a higher dimensional distributions and it turns out that Gaussian processes are just very high dimensional Gaussian distributions like, so we have to imagine a high dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So here's a 2 dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So instead of instead of sort of showing you the the the actual value of the probability density.",
                    "label": 0
                },
                {
                    "sent": "I've just shown you here the mean.",
                    "label": 1
                },
                {
                    "sent": "I've shown you the two... here that correspond to the one standard deviation and the two stand deviations.",
                    "label": 0
                },
                {
                    "sent": "And the and the variance here is encoded in the covariance matrix of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "In this case, has two eigenvectors eigenvectors, one that goes in this direction and one that goes in this direction and the eigenvalue going in this direction.",
                    "label": 0
                },
                {
                    "sent": "So the variance in that direction corresponds to the eigenvalue of the covariance.",
                    "label": 0
                },
                {
                    "sent": "And here I've written out the actual distribution and it's gory detail.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the two properties of these Gaussians that are going to be of central importance, so one is called the conditional distribution and one is called the marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "So conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So here I've got my 2 dimensional Gaussian from before and the conditional distribution of 1 variable given the other variable.",
                    "label": 0
                },
                {
                    "sent": "I've tried to illustrate that here, so let's say so this is the joint distribution over the two dimensional space here and now.",
                    "label": 0
                },
                {
                    "sent": "Let's say that I observe that the value of this variable.",
                    "label": 0
                },
                {
                    "sent": "And this variable has this.",
                    "label": 0
                },
                {
                    "sent": "At this variable, sorry, had this particular value OK, and now I say OK.",
                    "label": 0
                },
                {
                    "sent": "If this variable has this particular value, then what does that imply about the other variable?",
                    "label": 0
                },
                {
                    "sent": "Alright, So what I'm doing here is sort of I'm slicing through the distribution and seeing what's leftover as a function of the other variable, right?",
                    "label": 0
                },
                {
                    "sent": "And you can see over here if I know that this is the value of this variable, then the probability that the this variable takes on some negative value of some value over here is very, very small, right?",
                    "label": 0
                },
                {
                    "sent": "Because there isn't any mass down here, so the masses is over here, so the conditional distribution here looks something like this and.",
                    "label": 0
                },
                {
                    "sent": "And there's an interesting fact about Gaussians is that if you condition a Gaussian, then you get another Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "So the conditional distribution here is also Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And the last thing we need to know about is marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "So marginal distributions is just that you like to some out or integrate out one of the variables and look at what is now the marginal distribution for the other variable.",
                    "label": 0
                },
                {
                    "sent": "So I'm just collect collapsing or projecting down this joint distribution onto one particular axis.",
                    "label": 0
                },
                {
                    "sent": "And again the marginal distribution of when you marginalized the Gaussian, you get another Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now what is a Gaussian process?",
                    "label": 1
                },
                {
                    "sent": "Well, I got some process is just generalization of a multivariate Gaussian to infinitely many variables.",
                    "label": 1
                },
                {
                    "sent": "So just like so it's sort of a natural extension.",
                    "label": 0
                },
                {
                    "sent": "When you go from a from a univariate Gaussian to a multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And now we just go to a larger multivariate Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "So a Gaussian that contains infinitely very very many variables.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "One can ask, well, why are we interested in infinitely many variables?",
                    "label": 0
                },
                {
                    "sent": "That sounds that sounds to be a sort of odd thing to be interested in, but the reason for this is that we can think of functions and functions are sort of natural things to try and sort of infer properties of.",
                    "label": 0
                },
                {
                    "sent": "You can think of a function as just doing an infinitely long vector.",
                    "label": 0
                },
                {
                    "sent": "Right, so because you can simply specify what is the value of the function for any value of the argument X. OK, so this would be an infinitely long vector, so this is.",
                    "label": 0
                },
                {
                    "sent": "It seems like a very primitive notion, but it conveys exactly what we need.",
                    "label": 0
                },
                {
                    "sent": "Right, So what I'm getting at here is that a Gaussian process can be used to specify a distribution over functions, right?",
                    "label": 0
                },
                {
                    "sent": "And if we want to make probabilistic inference about functions, then we better have a way to write down distributions over these objects, right?",
                    "label": 0
                },
                {
                    "sent": "Because if you can't do that, then we can't apply probability theory.",
                    "label": 0
                },
                {
                    "sent": "OK, and the Gaussian process is exactly the object that we use to handle distribution or function to specify them and to manipulate them.",
                    "label": 1
                },
                {
                    "sent": "OK, so the formal definition here is a Gaussian processes collection of random variables.",
                    "label": 1
                },
                {
                    "sent": "Any finite number of which have Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "So mathematically there are number of technicalities involved in having infinite collections of things, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're strict mathematician, then you might wonder, is this this really well defined?",
                    "label": 0
                },
                {
                    "sent": "I'm not a strict mathematician, I don't care about these things.",
                    "label": 0
                },
                {
                    "sent": "But that's why.",
                    "label": 0
                },
                {
                    "sent": "So that's the mathematicians definition here, right?",
                    "label": 0
                },
                {
                    "sent": "So they try to avoid saying anything about infinities, right?",
                    "label": 0
                },
                {
                    "sent": "So they just say, well, it's an infinite collection, but only that any finite number of which have consistent Gaussian distributions, right?",
                    "label": 0
                },
                {
                    "sent": "So they haven't said anything about things that might be.",
                    "label": 0
                },
                {
                    "sent": "Unpleasant.",
                    "label": 0
                },
                {
                    "sent": "Alright, but we don't.",
                    "label": 1
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "Actually, we don't need to worry about that thing.",
                    "label": 0
                },
                {
                    "sent": "Nothing will nothing bad will happen here.",
                    "label": 0
                },
                {
                    "sent": "OK, so just like a Gaussian distribution here is now specified fully specified by a mean vector and covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "A Gaussian process is just specified in terms of the infinitely long versions of that right.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a mean vector, I now have the Gaussian process here hasn't had an infinitely long mean vector right?",
                    "label": 0
                },
                {
                    "sent": "And infinitely long vectors are functions, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so the Gaussian process has a mean function, and similarly it doesn't have a covariance matrix, but it has a covariance matrix here, which is an infinite by infinite dimensional matrix.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's a function with two arguments.",
                    "label": 0
                },
                {
                    "sent": "OK, so so formally at least we can write down something like this.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "At this point, you might be a little bit worried, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can't actually write down an infinitely long vector anywhere, right?",
                    "label": 0
                },
                {
                    "sent": "So if this machinery that I'm going to develop will require you to write that down in your computer, for example, then you know already that you know you can't do it exactly, at least, and it might.",
                    "label": 0
                },
                {
                    "sent": "Things might get very hairy, but it turns out that things don't get hairy, right?",
                    "label": 0
                },
                {
                    "sent": "And the reason why things will turn out OK is what's known as the what I call the marginalization property.",
                    "label": 0
                },
                {
                    "sent": "Alright, so basically it has to do with the fact that marginal distribution of Gaussian of a joint Gaussian is again a Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "So if we have something which is jointly jointly has a Gaussian distribution, and if we now marginalized out the distribution of Y then we get back the distribution of X and if this joint distribution was Gaussian, then the distribution of X will also be Gaussian, right?",
                    "label": 0
                },
                {
                    "sent": "And both X&Y could be vectors in this case.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "So let's pretend that we now have a full Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "So we have we have an infinite and infinitely large long vector here, right?",
                    "label": 0
                },
                {
                    "sent": "So now the joint distribution over, let's say we had.",
                    "label": 0
                },
                {
                    "sent": "Let's say we had a number of points that we were actually interested in, and then we had all the other points.",
                    "label": 0
                },
                {
                    "sent": "Why over here?",
                    "label": 0
                },
                {
                    "sent": "Right now, if you write down the joint distribution of this, that will have a mean where the mean of the X vector will be a in the mean of the Y vector will be, and the covariances between the access will be A and they cross covariances will begin by B, right?",
                    "label": 0
                },
                {
                    "sent": "So where this thing here?",
                    "label": 0
                },
                {
                    "sent": "If this is a Gaussian process and this thing here will be will be an infinitely large.",
                    "label": 0
                },
                {
                    "sent": "Matrix C matrix.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing here is that the marginal distribution for X is just going to be a Gaussian distribution with mean A.",
                    "label": 1
                },
                {
                    "sent": "An covariance matrix capital A, right?",
                    "label": 0
                },
                {
                    "sent": "So it means if you ask finite dimensional questions about this infinite dimensional objects, then you get you get simply get a finite dimensional answer right?",
                    "label": 0
                },
                {
                    "sent": "And the finite dimensional answer doesn't depend on all these things that were going on all the other coordinates of this very very long vector.",
                    "label": 0
                },
                {
                    "sent": "Right, so this sort of motivates that maybe there's hope that you can do inference using these objects without writing down everything.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I told you that.",
                    "label": 0
                },
                {
                    "sent": "The way I'm going to use Gaussian process is to specify distributions of our functions, right?",
                    "label": 0
                },
                {
                    "sent": "So now let me try to persuade you that that Gaussian process really is a distribution or functions, and the way I'm going to do that is I'm going to draw some random samples from that distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's write down some Gaussian process, so we have a Gaussian process here, which will have a mean of zero.",
                    "label": 0
                },
                {
                    "sent": "And actually I think for the rest of the talk today, my Gaussian process will always have means of 0.",
                    "label": 0
                },
                {
                    "sent": "This seems like this seems like a big restriction, but it actually turns out that it's not terribly important, so I always have zero mean Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "I'm more interested in what happens in the covariance function here, so here's an example of a covariance function.",
                    "label": 0
                },
                {
                    "sent": "Equivalent function is E to the minus distance between.",
                    "label": 0
                },
                {
                    "sent": "X&X prime squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is also sort of has a Gaussian look to it right?",
                    "label": 0
                },
                {
                    "sent": "But this is not a Gaussian process because this looks Gaussian, it's already a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "If this was some other function, then it could also still be a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Now it's going to be a Gaussian process of the inputs, right?",
                    "label": 1
                },
                {
                    "sent": "So the index set to the Gaussian process are going to be my input.",
                    "label": 0
                },
                {
                    "sent": "So if I'm going to use this to model a function and the function Maps inputs to outputs like, then the X is are going to be the the index set to the random variables, right?",
                    "label": 0
                },
                {
                    "sent": "So this is important that a lot of people have maybe heard about stochastic process in the time domain.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to talk about time here.",
                    "label": 0
                },
                {
                    "sent": "There is no time.",
                    "label": 0
                },
                {
                    "sent": "So the index set, which is usually given by time in this case, is given by the inputs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now if I have specified now this specifies a Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "I specify the mean function and the covariance function.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you a lot more about covariance functions later on.",
                    "label": 0
                },
                {
                    "sent": "For now, let's just let's just choose this one.",
                    "label": 0
                },
                {
                    "sent": "So once I've done that.",
                    "label": 0
                },
                {
                    "sent": "I can now say, well.",
                    "label": 0
                },
                {
                    "sent": "OK, how do I look at a function?",
                    "label": 0
                },
                {
                    "sent": "Well I look at a function by plotting the function value at some points.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just do that.",
                    "label": 0
                },
                {
                    "sent": "So I invent a bunch of X points.",
                    "label": 0
                },
                {
                    "sent": "Let's say the numbers between 1:02 hundred or something like that.",
                    "label": 0
                },
                {
                    "sent": "I just choose some X is.",
                    "label": 0
                },
                {
                    "sent": "So once I've chosen the X is.",
                    "label": 0
                },
                {
                    "sent": "I can then worry about well what are the corresponding values of the function evaluated at those points, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm interested in F at X1, the function values X1 and FX-2 and so on, right lump all those together in a vector which I call BF XF here, right?",
                    "label": 0
                },
                {
                    "sent": "And now use the marginalization property.",
                    "label": 0
                },
                {
                    "sent": "Now I say OK if the function was drawn from this Gaussian process then I had this big infinite dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "But I'm only interested in a subset of the values right?",
                    "label": 0
                },
                {
                    "sent": "Namely these ones right?",
                    "label": 0
                },
                {
                    "sent": "And the distribution of these ones will just be just as we had before, the marginal distribution of the X variable was just Gaussian with mean a, an covariance matrix capital A, so that will just be the mean vector will be.",
                    "label": 0
                },
                {
                    "sent": "Zero in this case, in the covariance matrix will just be the covariance function evaluated at all pairs of pairs of X is right, so the top diagonal element will be K evaluated at X1 and X1.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this defines now a Gaussian distribution, so this is no longer a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "But this is now a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So saying that the Gaussian distribution implies that the joint distribution of F follow a Gaussian and follows the Gaussian that has this particular covariance matrix, OK?",
                    "label": 0
                },
                {
                    "sent": "So let me try to actually do this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here I picked a bunch of of of X is here.",
                    "label": 0
                },
                {
                    "sent": "I think I picked maybe 20 or something like that.",
                    "label": 0
                },
                {
                    "sent": "I just picked them randomly in this interval.",
                    "label": 0
                },
                {
                    "sent": "And then I wrote down this joint Gaussian distribution, which would now be a 20 by 20, have a 20 by 20 covariance matrix, and then I draw a random sample from that distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So a random sample from a 20 dimensional Gaussian is a vector with 20 dimensions.",
                    "label": 0
                },
                {
                    "sent": "And then I just plot the Y values of corresponding to those X values.",
                    "label": 0
                },
                {
                    "sent": "And you can see what happens.",
                    "label": 0
                },
                {
                    "sent": "What happens here is that you can sort of see that there might be an underlying underlying function here, right?",
                    "label": 0
                },
                {
                    "sent": "These things are not independent of each other, definitely right?",
                    "label": 0
                },
                {
                    "sent": "And actually, and so we can sort of wonder, you know what are the properties of this function?",
                    "label": 0
                },
                {
                    "sent": "Why do the samples actually look like this right?",
                    "label": 0
                },
                {
                    "sent": "And if we go back?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the previous slide, here we can say, well, the properties of functions actually depend on the properties of the Korean function and the covariance function.",
                    "label": 0
                },
                {
                    "sent": "Here said that well, if X&X prime.",
                    "label": 0
                },
                {
                    "sent": "If the two axes are very close to each other, then the covariance will be E to the minus, something close to 0 is 1, right?",
                    "label": 0
                },
                {
                    "sent": "So then they will have covariance if the X is are very far apart, then you will have eaten the minus some large positive number, which is almost zero right?",
                    "label": 0
                },
                {
                    "sent": "So say that the targets corresponding to the.",
                    "label": 0
                },
                {
                    "sent": "To these inputs will then not covary very much right, and that's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly what we sort of see happening in the picture that what the function does over here doesn't really seem to.",
                    "label": 0
                },
                {
                    "sent": "Is not doesn't seem to be particularly influenced about what the what the function does over here, right?",
                    "label": 0
                },
                {
                    "sent": "But what the function does at this point?",
                    "label": 0
                },
                {
                    "sent": "At this point is somehow heavily correlated.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this this this already seems maybe a little bit mysterious, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can sort of say, well, where did that function come from?",
                    "label": 0
                },
                {
                    "sent": "Right, it seems a little odd that I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't write down.",
                    "label": 0
                },
                {
                    "sent": "A parametric function for the for this thing like this function, just sort of sort of came from.",
                    "label": 0
                },
                {
                    "sent": "Yeah, where did they come from?",
                    "label": 0
                },
                {
                    "sent": "So I have a little illustration here where you can sort of see.",
                    "label": 0
                },
                {
                    "sent": "Sort of where where the function is in this in this system, and so one way of thinking about this high dimensional Gaussian is you can try to fax about factorize it, right?",
                    "label": 0
                },
                {
                    "sent": "So you can have the high dimensional Gaussian, is it?",
                    "label": 0
                },
                {
                    "sent": "It's a joint distribution of all the all the F values that we were interested in conditioned on all the input variables.",
                    "label": 0
                },
                {
                    "sent": "An and you can always factorize a distribution in this form here.",
                    "label": 0
                },
                {
                    "sent": "So you can say well, the distribution here is the probability of of the one variable given the corresponding axis.",
                    "label": 0
                },
                {
                    "sent": "Always course probability of the first variable times the probability of the second variable given the previous one.",
                    "label": 0
                },
                {
                    "sent": "Times the probability of the third variable given the two previous ones.",
                    "label": 0
                },
                {
                    "sent": "OK, you can always factorize a distribution like that.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about thinking about a joint distribution as a product of factors of this form is that all of these factors are just one dimensional right?",
                    "label": 0
                },
                {
                    "sent": "So they are easier to think about.",
                    "label": 0
                },
                {
                    "sent": "It is very hard to make a picture or think about high dimensional Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But now we can.",
                    "label": 0
                },
                {
                    "sent": "We can we can make factorize this joint distribution in terms of these conditionals that are each owner distribution for a single variable.",
                    "label": 0
                },
                {
                    "sent": "So that means that now now we can use this to generate the samples right before I just draw, I just draw a random sample from this high dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Now let's try to do this using this formula.",
                    "label": 0
                },
                {
                    "sent": "So now we can draw them sequentially.",
                    "label": 0
                },
                {
                    "sent": "Can first draw the first Gaussian for the first random variable, and then I can draw the second one given the first one, and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we can see how things develop.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, then you will need the rule for how you draw.",
                    "label": 0
                },
                {
                    "sent": "What are conditional distributions for Gaussians, right?",
                    "label": 1
                },
                {
                    "sent": "So if you have a joint Gaussian here, I've just written this down for reference.",
                    "label": 0
                },
                {
                    "sent": "So there's a particular formula for how to find the Gaussian, which is the conditional distribution of 1 variable given another one.",
                    "label": 0
                },
                {
                    "sent": "OK, so just written down this for for reference.",
                    "label": 0
                },
                {
                    "sent": "So in the coffee break or instead of the coffee break you might try to do this on your laptop or you can do it maybe at home tonight.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to skip dinner or something.",
                    "label": 0
                },
                {
                    "sent": "OK so I give you the formula here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Alright, so now let me try to walk you through how this works.",
                    "label": 0
                },
                {
                    "sent": "So in this case I have the input values here and how the output values here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm just going to draw these samples from the function, so now.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I do is I pick an X value, right?",
                    "label": 0
                },
                {
                    "sent": "I say I'm interested in what the function does right here, so I've picked X value and I pick this value here.",
                    "label": 0
                },
                {
                    "sent": "Now when I pick this value I need now to draw a random sample from the conditional distribution of the value corresponding to that X value.",
                    "label": 0
                },
                {
                    "sent": "Given the other ones, but there aren't any at the moment, right?",
                    "label": 0
                },
                {
                    "sent": "So so it's just drawn from that distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I draw the Gaussian distribution is illustrated by the width of the Gaussian is illustrated by this by the grey zone here.",
                    "label": 0
                },
                {
                    "sent": "Become a little bit clearer as we go along, and the mean function here initially is just zero, just as we had before.",
                    "label": 0
                },
                {
                    "sent": "OK, so I picked something.",
                    "label": 0
                },
                {
                    "sent": "Actually this is a random sample.",
                    "label": 0
                },
                {
                    "sent": "It looks very close to zero, it's just a random sample from that.",
                    "label": 0
                },
                {
                    "sent": "OK, and haven't fixed the seed, so I don't know what's going to happen now.",
                    "label": 0
                },
                {
                    "sent": "OK so I picked this thing OK so now the grey zone says well.",
                    "label": 0
                },
                {
                    "sent": "If you pick random samples that are close to the sample like, then it has to covary with this value, right?",
                    "label": 0
                },
                {
                    "sent": "So the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Is now changed from before, right?",
                    "label": 0
                },
                {
                    "sent": "Because you're conditioning on the observation that you already made OK, Now I pick another X at random.",
                    "label": 0
                },
                {
                    "sent": "OK, picked a value over here.",
                    "label": 0
                },
                {
                    "sent": "So now the distribution over this for this variable happens to be not very influenced by this value, right?",
                    "label": 0
                },
                {
                    "sent": "So the conditional distribution of this one, given this one would be the same as just the distribution of this variable.",
                    "label": 0
                },
                {
                    "sent": "OK so I picked a random sample, so here it was a little bit below the mean.",
                    "label": 0
                },
                {
                    "sent": "I do the same thing, so now the the distribution for the next variable given these two variables look like this, right?",
                    "label": 1
                },
                {
                    "sent": "So if I'm close to this point then the function will have to agree with this value over here, but out here for example, things are not very influenced by what we've already observed.",
                    "label": 0
                },
                {
                    "sent": "So if I just keep going doing this.",
                    "label": 0
                },
                {
                    "sent": "You can see so now at this point I'm now picking a random variable which is heavily influenced by this point and also to some extent influenced by this point.",
                    "label": 0
                },
                {
                    "sent": "So you can sort of see that the underlying function is sort of.",
                    "label": 0
                },
                {
                    "sent": "It's characterized by the other data points, right?",
                    "label": 0
                },
                {
                    "sent": "So it's the data points themselves which are characterizing the function like I don't have any explicit formula for the function.",
                    "label": 0
                },
                {
                    "sent": "Inside my machine, right?",
                    "label": 0
                },
                {
                    "sent": "OK, this question.",
                    "label": 0
                },
                {
                    "sent": "What what's on the X axis here is just the input to the function.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there's going to be some.",
                    "label": 0
                },
                {
                    "sent": "There's some functional relationship between the output and the input of this function, right?",
                    "label": 0
                },
                {
                    "sent": "And currently I don't know what that function is right?",
                    "label": 0
                },
                {
                    "sent": "But having made some observations about the function has told me something about that function, so it hasn't told me anything about what the function is doing in this area, but it has told me what's doing what is doing around this area and around this area.",
                    "label": 0
                },
                {
                    "sent": "So if I continue doing this.",
                    "label": 0
                },
                {
                    "sent": "Then after.",
                    "label": 0
                },
                {
                    "sent": "Seeing more and more observations.",
                    "label": 0
                },
                {
                    "sent": "You will find that things will collapse towards.",
                    "label": 0
                },
                {
                    "sent": "And you can see it now.",
                    "label": 0
                },
                {
                    "sent": "You can sort of visualize the underlying function right?",
                    "label": 0
                },
                {
                    "sent": "And if I sample more and more samples here then they will lie exactly on that function right?",
                    "label": 0
                },
                {
                    "sent": "Because the function is already been pinned down by these things over here over here, it hasn't been pinned down exactly yet.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "That's right, so it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's the mean.",
                    "label": 0
                },
                {
                    "sent": "This greystone is mean plus and minus two times the standard deviation of the of the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Get a point.",
                    "label": 0
                },
                {
                    "sent": "Different shapes like Johnson.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is, you know why was it that that you had these funny shapes happening as we went along with the sampling right?",
                    "label": 0
                },
                {
                    "sent": "And so I only compute the conditional distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So I only use that formula on the previous slide, right?",
                    "label": 0
                },
                {
                    "sent": "And it just happens that that's the way.",
                    "label": 0
                },
                {
                    "sent": "So I just plug into this to this formula here right?",
                    "label": 0
                },
                {
                    "sent": "So for for every one of these I just look at, well, what's the probability at a new F given any possible point on the on the on the X axis, right?",
                    "label": 0
                },
                {
                    "sent": "So I haven't sort of injected anything into the system other than the covariance function.",
                    "label": 0
                },
                {
                    "sent": "I specify the covariance function, but everything followed from that.",
                    "label": 0
                },
                {
                    "sent": "So it's actually really is actually really interesting to try and do this on your computer, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's a it's a, it's not a very usual way to think about functions, right?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a picture of doing this in a 2 dimensional case.",
                    "label": 0
                },
                {
                    "sent": "So here I have an input which is which is of two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And I have a, so I've drawn.",
                    "label": 0
                },
                {
                    "sent": "Now my ex is our.",
                    "label": 0
                },
                {
                    "sent": "So there's 100 points along this axis and 100 points along this axis, so my input space contains 10,000 points.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "That means met my my covariance function.",
                    "label": 0
                },
                {
                    "sent": "So I have an entry in the covariance function for each pair of points, right?",
                    "label": 0
                },
                {
                    "sent": "So that means my covariance matrix is a 10,000 by 10,000 matrix.",
                    "label": 0
                },
                {
                    "sent": "Like and I draw a point from that.",
                    "label": 0
                },
                {
                    "sent": "10,000 dimensional Gaussian is at 10,000 dimensional vector, right?",
                    "label": 0
                },
                {
                    "sent": "And I can plot the values of that vector as a function of those corresponding X values, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the function that I draw this random function from that distribution, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so the question is, you know if I'm if I'm making inference about functions that I don't know, then how can I know the covariances?",
                    "label": 0
                },
                {
                    "sent": "So until now I've only been trying to characterize the notion of a Gaussian process, right?",
                    "label": 1
                },
                {
                    "sent": "So we haven't.",
                    "label": 0
                },
                {
                    "sent": "So this is all about random functions, right?",
                    "label": 0
                },
                {
                    "sent": "So I haven't actually told you about you know how would you actually.",
                    "label": 0
                },
                {
                    "sent": "Usually you're not interested in random functions, right?",
                    "label": 0
                },
                {
                    "sent": "You're interested in functions that are useful for predicting whatever it is you're interested in, right?",
                    "label": 0
                },
                {
                    "sent": "So we haven't gotten that far yet, so but the distribution or functions specify that there's a certain covariance and and the covariance depends in a certain way upon the upon the inputs.",
                    "label": 0
                },
                {
                    "sent": "And this somehow implies something about the functions, right?",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you more about that.",
                    "label": 0
                },
                {
                    "sent": "Exactly how that works, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so the Gaussian process defines the distribution of functions right?",
                    "label": 0
                },
                {
                    "sent": "And this is 1 function drawn from that distribution or functions.",
                    "label": 0
                },
                {
                    "sent": "Right and I can only show you.",
                    "label": 0
                },
                {
                    "sent": "A finite number of function values, right?",
                    "label": 0
                },
                {
                    "sent": "So it's only 10,000 values here.",
                    "label": 0
                },
                {
                    "sent": "Well, in the limit the picture would look the same, right?",
                    "label": 0
                },
                {
                    "sent": "Because all the points that would be in between would also be in between, because this particular Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "With this, with this covariance function.",
                    "label": 0
                },
                {
                    "sent": "Actually has the property that generates smooth functions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if I drew this on an even finer grid then it will look the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the function is a function of X1 and X2 and this is a function value F over here and this is just to illustrate that in my simple example I had a function which was from 1 dimensional X2A1 dimensional F of X but that need not be the case right?",
                    "label": 0
                },
                {
                    "sent": "You can also have the X is can live in any space.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "What happens when you specify the general way how different data points must?",
                    "label": 0
                },
                {
                    "sent": "And then you choose.",
                    "label": 0
                },
                {
                    "sent": "That's right, so the covariance structure.",
                    "label": 0
                },
                {
                    "sent": "The covariance structure tells you something about how the what the dependencies between cases, right?",
                    "label": 0
                },
                {
                    "sent": "So if I say well conditioned on that, I have this point that tells me something about what's going on elsewhere, right?",
                    "label": 0
                },
                {
                    "sent": "And that's enforced exactly what we're trying to utilize when trying to learn about function, were saying, oh, making this observation actually tells me something about the function, not just at that point, but also at nearby points in some sense.",
                    "label": 0
                },
                {
                    "sent": "Right and we have to get closer to exactly what is that sense and try to characterize that yeah?",
                    "label": 0
                },
                {
                    "sent": "Choose your ex is in there.",
                    "label": 0
                },
                {
                    "sent": "No, so that the ordering of the ex is completely irrelevant.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is a random sample from a Bell shaped curve, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a Bell shaped curve in one dimension, then if you draw a random sample from that then it's a number.",
                    "label": 0
                },
                {
                    "sent": "OK, now I have a high dimensional one so when I draw a sample from that distribution.",
                    "label": 0
                },
                {
                    "sent": "I can't show you a picture of the high dimensional Gaussian, but if you draw a sample from it, it's a vector right?",
                    "label": 0
                },
                {
                    "sent": "And I'm showing you a picture of that vector.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "As somebody have already mentioned, you know this was only about random functions, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we actually?",
                    "label": 0
                },
                {
                    "sent": "How do we use this to learn something about our functions?",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you how to do that in in a three step procedure.",
                    "label": 0
                },
                {
                    "sent": "1st I'm going to talk about how to do maximum likelihood and what I call a parametric method parametric model.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to show you how to do Bayesian inference in a parametric model, and then I'm going to show you how to do parameter based on inference in what in this thing which is known as a nonparametric model.",
                    "label": 0
                },
                {
                    "sent": "The reason why is known as a nonparametric model as opposed to a parametric model is in the parametric model we usually specify.",
                    "label": 0
                },
                {
                    "sent": "A functional form of the thing we're trying to fit, and it might have a bunch of free parameters like and now the inference task is to find out what are the most likely values or what are likely values or typical values, or some notion of typicality.",
                    "label": 0
                },
                {
                    "sent": "What are good values for those parameters?",
                    "label": 0
                },
                {
                    "sent": "But that's not what's going on in our model, like we haven't explicitly written down a functional form, right?",
                    "label": 0
                },
                {
                    "sent": "We're just treating it in this nonparametric way.",
                    "label": 0
                },
                {
                    "sent": "And actually.",
                    "label": 0
                },
                {
                    "sent": "It turns out that when you are used to thinking about this, the whole process of doing inference is much simpler, because when you're doing when you're using a parametric model.",
                    "label": 0
                },
                {
                    "sent": "Then the inference procedure.",
                    "label": 0
                },
                {
                    "sent": "The inference step is quite complicated, right?",
                    "label": 0
                },
                {
                    "sent": "You have first of all, you have some notion about the functions that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Then you formalize that in terms of a parametric model.",
                    "label": 1
                },
                {
                    "sent": "Then you have to say then you have to have your assumptions about the parameters in the parametric model.",
                    "label": 0
                },
                {
                    "sent": "Then you do inference about the parameters and that implies something about what's going on in the function.",
                    "label": 0
                },
                {
                    "sent": "That was quite a lot of steps, right?",
                    "label": 0
                },
                {
                    "sent": "What we're going to do here is just say what we have some notion about the weather functions are.",
                    "label": 0
                },
                {
                    "sent": "Then we condition on the data and then we're done.",
                    "label": 0
                },
                {
                    "sent": "Like there's no moving between.",
                    "label": 0
                },
                {
                    "sent": "As a parametric family of functions and moving back to the function space, we're just doing everything directly in the class of functions.",
                    "label": 0
                },
                {
                    "sent": "But since that's a little bit unusual to do things that way, I'm going to tell you how to do this inference in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "OK, so supervised learning, so this prove supervised parametric learning is what you're used to thinking about as learning.",
                    "label": 1
                },
                {
                    "sent": "So we have a data set so collections of X is and wise and we have a model that says that the wise are related to the FS here or corrupted.",
                    "label": 0
                },
                {
                    "sent": "Maybe buy some small amount of noise right?",
                    "label": 0
                },
                {
                    "sent": "And the F value here the function value the function is parameterized by a vector of parameters W and it's the WS that I don't know.",
                    "label": 0
                },
                {
                    "sent": "Right, so my task is now to figure out what are good values in the WS.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing that is based on what's known as.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood, so we can write down first, the likelihood the likelihood function is the probability of the data given the model given the model parameters probability of the data given the parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I've written the probability of the output given the inputs and the parameters, but that's because I'm interested in doing supervised learning and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "You just learn the the outputs given the inputs.",
                    "label": 0
                },
                {
                    "sent": "It's a purely conditional model, right?",
                    "label": 0
                },
                {
                    "sent": "We're not interested in actually modeling where those excess came from.",
                    "label": 0
                },
                {
                    "sent": "The X is, the inputs are somehow given.",
                    "label": 0
                },
                {
                    "sent": "OK, so the probability of the data is P of Y given X and the likelihood function is P of Y given X&W.",
                    "label": 0
                },
                {
                    "sent": "And this M here is just if you had different models there might be there might be different models.",
                    "label": 0
                },
                {
                    "sent": "You also have to condition on which model you're using, right?",
                    "label": 0
                },
                {
                    "sent": "In this case, if the noise here was Gaussian, then the likelihood function is Gaussian is basically saying that the discrepancy between Y&F so y -- F. Here is what distributors as epsilon here.",
                    "label": 0
                },
                {
                    "sent": "That's a Gaussian distribution, and in the maximum likelihood framework, what you do is you say, well, I don't know what the parameters of WR, but maybe I could try to find the parameters that maximize the likelihood if I make the probability of the outputs as large as possible, so that would sort of seem to be the most likely values of W, right?",
                    "label": 1
                },
                {
                    "sent": "So what we do here is you do the arc Max of the likelihood function over W and that's your maximum likelihood estimate of the parameters.",
                    "label": 1
                },
                {
                    "sent": "And then, once you've done that, you've trained your model.",
                    "label": 0
                },
                {
                    "sent": "An what do you want to do?",
                    "label": 0
                },
                {
                    "sent": "Your model you want to make predictions, so you plug in to make predictions.",
                    "label": 0
                },
                {
                    "sent": "You plug in the best possible estimate your head of your parameters.",
                    "label": 0
                },
                {
                    "sent": "You plug that into the likelihood function, and that gives you.",
                    "label": 0
                },
                {
                    "sent": "Now they predict if distribution will be the same as this distribution up here, except you're now plugging in W maximum likelihood, and I use the star notation here to indicate that I'm looking at Test quantities.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what you're probably most used used when you when you're fitting data right?",
                    "label": 0
                },
                {
                    "sent": "So in this case, doing maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "This is the same as doing least squares, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can look at the you can minimize minus the log likelihood instead.",
                    "label": 0
                },
                {
                    "sent": "That corresponds to doing these squares.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what we want to do is we want to do probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "We want to do Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "On this thing.",
                    "label": 0
                },
                {
                    "sent": "So how do you do?",
                    "label": 0
                },
                {
                    "sent": "How do you do Bayesian inference?",
                    "label": 0
                },
                {
                    "sent": "Well, you start by specifying the model in exactly the same way you have exactly the same likelihood function.",
                    "label": 0
                },
                {
                    "sent": "But in this case we are interested in the posterior distribution over the parameters.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So the posterior distribution over the parameters to use, you compute that using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So base rules tells you how to get from the likelihood, which is the probability of Y given X&W to the probability of W given X&Y, right?",
                    "label": 0
                },
                {
                    "sent": "So the Bayes rule is used to swap.",
                    "label": 0
                },
                {
                    "sent": "What you are taking the probability over and what you're conditioning over right?",
                    "label": 0
                },
                {
                    "sent": "So and if we want to write down based rule then we have to have here the probability we have to multiply this by the probability of W. OK, and everything is conditioned on on on EM here and essentially this should also be conditional X.",
                    "label": 0
                },
                {
                    "sent": "Normally you see base rule written as.",
                    "label": 0
                },
                {
                    "sent": "PFA given be.",
                    "label": 0
                },
                {
                    "sent": "Times, Peter B.",
                    "label": 0
                },
                {
                    "sent": "Over a.",
                    "label": 0
                },
                {
                    "sent": "Recipe of be given a.",
                    "label": 0
                },
                {
                    "sent": "Right, but I'm allowed to Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "I'm allowed to extend that to condition everywhere on some other variables, right?",
                    "label": 0
                },
                {
                    "sent": "So in this form of Bayes rule, I've Additionally conditioned everywhere on MI and I've conditioned everywhere on X also, except that this thing here, which also ought to be conditioned on X, is actually independent of X, so I don't need to explicitly write that conditioning.",
                    "label": 0
                },
                {
                    "sent": "So what is this thing where this thing is our?",
                    "label": 0
                },
                {
                    "sent": "It's called the prior on the parameters.",
                    "label": 0
                },
                {
                    "sent": "It's what you know about the parameters before you see the data.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And so this thing of course implies something about what possible values of the.",
                    "label": 0
                },
                {
                    "sent": "Of the parameters could be and.",
                    "label": 0
                },
                {
                    "sent": "The prior also tells you something about what the what.",
                    "label": 0
                },
                {
                    "sent": "Values of WR.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is a.",
                    "label": 0
                },
                {
                    "sent": "At the same time, the.",
                    "label": 0
                },
                {
                    "sent": "So at the moment you could sort of say, well, this looks like a sort of a weakness of the procedure that you that you would have to specify something about those parameters.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would just want to learn them from the data, but it turns out that it's actually not possible to do inference if you're not willing to assume anything.",
                    "label": 0
                },
                {
                    "sent": "If you're not willing to assume anything about your functions, then the function could do anything between two data points.",
                    "label": 0
                },
                {
                    "sent": "Right, so so it's not really possible to do inference in a meaningful way, right?",
                    "label": 0
                },
                {
                    "sent": "So this is somehow specifying the knowledge about the functions.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that it's crucial that we understand what that knowledge or over the over the functions are, right?",
                    "label": 0
                },
                {
                    "sent": "And I'll come back to exactly that.",
                    "label": 0
                },
                {
                    "sent": "That actually turns out to be a strength of the Gaussian process framework, so my posterior distribution will look like this, and it's just notice that it's just the product of the likelihood and the prior, and then normalized with the with the with the term here that doesn't depend on W. This just ensures that this product, when it's normalized this way, ends up being at.",
                    "label": 0
                },
                {
                    "sent": "Properly normalized distribution over W. But essentially, the posterior is just the product of the.",
                    "label": 0
                },
                {
                    "sent": "And of the knowledge from the prior compared combined with the knowledge from the likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and now when I've computed the posterior, then when I want to make predictions.",
                    "label": 0
                },
                {
                    "sent": "My predictions are going to be given by this equation here, so this equation is saying So what we're interested in is the probability of and you have a test output given a test input, and given that we've already observed a particular data set right?",
                    "label": 0
                },
                {
                    "sent": "But notice here we don't want to condition condition on a on a particular value of W. Right, because we don't know exactly what values what the right value of W is.",
                    "label": 0
                },
                {
                    "sent": "So what we do instead is we average the predictions for particular values, weighted by how likely the different WSR, how probable they are under the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the interpretation of what's going on here, But actually this is just a rule of probability theory.",
                    "label": 0
                },
                {
                    "sent": "Right probability of Y given W times the probability of W is a joint distribution of Y star and W. And then I'm marginalized out West.",
                    "label": 0
                },
                {
                    "sent": "That gives me the probability of Y star.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "Right, so let's try to.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's so.",
                    "label": 0
                },
                {
                    "sent": "There's another thing one can also ask about and that is.",
                    "label": 0
                },
                {
                    "sent": "The what's known as the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So one thing that I need to do is that if I trained a bunch of different models, then I want to know which of these models is actually best, right?",
                    "label": 0
                },
                {
                    "sent": "And what we can do is just we can ask for what's the probability of the different models given the observed data.",
                    "label": 0
                },
                {
                    "sent": "Again, I can just use Bayes rule to swap this around, right?",
                    "label": 0
                },
                {
                    "sent": "So it's going to be equal to the prior over the models times this object here, which is called the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So it's the probability of Y given X&M but not given W. It's called the marginal likelihood because the parameter has been marginalized out.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the marginal likelihood here is just the normalization constant that we got from the posterior, right?",
                    "label": 0
                },
                {
                    "sent": "So it's the integral of the prior times likelihood, and again it's just a rule of probability, right?",
                    "label": 0
                },
                {
                    "sent": "It's the probability of Y given W times the probability of W is a joint of Y&W, and now integrate out West.",
                    "label": 0
                },
                {
                    "sent": "And that gives me now the probability of Y.",
                    "label": 0
                },
                {
                    "sent": "Everything conditioned on X&M.",
                    "label": 0
                },
                {
                    "sent": "So the marginal likelihood is important because it tells us it helps us to figure out how good are the models.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of the model, right?",
                    "label": 0
                },
                {
                    "sent": "Actually, the probability of the model is proportional to the marginal likelihood for the model.",
                    "label": 0
                },
                {
                    "sent": "Times the prior on the model, but usually we don't have strong priors on the models, right?",
                    "label": 0
                },
                {
                    "sent": "Usually we would only.",
                    "label": 0
                },
                {
                    "sent": "We only try a model if we think it's likely that it's that it's a good fit, right?",
                    "label": 0
                },
                {
                    "sent": "And this is now again a normalization constant that doesn't depend on anything that we are modeling like this only depends on the data, so this is a sort of a constant.",
                    "label": 0
                },
                {
                    "sent": "So now the problem with this kind of inference.",
                    "label": 0
                },
                {
                    "sent": "So this is very nice that you can just drive these rules from probability theory.",
                    "label": 0
                },
                {
                    "sent": "The problems are these two integrals, right?",
                    "label": 0
                },
                {
                    "sent": "So normally if you have a complicated model then the posterior distribution will be very complicated because it's proportional to the prior to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So then you would have and if you have lots of parameters then you have a high dimensional nonlinear integral to do right and normally for many interesting models this is intractable.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem with Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "But Luckily for Gaussian process you can actually just do this.",
                    "label": 0
                },
                {
                    "sent": "Alright, this turns out just to be Gaussian integrals and you just write down the solution in closed form.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I'll let you go too.",
                    "label": 0
                },
                {
                    "sent": "Coffee.",
                    "label": 0
                },
                {
                    "sent": "So now the clue here is to say, well.",
                    "label": 0
                },
                {
                    "sent": "We're just going to do the same analysis as in the previous slide except.",
                    "label": 0
                },
                {
                    "sent": "Now what in the in the terms of a Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "What are the parameters?",
                    "label": 0
                },
                {
                    "sent": "We didn't have any parameters in the Gaussian process, right?",
                    "label": 0
                },
                {
                    "sent": "In the Gaussian process, it turns out that the parameters is the function itself.",
                    "label": 1
                },
                {
                    "sent": "OK, and this is the thing that's a little bit hard to think about, right, but let's just do the same analysis as we had in the previous slide, and every time we see parameters, we stick in the function OK and see what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the first thing I need to do is to write down the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "OK now likelihood function is OK in this.",
                    "label": 0
                },
                {
                    "sent": "In this slide I've used a notation where I don't write P of Y given X, but it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "I just skip the P. So the probability of Y given X and the function now is this used to say the parameter W, right?",
                    "label": 0
                },
                {
                    "sent": "So I have to substitute in the function.",
                    "label": 0
                },
                {
                    "sent": "It's a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is centered on the F values that correspond to the X is right, right?",
                    "label": 0
                },
                {
                    "sent": "So if we look back on the previous slide, notice that the likelihood function it's a Gaussian where the which is centered on the predicted values.",
                    "label": 0
                },
                {
                    "sent": "Here, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a sentence on the FS evaluated at the corresponding axes.",
                    "label": 0
                },
                {
                    "sent": "So in my case, that will just be for the first case, it will be F1 and for the second case it will be F2.",
                    "label": 0
                },
                {
                    "sent": "And remember I call these the collection of of F values.",
                    "label": 0
                },
                {
                    "sent": "I call them BF X right?",
                    "label": 0
                },
                {
                    "sent": "So that means that the likelihood is just going to be a Gaussian distribution with a mean of BF X and whatever noise variance we had.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Doesn't depend on all the parameters, right?",
                    "label": 0
                },
                {
                    "sent": "All the parameters would be the function everywhere.",
                    "label": 0
                },
                {
                    "sent": "The likelihood actually only depends on the value of the function at the points.",
                    "label": 0
                },
                {
                    "sent": "We're interested in.",
                    "label": 0
                },
                {
                    "sent": "That makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "If we if we want to evaluate whether a function is a good fit to the data or not, and then we look at is it predicting the values well at those points, right?",
                    "label": 0
                },
                {
                    "sent": "We're not look it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Actually, we don't care what the function does over here, right?",
                    "label": 0
                },
                {
                    "sent": "We care about whether it actually predicts the observed target.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have to so before.",
                    "label": 0
                },
                {
                    "sent": "I didn't specify what the what the prior was, but here we could have used the Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "In this case, we would now extend the Gaussian prior instead of being joint Gaussian distribution over those function values.",
                    "label": 0
                },
                {
                    "sent": "It will now be a Gaussian distribution over all the function values, right?",
                    "label": 0
                },
                {
                    "sent": "So this will be the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "OK, so we say what we know about the function before the data arrives is that it comes from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have to compute the posterior as before, so the posterior is the product of the likelihood and the prior and both of them have Gaussian shapes, although this one is a little bit funny, right?",
                    "label": 0
                },
                {
                    "sent": "Is this infinite dimensional Gaussian?",
                    "label": 0
                },
                {
                    "sent": "But still, if you multiply, you multiply 2 Gaussians, you get another Gaussian right?",
                    "label": 0
                },
                {
                    "sent": "Although unnormalized, right?",
                    "label": 0
                },
                {
                    "sent": "But the posterior here is a distribution over F. So we want the normalized version of that.",
                    "label": 0
                },
                {
                    "sent": "So if you multiply together 2 Gaussians, you get another Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you want to apply together 2 infinite dimensional Gaussians, you get an infinite dimensional Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here we are multiplying together an infinite dimensional Gaussian with well with something which is, which is finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "You get an infinite dimensional Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So the posterior will now be a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "OK, that makes sense right?",
                    "label": 0
                },
                {
                    "sent": "The posterior is a distribution of functions.",
                    "label": 0
                },
                {
                    "sent": "Is the distribution of functions that give high probability to the functions that agree with the data and agree with our prior assumptions.",
                    "label": 0
                },
                {
                    "sent": "And so now the posterior process here is has a mean function and has a covariance function and the covariance function in the mean function in various function are just simple functions of the likelihood and prior.",
                    "label": 0
                },
                {
                    "sent": "Right, so the so this thing here.",
                    "label": 0
                },
                {
                    "sent": "So the mean here is a vector.",
                    "label": 0
                },
                {
                    "sent": "It depends on the observation.",
                    "label": 0
                },
                {
                    "sent": "It depends on the on the values of the X is here and it depends on the covariance function.",
                    "label": 0
                },
                {
                    "sent": "So the K matrix here is now the matrix of covariances evaluated all pairs of training points.",
                    "label": 0
                },
                {
                    "sent": "OK, and now we have to do the last integral we have to now.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Compute the predictive distribution here using this integral.",
                    "label": 0
                },
                {
                    "sent": "So in this case this integral is also going to be tractable.",
                    "label": 0
                },
                {
                    "sent": "It's also going to be Gaussian, because this thing here was a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It was an infant dimensional one, but it was a Gaussian and this thing here is just the likelihood function written down for the test case.",
                    "label": 0
                },
                {
                    "sent": "So again, we have a product of two Gaussian terms, which is a Gaussian and an integral part of that is another Gaussian, so they predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "So what is now the predicted distribution?",
                    "label": 0
                },
                {
                    "sent": "The distribution of of a test case given the training set is also going to be just a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "It's going to have a mean and variance.",
                    "label": 0
                },
                {
                    "sent": "OK, so now these are a little bit long.",
                    "label": 0
                },
                {
                    "sent": "Equations will spend the next couple of slides actually.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking them apart, so I'll show you before coffee.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you a pictorial version of this, right?",
                    "label": 0
                },
                {
                    "sent": "So this thing here depicts a distribution the prior distribution, the distribution or functions.",
                    "label": 0
                },
                {
                    "sent": "Before we see the data like so.",
                    "label": 0
                },
                {
                    "sent": "And there are three different draws, so I've drawn three times from this Gaussian Gaussian process distribution.",
                    "label": 0
                },
                {
                    "sent": "So in blue, I've actually done things properly.",
                    "label": 0
                },
                {
                    "sent": "I've actually written down 50 values of X just equidistantly spaced out on this axis here, and then I've drawn from that high dimensional Gaussian those 50 values.",
                    "label": 0
                },
                {
                    "sent": "And then I plotted those 50 values as a function of X is now in the.",
                    "label": 0
                },
                {
                    "sent": "In red and green here, I've cheated a little bit.",
                    "label": 0
                },
                {
                    "sent": "I've actually drawn these 50 samples and then just connected up the lines right so you can see the function.",
                    "label": 0
                },
                {
                    "sent": "But technically I can only evaluate the function that this finite number of points.",
                    "label": 0
                },
                {
                    "sent": "OK, so the function sort of looked like this and you can see there they somehow have a characteristic look to them right?",
                    "label": 0
                },
                {
                    "sent": "They have the same sort of magnitude here in the same two they seem to fluctuate somehow in the same way, and they're all smooth right there?",
                    "label": 0
                },
                {
                    "sent": "Definitely some properties of these functions.",
                    "label": 0
                },
                {
                    "sent": "And then so this was a prior.",
                    "label": 0
                },
                {
                    "sent": "Right now I make some observations and make here make 5 observations.",
                    "label": 0
                },
                {
                    "sent": "So I observed that the function value here is this value here and I've said this value in this value.",
                    "label": 0
                },
                {
                    "sent": "This value.",
                    "label": 0
                },
                {
                    "sent": "And now I say OK. Now what is the posterior process?",
                    "label": 0
                },
                {
                    "sent": "That's what we computed on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "I repeated it down here like the posterior process I've drawn again 3 sample functions from that posterior process and you can see that these functions always agree with the data, right?",
                    "label": 0
                },
                {
                    "sent": "Because we're conditioning on the fact that they have to agree with the data, but there's still some places where we don't know what the function is doing.",
                    "label": 0
                },
                {
                    "sent": "So for example, here it's a little bit uncertain about what the function is doing it out here.",
                    "label": 0
                },
                {
                    "sent": "It still is as uncertain about what's going on as it was under the prior.",
                    "label": 0
                },
                {
                    "sent": "Alright, so one way of thinking about this would be to say, well.",
                    "label": 0
                },
                {
                    "sent": "One way of doing this kind of inference would be to say, well, I make my 5 observations OK, so I write them down.",
                    "label": 0
                },
                {
                    "sent": "Now I draw random functions and I keep drawing random functions.",
                    "label": 0
                },
                {
                    "sent": "Until I get one that agrees with all the five points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this might take awhile, right?",
                    "label": 0
                },
                {
                    "sent": "But if you if you did it then you would get exactly this distribution right?",
                    "label": 0
                },
                {
                    "sent": "So it just happens that you can compute what that distribution is using linear algebra, basically right?",
                    "label": 0
                },
                {
                    "sent": "You just manipulate these Gaussian variables, but it's equivalent to saying I'm interested in this distribution of random function condition on the fact that they agree with my training data.",
                    "label": 0
                },
                {
                    "sent": "And that gives you this distribution or functions.",
                    "label": 0
                },
                {
                    "sent": "OK, let's stop for a break for.",
                    "label": 0
                },
                {
                    "sent": "How long?",
                    "label": 0
                },
                {
                    "sent": "Half past OK.",
                    "label": 0
                }
            ]
        }
    }
}