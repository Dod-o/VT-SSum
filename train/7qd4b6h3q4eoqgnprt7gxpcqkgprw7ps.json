{
    "id": "7qd4b6h3q4eoqgnprt7gxpcqkgprw7ps",
    "title": "Archipelago: Nonparametric Bayesian Semi-Supervised Learning",
    "info": {
        "author": [
            "Ryan Prescott Adams, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_adams_anb/",
    "segmentation": [
        [
            "Alright, so this is work with Zoom Kermani at Cambridge and he's also at Carnegie Mellon and we call this archipelago nonparametric Bayesian semi supervised learning so.",
            "Just to you know, put everybody on the same page."
        ],
        [
            "Sure, what are we talking about with semi supervised learning?",
            "Well this is the classification problem where we have additional unlabeled data and we'd like to be able to use that unlabeled data in order to make better decision boundaries for classification.",
            "So here we have we're trying to say we have features about oranges and apples and bananas, and we might just set up a classifier that tries to partition this space.",
            "But maybe you're you know, maybe you have a bunch of fruit, you're an alien and you don't know what kind of fruit it is, but you have the features of fruit.",
            "You can measure it, and so you have a bunch of a bunch of unlabeled fruit and you'd like to come up with better decision boundaries.",
            "So the question is, how can we use this?"
        ],
        [
            "This additional unlabeled data, in order to make to make better, better classification, and in particular, the question is well, at least from a nonparametric Bayesian point of view.",
            "It's how can we construct this, a density model that will allow us to match the assumptions assumptions of some supervised learning.",
            "In particular, the idea is that we'd like the would like to have this cluster assumption that the boundaries between our densities of the different clusters matches up with the decision boundaries for our classifier classifier, and what that means in practice is that we we can't really do the simple kind of thing even with a very flexible nonparametric Bayesian type model where we would just lay down a bunch of, say, a bunch of Gaussians and hope that they captured the structure recently because they would be independent, in particular with the semi supervised type problem where we want to have.",
            "Our densities that tend to repel each other where we have natural boundaries between between the densities assigned to each class, and So what this talk is about is going to be constructing these kind of densities from Gaussian processes."
        ],
        [
            "So what I'm going to do is, I'm going to tell you a little bit about a Gaussian process density model that we've constructed in previous work, and then I'm going to tell you about how we extend that to the to this semi supervised case by coupling together several of these Gaussian process densities.",
            "Then I'll just give you an overview of inference and show you some toy examples to get a feel for what this does."
        ],
        [
            "Alright, so first let's talk about a."
        ],
        [
            "Gaussian process density model.",
            "But before we even talk about that, let's just I'm just going to tell you what the motivation is for using GPS for this kind of thing.",
            "So Gaussian processes are a very nice distribution on functions, and they allow us to to talk about distributions on functions without having to specify some finite set of basis functions.",
            "We pick a, we pick a kernel function that maybe specifies how smooth we think the function is, where it's different different differentiability properties or it's noise, it's noise level and and what's really neat is that because of the marginalization properties of the Gaussian distribution.",
            "We can then take samples, finite samples of different functions, and deal with these things in a tractable way without having to make approximations.",
            "So whereas we tend to draw these these wiggly lines that are filled in all the way around, in practice, of course we what we really want to do is sample the function at finite locations and then be able to deal with that tractably and a Gaussian process lets us do that.",
            "So what we want to do is turn this into into a set of couple of densities for semi supervised."
        ],
        [
            "Alright, so let me tell you about what we call the Gaussian process density sampler.",
            "So this is previous work for doing GP densities.",
            "But what we do with archipelagos extend this in a way to deal with multiple classes.",
            "So the idea is that we're going to start out with a set of with a set of draws from a GP, so these are just three samples from GP and I'm going to use G in general to describe this kind of thing, and we're going to want to turn that into a density.",
            "And of course densities need to be non negative and they had to integrate to one, so we're going to do a couple of things to it to make that work, and then the Gaussian process density sample of what we do.",
            "Is we first take these random functions and we squash them through a logistic so that now these are are non negative but upper bounded by one, then the next thing we do is we grab some something we call a base density.",
            "But in this talk is just going to be some Pi of X and you can think of this like just a simple parametric model, maybe a Gaussian distribution for your for the space that you're interested in.",
            "And so multiplying this this point wise with our new base density.",
            "Then we're going to get is a set of random functions that are upper bounded by this thing.",
            "And now we can go through and we can normalize these.",
            "And these give us a set of random densities that maybe look like modulations of this.",
            "Of this pie based density here.",
            "So that's the main idea for how to arrive at how to think about the density model."
        ],
        [
            "Soft, but what's neat?",
            "Is that of course, because of the GP, we're going to be able to get all kinds of funky densities out of this that might capture interesting structure and kind of a different way than you would get out of out of something like during the process mixture model.",
            "So what we hope is that with this kind of interesting structure will be able to capture clusters in the semi supervised learning problem.",
            "So, but what?"
        ],
        [
            "But what makes this particularly interesting is that it's going to be tractable without having to do the difficult integral that I totally skipped over right here.",
            "So I wrote this.",
            "So we'll just normalize this, but in practice, you can't really do that, because this is some infinite dimensional thing, and we can't integrate infinite dimensional random functions exactly, we can only we can only evaluate this to within a constant.",
            "But it turns out that because that, because we can generate data from this model.",
            "So this is where we can generate random functions and then draw data from them.",
            "Then we're then inference becomes tractable.",
            "So let me just give you a quick overview of how we generate data from this density and then later in the talk you'll see how we extend this to the multiclass case.",
            "So what we do is we start out with this with this base density here and so we just say take our Gaussian and we draw a bunch of proposals from this Gaussian distribution.",
            "Now conditioned on these proposals were going to be able to draw data from our Gaussian process.",
            "This is as I said, because the marginalization.",
            "Properties of the Gaussian process.",
            "We're going to draw these data exactly, so by exactly I mean drawn from the distribution without being biased by saying you know the starting state of some finite Markov chain, then what we can do is a rejection sampling procedure.",
            "Because of this function, because we squash this function through logistic, it's now bounded between zero and one.",
            "We interpret that as a Bernoulli probability that we.",
            "So we flip a coin to decide whether or not to accept or reject each of these.",
            "Each of these proposals that we just made, and it turns out that data that we keep our consistent with having drawn a random density.",
            "And then generated data from it.",
            "But we didn't have to integrate the function and we didn't have to know the function at more than a finite number of locations.",
            "So."
        ],
        [
            "Because we can do that, then we can do tractable inference via Markov chain Monte Carlo by creating a latent variable model that includes that latent state.",
            "And I'll tell you a little bit more about that in a minute.",
            "But just you can take away the idea that OK.",
            "Here's the density model for GPS, and because we have this general procedure is going to be tractable to do Markov chain Monte Carlo inference.",
            "So separately from this GPS are already good at classification.",
            "This is one of the first things machine learning that we started doing with Gaussian processes when in the context of you know of regression and things.",
            "So the question is, can we take this density model and plug that in with the classifier to come up with a single model that is both the classifier and and the density model for addressing this problem?",
            "And so that's what we."
        ],
        [
            "All archipelago.",
            "So let's go back to."
        ],
        [
            "This picture here what I did before is I showed you draws and then I squash him through a logistic and then I then I multiplied by the space density and then I normalize it.",
            "Now what I've done is I've changed this interpretation here.",
            "So we just talked about rejection sampling before this was a way to get an upper bound so that we could do rejection sampling.",
            "But of course what we could do is view this just as a partition of this of this interval so that we've got this random function that is, that is giving us a random partition here."
        ],
        [
            "We could generalize this so that instead of just having a rejection class, now we have we have K functions for K classes in our semi supervised classification problem, and we're going to just partition this so that we now have the option of either rejecting the point like before if it falls into this region here, or if we were going to accept it, we assign it to one class to one of our K classes.",
            "In this case, just K = 3.",
            "So just to be clear.",
            "So now this is very so.",
            "This is very similar model but with the labeling step essentially.",
            "So if we have again 3 classes.",
            "Three functions draw a random function from the GP.",
            "Squash them now with this softmax plus A plus projection class multiplied by the base density an we still wind up with the density over all of our data, but now conditioned on having been accepted, we now know that it's going to fall into one of these classes."
        ],
        [
            "Now, what's neat is that we can in a very similar way to the Gaussian process density sampler, generate data exactly from this model.",
            "And again, it's by exactly I mean that the data are not biased.",
            "There are directly from this directly from this model with particular setting of the parameters.",
            "So the way we do that is essentially the same idea we're going to.",
            "We're going to draw data from this base from the proposals from the base density.",
            "We're going to draw.",
            "Now all K functions at those proposals.",
            "And now in this kind of clutter diagram here, what we're doing is.",
            "The same thing as we would do with rejection sampling, but instead we're assigning to a particular class, and now at the bottom.",
            "Now we get a set of data where these things have been accepted, where these are the acceptances.",
            "But then also they have they have a color corresponding with class they fell into."
        ],
        [
            "Alright, so that's the model that's that's the way that we go from a set of K Gaussian processes to generating a set of labeled data.",
            "So obviously the problem that we're really interested in is the inference problem, which is where somebody hand you chunk of data, and some of which is labeled something which is unlabeled and you'd like to recover the this latent functions, the thing, and the thing that is actually classifying data at new locations."
        ],
        [
            "So that's the setup.",
            "So we're going to have to end labeled data and Q unlabeled data.",
            "And the question is.",
            "And in some ways from a generative Bayesian point of view.",
            "It's entirely the semi supervised learning problem boils down entirely too.",
            "How do we deal with the unlabeled data?",
            "And of course, when you the right Bayesian thing to do in some sense is just, well, we marginalized over these things that we don't know and you might ask with any kind of model for this is, that is the marginalization of the label of unlabeled data easy.",
            "And there are three different ways.",
            "You can imagine constructing a joint distribution over the feature and the label, so feature is just save the location in this space and then the label is one of these three classes.",
            "Now we could of course just use the product of these two things of a product of features and product labels have no coupling between them, But that's not really fitting the cluster assumption.",
            "That's because there's no reason to believe that the random labels that we would get would correspond in any way to the clusters that we created with their density model.",
            "Another intuitive way you might you might go about doing this is by well will generate a random label and they will have a cluster.",
            "You will have a density model for each cluster, and then we're going to generate.",
            "We're going to generate data that way, and that's a reasonable generative model, but quite often you would expect that trying to integrate out the label would be hard because that it doesn't.",
            "It doesn't fall out directly from this from this product, but what we're doing in archipelago.",
            "Here is the other way around.",
            "We're generating a feature and then we're generating the label.",
            "After we did that because in that if you remember from that from that very first picture I showed you with the partitions.",
            "The thing we got at the bottom here was a was a density for the whole model.",
            "So what that means is that we can."
        ],
        [
            "Very easily, right?",
            "The joint distribution, so the likelihood function for a set of four set of labeled an unlabeled data.",
            "And now this looks kind of complicated, but the take home message is just it's very simple to just write down the probability, some labeled data and the probability of unlabeled data will be marginalized out those labels and we don't have to do anything magic in order to do that.",
            "And we don't even have to include those labels in any kind of Markov chain for inference."
        ],
        [
            "Now the fly in the ointment is the fact that because we're using these Gaussian process densities, we're stuck with what's called a doubly intractable posterior distribution.",
            "Now, if you do a lot of Bayesian inference, then you know that we run into this bit on the bottom of the posterior distribution all the time.",
            "The marginal likelihood, but because it doesn't depend on the parameters, we don't really have to, in general, deal with it.",
            "If Markov chain Monte Carlo methods allow us to do inference in these kind of models without having to evaluate that typically, but in a doubly intractable problem we have this, we have some constant sitting out in front of our likelihood function.",
            "That we can evaluate, and this typically does depend on our on our parameters, and so in this case that corresponds to having to integrate the density implied by those K functions.",
            "Fortunately, when we have a Journal model, recent advances in Markov chain Monte Carlo methods will allow us to actually do."
        ],
        [
            "So to do inference and the way that we do.",
            "That is, we construct a larger Markov chain.",
            "Then we would strictly need we include a set of latent variables where we're modeling what we call the latent history of this generative process.",
            "So somebody hands you a chunk of labeled and unlabeled data and says use this prior, then that's kind of like saying that this this general procedure was run and then arrived at your data.",
            "So if we look back at this previous picture here.",
            "So what we got from the inference point of view is we just got these data along the bottom here.",
            "This is all we saw and but we imagine that this all happened along the way, and because we have a fully probabilistic model for this, we can just construct the joint distribution, introduce all this state is latent variables and then run the Markov chain on that whole object and then the thing.",
            "Of course we're interested in is almost certainly these functions that are classifying our data, or possibly also describing the density for it.",
            "And this is a well defined thing to do and we can just write the joint distribution for this whole.",
            "That whole latent state, without having to do any of these intractable integrals, Ann, I know this, seems really complicated and ugly, so let me just break it down a little bit more precisely, so the latent state we're introducing here is M, which is the number of rejections that we had along the way.",
            "And then we also had these locations of these rejections, and we don't know those.",
            "And then we don't know the values of these K functions anywhere, but we do have a joint distribution over this whole thing, and it just breaks down.",
            "Kind of like this, where we have a Gaussian process prior that we're treating is independent here over the K functions.",
            "We have the probability of the data that we saw this label data.",
            "We have the probability of the unlabeled data and then we introduce this additional term that is, the probability of the data that we rejected and this is something we can sample from straightforwardly with Markov chain Monte Carlo in a few different steps.",
            "So we have a birth death."
        ],
        [
            "Process that we run to introduce these latent these latent rejections and remove them in order to to sample from that space.",
            "And we do that via via Metropolis Hastings, and we introduce a new one we.",
            "We also propose a function setup function values for it.",
            "We can also do a very similar thing moving around the locations of these latent rejections to try to explain our data.",
            "And we do that by Metropolis Hastings as well.",
            "And then we also sampled from the latent function values, so to try to actually modulate these functions up and down to to.",
            "Which is of course the thing that probably were interested in postera inference, and we do this official with Hamiltonian Monte Carlo, which helps us avoid random walk behavior, and then we can also sample from the Gaussian process hyperparameters and this bounding this kind of bounding density as well the base density.",
            "And well, in by sampling from that I mean if you had parameters controlling it then you could you could adjust its shape."
        ],
        [
            "So that's the idea and let me just show you a couple of toy examples.",
            "We have a more comprehensive look at this in the paper."
        ],
        [
            "If you're interested in seeing something other than toy examples.",
            "But here's a comparison of looking at at some three class pinwheel data.",
            "So essentially what we have are are three classes arranged in these little these little arcs here and in each case we have we have about 50 data for each class and then each of them just has one labeled example and on the right is if we imagine just running a softmax GP type classifier on this and we didn't know about this unlabeled data, then what would it do?",
            "Well, it's just going to partition the space in a fairly simple way, but what?",
            "What archipelago is able to do?",
            "Is is grab hold of this state along the arcs and wind up with a better classifier?",
            "So So what I'm showing you here are the just the modes of the marginal predictive distributions at each class end."
        ],
        [
            "And just to give you a sense of what the uncertainty looks like, here's the marginal entropy.",
            "So if you have this categorical distribution at each of these little pixels in space, then that would have a particular interview that tells you how how sure you are about it, and so you can see that most of the you know we're most certain immediately around around the data that the actual label data.",
            "Also, I've shown you three class problem here and then it's worth pointing out that this isn't of course restricted at all to the to the binary case.",
            "We can do an arbitrary number of classes, and we conditioned on the number of classes so.",
            "It's a nonparametric Bayesian model in the sense that we have this very flexible density model.",
            "But what we're we're not allowing for, say, an infinite number of of possible classes.",
            "It should be set."
        ],
        [
            "And then we can also do this in an A4 class.",
            "Kind of these toy pinwheel data and you can see it.",
            "It does a similar thing and also in."
        ],
        [
            "In five classes in world data.",
            "So it's."
        ],
        [
            "The reasonable thing, obviously the."
        ],
        [
            "You know Gaussian processes.",
            "If you dealt with this before, and this is sort of future directions, are you know I tend to be slow so we haven't explored any kind of ways to speed up the fact that we get this in cube scaling with GPS that you invariably get, and in some ways it's even worse than normal with this, because our cubic scaling is in the not only the number of data, but also in the the latent variables that we're including, but the summary overall is that you know.",
            "So having a nonparametric Bayesian model for some supervised learning seems like a nice thing to be able to do.",
            "We have flexible models for densities.",
            "Flexible models for classifiers would be nice to be able to combine these things together.",
            "What our model does is it couples together a set of Gaussian process densities in order to exploit this cluster assumption for some supervised learning.",
            "In particular, what it does is it takes advantage of easy marginalization of the labels that in that was the right thing to do with your unlabeled data, is to marginalized it out and were able to do that analytically.",
            "An we avoid the general we avoid the intractability.",
            "Inherent too many Gaussian process based density models.",
            "By using this generative model.",
            "So there's a couple of interesting things I already mentioned.",
            "The fact that we could explore sparse methods to improve the performance to much larger datasets also should be mentioned because we use this rejection sampling procedure in general.",
            "If you if this pie is, if this base density is a really loose bound on the true density, then you're going to end up with with ever more inefficient.",
            "Well, you're going to end up with very many latent variables introduced into your model, because the rejection sampler is inefficient, but you can imagine more sophisticated pies that would allow you to have tighter bounds.",
            "On that one thing that's really worth pointing out as well is that I've discussed this entirely sort of in terms of densities on.",
            "You know, in some real space, but of course we have positive definite kernels on a lot of different things, and nothing about this is intrinsic to the real line, and so one thing that I think is it could be very interesting is to take some of the some of the machine learnings kernels on, for example on graph San on permutations and different things like that, and look at some supervised learning in those contexts.",
            "There's no reason that this can generalize to that directly.",
            "Also, I should say that one assumption we made that may be kind of strong is that we assume that the Gaussian process densities were independent, but you might have a priore knowledge that there were interesting say anti correlations between the class assignments and you could introduce that using something like a semiparametric latent factor model.",
            "So that's it.",
            "Thanks for listening."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is work with Zoom Kermani at Cambridge and he's also at Carnegie Mellon and we call this archipelago nonparametric Bayesian semi supervised learning so.",
                    "label": 0
                },
                {
                    "sent": "Just to you know, put everybody on the same page.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, what are we talking about with semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Well this is the classification problem where we have additional unlabeled data and we'd like to be able to use that unlabeled data in order to make better decision boundaries for classification.",
                    "label": 0
                },
                {
                    "sent": "So here we have we're trying to say we have features about oranges and apples and bananas, and we might just set up a classifier that tries to partition this space.",
                    "label": 0
                },
                {
                    "sent": "But maybe you're you know, maybe you have a bunch of fruit, you're an alien and you don't know what kind of fruit it is, but you have the features of fruit.",
                    "label": 0
                },
                {
                    "sent": "You can measure it, and so you have a bunch of a bunch of unlabeled fruit and you'd like to come up with better decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how can we use this?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This additional unlabeled data, in order to make to make better, better classification, and in particular, the question is well, at least from a nonparametric Bayesian point of view.",
                    "label": 0
                },
                {
                    "sent": "It's how can we construct this, a density model that will allow us to match the assumptions assumptions of some supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In particular, the idea is that we'd like the would like to have this cluster assumption that the boundaries between our densities of the different clusters matches up with the decision boundaries for our classifier classifier, and what that means in practice is that we we can't really do the simple kind of thing even with a very flexible nonparametric Bayesian type model where we would just lay down a bunch of, say, a bunch of Gaussians and hope that they captured the structure recently because they would be independent, in particular with the semi supervised type problem where we want to have.",
                    "label": 0
                },
                {
                    "sent": "Our densities that tend to repel each other where we have natural boundaries between between the densities assigned to each class, and So what this talk is about is going to be constructing these kind of densities from Gaussian processes.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to do is, I'm going to tell you a little bit about a Gaussian process density model that we've constructed in previous work, and then I'm going to tell you about how we extend that to the to this semi supervised case by coupling together several of these Gaussian process densities.",
                    "label": 0
                },
                {
                    "sent": "Then I'll just give you an overview of inference and show you some toy examples to get a feel for what this does.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so first let's talk about a.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gaussian process density model.",
                    "label": 0
                },
                {
                    "sent": "But before we even talk about that, let's just I'm just going to tell you what the motivation is for using GPS for this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian processes are a very nice distribution on functions, and they allow us to to talk about distributions on functions without having to specify some finite set of basis functions.",
                    "label": 0
                },
                {
                    "sent": "We pick a, we pick a kernel function that maybe specifies how smooth we think the function is, where it's different different differentiability properties or it's noise, it's noise level and and what's really neat is that because of the marginalization properties of the Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "We can then take samples, finite samples of different functions, and deal with these things in a tractable way without having to make approximations.",
                    "label": 0
                },
                {
                    "sent": "So whereas we tend to draw these these wiggly lines that are filled in all the way around, in practice, of course we what we really want to do is sample the function at finite locations and then be able to deal with that tractably and a Gaussian process lets us do that.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is turn this into into a set of couple of densities for semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me tell you about what we call the Gaussian process density sampler.",
                    "label": 1
                },
                {
                    "sent": "So this is previous work for doing GP densities.",
                    "label": 0
                },
                {
                    "sent": "But what we do with archipelagos extend this in a way to deal with multiple classes.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we're going to start out with a set of with a set of draws from a GP, so these are just three samples from GP and I'm going to use G in general to describe this kind of thing, and we're going to want to turn that into a density.",
                    "label": 0
                },
                {
                    "sent": "And of course densities need to be non negative and they had to integrate to one, so we're going to do a couple of things to it to make that work, and then the Gaussian process density sample of what we do.",
                    "label": 0
                },
                {
                    "sent": "Is we first take these random functions and we squash them through a logistic so that now these are are non negative but upper bounded by one, then the next thing we do is we grab some something we call a base density.",
                    "label": 0
                },
                {
                    "sent": "But in this talk is just going to be some Pi of X and you can think of this like just a simple parametric model, maybe a Gaussian distribution for your for the space that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "And so multiplying this this point wise with our new base density.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to get is a set of random functions that are upper bounded by this thing.",
                    "label": 0
                },
                {
                    "sent": "And now we can go through and we can normalize these.",
                    "label": 0
                },
                {
                    "sent": "And these give us a set of random densities that maybe look like modulations of this.",
                    "label": 0
                },
                {
                    "sent": "Of this pie based density here.",
                    "label": 0
                },
                {
                    "sent": "So that's the main idea for how to arrive at how to think about the density model.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Soft, but what's neat?",
                    "label": 0
                },
                {
                    "sent": "Is that of course, because of the GP, we're going to be able to get all kinds of funky densities out of this that might capture interesting structure and kind of a different way than you would get out of out of something like during the process mixture model.",
                    "label": 0
                },
                {
                    "sent": "So what we hope is that with this kind of interesting structure will be able to capture clusters in the semi supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So, but what?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what makes this particularly interesting is that it's going to be tractable without having to do the difficult integral that I totally skipped over right here.",
                    "label": 0
                },
                {
                    "sent": "So I wrote this.",
                    "label": 0
                },
                {
                    "sent": "So we'll just normalize this, but in practice, you can't really do that, because this is some infinite dimensional thing, and we can't integrate infinite dimensional random functions exactly, we can only we can only evaluate this to within a constant.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that because that, because we can generate data from this model.",
                    "label": 0
                },
                {
                    "sent": "So this is where we can generate random functions and then draw data from them.",
                    "label": 0
                },
                {
                    "sent": "Then we're then inference becomes tractable.",
                    "label": 0
                },
                {
                    "sent": "So let me just give you a quick overview of how we generate data from this density and then later in the talk you'll see how we extend this to the multiclass case.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we start out with this with this base density here and so we just say take our Gaussian and we draw a bunch of proposals from this Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Now conditioned on these proposals were going to be able to draw data from our Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "This is as I said, because the marginalization.",
                    "label": 0
                },
                {
                    "sent": "Properties of the Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "We're going to draw these data exactly, so by exactly I mean drawn from the distribution without being biased by saying you know the starting state of some finite Markov chain, then what we can do is a rejection sampling procedure.",
                    "label": 0
                },
                {
                    "sent": "Because of this function, because we squash this function through logistic, it's now bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "We interpret that as a Bernoulli probability that we.",
                    "label": 0
                },
                {
                    "sent": "So we flip a coin to decide whether or not to accept or reject each of these.",
                    "label": 0
                },
                {
                    "sent": "Each of these proposals that we just made, and it turns out that data that we keep our consistent with having drawn a random density.",
                    "label": 0
                },
                {
                    "sent": "And then generated data from it.",
                    "label": 0
                },
                {
                    "sent": "But we didn't have to integrate the function and we didn't have to know the function at more than a finite number of locations.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we can do that, then we can do tractable inference via Markov chain Monte Carlo by creating a latent variable model that includes that latent state.",
                    "label": 1
                },
                {
                    "sent": "And I'll tell you a little bit more about that in a minute.",
                    "label": 0
                },
                {
                    "sent": "But just you can take away the idea that OK.",
                    "label": 0
                },
                {
                    "sent": "Here's the density model for GPS, and because we have this general procedure is going to be tractable to do Markov chain Monte Carlo inference.",
                    "label": 1
                },
                {
                    "sent": "So separately from this GPS are already good at classification.",
                    "label": 0
                },
                {
                    "sent": "This is one of the first things machine learning that we started doing with Gaussian processes when in the context of you know of regression and things.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we take this density model and plug that in with the classifier to come up with a single model that is both the classifier and and the density model for addressing this problem?",
                    "label": 0
                },
                {
                    "sent": "And so that's what we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All archipelago.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This picture here what I did before is I showed you draws and then I squash him through a logistic and then I then I multiplied by the space density and then I normalize it.",
                    "label": 0
                },
                {
                    "sent": "Now what I've done is I've changed this interpretation here.",
                    "label": 0
                },
                {
                    "sent": "So we just talked about rejection sampling before this was a way to get an upper bound so that we could do rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "But of course what we could do is view this just as a partition of this of this interval so that we've got this random function that is, that is giving us a random partition here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We could generalize this so that instead of just having a rejection class, now we have we have K functions for K classes in our semi supervised classification problem, and we're going to just partition this so that we now have the option of either rejecting the point like before if it falls into this region here, or if we were going to accept it, we assign it to one class to one of our K classes.",
                    "label": 0
                },
                {
                    "sent": "In this case, just K = 3.",
                    "label": 0
                },
                {
                    "sent": "So just to be clear.",
                    "label": 0
                },
                {
                    "sent": "So now this is very so.",
                    "label": 0
                },
                {
                    "sent": "This is very similar model but with the labeling step essentially.",
                    "label": 0
                },
                {
                    "sent": "So if we have again 3 classes.",
                    "label": 0
                },
                {
                    "sent": "Three functions draw a random function from the GP.",
                    "label": 1
                },
                {
                    "sent": "Squash them now with this softmax plus A plus projection class multiplied by the base density an we still wind up with the density over all of our data, but now conditioned on having been accepted, we now know that it's going to fall into one of these classes.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what's neat is that we can in a very similar way to the Gaussian process density sampler, generate data exactly from this model.",
                    "label": 0
                },
                {
                    "sent": "And again, it's by exactly I mean that the data are not biased.",
                    "label": 0
                },
                {
                    "sent": "There are directly from this directly from this model with particular setting of the parameters.",
                    "label": 0
                },
                {
                    "sent": "So the way we do that is essentially the same idea we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to draw data from this base from the proposals from the base density.",
                    "label": 1
                },
                {
                    "sent": "We're going to draw.",
                    "label": 0
                },
                {
                    "sent": "Now all K functions at those proposals.",
                    "label": 0
                },
                {
                    "sent": "And now in this kind of clutter diagram here, what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "The same thing as we would do with rejection sampling, but instead we're assigning to a particular class, and now at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Now we get a set of data where these things have been accepted, where these are the acceptances.",
                    "label": 0
                },
                {
                    "sent": "But then also they have they have a color corresponding with class they fell into.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the model that's that's the way that we go from a set of K Gaussian processes to generating a set of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So obviously the problem that we're really interested in is the inference problem, which is where somebody hand you chunk of data, and some of which is labeled something which is unlabeled and you'd like to recover the this latent functions, the thing, and the thing that is actually classifying data at new locations.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the setup.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to end labeled data and Q unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                },
                {
                    "sent": "And in some ways from a generative Bayesian point of view.",
                    "label": 0
                },
                {
                    "sent": "It's entirely the semi supervised learning problem boils down entirely too.",
                    "label": 0
                },
                {
                    "sent": "How do we deal with the unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "And of course, when you the right Bayesian thing to do in some sense is just, well, we marginalized over these things that we don't know and you might ask with any kind of model for this is, that is the marginalization of the label of unlabeled data easy.",
                    "label": 1
                },
                {
                    "sent": "And there are three different ways.",
                    "label": 0
                },
                {
                    "sent": "You can imagine constructing a joint distribution over the feature and the label, so feature is just save the location in this space and then the label is one of these three classes.",
                    "label": 0
                },
                {
                    "sent": "Now we could of course just use the product of these two things of a product of features and product labels have no coupling between them, But that's not really fitting the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "That's because there's no reason to believe that the random labels that we would get would correspond in any way to the clusters that we created with their density model.",
                    "label": 0
                },
                {
                    "sent": "Another intuitive way you might you might go about doing this is by well will generate a random label and they will have a cluster.",
                    "label": 0
                },
                {
                    "sent": "You will have a density model for each cluster, and then we're going to generate.",
                    "label": 0
                },
                {
                    "sent": "We're going to generate data that way, and that's a reasonable generative model, but quite often you would expect that trying to integrate out the label would be hard because that it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fall out directly from this from this product, but what we're doing in archipelago.",
                    "label": 0
                },
                {
                    "sent": "Here is the other way around.",
                    "label": 0
                },
                {
                    "sent": "We're generating a feature and then we're generating the label.",
                    "label": 0
                },
                {
                    "sent": "After we did that because in that if you remember from that from that very first picture I showed you with the partitions.",
                    "label": 0
                },
                {
                    "sent": "The thing we got at the bottom here was a was a density for the whole model.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that we can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very easily, right?",
                    "label": 0
                },
                {
                    "sent": "The joint distribution, so the likelihood function for a set of four set of labeled an unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And now this looks kind of complicated, but the take home message is just it's very simple to just write down the probability, some labeled data and the probability of unlabeled data will be marginalized out those labels and we don't have to do anything magic in order to do that.",
                    "label": 0
                },
                {
                    "sent": "And we don't even have to include those labels in any kind of Markov chain for inference.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the fly in the ointment is the fact that because we're using these Gaussian process densities, we're stuck with what's called a doubly intractable posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do a lot of Bayesian inference, then you know that we run into this bit on the bottom of the posterior distribution all the time.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood, but because it doesn't depend on the parameters, we don't really have to, in general, deal with it.",
                    "label": 0
                },
                {
                    "sent": "If Markov chain Monte Carlo methods allow us to do inference in these kind of models without having to evaluate that typically, but in a doubly intractable problem we have this, we have some constant sitting out in front of our likelihood function.",
                    "label": 0
                },
                {
                    "sent": "That we can evaluate, and this typically does depend on our on our parameters, and so in this case that corresponds to having to integrate the density implied by those K functions.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, when we have a Journal model, recent advances in Markov chain Monte Carlo methods will allow us to actually do.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do inference and the way that we do.",
                    "label": 0
                },
                {
                    "sent": "That is, we construct a larger Markov chain.",
                    "label": 1
                },
                {
                    "sent": "Then we would strictly need we include a set of latent variables where we're modeling what we call the latent history of this generative process.",
                    "label": 0
                },
                {
                    "sent": "So somebody hands you a chunk of labeled and unlabeled data and says use this prior, then that's kind of like saying that this this general procedure was run and then arrived at your data.",
                    "label": 0
                },
                {
                    "sent": "So if we look back at this previous picture here.",
                    "label": 0
                },
                {
                    "sent": "So what we got from the inference point of view is we just got these data along the bottom here.",
                    "label": 0
                },
                {
                    "sent": "This is all we saw and but we imagine that this all happened along the way, and because we have a fully probabilistic model for this, we can just construct the joint distribution, introduce all this state is latent variables and then run the Markov chain on that whole object and then the thing.",
                    "label": 0
                },
                {
                    "sent": "Of course we're interested in is almost certainly these functions that are classifying our data, or possibly also describing the density for it.",
                    "label": 0
                },
                {
                    "sent": "And this is a well defined thing to do and we can just write the joint distribution for this whole.",
                    "label": 1
                },
                {
                    "sent": "That whole latent state, without having to do any of these intractable integrals, Ann, I know this, seems really complicated and ugly, so let me just break it down a little bit more precisely, so the latent state we're introducing here is M, which is the number of rejections that we had along the way.",
                    "label": 1
                },
                {
                    "sent": "And then we also had these locations of these rejections, and we don't know those.",
                    "label": 0
                },
                {
                    "sent": "And then we don't know the values of these K functions anywhere, but we do have a joint distribution over this whole thing, and it just breaks down.",
                    "label": 1
                },
                {
                    "sent": "Kind of like this, where we have a Gaussian process prior that we're treating is independent here over the K functions.",
                    "label": 0
                },
                {
                    "sent": "We have the probability of the data that we saw this label data.",
                    "label": 0
                },
                {
                    "sent": "We have the probability of the unlabeled data and then we introduce this additional term that is, the probability of the data that we rejected and this is something we can sample from straightforwardly with Markov chain Monte Carlo in a few different steps.",
                    "label": 0
                },
                {
                    "sent": "So we have a birth death.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process that we run to introduce these latent these latent rejections and remove them in order to to sample from that space.",
                    "label": 0
                },
                {
                    "sent": "And we do that via via Metropolis Hastings, and we introduce a new one we.",
                    "label": 0
                },
                {
                    "sent": "We also propose a function setup function values for it.",
                    "label": 0
                },
                {
                    "sent": "We can also do a very similar thing moving around the locations of these latent rejections to try to explain our data.",
                    "label": 0
                },
                {
                    "sent": "And we do that by Metropolis Hastings as well.",
                    "label": 0
                },
                {
                    "sent": "And then we also sampled from the latent function values, so to try to actually modulate these functions up and down to to.",
                    "label": 0
                },
                {
                    "sent": "Which is of course the thing that probably were interested in postera inference, and we do this official with Hamiltonian Monte Carlo, which helps us avoid random walk behavior, and then we can also sample from the Gaussian process hyperparameters and this bounding this kind of bounding density as well the base density.",
                    "label": 1
                },
                {
                    "sent": "And well, in by sampling from that I mean if you had parameters controlling it then you could you could adjust its shape.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the idea and let me just show you a couple of toy examples.",
                    "label": 0
                },
                {
                    "sent": "We have a more comprehensive look at this in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you're interested in seeing something other than toy examples.",
                    "label": 0
                },
                {
                    "sent": "But here's a comparison of looking at at some three class pinwheel data.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we have are are three classes arranged in these little these little arcs here and in each case we have we have about 50 data for each class and then each of them just has one labeled example and on the right is if we imagine just running a softmax GP type classifier on this and we didn't know about this unlabeled data, then what would it do?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just going to partition the space in a fairly simple way, but what?",
                    "label": 0
                },
                {
                    "sent": "What archipelago is able to do?",
                    "label": 0
                },
                {
                    "sent": "Is is grab hold of this state along the arcs and wind up with a better classifier?",
                    "label": 0
                },
                {
                    "sent": "So So what I'm showing you here are the just the modes of the marginal predictive distributions at each class end.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just to give you a sense of what the uncertainty looks like, here's the marginal entropy.",
                    "label": 1
                },
                {
                    "sent": "So if you have this categorical distribution at each of these little pixels in space, then that would have a particular interview that tells you how how sure you are about it, and so you can see that most of the you know we're most certain immediately around around the data that the actual label data.",
                    "label": 0
                },
                {
                    "sent": "Also, I've shown you three class problem here and then it's worth pointing out that this isn't of course restricted at all to the to the binary case.",
                    "label": 0
                },
                {
                    "sent": "We can do an arbitrary number of classes, and we conditioned on the number of classes so.",
                    "label": 0
                },
                {
                    "sent": "It's a nonparametric Bayesian model in the sense that we have this very flexible density model.",
                    "label": 0
                },
                {
                    "sent": "But what we're we're not allowing for, say, an infinite number of of possible classes.",
                    "label": 0
                },
                {
                    "sent": "It should be set.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can also do this in an A4 class.",
                    "label": 0
                },
                {
                    "sent": "Kind of these toy pinwheel data and you can see it.",
                    "label": 0
                },
                {
                    "sent": "It does a similar thing and also in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In five classes in world data.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The reasonable thing, obviously the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "If you dealt with this before, and this is sort of future directions, are you know I tend to be slow so we haven't explored any kind of ways to speed up the fact that we get this in cube scaling with GPS that you invariably get, and in some ways it's even worse than normal with this, because our cubic scaling is in the not only the number of data, but also in the the latent variables that we're including, but the summary overall is that you know.",
                    "label": 0
                },
                {
                    "sent": "So having a nonparametric Bayesian model for some supervised learning seems like a nice thing to be able to do.",
                    "label": 1
                },
                {
                    "sent": "We have flexible models for densities.",
                    "label": 0
                },
                {
                    "sent": "Flexible models for classifiers would be nice to be able to combine these things together.",
                    "label": 0
                },
                {
                    "sent": "What our model does is it couples together a set of Gaussian process densities in order to exploit this cluster assumption for some supervised learning.",
                    "label": 0
                },
                {
                    "sent": "In particular, what it does is it takes advantage of easy marginalization of the labels that in that was the right thing to do with your unlabeled data, is to marginalized it out and were able to do that analytically.",
                    "label": 0
                },
                {
                    "sent": "An we avoid the general we avoid the intractability.",
                    "label": 0
                },
                {
                    "sent": "Inherent too many Gaussian process based density models.",
                    "label": 0
                },
                {
                    "sent": "By using this generative model.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple of interesting things I already mentioned.",
                    "label": 0
                },
                {
                    "sent": "The fact that we could explore sparse methods to improve the performance to much larger datasets also should be mentioned because we use this rejection sampling procedure in general.",
                    "label": 0
                },
                {
                    "sent": "If you if this pie is, if this base density is a really loose bound on the true density, then you're going to end up with with ever more inefficient.",
                    "label": 0
                },
                {
                    "sent": "Well, you're going to end up with very many latent variables introduced into your model, because the rejection sampler is inefficient, but you can imagine more sophisticated pies that would allow you to have tighter bounds.",
                    "label": 0
                },
                {
                    "sent": "On that one thing that's really worth pointing out as well is that I've discussed this entirely sort of in terms of densities on.",
                    "label": 0
                },
                {
                    "sent": "You know, in some real space, but of course we have positive definite kernels on a lot of different things, and nothing about this is intrinsic to the real line, and so one thing that I think is it could be very interesting is to take some of the some of the machine learnings kernels on, for example on graph San on permutations and different things like that, and look at some supervised learning in those contexts.",
                    "label": 0
                },
                {
                    "sent": "There's no reason that this can generalize to that directly.",
                    "label": 0
                },
                {
                    "sent": "Also, I should say that one assumption we made that may be kind of strong is that we assume that the Gaussian process densities were independent, but you might have a priore knowledge that there were interesting say anti correlations between the class assignments and you could introduce that using something like a semiparametric latent factor model.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Thanks for listening.",
                    "label": 0
                }
            ]
        }
    }
}