{
    "id": "2ahz3bgugs7hebbgzghiuzipd5t2amhj",
    "title": "The Skew Spectrum of Graphs",
    "info": {
        "author": [
            "Risi Kondor, Gatsby Computational Neuroscience Unit, University College London"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/icml08_kondor_ssg/",
    "segmentation": [
        [
            "This Q spectrum is a means of comparing graphs in learning algorithms and the key idea behind it is invariants, which was mentioned several times already today."
        ],
        [
            "OK, So what?",
            "So the question, I'm really asking is or is it?",
            "Can we do this with just 49 numbers?",
            "Yeah they have to press something here too.",
            "Which is the magic button.",
            "Do you reminder?",
            "Anne.",
            "OK.",
            "So can we.",
            "Can we really characterize graphs by just 49 features?",
            "It seems kind of impossible or surprising at first.",
            "Of course, it depends on."
        ],
        [
            "How big the graphs are.",
            "So this is the sort of thing that I have in mind.",
            "This is an organic molecule.",
            "I'm not sure if you know which one it is.",
            "It's a very important molecule.",
            "And actually, comparing molecules like this is an important business, and Kim Lee, informatics drug discovery and so on so we can do slightly bigger.",
            "We can do up to a couple of 100 vertices.",
            "That is the sort of range of graphs I'm interested in, but I'm not going to be dealing with something like the previous speaker.",
            "I'm not going to be dealing with the Internet or something that's far too big for the sort of methods I'm going to be talking about.",
            "I'm also not going to be talking about like tiny graphs, five or six vertices or something, because there you can do an exhaustive enumeration, some something simpler than what's what's coming up.",
            "So this is the task comparing medium sized graph, say a couple of."
        ],
        [
            "For hundred vertices at the most.",
            "So what is the problem here?",
            "And the problem here is invariants, right?",
            "So you're representing these graphs in your computer somehow.",
            "And inevitably that starts with numbering the vertices.",
            "So you put labels on the vertices, and then you say, construct the adjacency matrix, right?",
            "But of course, the form of the adjacency matrix really depends on how you number the vertices, so you do something innocent looking like just swap two vertices.",
            "And this terrible thing happens to the adjacency matrix.",
            "But you still wanted to represent is still the same graph, right?",
            "So you want your learning algorithm to be insensitive to this invariant, well functions of the adjacency matrix which have this property are called graph invariants, and they actually have a large literature both."
        ],
        [
            "In the pure mathematics community and in computer science in machine learning nowadays, so this talk is going to be about a new set of graph invariants, which is particularly tailored to the sort of things that we want to do in machine learning.",
            "So there are various reasons for looking at graph invariants in the theory community.",
            "People are interested in them because sure, let's work my friend.",
            "Soup OK does this work?",
            "So one reason that you might be interested in crossing the fence is because they might be.",
            "Might be a way to approach the famous morphing problem, right?",
            "If they have a sufficient number of invariants.",
            "If they really capture everything about your graph, then you can easily tell whether two graphs the same or not.",
            "So if I could compute polynomial number of invariants which had this property that they were complete, the uniquely characterized graph, and I could do that in polynomial time, then I wouldn't be here because then I would have sold the graph isomorphism problem.",
            "And that's one of the major outstanding issues in complexity theory, so that would be great.",
            "Unfortunately, we're not going to be able to do quite that.",
            "On the other hand, there other uses for graph invariants, which are a little bit more humble, but not any less important from the point of view of practical work in machine learning, say, and the emphasis is slightly different, so there you won't really efficient computability, and you might not care so much about completeness, so this links up with the now sizable literature on graph kernels from this community.",
            "Several people in the audience have have contributed to this literature, like Thomas Gardner and the shame and other people who want to be pointing fingers too much.",
            "But this is the kind of context in which you should see this way."
        ],
        [
            "OK, so so let's look at the issue.",
            "It let me give you some intuition of what is.",
            "What is the crucial thing that I'm addressing addressing in this talk.",
            "So this is the adjacency matrix of a small graph.",
            "Let's see what happens to this through this matrix.",
            "The first thing that I want to do is I want from taking this matrix.",
            "I want to transform it into a vector.",
            "So I'm going to just move those all these edge weights into a single.",
            "And then I'm thinking about what happens to this vector when I permute the labels of the vertices.",
            "So something like this happens, right the reshuffled?",
            "How they get reshuffled?",
            "Well, this is the important question, but you Def."
        ],
        [
            "The new is that this is a linear transformation, is just the reordering.",
            "It's just the permutation, right?",
            "So you start with the vector of edge weights and then some matrix apply is applied to it to get the reshuffled version of vector of edge weights.",
            "And if you think about the matrix operating on this vector, is simply the tensor product like the Chronicle product of two permutation matrices of the permutation, that is, the graph is subjected to.",
            "So in an algebraic setting, this is the problem I'm facing.",
            "I'm going to vector wait, we have this particular matrix acting on it, we get the new vector of weights, but on those two we want to be able to capture this invariant.",
            "There's actually the invariant under the action of this particular type of matrix.",
            "OK, but immediately you see that you've got one invariant, right?",
            "The permutation matrices are orthogonal matrices, so their Chronicle product is also also orthogonal, so this does not change the vector norm if you just look at the norm.",
            "So V Times V transpose, then that's going to be one graph invariant that's a candidate for a graph feature, but a bit of a silly invariant, because essentially what it does, it just count the weights right?",
            "The L2 of the weights, or something like that.",
            "So it's not quite going to cut it when you want to compare.",
            "Graphs."
        ],
        [
            "But maybe you can do.",
            "Maybe if you look at the algebraic structure of this problem, you can do more.",
            "And of course you can do more the trick here.",
            "The crucial insight is that these matrices these tensor product matrices can be transformed.",
            "Just buy a transformation of coordinate system.",
            "They can be put in this block diagonal form.",
            "OK, so now this is the same matrix.",
            "It's this tensor product matrix.",
            "But now we've decomposed it into a direct sum of smaller matrices.",
            "And what that means.",
            "Is that if you similarly decompose the vectors that it's acting on, then each part of that vector is individually going to be rotated by the matrix, so it's enormous.",
            "Individually going to be maintained.",
            "So now we have not one invariant, but we have say K invariants.",
            "So this is the sort of thing this is the sort of algebraic mechanism that is behind our math."
        ],
        [
            "But let's be a bit more formal and let's see what the real underlying theory is.",
            "Well, for the last two years or so, this is the equation I've been looking at.",
            "This is called the Fourier transform.",
            "A generalized Fourier transform.",
            "In this case, the Fourier transform on the symmetric group, which is just a fancy way of saying that it's a Fourier transform on permutations.",
            "OK, so you see that it looks a bit like a Fourier transform.",
            "You start out with a function.",
            "This function F on permutations.",
            "Sigma is always going to be.",
            "Imitation you some overall permutations weighted by something, and that's going to give you the Fourier coefficient.",
            "Now the trick is that the weights in this case instead of the usual Internet E to the minus Icx or something that you might be used to from ordering harmonic analysis.",
            "In this case there actually matrices, so they're called representation matrices, and the important thing about them is that they obey this relationship, so this is the fundamental idea behind representation theory.",
            "Is to take an algebraic object like the group of permutations which has a natural structure.",
            "In this case just composition of permutations which corresponds to the product and to reflect this structure in matrices, right?",
            "So this complex things of permutations they multiply in a weird way, but these can also multiply in a weird way, so you can use it by a suitable mapping to matrices.",
            "You can model the group model permutations by matrices.",
            "And then the matrices are going to multiply the same way as the permutations do.",
            "And so there's this whole theory behind this.",
            "People have been looking at this for about 100 years.",
            "It's really crucial to physics.",
            "They've charted the sort of representations of the sort of mappings you can have.",
            "In particular, there is a theorem which says that for any finite groups, such as the group permutations, you can always find circle irreducible representations, so the smallest possible representations you can find, the complete set of these.",
            "Again, we are finite.",
            "Number of them, say for the symmetric group, is going to be always a finite number of them.",
            "And if you look in the big books they've described and tedious detail, the exact way of computing these matrices and their properties, and so I'm not going to actually go into much detail about what these matrices are, those all that we need is this crucial property that they obey, and this wonderful generalized Fourier transform that."
        ],
        [
            "They give rise to.",
            "OK, so why do I say that?",
            "It's the it's wonderful and amazing and position really powerful.",
            "It's because it inherits the same crucial properties that ordinary Fourier transformation has.",
            "So this is an invertible transformation.",
            "It's unitary, so it preserves norms.",
            "And it obeys the translation theorem, convolution theorem and everything else that follows from this.",
            "So this is a really powerful tool when you want to do when you do, when you want to do algebra when you want to do computations on some sort of algebraic structure, like the symmetric group, the only property that we're going to focus on in this talk is the translation property.",
            "So you have a function.",
            "Again, we have a function on permutations, so F is a function of.",
            "Sigma.",
            "Then there's a natural way of defining the translator of that function.",
            "So if F is a function on the on permutations and you translated by \u03c0, then that function is going to be just this, so it just introduces this extra pie in verse in the argument.",
            "So this is a bit like when do in grade school or something you are talking about taking a function on the real line and moving it along on the X axis.",
            "So you move it along by Zedd.",
            "Then the new function is going to be F. X minus said right this is the analogue here.",
            "You just introduce this by inverse there.",
            "Now the wonderful thing about the Fourier transform is that this operation looks really simple, infuriate space.",
            "It just corresponds to multiplying by the single constant representation matrix.",
            "So to get the Fourier transform your translator function, all that you need to do is multiply by these, translated by these translation or end of rotation."
        ],
        [
            "Matrices in this space.",
            "So because the Fourier transform of these powerful properties, and there's all sorts of strong analogs with ordering for transformation, it's a.",
            "It's a useful tool in analyzing data on permutations.",
            "Diaconis has a famous and very entertaining group.",
            "Very entertaining book on applications of this theory to statistics.",
            "I highly recommend looking at it if you are not familiar with that.",
            "What's really important for our purposes?",
            "Is that theory also has an algorithmic side so people have developed fast Fourier transform Taylor to these special settings of functions on permutations?",
            "The names some of them names mentioned here are clouds and maslyn, Rockmore, Healy, and a couple of others, and this theory has also started infiltrating the machine learning community lately.",
            "So last year we had a paper at AI stats on using exactly this for this identity management kind of Association problem in multi object tracking track of who is who.",
            "When you've got multiple.",
            "People on the radar and Jonathan Hancock, Carlos Guestrin, and Leo Guibas.",
            "Used essentially the same model, but took it a bit further and wrote a NIPS paper about it a little bit later, so it seems like it seems like this is a new, potentially useful tool that's coming into the community."
        ],
        [
            "OK, So what does do with graphs?",
            "Well, the key thing is to associate a function on permutations to each graph, right?",
            "Because we're trying to get rid of this invariant with respect to permutations, there's a clear link between the graph problem and this business of Fourier transformation on permutations, and the particular function that I'm going to care about is this.",
            "So what this means is that F Sigma only cares about what Sigma permutation does 2 N&N minus.",
            "Right, so mutation is a mapping from numbers one to one.",
            "Where are you?",
            "Numbers.",
            "Going to define our fun.",
            "And.",
            "Expect.",
            "Defined as just the Fourier transform, this fancy non Fourier transform of of this function OK. Maybe it wasn't on the previous line.",
            "That this is going to get a bunch of matrices right?",
            "So this you have one of these sums for each irreducible representation representations are matrices, so now I'm going to be representing my graph as just this sequence of matrices corresponding to the different representations.",
            "Now why do I define F?",
            "The key thing is, which I'm not going to prove here, but it's like a one 9 proof.",
            "This particular definition of F reduces the reshuffle, sees problem to translate on the.",
            "As I.",
            "So.",
            "Re labeling in their two invariants to this rotation thing in for a space just by these representations.",
            "Now these station matrices are unitary.",
            "In the case with symmetry, when in fact they are just orthogonal.",
            "So the same sort of thing that works very signal processing works here as well.",
            "You think think this inner product between the two between the individual three components and their transpose.",
            "These matrices cancel out.",
            "Get invariants you get matrices of invariants, so this is."
        ],
        [
            "So first of all, bodies representation matrices, right?",
            "We are active.",
            "300 years ago.",
            "By by Cambridge so individual representatives are.",
            "I.",
            "In with N. I just had the sequence starts.",
            "One and then.",
            "In written two books ago, insurance on this continued.",
            "Just a vertical diagram, but.",
            "And say for a particular partition in particular permutation.",
            "Which is just.",
            "Cyclic retranslates everything on the symmetric group of five elements.",
            "This is what the.",
            "Black I don't know how much this means to you, but you said they're like real valued matrices."
        ],
        [
            "OK. Fourier transform like graph like this booty graph is what that.",
            "On my software before I like him.",
            "Not really, but the structure is very clear here, right?",
            "So I can see that these lots of matrices dump.",
            "Or sure.",
            "Is a bit like karaoke.",
            "So.",
            "So the thing to notice here is that these matrices are very special form because they come from a graph because there are spectrum of a graph, so all the matrices below here of zero and even these matrices are very sparse.",
            "He's got a constant of their two nonzero columns there and then 10 column in each of these.",
            "In each of these matrices.",
            "And this is true not just for the boat I graph.",
            "This is a general property of the Fourier transform of graphs which is going to be crucial for.",
            "What follows now what are these columns?",
            "How do they relate to what I said earlier?",
            "Well, remember what we had earlier was."
        ],
        [
            "This decomposition of the tensor product of representation matrices.",
            "This block diagonal thing and we had these components on the OVI and rotated version of the vector of weights.",
            "But it turns out that these subvectors there are exactly the."
        ],
        [
            "These columns that we have in the Fourier transform, so this is less a systematic, more mathematical way of formalizing this intuition that you can like break up your space into individual parts which are rotated individually so the norm is conserved in the individual subspaces.",
            "OK now, so how many invariants do you get out of here?",
            "You get 7, which is better than one, but maybe it's not quite satisfactory."
        ],
        [
            "Yeah, but once you are in this algebraic framework, there you can appeal to more general results and you see that there is more stuff out there."
        ],
        [
            "So in particular, the invariants I'm looking at are not 2nd order products of the Fourier transform, but third order products.",
            "This is called the bispectral such thing as the ordinary by spectrum of functions on the real line.",
            "This is a direct generalization to groups, so what's happening here?",
            "I'm not going to explain this in detail, just give you some intuition.",
            "So what's happening here is that you're taking two of these vectors, and you're taking the tensor product, and you're asking how does that tensor product decompose, right?",
            "How does that behave under relabeling?",
            "Well, there, since the individual irreducible representations also decompose when you take the tensor product well defined way.",
            "That way is described by circled clips Gordon theories.",
            "Which is.",
            "You know, like a fundamental thing in representation theory, mathematicians spend a lot of time, alot of time figuring out its form, but for our purposes, all that you have to know is that there is some appropriate transformation matrix here which you can stick in between the tensor product of the Fourier components and particular direct sum of three components, which ensures that this product is going to be invariant.",
            "So these matrices it's easy to kind of write him down as C row one, row 2.",
            "But computing them is not necessarily so easy.",
            "I needed this for my software and I started looking at the literature where there's a paper from the 70s which describes an algorithm refers to some Fortran code which I could never find, and after three weeks of looking at the paper, I decided that I was not quite ready to code it up.",
            "And then there's a book from the 80s by two authors, which it only takes about 200 pages to describe what's happening.",
            "For the first time finally in the history of the universe, these things which are like universal constants, they were computed by Jonathan Huang and then others.",
            "So so now we can do this.",
            "Now we can.",
            "It's either on the Internet somewhere we can.",
            "We could do this product.",
            "Unfortunately it's still really expensive because these matrices get huge.",
            "Even the tensor product gets huge."
        ],
        [
            "So if you really want graph invariants in a practical setting, it's maybe not so helpful.",
            "Excuse spectrum itself, which is in the title of the paper, is a unitarily equivalent form of the same thing, so it's just the transformation.",
            "But it looks much nicer from the computer's point of view, because you see there are new clips Gordon matrices.",
            "Here there are new tensor product is just the ordinary product of two matrices, so it only goes with the size of the irreducible representations.",
            "But there are several of them, so there's an extra index new.",
            "Which in the case of the full spectrum of graphs has two range over 7 separate items.",
            "So at the beginning I had this magic NUM."
        ],
        [
            "Seven, well, that number comes from the 7 * 7 is still remains to talk about how you can compute this sufficiently right I I promised efficiently computable invariants, and this is where the technology is fast.",
            "Fourier transforms comes in, so this is what completes the picture.",
            "The algorithmic thing.",
            "The fast Fourier transform hinges on something similar to ordinary fast Fourier transform, and that is a decomposition like iterative decomposition of the group into subgroups and building up the big fast.",
            "For a big transform from."
        ],
        [
            "Morning transforms again.",
            "I have no way I could describe this in 25 minutes, but just to give you an illustration again, these Tetris shapes player a crucial role.",
            "The transformation actually.",
            "So the computations go along these arrows starting from over there, and in particular, the reason that we had this special form of fast Fourier transform confined to just a few components.",
            "If you start with that thing over there with three boxes.",
            "Then if you arrow is just from there, you can only get to these top four shapes.",
            "OK, so please come to the poster if you want to hear more about."
        ],
        [
            "Young diagrams and tableau and so on.",
            "Well, I can tell you is that there is software out there.",
            "This is downloadable from my web page in which you can hack into it's in C++ and nicely object oriented and so on.",
            "It's called snob for that reason, which does this for you, and it does it for you really fast and also put in specialized software which just does it specifically for graphs and runs on these medium sized graphs and say under."
        ],
        [
            "2nd just input the adjacency matrix outcomes.",
            "The invariants OK, so we've got 49 graph invariants.",
            "Can compute them in Ncube.",
            "Time progression is whether they're any good, right?",
            "So can 49 invariants be possibly sufficient?"
        ],
        [
            "To solve machine learning tasks.",
            "And the surprising result is that for the sort of task that people are interested in, well, maybe yes, right?",
            "So we took these datasets of organic compounds as advertised at the beginning, and we compare to kind of the state of the art in graph kernels.",
            "So the random walk kernels which are popular, the shortest path kernel which seems to be the best at the moment.",
            "And when they turned out that our method with new tunable parameters just straight out of the box in three out of four cases, actually beats all the others.",
            "In this enzyme case, it's a slightly worse than the shortest path or not, so I'm hoping that this is something that people will use in practice and you just plug in."
        ],
        [
            "Matrices, alchemy invariants, and you can use it in your machine learning algorithm.",
            "OK, so just to recap, I'm sorry for running overtime slightly.",
            "I presented a general method of finding invariants to the action of a group with the specific application of graph invariants.",
            "We only got 49 invariants out, but somehow surprisingly, they seem to be quite powerful at distinguishing between different graphs.",
            "In fact, I should say that I tried it out for all possible graphs of small sizes, like 567 vertices, and there are very few coincidences, so it's almost good enough to solve the question of, you know whether two graphs are either the same or not, it can be.",
            "Really distinguish between graphs.",
            "They are very fast to compute.",
            "You only need to compute them once and then you plug the computer features in your algorithm so it's linear in the number of learning examples.",
            "And of course the next thing one that we want to do is to generalize it to labeled graphs, because often people are interested in that sort of thing.",
            "But actually when it comes to organic compounds, you know you only have 4 atoms.",
            "I mean sometimes you have an extra which which are like iron or something which is crucial, but if you only have carbon, oxygen, nitrogen and hydrogen then the bonds themselves can give away the identity of the of the.",
            "The atoms, right?",
            "So it's so labels in this case, or maybe not such a crucial issue as if they might appear at first sight.",
            "Great, thank you very much.",
            "Happy to field any questions.",
            "So we've given it.",
            "Underground.",
            "Present your results.",
            "Do you have?",
            "General enough to deal with.",
            "So this deals with.",
            "I should have emphasized this more.",
            "This deals with directed weighted graphs.",
            "It does not deal with what I call labeled graphs.",
            "Are you calling for them?",
            "Attributed graphs, but we are busy working on extending it to that case.",
            "There are a couple of ways in which you could do that is not trivial, but the Canonical way of doing that, but obviously from the point of view of applications that's important at the moment, is just for just the adjacency matrix.",
            "We'll see.",
            "Relocation usually work on being by possible set.",
            "Actually.",
            "Drafted.",
            "So couldn't you actually use them?",
            "Store.",
            "Actually.",
            "Virus.",
            "I've.",
            "The trouble is that these are giving is kind of holistic features, and they're difficult to interpret, so it's difficult to say how to fiddle with them.",
            "If you want to tailor them particular types, graphs, some things you can do this in the Fourier spectrum.",
            "Remember these you have these.",
            "Seven nonzero columns in total for the last one disappears.",
            "For undirected graphs, for example.",
            "So you can make your life that much easier if you know that everything is going to be undirected, but otherwise just looking at this for transfers and interesting question, it's not figuring out what the relationship between the graph and the Fourier transform is.",
            "Thank God speakers."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This Q spectrum is a means of comparing graphs in learning algorithms and the key idea behind it is invariants, which was mentioned several times already today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what?",
                    "label": 0
                },
                {
                    "sent": "So the question, I'm really asking is or is it?",
                    "label": 0
                },
                {
                    "sent": "Can we do this with just 49 numbers?",
                    "label": 0
                },
                {
                    "sent": "Yeah they have to press something here too.",
                    "label": 0
                },
                {
                    "sent": "Which is the magic button.",
                    "label": 0
                },
                {
                    "sent": "Do you reminder?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So can we.",
                    "label": 0
                },
                {
                    "sent": "Can we really characterize graphs by just 49 features?",
                    "label": 1
                },
                {
                    "sent": "It seems kind of impossible or surprising at first.",
                    "label": 0
                },
                {
                    "sent": "Of course, it depends on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How big the graphs are.",
                    "label": 0
                },
                {
                    "sent": "So this is the sort of thing that I have in mind.",
                    "label": 0
                },
                {
                    "sent": "This is an organic molecule.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you know which one it is.",
                    "label": 0
                },
                {
                    "sent": "It's a very important molecule.",
                    "label": 0
                },
                {
                    "sent": "And actually, comparing molecules like this is an important business, and Kim Lee, informatics drug discovery and so on so we can do slightly bigger.",
                    "label": 0
                },
                {
                    "sent": "We can do up to a couple of 100 vertices.",
                    "label": 0
                },
                {
                    "sent": "That is the sort of range of graphs I'm interested in, but I'm not going to be dealing with something like the previous speaker.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be dealing with the Internet or something that's far too big for the sort of methods I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "I'm also not going to be talking about like tiny graphs, five or six vertices or something, because there you can do an exhaustive enumeration, some something simpler than what's what's coming up.",
                    "label": 0
                },
                {
                    "sent": "So this is the task comparing medium sized graph, say a couple of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For hundred vertices at the most.",
                    "label": 0
                },
                {
                    "sent": "So what is the problem here?",
                    "label": 0
                },
                {
                    "sent": "And the problem here is invariants, right?",
                    "label": 0
                },
                {
                    "sent": "So you're representing these graphs in your computer somehow.",
                    "label": 0
                },
                {
                    "sent": "And inevitably that starts with numbering the vertices.",
                    "label": 0
                },
                {
                    "sent": "So you put labels on the vertices, and then you say, construct the adjacency matrix, right?",
                    "label": 0
                },
                {
                    "sent": "But of course, the form of the adjacency matrix really depends on how you number the vertices, so you do something innocent looking like just swap two vertices.",
                    "label": 0
                },
                {
                    "sent": "And this terrible thing happens to the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "But you still wanted to represent is still the same graph, right?",
                    "label": 0
                },
                {
                    "sent": "So you want your learning algorithm to be insensitive to this invariant, well functions of the adjacency matrix which have this property are called graph invariants, and they actually have a large literature both.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the pure mathematics community and in computer science in machine learning nowadays, so this talk is going to be about a new set of graph invariants, which is particularly tailored to the sort of things that we want to do in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So there are various reasons for looking at graph invariants in the theory community.",
                    "label": 0
                },
                {
                    "sent": "People are interested in them because sure, let's work my friend.",
                    "label": 0
                },
                {
                    "sent": "Soup OK does this work?",
                    "label": 0
                },
                {
                    "sent": "So one reason that you might be interested in crossing the fence is because they might be.",
                    "label": 0
                },
                {
                    "sent": "Might be a way to approach the famous morphing problem, right?",
                    "label": 0
                },
                {
                    "sent": "If they have a sufficient number of invariants.",
                    "label": 0
                },
                {
                    "sent": "If they really capture everything about your graph, then you can easily tell whether two graphs the same or not.",
                    "label": 0
                },
                {
                    "sent": "So if I could compute polynomial number of invariants which had this property that they were complete, the uniquely characterized graph, and I could do that in polynomial time, then I wouldn't be here because then I would have sold the graph isomorphism problem.",
                    "label": 1
                },
                {
                    "sent": "And that's one of the major outstanding issues in complexity theory, so that would be great.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we're not going to be able to do quite that.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there other uses for graph invariants, which are a little bit more humble, but not any less important from the point of view of practical work in machine learning, say, and the emphasis is slightly different, so there you won't really efficient computability, and you might not care so much about completeness, so this links up with the now sizable literature on graph kernels from this community.",
                    "label": 0
                },
                {
                    "sent": "Several people in the audience have have contributed to this literature, like Thomas Gardner and the shame and other people who want to be pointing fingers too much.",
                    "label": 0
                },
                {
                    "sent": "But this is the kind of context in which you should see this way.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so let's look at the issue.",
                    "label": 0
                },
                {
                    "sent": "It let me give you some intuition of what is.",
                    "label": 0
                },
                {
                    "sent": "What is the crucial thing that I'm addressing addressing in this talk.",
                    "label": 0
                },
                {
                    "sent": "So this is the adjacency matrix of a small graph.",
                    "label": 0
                },
                {
                    "sent": "Let's see what happens to this through this matrix.",
                    "label": 0
                },
                {
                    "sent": "The first thing that I want to do is I want from taking this matrix.",
                    "label": 0
                },
                {
                    "sent": "I want to transform it into a vector.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to just move those all these edge weights into a single.",
                    "label": 0
                },
                {
                    "sent": "And then I'm thinking about what happens to this vector when I permute the labels of the vertices.",
                    "label": 0
                },
                {
                    "sent": "So something like this happens, right the reshuffled?",
                    "label": 0
                },
                {
                    "sent": "How they get reshuffled?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the important question, but you Def.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The new is that this is a linear transformation, is just the reordering.",
                    "label": 0
                },
                {
                    "sent": "It's just the permutation, right?",
                    "label": 0
                },
                {
                    "sent": "So you start with the vector of edge weights and then some matrix apply is applied to it to get the reshuffled version of vector of edge weights.",
                    "label": 0
                },
                {
                    "sent": "And if you think about the matrix operating on this vector, is simply the tensor product like the Chronicle product of two permutation matrices of the permutation, that is, the graph is subjected to.",
                    "label": 0
                },
                {
                    "sent": "So in an algebraic setting, this is the problem I'm facing.",
                    "label": 0
                },
                {
                    "sent": "I'm going to vector wait, we have this particular matrix acting on it, we get the new vector of weights, but on those two we want to be able to capture this invariant.",
                    "label": 0
                },
                {
                    "sent": "There's actually the invariant under the action of this particular type of matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, but immediately you see that you've got one invariant, right?",
                    "label": 0
                },
                {
                    "sent": "The permutation matrices are orthogonal matrices, so their Chronicle product is also also orthogonal, so this does not change the vector norm if you just look at the norm.",
                    "label": 0
                },
                {
                    "sent": "So V Times V transpose, then that's going to be one graph invariant that's a candidate for a graph feature, but a bit of a silly invariant, because essentially what it does, it just count the weights right?",
                    "label": 0
                },
                {
                    "sent": "The L2 of the weights, or something like that.",
                    "label": 0
                },
                {
                    "sent": "So it's not quite going to cut it when you want to compare.",
                    "label": 0
                },
                {
                    "sent": "Graphs.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But maybe you can do.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you look at the algebraic structure of this problem, you can do more.",
                    "label": 0
                },
                {
                    "sent": "And of course you can do more the trick here.",
                    "label": 0
                },
                {
                    "sent": "The crucial insight is that these matrices these tensor product matrices can be transformed.",
                    "label": 0
                },
                {
                    "sent": "Just buy a transformation of coordinate system.",
                    "label": 0
                },
                {
                    "sent": "They can be put in this block diagonal form.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is the same matrix.",
                    "label": 0
                },
                {
                    "sent": "It's this tensor product matrix.",
                    "label": 0
                },
                {
                    "sent": "But now we've decomposed it into a direct sum of smaller matrices.",
                    "label": 0
                },
                {
                    "sent": "And what that means.",
                    "label": 0
                },
                {
                    "sent": "Is that if you similarly decompose the vectors that it's acting on, then each part of that vector is individually going to be rotated by the matrix, so it's enormous.",
                    "label": 0
                },
                {
                    "sent": "Individually going to be maintained.",
                    "label": 0
                },
                {
                    "sent": "So now we have not one invariant, but we have say K invariants.",
                    "label": 1
                },
                {
                    "sent": "So this is the sort of thing this is the sort of algebraic mechanism that is behind our math.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's be a bit more formal and let's see what the real underlying theory is.",
                    "label": 0
                },
                {
                    "sent": "Well, for the last two years or so, this is the equation I've been looking at.",
                    "label": 0
                },
                {
                    "sent": "This is called the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "A generalized Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "In this case, the Fourier transform on the symmetric group, which is just a fancy way of saying that it's a Fourier transform on permutations.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see that it looks a bit like a Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "You start out with a function.",
                    "label": 0
                },
                {
                    "sent": "This function F on permutations.",
                    "label": 0
                },
                {
                    "sent": "Sigma is always going to be.",
                    "label": 0
                },
                {
                    "sent": "Imitation you some overall permutations weighted by something, and that's going to give you the Fourier coefficient.",
                    "label": 0
                },
                {
                    "sent": "Now the trick is that the weights in this case instead of the usual Internet E to the minus Icx or something that you might be used to from ordering harmonic analysis.",
                    "label": 0
                },
                {
                    "sent": "In this case there actually matrices, so they're called representation matrices, and the important thing about them is that they obey this relationship, so this is the fundamental idea behind representation theory.",
                    "label": 0
                },
                {
                    "sent": "Is to take an algebraic object like the group of permutations which has a natural structure.",
                    "label": 0
                },
                {
                    "sent": "In this case just composition of permutations which corresponds to the product and to reflect this structure in matrices, right?",
                    "label": 0
                },
                {
                    "sent": "So this complex things of permutations they multiply in a weird way, but these can also multiply in a weird way, so you can use it by a suitable mapping to matrices.",
                    "label": 0
                },
                {
                    "sent": "You can model the group model permutations by matrices.",
                    "label": 0
                },
                {
                    "sent": "And then the matrices are going to multiply the same way as the permutations do.",
                    "label": 0
                },
                {
                    "sent": "And so there's this whole theory behind this.",
                    "label": 0
                },
                {
                    "sent": "People have been looking at this for about 100 years.",
                    "label": 0
                },
                {
                    "sent": "It's really crucial to physics.",
                    "label": 0
                },
                {
                    "sent": "They've charted the sort of representations of the sort of mappings you can have.",
                    "label": 0
                },
                {
                    "sent": "In particular, there is a theorem which says that for any finite groups, such as the group permutations, you can always find circle irreducible representations, so the smallest possible representations you can find, the complete set of these.",
                    "label": 0
                },
                {
                    "sent": "Again, we are finite.",
                    "label": 0
                },
                {
                    "sent": "Number of them, say for the symmetric group, is going to be always a finite number of them.",
                    "label": 0
                },
                {
                    "sent": "And if you look in the big books they've described and tedious detail, the exact way of computing these matrices and their properties, and so I'm not going to actually go into much detail about what these matrices are, those all that we need is this crucial property that they obey, and this wonderful generalized Fourier transform that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They give rise to.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do I say that?",
                    "label": 0
                },
                {
                    "sent": "It's the it's wonderful and amazing and position really powerful.",
                    "label": 0
                },
                {
                    "sent": "It's because it inherits the same crucial properties that ordinary Fourier transformation has.",
                    "label": 0
                },
                {
                    "sent": "So this is an invertible transformation.",
                    "label": 0
                },
                {
                    "sent": "It's unitary, so it preserves norms.",
                    "label": 0
                },
                {
                    "sent": "And it obeys the translation theorem, convolution theorem and everything else that follows from this.",
                    "label": 1
                },
                {
                    "sent": "So this is a really powerful tool when you want to do when you do, when you want to do algebra when you want to do computations on some sort of algebraic structure, like the symmetric group, the only property that we're going to focus on in this talk is the translation property.",
                    "label": 0
                },
                {
                    "sent": "So you have a function.",
                    "label": 0
                },
                {
                    "sent": "Again, we have a function on permutations, so F is a function of.",
                    "label": 0
                },
                {
                    "sent": "Sigma.",
                    "label": 0
                },
                {
                    "sent": "Then there's a natural way of defining the translator of that function.",
                    "label": 0
                },
                {
                    "sent": "So if F is a function on the on permutations and you translated by \u03c0, then that function is going to be just this, so it just introduces this extra pie in verse in the argument.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit like when do in grade school or something you are talking about taking a function on the real line and moving it along on the X axis.",
                    "label": 0
                },
                {
                    "sent": "So you move it along by Zedd.",
                    "label": 0
                },
                {
                    "sent": "Then the new function is going to be F. X minus said right this is the analogue here.",
                    "label": 0
                },
                {
                    "sent": "You just introduce this by inverse there.",
                    "label": 0
                },
                {
                    "sent": "Now the wonderful thing about the Fourier transform is that this operation looks really simple, infuriate space.",
                    "label": 0
                },
                {
                    "sent": "It just corresponds to multiplying by the single constant representation matrix.",
                    "label": 0
                },
                {
                    "sent": "So to get the Fourier transform your translator function, all that you need to do is multiply by these, translated by these translation or end of rotation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrices in this space.",
                    "label": 0
                },
                {
                    "sent": "So because the Fourier transform of these powerful properties, and there's all sorts of strong analogs with ordering for transformation, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a useful tool in analyzing data on permutations.",
                    "label": 1
                },
                {
                    "sent": "Diaconis has a famous and very entertaining group.",
                    "label": 0
                },
                {
                    "sent": "Very entertaining book on applications of this theory to statistics.",
                    "label": 0
                },
                {
                    "sent": "I highly recommend looking at it if you are not familiar with that.",
                    "label": 0
                },
                {
                    "sent": "What's really important for our purposes?",
                    "label": 0
                },
                {
                    "sent": "Is that theory also has an algorithmic side so people have developed fast Fourier transform Taylor to these special settings of functions on permutations?",
                    "label": 0
                },
                {
                    "sent": "The names some of them names mentioned here are clouds and maslyn, Rockmore, Healy, and a couple of others, and this theory has also started infiltrating the machine learning community lately.",
                    "label": 0
                },
                {
                    "sent": "So last year we had a paper at AI stats on using exactly this for this identity management kind of Association problem in multi object tracking track of who is who.",
                    "label": 0
                },
                {
                    "sent": "When you've got multiple.",
                    "label": 1
                },
                {
                    "sent": "People on the radar and Jonathan Hancock, Carlos Guestrin, and Leo Guibas.",
                    "label": 0
                },
                {
                    "sent": "Used essentially the same model, but took it a bit further and wrote a NIPS paper about it a little bit later, so it seems like it seems like this is a new, potentially useful tool that's coming into the community.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what does do with graphs?",
                    "label": 0
                },
                {
                    "sent": "Well, the key thing is to associate a function on permutations to each graph, right?",
                    "label": 0
                },
                {
                    "sent": "Because we're trying to get rid of this invariant with respect to permutations, there's a clear link between the graph problem and this business of Fourier transformation on permutations, and the particular function that I'm going to care about is this.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that F Sigma only cares about what Sigma permutation does 2 N&N minus.",
                    "label": 0
                },
                {
                    "sent": "Right, so mutation is a mapping from numbers one to one.",
                    "label": 0
                },
                {
                    "sent": "Where are you?",
                    "label": 0
                },
                {
                    "sent": "Numbers.",
                    "label": 0
                },
                {
                    "sent": "Going to define our fun.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Expect.",
                    "label": 0
                },
                {
                    "sent": "Defined as just the Fourier transform, this fancy non Fourier transform of of this function OK. Maybe it wasn't on the previous line.",
                    "label": 1
                },
                {
                    "sent": "That this is going to get a bunch of matrices right?",
                    "label": 0
                },
                {
                    "sent": "So this you have one of these sums for each irreducible representation representations are matrices, so now I'm going to be representing my graph as just this sequence of matrices corresponding to the different representations.",
                    "label": 0
                },
                {
                    "sent": "Now why do I define F?",
                    "label": 0
                },
                {
                    "sent": "The key thing is, which I'm not going to prove here, but it's like a one 9 proof.",
                    "label": 0
                },
                {
                    "sent": "This particular definition of F reduces the reshuffle, sees problem to translate on the.",
                    "label": 0
                },
                {
                    "sent": "As I.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Re labeling in their two invariants to this rotation thing in for a space just by these representations.",
                    "label": 0
                },
                {
                    "sent": "Now these station matrices are unitary.",
                    "label": 0
                },
                {
                    "sent": "In the case with symmetry, when in fact they are just orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So the same sort of thing that works very signal processing works here as well.",
                    "label": 0
                },
                {
                    "sent": "You think think this inner product between the two between the individual three components and their transpose.",
                    "label": 0
                },
                {
                    "sent": "These matrices cancel out.",
                    "label": 0
                },
                {
                    "sent": "Get invariants you get matrices of invariants, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, bodies representation matrices, right?",
                    "label": 0
                },
                {
                    "sent": "We are active.",
                    "label": 0
                },
                {
                    "sent": "300 years ago.",
                    "label": 0
                },
                {
                    "sent": "By by Cambridge so individual representatives are.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "In with N. I just had the sequence starts.",
                    "label": 0
                },
                {
                    "sent": "One and then.",
                    "label": 0
                },
                {
                    "sent": "In written two books ago, insurance on this continued.",
                    "label": 0
                },
                {
                    "sent": "Just a vertical diagram, but.",
                    "label": 0
                },
                {
                    "sent": "And say for a particular partition in particular permutation.",
                    "label": 0
                },
                {
                    "sent": "Which is just.",
                    "label": 0
                },
                {
                    "sent": "Cyclic retranslates everything on the symmetric group of five elements.",
                    "label": 0
                },
                {
                    "sent": "This is what the.",
                    "label": 0
                },
                {
                    "sent": "Black I don't know how much this means to you, but you said they're like real valued matrices.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Fourier transform like graph like this booty graph is what that.",
                    "label": 0
                },
                {
                    "sent": "On my software before I like him.",
                    "label": 0
                },
                {
                    "sent": "Not really, but the structure is very clear here, right?",
                    "label": 0
                },
                {
                    "sent": "So I can see that these lots of matrices dump.",
                    "label": 0
                },
                {
                    "sent": "Or sure.",
                    "label": 0
                },
                {
                    "sent": "Is a bit like karaoke.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the thing to notice here is that these matrices are very special form because they come from a graph because there are spectrum of a graph, so all the matrices below here of zero and even these matrices are very sparse.",
                    "label": 0
                },
                {
                    "sent": "He's got a constant of their two nonzero columns there and then 10 column in each of these.",
                    "label": 0
                },
                {
                    "sent": "In each of these matrices.",
                    "label": 0
                },
                {
                    "sent": "And this is true not just for the boat I graph.",
                    "label": 0
                },
                {
                    "sent": "This is a general property of the Fourier transform of graphs which is going to be crucial for.",
                    "label": 0
                },
                {
                    "sent": "What follows now what are these columns?",
                    "label": 0
                },
                {
                    "sent": "How do they relate to what I said earlier?",
                    "label": 0
                },
                {
                    "sent": "Well, remember what we had earlier was.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This decomposition of the tensor product of representation matrices.",
                    "label": 0
                },
                {
                    "sent": "This block diagonal thing and we had these components on the OVI and rotated version of the vector of weights.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that these subvectors there are exactly the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These columns that we have in the Fourier transform, so this is less a systematic, more mathematical way of formalizing this intuition that you can like break up your space into individual parts which are rotated individually so the norm is conserved in the individual subspaces.",
                    "label": 0
                },
                {
                    "sent": "OK now, so how many invariants do you get out of here?",
                    "label": 0
                },
                {
                    "sent": "You get 7, which is better than one, but maybe it's not quite satisfactory.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, but once you are in this algebraic framework, there you can appeal to more general results and you see that there is more stuff out there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, the invariants I'm looking at are not 2nd order products of the Fourier transform, but third order products.",
                    "label": 0
                },
                {
                    "sent": "This is called the bispectral such thing as the ordinary by spectrum of functions on the real line.",
                    "label": 0
                },
                {
                    "sent": "This is a direct generalization to groups, so what's happening here?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain this in detail, just give you some intuition.",
                    "label": 0
                },
                {
                    "sent": "So what's happening here is that you're taking two of these vectors, and you're taking the tensor product, and you're asking how does that tensor product decompose, right?",
                    "label": 0
                },
                {
                    "sent": "How does that behave under relabeling?",
                    "label": 0
                },
                {
                    "sent": "Well, there, since the individual irreducible representations also decompose when you take the tensor product well defined way.",
                    "label": 0
                },
                {
                    "sent": "That way is described by circled clips Gordon theories.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "You know, like a fundamental thing in representation theory, mathematicians spend a lot of time, alot of time figuring out its form, but for our purposes, all that you have to know is that there is some appropriate transformation matrix here which you can stick in between the tensor product of the Fourier components and particular direct sum of three components, which ensures that this product is going to be invariant.",
                    "label": 0
                },
                {
                    "sent": "So these matrices it's easy to kind of write him down as C row one, row 2.",
                    "label": 0
                },
                {
                    "sent": "But computing them is not necessarily so easy.",
                    "label": 0
                },
                {
                    "sent": "I needed this for my software and I started looking at the literature where there's a paper from the 70s which describes an algorithm refers to some Fortran code which I could never find, and after three weeks of looking at the paper, I decided that I was not quite ready to code it up.",
                    "label": 0
                },
                {
                    "sent": "And then there's a book from the 80s by two authors, which it only takes about 200 pages to describe what's happening.",
                    "label": 0
                },
                {
                    "sent": "For the first time finally in the history of the universe, these things which are like universal constants, they were computed by Jonathan Huang and then others.",
                    "label": 0
                },
                {
                    "sent": "So so now we can do this.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "It's either on the Internet somewhere we can.",
                    "label": 0
                },
                {
                    "sent": "We could do this product.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it's still really expensive because these matrices get huge.",
                    "label": 0
                },
                {
                    "sent": "Even the tensor product gets huge.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you really want graph invariants in a practical setting, it's maybe not so helpful.",
                    "label": 0
                },
                {
                    "sent": "Excuse spectrum itself, which is in the title of the paper, is a unitarily equivalent form of the same thing, so it's just the transformation.",
                    "label": 0
                },
                {
                    "sent": "But it looks much nicer from the computer's point of view, because you see there are new clips Gordon matrices.",
                    "label": 0
                },
                {
                    "sent": "Here there are new tensor product is just the ordinary product of two matrices, so it only goes with the size of the irreducible representations.",
                    "label": 0
                },
                {
                    "sent": "But there are several of them, so there's an extra index new.",
                    "label": 0
                },
                {
                    "sent": "Which in the case of the full spectrum of graphs has two range over 7 separate items.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning I had this magic NUM.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seven, well, that number comes from the 7 * 7 is still remains to talk about how you can compute this sufficiently right I I promised efficiently computable invariants, and this is where the technology is fast.",
                    "label": 0
                },
                {
                    "sent": "Fourier transforms comes in, so this is what completes the picture.",
                    "label": 0
                },
                {
                    "sent": "The algorithmic thing.",
                    "label": 0
                },
                {
                    "sent": "The fast Fourier transform hinges on something similar to ordinary fast Fourier transform, and that is a decomposition like iterative decomposition of the group into subgroups and building up the big fast.",
                    "label": 0
                },
                {
                    "sent": "For a big transform from.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning transforms again.",
                    "label": 0
                },
                {
                    "sent": "I have no way I could describe this in 25 minutes, but just to give you an illustration again, these Tetris shapes player a crucial role.",
                    "label": 0
                },
                {
                    "sent": "The transformation actually.",
                    "label": 0
                },
                {
                    "sent": "So the computations go along these arrows starting from over there, and in particular, the reason that we had this special form of fast Fourier transform confined to just a few components.",
                    "label": 0
                },
                {
                    "sent": "If you start with that thing over there with three boxes.",
                    "label": 0
                },
                {
                    "sent": "Then if you arrow is just from there, you can only get to these top four shapes.",
                    "label": 0
                },
                {
                    "sent": "OK, so please come to the poster if you want to hear more about.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Young diagrams and tableau and so on.",
                    "label": 0
                },
                {
                    "sent": "Well, I can tell you is that there is software out there.",
                    "label": 0
                },
                {
                    "sent": "This is downloadable from my web page in which you can hack into it's in C++ and nicely object oriented and so on.",
                    "label": 0
                },
                {
                    "sent": "It's called snob for that reason, which does this for you, and it does it for you really fast and also put in specialized software which just does it specifically for graphs and runs on these medium sized graphs and say under.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2nd just input the adjacency matrix outcomes.",
                    "label": 0
                },
                {
                    "sent": "The invariants OK, so we've got 49 graph invariants.",
                    "label": 1
                },
                {
                    "sent": "Can compute them in Ncube.",
                    "label": 0
                },
                {
                    "sent": "Time progression is whether they're any good, right?",
                    "label": 0
                },
                {
                    "sent": "So can 49 invariants be possibly sufficient?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve machine learning tasks.",
                    "label": 0
                },
                {
                    "sent": "And the surprising result is that for the sort of task that people are interested in, well, maybe yes, right?",
                    "label": 0
                },
                {
                    "sent": "So we took these datasets of organic compounds as advertised at the beginning, and we compare to kind of the state of the art in graph kernels.",
                    "label": 0
                },
                {
                    "sent": "So the random walk kernels which are popular, the shortest path kernel which seems to be the best at the moment.",
                    "label": 1
                },
                {
                    "sent": "And when they turned out that our method with new tunable parameters just straight out of the box in three out of four cases, actually beats all the others.",
                    "label": 0
                },
                {
                    "sent": "In this enzyme case, it's a slightly worse than the shortest path or not, so I'm hoping that this is something that people will use in practice and you just plug in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrices, alchemy invariants, and you can use it in your machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to recap, I'm sorry for running overtime slightly.",
                    "label": 0
                },
                {
                    "sent": "I presented a general method of finding invariants to the action of a group with the specific application of graph invariants.",
                    "label": 1
                },
                {
                    "sent": "We only got 49 invariants out, but somehow surprisingly, they seem to be quite powerful at distinguishing between different graphs.",
                    "label": 0
                },
                {
                    "sent": "In fact, I should say that I tried it out for all possible graphs of small sizes, like 567 vertices, and there are very few coincidences, so it's almost good enough to solve the question of, you know whether two graphs are either the same or not, it can be.",
                    "label": 0
                },
                {
                    "sent": "Really distinguish between graphs.",
                    "label": 0
                },
                {
                    "sent": "They are very fast to compute.",
                    "label": 1
                },
                {
                    "sent": "You only need to compute them once and then you plug the computer features in your algorithm so it's linear in the number of learning examples.",
                    "label": 0
                },
                {
                    "sent": "And of course the next thing one that we want to do is to generalize it to labeled graphs, because often people are interested in that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But actually when it comes to organic compounds, you know you only have 4 atoms.",
                    "label": 0
                },
                {
                    "sent": "I mean sometimes you have an extra which which are like iron or something which is crucial, but if you only have carbon, oxygen, nitrogen and hydrogen then the bonds themselves can give away the identity of the of the.",
                    "label": 0
                },
                {
                    "sent": "The atoms, right?",
                    "label": 0
                },
                {
                    "sent": "So it's so labels in this case, or maybe not such a crucial issue as if they might appear at first sight.",
                    "label": 0
                },
                {
                    "sent": "Great, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Happy to field any questions.",
                    "label": 0
                },
                {
                    "sent": "So we've given it.",
                    "label": 0
                },
                {
                    "sent": "Underground.",
                    "label": 0
                },
                {
                    "sent": "Present your results.",
                    "label": 0
                },
                {
                    "sent": "Do you have?",
                    "label": 0
                },
                {
                    "sent": "General enough to deal with.",
                    "label": 0
                },
                {
                    "sent": "So this deals with.",
                    "label": 0
                },
                {
                    "sent": "I should have emphasized this more.",
                    "label": 0
                },
                {
                    "sent": "This deals with directed weighted graphs.",
                    "label": 0
                },
                {
                    "sent": "It does not deal with what I call labeled graphs.",
                    "label": 0
                },
                {
                    "sent": "Are you calling for them?",
                    "label": 0
                },
                {
                    "sent": "Attributed graphs, but we are busy working on extending it to that case.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of ways in which you could do that is not trivial, but the Canonical way of doing that, but obviously from the point of view of applications that's important at the moment, is just for just the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "We'll see.",
                    "label": 0
                },
                {
                    "sent": "Relocation usually work on being by possible set.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Drafted.",
                    "label": 0
                },
                {
                    "sent": "So couldn't you actually use them?",
                    "label": 0
                },
                {
                    "sent": "Store.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Virus.",
                    "label": 0
                },
                {
                    "sent": "I've.",
                    "label": 0
                },
                {
                    "sent": "The trouble is that these are giving is kind of holistic features, and they're difficult to interpret, so it's difficult to say how to fiddle with them.",
                    "label": 0
                },
                {
                    "sent": "If you want to tailor them particular types, graphs, some things you can do this in the Fourier spectrum.",
                    "label": 0
                },
                {
                    "sent": "Remember these you have these.",
                    "label": 0
                },
                {
                    "sent": "Seven nonzero columns in total for the last one disappears.",
                    "label": 0
                },
                {
                    "sent": "For undirected graphs, for example.",
                    "label": 0
                },
                {
                    "sent": "So you can make your life that much easier if you know that everything is going to be undirected, but otherwise just looking at this for transfers and interesting question, it's not figuring out what the relationship between the graph and the Fourier transform is.",
                    "label": 0
                },
                {
                    "sent": "Thank God speakers.",
                    "label": 0
                }
            ]
        }
    }
}