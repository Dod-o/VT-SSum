{
    "id": "fhggsqplknz6xufqrpipqzu3lqpwf74r",
    "title": "Pure Spreading Activation is Pointless",
    "info": {
        "author": [
            "Kilian Thiel, University of Konstanz"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_berthold_psap/",
    "segmentation": [
        [
            "OK, so this is about pure spreading.",
            "Activation invites pointless.",
            "First I give."
        ],
        [
            "A short introduction into spreading activation and explain the underlying framework we used.",
            "Then based on this underlying framework, I show why spreading activation is pointless because of the query independency of the results.",
            "And then I show three methods we invited to avoid query independency.",
            "I'll say something about an application we applied on the information retrieval data set.",
            "And yeah.",
            "Explain the results and then finally I concluded and give a small outlook.",
            "Spreading activation basic."
        ],
        [
            "Is used to query graphs to find interesting units of information which are nodes and relations between such units, which are edges and to rank these units and edges according to their degree of activation."
        ],
        [
            "It is an iterative process and initially query nodes or nodes that represent the query are activated so we can see a graph.",
            "Here a small graph on the right.",
            "One note is activated, which represents the query.",
            "Then in each iteration the activation is spread across along incident edges to adjacent nodes and these nodes will be activated as well according to the activation function and in the next iteration these nodes will spread the activation on and on and this iterative process goes on and until the process converges or certain constraints are reached.",
            "For example, a maximum number of activate.",
            "Note this reach or a maximum number of iterations, that these are constrained.",
            "So here I'm talking about pure spreading activation, which is constrained less.",
            "Finally, the activated nodes and edges can be returned as results and can be ranked by their activation.",
            "He"
        ],
        [
            "I show the framework.",
            "We used.",
            "This framework is split into three parts.",
            "The input activation and output part, which is very closely related to neural networks.",
            "Each part consists of a function and a state, and the function defines the transition into a state.",
            "I start with the input section.",
            "The input section combines the outgoing activation of a descent nodes to an incoming activation.",
            "The activation function determine or the activation section or function determines whether a node is activated or not and to which degree is it it is activated and only activated nodes can spread their activation in the next iteration.",
            "Here, non linearity can be added, for example, by threshold or a sequel.",
            "It functions and then we have the last section which is the output section.",
            "Here it is determined the.",
            "Amount of outgoing activation.",
            "For example, normalization can be applied to normalize the spread in the whole network.",
            "OK, we used."
        ],
        [
            "A very simple linear framework to show that to show the query independency we have for activation and output function, the identity function and the input function is simply the weighted sum of the outgoing activation of idea sent nodes.",
            "We can simplify this and skip the input and output function and just have a look or express it by the activation and then we can see that the activation of a certain node V at time T. Is based on the activation of the nodes of the neighboring nodes U at time T -- 1 and it is the weighted sum over all these activations.",
            "We can write this in matrix vector multiplication.",
            "So we have activation vector.",
            "We have the activation vector.",
            "And here.",
            "A representing or consisting of all the activations of all nodes at a certain time T and this is based on the activation of the of the iteration T -- 1.",
            "We can multiply this vector with the adjacency matrix W and then we have the new activations in the new iteration.",
            "We can have also a normalized version which can be seen here and here.",
            "And we can substitute this.",
            "This formula so that we can get a power iteration on the adjacency matrix W and this leads us."
        ],
        [
            "To the convergence theorem of parent Frobenius.",
            "Some conditions the activation vector A at a certain time T converges against the principle eigenvector according to the eigenvalue or adding value with the maximum absolute value of the adjacency matrix W for increasing tease.",
            "The order of the Notes of the sort or do too.",
            "The sorting of the notes by the activation.",
            "The order of these nodes is depends on the direction of the activation vector T, and we have seen that this direction is query independent because it's convergence to the principle eigenvector and therefore it's not really suitable to answer queries because all we get is each query we ask is kind of a global result, yeah?",
            "So, uh, Nick mechanism needs to be found in which the convergence to one fixed point can be avoided, and.",
            "Which are convergence to a query dependent fixed point can be induced therefore.",
            "We invited 3."
        ],
        [
            "Methods the accumulation, activation, renewing, and inertia.",
            "The accumulation is simply accumulation of each activation vector with a certain decay function.",
            "The accumulation renewing or the activation renewing is based on the.",
            "On their repeated compilation of the query vector of the query activation and finally inertia, which in which the activation of the last iteration is, is accumulated to the spreading result.",
            "So first they accumulate."
        ],
        [
            "Mission.",
            "We see that the.",
            "Activation results of iteration here are accumulated with a certain decay Lambda, and if this decay.",
            "Fits to some conditions then and convergence can be assured according to cuts and if this convergence is assured, we can define a closed form as we can see here.",
            "Then we get a final result and this final result.",
            "This final activation result A is query dependent now.",
            "Yeah, the the decay factor controls the impact of the convergence controls.",
            "The impact of the of each additional iteration, so we can.",
            "We can slide between a more local or more global result depending on the decay factor.",
            "So this is the first method."
        ],
        [
            "The next method is the activation renewing method.",
            "Here we add the initial activation the query.",
            "In each step here.",
            "And this leads us to activation vector for each time T. Which is based on this formula here.",
            "And we can see if we apply the same convergence behavior or it has to be applied if you want to assure convergence.",
            "This can only be assured if the if the principle I invert of the adjacency matrix W of the graph is smaller than one.",
            "Because here we have a deep.",
            "We have no decay, so this is basically a decay of 1.",
            "So the eigenvalue of the of the adjacency matrix, the principle needs to be smaller than one.",
            "OK, so here the convergence is not assured for all networks.",
            "Yeah, which is.",
            "Kind of problematic cause since we cannot ensure for networks that they have principali invert smaller than one, we cannot ensure the convergence for this method.",
            "OK, the the certain."
        ],
        [
            "What is inertia here?",
            "The activation of the last iteration is conserved and edit in each step as we can see here.",
            "Here this can be reformulated.",
            "And.",
            "To this expression, and here we can see that it's very similar to spreading activation on the adjacency matrix W But with self loops because the identity matrix I.",
            "Is added.",
            "To the adjacency matrix W here.",
            "Again, these results can be accumulated, and if a certain if certain decay is applied and this decay fits certain conditions, then we can ensure.",
            "Convergence."
        ],
        [
            "OK, so now to the example to confirm our convergence behavior and.",
            "See that spreading activation of you spreading activation is pointless.",
            "We applied the spreading activation on an information retrieval application.",
            "We use the time and meta data set of the smart test collection and we build a BI party document term graph out of this.",
            "The datasets with the terms on the one partition and documents on the other partition.",
            "We waited the edges with TF IDF weights and if we take a look at the adjacency matrix W of this graph, we can see that each document is represented by a document vector in the term space, and each term is represented as a term vector in the document space.",
            "Um?",
            "Since the cosine similarity is a very."
        ],
        [
            "Popular similarity measure and information retrieval.",
            "We modified our spreading activation framework in a way that the activation will fit or will equal the this cosine similarity so I can explain this better on this example here down there you can see a small bipartite document term graph with three documents and six terms.",
            "We have one query applied to this graph.",
            "The query consists of only one term which is T1 and this term is activated.",
            "We can think now of a document vector with all zeros and only one one at the index of term one.",
            "So this vector this query vector represents a virtual document.",
            "Now we spread the activation of term one across the incident edges.",
            "Two, the idea sent note D1 and this node is activated as well.",
            "As I've said before and the activation.",
            "Is equal to the cosine between this document and the virtual query document.",
            "This is the way we build up our activation framework or spreading activation framework.",
            "Now, once the document vector."
        ],
        [
            "Or the document node D1 is activated.",
            "It can spread the activation on and on and we have this iterative process until convergency.",
            "So let's have a look at the results.",
            "First, the results of pure spreading activation.",
            "We measured the precision and recall values and precision have been measured at 11 recall values and have been averaged overall.",
            "Overall, queries on the left we can see the results for the meta data set on the right for the time data set on the Y axis.",
            "You can see the precision and on the X axis you can see the recall.",
            "You can see the continuous line which represents the precision recall results for the.",
            "Simple cosine measure, which is also the result of the first iteration and then the dashed line represents the second iteration.",
            "The dotted line, the third iteration, and so on.",
            "And we cannot distinguish between the 10th and the 100 iteration here.",
            "Becauses after the 10th iteration of the algorithm already converged.",
            "So the result is the same.",
            "We can see that the precision values are very low and this means that this global answer to all these queries are not reasonable.",
            "In order to answer them.",
            "So this fixed point.",
            "This global fixed point is kind of an inadequate result.",
            "In order to answer."
        ],
        [
            "Queries.",
            "No.",
            "We can take a look at the results of the iteration accumulation with the decay of 0.9, again on the left, we can see the results for the meta data set and on the right of the time data set and we can see that the convergence to the fixed point or that the query dependent fixed points now are much more reasonable.",
            "So in contrast to the query independent fixed points becausw the average precision overall queries is much better.",
            "As we can see it here.",
            "In contrast to the.",
            "It's not working.",
            "What's wrong with the computer?",
            "Yeah it's yeah it's true and not the most but.",
            "OK. OK, once again we can see that.",
            "The average precision about across all queries here is much better than.",
            "At the pure iterations without accumulation becausw, here we have one global answer for all queries and there we have a query dependent answer for each query.",
            "So of course the precision is better."
        ],
        [
            "Um?",
            "To conclude this, I showed or we show that the pure linear spreading activation converges to a query independent fixed point, which is not reasonable to answer queries.",
            "So approaches need to be applied in order to avoid query independency which on the one hand can be constrained.",
            "But these are hard to analyze and on the other hand the accumulation of iteration results which we have shown here.",
            "Additionally, the accumulation function or the decay value regulates the global and local characteristic of the final results."
        ],
        [
            "And to give a small outlook on the according to the Bison Project, we want to apply the spreading activation on bison Nets in order to find possible by associations related to a certain topic in our local environment.",
            "To find relevant information in order to answer a query which is more a local answer or to find interesting information to support creativity, which can be a more global answer.",
            "Since the bison Nets are based on partition graphs, we want to.",
            "Use these partition graphs in order to define input activation and output function for each partition and to specify decay value or a DK function for each partition separately.",
            "And Additionally it needs to be analyzed in which way the certain spreading activation frameworks can be defined in order to find by associations or to optimize what we want to find.",
            "We can, for example apply nonlinear activation functions which have other convergence behavior or more fixed points, or we can change the breadth first search to a depth first search.",
            "Yeah, this would be an option too, so that's it from my side."
        ],
        [
            "Question.",
            "1515 OK. Basically shows that propagate at all.",
            "Um, basically.",
            "Position once you started.",
            "Actually, I'm not really surprised by this, cause I I'm not sure.",
            "Maybe it's because of the huge OK so so perhaps I have to make myself clear that we don't want to find another algorithm which with which we can get better precision or recall values.",
            "We want to show that the pure spreading activation without constraints and without accumulation techniques is.",
            "Really not reasonable and so.",
            "What you achieve by adding this?",
            "Yeah, in this case the accumulated iteration.",
            "That you stabilize the system at the.",
            "Initial.",
            "PowerPoint.",
            "You create an artificial fixed point in the system which defines basically the query.",
            "No, not really, becausw.",
            "The accumulation expresses kind of a geometric series, and this induces a query dependent fixed point.",
            "But it's not where it's.",
            "Yeah, it's clearly related or it's query dependent, and so it's much more reasonable to use a method like this.",
            "OK, yeah.",
            "Perhaps at this chart, but here.",
            "Activation vector after the first iteration is roughly the same as the activation vector after the wars or even tense.",
            "Iteration, so the question is, do you actually get away much from the query?",
            "I don't know whether you can see this from this up.",
            "Explained by not having exactly the same activity.",
            "Different, you just get the same recall precision values, but somehow it's possible that you stay very close to the initial query and that may not be quite what you want, so perhaps this.",
            "This plot here is not very informative.",
            "We have other plots, but unfortunately I haven't them in in our presentation.",
            "Now here you can see it better that the 2nd and 3rd and 4th iteration really differs and this is only a precision recall plot so we cannot see the activation vector here we made also.",
            "MDF scaled.",
            "Lot of these activation vectors and there you can see it much better, but unfortunately I have them at in this slide.",
            "So with the documents which are in the different retreats, sets in the correct enrollments Saints table and set of document so.",
            "Basically this is a set of documents retrieved with the first iterations of same.",
            "Set of documents with same randomness as with the second certain force infected know the set of documents differs and a lot more documents came into the retrieved set with the 2nd third iteration.",
            "Because of the of the spread of course.",
            "So the the precision and recall measured.",
            "Was based on the.",
            "The continuous line is the cosine similarity between the query document and all other documents.",
            "So we have all other documents in our result set ordered by a certain order or rank by a certain order.",
            "And now with the 2nd third iteration the order will change and so the precision and recall not the recall but the precision at certain recall values will change.",
            "So we always get the whole set of documents as a result set, but the order is different in different iterations.",
            "And the first iteration is the cosine, so.",
            "Some documents will be ranked with zero.",
            "And.",
            "Yeah.",
            "Has to think about before one really gets worse.",
            "Because I also have trouble now, but I understand how it's computed, but I would have to think about what it actually mean.",
            "Every ranking effect with you?",
            "Yeah, that's yeah.",
            "So yeah, as I said, unfortunately I haven't the MD's plot and.",
            "SIM card.",
            "Very small number of iterations.",
            "Something about the relationship between what happens about after a small number of iterations to what happened to leave, maybe after small other iterations.",
            "You do have pre dependence and it's only the limits, but it goes away.",
            "If you don't use the accumulation, for example.",
            "You have query dependent.",
            "Yes, you have query depends when you.",
            "Yeah, when you stop after.",
            "When you stop after a few numbers of iteration, you do have a query dependence.",
            "In contrast, if you go along till it converges, but it's difficult to determine the exact number of iterations, when do you stop?",
            "So it depends on the data set so.",
            "We we decided to go to convergence and then find reasonable answers or reasonable results there.",
            "I think you can demonstrate that for just a little bit more.",
            "Actually pretty quick that you get very close here, yeah?",
            "So this.",
            "This is the the tense in the hundreds iterations, and this is the 4th iteration, so.",
            "That piece of fence in the 100 situations are closed.",
            "It will not suddenly go away from these 10,000 yeah.",
            "Wait?",
            "Question.",
            "Sure that going.",
            "Since we want to analyze this in a mathematical way, we yeah we wanted to go to the fixed point and determine if the fixed point is reasonable or not, and so we decided for the accumulation strategies in the future I think.",
            "Perhaps going to the fixed point is not necessary or not reasonable.",
            "Also, the breadth first search which have been applied here is perhaps not the best strategy and depth first search is preferable so only to activate only one node at iteration, but I cannot say anything to this because we haven't tried.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is about pure spreading.",
                    "label": 1
                },
                {
                    "sent": "Activation invites pointless.",
                    "label": 0
                },
                {
                    "sent": "First I give.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A short introduction into spreading activation and explain the underlying framework we used.",
                    "label": 1
                },
                {
                    "sent": "Then based on this underlying framework, I show why spreading activation is pointless because of the query independency of the results.",
                    "label": 0
                },
                {
                    "sent": "And then I show three methods we invited to avoid query independency.",
                    "label": 0
                },
                {
                    "sent": "I'll say something about an application we applied on the information retrieval data set.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                },
                {
                    "sent": "Explain the results and then finally I concluded and give a small outlook.",
                    "label": 0
                },
                {
                    "sent": "Spreading activation basic.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is used to query graphs to find interesting units of information which are nodes and relations between such units, which are edges and to rank these units and edges according to their degree of activation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is an iterative process and initially query nodes or nodes that represent the query are activated so we can see a graph.",
                    "label": 0
                },
                {
                    "sent": "Here a small graph on the right.",
                    "label": 0
                },
                {
                    "sent": "One note is activated, which represents the query.",
                    "label": 0
                },
                {
                    "sent": "Then in each iteration the activation is spread across along incident edges to adjacent nodes and these nodes will be activated as well according to the activation function and in the next iteration these nodes will spread the activation on and on and this iterative process goes on and until the process converges or certain constraints are reached.",
                    "label": 0
                },
                {
                    "sent": "For example, a maximum number of activate.",
                    "label": 1
                },
                {
                    "sent": "Note this reach or a maximum number of iterations, that these are constrained.",
                    "label": 1
                },
                {
                    "sent": "So here I'm talking about pure spreading activation, which is constrained less.",
                    "label": 0
                },
                {
                    "sent": "Finally, the activated nodes and edges can be returned as results and can be ranked by their activation.",
                    "label": 1
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I show the framework.",
                    "label": 0
                },
                {
                    "sent": "We used.",
                    "label": 0
                },
                {
                    "sent": "This framework is split into three parts.",
                    "label": 0
                },
                {
                    "sent": "The input activation and output part, which is very closely related to neural networks.",
                    "label": 1
                },
                {
                    "sent": "Each part consists of a function and a state, and the function defines the transition into a state.",
                    "label": 0
                },
                {
                    "sent": "I start with the input section.",
                    "label": 0
                },
                {
                    "sent": "The input section combines the outgoing activation of a descent nodes to an incoming activation.",
                    "label": 1
                },
                {
                    "sent": "The activation function determine or the activation section or function determines whether a node is activated or not and to which degree is it it is activated and only activated nodes can spread their activation in the next iteration.",
                    "label": 0
                },
                {
                    "sent": "Here, non linearity can be added, for example, by threshold or a sequel.",
                    "label": 0
                },
                {
                    "sent": "It functions and then we have the last section which is the output section.",
                    "label": 0
                },
                {
                    "sent": "Here it is determined the.",
                    "label": 0
                },
                {
                    "sent": "Amount of outgoing activation.",
                    "label": 0
                },
                {
                    "sent": "For example, normalization can be applied to normalize the spread in the whole network.",
                    "label": 0
                },
                {
                    "sent": "OK, we used.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A very simple linear framework to show that to show the query independency we have for activation and output function, the identity function and the input function is simply the weighted sum of the outgoing activation of idea sent nodes.",
                    "label": 0
                },
                {
                    "sent": "We can simplify this and skip the input and output function and just have a look or express it by the activation and then we can see that the activation of a certain node V at time T. Is based on the activation of the nodes of the neighboring nodes U at time T -- 1 and it is the weighted sum over all these activations.",
                    "label": 0
                },
                {
                    "sent": "We can write this in matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "So we have activation vector.",
                    "label": 0
                },
                {
                    "sent": "We have the activation vector.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "A representing or consisting of all the activations of all nodes at a certain time T and this is based on the activation of the of the iteration T -- 1.",
                    "label": 0
                },
                {
                    "sent": "We can multiply this vector with the adjacency matrix W and then we have the new activations in the new iteration.",
                    "label": 1
                },
                {
                    "sent": "We can have also a normalized version which can be seen here and here.",
                    "label": 0
                },
                {
                    "sent": "And we can substitute this.",
                    "label": 0
                },
                {
                    "sent": "This formula so that we can get a power iteration on the adjacency matrix W and this leads us.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the convergence theorem of parent Frobenius.",
                    "label": 0
                },
                {
                    "sent": "Some conditions the activation vector A at a certain time T converges against the principle eigenvector according to the eigenvalue or adding value with the maximum absolute value of the adjacency matrix W for increasing tease.",
                    "label": 1
                },
                {
                    "sent": "The order of the Notes of the sort or do too.",
                    "label": 0
                },
                {
                    "sent": "The sorting of the notes by the activation.",
                    "label": 1
                },
                {
                    "sent": "The order of these nodes is depends on the direction of the activation vector T, and we have seen that this direction is query independent because it's convergence to the principle eigenvector and therefore it's not really suitable to answer queries because all we get is each query we ask is kind of a global result, yeah?",
                    "label": 0
                },
                {
                    "sent": "So, uh, Nick mechanism needs to be found in which the convergence to one fixed point can be avoided, and.",
                    "label": 0
                },
                {
                    "sent": "Which are convergence to a query dependent fixed point can be induced therefore.",
                    "label": 0
                },
                {
                    "sent": "We invited 3.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods the accumulation, activation, renewing, and inertia.",
                    "label": 1
                },
                {
                    "sent": "The accumulation is simply accumulation of each activation vector with a certain decay function.",
                    "label": 0
                },
                {
                    "sent": "The accumulation renewing or the activation renewing is based on the.",
                    "label": 0
                },
                {
                    "sent": "On their repeated compilation of the query vector of the query activation and finally inertia, which in which the activation of the last iteration is, is accumulated to the spreading result.",
                    "label": 0
                },
                {
                    "sent": "So first they accumulate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "We see that the.",
                    "label": 0
                },
                {
                    "sent": "Activation results of iteration here are accumulated with a certain decay Lambda, and if this decay.",
                    "label": 1
                },
                {
                    "sent": "Fits to some conditions then and convergence can be assured according to cuts and if this convergence is assured, we can define a closed form as we can see here.",
                    "label": 0
                },
                {
                    "sent": "Then we get a final result and this final result.",
                    "label": 0
                },
                {
                    "sent": "This final activation result A is query dependent now.",
                    "label": 1
                },
                {
                    "sent": "Yeah, the the decay factor controls the impact of the convergence controls.",
                    "label": 1
                },
                {
                    "sent": "The impact of the of each additional iteration, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can slide between a more local or more global result depending on the decay factor.",
                    "label": 0
                },
                {
                    "sent": "So this is the first method.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next method is the activation renewing method.",
                    "label": 0
                },
                {
                    "sent": "Here we add the initial activation the query.",
                    "label": 1
                },
                {
                    "sent": "In each step here.",
                    "label": 0
                },
                {
                    "sent": "And this leads us to activation vector for each time T. Which is based on this formula here.",
                    "label": 0
                },
                {
                    "sent": "And we can see if we apply the same convergence behavior or it has to be applied if you want to assure convergence.",
                    "label": 0
                },
                {
                    "sent": "This can only be assured if the if the principle I invert of the adjacency matrix W of the graph is smaller than one.",
                    "label": 0
                },
                {
                    "sent": "Because here we have a deep.",
                    "label": 0
                },
                {
                    "sent": "We have no decay, so this is basically a decay of 1.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvalue of the of the adjacency matrix, the principle needs to be smaller than one.",
                    "label": 0
                },
                {
                    "sent": "OK, so here the convergence is not assured for all networks.",
                    "label": 1
                },
                {
                    "sent": "Yeah, which is.",
                    "label": 0
                },
                {
                    "sent": "Kind of problematic cause since we cannot ensure for networks that they have principali invert smaller than one, we cannot ensure the convergence for this method.",
                    "label": 0
                },
                {
                    "sent": "OK, the the certain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is inertia here?",
                    "label": 0
                },
                {
                    "sent": "The activation of the last iteration is conserved and edit in each step as we can see here.",
                    "label": 1
                },
                {
                    "sent": "Here this can be reformulated.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To this expression, and here we can see that it's very similar to spreading activation on the adjacency matrix W But with self loops because the identity matrix I.",
                    "label": 1
                },
                {
                    "sent": "Is added.",
                    "label": 0
                },
                {
                    "sent": "To the adjacency matrix W here.",
                    "label": 0
                },
                {
                    "sent": "Again, these results can be accumulated, and if a certain if certain decay is applied and this decay fits certain conditions, then we can ensure.",
                    "label": 0
                },
                {
                    "sent": "Convergence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now to the example to confirm our convergence behavior and.",
                    "label": 1
                },
                {
                    "sent": "See that spreading activation of you spreading activation is pointless.",
                    "label": 0
                },
                {
                    "sent": "We applied the spreading activation on an information retrieval application.",
                    "label": 1
                },
                {
                    "sent": "We use the time and meta data set of the smart test collection and we build a BI party document term graph out of this.",
                    "label": 1
                },
                {
                    "sent": "The datasets with the terms on the one partition and documents on the other partition.",
                    "label": 0
                },
                {
                    "sent": "We waited the edges with TF IDF weights and if we take a look at the adjacency matrix W of this graph, we can see that each document is represented by a document vector in the term space, and each term is represented as a term vector in the document space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Since the cosine similarity is a very.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Popular similarity measure and information retrieval.",
                    "label": 1
                },
                {
                    "sent": "We modified our spreading activation framework in a way that the activation will fit or will equal the this cosine similarity so I can explain this better on this example here down there you can see a small bipartite document term graph with three documents and six terms.",
                    "label": 0
                },
                {
                    "sent": "We have one query applied to this graph.",
                    "label": 0
                },
                {
                    "sent": "The query consists of only one term which is T1 and this term is activated.",
                    "label": 1
                },
                {
                    "sent": "We can think now of a document vector with all zeros and only one one at the index of term one.",
                    "label": 0
                },
                {
                    "sent": "So this vector this query vector represents a virtual document.",
                    "label": 1
                },
                {
                    "sent": "Now we spread the activation of term one across the incident edges.",
                    "label": 0
                },
                {
                    "sent": "Two, the idea sent note D1 and this node is activated as well.",
                    "label": 0
                },
                {
                    "sent": "As I've said before and the activation.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the cosine between this document and the virtual query document.",
                    "label": 1
                },
                {
                    "sent": "This is the way we build up our activation framework or spreading activation framework.",
                    "label": 0
                },
                {
                    "sent": "Now, once the document vector.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or the document node D1 is activated.",
                    "label": 0
                },
                {
                    "sent": "It can spread the activation on and on and we have this iterative process until convergency.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look at the results.",
                    "label": 0
                },
                {
                    "sent": "First, the results of pure spreading activation.",
                    "label": 1
                },
                {
                    "sent": "We measured the precision and recall values and precision have been measured at 11 recall values and have been averaged overall.",
                    "label": 0
                },
                {
                    "sent": "Overall, queries on the left we can see the results for the meta data set on the right for the time data set on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "You can see the precision and on the X axis you can see the recall.",
                    "label": 0
                },
                {
                    "sent": "You can see the continuous line which represents the precision recall results for the.",
                    "label": 0
                },
                {
                    "sent": "Simple cosine measure, which is also the result of the first iteration and then the dashed line represents the second iteration.",
                    "label": 0
                },
                {
                    "sent": "The dotted line, the third iteration, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we cannot distinguish between the 10th and the 100 iteration here.",
                    "label": 0
                },
                {
                    "sent": "Becauses after the 10th iteration of the algorithm already converged.",
                    "label": 0
                },
                {
                    "sent": "So the result is the same.",
                    "label": 0
                },
                {
                    "sent": "We can see that the precision values are very low and this means that this global answer to all these queries are not reasonable.",
                    "label": 0
                },
                {
                    "sent": "In order to answer them.",
                    "label": 0
                },
                {
                    "sent": "So this fixed point.",
                    "label": 0
                },
                {
                    "sent": "This global fixed point is kind of an inadequate result.",
                    "label": 1
                },
                {
                    "sent": "In order to answer.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Queries.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "We can take a look at the results of the iteration accumulation with the decay of 0.9, again on the left, we can see the results for the meta data set and on the right of the time data set and we can see that the convergence to the fixed point or that the query dependent fixed points now are much more reasonable.",
                    "label": 0
                },
                {
                    "sent": "So in contrast to the query independent fixed points becausw the average precision overall queries is much better.",
                    "label": 1
                },
                {
                    "sent": "As we can see it here.",
                    "label": 0
                },
                {
                    "sent": "In contrast to the.",
                    "label": 0
                },
                {
                    "sent": "It's not working.",
                    "label": 0
                },
                {
                    "sent": "What's wrong with the computer?",
                    "label": 0
                },
                {
                    "sent": "Yeah it's yeah it's true and not the most but.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, once again we can see that.",
                    "label": 0
                },
                {
                    "sent": "The average precision about across all queries here is much better than.",
                    "label": 1
                },
                {
                    "sent": "At the pure iterations without accumulation becausw, here we have one global answer for all queries and there we have a query dependent answer for each query.",
                    "label": 0
                },
                {
                    "sent": "So of course the precision is better.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To conclude this, I showed or we show that the pure linear spreading activation converges to a query independent fixed point, which is not reasonable to answer queries.",
                    "label": 1
                },
                {
                    "sent": "So approaches need to be applied in order to avoid query independency which on the one hand can be constrained.",
                    "label": 1
                },
                {
                    "sent": "But these are hard to analyze and on the other hand the accumulation of iteration results which we have shown here.",
                    "label": 0
                },
                {
                    "sent": "Additionally, the accumulation function or the decay value regulates the global and local characteristic of the final results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to give a small outlook on the according to the Bison Project, we want to apply the spreading activation on bison Nets in order to find possible by associations related to a certain topic in our local environment.",
                    "label": 0
                },
                {
                    "sent": "To find relevant information in order to answer a query which is more a local answer or to find interesting information to support creativity, which can be a more global answer.",
                    "label": 1
                },
                {
                    "sent": "Since the bison Nets are based on partition graphs, we want to.",
                    "label": 1
                },
                {
                    "sent": "Use these partition graphs in order to define input activation and output function for each partition and to specify decay value or a DK function for each partition separately.",
                    "label": 0
                },
                {
                    "sent": "And Additionally it needs to be analyzed in which way the certain spreading activation frameworks can be defined in order to find by associations or to optimize what we want to find.",
                    "label": 0
                },
                {
                    "sent": "We can, for example apply nonlinear activation functions which have other convergence behavior or more fixed points, or we can change the breadth first search to a depth first search.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this would be an option too, so that's it from my side.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "1515 OK. Basically shows that propagate at all.",
                    "label": 0
                },
                {
                    "sent": "Um, basically.",
                    "label": 0
                },
                {
                    "sent": "Position once you started.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm not really surprised by this, cause I I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's because of the huge OK so so perhaps I have to make myself clear that we don't want to find another algorithm which with which we can get better precision or recall values.",
                    "label": 0
                },
                {
                    "sent": "We want to show that the pure spreading activation without constraints and without accumulation techniques is.",
                    "label": 1
                },
                {
                    "sent": "Really not reasonable and so.",
                    "label": 0
                },
                {
                    "sent": "What you achieve by adding this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this case the accumulated iteration.",
                    "label": 0
                },
                {
                    "sent": "That you stabilize the system at the.",
                    "label": 0
                },
                {
                    "sent": "Initial.",
                    "label": 0
                },
                {
                    "sent": "PowerPoint.",
                    "label": 0
                },
                {
                    "sent": "You create an artificial fixed point in the system which defines basically the query.",
                    "label": 0
                },
                {
                    "sent": "No, not really, becausw.",
                    "label": 0
                },
                {
                    "sent": "The accumulation expresses kind of a geometric series, and this induces a query dependent fixed point.",
                    "label": 0
                },
                {
                    "sent": "But it's not where it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's clearly related or it's query dependent, and so it's much more reasonable to use a method like this.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Perhaps at this chart, but here.",
                    "label": 0
                },
                {
                    "sent": "Activation vector after the first iteration is roughly the same as the activation vector after the wars or even tense.",
                    "label": 0
                },
                {
                    "sent": "Iteration, so the question is, do you actually get away much from the query?",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you can see this from this up.",
                    "label": 0
                },
                {
                    "sent": "Explained by not having exactly the same activity.",
                    "label": 0
                },
                {
                    "sent": "Different, you just get the same recall precision values, but somehow it's possible that you stay very close to the initial query and that may not be quite what you want, so perhaps this.",
                    "label": 0
                },
                {
                    "sent": "This plot here is not very informative.",
                    "label": 0
                },
                {
                    "sent": "We have other plots, but unfortunately I haven't them in in our presentation.",
                    "label": 0
                },
                {
                    "sent": "Now here you can see it better that the 2nd and 3rd and 4th iteration really differs and this is only a precision recall plot so we cannot see the activation vector here we made also.",
                    "label": 0
                },
                {
                    "sent": "MDF scaled.",
                    "label": 0
                },
                {
                    "sent": "Lot of these activation vectors and there you can see it much better, but unfortunately I have them at in this slide.",
                    "label": 0
                },
                {
                    "sent": "So with the documents which are in the different retreats, sets in the correct enrollments Saints table and set of document so.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a set of documents retrieved with the first iterations of same.",
                    "label": 0
                },
                {
                    "sent": "Set of documents with same randomness as with the second certain force infected know the set of documents differs and a lot more documents came into the retrieved set with the 2nd third iteration.",
                    "label": 0
                },
                {
                    "sent": "Because of the of the spread of course.",
                    "label": 0
                },
                {
                    "sent": "So the the precision and recall measured.",
                    "label": 0
                },
                {
                    "sent": "Was based on the.",
                    "label": 0
                },
                {
                    "sent": "The continuous line is the cosine similarity between the query document and all other documents.",
                    "label": 0
                },
                {
                    "sent": "So we have all other documents in our result set ordered by a certain order or rank by a certain order.",
                    "label": 0
                },
                {
                    "sent": "And now with the 2nd third iteration the order will change and so the precision and recall not the recall but the precision at certain recall values will change.",
                    "label": 0
                },
                {
                    "sent": "So we always get the whole set of documents as a result set, but the order is different in different iterations.",
                    "label": 0
                },
                {
                    "sent": "And the first iteration is the cosine, so.",
                    "label": 0
                },
                {
                    "sent": "Some documents will be ranked with zero.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Has to think about before one really gets worse.",
                    "label": 0
                },
                {
                    "sent": "Because I also have trouble now, but I understand how it's computed, but I would have to think about what it actually mean.",
                    "label": 0
                },
                {
                    "sent": "Every ranking effect with you?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, as I said, unfortunately I haven't the MD's plot and.",
                    "label": 0
                },
                {
                    "sent": "SIM card.",
                    "label": 0
                },
                {
                    "sent": "Very small number of iterations.",
                    "label": 0
                },
                {
                    "sent": "Something about the relationship between what happens about after a small number of iterations to what happened to leave, maybe after small other iterations.",
                    "label": 0
                },
                {
                    "sent": "You do have pre dependence and it's only the limits, but it goes away.",
                    "label": 0
                },
                {
                    "sent": "If you don't use the accumulation, for example.",
                    "label": 0
                },
                {
                    "sent": "You have query dependent.",
                    "label": 0
                },
                {
                    "sent": "Yes, you have query depends when you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when you stop after.",
                    "label": 0
                },
                {
                    "sent": "When you stop after a few numbers of iteration, you do have a query dependence.",
                    "label": 0
                },
                {
                    "sent": "In contrast, if you go along till it converges, but it's difficult to determine the exact number of iterations, when do you stop?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the data set so.",
                    "label": 0
                },
                {
                    "sent": "We we decided to go to convergence and then find reasonable answers or reasonable results there.",
                    "label": 0
                },
                {
                    "sent": "I think you can demonstrate that for just a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Actually pretty quick that you get very close here, yeah?",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This is the the tense in the hundreds iterations, and this is the 4th iteration, so.",
                    "label": 0
                },
                {
                    "sent": "That piece of fence in the 100 situations are closed.",
                    "label": 0
                },
                {
                    "sent": "It will not suddenly go away from these 10,000 yeah.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Sure that going.",
                    "label": 0
                },
                {
                    "sent": "Since we want to analyze this in a mathematical way, we yeah we wanted to go to the fixed point and determine if the fixed point is reasonable or not, and so we decided for the accumulation strategies in the future I think.",
                    "label": 0
                },
                {
                    "sent": "Perhaps going to the fixed point is not necessary or not reasonable.",
                    "label": 0
                },
                {
                    "sent": "Also, the breadth first search which have been applied here is perhaps not the best strategy and depth first search is preferable so only to activate only one node at iteration, but I cannot say anything to this because we haven't tried.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}