{
    "id": "lonxejlcyvuyczkrkxwhojl2hemo4f67",
    "title": "A Bahadur Type Representation of the Linear Support Vector Machine and its Relative Efficiency",
    "info": {
        "author": [
            "Yoonkyung Lee, Department of Statistics, Ohio State University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/mlss09us_lee_btrlsvm/",
    "segmentation": [
        [
            "So can you hear me in the back?",
            "Alright.",
            "So today I will talk about.",
            "Powder type representation of linear support vector machine and some statistical properties that we can infer from the representation.",
            "So, generally speaking or Bahadur representation of statistical estimator means some sort of stochastic approximation of the estimator as an average or sum of IID random variables.",
            "So once we have such a representation, you can easily analyze the estimator.",
            "Loblaws numbers.",
            "Well, speaking on this topic here at University of Chicago, I'd like to mention that.",
            "The term powder representation originates from such a representation for sample quantiles proposed by padrew in 1960s, and he was a faculty member here in the Department of Statistics.",
            "Before he passed away."
        ],
        [
            "Alright, so here is an outline of my talk.",
            "I just arrived last night so I didn't really hear other talks before, but I'm pretty sure that many speakers before me talked about support vector machine, so I will give you a very quick review of support vector machine and briefly describe some statistical properties that people have studied and then I will present the main results.",
            "Of synthetic analysis of linear support vector machine through bahadue representation of the coefficient vector of linear support vector machine an, I will show you.",
            "An example to illustrate the main results and then I will talk about some other implication of the result in comparing different classification procedures in terms of relative efficiency."
        ],
        [
            "Alright, so.",
            "Support vector machine has been used successfully in many real applications.",
            "Ranging from handwritten December cognition problem to bioinformatics.",
            "Problems say where you want to diagnose different disease types based on genetic profiles of patients.",
            "And these are examples of classification problems."
        ],
        [
            "So in more abstract sense, classification problem is given as follows.",
            "So in classification we have Sadie predictors, so D attributes denoted by X Ann.",
            "We have class label Y which is one of K labels and in this talk I will talk about mostly binary classification problem where case too.",
            "And given training data, set up an pairs of XIYI, we want to find some rule fee which is a mapping from D dimensional input space to class label space, which can be generalized to unseen cases.",
            "And typically people use the 01 loss function to assess in accuracy of classification or classification goofy, which is simply indicator of misclassification."
        ],
        [
            "And now it is well known that support vector machine is an example of regularization method or penalization method.",
            "So in general regularization method can be formulated as a procedure of finding some function unknown function F in a candidate function space script F that minimizes this penalized empirical risk.",
            "So here we have.",
            "All that is lost to measure goodness of fit of F an over there jail web measures sort of model complexity of F. And here Lambda is a tuning parameter that balances.",
            "The data fit measured by empirical risk and the model complexity, and if you choose the function space script have to be very large.",
            "Then without this penalty functionalty above, the optimization problem becomes ill posed.",
            "Hi."
        ],
        [
            "So here is very brief introduction to linear support vector machine.",
            "So the linear support vector machine looks for the hyperplane that maximizes margin margin between two classes.",
            "Here the margin is mathematically given as two over the norm of W when you have linear discriminant function of this form.",
            "So you have normal vector W, an intercept be an classification boundaries given by the zero level set.",
            "All this linear discriminant function and positive and negative margins are given by 1 N minus one level sets.",
            "So here I'm considering class labels to be either one or minus one.",
            "So I'm going to use passive class or negative class to report to those two classes.",
            "K so maximizing margin is now equivalent to, say, minimizing squared norm of the W normal vector."
        ],
        [
            "And more generally, when we allow two classes to have some overlap, we end up with this formulation of linear support vector machine.",
            "So linear support vector machine is given as a solution to this optimization problem.",
            "Again, we minimize.",
            "We want to find linear discriminant function with normal vector W an intercept B, minimizing this penalized empirical risk.",
            "And here we are using most function so called hinge loss.",
            "And as you can see here the penalty function is given as squared normal the normal vector WK and once we have real value discriminant function, the classification rule is given by this.",
            "So we simply take the sign up at Publix."
        ],
        [
            "Alright, and this plot shows the relationship between the hinge loss an the misclassification loss that I mentioned before.",
            "So these are functions of the product top class label, which is either one or minus one, and the value of linear discriminant function F of X.",
            "And of course, when the class label and the sign of FX match, we don't have any loss.",
            "So we have 0 otherwise one, so there's so little line is.",
            "This classification loss and the dashed line is the hinge loss.",
            "So clearly from this picture you can see that the hinge loss is convex upper bound of misclassification loss and you notice here that this hinge loss has singularity at one."
        ],
        [
            "More generally, we could consider extension from linear support vector machine to nonlinear case.",
            "So the support vector machine can be cast as a regularization problem regularization method in a reproducing kernel Hilbert space.",
            "So here we want to find function of this form, minimizing the same penalized.",
            "Average hinge loss an you can notice here that now we replace the squared normal W with appropriate squared norm of this H, which is part of this at Publix.",
            "Came and since there is 1 to one correspondence between reproducing kernel Hilbert space and the kernel function for nonlinear generalization, it is sufficient to choose a nonlinear kernel function.",
            "So you could choose Gaussian kernel or polynomial kernel to get nonlinear support vector machine."
        ],
        [
            "Alright, so let me briefly go over some statiscal proper properties that people have started.",
            "The first lien studied feature consistency of linear support vector machine which is very weak notion of consistency and result says that the population minimizer of hinge loss is simply sign of \u03c0 / X minus half here.",
            "X is the conditional probability of.",
            "Massive class given X and this result indicates that support vector machine aims to approximate the best classification, which is based decision model.",
            "Directly, without estimating this conditional probability.",
            "So which is different from other classical approaches to classification?",
            "And also down & word.",
            "Bartlett and others studied based consistency of support recognition and they showed that the risk of support vector machine.",
            "So this is the error rate of support vector.",
            "Motion converges to the Bayes error rate in probability under some universal approximation condition on the function space, or equivalently condition on the kernel.",
            "And Steinert and others also studied radar convergence of this error rate to the Bayes risk.",
            "So as you can see here, um, previously people have focused on.",
            "Risk consistency of support vector machine.",
            "But today I like talk about some other properties that people haven't really looked at, so.",
            "Here is motivation."
        ],
        [
            "So.",
            "Keanan others propose so-called recursive feature elimination, which is akin to backward elimination used in linear modeling for selection of variables, but they apply this idea to the fitted coefficients of linear support vector machine, not standardized coefficients, and this method brought some questions to us.",
            "For example, what is the status quo behavior of the estimated query visions?",
            "Through support vector mission and what really determines their variances?",
            "And these questions prompted us to study some more sympathetic properties of the coefficients of the linear support vector machine and related questions like relative efficiency of support vector machine compared to other more classical approaches.",
            "And it turns out that we can tackle these questions by."
        ],
        [
            "Following some wedding custom.",
            "So in this question is something knew yet something old and something to power 04.",
            "So what's new in this problem is the hinge loss that we gotta deal with, which is singularity at one as I mentioned.",
            "Because of this people get the support vector machine solution through quadratic programming, so we don't really have closed form expression for the solution.",
            "However, it is very useful links between support vector machine and some other estimators that we know very well.",
            "So the population minimizer sign appeared X minus half.",
            "With respect to the hinge loss, this is in fact the true median of class label given X, so in a way it is suggest that.",
            "Hinge loss is a lot like absolute deviation.",
            "Lots that we know very well about.",
            "So, for example, Pollard studied some of synthetics for least absolute deviation estimators, and it turns out that we can use his techniques to analyze linear support vector machine and the convexity of the loss is the key in the proof and hinge losses convex function."
        ],
        [
            "Alright, so let me define a few notations to state the main result, so I'm going to use X&Y to report to the pair of random variables and \u03c0 plus anti minus four class proportions an.",
            "I'm going to use F&G to denote the probability density functions of predictors for passive and negative classes.",
            "And for more compact expression of the result, I'm going to include one in the.",
            "Attributes so that we have Explorer as.",
            "D + 1 dimensional attribute and again include intercept with the normal vector so that we had just had to pull the coefficient vector.",
            "Then given any linear discriminant function linear discriminant function, we can use bad time next tilt to report to the function.",
            "And the coefficient vector for linear support vector machine is given as the minimizer of.",
            "This penalized empirical risk.",
            "Alright."
        ],
        [
            "And let me define a few more so.",
            "I'm going to use LR better to refer to the expectation of the hinge loss and para star will denote the minimizer of this L or better.",
            "So this is like the population minimizer and define a sub para to be minus expectation of P of 1 minus some white times H times.",
            "Why times tilled here fee up key is simply.",
            "The indicator of non negative nonnegativity of the argument.",
            "And each of better to be expect Tatian up, but this quantity which involves XRX till Dec still transpose and director top function OK and later I will talk about some regularity conditions to justify that these as sub F&HSA beta and H are better.",
            "They are in fact gradient of this Ella Beth Ann Hessian matrix of this LOL."
        ],
        [
            "And let me give you some more words about the Hessian matrix.",
            "So here is the definition which involves directed to function and JKS entry of this matrix is given as this.",
            "So here we have some weighted sum up.",
            "Some special integral transform transform sub.",
            "Density functions an XJ times XK.",
            "These are in fact integrals sub these functions over hyperplanes defined by WNB, and these integral transforms are known as radon transforms.",
            "Case simply JKS entry of this station matrix is weighted average or weighted sum up radon transforms of.",
            "X Ray times.",
            "XK times the probability density function.",
            "Oops."
        ],
        [
            "Now let me talk about some regularity conditions so that we have proper powder type representation for the quick vision vector.",
            "The 1st I'm going to assume that two density functions are continuous an have finite second moments and we need this condition to make sure that the Hessian matrix H or better is well defined and it is continuous in better.",
            "And the second condition I'm going to assume that there is a point, XON radius Delta zero such that two probability density functions are strictly greater than some positive constant over the pool.",
            "With Center X0, an radius Delta 0.",
            "So this condition in fact excludes set problem case.",
            "And we need this condition to make sure that population minimizer pet ASTA is properly defined.",
            "And the third condition is a bit more technical, but in a nutshell, what the condition says is that 2 mean vectors for the classes.",
            "They are different so that we don't have the general case.",
            "By degenerate case I mean the case when the note the normal vector for the populate population minimizer being equal to 0.",
            "So this we need this third condition to make sure that normal vector is not zero.",
            "And one more condition force condition for one and minus one level sets of the theoretically optimal linear discriminant function denoted as M plus and minus their ages, exist to subsets of M plus and minus on which the class tends to functions.",
            "F&G are bounded away from Zero, and again we need this condition to make sure that.",
            "The Hessian matrix at the optimal coefficient vector, better star is positive definite, so that peristyle is uniquely defined.",
            "So under these four conditions we can divide paratype representation of better head for linear support vector machine."
        ],
        [
            "And I already gave you this historical remarks so."
        ],
        [
            "Let me skip so this is a main result.",
            "So under those four conditions, and assuming that tuning parameter goes to 0 faster than 1 / sqrt N, then the subcription vector of linear support vector machine when it's centered at barrister and scaled by square root N, is some sort of linear transformation of sum of iid random vectors.",
            "Except some.",
            "Small remainder kayan.",
            "Recall that H. Peristyles simply the Hessian matrix that I defined and fears this indicator.",
            "OK, so once you have this representation it is now very easy to derive some synthetic properties of linear support vector machine by simply applying Central Limit Theorem so as."
        ],
        [
            "An immediate consequence of the previous representation.",
            "We have a SIM card in normal distribution for the coefficient vector.",
            "K and.",
            "The variance of better head or synthetic variants or better hat depends on the Hessian matrix and Geo barrister, which is expectation of.",
            "This quantity and this is actually the covariance of the terms appeared in the sum of IID random variables.",
            "Kim Ann as a simple corollary, we get a symmetric distribution of the value of linear discriminant function.",
            "The for the support vector machine.",
            "And this result could be useful in determining whether a given point is close to boundary or not given X. Alright."
        ],
        [
            "So let me show you an example to illustrate a result in more concrete terms.",
            "So let's consider very simple case where we have two multivariate normal distribution for two classes, one mid mean vector, mu sub F, the other with mean vector mu subji.",
            "Some use of F is for positive class and use of G4 negative class and they have common covariance matrix say Sigma and assume that two classes equally likely.",
            "OK, so in this case we know what the optimal classification boundary is.",
            "So the main question that we're going to explore here is the relationship between the page decision boundary an the optimal hyperplane given by SVM, which is simply given by this equation K with para star, which is the population minimizer of expectation of Ingels.",
            "It is well known that the optimal classification boundary is so one that bisects the mean difference between two classes.",
            "So."
        ],
        [
            "So mathematically, the decision boundary, optimal decision boundary, which is to Fishers LDA, is given by this equation.",
            "So clearly you can see that the boundary contains the midpoint of two means and the normal vector is defined by the mean difference and the covariance matrix.",
            "And we call that the hyperplane determined by the linear support vector machine.",
            "Is this one with Paris to an better star?",
            "Is the one that satisfies this equation.",
            "So here as is a gradient of a lot better.",
            "OK, so if you have a closer look at this.",
            "Condition then you can learn more about what this better style.",
            "So this is this.",
            "We expression of a self Battlestar and we have D + 1 conditions.",
            "And for intercept we have this condition that probability of X.",
            "Below the theoretically optimal passive margin for passive class is same as the probability of X.",
            "Now Bob the theoretical negative margin for negative class.",
            "Likewise for the first moments for XJ, if you restrict the space to this, which is again the region where X is below the positive margin an X above the negative margin, these to expectations have to be the same.",
            "So in a way peristyles to vector that balances these two classes within the merger.",
            "OK, so this is a general equation that parista should satisfy, and for this particular example, OB two multivariate normal distributions."
        ],
        [
            "Direct calculation shows that peristyles given by this came so here you can see that this better star, The Intercept and the normal vector, they are the same as the Intercept and normal vector for Fishers LDA except a multiplicative constant which depends on the distance between two distributions, so called Mahalanobis distance K. So that means these two procedures.",
            "Produce the same boundary, so in this sense we could say that linear support vector machine is equivalent to Fishers LDA or synthetically.",
            "Kim Ann you can easily verify that those four assumptions that we have for the main theorem they are satisfied the regularity conditions for this example.",
            "So the main theorem applies.",
            "K so to see exactly what the theorem says, let's consider the simplest case where we have only one covariate an the optimal bounder is zero and the Sigma is 1.",
            "So in this example haha.",
            "Do not excuse me, the Maulana with distance between two classes becomes the absolute difference between 2 means."
        ],
        [
            "Hey, so this figure shows.",
            "How do positive and negative margins, theoretical positive and negative margins?",
            "Which are the one N -- 1 level sets of each with Paris to change as we vary the distance between two classes from .5 to 6?",
            "So here the green lines are the density functions and red lines are red particle bars.",
            "They are the positive and negative margins.",
            "K so now."
        ],
        [
            "Let's take a look at our synthetic variants of The Intercept an the slope that we get from linear support with linear support vector machine in this example so.",
            "Here we see the synthetic variance as a function of the distance between 2 means and.",
            "Typically we are interested in the range from one to 4K and what the result tells us is that when the distance is too small or too large.",
            "The variance of.",
            "Intercept and slope tends to be large.",
            "Then means if two classes will let too much then the variance increases.",
            "Also when there is too little.",
            "I mean when two classes two separable again we have increase in variance.",
            "Either way I'd like to mention that in the Cemetery on Alesis we let the tuning parameter go to zero at certain rate, so this result so indicates that when two classes are two step problem, we do need regularization."
        ],
        [
            "Alright.",
            "Now let's consider the more direct comparison between the sampling distribution of Para jhed from linear support vector machine an the OR synthetic distribution given by the theory.",
            "OK, so here I considered bivariate normal example with means set to one 1 -- 1 -- 1 and this is the covariance matrix and in this example the Bayes error rate is .07.",
            "Kate Ann I found these.",
            "Better J hat by directly minimizing the empirical risk with respect to the hinge loss without the penalty part OK and what this table tells us is the average job estimated quit visions over 1000 replicates as we change sample size N from 100 to 500 K and the last column is.",
            "The optimal coefficients parastar came here.",
            "You can see that as sample size increases these as made, it means get closer to the better stuff as the theory indicates.",
            "Anne."
        ],
        [
            "And for more direct comparison between the sampling distribution of the coefficients and limit distribution given by the main theorem, here we have sampling distribution sub para O had to intercept an Parowan hat in so little lines an the dotted lines are the synthetic distributions given by the theorem and this is for sample size.",
            "500K so here we can see see that these two distributions are pretty close.",
            "Alright."
        ],
        [
            "Now.",
            "Here is an application of this or syntactic analysis.",
            "So think about variable selection for linear support vector machine as I mentioned to you, one can use now standardize quick visions as test statistics for ranking or screening important variables.",
            "OK, so using the synthetic distribution we have synthetic variances.",
            "There we can come up with standardized coefficients as test statistic.",
            "And what this plot shows is the Type 1 error rates for various settings where we vary sample size N and the dimension D from 6224K.",
            "And here type an error rate means the probability of falsely declaring irrelevant variables as relevant K. And here I have a muse of F which is factor given by.",
            "This is only the first 3 / 2 variables have once and the rest the over 2 have zeros and mu subject is same as zero vector.",
            "So only the first day over 2 variables are relevant and the rest are not relevant.",
            "And the nominal level is 5%, so if the asymptotics distributions really valid, we expect to see that type an error rates will be very close to the nominal level, 5%, which is indicated by this red.",
            "Dotted line OK. And here we can see that when the dimension is relatively small, 6 or 12.",
            "For different values of the sample size.",
            "Type an error rates are pretty close to the nominal level.",
            "OK, yet when the dimension is very large for.",
            "Fordyce result a synthetic result to be valid sample size has to be very large.",
            "Alright, so this is 1 application of the asymptotical analysis of linear support vector mission and also we can use this synthetic distribution to compare different procedures.",
            "So let me talk about."
        ],
        [
            "How to talk about relative efficiency of 1 method compared to?",
            "Others using this, uh, synthetic distribution.",
            "And in fact at front did some comparison between logistic regression and linear discriminant analysis after getting or synthetic distribution for the coefficients of logistic regression and LDA.",
            "And we can really follow the same approach to compare linear support vector machine with other procedures.",
            "OK, so this is the Canonical LDA setting that up front considered.",
            "So here we have again two normal distributions.",
            "Which means given by this, so two distributions are Delta part only in the first coordinate and the means are the same for the rest OK. And in this example, the optimal discriminant linear discriminant function is given by this with INTERCEPT.",
            "Equal to logo \u03c0 + / \u03c0 minus and the normal vector is Delta times E1.",
            "Keep an affront derived.",
            "The following expression for.",
            "Expectation of regret or expectations.",
            "Increased error rates compared to the Bayes error rate.",
            "When you have linear discrete math, so to say L hat with coefficient vector that has TA.",
            "And the coefficient vector has a symptomatic normal distribution with center better star and say variance covariance matrix Sigma K. Then here we can see that this expectation of regret or expectation of increased error rate is a function of those variances in the limit.",
            "In the covariance matrix of the limit distribution and some other model parameters.",
            "So using this result, now we can compare different linear discriminant procedures with different or synthetic distributions.",
            "So in his."
        ],
        [
            "Paper.",
            "Efron studded or synthetic volitive efficiency up logistic regression.",
            "Two LDA normal discrimination and he defined or synthetic relative efficiency as the limit of the ratio of expected increase error rate of LDA 2.",
            "The expectation increase error rate of logistic regression.",
            "And he found that this regression is between 1/2 and 2/3 as effective as LDA in typical setting.",
            "So this means that.",
            "To achieve same accuracy for logistic regression, you need twice to 1.5 times as larger sample as.",
            "LD K. The following the same approach."
        ],
        [
            "We compared linear support vector machine with LDA K and this is under under.",
            "Same Canonical LDA setting with.",
            "Equal probability for two classes.",
            "And it can be shown that the synthetic will lot of efficiency of linear support vector machine 2, LDA is given by this expression.",
            "Here Delta is the distance between 2 means and P is the PDF of standard normal distribution an A star is the constant satisfying this equation K. So we consider the different values of Delta which is in this case actually same as the model is distance.",
            "And this column shows the corresponding Bayes error rates from very easy to relatively hard problems.",
            "an A star.",
            "The other values.",
            "Satisfying this equation and this column shows that our synthetic relative efficiencies of linear support vector machine to LDA ranging from 76% to about 29% OK. And for comparison I just reproduced results in reference paper about our synthetic relative efficiencies of logistics regression to LDA.",
            "And here you can see that.",
            "The relative efficiencies of support vector machine are lower than those of logistic regression, and of course lower than LDA, and this is not too surprising because LDA LDA uses specific information about multi variate normal distributions, and this computation is under the ideal LDA setting.",
            "Hey Brother, um, interesting question.",
            "Is something like this?",
            "How these three methods compare in some other setting different from idea LDA setting, yet we have still linear boundary as the optimal decision boundary and.",
            "Finding such a case was in as easy as I initially thought about."
        ],
        [
            "There is such a situation and here's an example.",
            "OK, so here for each class, now we have mixture of two Gaussian distributions.",
            "So we have two Gaussian components with mean mu sub, F on and music F2.",
            "Likewise we have two components for negative class with mean vectors, musaab, gewonnen, mu subject 2.",
            "And here there's four points.",
            "They form a rectangle OK, and.",
            "Delta W. Refers to the mean difference within each class, and teletubby refers to the mean distance between two classes.",
            "K depending on pipe, Lawson pie minus you could have classification boundary closer to whatever the majority class.",
            "So initially we thought as Delta W increases since that indicates more deviation from ideal LDA setting without logistic regression or a support vector machine would come out to be better than LDA.",
            "And this setting is more complicated than the simple LDA settings, so we carried out some simulation to verify this conjecture."
        ],
        [
            "And it turns out that.",
            "The error rates of LDA were the smallest among these three methods.",
            "When we varied the distance between 2 means within each class from one to four, and in this example we can show that the page error, it doesn't really change.",
            "So in terms of base error rate.",
            "This mixture problem is the same as the idea LDA problem.",
            "OK so.",
            "LDA, so best and.",
            "Load is regression.",
            "Second, an linear support vector machine, the third."
        ],
        [
            "Now.",
            "We varied Delta be of course, depending on Delta be we have different error rates, so the larger the Delta be the mean distance between two classes, the smaller the Bayes error rate.",
            "So the black line is again the Bayes error rate and.",
            "Red for LDA, Green for Lotus regression, an blue for support vector mission.",
            "In this case 3 methods are about the same, except when Delta B is very large in.",
            "Again, LDA came out to be the best.",
            "OK, and later we learned that there is some reason why LDA shows some robustness in this particular city.",
            "And my students and I were a bit frustrated in that despite all the empirical successes of linear support vector machine, we couldn't quite get decisive numerical evidence that shows support vector Machine works very well in this controlled setting.",
            "However, when we varied the dimension."
        ],
        [
            "We got very interesting results, so here the sample size is 100 and we very dimension from the five to about 90.",
            "And as you can see here, when the dimension is relatively small compared to sample size, again LDS are best followed by.",
            "Logistic regression and support recognition.",
            "But as the dimension increases, the error rate of support vector machine didn't really grow as fast as those error rate sub.",
            "Noticed regression and LDA.",
            "OK, this is pretty interesting result.",
            "So.",
            "Let me come.",
            "Conclude my talk by giving some room."
        ],
        [
            "So in this talk I examined some of synthetic properties of coefficients of linear support vector machine and studied it spell volitive efficiency in comparison with other classical approaches to classification through probability estimation, such as logistical regression and LDA.",
            "An this comparison was through powder type representation of the coefficients.",
            "And the synthetic result shows how the margins of optimal hyperplane given by the support vector mission.",
            "And on the line know how the margins they are.",
            "Margins and underlying probability distributions determine the statistical behavior of the coefficient vector, and I showed possibility of using hypothesis testing for variable selection for linear support vector machine.",
            "With the synthetic distribution of the coefficient vector, although for practical application of this approach to variable selection, we need some consistent estimators of this Geo Bannister and H business OK. And it became very clear that the situation where the support vector machine has an edge over.",
            "Although classical approaches lodis regression and LDA involve high dimensional data.",
            "So for more interesting theoretical analysis, we need to consider different mode of our synthetics where the dimension also grows with sample sites.",
            "Came and finally extension of this linear support vector machine or synthetics to nonlinear case would be very interesting.",
            "Future direction, right?",
            "So thank you for your attention and."
        ],
        [
            "If you're interested in some details of this work, then you can take a look at the paper that I wrote with KU and others which appeared in Journal of Machine Learning Research.",
            "And for the later part, volitive efficiency analysis an the related numerical results are from joint work.",
            "With my student we won, so this is work in progress and we don't have any paper yet.",
            "Um?",
            "Can we put this back?",
            "So if you have read the original paper by Vapnik and others that proposed support vector machine in the beginning, there is comment about the comparison between LDA and support vector machine.",
            "So in LDA as a dimension increases, the number of parameters that you have to estimate increases very rapidly.",
            "Is that so?",
            "I think one reason why for high dimensional case support vector Machine Works works better than LDA, yet I cannot quite figure out another puzzle.",
            "That for logistic regression again that has same number of parameters as support vector machine.",
            "That doesn't really work well, so that's part of.",
            "Future problems that I need to look at.",
            "Dimension gross.",
            "LDS, It plays the estimation of it.",
            "Have you tried like this?",
            "Have this regular.",
            "Tried.",
            "Oh, that's that's also very good question.",
            "So the results that I presented.",
            "To merely include regularization, although for support vector machine, even when we tune the regularization parameter we had about the same result.",
            "So at first we were very curious whether the improvement of SVM over other procedures when dimension increases, whether that is due to regularization or simply the particular loss that we use.",
            "But partial answer that I have right now is maybe it's due to.",
            "The specific mechanism that you used, but now I'm very curious whether when we regularize LDA we we get any better result and how three methods would compare in that case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So can you hear me in the back?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So today I will talk about.",
                    "label": 0
                },
                {
                    "sent": "Powder type representation of linear support vector machine and some statistical properties that we can infer from the representation.",
                    "label": 1
                },
                {
                    "sent": "So, generally speaking or Bahadur representation of statistical estimator means some sort of stochastic approximation of the estimator as an average or sum of IID random variables.",
                    "label": 0
                },
                {
                    "sent": "So once we have such a representation, you can easily analyze the estimator.",
                    "label": 0
                },
                {
                    "sent": "Loblaws numbers.",
                    "label": 1
                },
                {
                    "sent": "Well, speaking on this topic here at University of Chicago, I'd like to mention that.",
                    "label": 0
                },
                {
                    "sent": "The term powder representation originates from such a representation for sample quantiles proposed by padrew in 1960s, and he was a faculty member here in the Department of Statistics.",
                    "label": 0
                },
                {
                    "sent": "Before he passed away.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here is an outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I just arrived last night so I didn't really hear other talks before, but I'm pretty sure that many speakers before me talked about support vector machine, so I will give you a very quick review of support vector machine and briefly describe some statistical properties that people have studied and then I will present the main results.",
                    "label": 0
                },
                {
                    "sent": "Of synthetic analysis of linear support vector machine through bahadue representation of the coefficient vector of linear support vector machine an, I will show you.",
                    "label": 1
                },
                {
                    "sent": "An example to illustrate the main results and then I will talk about some other implication of the result in comparing different classification procedures in terms of relative efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine has been used successfully in many real applications.",
                    "label": 0
                },
                {
                    "sent": "Ranging from handwritten December cognition problem to bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Problems say where you want to diagnose different disease types based on genetic profiles of patients.",
                    "label": 0
                },
                {
                    "sent": "And these are examples of classification problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in more abstract sense, classification problem is given as follows.",
                    "label": 0
                },
                {
                    "sent": "So in classification we have Sadie predictors, so D attributes denoted by X Ann.",
                    "label": 0
                },
                {
                    "sent": "We have class label Y which is one of K labels and in this talk I will talk about mostly binary classification problem where case too.",
                    "label": 0
                },
                {
                    "sent": "And given training data, set up an pairs of XIYI, we want to find some rule fee which is a mapping from D dimensional input space to class label space, which can be generalized to unseen cases.",
                    "label": 0
                },
                {
                    "sent": "And typically people use the 01 loss function to assess in accuracy of classification or classification goofy, which is simply indicator of misclassification.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now it is well known that support vector machine is an example of regularization method or penalization method.",
                    "label": 1
                },
                {
                    "sent": "So in general regularization method can be formulated as a procedure of finding some function unknown function F in a candidate function space script F that minimizes this penalized empirical risk.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "All that is lost to measure goodness of fit of F an over there jail web measures sort of model complexity of F. And here Lambda is a tuning parameter that balances.",
                    "label": 0
                },
                {
                    "sent": "The data fit measured by empirical risk and the model complexity, and if you choose the function space script have to be very large.",
                    "label": 1
                },
                {
                    "sent": "Then without this penalty functionalty above, the optimization problem becomes ill posed.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is very brief introduction to linear support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So the linear support vector machine looks for the hyperplane that maximizes margin margin between two classes.",
                    "label": 0
                },
                {
                    "sent": "Here the margin is mathematically given as two over the norm of W when you have linear discriminant function of this form.",
                    "label": 0
                },
                {
                    "sent": "So you have normal vector W, an intercept be an classification boundaries given by the zero level set.",
                    "label": 0
                },
                {
                    "sent": "All this linear discriminant function and positive and negative margins are given by 1 N minus one level sets.",
                    "label": 0
                },
                {
                    "sent": "So here I'm considering class labels to be either one or minus one.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use passive class or negative class to report to those two classes.",
                    "label": 0
                },
                {
                    "sent": "K so maximizing margin is now equivalent to, say, minimizing squared norm of the W normal vector.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more generally, when we allow two classes to have some overlap, we end up with this formulation of linear support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So linear support vector machine is given as a solution to this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Again, we minimize.",
                    "label": 0
                },
                {
                    "sent": "We want to find linear discriminant function with normal vector W an intercept B, minimizing this penalized empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And here we are using most function so called hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And as you can see here the penalty function is given as squared normal the normal vector WK and once we have real value discriminant function, the classification rule is given by this.",
                    "label": 0
                },
                {
                    "sent": "So we simply take the sign up at Publix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and this plot shows the relationship between the hinge loss an the misclassification loss that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "So these are functions of the product top class label, which is either one or minus one, and the value of linear discriminant function F of X.",
                    "label": 0
                },
                {
                    "sent": "And of course, when the class label and the sign of FX match, we don't have any loss.",
                    "label": 0
                },
                {
                    "sent": "So we have 0 otherwise one, so there's so little line is.",
                    "label": 0
                },
                {
                    "sent": "This classification loss and the dashed line is the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So clearly from this picture you can see that the hinge loss is convex upper bound of misclassification loss and you notice here that this hinge loss has singularity at one.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More generally, we could consider extension from linear support vector machine to nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "So the support vector machine can be cast as a regularization problem regularization method in a reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So here we want to find function of this form, minimizing the same penalized.",
                    "label": 0
                },
                {
                    "sent": "Average hinge loss an you can notice here that now we replace the squared normal W with appropriate squared norm of this H, which is part of this at Publix.",
                    "label": 0
                },
                {
                    "sent": "Came and since there is 1 to one correspondence between reproducing kernel Hilbert space and the kernel function for nonlinear generalization, it is sufficient to choose a nonlinear kernel function.",
                    "label": 0
                },
                {
                    "sent": "So you could choose Gaussian kernel or polynomial kernel to get nonlinear support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me briefly go over some statiscal proper properties that people have started.",
                    "label": 0
                },
                {
                    "sent": "The first lien studied feature consistency of linear support vector machine which is very weak notion of consistency and result says that the population minimizer of hinge loss is simply sign of \u03c0 / X minus half here.",
                    "label": 0
                },
                {
                    "sent": "X is the conditional probability of.",
                    "label": 0
                },
                {
                    "sent": "Massive class given X and this result indicates that support vector machine aims to approximate the best classification, which is based decision model.",
                    "label": 0
                },
                {
                    "sent": "Directly, without estimating this conditional probability.",
                    "label": 0
                },
                {
                    "sent": "So which is different from other classical approaches to classification?",
                    "label": 0
                },
                {
                    "sent": "And also down & word.",
                    "label": 0
                },
                {
                    "sent": "Bartlett and others studied based consistency of support recognition and they showed that the risk of support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So this is the error rate of support vector.",
                    "label": 0
                },
                {
                    "sent": "Motion converges to the Bayes error rate in probability under some universal approximation condition on the function space, or equivalently condition on the kernel.",
                    "label": 1
                },
                {
                    "sent": "And Steinert and others also studied radar convergence of this error rate to the Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "So as you can see here, um, previously people have focused on.",
                    "label": 0
                },
                {
                    "sent": "Risk consistency of support vector machine.",
                    "label": 0
                },
                {
                    "sent": "But today I like talk about some other properties that people haven't really looked at, so.",
                    "label": 0
                },
                {
                    "sent": "Here is motivation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Keanan others propose so-called recursive feature elimination, which is akin to backward elimination used in linear modeling for selection of variables, but they apply this idea to the fitted coefficients of linear support vector machine, not standardized coefficients, and this method brought some questions to us.",
                    "label": 1
                },
                {
                    "sent": "For example, what is the status quo behavior of the estimated query visions?",
                    "label": 1
                },
                {
                    "sent": "Through support vector mission and what really determines their variances?",
                    "label": 0
                },
                {
                    "sent": "And these questions prompted us to study some more sympathetic properties of the coefficients of the linear support vector machine and related questions like relative efficiency of support vector machine compared to other more classical approaches.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that we can tackle these questions by.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Following some wedding custom.",
                    "label": 0
                },
                {
                    "sent": "So in this question is something knew yet something old and something to power 04.",
                    "label": 0
                },
                {
                    "sent": "So what's new in this problem is the hinge loss that we gotta deal with, which is singularity at one as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Because of this people get the support vector machine solution through quadratic programming, so we don't really have closed form expression for the solution.",
                    "label": 1
                },
                {
                    "sent": "However, it is very useful links between support vector machine and some other estimators that we know very well.",
                    "label": 1
                },
                {
                    "sent": "So the population minimizer sign appeared X minus half.",
                    "label": 1
                },
                {
                    "sent": "With respect to the hinge loss, this is in fact the true median of class label given X, so in a way it is suggest that.",
                    "label": 0
                },
                {
                    "sent": "Hinge loss is a lot like absolute deviation.",
                    "label": 0
                },
                {
                    "sent": "Lots that we know very well about.",
                    "label": 0
                },
                {
                    "sent": "So, for example, Pollard studied some of synthetics for least absolute deviation estimators, and it turns out that we can use his techniques to analyze linear support vector machine and the convexity of the loss is the key in the proof and hinge losses convex function.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let me define a few notations to state the main result, so I'm going to use X&Y to report to the pair of random variables and \u03c0 plus anti minus four class proportions an.",
                    "label": 1
                },
                {
                    "sent": "I'm going to use F&G to denote the probability density functions of predictors for passive and negative classes.",
                    "label": 0
                },
                {
                    "sent": "And for more compact expression of the result, I'm going to include one in the.",
                    "label": 0
                },
                {
                    "sent": "Attributes so that we have Explorer as.",
                    "label": 0
                },
                {
                    "sent": "D + 1 dimensional attribute and again include intercept with the normal vector so that we had just had to pull the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "Then given any linear discriminant function linear discriminant function, we can use bad time next tilt to report to the function.",
                    "label": 0
                },
                {
                    "sent": "And the coefficient vector for linear support vector machine is given as the minimizer of.",
                    "label": 0
                },
                {
                    "sent": "This penalized empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me define a few more so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use LR better to refer to the expectation of the hinge loss and para star will denote the minimizer of this L or better.",
                    "label": 0
                },
                {
                    "sent": "So this is like the population minimizer and define a sub para to be minus expectation of P of 1 minus some white times H times.",
                    "label": 0
                },
                {
                    "sent": "Why times tilled here fee up key is simply.",
                    "label": 0
                },
                {
                    "sent": "The indicator of non negative nonnegativity of the argument.",
                    "label": 0
                },
                {
                    "sent": "And each of better to be expect Tatian up, but this quantity which involves XRX till Dec still transpose and director top function OK and later I will talk about some regularity conditions to justify that these as sub F&HSA beta and H are better.",
                    "label": 0
                },
                {
                    "sent": "They are in fact gradient of this Ella Beth Ann Hessian matrix of this LOL.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me give you some more words about the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "So here is the definition which involves directed to function and JKS entry of this matrix is given as this.",
                    "label": 0
                },
                {
                    "sent": "So here we have some weighted sum up.",
                    "label": 0
                },
                {
                    "sent": "Some special integral transform transform sub.",
                    "label": 0
                },
                {
                    "sent": "Density functions an XJ times XK.",
                    "label": 0
                },
                {
                    "sent": "These are in fact integrals sub these functions over hyperplanes defined by WNB, and these integral transforms are known as radon transforms.",
                    "label": 0
                },
                {
                    "sent": "Case simply JKS entry of this station matrix is weighted average or weighted sum up radon transforms of.",
                    "label": 0
                },
                {
                    "sent": "X Ray times.",
                    "label": 0
                },
                {
                    "sent": "XK times the probability density function.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me talk about some regularity conditions so that we have proper powder type representation for the quick vision vector.",
                    "label": 0
                },
                {
                    "sent": "The 1st I'm going to assume that two density functions are continuous an have finite second moments and we need this condition to make sure that the Hessian matrix H or better is well defined and it is continuous in better.",
                    "label": 1
                },
                {
                    "sent": "And the second condition I'm going to assume that there is a point, XON radius Delta zero such that two probability density functions are strictly greater than some positive constant over the pool.",
                    "label": 0
                },
                {
                    "sent": "With Center X0, an radius Delta 0.",
                    "label": 0
                },
                {
                    "sent": "So this condition in fact excludes set problem case.",
                    "label": 0
                },
                {
                    "sent": "And we need this condition to make sure that population minimizer pet ASTA is properly defined.",
                    "label": 0
                },
                {
                    "sent": "And the third condition is a bit more technical, but in a nutshell, what the condition says is that 2 mean vectors for the classes.",
                    "label": 0
                },
                {
                    "sent": "They are different so that we don't have the general case.",
                    "label": 0
                },
                {
                    "sent": "By degenerate case I mean the case when the note the normal vector for the populate population minimizer being equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this we need this third condition to make sure that normal vector is not zero.",
                    "label": 1
                },
                {
                    "sent": "And one more condition force condition for one and minus one level sets of the theoretically optimal linear discriminant function denoted as M plus and minus their ages, exist to subsets of M plus and minus on which the class tends to functions.",
                    "label": 1
                },
                {
                    "sent": "F&G are bounded away from Zero, and again we need this condition to make sure that.",
                    "label": 0
                },
                {
                    "sent": "The Hessian matrix at the optimal coefficient vector, better star is positive definite, so that peristyle is uniquely defined.",
                    "label": 0
                },
                {
                    "sent": "So under these four conditions we can divide paratype representation of better head for linear support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I already gave you this historical remarks so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me skip so this is a main result.",
                    "label": 0
                },
                {
                    "sent": "So under those four conditions, and assuming that tuning parameter goes to 0 faster than 1 / sqrt N, then the subcription vector of linear support vector machine when it's centered at barrister and scaled by square root N, is some sort of linear transformation of sum of iid random vectors.",
                    "label": 0
                },
                {
                    "sent": "Except some.",
                    "label": 0
                },
                {
                    "sent": "Small remainder kayan.",
                    "label": 0
                },
                {
                    "sent": "Recall that H. Peristyles simply the Hessian matrix that I defined and fears this indicator.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you have this representation it is now very easy to derive some synthetic properties of linear support vector machine by simply applying Central Limit Theorem so as.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An immediate consequence of the previous representation.",
                    "label": 0
                },
                {
                    "sent": "We have a SIM card in normal distribution for the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "K and.",
                    "label": 0
                },
                {
                    "sent": "The variance of better head or synthetic variants or better hat depends on the Hessian matrix and Geo barrister, which is expectation of.",
                    "label": 0
                },
                {
                    "sent": "This quantity and this is actually the covariance of the terms appeared in the sum of IID random variables.",
                    "label": 0
                },
                {
                    "sent": "Kim Ann as a simple corollary, we get a symmetric distribution of the value of linear discriminant function.",
                    "label": 0
                },
                {
                    "sent": "The for the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And this result could be useful in determining whether a given point is close to boundary or not given X. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me show you an example to illustrate a result in more concrete terms.",
                    "label": 0
                },
                {
                    "sent": "So let's consider very simple case where we have two multivariate normal distribution for two classes, one mid mean vector, mu sub F, the other with mean vector mu subji.",
                    "label": 1
                },
                {
                    "sent": "Some use of F is for positive class and use of G4 negative class and they have common covariance matrix say Sigma and assume that two classes equally likely.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case we know what the optimal classification boundary is.",
                    "label": 0
                },
                {
                    "sent": "So the main question that we're going to explore here is the relationship between the page decision boundary an the optimal hyperplane given by SVM, which is simply given by this equation K with para star, which is the population minimizer of expectation of Ingels.",
                    "label": 1
                },
                {
                    "sent": "It is well known that the optimal classification boundary is so one that bisects the mean difference between two classes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So mathematically, the decision boundary, optimal decision boundary, which is to Fishers LDA, is given by this equation.",
                    "label": 1
                },
                {
                    "sent": "So clearly you can see that the boundary contains the midpoint of two means and the normal vector is defined by the mean difference and the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And we call that the hyperplane determined by the linear support vector machine.",
                    "label": 1
                },
                {
                    "sent": "Is this one with Paris to an better star?",
                    "label": 0
                },
                {
                    "sent": "Is the one that satisfies this equation.",
                    "label": 0
                },
                {
                    "sent": "So here as is a gradient of a lot better.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have a closer look at this.",
                    "label": 0
                },
                {
                    "sent": "Condition then you can learn more about what this better style.",
                    "label": 0
                },
                {
                    "sent": "So this is this.",
                    "label": 0
                },
                {
                    "sent": "We expression of a self Battlestar and we have D + 1 conditions.",
                    "label": 0
                },
                {
                    "sent": "And for intercept we have this condition that probability of X.",
                    "label": 0
                },
                {
                    "sent": "Below the theoretically optimal passive margin for passive class is same as the probability of X.",
                    "label": 0
                },
                {
                    "sent": "Now Bob the theoretical negative margin for negative class.",
                    "label": 0
                },
                {
                    "sent": "Likewise for the first moments for XJ, if you restrict the space to this, which is again the region where X is below the positive margin an X above the negative margin, these to expectations have to be the same.",
                    "label": 1
                },
                {
                    "sent": "So in a way peristyles to vector that balances these two classes within the merger.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a general equation that parista should satisfy, and for this particular example, OB two multivariate normal distributions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Direct calculation shows that peristyles given by this came so here you can see that this better star, The Intercept and the normal vector, they are the same as the Intercept and normal vector for Fishers LDA except a multiplicative constant which depends on the distance between two distributions, so called Mahalanobis distance K. So that means these two procedures.",
                    "label": 0
                },
                {
                    "sent": "Produce the same boundary, so in this sense we could say that linear support vector machine is equivalent to Fishers LDA or synthetically.",
                    "label": 1
                },
                {
                    "sent": "Kim Ann you can easily verify that those four assumptions that we have for the main theorem they are satisfied the regularity conditions for this example.",
                    "label": 0
                },
                {
                    "sent": "So the main theorem applies.",
                    "label": 1
                },
                {
                    "sent": "K so to see exactly what the theorem says, let's consider the simplest case where we have only one covariate an the optimal bounder is zero and the Sigma is 1.",
                    "label": 0
                },
                {
                    "sent": "So in this example haha.",
                    "label": 0
                },
                {
                    "sent": "Do not excuse me, the Maulana with distance between two classes becomes the absolute difference between 2 means.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey, so this figure shows.",
                    "label": 0
                },
                {
                    "sent": "How do positive and negative margins, theoretical positive and negative margins?",
                    "label": 0
                },
                {
                    "sent": "Which are the one N -- 1 level sets of each with Paris to change as we vary the distance between two classes from .5 to 6?",
                    "label": 0
                },
                {
                    "sent": "So here the green lines are the density functions and red lines are red particle bars.",
                    "label": 0
                },
                {
                    "sent": "They are the positive and negative margins.",
                    "label": 0
                },
                {
                    "sent": "K so now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take a look at our synthetic variants of The Intercept an the slope that we get from linear support with linear support vector machine in this example so.",
                    "label": 1
                },
                {
                    "sent": "Here we see the synthetic variance as a function of the distance between 2 means and.",
                    "label": 1
                },
                {
                    "sent": "Typically we are interested in the range from one to 4K and what the result tells us is that when the distance is too small or too large.",
                    "label": 0
                },
                {
                    "sent": "The variance of.",
                    "label": 0
                },
                {
                    "sent": "Intercept and slope tends to be large.",
                    "label": 0
                },
                {
                    "sent": "Then means if two classes will let too much then the variance increases.",
                    "label": 0
                },
                {
                    "sent": "Also when there is too little.",
                    "label": 0
                },
                {
                    "sent": "I mean when two classes two separable again we have increase in variance.",
                    "label": 0
                },
                {
                    "sent": "Either way I'd like to mention that in the Cemetery on Alesis we let the tuning parameter go to zero at certain rate, so this result so indicates that when two classes are two step problem, we do need regularization.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now let's consider the more direct comparison between the sampling distribution of Para jhed from linear support vector machine an the OR synthetic distribution given by the theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I considered bivariate normal example with means set to one 1 -- 1 -- 1 and this is the covariance matrix and in this example the Bayes error rate is .07.",
                    "label": 1
                },
                {
                    "sent": "Kate Ann I found these.",
                    "label": 0
                },
                {
                    "sent": "Better J hat by directly minimizing the empirical risk with respect to the hinge loss without the penalty part OK and what this table tells us is the average job estimated quit visions over 1000 replicates as we change sample size N from 100 to 500 K and the last column is.",
                    "label": 0
                },
                {
                    "sent": "The optimal coefficients parastar came here.",
                    "label": 0
                },
                {
                    "sent": "You can see that as sample size increases these as made, it means get closer to the better stuff as the theory indicates.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for more direct comparison between the sampling distribution of the coefficients and limit distribution given by the main theorem, here we have sampling distribution sub para O had to intercept an Parowan hat in so little lines an the dotted lines are the synthetic distributions given by the theorem and this is for sample size.",
                    "label": 0
                },
                {
                    "sent": "500K so here we can see see that these two distributions are pretty close.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Here is an application of this or syntactic analysis.",
                    "label": 0
                },
                {
                    "sent": "So think about variable selection for linear support vector machine as I mentioned to you, one can use now standardize quick visions as test statistics for ranking or screening important variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so using the synthetic distribution we have synthetic variances.",
                    "label": 0
                },
                {
                    "sent": "There we can come up with standardized coefficients as test statistic.",
                    "label": 0
                },
                {
                    "sent": "And what this plot shows is the Type 1 error rates for various settings where we vary sample size N and the dimension D from 6224K.",
                    "label": 1
                },
                {
                    "sent": "And here type an error rate means the probability of falsely declaring irrelevant variables as relevant K. And here I have a muse of F which is factor given by.",
                    "label": 0
                },
                {
                    "sent": "This is only the first 3 / 2 variables have once and the rest the over 2 have zeros and mu subject is same as zero vector.",
                    "label": 0
                },
                {
                    "sent": "So only the first day over 2 variables are relevant and the rest are not relevant.",
                    "label": 0
                },
                {
                    "sent": "And the nominal level is 5%, so if the asymptotics distributions really valid, we expect to see that type an error rates will be very close to the nominal level, 5%, which is indicated by this red.",
                    "label": 0
                },
                {
                    "sent": "Dotted line OK. And here we can see that when the dimension is relatively small, 6 or 12.",
                    "label": 0
                },
                {
                    "sent": "For different values of the sample size.",
                    "label": 1
                },
                {
                    "sent": "Type an error rates are pretty close to the nominal level.",
                    "label": 0
                },
                {
                    "sent": "OK, yet when the dimension is very large for.",
                    "label": 0
                },
                {
                    "sent": "Fordyce result a synthetic result to be valid sample size has to be very large.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is 1 application of the asymptotical analysis of linear support vector mission and also we can use this synthetic distribution to compare different procedures.",
                    "label": 0
                },
                {
                    "sent": "So let me talk about.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to talk about relative efficiency of 1 method compared to?",
                    "label": 0
                },
                {
                    "sent": "Others using this, uh, synthetic distribution.",
                    "label": 0
                },
                {
                    "sent": "And in fact at front did some comparison between logistic regression and linear discriminant analysis after getting or synthetic distribution for the coefficients of logistic regression and LDA.",
                    "label": 1
                },
                {
                    "sent": "And we can really follow the same approach to compare linear support vector machine with other procedures.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the Canonical LDA setting that up front considered.",
                    "label": 0
                },
                {
                    "sent": "So here we have again two normal distributions.",
                    "label": 0
                },
                {
                    "sent": "Which means given by this, so two distributions are Delta part only in the first coordinate and the means are the same for the rest OK. And in this example, the optimal discriminant linear discriminant function is given by this with INTERCEPT.",
                    "label": 1
                },
                {
                    "sent": "Equal to logo \u03c0 + / \u03c0 minus and the normal vector is Delta times E1.",
                    "label": 0
                },
                {
                    "sent": "Keep an affront derived.",
                    "label": 0
                },
                {
                    "sent": "The following expression for.",
                    "label": 0
                },
                {
                    "sent": "Expectation of regret or expectations.",
                    "label": 0
                },
                {
                    "sent": "Increased error rates compared to the Bayes error rate.",
                    "label": 0
                },
                {
                    "sent": "When you have linear discrete math, so to say L hat with coefficient vector that has TA.",
                    "label": 0
                },
                {
                    "sent": "And the coefficient vector has a symptomatic normal distribution with center better star and say variance covariance matrix Sigma K. Then here we can see that this expectation of regret or expectation of increased error rate is a function of those variances in the limit.",
                    "label": 0
                },
                {
                    "sent": "In the covariance matrix of the limit distribution and some other model parameters.",
                    "label": 0
                },
                {
                    "sent": "So using this result, now we can compare different linear discriminant procedures with different or synthetic distributions.",
                    "label": 0
                },
                {
                    "sent": "So in his.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "Efron studded or synthetic volitive efficiency up logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Two LDA normal discrimination and he defined or synthetic relative efficiency as the limit of the ratio of expected increase error rate of LDA 2.",
                    "label": 0
                },
                {
                    "sent": "The expectation increase error rate of logistic regression.",
                    "label": 1
                },
                {
                    "sent": "And he found that this regression is between 1/2 and 2/3 as effective as LDA in typical setting.",
                    "label": 1
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "To achieve same accuracy for logistic regression, you need twice to 1.5 times as larger sample as.",
                    "label": 0
                },
                {
                    "sent": "LD K. The following the same approach.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compared linear support vector machine with LDA K and this is under under.",
                    "label": 0
                },
                {
                    "sent": "Same Canonical LDA setting with.",
                    "label": 1
                },
                {
                    "sent": "Equal probability for two classes.",
                    "label": 0
                },
                {
                    "sent": "And it can be shown that the synthetic will lot of efficiency of linear support vector machine 2, LDA is given by this expression.",
                    "label": 0
                },
                {
                    "sent": "Here Delta is the distance between 2 means and P is the PDF of standard normal distribution an A star is the constant satisfying this equation K. So we consider the different values of Delta which is in this case actually same as the model is distance.",
                    "label": 0
                },
                {
                    "sent": "And this column shows the corresponding Bayes error rates from very easy to relatively hard problems.",
                    "label": 0
                },
                {
                    "sent": "an A star.",
                    "label": 0
                },
                {
                    "sent": "The other values.",
                    "label": 1
                },
                {
                    "sent": "Satisfying this equation and this column shows that our synthetic relative efficiencies of linear support vector machine to LDA ranging from 76% to about 29% OK. And for comparison I just reproduced results in reference paper about our synthetic relative efficiencies of logistics regression to LDA.",
                    "label": 0
                },
                {
                    "sent": "And here you can see that.",
                    "label": 0
                },
                {
                    "sent": "The relative efficiencies of support vector machine are lower than those of logistic regression, and of course lower than LDA, and this is not too surprising because LDA LDA uses specific information about multi variate normal distributions, and this computation is under the ideal LDA setting.",
                    "label": 0
                },
                {
                    "sent": "Hey Brother, um, interesting question.",
                    "label": 0
                },
                {
                    "sent": "Is something like this?",
                    "label": 0
                },
                {
                    "sent": "How these three methods compare in some other setting different from idea LDA setting, yet we have still linear boundary as the optimal decision boundary and.",
                    "label": 0
                },
                {
                    "sent": "Finding such a case was in as easy as I initially thought about.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is such a situation and here's an example.",
                    "label": 0
                },
                {
                    "sent": "OK, so here for each class, now we have mixture of two Gaussian distributions.",
                    "label": 1
                },
                {
                    "sent": "So we have two Gaussian components with mean mu sub, F on and music F2.",
                    "label": 0
                },
                {
                    "sent": "Likewise we have two components for negative class with mean vectors, musaab, gewonnen, mu subject 2.",
                    "label": 0
                },
                {
                    "sent": "And here there's four points.",
                    "label": 0
                },
                {
                    "sent": "They form a rectangle OK, and.",
                    "label": 0
                },
                {
                    "sent": "Delta W. Refers to the mean difference within each class, and teletubby refers to the mean distance between two classes.",
                    "label": 1
                },
                {
                    "sent": "K depending on pipe, Lawson pie minus you could have classification boundary closer to whatever the majority class.",
                    "label": 0
                },
                {
                    "sent": "So initially we thought as Delta W increases since that indicates more deviation from ideal LDA setting without logistic regression or a support vector machine would come out to be better than LDA.",
                    "label": 0
                },
                {
                    "sent": "And this setting is more complicated than the simple LDA settings, so we carried out some simulation to verify this conjecture.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that.",
                    "label": 0
                },
                {
                    "sent": "The error rates of LDA were the smallest among these three methods.",
                    "label": 0
                },
                {
                    "sent": "When we varied the distance between 2 means within each class from one to four, and in this example we can show that the page error, it doesn't really change.",
                    "label": 0
                },
                {
                    "sent": "So in terms of base error rate.",
                    "label": 0
                },
                {
                    "sent": "This mixture problem is the same as the idea LDA problem.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "LDA, so best and.",
                    "label": 0
                },
                {
                    "sent": "Load is regression.",
                    "label": 0
                },
                {
                    "sent": "Second, an linear support vector machine, the third.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We varied Delta be of course, depending on Delta be we have different error rates, so the larger the Delta be the mean distance between two classes, the smaller the Bayes error rate.",
                    "label": 0
                },
                {
                    "sent": "So the black line is again the Bayes error rate and.",
                    "label": 1
                },
                {
                    "sent": "Red for LDA, Green for Lotus regression, an blue for support vector mission.",
                    "label": 0
                },
                {
                    "sent": "In this case 3 methods are about the same, except when Delta B is very large in.",
                    "label": 0
                },
                {
                    "sent": "Again, LDA came out to be the best.",
                    "label": 0
                },
                {
                    "sent": "OK, and later we learned that there is some reason why LDA shows some robustness in this particular city.",
                    "label": 0
                },
                {
                    "sent": "And my students and I were a bit frustrated in that despite all the empirical successes of linear support vector machine, we couldn't quite get decisive numerical evidence that shows support vector Machine works very well in this controlled setting.",
                    "label": 0
                },
                {
                    "sent": "However, when we varied the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We got very interesting results, so here the sample size is 100 and we very dimension from the five to about 90.",
                    "label": 0
                },
                {
                    "sent": "And as you can see here, when the dimension is relatively small compared to sample size, again LDS are best followed by.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression and support recognition.",
                    "label": 0
                },
                {
                    "sent": "But as the dimension increases, the error rate of support vector machine didn't really grow as fast as those error rate sub.",
                    "label": 0
                },
                {
                    "sent": "Noticed regression and LDA.",
                    "label": 0
                },
                {
                    "sent": "OK, this is pretty interesting result.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me come.",
                    "label": 0
                },
                {
                    "sent": "Conclude my talk by giving some room.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk I examined some of synthetic properties of coefficients of linear support vector machine and studied it spell volitive efficiency in comparison with other classical approaches to classification through probability estimation, such as logistical regression and LDA.",
                    "label": 0
                },
                {
                    "sent": "An this comparison was through powder type representation of the coefficients.",
                    "label": 1
                },
                {
                    "sent": "And the synthetic result shows how the margins of optimal hyperplane given by the support vector mission.",
                    "label": 1
                },
                {
                    "sent": "And on the line know how the margins they are.",
                    "label": 0
                },
                {
                    "sent": "Margins and underlying probability distributions determine the statistical behavior of the coefficient vector, and I showed possibility of using hypothesis testing for variable selection for linear support vector machine.",
                    "label": 1
                },
                {
                    "sent": "With the synthetic distribution of the coefficient vector, although for practical application of this approach to variable selection, we need some consistent estimators of this Geo Bannister and H business OK. And it became very clear that the situation where the support vector machine has an edge over.",
                    "label": 0
                },
                {
                    "sent": "Although classical approaches lodis regression and LDA involve high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "So for more interesting theoretical analysis, we need to consider different mode of our synthetics where the dimension also grows with sample sites.",
                    "label": 0
                },
                {
                    "sent": "Came and finally extension of this linear support vector machine or synthetics to nonlinear case would be very interesting.",
                    "label": 0
                },
                {
                    "sent": "Future direction, right?",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention and.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you're interested in some details of this work, then you can take a look at the paper that I wrote with KU and others which appeared in Journal of Machine Learning Research.",
                    "label": 1
                },
                {
                    "sent": "And for the later part, volitive efficiency analysis an the related numerical results are from joint work.",
                    "label": 1
                },
                {
                    "sent": "With my student we won, so this is work in progress and we don't have any paper yet.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Can we put this back?",
                    "label": 0
                },
                {
                    "sent": "So if you have read the original paper by Vapnik and others that proposed support vector machine in the beginning, there is comment about the comparison between LDA and support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So in LDA as a dimension increases, the number of parameters that you have to estimate increases very rapidly.",
                    "label": 0
                },
                {
                    "sent": "Is that so?",
                    "label": 0
                },
                {
                    "sent": "I think one reason why for high dimensional case support vector Machine Works works better than LDA, yet I cannot quite figure out another puzzle.",
                    "label": 0
                },
                {
                    "sent": "That for logistic regression again that has same number of parameters as support vector machine.",
                    "label": 0
                },
                {
                    "sent": "That doesn't really work well, so that's part of.",
                    "label": 0
                },
                {
                    "sent": "Future problems that I need to look at.",
                    "label": 0
                },
                {
                    "sent": "Dimension gross.",
                    "label": 0
                },
                {
                    "sent": "LDS, It plays the estimation of it.",
                    "label": 0
                },
                {
                    "sent": "Have you tried like this?",
                    "label": 0
                },
                {
                    "sent": "Have this regular.",
                    "label": 0
                },
                {
                    "sent": "Tried.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's that's also very good question.",
                    "label": 0
                },
                {
                    "sent": "So the results that I presented.",
                    "label": 0
                },
                {
                    "sent": "To merely include regularization, although for support vector machine, even when we tune the regularization parameter we had about the same result.",
                    "label": 0
                },
                {
                    "sent": "So at first we were very curious whether the improvement of SVM over other procedures when dimension increases, whether that is due to regularization or simply the particular loss that we use.",
                    "label": 0
                },
                {
                    "sent": "But partial answer that I have right now is maybe it's due to.",
                    "label": 0
                },
                {
                    "sent": "The specific mechanism that you used, but now I'm very curious whether when we regularize LDA we we get any better result and how three methods would compare in that case.",
                    "label": 0
                }
            ]
        }
    }
}