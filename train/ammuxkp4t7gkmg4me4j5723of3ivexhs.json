{
    "id": "ammuxkp4t7gkmg4me4j5723of3ivexhs",
    "title": "Learning Structural Support Vector Machines with Latent Variables",
    "info": {
        "author": [
            "Chun-Nam Yu, Department of Computer Science, Cornell University"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/siso08_yu_lssv/",
    "segmentation": [
        [
            "So the title of my talk is learning structure, SVM, certain variables and this is joint work with my advisor, Torsten Yokums done, then call Now Universe."
        ],
        [
            "So latent variable models are used very widely.",
            "User in general in statistics and machine learning.",
            "For example, they can be used to represent an observed quantities and experiments.",
            "For example, like intelligence.",
            "And also we can use latent variable models to do dimensionality reduction to control the number of degrees of freedom we want to have in our experimental data.",
            "And there are many class examples, for example like packing, analysis, mixture models and principal component analysis.",
            "And in this work we are going to focus on, particularly on the use of latent variables in a structured output predictions."
        ],
        [
            "So here is an motivating example.",
            "Here we have a passage containing many different noun phrases.",
            "And we want to cluster the noun phrases into a coreference clusters, meaning that, for example, in this particular example, we've got 2 numbers clusters, one of them is John Simon, which is a person, and then the other one is the prime code, which is an organization an.",
            "We can formulate this as a structured prediction task.",
            "So the input would be the number system selves and the associated features.",
            "For example, whether they are proper nouns or whether they are the gender of the noun.",
            "And then the label.",
            "Why would be the clusters themselves?",
            "The cluster of the correct noun phrases?",
            "And when we human tried?",
            "Do this task nonphrasal weapons.",
            "Sometimes it's hard to determine whether a particular phrase are the top of the document.",
            "Weather in school reference with another noun phrase at the bottom of the end of the document.",
            "So usually what we do is like we use make use of transitivity.",
            "When we make these kinds of inferences of deciding whether to phrases I could wrap it or not.",
            "And these type of like links that connect these noun phrases together and a single cluster is not observed in our given labels.",
            "But they are very useful when we're trying to do prediction an interface corrections."
        ],
        [
            "So there are many previous works of like using latent variable models and structured prediction.",
            "For example, in the generated case, the hidden Markov models are just everywhere in speech recognition and both metrics and it's very natural to incorporate latent variables and missing values in these generative models.",
            "And then recently there are many, many works in the discriminative structure, learning with latent variables.",
            "For example, there's work on hidden variable CRF in object recognition and in that particular paper the authors use the latent variables to represent the possible objects to help recognition task.",
            "And there's also work with probabilistic graphical probabilistic context free grammar, but later annotation for pausing an in that particular work there using the orders are using.",
            "The latent variables to represent a mixture distribution to represent to model the different types of parts of speech text.",
            "As a. Grammar refinement and then on a slightly different line of work.",
            "There's also like semi supervised structure SVM.",
            "Recent papers in Isaiah mode."
        ],
        [
            "So many of the previous works are based on public probabilistic models.",
            "For example, like you already invited talk this morning was also everything is all based on turning.",
            "SVM or Markov model network into a probabilistic setting, and then at the prior to.",
            "And what we're trying to do here is we are trying to directly.",
            "Introduce latent variables into structural SVM setting and there are many interesting questions to ask in this particular problem.",
            "For example, what kind of changes do we need to make when we're trying to change the joint vision vector and the loss functions?",
            "And how are we going to deal with a potentially nonconvex objective?",
            "And then there's also the question of whether there are any changes to inference procedures and training and testing."
        ],
        [
            "So in conventional structural SVM we learn a linear prediction room, which is just the F of X which is from parameterized by weight vector W, which is the Max over all the other structures, why?",
            "Anna Linear scoring function W in the product with a joint feature vector fee of between X&Y.",
            "And now we are saying that the relation between X&Y is not completely described by X&Y alone and need to.",
            "We need to supplement a set of latent variables to complete the description between X&Y.",
            "And so we extend to join feature vector with H. And we propose a very simple new protection which is just F of X equals white bar, where yhey equals to APMEX.",
            "So join up mix between the set of output variables and latent variables H over this particular new linear scoring function, which is also a inner product between the W and the fee of XY and H. And you can imagine that in a public setting we can have like alternative like in France for example by integrating out the latent variables But this.",
            "Particular setting with structure SPM the taking the out negative.",
            "Easier to incorporate into the framework and also in many cases the inference is easy."
        ],
        [
            "So this is our proposal for the latent variable structure SPM, and it's actually very similar to the original structure.",
            "Is game setting.",
            "The objective is actually the same, so you have a regularizer on the weight vector W and you also have some.",
            "Regularization constancy with.",
            "Times the sum of the slack variables I I, which upper bounds your loss.",
            "And then what is different is the set of constraints here.",
            "So here we have.",
            "For all examples from one to N and all four possible output structures Wyatt.",
            "We are requiring that the maximum.",
            "Score for the correct structures exactly why I completed by the best explained by the best set of latent variables has to be graded by by any alternative output prediction YN but completed by the best possible latent variable explanation.",
            "And it has to be greater benefit by certain margin.",
            "Why measured by the loss function Delta between Y&YN and we also have a slack variable.",
            "Because sometimes the data is not separable.",
            "So.",
            "What is are interesting here is that although we have extended the joint venture map with latent variable H, the we are still restricting the loss function so that it it only depends on the label output label yny head.",
            "An the."
        ],
        [
            "So here is the reason for this so.",
            "If we make the assumption that the losses only depends on the Y&Y head, this is we can apply the same trick for bounding the prediction loss by using this.",
            "Law cemented kind of law cemented bound.",
            "So what this bound is?",
            "Doing is that it replaces this complex loss function, which could be a continuous and discrete by piecewise linear maximum over all possible output structures.",
            "And that's why.",
            "Optimization problems, stress beam convex and also tractable.",
            "And.",
            "The reason why we are making this particular assumption on the loss function being independent operating variable H is that if we insist that the loss function has to take into account, then we introduce extra dependencies between the loss function in the loss, Amanda inference bound and.",
            "Yes, possible completion of the latent variable H in the correct label.",
            "Why I and this will breaks down this particular of bound.",
            "And by making this particular assumption, we so there are some restrictions, so we cannot do.",
            "For example, we do semi supervised learning, for example, because now the loss function that depend on the latent variables.",
            "But still this particular version is still applicable, applicable to many problems that I've mentioned in the introduction."
        ],
        [
            "So.",
            "So after the.",
            "The position decomposition in the previous slide, the objective is still nonconvex an to solve this particular optimization problem.",
            "We're using the constraint concave convex procedure, which has also been used in many other machine learning papers, so the basic idea is actually quite simple and quite nice.",
            "So what we have is that we have a nonconvex objective.",
            "And we decompose it into a sum of convex part and the concave part.",
            "And then we what we do is that at each iteration we upper bound the concave part with a linear hyperplane.",
            "And then we optimize the sum of this convex party plus this particular linear hyperplane, and this is a convex optimization problem and we guarantee at each iteration we are guaranteed to obtain some improvement in the objective, and so we can just iterate this until convergence."
        ],
        [
            "So what does the CCP algorithm translate into our particular formulation so we can actually write our objective?",
            "As a sum of convex part.",
            "So this is, uh, so these expressions are like what the slack variables I look like after we expanded Italians.",
            "So this is the convex part and this is the.",
            "So minus a convex part, which is a concave part.",
            "And so the operation of computing the hybrid upper bounding hyperplanes is actually equivalent to a very simple operation of completing the latent variables for the correct labels.",
            "Why?",
            "And so this is.",
            "Very simple.",
            "And then after we complete the latent variables for H, then the optimization problem becomes convex and so we can solve this particular optimization problem just using any cutting plane algorithm that many of the speakers discussed before.",
            "So intuitively, what this algorithm is doing is actually it's trying to impute the label for the hidden, complete the latent variable for the correct label.",
            "Why I and then treat them as the correct label and then optimize?",
            "And then we and then using the new weight vector and then we input the latent variables and iterate is until convergence."
        ],
        [
            "So this is a summary of the algorithm we're trying to solve.",
            "This particular optimization problem using the CP algorithm, and we have a particular assumption on the restriction of the loss function.",
            "And then we have got three late related inference problem.",
            "The first one is the prediction problem, which is a joint inference between the output and the hidden variable and we also have a loss of mental inferences in structure SVM.",
            "And then we also have a new inference problem which is the latent variable completion.",
            "Anne.",
            "And we designed them.",
            "Correctly or like depending on what kind of problem we're trying to model, we can these three inference problems usually can be solved using the same type of algorithm."
        ],
        [
            "So here is an application that motivates our development of latent variables.",
            "So what we have here is we have two sets of DNA sequences from two different species, and these codes via an library.",
            "And then they all contain some autonomous replicating sequences called arts.",
            "So what we're interested is the booty that is responsible for these replicating process an.",
            "And then what about it?",
            "Just find is that.",
            "If you put the ask in the first species cervisia into the second species Carberry, some of them is still functional, or some of them lost their functions.",
            "So if there still functional and in the case that there still the multi in the library species if it's not functional, then the movie is probably has mutated to something that's not, that is two different for the biological process too.",
            "So.",
            "The data we have we have we given these sequences and we also have the labels or whether the.",
            "Papers, functional or not, and so the problem in this case is we want to find out the.",
            "Classify these sequences and we also want to find out the.",
            "I guess I'm not knowledge information about the position of these motifs."
        ],
        [
            "And so the hidden variable and later very well in this case, is that we don't know.",
            "Although we have labels over these sequences, DNA sequences, we don't know exactly where the multiples are, so this age, particular age, is the position of the sequence that is present.",
            "And then the feature vector V depends on the latent variable H because.",
            "We scored the motif using a particular position specific weight matrix, while we score all the other positions using just a background model.",
            "And then in this particular problem, the loss function Delta is just the 01 loss, and then the latent variables are, as I said, is just the position of the motif in the sequence.",
            "And so, because of the particular of this problem, we only have a binary label and we only have a linearly many possible positions for the latent variable H. So the joint inference and the lowest moment inference problems can be solved very efficiently, because basically you can enumerate all the possibilities in this case."
        ],
        [
            "So here is our experiment.",
            "We've got about 200 S DNA sequences from our collaborators.",
            "And then we also have some sequences for background estimation.",
            "And then we do a 10 fold cross validation and 10 random restarts for each parameter setting over the regularization parameter C. And then he is our results.",
            "So the Gibbs sampler results are given to us by the our collaborators.",
            "And they believe that the width of 11 and 17 they can.",
            "They believe that they have some signals over there, so.",
            "And there are good movies, and so we use them for classification.",
            "Classifying the sequences an we're talking about.",
            "For these seven 235% error rate and the baseline is around 40% of error rate and our leading variable SVM for 11 and 17 is doing much better than the get some identity found by the Gibbs sampler.",
            "By a lot.",
            "Anne.",
            "Right now we're so we've got very good classification accuracy, but right now we're working on ways to interpret the motif signals because the weights from the weight vector in the latent variables, such as VM, is kind of difficult to interpret compared to probabilistic models and.",
            "And is also.",
            "One of the reason why it's doing so much better is also a because it seems to be capturing multiple signals at multiple places that differentiate the positive and negative sets, and so the signals are not as clean as the.",
            "As the accomm unsupervised setting but Gibbs sampler."
        ],
        [
            "So here's the conclusion.",
            "We have proposed a general formulation of structural SVM where they can vary books and efficient algorithm for solving the problem and the results on discriminative finding in each DNA indicates the proposed algorithm is quite promising and then right now we're working on other applications applications to test our algorithm further suggests clustering phone numbers for reference.",
            "And there's also other interesting questions for further research, for example, like how we can extend this approach to slightly scaling, because our algorithm actually depends crucially on the linear.",
            "I mean the loss being able to be linear, decomposed, and in the like rescaling case.",
            "The problem is more difficult.",
            "Yep.",
            "Thank you."
        ],
        [
            "Actually CPR this year.",
            "There is a paper on Legion as well.",
            "I'm headed back to the same formulation.",
            "Yeah, so so you define your loss function early on the observed level so so, but in prediction you just actually predict the hidden variables.",
            "So I mean, this is some kind of inconsistent, so, so what's your intuitive intuition about this formulation of the?",
            "Yeah, we.",
            "We designed a design, then we won't actually in many cases we only care about the loss between our production.",
            "But in this particular setting the biologist we work with actually wants to learn the hidden variables as well.",
            "So of course there's some inconsistency.",
            "I made that some inconsistency in here, but then we also want to work towards like how to actually extract these signals in here.",
            "It is not necessary to predict that it doesn't predict the hidden variables and actually if you think about it is not even clear even if you give it your given, the position of the multi.",
            "How you can construct a good loss function out of it for example?",
            "All the distances between positions probably doesn't work for these sequences.",
            "Yeah, number.",
            "How many days in Redwood City, without how many latent variables?",
            "I think that the algorithm doesn't have any restriction on that, yeah?",
            "There's definitely a problem.",
            "There can be problems with overheating and also like local minima if you use a little bit like you had mentioned.",
            "Function of the native variable.",
            "Wondering whether using paper marks as a loss function relies on with the fact that function is unimodal.",
            "Related unit volume district.",
            "Verify function of the latent variable fits on most models and wait, please delete this one.",
            "Yeah, I think yeah we are.",
            "We would expect that if the distribution of the latent variable is actually peaked around the like, as you said in the model, then this method would work better, and if so, yeah, yeah.",
            "If it's multimodal, then it would probably be.",
            "Motor vehicle.",
            "Yeah, yeah, actually actually about this question.",
            "You can also use another approach to to to influence the distribution of the hidden variable.",
            "We do not need to use the party mode of the distribution to todo todo the incorporation of everything OK?",
            "So yeah, I think it would be.",
            "I should we have?",
            "We have done some similar what we do to incorporate femur instructional swim.",
            "You reuse this probability method.",
            "So so in our problem we can influence the distribution operating level.",
            "So when you do particular just marginalized over there.",
            "OK, yeah yeah.",
            "Yeah, in that case you do not need to produce anyway.",
            "I think that's another possible approach.",
            "So the approach that we've taken here is like non probabilistic and different.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the title of my talk is learning structure, SVM, certain variables and this is joint work with my advisor, Torsten Yokums done, then call Now Universe.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So latent variable models are used very widely.",
                    "label": 0
                },
                {
                    "sent": "User in general in statistics and machine learning.",
                    "label": 1
                },
                {
                    "sent": "For example, they can be used to represent an observed quantities and experiments.",
                    "label": 0
                },
                {
                    "sent": "For example, like intelligence.",
                    "label": 1
                },
                {
                    "sent": "And also we can use latent variable models to do dimensionality reduction to control the number of degrees of freedom we want to have in our experimental data.",
                    "label": 0
                },
                {
                    "sent": "And there are many class examples, for example like packing, analysis, mixture models and principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "And in this work we are going to focus on, particularly on the use of latent variables in a structured output predictions.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an motivating example.",
                    "label": 0
                },
                {
                    "sent": "Here we have a passage containing many different noun phrases.",
                    "label": 1
                },
                {
                    "sent": "And we want to cluster the noun phrases into a coreference clusters, meaning that, for example, in this particular example, we've got 2 numbers clusters, one of them is John Simon, which is a person, and then the other one is the prime code, which is an organization an.",
                    "label": 0
                },
                {
                    "sent": "We can formulate this as a structured prediction task.",
                    "label": 0
                },
                {
                    "sent": "So the input would be the number system selves and the associated features.",
                    "label": 0
                },
                {
                    "sent": "For example, whether they are proper nouns or whether they are the gender of the noun.",
                    "label": 0
                },
                {
                    "sent": "And then the label.",
                    "label": 0
                },
                {
                    "sent": "Why would be the clusters themselves?",
                    "label": 0
                },
                {
                    "sent": "The cluster of the correct noun phrases?",
                    "label": 1
                },
                {
                    "sent": "And when we human tried?",
                    "label": 1
                },
                {
                    "sent": "Do this task nonphrasal weapons.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's hard to determine whether a particular phrase are the top of the document.",
                    "label": 0
                },
                {
                    "sent": "Weather in school reference with another noun phrase at the bottom of the end of the document.",
                    "label": 0
                },
                {
                    "sent": "So usually what we do is like we use make use of transitivity.",
                    "label": 0
                },
                {
                    "sent": "When we make these kinds of inferences of deciding whether to phrases I could wrap it or not.",
                    "label": 0
                },
                {
                    "sent": "And these type of like links that connect these noun phrases together and a single cluster is not observed in our given labels.",
                    "label": 1
                },
                {
                    "sent": "But they are very useful when we're trying to do prediction an interface corrections.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are many previous works of like using latent variable models and structured prediction.",
                    "label": 0
                },
                {
                    "sent": "For example, in the generated case, the hidden Markov models are just everywhere in speech recognition and both metrics and it's very natural to incorporate latent variables and missing values in these generative models.",
                    "label": 1
                },
                {
                    "sent": "And then recently there are many, many works in the discriminative structure, learning with latent variables.",
                    "label": 0
                },
                {
                    "sent": "For example, there's work on hidden variable CRF in object recognition and in that particular paper the authors use the latent variables to represent the possible objects to help recognition task.",
                    "label": 0
                },
                {
                    "sent": "And there's also work with probabilistic graphical probabilistic context free grammar, but later annotation for pausing an in that particular work there using the orders are using.",
                    "label": 0
                },
                {
                    "sent": "The latent variables to represent a mixture distribution to represent to model the different types of parts of speech text.",
                    "label": 1
                },
                {
                    "sent": "As a. Grammar refinement and then on a slightly different line of work.",
                    "label": 0
                },
                {
                    "sent": "There's also like semi supervised structure SVM.",
                    "label": 0
                },
                {
                    "sent": "Recent papers in Isaiah mode.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So many of the previous works are based on public probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "For example, like you already invited talk this morning was also everything is all based on turning.",
                    "label": 0
                },
                {
                    "sent": "SVM or Markov model network into a probabilistic setting, and then at the prior to.",
                    "label": 0
                },
                {
                    "sent": "And what we're trying to do here is we are trying to directly.",
                    "label": 0
                },
                {
                    "sent": "Introduce latent variables into structural SVM setting and there are many interesting questions to ask in this particular problem.",
                    "label": 1
                },
                {
                    "sent": "For example, what kind of changes do we need to make when we're trying to change the joint vision vector and the loss functions?",
                    "label": 0
                },
                {
                    "sent": "And how are we going to deal with a potentially nonconvex objective?",
                    "label": 1
                },
                {
                    "sent": "And then there's also the question of whether there are any changes to inference procedures and training and testing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conventional structural SVM we learn a linear prediction room, which is just the F of X which is from parameterized by weight vector W, which is the Max over all the other structures, why?",
                    "label": 1
                },
                {
                    "sent": "Anna Linear scoring function W in the product with a joint feature vector fee of between X&Y.",
                    "label": 0
                },
                {
                    "sent": "And now we are saying that the relation between X&Y is not completely described by X&Y alone and need to.",
                    "label": 0
                },
                {
                    "sent": "We need to supplement a set of latent variables to complete the description between X&Y.",
                    "label": 0
                },
                {
                    "sent": "And so we extend to join feature vector with H. And we propose a very simple new protection which is just F of X equals white bar, where yhey equals to APMEX.",
                    "label": 0
                },
                {
                    "sent": "So join up mix between the set of output variables and latent variables H over this particular new linear scoring function, which is also a inner product between the W and the fee of XY and H. And you can imagine that in a public setting we can have like alternative like in France for example by integrating out the latent variables But this.",
                    "label": 0
                },
                {
                    "sent": "Particular setting with structure SPM the taking the out negative.",
                    "label": 0
                },
                {
                    "sent": "Easier to incorporate into the framework and also in many cases the inference is easy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our proposal for the latent variable structure SPM, and it's actually very similar to the original structure.",
                    "label": 0
                },
                {
                    "sent": "Is game setting.",
                    "label": 0
                },
                {
                    "sent": "The objective is actually the same, so you have a regularizer on the weight vector W and you also have some.",
                    "label": 0
                },
                {
                    "sent": "Regularization constancy with.",
                    "label": 0
                },
                {
                    "sent": "Times the sum of the slack variables I I, which upper bounds your loss.",
                    "label": 0
                },
                {
                    "sent": "And then what is different is the set of constraints here.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "For all examples from one to N and all four possible output structures Wyatt.",
                    "label": 1
                },
                {
                    "sent": "We are requiring that the maximum.",
                    "label": 0
                },
                {
                    "sent": "Score for the correct structures exactly why I completed by the best explained by the best set of latent variables has to be graded by by any alternative output prediction YN but completed by the best possible latent variable explanation.",
                    "label": 1
                },
                {
                    "sent": "And it has to be greater benefit by certain margin.",
                    "label": 0
                },
                {
                    "sent": "Why measured by the loss function Delta between Y&YN and we also have a slack variable.",
                    "label": 0
                },
                {
                    "sent": "Because sometimes the data is not separable.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What is are interesting here is that although we have extended the joint venture map with latent variable H, the we are still restricting the loss function so that it it only depends on the label output label yny head.",
                    "label": 0
                },
                {
                    "sent": "An the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the reason for this so.",
                    "label": 0
                },
                {
                    "sent": "If we make the assumption that the losses only depends on the Y&Y head, this is we can apply the same trick for bounding the prediction loss by using this.",
                    "label": 1
                },
                {
                    "sent": "Law cemented kind of law cemented bound.",
                    "label": 0
                },
                {
                    "sent": "So what this bound is?",
                    "label": 0
                },
                {
                    "sent": "Doing is that it replaces this complex loss function, which could be a continuous and discrete by piecewise linear maximum over all possible output structures.",
                    "label": 0
                },
                {
                    "sent": "And that's why.",
                    "label": 0
                },
                {
                    "sent": "Optimization problems, stress beam convex and also tractable.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The reason why we are making this particular assumption on the loss function being independent operating variable H is that if we insist that the loss function has to take into account, then we introduce extra dependencies between the loss function in the loss, Amanda inference bound and.",
                    "label": 0
                },
                {
                    "sent": "Yes, possible completion of the latent variable H in the correct label.",
                    "label": 1
                },
                {
                    "sent": "Why I and this will breaks down this particular of bound.",
                    "label": 0
                },
                {
                    "sent": "And by making this particular assumption, we so there are some restrictions, so we cannot do.",
                    "label": 1
                },
                {
                    "sent": "For example, we do semi supervised learning, for example, because now the loss function that depend on the latent variables.",
                    "label": 0
                },
                {
                    "sent": "But still this particular version is still applicable, applicable to many problems that I've mentioned in the introduction.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So after the.",
                    "label": 0
                },
                {
                    "sent": "The position decomposition in the previous slide, the objective is still nonconvex an to solve this particular optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We're using the constraint concave convex procedure, which has also been used in many other machine learning papers, so the basic idea is actually quite simple and quite nice.",
                    "label": 0
                },
                {
                    "sent": "So what we have is that we have a nonconvex objective.",
                    "label": 0
                },
                {
                    "sent": "And we decompose it into a sum of convex part and the concave part.",
                    "label": 1
                },
                {
                    "sent": "And then we what we do is that at each iteration we upper bound the concave part with a linear hyperplane.",
                    "label": 0
                },
                {
                    "sent": "And then we optimize the sum of this convex party plus this particular linear hyperplane, and this is a convex optimization problem and we guarantee at each iteration we are guaranteed to obtain some improvement in the objective, and so we can just iterate this until convergence.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does the CCP algorithm translate into our particular formulation so we can actually write our objective?",
                    "label": 0
                },
                {
                    "sent": "As a sum of convex part.",
                    "label": 0
                },
                {
                    "sent": "So this is, uh, so these expressions are like what the slack variables I look like after we expanded Italians.",
                    "label": 0
                },
                {
                    "sent": "So this is the convex part and this is the.",
                    "label": 1
                },
                {
                    "sent": "So minus a convex part, which is a concave part.",
                    "label": 1
                },
                {
                    "sent": "And so the operation of computing the hybrid upper bounding hyperplanes is actually equivalent to a very simple operation of completing the latent variables for the correct labels.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "And then after we complete the latent variables for H, then the optimization problem becomes convex and so we can solve this particular optimization problem just using any cutting plane algorithm that many of the speakers discussed before.",
                    "label": 1
                },
                {
                    "sent": "So intuitively, what this algorithm is doing is actually it's trying to impute the label for the hidden, complete the latent variable for the correct label.",
                    "label": 0
                },
                {
                    "sent": "Why I and then treat them as the correct label and then optimize?",
                    "label": 0
                },
                {
                    "sent": "And then we and then using the new weight vector and then we input the latent variables and iterate is until convergence.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a summary of the algorithm we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "This particular optimization problem using the CP algorithm, and we have a particular assumption on the restriction of the loss function.",
                    "label": 1
                },
                {
                    "sent": "And then we have got three late related inference problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is the prediction problem, which is a joint inference between the output and the hidden variable and we also have a loss of mental inferences in structure SVM.",
                    "label": 0
                },
                {
                    "sent": "And then we also have a new inference problem which is the latent variable completion.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And we designed them.",
                    "label": 0
                },
                {
                    "sent": "Correctly or like depending on what kind of problem we're trying to model, we can these three inference problems usually can be solved using the same type of algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an application that motivates our development of latent variables.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is we have two sets of DNA sequences from two different species, and these codes via an library.",
                    "label": 1
                },
                {
                    "sent": "And then they all contain some autonomous replicating sequences called arts.",
                    "label": 1
                },
                {
                    "sent": "So what we're interested is the booty that is responsible for these replicating process an.",
                    "label": 0
                },
                {
                    "sent": "And then what about it?",
                    "label": 0
                },
                {
                    "sent": "Just find is that.",
                    "label": 0
                },
                {
                    "sent": "If you put the ask in the first species cervisia into the second species Carberry, some of them is still functional, or some of them lost their functions.",
                    "label": 0
                },
                {
                    "sent": "So if there still functional and in the case that there still the multi in the library species if it's not functional, then the movie is probably has mutated to something that's not, that is two different for the biological process too.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The data we have we have we given these sequences and we also have the labels or whether the.",
                    "label": 0
                },
                {
                    "sent": "Papers, functional or not, and so the problem in this case is we want to find out the.",
                    "label": 1
                },
                {
                    "sent": "Classify these sequences and we also want to find out the.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm not knowledge information about the position of these motifs.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the hidden variable and later very well in this case, is that we don't know.",
                    "label": 0
                },
                {
                    "sent": "Although we have labels over these sequences, DNA sequences, we don't know exactly where the multiples are, so this age, particular age, is the position of the sequence that is present.",
                    "label": 0
                },
                {
                    "sent": "And then the feature vector V depends on the latent variable H because.",
                    "label": 0
                },
                {
                    "sent": "We scored the motif using a particular position specific weight matrix, while we score all the other positions using just a background model.",
                    "label": 1
                },
                {
                    "sent": "And then in this particular problem, the loss function Delta is just the 01 loss, and then the latent variables are, as I said, is just the position of the motif in the sequence.",
                    "label": 0
                },
                {
                    "sent": "And so, because of the particular of this problem, we only have a binary label and we only have a linearly many possible positions for the latent variable H. So the joint inference and the lowest moment inference problems can be solved very efficiently, because basically you can enumerate all the possibilities in this case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is our experiment.",
                    "label": 0
                },
                {
                    "sent": "We've got about 200 S DNA sequences from our collaborators.",
                    "label": 1
                },
                {
                    "sent": "And then we also have some sequences for background estimation.",
                    "label": 1
                },
                {
                    "sent": "And then we do a 10 fold cross validation and 10 random restarts for each parameter setting over the regularization parameter C. And then he is our results.",
                    "label": 1
                },
                {
                    "sent": "So the Gibbs sampler results are given to us by the our collaborators.",
                    "label": 0
                },
                {
                    "sent": "And they believe that the width of 11 and 17 they can.",
                    "label": 0
                },
                {
                    "sent": "They believe that they have some signals over there, so.",
                    "label": 0
                },
                {
                    "sent": "And there are good movies, and so we use them for classification.",
                    "label": 0
                },
                {
                    "sent": "Classifying the sequences an we're talking about.",
                    "label": 0
                },
                {
                    "sent": "For these seven 235% error rate and the baseline is around 40% of error rate and our leading variable SVM for 11 and 17 is doing much better than the get some identity found by the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "By a lot.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Right now we're so we've got very good classification accuracy, but right now we're working on ways to interpret the motif signals because the weights from the weight vector in the latent variables, such as VM, is kind of difficult to interpret compared to probabilistic models and.",
                    "label": 1
                },
                {
                    "sent": "And is also.",
                    "label": 0
                },
                {
                    "sent": "One of the reason why it's doing so much better is also a because it seems to be capturing multiple signals at multiple places that differentiate the positive and negative sets, and so the signals are not as clean as the.",
                    "label": 0
                },
                {
                    "sent": "As the accomm unsupervised setting but Gibbs sampler.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the conclusion.",
                    "label": 0
                },
                {
                    "sent": "We have proposed a general formulation of structural SVM where they can vary books and efficient algorithm for solving the problem and the results on discriminative finding in each DNA indicates the proposed algorithm is quite promising and then right now we're working on other applications applications to test our algorithm further suggests clustering phone numbers for reference.",
                    "label": 1
                },
                {
                    "sent": "And there's also other interesting questions for further research, for example, like how we can extend this approach to slightly scaling, because our algorithm actually depends crucially on the linear.",
                    "label": 0
                },
                {
                    "sent": "I mean the loss being able to be linear, decomposed, and in the like rescaling case.",
                    "label": 0
                },
                {
                    "sent": "The problem is more difficult.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually CPR this year.",
                    "label": 0
                },
                {
                    "sent": "There is a paper on Legion as well.",
                    "label": 0
                },
                {
                    "sent": "I'm headed back to the same formulation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so you define your loss function early on the observed level so so, but in prediction you just actually predict the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this is some kind of inconsistent, so, so what's your intuitive intuition about this formulation of the?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we.",
                    "label": 0
                },
                {
                    "sent": "We designed a design, then we won't actually in many cases we only care about the loss between our production.",
                    "label": 0
                },
                {
                    "sent": "But in this particular setting the biologist we work with actually wants to learn the hidden variables as well.",
                    "label": 0
                },
                {
                    "sent": "So of course there's some inconsistency.",
                    "label": 0
                },
                {
                    "sent": "I made that some inconsistency in here, but then we also want to work towards like how to actually extract these signals in here.",
                    "label": 0
                },
                {
                    "sent": "It is not necessary to predict that it doesn't predict the hidden variables and actually if you think about it is not even clear even if you give it your given, the position of the multi.",
                    "label": 0
                },
                {
                    "sent": "How you can construct a good loss function out of it for example?",
                    "label": 0
                },
                {
                    "sent": "All the distances between positions probably doesn't work for these sequences.",
                    "label": 0
                },
                {
                    "sent": "Yeah, number.",
                    "label": 0
                },
                {
                    "sent": "How many days in Redwood City, without how many latent variables?",
                    "label": 1
                },
                {
                    "sent": "I think that the algorithm doesn't have any restriction on that, yeah?",
                    "label": 0
                },
                {
                    "sent": "There's definitely a problem.",
                    "label": 0
                },
                {
                    "sent": "There can be problems with overheating and also like local minima if you use a little bit like you had mentioned.",
                    "label": 0
                },
                {
                    "sent": "Function of the native variable.",
                    "label": 0
                },
                {
                    "sent": "Wondering whether using paper marks as a loss function relies on with the fact that function is unimodal.",
                    "label": 0
                },
                {
                    "sent": "Related unit volume district.",
                    "label": 0
                },
                {
                    "sent": "Verify function of the latent variable fits on most models and wait, please delete this one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think yeah we are.",
                    "label": 0
                },
                {
                    "sent": "We would expect that if the distribution of the latent variable is actually peaked around the like, as you said in the model, then this method would work better, and if so, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "If it's multimodal, then it would probably be.",
                    "label": 0
                },
                {
                    "sent": "Motor vehicle.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, actually actually about this question.",
                    "label": 0
                },
                {
                    "sent": "You can also use another approach to to to influence the distribution of the hidden variable.",
                    "label": 0
                },
                {
                    "sent": "We do not need to use the party mode of the distribution to todo todo the incorporation of everything OK?",
                    "label": 0
                },
                {
                    "sent": "So yeah, I think it would be.",
                    "label": 0
                },
                {
                    "sent": "I should we have?",
                    "label": 1
                },
                {
                    "sent": "We have done some similar what we do to incorporate femur instructional swim.",
                    "label": 0
                },
                {
                    "sent": "You reuse this probability method.",
                    "label": 0
                },
                {
                    "sent": "So so in our problem we can influence the distribution operating level.",
                    "label": 0
                },
                {
                    "sent": "So when you do particular just marginalized over there.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in that case you do not need to produce anyway.",
                    "label": 0
                },
                {
                    "sent": "I think that's another possible approach.",
                    "label": 0
                },
                {
                    "sent": "So the approach that we've taken here is like non probabilistic and different.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}