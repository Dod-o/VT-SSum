{
    "id": "2nolwhmvms6prlh6e5aaoyagutd5sq63",
    "title": "Type-Constrained Representation Learning in Knowledge Graphs",
    "info": {
        "author": [
            "Denis Krompa\u00df, LMU Institut f\u00fcr Informatik, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "Nov. 10, 2015",
        "recorded": "October 2015",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2015_krompass_representation_learning/",
    "segmentation": [
        [
            "So hello everybody, thanks for the introduction.",
            "I just want to add this is joint work with Steven Beyer was also a PhD student.",
            "Our lab and focus trespass my PhD supervisor."
        ],
        [
            "So it's a short outline of my talk.",
            "Basically will have two parts.",
            "In the first part I will give a brief introduction, well to Knowledge Graph, where it is the structure, some applications and learning and knowledge graphs with latent variable models.",
            "In the last part I will talk about our contribution that is like integrating prior knowledge about relation types into latent variable models to actually improve link prediction under realistic settings.",
            "What I understand.",
            "Under a realistic setting, I will clarify that later in the talk."
        ],
        [
            "So to me as machine learner knowledge graphs is a database that stores facts about the world's relation between entities.",
            "Were entities, real world objects such as places or persons that are certain kind of relationships, and in addition knowledge graphs are also well often backed by a schema or an anthology that provide additional semantic information."
        ],
        [
            "And were given this knowledge graphs, we actually able to somehow introduce the notion of things and semantics into various applications that range from search engines to information extraction on unstructured structured text, but also very sophisticated question answering systems such as IBM Watson system or smart mobile assistance such as Apple Siri or Microsoft's Cortana."
        ],
        [
            "So as I said, I'm I have a machine learning background and the might arise a question why would need machine learning in the context of knowledge graphs.",
            "So first of all, knowledge graphs are not perfect where they're they're fairly incomplete, just to give an example, in DP and Freebase more than 60% of the persons do not have a place of birth.",
            "Facts can be wrong, outdated, and if you in addition consider the size of present knowledge graphs, it becomes pretty clear that this.",
            "Problems cannot be tackled manually, so you need automated automated approaches for that."
        ],
        [
            "And well, in this regard, latent variable models have been shown in the recent years to work quite well.",
            "They have been used for knowledge graph completion.",
            "So link prediction, clustering, disambiguation.",
            "These models do they?",
            "They learn latent embeddings for entities in relation types, and will."
        ],
        [
            "So next, how this works well in principle, so we are given with the knowledge graphs where the notes are the entities and the relation types identify well defined the edges in this graph and basically it's a multi graph.",
            "So each relation type defines a graph by its own and well of course internally it's a triple store and we also exploit this trip."
        ],
        [
            "Store and put it in a latent variable model and then it comes out with these embeddings for entities in relation types with what are in this case now in this example, just vectors of features and.",
            "These features have different values, so where, where do these features come from?",
            "So the trick is so before I trained the model I tell them I tell the model.",
            "OK, I want 10 features, for example, for example to describe entities.",
            "And then the model that takes the triples and the depending structured observed.",
            "Since the data in somehow balances the values of these different features, what we also correlated factors to somehow describe the entities and relation types and as a result, if we if an entity if two entities have very similar relationships in the graph, they end up to have very similar embeddings on the other side.",
            "If the two entities will have very different kinds of relationships in the graph.",
            "They will end up to be also very different in the will have different embeddings.",
            "So given this train model we can do different things."
        ],
        [
            "So first of all, we can just take this embeddings into link based clustering.",
            "So we took to embedding social entities and just ask.",
            "It's a similar.",
            "This can be used for this application, but also the link based clustering.",
            "Second we connect."
        ],
        [
            "Poison model and do simple link prediction.",
            "So we complete the graph and just predict new edges in the graph.",
            "Another thing."
        ],
        [
            "We could do with short this in a prior publication.",
            "Then we that we can also exploit some for probabilistic querying, so we can even ask more complex questions.",
            "So in this case we have query with multiple conditions or in this made up example if I would Einstein would like cats and dogs and not just just one."
        ],
        [
            "OK, but also these models have kind of problems when we look at really large knowledge graphs.",
            "So as I said, we learn actually embeddings for for every entity and well independence, how long this embeddings are.",
            "We run into problems here, so if we look at really large knowledge graph such as Freebase for example with 15 million entity topics topic entities.",
            "Then we will end up the model will have a lot of parameters and.",
            "If we would enforce like that, OK, we want a lot of latent factors, so we would have high dimensional embeddings.",
            "This is not possible.",
            "This infeasible becausw this will cause a lot of memory consumption of the model and because of the large amount of parameters, also training times becomes long or infeasible."
        ],
        [
            "So as a conclusion in large scale problems, we actually force to exploit less expressive models.",
            "This is kind of a dilemma.",
            "We cause we could expect when the graph grows bigger.",
            "Also the complexity rises."
        ],
        [
            "So our belief is that one solution to tackle this problem is to introduce prior knowledge about the learning task into the model.",
            "And."
        ],
        [
            "This is where we come to type constraints.",
            "So in schema based knowledge graphs we have well semantic description of relation types and the question is why?",
            "Why not mind them directly in this latent variable models?",
            "This is the whole idea.",
            "So for example, in the RDF schema we have these two concepts.",
            "It's domain and range that actually defined for relation types.",
            "The classes of entities that should be related as subject and as always object by a certain relation type.",
            "So in this case.",
            "Person knows relation type is defined.",
            "Its domain is defined for person entities.",
            "It's range for.",
            "Also for person entities.",
            "So this example triple that says that Max Planck knows I belanja time is valid in sense of the relation."
        ],
        [
            "But this example is not valid.",
            "So if we look at another relation type says born in which is defined over persons and places the relation between Max Planck and Albert mentioned doesn't make any sense.",
            "Of course, for humans this is.",
            "This is quite clear, but the machines we have to tell them explicitly and then with a schema based knowledge graph we have this knowledge there, so we're going to mine them."
        ],
        [
            "So the approach is basically as follows.",
            "We extract the triples from the knowledge graph and."
        ],
        [
            "Well, we also mind the schema.",
            "We extract the typing of entities and we extract the domain and range constraints from the schema and what we do then is doing the training.",
            "So most of this model how they work is that they they takes a triple that they observed and then to do negative sampling.",
            "So there's somehow try to just pick random random triples and assume that there are negative.",
            "But we make sure that the that this selection of negative triples is more selective, so we make sure that this this sample.",
            "This negative sampling only takes triples that agree with type constraints."
        ],
        [
            "So to show the generality of our approach, we covered multiple aspects in our experiments.",
            "So first of all, we integrated this prior knowledge in multiple.",
            "Latent variable models that cover the diversity of the current state of the art of these models that are applied to knowledge graphs for link prediction.",
            "So here I show three of them.",
            "It's rest card.",
            "It's sort of the tensor factorization that has been developed by a former colleague of mine, translation.",
            "Embeddings is a distance based method developed by borders and a neural network model that was exploited last year.",
            "In the Google Knowledge world project."
        ],
        [
            "The second aspect, we also constructed different datasets that somehow simulate different types of graphs where this models could be applied to.",
            "So for the first one we extracted a sample from Freebase that somehow simulates a general purpose knowledge graph in our viewer general purpose knowledge graph has multiple relations and each relation only relates a small fraction of entities which each other.",
            "Well, this is the next one.",
            "The DB Pedia music would be.",
            "Domain specific knowledge graph, where we have a lower low amount of relation types at all relate a large fraction of entities compared to the entities that are present in the graph and the last one will represent the high quality, high quality knowledge graph that we extracted from Yahoo.",
            "And of course, the third aspect is that we only allow in our experimental model to to exploit a very limited amount of parameters that would somehow.",
            "Somehow represent their their application.",
            "Two very large problems."
        ],
        [
            "So I will first explain this this graph.",
            "OK, so the Y axis is always the area under precision recall curve score.",
            "It's just a score to measure how well we're in predicting Nunu triples.",
            "The X axis is.",
            "Represents the complexity of the models or so.",
            "Here we allow the embeddings have a length of 10, which is really really low.",
            "100 is well, it's it's a little bit higher, but still quite low.",
            "The doc, a boss always is always some models that exploits the original model that doesn't exploit any prior knowledge about type constraints, and the lighter ones are always a model that exploits the type constraints, and we have the three datasets.",
            "And two things we can see here.",
            "So first of all, integrating type constraints really pays off for rescue in this case, but also in other models.",
            "I will show this in a minute, but we also see another thing here.",
            "So the left one, the Freebase datasets represents a general purpose knowledge graph.",
            "So in this case the relation types are only defined over a small fraction of entity, so the chance that's a negative sampling in the original model.",
            "Would actually sample triples that disagree with the type constraint is much higher than in the ones on the right side which which have much broader.",
            "Much broader well set of entities they relate.",
            "So and well for."
        ],
        [
            "Other models it."
        ],
        [
            "It's it's really here.",
            "It works really well with the neural network model, it's it's really similar.",
            "OK, but what are we going to do if the type constraints in the in the knowledge graph of fuzzy or they're not present As for example in schema less knowledge graphs?"
        ],
        [
            "So for that we propose a local close word assumption and we also did the same experiment with that.",
            "How this local closed world assumption works?",
            "I will explain now so.",
            "Give him a knowledge graph and we look at one relation.",
            "So in this example on the visited relation we put every entity in its domain which has been observed to relate to be related to subject and if the entity is related by this relation type as object we put it in its range.",
            "So it's really simple and compared to the to the.",
            "Type constraints of the schema we define here.",
            "The domain and range constraints on instance level instead of class level because we don't have any classes in the schema, less knowledge graph, for example."
        ],
        [
            "So in answers can be automated, of course, because we only need to view the triples and from the triples we can ultimately extract this domain and range constraints where they are approximated and then integrating the same way into the model.",
            "We make sure that during the negative sampling we only sample entities that agree with this domain and range constraints."
        ],
        [
            "And we see really similar results on the same datasets, so we see that the model that considers this local swirled assumption outperforms the original model by a wide margin."
        ],
        [
            "And and also in the other cases in the other models.",
            "And one thing I forgot to mention is this integration of this type constraints and the local crossword assumption does not come with a cost that the algorithm becomes less less efficient.",
            "So this has absolutely no impact on the runtime of algorithm.",
            "So it works at the same speed but just the learning problem is easier for the algorithm and this way we get better results."
        ],
        [
            "OK, so I will conclude my talk now.",
            "Well, we conclude that actually type constraints are essential when we model large knowledge graph with latent variable models.",
            "So we saw up to 77% improvement in our area under precision recall curve score.",
            "It has no negative impact on the efficiency of the algorithm and a second point.",
            "So in case the type of things are missing or fuzzy so they can be fuzzy's for typing of the entities, not perfect or other errors in the database, then we can exploit this locus words assumptions.",
            "We get similar results.",
            "Um?",
            "With the same properties and, well, we would also conclude that well in the real setting we could also combine both.",
            "So if you have a knowledge graph where some relation times better annotated than others, we can combine them.",
            "So in the in the case where relation types that have a good annotation will exploit the type constraints, in case we have relation types that are not well annotated, we take the local crossword assumption."
        ],
        [
            "OK um.",
            "Just some words on the code and data set, so when I'm when I will be back next week I will put my code in datasets online so whoever wants to try or exploit the datasets for its own research is invited to do so.",
            "Yeah, now I'm happy to answer any questions.",
            "Thank you nice.",
            "So this is just a clarification question.",
            "You assume that if something has a domain of a particular type, so people and there is an instance that is a, I don't know a bridge that that violates that constraint, so you assume the domain RDF's domain and range or you use them as constraints, yes, so my assumption is that somebody so.",
            "So somebody has has gone there and has put this domain and range constraints of this for some reason to somehow semantically semantically define this relation type.",
            "And I assume that this is the truth, of course.",
            "Right, so that's just I mean, that's great.",
            "I think it's great work, it just violates the definition of the RDF semantics for domain and range.",
            "But that's fine.",
            "I think it's good.",
            "OK, I just wanted to clarify that.",
            "OK, well may I ask back what would be the right definition then?",
            "The right definition is that you can infer something is that type.",
            "So if I have lives in person lives in location, then I can infer if I'm using that relation.",
            "That in fact has the type of person.",
            "So OK, it's addition KICS, not a constraint.",
            "It's fine to violate it.",
            "I think it's a good idea.",
            "So Datalog sorry.",
            "Oh yeah, I think I wanted to follow up with that in the example if I'm.",
            "If I'm right and understanding it.",
            "There was an associate saying somebody born in Albert Einstein.",
            "With your with the definition of RDF semantics of domain range, wouldn't Albert Einstein become a place?",
            "So that's the problem, right?",
            "Right, but the data doesn't agree with the semantics definition.",
            "Yeah, I mean, I think that's fine, right?",
            "But I'm probably in the wrong.",
            "We're in the right section for that.",
            "So some other people in some other stuff which Sessions may get upset, but it's fun.",
            "I may have a comment on that.",
            "The problem we have as well when we built this kind of models is.",
            "That the machine learning methods would exactly consider this kind of triples.",
            "So because it doesn't have this human understanding.",
            "And our idea was just to somehow exploit some background knowledge.",
            "That yeah, OK. Yeah, it's.",
            "Yeah, OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello everybody, thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "I just want to add this is joint work with Steven Beyer was also a PhD student.",
                    "label": 0
                },
                {
                    "sent": "Our lab and focus trespass my PhD supervisor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's a short outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "Basically will have two parts.",
                    "label": 0
                },
                {
                    "sent": "In the first part I will give a brief introduction, well to Knowledge Graph, where it is the structure, some applications and learning and knowledge graphs with latent variable models.",
                    "label": 1
                },
                {
                    "sent": "In the last part I will talk about our contribution that is like integrating prior knowledge about relation types into latent variable models to actually improve link prediction under realistic settings.",
                    "label": 0
                },
                {
                    "sent": "What I understand.",
                    "label": 0
                },
                {
                    "sent": "Under a realistic setting, I will clarify that later in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to me as machine learner knowledge graphs is a database that stores facts about the world's relation between entities.",
                    "label": 0
                },
                {
                    "sent": "Were entities, real world objects such as places or persons that are certain kind of relationships, and in addition knowledge graphs are also well often backed by a schema or an anthology that provide additional semantic information.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And were given this knowledge graphs, we actually able to somehow introduce the notion of things and semantics into various applications that range from search engines to information extraction on unstructured structured text, but also very sophisticated question answering systems such as IBM Watson system or smart mobile assistance such as Apple Siri or Microsoft's Cortana.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said, I'm I have a machine learning background and the might arise a question why would need machine learning in the context of knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "So first of all, knowledge graphs are not perfect where they're they're fairly incomplete, just to give an example, in DP and Freebase more than 60% of the persons do not have a place of birth.",
                    "label": 0
                },
                {
                    "sent": "Facts can be wrong, outdated, and if you in addition consider the size of present knowledge graphs, it becomes pretty clear that this.",
                    "label": 1
                },
                {
                    "sent": "Problems cannot be tackled manually, so you need automated automated approaches for that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And well, in this regard, latent variable models have been shown in the recent years to work quite well.",
                    "label": 1
                },
                {
                    "sent": "They have been used for knowledge graph completion.",
                    "label": 0
                },
                {
                    "sent": "So link prediction, clustering, disambiguation.",
                    "label": 0
                },
                {
                    "sent": "These models do they?",
                    "label": 0
                },
                {
                    "sent": "They learn latent embeddings for entities in relation types, and will.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next, how this works well in principle, so we are given with the knowledge graphs where the notes are the entities and the relation types identify well defined the edges in this graph and basically it's a multi graph.",
                    "label": 0
                },
                {
                    "sent": "So each relation type defines a graph by its own and well of course internally it's a triple store and we also exploit this trip.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Store and put it in a latent variable model and then it comes out with these embeddings for entities in relation types with what are in this case now in this example, just vectors of features and.",
                    "label": 1
                },
                {
                    "sent": "These features have different values, so where, where do these features come from?",
                    "label": 0
                },
                {
                    "sent": "So the trick is so before I trained the model I tell them I tell the model.",
                    "label": 0
                },
                {
                    "sent": "OK, I want 10 features, for example, for example to describe entities.",
                    "label": 0
                },
                {
                    "sent": "And then the model that takes the triples and the depending structured observed.",
                    "label": 0
                },
                {
                    "sent": "Since the data in somehow balances the values of these different features, what we also correlated factors to somehow describe the entities and relation types and as a result, if we if an entity if two entities have very similar relationships in the graph, they end up to have very similar embeddings on the other side.",
                    "label": 0
                },
                {
                    "sent": "If the two entities will have very different kinds of relationships in the graph.",
                    "label": 0
                },
                {
                    "sent": "They will end up to be also very different in the will have different embeddings.",
                    "label": 0
                },
                {
                    "sent": "So given this train model we can do different things.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, we can just take this embeddings into link based clustering.",
                    "label": 0
                },
                {
                    "sent": "So we took to embedding social entities and just ask.",
                    "label": 1
                },
                {
                    "sent": "It's a similar.",
                    "label": 0
                },
                {
                    "sent": "This can be used for this application, but also the link based clustering.",
                    "label": 0
                },
                {
                    "sent": "Second we connect.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Poison model and do simple link prediction.",
                    "label": 0
                },
                {
                    "sent": "So we complete the graph and just predict new edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "Another thing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could do with short this in a prior publication.",
                    "label": 0
                },
                {
                    "sent": "Then we that we can also exploit some for probabilistic querying, so we can even ask more complex questions.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have query with multiple conditions or in this made up example if I would Einstein would like cats and dogs and not just just one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but also these models have kind of problems when we look at really large knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "So as I said, we learn actually embeddings for for every entity and well independence, how long this embeddings are.",
                    "label": 0
                },
                {
                    "sent": "We run into problems here, so if we look at really large knowledge graph such as Freebase for example with 15 million entity topics topic entities.",
                    "label": 0
                },
                {
                    "sent": "Then we will end up the model will have a lot of parameters and.",
                    "label": 0
                },
                {
                    "sent": "If we would enforce like that, OK, we want a lot of latent factors, so we would have high dimensional embeddings.",
                    "label": 1
                },
                {
                    "sent": "This is not possible.",
                    "label": 1
                },
                {
                    "sent": "This infeasible becausw this will cause a lot of memory consumption of the model and because of the large amount of parameters, also training times becomes long or infeasible.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as a conclusion in large scale problems, we actually force to exploit less expressive models.",
                    "label": 1
                },
                {
                    "sent": "This is kind of a dilemma.",
                    "label": 0
                },
                {
                    "sent": "We cause we could expect when the graph grows bigger.",
                    "label": 0
                },
                {
                    "sent": "Also the complexity rises.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our belief is that one solution to tackle this problem is to introduce prior knowledge about the learning task into the model.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is where we come to type constraints.",
                    "label": 0
                },
                {
                    "sent": "So in schema based knowledge graphs we have well semantic description of relation types and the question is why?",
                    "label": 0
                },
                {
                    "sent": "Why not mind them directly in this latent variable models?",
                    "label": 0
                },
                {
                    "sent": "This is the whole idea.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the RDF schema we have these two concepts.",
                    "label": 0
                },
                {
                    "sent": "It's domain and range that actually defined for relation types.",
                    "label": 0
                },
                {
                    "sent": "The classes of entities that should be related as subject and as always object by a certain relation type.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "Person knows relation type is defined.",
                    "label": 1
                },
                {
                    "sent": "Its domain is defined for person entities.",
                    "label": 0
                },
                {
                    "sent": "It's range for.",
                    "label": 0
                },
                {
                    "sent": "Also for person entities.",
                    "label": 0
                },
                {
                    "sent": "So this example triple that says that Max Planck knows I belanja time is valid in sense of the relation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this example is not valid.",
                    "label": 0
                },
                {
                    "sent": "So if we look at another relation type says born in which is defined over persons and places the relation between Max Planck and Albert mentioned doesn't make any sense.",
                    "label": 0
                },
                {
                    "sent": "Of course, for humans this is.",
                    "label": 0
                },
                {
                    "sent": "This is quite clear, but the machines we have to tell them explicitly and then with a schema based knowledge graph we have this knowledge there, so we're going to mine them.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the approach is basically as follows.",
                    "label": 0
                },
                {
                    "sent": "We extract the triples from the knowledge graph and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we also mind the schema.",
                    "label": 0
                },
                {
                    "sent": "We extract the typing of entities and we extract the domain and range constraints from the schema and what we do then is doing the training.",
                    "label": 1
                },
                {
                    "sent": "So most of this model how they work is that they they takes a triple that they observed and then to do negative sampling.",
                    "label": 0
                },
                {
                    "sent": "So there's somehow try to just pick random random triples and assume that there are negative.",
                    "label": 0
                },
                {
                    "sent": "But we make sure that the that this selection of negative triples is more selective, so we make sure that this this sample.",
                    "label": 1
                },
                {
                    "sent": "This negative sampling only takes triples that agree with type constraints.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to show the generality of our approach, we covered multiple aspects in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we integrated this prior knowledge in multiple.",
                    "label": 0
                },
                {
                    "sent": "Latent variable models that cover the diversity of the current state of the art of these models that are applied to knowledge graphs for link prediction.",
                    "label": 0
                },
                {
                    "sent": "So here I show three of them.",
                    "label": 0
                },
                {
                    "sent": "It's rest card.",
                    "label": 0
                },
                {
                    "sent": "It's sort of the tensor factorization that has been developed by a former colleague of mine, translation.",
                    "label": 0
                },
                {
                    "sent": "Embeddings is a distance based method developed by borders and a neural network model that was exploited last year.",
                    "label": 0
                },
                {
                    "sent": "In the Google Knowledge world project.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second aspect, we also constructed different datasets that somehow simulate different types of graphs where this models could be applied to.",
                    "label": 0
                },
                {
                    "sent": "So for the first one we extracted a sample from Freebase that somehow simulates a general purpose knowledge graph in our viewer general purpose knowledge graph has multiple relations and each relation only relates a small fraction of entities which each other.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the next one.",
                    "label": 0
                },
                {
                    "sent": "The DB Pedia music would be.",
                    "label": 0
                },
                {
                    "sent": "Domain specific knowledge graph, where we have a lower low amount of relation types at all relate a large fraction of entities compared to the entities that are present in the graph and the last one will represent the high quality, high quality knowledge graph that we extracted from Yahoo.",
                    "label": 0
                },
                {
                    "sent": "And of course, the third aspect is that we only allow in our experimental model to to exploit a very limited amount of parameters that would somehow.",
                    "label": 0
                },
                {
                    "sent": "Somehow represent their their application.",
                    "label": 0
                },
                {
                    "sent": "Two very large problems.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will first explain this this graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so the Y axis is always the area under precision recall curve score.",
                    "label": 0
                },
                {
                    "sent": "It's just a score to measure how well we're in predicting Nunu triples.",
                    "label": 0
                },
                {
                    "sent": "The X axis is.",
                    "label": 0
                },
                {
                    "sent": "Represents the complexity of the models or so.",
                    "label": 0
                },
                {
                    "sent": "Here we allow the embeddings have a length of 10, which is really really low.",
                    "label": 0
                },
                {
                    "sent": "100 is well, it's it's a little bit higher, but still quite low.",
                    "label": 0
                },
                {
                    "sent": "The doc, a boss always is always some models that exploits the original model that doesn't exploit any prior knowledge about type constraints, and the lighter ones are always a model that exploits the type constraints, and we have the three datasets.",
                    "label": 0
                },
                {
                    "sent": "And two things we can see here.",
                    "label": 0
                },
                {
                    "sent": "So first of all, integrating type constraints really pays off for rescue in this case, but also in other models.",
                    "label": 0
                },
                {
                    "sent": "I will show this in a minute, but we also see another thing here.",
                    "label": 0
                },
                {
                    "sent": "So the left one, the Freebase datasets represents a general purpose knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case the relation types are only defined over a small fraction of entity, so the chance that's a negative sampling in the original model.",
                    "label": 0
                },
                {
                    "sent": "Would actually sample triples that disagree with the type constraint is much higher than in the ones on the right side which which have much broader.",
                    "label": 0
                },
                {
                    "sent": "Much broader well set of entities they relate.",
                    "label": 0
                },
                {
                    "sent": "So and well for.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other models it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's it's really here.",
                    "label": 0
                },
                {
                    "sent": "It works really well with the neural network model, it's it's really similar.",
                    "label": 0
                },
                {
                    "sent": "OK, but what are we going to do if the type constraints in the in the knowledge graph of fuzzy or they're not present As for example in schema less knowledge graphs?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for that we propose a local close word assumption and we also did the same experiment with that.",
                    "label": 0
                },
                {
                    "sent": "How this local closed world assumption works?",
                    "label": 0
                },
                {
                    "sent": "I will explain now so.",
                    "label": 0
                },
                {
                    "sent": "Give him a knowledge graph and we look at one relation.",
                    "label": 0
                },
                {
                    "sent": "So in this example on the visited relation we put every entity in its domain which has been observed to relate to be related to subject and if the entity is related by this relation type as object we put it in its range.",
                    "label": 0
                },
                {
                    "sent": "So it's really simple and compared to the to the.",
                    "label": 0
                },
                {
                    "sent": "Type constraints of the schema we define here.",
                    "label": 0
                },
                {
                    "sent": "The domain and range constraints on instance level instead of class level because we don't have any classes in the schema, less knowledge graph, for example.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in answers can be automated, of course, because we only need to view the triples and from the triples we can ultimately extract this domain and range constraints where they are approximated and then integrating the same way into the model.",
                    "label": 0
                },
                {
                    "sent": "We make sure that during the negative sampling we only sample entities that agree with this domain and range constraints.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see really similar results on the same datasets, so we see that the model that considers this local swirled assumption outperforms the original model by a wide margin.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and also in the other cases in the other models.",
                    "label": 0
                },
                {
                    "sent": "And one thing I forgot to mention is this integration of this type constraints and the local crossword assumption does not come with a cost that the algorithm becomes less less efficient.",
                    "label": 0
                },
                {
                    "sent": "So this has absolutely no impact on the runtime of algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it works at the same speed but just the learning problem is easier for the algorithm and this way we get better results.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I will conclude my talk now.",
                    "label": 0
                },
                {
                    "sent": "Well, we conclude that actually type constraints are essential when we model large knowledge graph with latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So we saw up to 77% improvement in our area under precision recall curve score.",
                    "label": 0
                },
                {
                    "sent": "It has no negative impact on the efficiency of the algorithm and a second point.",
                    "label": 0
                },
                {
                    "sent": "So in case the type of things are missing or fuzzy so they can be fuzzy's for typing of the entities, not perfect or other errors in the database, then we can exploit this locus words assumptions.",
                    "label": 0
                },
                {
                    "sent": "We get similar results.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "With the same properties and, well, we would also conclude that well in the real setting we could also combine both.",
                    "label": 0
                },
                {
                    "sent": "So if you have a knowledge graph where some relation times better annotated than others, we can combine them.",
                    "label": 0
                },
                {
                    "sent": "So in the in the case where relation types that have a good annotation will exploit the type constraints, in case we have relation types that are not well annotated, we take the local crossword assumption.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "Just some words on the code and data set, so when I'm when I will be back next week I will put my code in datasets online so whoever wants to try or exploit the datasets for its own research is invited to do so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, now I'm happy to answer any questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you nice.",
                    "label": 0
                },
                {
                    "sent": "So this is just a clarification question.",
                    "label": 0
                },
                {
                    "sent": "You assume that if something has a domain of a particular type, so people and there is an instance that is a, I don't know a bridge that that violates that constraint, so you assume the domain RDF's domain and range or you use them as constraints, yes, so my assumption is that somebody so.",
                    "label": 0
                },
                {
                    "sent": "So somebody has has gone there and has put this domain and range constraints of this for some reason to somehow semantically semantically define this relation type.",
                    "label": 0
                },
                {
                    "sent": "And I assume that this is the truth, of course.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's just I mean, that's great.",
                    "label": 0
                },
                {
                    "sent": "I think it's great work, it just violates the definition of the RDF semantics for domain and range.",
                    "label": 0
                },
                {
                    "sent": "But that's fine.",
                    "label": 0
                },
                {
                    "sent": "I think it's good.",
                    "label": 0
                },
                {
                    "sent": "OK, I just wanted to clarify that.",
                    "label": 0
                },
                {
                    "sent": "OK, well may I ask back what would be the right definition then?",
                    "label": 0
                },
                {
                    "sent": "The right definition is that you can infer something is that type.",
                    "label": 0
                },
                {
                    "sent": "So if I have lives in person lives in location, then I can infer if I'm using that relation.",
                    "label": 0
                },
                {
                    "sent": "That in fact has the type of person.",
                    "label": 0
                },
                {
                    "sent": "So OK, it's addition KICS, not a constraint.",
                    "label": 0
                },
                {
                    "sent": "It's fine to violate it.",
                    "label": 0
                },
                {
                    "sent": "I think it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "So Datalog sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I think I wanted to follow up with that in the example if I'm.",
                    "label": 0
                },
                {
                    "sent": "If I'm right and understanding it.",
                    "label": 0
                },
                {
                    "sent": "There was an associate saying somebody born in Albert Einstein.",
                    "label": 0
                },
                {
                    "sent": "With your with the definition of RDF semantics of domain range, wouldn't Albert Einstein become a place?",
                    "label": 0
                },
                {
                    "sent": "So that's the problem, right?",
                    "label": 0
                },
                {
                    "sent": "Right, but the data doesn't agree with the semantics definition.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I think that's fine, right?",
                    "label": 0
                },
                {
                    "sent": "But I'm probably in the wrong.",
                    "label": 0
                },
                {
                    "sent": "We're in the right section for that.",
                    "label": 0
                },
                {
                    "sent": "So some other people in some other stuff which Sessions may get upset, but it's fun.",
                    "label": 0
                },
                {
                    "sent": "I may have a comment on that.",
                    "label": 0
                },
                {
                    "sent": "The problem we have as well when we built this kind of models is.",
                    "label": 0
                },
                {
                    "sent": "That the machine learning methods would exactly consider this kind of triples.",
                    "label": 0
                },
                {
                    "sent": "So because it doesn't have this human understanding.",
                    "label": 0
                },
                {
                    "sent": "And our idea was just to somehow exploit some background knowledge.",
                    "label": 0
                },
                {
                    "sent": "That yeah, OK. Yeah, it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        }
    }
}