{
    "id": "rqswf224gb3ipsbsvtp3fi5kio7eaznp",
    "title": "Machine learning and the cognitive science of natural language",
    "info": {
        "author": [
            "Alexander Clark, Royal Holloway, University of London"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/mlss2010_clark_mlatcs/",
    "segmentation": [
        [
            "I'm Alex Clark.",
            "I'm from computer science Department, but I sorta identify myself more as a computational mathematical linguist.",
            "I have"
        ],
        [
            "Three, there are three fields in this topic.",
            "There's linguistics, cognitive science, and machine learning.",
            "I'ma talk briefly about the sort of two pairwise combinations of linguistics and cognitive science and computational linguistics and machine learning, but I'm going to focus most of the talk on a topic that's at the intersection of all three, which is really, I think, one of the central problems of cognitive science, which is how to count with computation explicit and cognitively plausible models of language acquisition.",
            "And I should, I guess, warn you that I have a sort of non standard view of the field, so you shouldn't.",
            "You know.",
            "Obviously what I'm.",
            "I believe in what I'm saying, but I accept their other alternate views, so you shouldn't necessarily take my views as being representative of the field as a whole.",
            "What I want to one of the things I want to convince you.",
            "Is that a kind of fairly narrow field of of mathematics, really, which is grammar induction actually potentially is very important to the overall cognitive science enterprise.",
            "Now, obviously I may just be deluding myself, and it's because I work in this field, but I think it's so important, but I chose to work in this field because I thought it was important, not the other way around, so."
        ],
        [
            "So what I'm going to talk about in the first part of the talk, I'm going to talk about.",
            "Something that's called the argument from the poverty of the stimulus, and a few related ideas about cognitive science about language acquisition, about what the proper study of linguistics is.",
            "I'm going to talk very briefly, then also about supervised learning and unsupervised learning.",
            "In computational linguistics, and then the second half of the talk, I'm going to sort of propose a solution.",
            "So the first half is really posing a question, and the second half is trying to sketch a solution, and I'm going to talk about what's called distributional learning, which is kind of an old-fashioned idea from the from the 1950s there actually.",
            "Predates that you can find roots of it in in car naps ratings from the 30s.",
            "And I'm going to talk about how you can use distributional learning to find efficient so polynomial probably correct algorithms for learning certain classes of context free and context sensitive languages.",
            "And then I'm going to talk a little bit about structural descriptions.",
            "I wasn't initially planning to talk about that, but out of discussions with some people here, I think it's maybe a more important thing.",
            "So I've stuck in some some stuff there.",
            "So really, this talk can be thought of as a compliment to next talk to half of next talk Nick was talking about.",
            "Inefficient algorithms that learn from our realistic data.",
            "I'm focusing much more on the efficiency point of view, which I think is particularly important for language acquisition."
        ],
        [
            "So.",
            "Jerry Fodor, who's always good for a laugh.",
            "Said he said the argument from the party of stimulus is the existence proof for the possibility of cognitive science.",
            "So I mean, Joe does not afraid of stating things strongly.",
            "I think in a sense this is right.",
            "I mean, if you look historically their engines of cognitive science in the cognitive revolution, the argument from the poverty of the stimulus, and particularly Chomsky's."
        ],
        [
            "Early writings.",
            "Nuclear packing simple behaviors.",
            "Models like like skinners model.",
            "So his famous review in 1959 of Skinner's book on Language is kind of in some respects kicked off cognizance.",
            "And I think that he was right in a number of important areas.",
            "First of all, he was right to say that those simple behaviourists models were inadequate.",
            "He was right, they do not explain language use.",
            "You need to hypothesize are very rich internal structure and I think he was one of the first people to point out just how.",
            "How kind of strange and puzzling the structure of languages?",
            "And he was also right to insist on computationally explicit models of natural language, and he developed the well known Chomsky hierarchy and just a quick, how many people here know what are context free grammars?",
            "OK, good excellent.",
            "OK, I was worried that would OK so so he did not bring hierarchy.",
            "And finally I think his view of linguistics as basically being a branch of psychology.",
            "That when we study linguistic we should study it as being some sort of property of the human mind.",
            "So I think you will spot on."
        ],
        [
            "Lots of things.",
            "And he identified the basic problems of linguistics.",
            "He framed the questions in one of his books.",
            "I think this is the knowledge of language.",
            "In 96 is the key quicker.",
            "The two key questions were what constitutes knowledge of language in the individual, and how is this knowledge acquired by 8 speakers so very much forefronts the acquisition as being the acquisition plans being one of the key problems that we need to address.",
            "And again, I think he's he's absolutely right.",
            "'cause it turns out to be a very, very puzzling issue.",
            "More recently, Ray Jackendoff in a paper called 3 Alternative Mill misconceptions of language Alternative misconceptions of Language says that a theory of language needs to come up with three need to satisfy three simultaneous constraints.",
            "You need to have a descriptive constraint, so the class of languages that your model defines must be sufficiently rich to represent natural languages.",
            "It has to satisfy learnability constraint.",
            "And then you have to.",
            "You have to have some sort of plausible story about how the trial could acquire these representations from the data available, and finally he's adding additional constraint that wasn't present in Chomsky's original writings.",
            "That Chomsky is breaking right, which is an evolutionary constraint saying must not posit a rich and evolutionary implausible language faculty, so going back to next points about evolution.",
            "Evolution is comparatively recent innovation, evolutionary history, so we can't any theory that posits.",
            "A huge amount of very rich of all structure.",
            "This is probably not right now.",
            "Satisfying these three conditions simultaneously turns out to be extremely hard.",
            "In fact, at the moment, I'd say we really have no, no, no satisfactory solutions, though obviously I'm going to at some point.",
            "I'm going to say, but my theory can solve these problems.",
            "But there are a number of strategies that people have used to reconcile these three things.",
            "So one strategy that Chomsky's favorite has been basically to prioritize the first 2 and ignore the third one.",
            "So basically to say, let's allow ourselves evolutionary implausible, very rich innate structure and then we can maybe solve the first 2 problems.",
            "Another way, which is maybe more connectionist way, is to sort of slightly ignore the true complexity of natural languages to us to simplify the problem by considering much more restricted class of languages, and then maybe we can learn them using some simple connections model SRN or something which is evolution implausible.",
            "OK, I don't think that's the right strategy because we have to confront the real complexity of language head on.",
            "OK, we have to recognize the fact languages are really, really complicated.",
            "I'm not going to be able to solve them using some simple finite state model."
        ],
        [
            "So."
        ],
        [
            "Talk about the APS now so the argument from the poverty of the stimulus.",
            "It's a very classic argument, and there are many different versions.",
            "There are some formal versions.",
            "There are some empirical versions, but one sort of simple conception is that people attain knowledge of the structure of their language, which no evidence is available in the data which they are exposed as children.",
            "OK, so the.",
            "I think so."
        ],
        [
            "Overstating it.",
            "Amor a slight more nuanced view is presenting a very good article by Jeff Pohlman, Barbara Schultz in the Linguistic Review, where they summarize a whole class of these organs very well, but basically what they're saying that children learn natural languages.",
            "They learn them rapidly in just a few years.",
            "They learn them consistently.",
            "That which means that they pretty much always learn them.",
            "They arrive at a fairly uniform outcome.",
            "They don't receive explicit instruction or explicit reward.",
            "The data they receive is sparse.",
            "It's incomplete, and it's noisy.",
            "And the basic idea is that the course of language acquisition, given this environment is language acquisition is just too hard to be explained by by standard models of learning.",
            "OK, the task is too difficult, right?",
            "So there must be something else going on.",
            "There must be some additional resource that the child brings to solve this problem, and that resource is considered to be some.",
            "Innate language specific knowledge, so the reason that the child is able to do it is it comes to the task with very rich prior knowledge about the sorts of languages there are."
        ],
        [
            "So the standard sort of what we call the empirical version of the APS, the citation there is to a very good paper by Amy Perforce, Josh and Terry Rigor, and it's kind of old Chestnut here.",
            "So the idea is that when you're learning English, you have simple declarative sentence like the student is hungry and then to form the polar interrogative.",
            "From that you'll grammatical form is is the student hungry right so?",
            "I think even in terms of movement, what's happened here is that the auxiliary here is being has been fronted.",
            "So given these two examples there is there are one can think of those being if your child confronted with this data, you can come with various possible hypothesis.",
            "One is that the.",
            "I'll just move the first you move the first auxiliary you see to the front of the sentence.",
            "Another hypothesis might be that you'll move the auxiliary from the main verb from the main clause to the front.",
            "Now these obviously on this simple pair of sentences, these get exactly."
        ],
        [
            "And result, but when you consider more complex sentence here, so the student who is in the garden is hungry, we can see that these two rules will give different results."
        ],
        [
            "But if you apply one rule, you'll get the correct answer is the student who is in the garden hungry?",
            "And if you apply the other rule the wrong rule you'll get is the student who in the garden is hungry, which is ungrammatical.",
            "And the claim is that children do not see examples of Type 4 that would help them to distinguish between these two hypothesis, but do nonetheless produce the right examples.",
            "OK, now there is a factual problem with this.",
            "Which Pullman Schultz point out that in fact, children do see examples of this type, but in a way that's slightly missing the point.",
            "Because given the children do learn, they generalize beyond their experience.",
            "There going to be some classes of sentence that they don't see but nonetheless learn to correct.",
            "Learn to predict, learn to perform correctly on."
        ],
        [
            "So.",
            "There's a good long quote from from Chomsky giving with some sort of fairy.",
            "What's a good word right rhetoric here?",
            "So basically what he's doing in this in this in this quote is he's attacking a certain class of learning algorithms.",
            "He's saying that the evidence about the character of the generative grammars when we look at sorts of grammars that we need, we find that they cannot be.",
            "There's no way these could be acquired through simple inductive process through step by step inductive operations, segmentation, classification, substitution procedures of any sort that have been developed within linguistics, psychology, or philosophy.",
            "Further increases speculations contribute nothing that even faintly suggests a way of overcoming the intrinsic limitations of the methods that have so far been proposing elaborated.",
            "So he's forcefully expressed his view about the inadequacy of certain methods, and I guess part of my part of this talk is really saying, well, actually precisely these sorts of methods.",
            "And in fact, explain the acquisition of large chunks of language.",
            "OK, that at the time he was in a way quite ripe.",
            "I mean in the 1950s in the early 60s, when these methods were originally studied, everyone, any concrete proposals.",
            "But now we can come up with quite explicit concrete proposals, and we can prove that they are correct and we can prove they will.",
            "An interesting class of languages."
        ],
        [
            "So just again touching on what Nick was saying the language instinct.",
            "So he the APS is taking be an argument for linguistic nativism and linguistic nativism is, broadly speaking, the claim that language acquisition proceeds largely through innate language, specific mechanisms and representations.",
            "So by language specific, I mean representations of specific to the domain of language, not specific to one particular language, like English.",
            "Whatever.",
            "So, informally, you know, a lot of grammar is actually.",
            "Encoded in the genome, directly or indirectly.",
            "And it's important to distinguish between this, which is has some sort of content, and the rather vacuous claim that you know we have some innate ability to acquire language.",
            "I mean clearly human beings acquire language, lobsters, dogs, etc don't.",
            "Therefore we have some innate ability.",
            "The question is whether it's a domain specific ability, whether specific to language, or whether it's just an offshoot of other more general purpose cognitive facilities."
        ],
        [
            "Now it's worth saying that Chomsky no longer really subscribes to the view of linguistic nativism.",
            "So, in the minimalist program here is largely abandoned.",
            "This claim that there's a very rich, domain specific thing.",
            "How we store maintains that the APS is valid, and thus there's a kind of tension or contradiction.",
            "If you like, because there's really no satisfactory explanation about how language acquisition takes place, since he no longer considers there's a very rich domain specific.",
            "Learning bias and he also no longer think he still thinks that distributional methods won't work.",
            "So there is a kind of a gap."
        ],
        [
            "Now there's also a formal version of the APS.",
            "And this is a quote from Ken Wexler saying, well, the strongest, most central arguments for night Nurse does continue to be the organs from APS.",
            "An learnability theory.",
            "The basic results of the field include the demonstration without serious constraints on nature of human grammar.",
            "No possible learning mechanism can in fact learn the class of human grammars.",
            "So I think there's a.",
            "There's a sense in which there is right.",
            "This is right, I think.",
            "Complete tabular rasa learning is not possible, but it does sort of negative results.",
            "There are don't really rule out domain general learning, so the formal items that have been put forward where they've been explicit don't really support linguistic nativism.",
            "Even under the most optimistic interpretations.",
            "And I have a book coming out on this this year, which covers us in great detail."
        ],
        [
            "But sort of summarizing it.",
            "The problem with these arguments is that there's a very important distinction which to machine learning people.",
            "I guess it's fairly clear that there's an important distinction.",
            "The hypothesis class for learning algorithm and the class of anger languages that the algorithm can learn.",
            "So clearly the hypothesis class has to include in a sense the class of languages.",
            "And show the learnable class restriction in some way, but it's very difficult to show that the hypothesis class needs to be restricted, and so to give a concrete example, if you have some distribution free uniform pack learning model, then the learnable class must have finite VC dimension.",
            "But the hypothesis class does not need to have VC dimension finite.",
            "It can be unbounded, and that's kind of a standard result from one of David House those papers."
        ],
        [
            "So.",
            "Let me now move on to how language acquisition has been studied and this is a comment by Steven Pinker, which is, I think, wonderfully lucid.",
            "Just how X is learned.",
            "You first have to understand what X is.",
            "I think this is.",
            "Completely reasonable, kind of obvious, and I think it's you know completely wrong, but it's it's.",
            "It's certainly very natural way to proceed the problem."
        ],
        [
            "To that problem, so craning Petrosky ships number.",
            "Say you know first step one is you try to find principles that characterize human grammars and then you try to find out how they could be learned.",
            "Which things could be learned?",
            "Which are more likely to be innately specified?"
        ],
        [
            "So this suggests a sort of standard methodology.",
            "OK, step one, you try to construct a descriptively adequate representation for natural language and then Step 2.",
            "You try to design learning arguments with those representations, so step one would be done by linguists like Chomsky.",
            "Whatever Step 2 is meant to be done by sort of people like me, people in this room, I suppose, and I spend a certain amount of time in the trenches trying to do justice, but."
        ],
        [
            "The problem with step one, which is to construct a descriptively adequate grammar, is that it failed OK, no one has ever managed to make a descriptively adequate generative grammar for any natural language, not even the most studied language which is English in order to account for NYU fax which occasion cropped up, such as the context sensitive sensitivity of Swiss German representations, were made more powerful and expressive.",
            "Current source statistical techniques don't really try to separate grammatical from ungrammatical sentences.",
            "General grammarians have largely abandoned the task of trying to construct a large scale grammars."
        ],
        [
            "In step two, well, this also failed, so learning even the lowest level of the Chomsky hierarchy, regular grammars is computationally hard even under really quite an easy paradigm.",
            "So this is done Anglin, and correcting off without even using queries.",
            "You can't learn regular grammars.",
            "We have some heuristic algorithms that can induce constituent structures, such as the excellent work of Dan Klein.",
            "But the cost of representation, do we actually need to represent the full complexity of natural language are even richer?",
            "You need things like El tags or various things in the hierarchy of abstract categorial grammars and it's really you know out of the question that we could come up with.",
            "Efficient learning algorithms for these classes.",
            "So."
        ],
        [
            "This this basic strategy doesn't really seem to workout trumpski.",
            "Again, I think here saying hitting the target absolutely spot on says that you know there is this tension in order to achieve descriptive adequacy, I in order to capture the full complexity of natural language, you need to increase the power of your devices.",
            "But in order to solve the learning problem which equals Plato's problem, we need to restrict.",
            "So there's this tension between the desire to.",
            "Increase the class of languages as you encounter GNU GNU Facts new languages you know as you discover some new tribe like the Pijar or whatever you need, you may need to increase your class in order to learn them.",
            "You want your class to be as restrictive as possible, so is this tension between these two desires to increase and to decrease it?"
        ],
        [
            "But the model that Chomsky proposed as a solution to this, the principles and parameters models where language is essentially entirely innately specified apart from a finite number of binary value parameters, is fails for a number of reasons, it's.",
            "It's evolution implausible.",
            "There's no good learning model.",
            "As we probably know, simply the fact that you have a finite number of parameters doesn't solve the learning problem.",
            "There's no agreement on what the parameters might be.",
            "There's no tension between these two classes, and she simply sort of stipulated a finite constraint.",
            "Ann is currently being abandoned.",
            "So what are the?"
        ],
        [
            "Mobile solutions, well, it's important to realize that linguists don't know what the representations are, so you wouldn't necessarily believe that.",
            "If I said it because I'm not, I'm not a proper linguist, but Jim Blevins, in a recent survey article in the Journal of Linguistics, says at the most fundamental level, it is not clear there is any meaningful empirical motivation for the representational assumptions of any current formal model of syntax which is.",
            "Incorrect linguists cannot agree about very rudimentary facts about linguistic representations.",
            "So, for example, if you have a what we would call a non phrase of the cat, most of us would consider that the word cat is the head.",
            "But nearly all linguists were considered that in fact this is a DP and it is headed by the determiner phrase the.",
            "Not any cannot agree on this.",
            "They cannot agree even on a methodology where one might.",
            "One might have to decide between these two things is just not an empirical empirical point."
        ],
        [
            "So we don't know what the representations are OK, but we do know that they are learnable.",
            "We know that they are learnable because children do in fact succeed in learning.",
            "So the obvious thing is really."
        ],
        [
            "To turn that that basic methodology around rather than starting off by trying to build a descriptively adequate model and then trying to learn it, you should put learnability first.",
            "If you construct some super powerful class of languages and representation class without any thought of learnability, you're not going to be able to succeed, so you have to design representations from the ground to be learnable."
        ],
        [
            "So so step one here is, you build simple learnable representations and then Step 2 is you try to gradually increase their expressive power while maintaining learnability.",
            "OK, so the end result is the same point.",
            "You want something that is both large enough and learnable.",
            "But we're putting the more the more difficult constraint first.",
            "Learnability is hard.",
            "Just come out with a big class.",
            "Representations is easy.",
            "There are literally dozens of them.",
            "There are very few approaches.",
            "Too efficient learning."
        ],
        [
            "So I think I'd share what I'm doing.",
            "I'm going to skip these two students."
        ],
        [
            "So talk a little bit about unsupervised learning.",
            "So in computational linguistics.",
            "As a community, people in computational linguistics are generally not interested in cognitive issues.",
            "They are interested in building efficient systems that can press this language.",
            "Supervised learning, which is a classic paradigm, involves learning from annotated data, and that causes an annotation bottleneck.",
            "OK, both because there are some languages which have a resource poor have compared to the few annotation linguistic resources like annotated corpora, tree bank and so on.",
            "And Secondly also for the minority people interesting cognitive modeling.",
            "So there's particularly, I think, in the context of this talk, there's some this workshop.",
            "There's some trick, interesting work done on segmentation using nonparametric Bayesian inference.",
            "By Sharon Goldwater, Mark Johnson and some people like that, but."
        ],
        [
            "I'll talk a little bit about grammar induction.",
            "And the reason this is strictly important is becausw syntax clearly needs to have more powerful models and simple regular grammars of finite state models.",
            "Lot of these other problems are quite well modeled by regular orphan assembles, and we have a good idea about how to solve those, but we have very few good ideas about how to learn context free grammars.",
            "So there are two basically ways of doing this.",
            "One is an empirical approach and one is a more theoretical approach."
        ],
        [
            "So the empirical approaches that you use you take some real data, a corpus of language, possibly from the Wall Street Journal or possibly from the child's database of child directed speech.",
            "We run our learning.",
            "We implement some learning algorithm.",
            "We run this algorithm and then we evaluated against some gold standard, some linguistic annotations and typically use some heuristic algorithms.",
            "So client and Manning Dan Klein's work is very typical.",
            "This you use sentences of length lesson town from the Wall Street Journal.",
            "You evaluate using modified positive allometric.",
            "So modifications of the way that you would use to evaluate standard supervised parsing algorithms and they show.",
            "Good results using constraint generating binary tree is subject to some distributional heuristic."
        ],
        [
            "Um?",
            "I'm not really focusing on that.",
            "I'm focusing more on theoretical approach where we try to, you know, basically find out what sorts of algorithms are there that can perform this task well, and there are two basic problems.",
            "The first is problems that we can think of is information theoretic problems, so this is problems with the absence of negative data which Nick talked about problems to do with VC dimension problems with sparsity noise and so on.",
            "And my view is that we know how to attack these problems.",
            "Whether using a minimum description length, nonparametric Break, Bayes structure, risk minimization, we have a variety of ways of."
        ],
        [
            "Attacking these problems.",
            "So the second problem is the computational complexity problem, and that is the complexity of finding the best hypothesis given the data.",
            "And there's some for quite some time now.",
            "People have realized there's this problem is basically extremely hard.",
            "The standard representations so gold gave the 1st result in 1978.",
            "There's some famous papers by cons and violent and so on, and one of the reasons why I think we should take these particular problem seriously is that the previous results about VC dimension, so not specific to language there in general.",
            "Like the very general problems about machine learning about inference, the limits of learning the limits of inductive inference.",
            "But these problems are specific, ready to learning particular class of representation, and they are often based on embedding cryptographic problems in learning problems.",
            "So the idea that is finding the right hidden structure, the right hidden representation is as hard as cracking a code so you can embed problems like factoring Blum integers and so on into this learning problem Now.",
            "It shouldn't make you slightly suspicious.",
            "I mean if you have a learning model that doesn't distinguish between cracking a cryptographically hard problem, and which is a very adverse aerial system, and the situation where the child is in where he's in comparatively, your shares in a comparatively helpful environment where the parent is trying to support the learning process, then you should be a little bit suspicious about this learning model.",
            "Being robust specialist is right because even quite simple problems like doing a bit of clustering some data, using a mixture of Gaussians.",
            "This turns out to be intractably hard in general.",
            "Now obviously sometimes it's quite easy when the sense of Gaussians are widely separated.",
            "It's very easy when they're very close together.",
            "It turns out to be hard, so I think we need to use a little bit of common sense of these algorithms and realize that you know the overall problem may be hard, but nonetheless we identify situations analogous to clustering Gaussians where they.",
            "Centers are very well separated where he turns out to be quite easy.",
            "And I think it is important to recognize this constraint, and there's what's called tractable cognition thesis.",
            "You know, human cognitive capacities are constrained by the fact that humans are finite systems with limited resource for computation.",
            "That's kind of a. I guess it's a.",
            "It's a truism really, but I think we should recognize this now.",
            "My view is we have these two problems.",
            "We have the information theoretic problem with computational complexity problem is too hard to try to solve both these problems together.",
            "So here I'm going to try to solve the 2nd and assume the first is being dealt with.",
            "Now this can be a very, very annoying thing to do.",
            "You're going to basically saying to these guys, I'm going to assume that you've successfully brought your little research problem to a satisfactory conclusion.",
            "OK, I'm not.",
            "There's not view at all.",
            "I want to have.",
            "This is a very important problem, but I think we we sort of have a good idea about how we can deal with it, whereas we don't really have a very good idea about how to deal with these computational complexity."
        ],
        [
            "Once.",
            "Angest is a diagram really that.",
            "So.",
            "So Gold showed that you could learn pretty much anything using positive data and membership queries.",
            "If you didn't care about efficiency.",
            "Then Horning in 969 shady you can learn probabilistic context free grammars if you don't care about efficiency.",
            "Donna Anglin did some work and Nick Cater and pull that down.",
            "You have done some work.",
            "So what I'm really saying is I think we know in a way how to go down.",
            "We don't really know about how to go across So what we want to start off really is solve the next most obvious problem is not to go straight to the bottom right hand corner but just to go to the top right angles.",
            "Look at efficient algorithms using positive data.",
            "Membership queries."
        ],
        [
            "And in regular inference, this is pretty much what panned out.",
            "The 1st result in regular inference was done.",
            "Anglins work in 1982, where you just use positive data and you get a very small class called the reversible languages that you can learn.",
            "So that's going to be.",
            "She extended that using a query model, so using positive data, but where you are allowed to ask queries as a result by Jose Ansina.",
            "They use positive and negative data and when you stochastic data there is some results by diner on cross concina and a paper I wrote and what these results re suggest is that I think very much confirming.",
            "Next point is that the presence of probabilistic data largely compensates for the absence of negative data and so these are all efficient algorithms for learning these representations.",
            "So we're going to do.",
            "Here is we're going to assume positive data membership queries.",
            "And it's just basically a placeholder for a more realistic probabilistic learning model."
        ],
        [
            "So.",
            "So these algorithms for learning regular languages don't use regular grammars as a representation.",
            "They use deterministic finite automatic.",
            "And it's worth pausing from him and saying, why are these DFA is learnable?",
            "Because this really is the up until sort of five or six years ago, this was really the only positive really, really interesting positive result in grammatical inference, and the reason is this that if you have a language L, you can define the residual languages which are the.",
            "For any string you you can send the set of strings that will make a grammatical, the sort of set of suffixes that line the language.",
            "And by the Myhill nerode theorem.",
            "There are a finite number of these if and if only if the language is regular.",
            "Now when you learn the minimal DFA for each state, you can define the language generated from that state.",
            "So here we have a state of Q and we're saying the language generate from the state is the set of strings such that when you start from that state Q, you end up in accepting an accepting state.",
            "Now the minimal DFA has a property, there is a bijection.",
            "OK, between these two things.",
            "There's an exact correspondence between the residual languages OK, which is defined purely in terms of the language.",
            "OK, so if you look at the definition residual language, there's no reference to the state or anything like that is just the language itself and the.",
            "And the states here correspond precisely to those.",
            "OK, so it is in a sort of special sense when you think of being objective.",
            "OK, the states correspond directly to some well defined sets of strings in the language.",
            "OK. And is that which allows DFA's to be learnable?",
            "So the natural question."
        ],
        [
            "As you know, how can we?",
            "How can we extend this to context free inference?",
            "So the slogan here is I guess sort of in Paris is model empiricist is a slightly annoying term.",
            "What I mean is you know empiricism is a philosophical doctrine.",
            "Is that the knowledge is based on experience so linguistic in person with the idea that our linguistic knowledge is based on linguistic experience.",
            "So these models really had this impressive property that the.",
            "Structured representation should be based on the structure of the language, not something you arbitrarily imposed from outside.",
            "And this is, I think, a very proud.",
            "I hope this seems like a very reasonable natural assumption.",
            "We're just saying the structure or representation should be based on the structure of the data, but it's worth realizing this completely rules out the standard Chomsky representations.",
            "They do not have this property, so the research program.",
            "Then as you identify some structure in language.",
            "You construct representation which is based on that structure.",
            "An richer structure give you more powerful representations."
        ],
        [
            "So normal direction.",
            "So when you did your when you learned about context free grammars, you define a grammar.",
            "You define a derivation relationship and then you show how this grammar here can define a context free language.",
            "Here we're putting we're putting learnability first, so we're going to go back."
        ],
        [
            "That's right, we're going to go.",
            "We won't say whenever function from the language.",
            "The representation we're going to start from a set of strings, and they're going to map that onto some representational primitive of the formalism."
        ],
        [
            "There.",
            "So are there any questions on this part so far?",
            "Probably a good moment to take sort of any general questions about linguistics before we get into more technical stuff.",
            "OK, so I'll start talking now about.",
            "Does."
        ],
        [
            "Traditional learning.",
            "So we're going to be building up to is local distribution lattice grammars which are.",
            "A richly structured, context sensitive representation that seemed to have a very good match.",
            "The class of natural languages, and we have efficient correct algorithms for these, which are based on distribution learning.",
            "We have a theoretical foundation in the theorization lapses that I'm not going to talk very much about, and we also have the fact these form results were going to be using this symbolic learning paradigm where we assume that we have a source of positive data and we can ask queries."
        ],
        [
            "So.",
            "So I think there's a natural skepticism I get from people is when I say I talk about child.",
            "I talk about language acquisition.",
            "That's what I'm interested in.",
            "People always assume that I spend all of my time in a lab.",
            "Clipping electrodes to babies, heads and things like that.",
            "OK, and they were shocked when I say, well, actually I just sit, you know, sit at home in my arm chair proving theorems.",
            "I think The thing is really we can use data to distinguish between competing theories, but remember, there simply aren't any competing theories.",
            "There are no theories that satisfy the most sort of minimal requirements.",
            "There are, so two rough candidates.",
            "One is, I guess, construction grammar to Mike Tomasello Annadell Goldbergs, but it's.",
            "It's not really a theory of acquisition, they have a.",
            "They simply don't talk about acquisition in a formal way, and the principles and parameters model, which is the other competing model, doesn't really have a solution, so.",
            "What I'm going to be doing here is proving things and giving some examples, and I think I want to contrast the exams I'm giving with how other people maybe use examples exams.",
            "I'm going to be very very simple.",
            "Alright, so I have proof that these algorithms work and those proofs that I'm not going to present them should convince you what I'm going to give you.",
            "These little examples is just to illustrate to illuminate they're not meant to convince you so.",
            "One of the problems I have is this.",
            "This work is is, I think, a lot of the research I'm doing should really been done in the 1960s and wasn't so this whole field have a slight 1960s filter.",
            "I'm going to be using very small examples.",
            "This works on very large examples, but I'm going to be giving you just the simplest ones."
        ],
        [
            "So the basic assumptions here is that we have some finite set of symbols.",
            "OK, so Sigma is for example the set of words in English say the a cat dog is Sigma star.",
            "Here is a set of all finite strings.",
            "An L is just a subset of grammatical sentences, so it's those strings which are grammatically well formed.",
            "The cat is dead, I run away, etc.",
            "And the remaining strings are the ungrammatical ones, which are.",
            "Some small, numerous things, like the other cat pattern helicopters, so random strings that have no no meaning and this is very crude.",
            "It's much better to have some sort of distribution over signal over signal starts a more natural way, but as I said, I'm working with a non probabilistic learning paradigm."
        ],
        [
            "Plus every simple sort of formal example here is that we have a.",
            "We abstract it and just consider some very simple formal language like Sigma equals where we have a alphabet.",
            "Just have two symbols A&B.",
            "We have some language which consists of any number of days followed by the same number of bees, which is a classic example of a non regular context free language.",
            "And Sigma star minus L is just every other string.",
            "OK, so I'm going to be using predominantly these sorts of very simple examples as a proxy for the more complicated ones."
        ],
        [
            "So the learning problem is basically that were given some information about L about the language, the set of strings and we want to construct a representation G such that G defines the language L. So typically have a sequence of examples W1W2 we assume that all of those examples are in the language, and at every example of language will eventually occur.",
            "This is a very weak constraint, and so we're going to compensate by allowing the learner to query whether a W as in L and we wanted to converge to right answer.",
            "So this is a very weak source of information.",
            "We don't have any context.",
            "We can't.",
            "We don't have any interaction, we just passively receive these examples.",
            "But we compensate for this by allowing ourselves to to query.",
            "Now what I think is extraordinary about the field, right?",
            "Is that this is a fairly straightforward problem and there are almost no algorithms for doing this.",
            "So which I find which I I will come back to, why this is?"
        ],
        [
            "While this is interesting, so distributional learning, I think if people think as far as cognitive science thinks about language acquisition, the default assumptions, broadly speaking, that you store some set of examples and then you generalize in somewhere an it's very natural algorithm idea.",
            "Particularly, look at child directed speech is that you have the child is confronted with the settings like look at the doggie look at the car, look at the biscuit, look at the blue car.",
            "The doggie is over there.",
            "The biscuit is over there, and seeing this data.",
            "You naturally think well, be obvious there's something like.",
            "You know doggy car biscuit and blue car or similar.",
            "We can substitute them one for the other, and that's a very natural algorithmic idea.",
            "It's been around for a long time, and the question really is what class of languages can we learn using this approach?",
            "OK. We obviously can't learn every possible language, but there must be some way of characterizing sorts of languages that we can learn using this approach, and I guess the subsidiary question is how does that class of languages correspond to the class of natural languages?"
        ],
        [
            "Now there are some problems with this naive approach, which Chomsky was one of the first to point out.",
            "Johnson is very much reacting to distribution learning.",
            "Zelek Harris was Chomsky supervisor.",
            "One of the examples of this is John is easy to please and John is eager to please.",
            "OK, so these two sentences superficially very similar.",
            "But if you think about from him, you realize there structurally very different in that in the first one John is easy to please.",
            "Jaune is the person that is being pleased.",
            "OK, in some sense is the object of the verb please.",
            "Whereas in the second sentence John is eager to please.",
            "Jaune is in fact the one that wants to isn't going to be doing the prison, so he is the subject of the thing so.",
            "This very simple, so we have these two sentences that appear superficially to be very, very similar and yet have very different interpretations.",
            "So how can a simple distribution a learner figure out these different interpretations?"
        ],
        [
            "And we can see that there different by considering a sentence like this, they are ready to eat again.",
            "This illustrates a problem that simple structuralist models had.",
            "But there's no real way to account for the ambiguity of this sentence.",
            "So they are ready to eat can mean two things that could mean you know the hot dogs are ready to eat, or it could be that the students are ready to eat.",
            "OK, so there are two different interpretations here.",
            "I thought about putting up some non PC joke about you know cannibalism, but I'll, I'll leave that."
        ],
        [
            "And you also have displaced constituents, so you know this is the book that John said that Mary had.",
            "You know what verb can go there?",
            "You know it's gotta be something like red.",
            "OK, something that can take the book as an object.",
            "Alright, so here we have a case where there is a dependency between book there and it's an unbounded dependency because you can have as many John said that Mary had told Jaune or whatever you connect.",
            "You can extend these as long as you like, so these are sorts of problems that.",
            "Many people found convincing as attacks on on these sort of naive distribution."
        ],
        [
            "And.",
            "So let me just define a bit more formally here.",
            "Suppose we have a simple sentence that man over there is bothering me."
        ],
        [
            "So we can split it into two parts we can consider take any substring, say man over Anna context that blank there is bothering me.",
            "OK, so we're going to find a context really just being a pair of strings."
        ],
        [
            "A.",
            "A pair L which occurs the left an abit are which occurs to the right and we can combine a context LR with a symbol year with a substring you to get LUR and clearly got a special context here which is Lambda Lambda Lambda.",
            "Here is the empty string.",
            "OK so I'm using languages refer to empty string, so the special context Lambda Lambda is sometimes like the identity context.",
            "Given a language L, we can define the distribution of a string here just to be this.",
            "Here is the set of all contexts.",
            "That a particular string you will have OK in the language so."
        ],
        [
            "To make that explicit, let's say we have.",
            "Simple form example here, GmbH then.",
            "Then we have that the context, the distribution of a consists of the strings that the context, Lambda, BABBAABB and so on.",
            "And if we look at the distribution of AAB we see it is almost the same, but not exactly the same.",
            "OK Becausw, you may notice that a can occur in the last one of those contexts, a AVB but a B.",
            "Cannot we stick AB into the last context of the distribution of a?",
            "We get a string that is not in the grammar.",
            "But we can see that a ABB is does have exactly the same distribution as a B. OK answer me in English, if we look at the word cat, we might find a lot of sentence.",
            "Look at the blank.",
            "The blank is on the mat and seeing with dog however, justice in this game result.",
            "It's quite easy to come up with contexts where cat can occur, but dog can't.",
            "So what's what's an example of a?",
            "Contacts that context would differentiate Cat and dog.",
            "So I mean, you know I have a cat flap, but in my door but you don't have a dog flat.",
            "I'm going to dog his footsteps, but you didn't catch his footsteps.",
            "You know there are a bunch of ways in which cat subtly differs from dog, so when you're dealing with when you're doing this very, very simple artificial example like this, this sort of congruence.",
            "Exact identity of distribution seems like a reasonable thing to do when you look at sort of real language.",
            "In fact, exact congruences is quite rare, but nonetheless there's a great deal of similar.",
            "I mean, the fact that nobody instant laptop with dozens of examples is good, because in fact you know the distribution.",
            "Can dog are very, very similar in many respects, and we can exploit that similarity when we write."
        ],
        [
            "About learning algorithms.",
            "Now there are.",
            "There are several reasons to take distribution learning seriously, so one is it is cognitively plausible.",
            "So there's work by Saffron aslund, Newport that show that newborn children are sensitive to properties of the distribution.",
            "At least an artificial grammar learning experiments.",
            "It works in practice, so large scale lexical induction, like James Curran Works, is a staple of statistical natural language processing.",
            "Linguists used as constituent structure tests.",
            "Recent book by Andrew Connie called constituent structure.",
            "You know, argues for for distribution, learning an historically phrase structure.",
            "Grammars were meant to be learnable.",
            "Using these approaches, I mean this is quite striking.",
            "Chomsky designed the context free grammar OK college program is not some.",
            "It's an invention of man.",
            "He created it with the intention that it would be learnable alright, so.",
            "I'm trying to do here is is fixed some technical details and context free grammars to make them making reliable."
        ],
        [
            "So the distribution learning here what we're going to do.",
            "The foundational idea here is that what we're going to do is try to predict the distribution of strengths.",
            "OK, so CL here is just the distribution of string.",
            "So we want to learn some finite representation G. This is going to define a function from.",
            "We're going to try to model A function from you to the distribution of you.",
            "Now.",
            "Obviously, if we can do that.",
            "Then we've learned a language becausw.",
            "If Lambda Landers in the distribution of you, then you, as in L. So if we predict the distribution, then we will implicitly have defined language OK.",
            "So there are two problems with doing this.",
            "First is that the distribution is normally infinite, so I'm going to need some representation, and the 2nd is there going to be Norman infinite number of strings.",
            "You in Sigma star, so we're going to do is do a standard sort of recursive decomposition, and we're going to try to predict the distribution of UV from our estimates of the distribution of you and the distribution of E. So it's again a sort of fairly classic cognitive idea.",
            "We have a small number of primitive elements.",
            "We have some rule for combining this primitive elements and we can use that to recursively define something over an infinite set."
        ],
        [
            "So the basic idea is we are going to take a finite set of Contacts F which are going to include Lambda Lambda.",
            "We're going to take a finite set of substrings which are going to include the primitive elements which have got to be at a minimum, the words the elements of Sigma, and we're going to take any language L OK, and we're just going to be considering a small part of this data, so the particular, but we're going to be talking looking at is this F rap KK.",
            "So this just means we take.",
            "Any element from K any two item case sticking together and wrap them in any element from F. OK, so this here is polynomially bounded set OK. And what the data actually look at is just the grammatical elements of this set.",
            "OK, so we have three elements.",
            "We have all sort of examples K. We have our set of context.",
            "We're going to use to model the distribution, and then we have our data.",
            "If you like D, which is represents a finite set of of the language."
        ],
        [
            "So the 1st way you could do this OK?",
            "And this is the most natural way to do it is to partition all of our strings into congruence classes.",
            "So we're going to have exact identity of distribution.",
            "So here this congruent sign there we're going to have consider each string you are going to consider the set of all strings that are exactly distribution identical.",
            "So that means we can swap completely freely any you for a V without changing dramatic County.",
            "So using our simple example here, we have.",
            "A.",
            "There's only one string with that precise distribution.",
            "We have a B which is infinite AAB, which is infinite and then we have a few slightly weird ones.",
            "The empty string is is unique, and then we have another class of all things that have an empty distribution, so these are substrings that don't occur anywhere in our language.",
            "OK, so this is a partition of all the strings of Sigma star into these congruence classes."
        ],
        [
            "Now, let's say we wanted to build a context free grammar.",
            "OK, now I'm.",
            "This is meant to just illustrate the 'cause.",
            "I assume people if people are familiar with context free grammars, then this will be, I think, a good way of understand what's going on here so we can consider for a non tunnel and the set of all strings are generated from that non terminal right?",
            "And suppose you have a rule an goes to PQ, then clearly the set of strings generated from N is going to include the concatenation of the set cell of PLQ, right?",
            "This is almost the defining property of water context free grammar is.",
            "So if we go."
        ],
        [
            "Backwards, let's say we define a set of strings XY, zed, and suppose it happens to be that X contains Y. Zed, then maybe we can add a rule X goes to.",
            "Why is that OK?",
            "So we're just taking we just in line with this basic idea of running things backwards.",
            "Here we start off by defining what are nonterminals going to kind of refer to, what they're going to generate, and then from that we can then infer the rules so."
        ],
        [
            "Now Congress cause haven't had one very nice property, right?",
            "Which is that if we take the Congress cause of you and the current cost of V and we concatenate them, then it is always a subset of the Congress class of UV.",
            "So that means we can always add a rule.",
            "From the comments, cost of UV to the common cause of you and the comments of E. OK, so this is very, very fundamental.",
            "What this is saying is that we can essentially read off the structure of our representation from the structure of these congruence classes.",
            "There's what's called the syntactic monoid, and we can just simply define a natural representation and natural context free grammar based on.",
            "On these symbols, the problem is it's kind of hard to tell whether you is congruent to V because we have to tell whether you is congruent to V. We need to test infinitely many.",
            "Infinitely many context mean to check that we need to have precise equality of distribution."
        ],
        [
            "So let me just give an example of this.",
            "So again, using the simple example here, we have the common cause of a, which is just the string.",
            "A further cons cause of ABB.",
            "If we concatenate the common cause of a with the concept of a B, then we have the input set of strings a ABB ABB which are all.",
            "In the Congress costs ABB, which is equal to the current source AP, which is in the language.",
            "OK.",
            "So this."
        ],
        [
            "Means that we can simply write down a grammar.",
            "We start off by having our stock symbol S an we.",
            "We go to the two congruence classes that it contains."
        ],
        [
            "We have some other trivial rules here.",
            "We can just say that the columns cost of a can be rewritten as a common customer.",
            "A console should be can be rewritten as B."
        ],
        [
            "And then we have some slightly more interesting rules.",
            "Here we have that baby can go to ABB.",
            "Baby can also go to A&BAB can go to a ABB and so on.",
            "So The thing is we don't have.",
            "There's no, there's no search here.",
            "There's no computation right?",
            "Once we define the reference of this thing then the inference problem simply dissolves OK.",
            "If something becomes trivial."
        ],
        [
            "And then you also have a few other trivial rules which you don't need to worry about.",
            "Perhaps he is example reasonably clear.",
            "OK, so."
        ],
        [
            "How can we actually?",
            "No, what we're doing is basically the strategy here is defined some chunks of strings distributionally, so the this way we define the strings to be congruence classes, and this is in some sense as small as possible's classes.",
            "We can consider OK if two strings are absolutely identical, then there's no way you can distinguish but absolutely identical distribution.",
            "There's no way you can distinguish them, so this is the most fine grain possible representation we could have.",
            "An alternative way of sort of jewel way would be to take a particular context and consider all strings that occur in that context.",
            "OK, so the first one comments causes a set of strings that share every single.",
            "Context these here are just simply sets of strings that have one context in common OK, and if there's a certain particular case which is when these two coincide, which is when they're called substitutable."
        ],
        [
            "Now this is no concept.",
            "I recently found a paper by John Myhill who's the guy who counted Myhill Nerode theorem in 1950.",
            "He wrote a letter to the journalist symbolic Logic, so I don't feel bad about not having found it until recently, but he defines a particular concept which calls regular.",
            "Obviously, now we use regular to refer to something different, but it's very natural concept now in 2005.",
            "We came up with a polynomial learning which is kind of the first, the most primitive elementary result.",
            "The context for inference, which is based on precisely that property.",
            "OK, mathematically, identical definition.",
            "So natural question is, you know, while the delay OK, Why was there basically a 50 year gap between this?",
            "This property being hypothesized as being something learnable and this is in the context of, you know, a discussion about distributional learning.",
            "Why's there 50 year gap between that and the first formal result in the field?",
            "OK, so I'll answer that later on, but that is I think that's if I'm saying, you know, I'm here saying, oh, there's this wonderful new idea called distribution.",
            "Learning is not enough.",
            "I could explain why is such an obvious idea being neglected for so long.",
            "And if you look at this paper, you will see that it is really quite trivial and the Journal version is a bit long because the review is basically didn't believe the result they wanted.",
            "You know I kept on having to expand every every lemma right, but the basic result is very trivial.",
            "So."
        ],
        [
            "The basic results here that from if we just have positive data then we get this substitutable result.",
            "We polynomial result from positive data alone.",
            "For a small class are substitutable languages if the data is generated from a probabilistic context free grammar, then we have a a result on PAC Learning a particular class of languages.",
            "This is slightly incomplete.",
            "And then this year I have a paper on the submission which is showing an efficient membership query algorithm where you pick a finite set of contexts and you test for congruence using whether you.",
            "Whether the basically rather testing an infinite set of of context you test a finite set OK, this gives you a polynomial query result for a very large class of context free grammars."
        ],
        [
            "So this is progress, but.",
            "One symbol per congruence class is is very natural, but it's just not going to work for natural language is OK 'cause you have very, very many congruence classes and they are very close together.",
            "Exact substitutability is very, very rare, as the cat dog example.",
            "I mean, I think Tuesday is probably substitutable for Wednesday, but beyond that is very very difficult to find, you know.",
            "Beyond this very efficient, it's very difficult to find words that exactly."
        ],
        [
            "Substitutable.",
            "And the land, were you using really assumes that either there, either 2.",
            "Either 2 strings are completely identical, or they're completely unrelated, and that's too crude.",
            "We need to have a representation that can represent the structure of the congruence classes, and there's another problem, which is the languages aren't context free.",
            "OK, so if you're using a context free representation, we know right away that we're we're not.",
            "We're not looking the right place."
        ],
        [
            "So what we gonna do is I'll just describe this using.",
            "What kind of simple observation table?",
            "So here we have a table and down the left.",
            "Here we're going to have a sequence of strings from K K1 up to Kate and along the top we're going to label these with Contacts F1F up to F-10."
        ],
        [
            "And what we can do is you can just fill this in with membership queries, right?",
            "So what we're going to do is we want to say this is going to be filled in.",
            "If this string, combined with that context is grammatical alright.",
            "So the question really is, given this sort of information about a language, can we construct representation?",
            "Can we learn grammar for this?"
        ],
        [
            "Now, substitutable grammars are precisely the ones where these form into blocks like this.",
            "Again, in this case, it's pretty easy to learn these things.",
            "We just take one non tonal feature these blocks and read off the grammar and if we have enough data, we're going to, we're going to get it right.",
            "But even in very."
        ],
        [
            "Very simple examples.",
            "You're not going to get this, so we're going to run looking at blocks.",
            "We're going to look at kind of maximal rectangles here, so maximal rectangle here, which were coloring in red is a rectangle, so it's an area which is completely filled in, and its maximal in the sense you can't increase either the set of strings or the set of contexts without violating the fact that it's all filled in.",
            "OK, so this is one of those things where it's very easy to just show you.",
            "Which set of maximal rectangles are?",
            "OK, so these are the maximum rectangles.",
            "OK, so these are these all the maximum rectangles."
        ],
        [
            "Now what's the relation to context free grammars?",
            "Again, I'm not really interesting context free grammars, 'cause I think they're wrong, but it's a good way of explaining it.",
            "Let's say we have a context free grammar for each non time.",
            "We can consider the yield right?",
            "The set of strings that you can derive.",
            "From that.",
            "I'm going to the context of that non terminal which is a set of context you can derive from that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm Alex Clark.",
                    "label": 0
                },
                {
                    "sent": "I'm from computer science Department, but I sorta identify myself more as a computational mathematical linguist.",
                    "label": 0
                },
                {
                    "sent": "I have",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three, there are three fields in this topic.",
                    "label": 0
                },
                {
                    "sent": "There's linguistics, cognitive science, and machine learning.",
                    "label": 1
                },
                {
                    "sent": "I'ma talk briefly about the sort of two pairwise combinations of linguistics and cognitive science and computational linguistics and machine learning, but I'm going to focus most of the talk on a topic that's at the intersection of all three, which is really, I think, one of the central problems of cognitive science, which is how to count with computation explicit and cognitively plausible models of language acquisition.",
                    "label": 1
                },
                {
                    "sent": "And I should, I guess, warn you that I have a sort of non standard view of the field, so you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Obviously what I'm.",
                    "label": 0
                },
                {
                    "sent": "I believe in what I'm saying, but I accept their other alternate views, so you shouldn't necessarily take my views as being representative of the field as a whole.",
                    "label": 0
                },
                {
                    "sent": "What I want to one of the things I want to convince you.",
                    "label": 0
                },
                {
                    "sent": "Is that a kind of fairly narrow field of of mathematics, really, which is grammar induction actually potentially is very important to the overall cognitive science enterprise.",
                    "label": 0
                },
                {
                    "sent": "Now, obviously I may just be deluding myself, and it's because I work in this field, but I think it's so important, but I chose to work in this field because I thought it was important, not the other way around, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to talk about in the first part of the talk, I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Something that's called the argument from the poverty of the stimulus, and a few related ideas about cognitive science about language acquisition, about what the proper study of linguistics is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk very briefly, then also about supervised learning and unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "In computational linguistics, and then the second half of the talk, I'm going to sort of propose a solution.",
                    "label": 0
                },
                {
                    "sent": "So the first half is really posing a question, and the second half is trying to sketch a solution, and I'm going to talk about what's called distributional learning, which is kind of an old-fashioned idea from the from the 1950s there actually.",
                    "label": 0
                },
                {
                    "sent": "Predates that you can find roots of it in in car naps ratings from the 30s.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to talk about how you can use distributional learning to find efficient so polynomial probably correct algorithms for learning certain classes of context free and context sensitive languages.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to talk a little bit about structural descriptions.",
                    "label": 0
                },
                {
                    "sent": "I wasn't initially planning to talk about that, but out of discussions with some people here, I think it's maybe a more important thing.",
                    "label": 0
                },
                {
                    "sent": "So I've stuck in some some stuff there.",
                    "label": 0
                },
                {
                    "sent": "So really, this talk can be thought of as a compliment to next talk to half of next talk Nick was talking about.",
                    "label": 0
                },
                {
                    "sent": "Inefficient algorithms that learn from our realistic data.",
                    "label": 0
                },
                {
                    "sent": "I'm focusing much more on the efficiency point of view, which I think is particularly important for language acquisition.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Jerry Fodor, who's always good for a laugh.",
                    "label": 0
                },
                {
                    "sent": "Said he said the argument from the party of stimulus is the existence proof for the possibility of cognitive science.",
                    "label": 1
                },
                {
                    "sent": "So I mean, Joe does not afraid of stating things strongly.",
                    "label": 0
                },
                {
                    "sent": "I think in a sense this is right.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look historically their engines of cognitive science in the cognitive revolution, the argument from the poverty of the stimulus, and particularly Chomsky's.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Early writings.",
                    "label": 0
                },
                {
                    "sent": "Nuclear packing simple behaviors.",
                    "label": 0
                },
                {
                    "sent": "Models like like skinners model.",
                    "label": 0
                },
                {
                    "sent": "So his famous review in 1959 of Skinner's book on Language is kind of in some respects kicked off cognizance.",
                    "label": 0
                },
                {
                    "sent": "And I think that he was right in a number of important areas.",
                    "label": 0
                },
                {
                    "sent": "First of all, he was right to say that those simple behaviourists models were inadequate.",
                    "label": 0
                },
                {
                    "sent": "He was right, they do not explain language use.",
                    "label": 0
                },
                {
                    "sent": "You need to hypothesize are very rich internal structure and I think he was one of the first people to point out just how.",
                    "label": 1
                },
                {
                    "sent": "How kind of strange and puzzling the structure of languages?",
                    "label": 1
                },
                {
                    "sent": "And he was also right to insist on computationally explicit models of natural language, and he developed the well known Chomsky hierarchy and just a quick, how many people here know what are context free grammars?",
                    "label": 0
                },
                {
                    "sent": "OK, good excellent.",
                    "label": 0
                },
                {
                    "sent": "OK, I was worried that would OK so so he did not bring hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And finally I think his view of linguistics as basically being a branch of psychology.",
                    "label": 1
                },
                {
                    "sent": "That when we study linguistic we should study it as being some sort of property of the human mind.",
                    "label": 0
                },
                {
                    "sent": "So I think you will spot on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lots of things.",
                    "label": 0
                },
                {
                    "sent": "And he identified the basic problems of linguistics.",
                    "label": 0
                },
                {
                    "sent": "He framed the questions in one of his books.",
                    "label": 0
                },
                {
                    "sent": "I think this is the knowledge of language.",
                    "label": 0
                },
                {
                    "sent": "In 96 is the key quicker.",
                    "label": 0
                },
                {
                    "sent": "The two key questions were what constitutes knowledge of language in the individual, and how is this knowledge acquired by 8 speakers so very much forefronts the acquisition as being the acquisition plans being one of the key problems that we need to address.",
                    "label": 0
                },
                {
                    "sent": "And again, I think he's he's absolutely right.",
                    "label": 0
                },
                {
                    "sent": "'cause it turns out to be a very, very puzzling issue.",
                    "label": 0
                },
                {
                    "sent": "More recently, Ray Jackendoff in a paper called 3 Alternative Mill misconceptions of language Alternative misconceptions of Language says that a theory of language needs to come up with three need to satisfy three simultaneous constraints.",
                    "label": 0
                },
                {
                    "sent": "You need to have a descriptive constraint, so the class of languages that your model defines must be sufficiently rich to represent natural languages.",
                    "label": 0
                },
                {
                    "sent": "It has to satisfy learnability constraint.",
                    "label": 0
                },
                {
                    "sent": "And then you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to have some sort of plausible story about how the trial could acquire these representations from the data available, and finally he's adding additional constraint that wasn't present in Chomsky's original writings.",
                    "label": 0
                },
                {
                    "sent": "That Chomsky is breaking right, which is an evolutionary constraint saying must not posit a rich and evolutionary implausible language faculty, so going back to next points about evolution.",
                    "label": 0
                },
                {
                    "sent": "Evolution is comparatively recent innovation, evolutionary history, so we can't any theory that posits.",
                    "label": 0
                },
                {
                    "sent": "A huge amount of very rich of all structure.",
                    "label": 0
                },
                {
                    "sent": "This is probably not right now.",
                    "label": 0
                },
                {
                    "sent": "Satisfying these three conditions simultaneously turns out to be extremely hard.",
                    "label": 0
                },
                {
                    "sent": "In fact, at the moment, I'd say we really have no, no, no satisfactory solutions, though obviously I'm going to at some point.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say, but my theory can solve these problems.",
                    "label": 0
                },
                {
                    "sent": "But there are a number of strategies that people have used to reconcile these three things.",
                    "label": 0
                },
                {
                    "sent": "So one strategy that Chomsky's favorite has been basically to prioritize the first 2 and ignore the third one.",
                    "label": 0
                },
                {
                    "sent": "So basically to say, let's allow ourselves evolutionary implausible, very rich innate structure and then we can maybe solve the first 2 problems.",
                    "label": 0
                },
                {
                    "sent": "Another way, which is maybe more connectionist way, is to sort of slightly ignore the true complexity of natural languages to us to simplify the problem by considering much more restricted class of languages, and then maybe we can learn them using some simple connections model SRN or something which is evolution implausible.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't think that's the right strategy because we have to confront the real complexity of language head on.",
                    "label": 0
                },
                {
                    "sent": "OK, we have to recognize the fact languages are really, really complicated.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be able to solve them using some simple finite state model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about the APS now so the argument from the poverty of the stimulus.",
                    "label": 1
                },
                {
                    "sent": "It's a very classic argument, and there are many different versions.",
                    "label": 0
                },
                {
                    "sent": "There are some formal versions.",
                    "label": 0
                },
                {
                    "sent": "There are some empirical versions, but one sort of simple conception is that people attain knowledge of the structure of their language, which no evidence is available in the data which they are exposed as children.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overstating it.",
                    "label": 0
                },
                {
                    "sent": "Amor a slight more nuanced view is presenting a very good article by Jeff Pohlman, Barbara Schultz in the Linguistic Review, where they summarize a whole class of these organs very well, but basically what they're saying that children learn natural languages.",
                    "label": 0
                },
                {
                    "sent": "They learn them rapidly in just a few years.",
                    "label": 0
                },
                {
                    "sent": "They learn them consistently.",
                    "label": 0
                },
                {
                    "sent": "That which means that they pretty much always learn them.",
                    "label": 0
                },
                {
                    "sent": "They arrive at a fairly uniform outcome.",
                    "label": 0
                },
                {
                    "sent": "They don't receive explicit instruction or explicit reward.",
                    "label": 0
                },
                {
                    "sent": "The data they receive is sparse.",
                    "label": 1
                },
                {
                    "sent": "It's incomplete, and it's noisy.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea is that the course of language acquisition, given this environment is language acquisition is just too hard to be explained by by standard models of learning.",
                    "label": 1
                },
                {
                    "sent": "OK, the task is too difficult, right?",
                    "label": 0
                },
                {
                    "sent": "So there must be something else going on.",
                    "label": 0
                },
                {
                    "sent": "There must be some additional resource that the child brings to solve this problem, and that resource is considered to be some.",
                    "label": 0
                },
                {
                    "sent": "Innate language specific knowledge, so the reason that the child is able to do it is it comes to the task with very rich prior knowledge about the sorts of languages there are.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the standard sort of what we call the empirical version of the APS, the citation there is to a very good paper by Amy Perforce, Josh and Terry Rigor, and it's kind of old Chestnut here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that when you're learning English, you have simple declarative sentence like the student is hungry and then to form the polar interrogative.",
                    "label": 1
                },
                {
                    "sent": "From that you'll grammatical form is is the student hungry right so?",
                    "label": 1
                },
                {
                    "sent": "I think even in terms of movement, what's happened here is that the auxiliary here is being has been fronted.",
                    "label": 0
                },
                {
                    "sent": "So given these two examples there is there are one can think of those being if your child confronted with this data, you can come with various possible hypothesis.",
                    "label": 0
                },
                {
                    "sent": "One is that the.",
                    "label": 0
                },
                {
                    "sent": "I'll just move the first you move the first auxiliary you see to the front of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Another hypothesis might be that you'll move the auxiliary from the main verb from the main clause to the front.",
                    "label": 0
                },
                {
                    "sent": "Now these obviously on this simple pair of sentences, these get exactly.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And result, but when you consider more complex sentence here, so the student who is in the garden is hungry, we can see that these two rules will give different results.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you apply one rule, you'll get the correct answer is the student who is in the garden hungry?",
                    "label": 1
                },
                {
                    "sent": "And if you apply the other rule the wrong rule you'll get is the student who in the garden is hungry, which is ungrammatical.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that children do not see examples of Type 4 that would help them to distinguish between these two hypothesis, but do nonetheless produce the right examples.",
                    "label": 1
                },
                {
                    "sent": "OK, now there is a factual problem with this.",
                    "label": 0
                },
                {
                    "sent": "Which Pullman Schultz point out that in fact, children do see examples of this type, but in a way that's slightly missing the point.",
                    "label": 0
                },
                {
                    "sent": "Because given the children do learn, they generalize beyond their experience.",
                    "label": 0
                },
                {
                    "sent": "There going to be some classes of sentence that they don't see but nonetheless learn to correct.",
                    "label": 0
                },
                {
                    "sent": "Learn to predict, learn to perform correctly on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's a good long quote from from Chomsky giving with some sort of fairy.",
                    "label": 0
                },
                {
                    "sent": "What's a good word right rhetoric here?",
                    "label": 0
                },
                {
                    "sent": "So basically what he's doing in this in this in this quote is he's attacking a certain class of learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "He's saying that the evidence about the character of the generative grammars when we look at sorts of grammars that we need, we find that they cannot be.",
                    "label": 1
                },
                {
                    "sent": "There's no way these could be acquired through simple inductive process through step by step inductive operations, segmentation, classification, substitution procedures of any sort that have been developed within linguistics, psychology, or philosophy.",
                    "label": 1
                },
                {
                    "sent": "Further increases speculations contribute nothing that even faintly suggests a way of overcoming the intrinsic limitations of the methods that have so far been proposing elaborated.",
                    "label": 1
                },
                {
                    "sent": "So he's forcefully expressed his view about the inadequacy of certain methods, and I guess part of my part of this talk is really saying, well, actually precisely these sorts of methods.",
                    "label": 0
                },
                {
                    "sent": "And in fact, explain the acquisition of large chunks of language.",
                    "label": 0
                },
                {
                    "sent": "OK, that at the time he was in a way quite ripe.",
                    "label": 0
                },
                {
                    "sent": "I mean in the 1950s in the early 60s, when these methods were originally studied, everyone, any concrete proposals.",
                    "label": 0
                },
                {
                    "sent": "But now we can come up with quite explicit concrete proposals, and we can prove that they are correct and we can prove they will.",
                    "label": 0
                },
                {
                    "sent": "An interesting class of languages.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just again touching on what Nick was saying the language instinct.",
                    "label": 0
                },
                {
                    "sent": "So he the APS is taking be an argument for linguistic nativism and linguistic nativism is, broadly speaking, the claim that language acquisition proceeds largely through innate language, specific mechanisms and representations.",
                    "label": 1
                },
                {
                    "sent": "So by language specific, I mean representations of specific to the domain of language, not specific to one particular language, like English.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 1
                },
                {
                    "sent": "So, informally, you know, a lot of grammar is actually.",
                    "label": 0
                },
                {
                    "sent": "Encoded in the genome, directly or indirectly.",
                    "label": 1
                },
                {
                    "sent": "And it's important to distinguish between this, which is has some sort of content, and the rather vacuous claim that you know we have some innate ability to acquire language.",
                    "label": 0
                },
                {
                    "sent": "I mean clearly human beings acquire language, lobsters, dogs, etc don't.",
                    "label": 0
                },
                {
                    "sent": "Therefore we have some innate ability.",
                    "label": 0
                },
                {
                    "sent": "The question is whether it's a domain specific ability, whether specific to language, or whether it's just an offshoot of other more general purpose cognitive facilities.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it's worth saying that Chomsky no longer really subscribes to the view of linguistic nativism.",
                    "label": 1
                },
                {
                    "sent": "So, in the minimalist program here is largely abandoned.",
                    "label": 0
                },
                {
                    "sent": "This claim that there's a very rich, domain specific thing.",
                    "label": 0
                },
                {
                    "sent": "How we store maintains that the APS is valid, and thus there's a kind of tension or contradiction.",
                    "label": 0
                },
                {
                    "sent": "If you like, because there's really no satisfactory explanation about how language acquisition takes place, since he no longer considers there's a very rich domain specific.",
                    "label": 0
                },
                {
                    "sent": "Learning bias and he also no longer think he still thinks that distributional methods won't work.",
                    "label": 0
                },
                {
                    "sent": "So there is a kind of a gap.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there's also a formal version of the APS.",
                    "label": 1
                },
                {
                    "sent": "And this is a quote from Ken Wexler saying, well, the strongest, most central arguments for night Nurse does continue to be the organs from APS.",
                    "label": 1
                },
                {
                    "sent": "An learnability theory.",
                    "label": 0
                },
                {
                    "sent": "The basic results of the field include the demonstration without serious constraints on nature of human grammar.",
                    "label": 1
                },
                {
                    "sent": "No possible learning mechanism can in fact learn the class of human grammars.",
                    "label": 0
                },
                {
                    "sent": "So I think there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a sense in which there is right.",
                    "label": 0
                },
                {
                    "sent": "This is right, I think.",
                    "label": 1
                },
                {
                    "sent": "Complete tabular rasa learning is not possible, but it does sort of negative results.",
                    "label": 0
                },
                {
                    "sent": "There are don't really rule out domain general learning, so the formal items that have been put forward where they've been explicit don't really support linguistic nativism.",
                    "label": 0
                },
                {
                    "sent": "Even under the most optimistic interpretations.",
                    "label": 0
                },
                {
                    "sent": "And I have a book coming out on this this year, which covers us in great detail.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But sort of summarizing it.",
                    "label": 0
                },
                {
                    "sent": "The problem with these arguments is that there's a very important distinction which to machine learning people.",
                    "label": 0
                },
                {
                    "sent": "I guess it's fairly clear that there's an important distinction.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis class for learning algorithm and the class of anger languages that the algorithm can learn.",
                    "label": 1
                },
                {
                    "sent": "So clearly the hypothesis class has to include in a sense the class of languages.",
                    "label": 1
                },
                {
                    "sent": "And show the learnable class restriction in some way, but it's very difficult to show that the hypothesis class needs to be restricted, and so to give a concrete example, if you have some distribution free uniform pack learning model, then the learnable class must have finite VC dimension.",
                    "label": 1
                },
                {
                    "sent": "But the hypothesis class does not need to have VC dimension finite.",
                    "label": 0
                },
                {
                    "sent": "It can be unbounded, and that's kind of a standard result from one of David House those papers.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me now move on to how language acquisition has been studied and this is a comment by Steven Pinker, which is, I think, wonderfully lucid.",
                    "label": 0
                },
                {
                    "sent": "Just how X is learned.",
                    "label": 0
                },
                {
                    "sent": "You first have to understand what X is.",
                    "label": 1
                },
                {
                    "sent": "I think this is.",
                    "label": 0
                },
                {
                    "sent": "Completely reasonable, kind of obvious, and I think it's you know completely wrong, but it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's certainly very natural way to proceed the problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To that problem, so craning Petrosky ships number.",
                    "label": 0
                },
                {
                    "sent": "Say you know first step one is you try to find principles that characterize human grammars and then you try to find out how they could be learned.",
                    "label": 1
                },
                {
                    "sent": "Which things could be learned?",
                    "label": 0
                },
                {
                    "sent": "Which are more likely to be innately specified?",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this suggests a sort of standard methodology.",
                    "label": 0
                },
                {
                    "sent": "OK, step one, you try to construct a descriptively adequate representation for natural language and then Step 2.",
                    "label": 1
                },
                {
                    "sent": "You try to design learning arguments with those representations, so step one would be done by linguists like Chomsky.",
                    "label": 0
                },
                {
                    "sent": "Whatever Step 2 is meant to be done by sort of people like me, people in this room, I suppose, and I spend a certain amount of time in the trenches trying to do justice, but.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem with step one, which is to construct a descriptively adequate grammar, is that it failed OK, no one has ever managed to make a descriptively adequate generative grammar for any natural language, not even the most studied language which is English in order to account for NYU fax which occasion cropped up, such as the context sensitive sensitivity of Swiss German representations, were made more powerful and expressive.",
                    "label": 1
                },
                {
                    "sent": "Current source statistical techniques don't really try to separate grammatical from ungrammatical sentences.",
                    "label": 1
                },
                {
                    "sent": "General grammarians have largely abandoned the task of trying to construct a large scale grammars.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In step two, well, this also failed, so learning even the lowest level of the Chomsky hierarchy, regular grammars is computationally hard even under really quite an easy paradigm.",
                    "label": 1
                },
                {
                    "sent": "So this is done Anglin, and correcting off without even using queries.",
                    "label": 0
                },
                {
                    "sent": "You can't learn regular grammars.",
                    "label": 1
                },
                {
                    "sent": "We have some heuristic algorithms that can induce constituent structures, such as the excellent work of Dan Klein.",
                    "label": 0
                },
                {
                    "sent": "But the cost of representation, do we actually need to represent the full complexity of natural language are even richer?",
                    "label": 1
                },
                {
                    "sent": "You need things like El tags or various things in the hierarchy of abstract categorial grammars and it's really you know out of the question that we could come up with.",
                    "label": 0
                },
                {
                    "sent": "Efficient learning algorithms for these classes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This this basic strategy doesn't really seem to workout trumpski.",
                    "label": 0
                },
                {
                    "sent": "Again, I think here saying hitting the target absolutely spot on says that you know there is this tension in order to achieve descriptive adequacy, I in order to capture the full complexity of natural language, you need to increase the power of your devices.",
                    "label": 1
                },
                {
                    "sent": "But in order to solve the learning problem which equals Plato's problem, we need to restrict.",
                    "label": 1
                },
                {
                    "sent": "So there's this tension between the desire to.",
                    "label": 0
                },
                {
                    "sent": "Increase the class of languages as you encounter GNU GNU Facts new languages you know as you discover some new tribe like the Pijar or whatever you need, you may need to increase your class in order to learn them.",
                    "label": 1
                },
                {
                    "sent": "You want your class to be as restrictive as possible, so is this tension between these two desires to increase and to decrease it?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the model that Chomsky proposed as a solution to this, the principles and parameters models where language is essentially entirely innately specified apart from a finite number of binary value parameters, is fails for a number of reasons, it's.",
                    "label": 1
                },
                {
                    "sent": "It's evolution implausible.",
                    "label": 1
                },
                {
                    "sent": "There's no good learning model.",
                    "label": 1
                },
                {
                    "sent": "As we probably know, simply the fact that you have a finite number of parameters doesn't solve the learning problem.",
                    "label": 0
                },
                {
                    "sent": "There's no agreement on what the parameters might be.",
                    "label": 1
                },
                {
                    "sent": "There's no tension between these two classes, and she simply sort of stipulated a finite constraint.",
                    "label": 0
                },
                {
                    "sent": "Ann is currently being abandoned.",
                    "label": 0
                },
                {
                    "sent": "So what are the?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mobile solutions, well, it's important to realize that linguists don't know what the representations are, so you wouldn't necessarily believe that.",
                    "label": 1
                },
                {
                    "sent": "If I said it because I'm not, I'm not a proper linguist, but Jim Blevins, in a recent survey article in the Journal of Linguistics, says at the most fundamental level, it is not clear there is any meaningful empirical motivation for the representational assumptions of any current formal model of syntax which is.",
                    "label": 1
                },
                {
                    "sent": "Incorrect linguists cannot agree about very rudimentary facts about linguistic representations.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you have a what we would call a non phrase of the cat, most of us would consider that the word cat is the head.",
                    "label": 0
                },
                {
                    "sent": "But nearly all linguists were considered that in fact this is a DP and it is headed by the determiner phrase the.",
                    "label": 0
                },
                {
                    "sent": "Not any cannot agree on this.",
                    "label": 0
                },
                {
                    "sent": "They cannot agree even on a methodology where one might.",
                    "label": 0
                },
                {
                    "sent": "One might have to decide between these two things is just not an empirical empirical point.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we don't know what the representations are OK, but we do know that they are learnable.",
                    "label": 1
                },
                {
                    "sent": "We know that they are learnable because children do in fact succeed in learning.",
                    "label": 0
                },
                {
                    "sent": "So the obvious thing is really.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To turn that that basic methodology around rather than starting off by trying to build a descriptively adequate model and then trying to learn it, you should put learnability first.",
                    "label": 0
                },
                {
                    "sent": "If you construct some super powerful class of languages and representation class without any thought of learnability, you're not going to be able to succeed, so you have to design representations from the ground to be learnable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so step one here is, you build simple learnable representations and then Step 2 is you try to gradually increase their expressive power while maintaining learnability.",
                    "label": 1
                },
                {
                    "sent": "OK, so the end result is the same point.",
                    "label": 0
                },
                {
                    "sent": "You want something that is both large enough and learnable.",
                    "label": 0
                },
                {
                    "sent": "But we're putting the more the more difficult constraint first.",
                    "label": 0
                },
                {
                    "sent": "Learnability is hard.",
                    "label": 0
                },
                {
                    "sent": "Just come out with a big class.",
                    "label": 0
                },
                {
                    "sent": "Representations is easy.",
                    "label": 0
                },
                {
                    "sent": "There are literally dozens of them.",
                    "label": 0
                },
                {
                    "sent": "There are very few approaches.",
                    "label": 0
                },
                {
                    "sent": "Too efficient learning.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think I'd share what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "I'm going to skip these two students.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So talk a little bit about unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So in computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "As a community, people in computational linguistics are generally not interested in cognitive issues.",
                    "label": 0
                },
                {
                    "sent": "They are interested in building efficient systems that can press this language.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning, which is a classic paradigm, involves learning from annotated data, and that causes an annotation bottleneck.",
                    "label": 1
                },
                {
                    "sent": "OK, both because there are some languages which have a resource poor have compared to the few annotation linguistic resources like annotated corpora, tree bank and so on.",
                    "label": 0
                },
                {
                    "sent": "And Secondly also for the minority people interesting cognitive modeling.",
                    "label": 0
                },
                {
                    "sent": "So there's particularly, I think, in the context of this talk, there's some this workshop.",
                    "label": 0
                },
                {
                    "sent": "There's some trick, interesting work done on segmentation using nonparametric Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "By Sharon Goldwater, Mark Johnson and some people like that, but.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll talk a little bit about grammar induction.",
                    "label": 1
                },
                {
                    "sent": "And the reason this is strictly important is becausw syntax clearly needs to have more powerful models and simple regular grammars of finite state models.",
                    "label": 1
                },
                {
                    "sent": "Lot of these other problems are quite well modeled by regular orphan assembles, and we have a good idea about how to solve those, but we have very few good ideas about how to learn context free grammars.",
                    "label": 1
                },
                {
                    "sent": "So there are two basically ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "One is an empirical approach and one is a more theoretical approach.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the empirical approaches that you use you take some real data, a corpus of language, possibly from the Wall Street Journal or possibly from the child's database of child directed speech.",
                    "label": 0
                },
                {
                    "sent": "We run our learning.",
                    "label": 0
                },
                {
                    "sent": "We implement some learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "We run this algorithm and then we evaluated against some gold standard, some linguistic annotations and typically use some heuristic algorithms.",
                    "label": 1
                },
                {
                    "sent": "So client and Manning Dan Klein's work is very typical.",
                    "label": 1
                },
                {
                    "sent": "This you use sentences of length lesson town from the Wall Street Journal.",
                    "label": 1
                },
                {
                    "sent": "You evaluate using modified positive allometric.",
                    "label": 0
                },
                {
                    "sent": "So modifications of the way that you would use to evaluate standard supervised parsing algorithms and they show.",
                    "label": 0
                },
                {
                    "sent": "Good results using constraint generating binary tree is subject to some distributional heuristic.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm not really focusing on that.",
                    "label": 0
                },
                {
                    "sent": "I'm focusing more on theoretical approach where we try to, you know, basically find out what sorts of algorithms are there that can perform this task well, and there are two basic problems.",
                    "label": 0
                },
                {
                    "sent": "The first is problems that we can think of is information theoretic problems, so this is problems with the absence of negative data which Nick talked about problems to do with VC dimension problems with sparsity noise and so on.",
                    "label": 1
                },
                {
                    "sent": "And my view is that we know how to attack these problems.",
                    "label": 1
                },
                {
                    "sent": "Whether using a minimum description length, nonparametric Break, Bayes structure, risk minimization, we have a variety of ways of.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attacking these problems.",
                    "label": 0
                },
                {
                    "sent": "So the second problem is the computational complexity problem, and that is the complexity of finding the best hypothesis given the data.",
                    "label": 1
                },
                {
                    "sent": "And there's some for quite some time now.",
                    "label": 0
                },
                {
                    "sent": "People have realized there's this problem is basically extremely hard.",
                    "label": 0
                },
                {
                    "sent": "The standard representations so gold gave the 1st result in 1978.",
                    "label": 0
                },
                {
                    "sent": "There's some famous papers by cons and violent and so on, and one of the reasons why I think we should take these particular problem seriously is that the previous results about VC dimension, so not specific to language there in general.",
                    "label": 0
                },
                {
                    "sent": "Like the very general problems about machine learning about inference, the limits of learning the limits of inductive inference.",
                    "label": 0
                },
                {
                    "sent": "But these problems are specific, ready to learning particular class of representation, and they are often based on embedding cryptographic problems in learning problems.",
                    "label": 1
                },
                {
                    "sent": "So the idea that is finding the right hidden structure, the right hidden representation is as hard as cracking a code so you can embed problems like factoring Blum integers and so on into this learning problem Now.",
                    "label": 0
                },
                {
                    "sent": "It shouldn't make you slightly suspicious.",
                    "label": 0
                },
                {
                    "sent": "I mean if you have a learning model that doesn't distinguish between cracking a cryptographically hard problem, and which is a very adverse aerial system, and the situation where the child is in where he's in comparatively, your shares in a comparatively helpful environment where the parent is trying to support the learning process, then you should be a little bit suspicious about this learning model.",
                    "label": 0
                },
                {
                    "sent": "Being robust specialist is right because even quite simple problems like doing a bit of clustering some data, using a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be intractably hard in general.",
                    "label": 0
                },
                {
                    "sent": "Now obviously sometimes it's quite easy when the sense of Gaussians are widely separated.",
                    "label": 0
                },
                {
                    "sent": "It's very easy when they're very close together.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be hard, so I think we need to use a little bit of common sense of these algorithms and realize that you know the overall problem may be hard, but nonetheless we identify situations analogous to clustering Gaussians where they.",
                    "label": 0
                },
                {
                    "sent": "Centers are very well separated where he turns out to be quite easy.",
                    "label": 0
                },
                {
                    "sent": "And I think it is important to recognize this constraint, and there's what's called tractable cognition thesis.",
                    "label": 0
                },
                {
                    "sent": "You know, human cognitive capacities are constrained by the fact that humans are finite systems with limited resource for computation.",
                    "label": 1
                },
                {
                    "sent": "That's kind of a. I guess it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a truism really, but I think we should recognize this now.",
                    "label": 1
                },
                {
                    "sent": "My view is we have these two problems.",
                    "label": 1
                },
                {
                    "sent": "We have the information theoretic problem with computational complexity problem is too hard to try to solve both these problems together.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to try to solve the 2nd and assume the first is being dealt with.",
                    "label": 0
                },
                {
                    "sent": "Now this can be a very, very annoying thing to do.",
                    "label": 0
                },
                {
                    "sent": "You're going to basically saying to these guys, I'm going to assume that you've successfully brought your little research problem to a satisfactory conclusion.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not.",
                    "label": 0
                },
                {
                    "sent": "There's not view at all.",
                    "label": 0
                },
                {
                    "sent": "I want to have.",
                    "label": 0
                },
                {
                    "sent": "This is a very important problem, but I think we we sort of have a good idea about how we can deal with it, whereas we don't really have a very good idea about how to deal with these computational complexity.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "Angest is a diagram really that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So Gold showed that you could learn pretty much anything using positive data and membership queries.",
                    "label": 1
                },
                {
                    "sent": "If you didn't care about efficiency.",
                    "label": 0
                },
                {
                    "sent": "Then Horning in 969 shady you can learn probabilistic context free grammars if you don't care about efficiency.",
                    "label": 0
                },
                {
                    "sent": "Donna Anglin did some work and Nick Cater and pull that down.",
                    "label": 0
                },
                {
                    "sent": "You have done some work.",
                    "label": 0
                },
                {
                    "sent": "So what I'm really saying is I think we know in a way how to go down.",
                    "label": 0
                },
                {
                    "sent": "We don't really know about how to go across So what we want to start off really is solve the next most obvious problem is not to go straight to the bottom right hand corner but just to go to the top right angles.",
                    "label": 0
                },
                {
                    "sent": "Look at efficient algorithms using positive data.",
                    "label": 0
                },
                {
                    "sent": "Membership queries.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in regular inference, this is pretty much what panned out.",
                    "label": 0
                },
                {
                    "sent": "The 1st result in regular inference was done.",
                    "label": 1
                },
                {
                    "sent": "Anglins work in 1982, where you just use positive data and you get a very small class called the reversible languages that you can learn.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be.",
                    "label": 0
                },
                {
                    "sent": "She extended that using a query model, so using positive data, but where you are allowed to ask queries as a result by Jose Ansina.",
                    "label": 1
                },
                {
                    "sent": "They use positive and negative data and when you stochastic data there is some results by diner on cross concina and a paper I wrote and what these results re suggest is that I think very much confirming.",
                    "label": 0
                },
                {
                    "sent": "Next point is that the presence of probabilistic data largely compensates for the absence of negative data and so these are all efficient algorithms for learning these representations.",
                    "label": 1
                },
                {
                    "sent": "So we're going to do.",
                    "label": 1
                },
                {
                    "sent": "Here is we're going to assume positive data membership queries.",
                    "label": 0
                },
                {
                    "sent": "And it's just basically a placeholder for a more realistic probabilistic learning model.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So these algorithms for learning regular languages don't use regular grammars as a representation.",
                    "label": 0
                },
                {
                    "sent": "They use deterministic finite automatic.",
                    "label": 0
                },
                {
                    "sent": "And it's worth pausing from him and saying, why are these DFA is learnable?",
                    "label": 0
                },
                {
                    "sent": "Because this really is the up until sort of five or six years ago, this was really the only positive really, really interesting positive result in grammatical inference, and the reason is this that if you have a language L, you can define the residual languages which are the.",
                    "label": 0
                },
                {
                    "sent": "For any string you you can send the set of strings that will make a grammatical, the sort of set of suffixes that line the language.",
                    "label": 0
                },
                {
                    "sent": "And by the Myhill nerode theorem.",
                    "label": 0
                },
                {
                    "sent": "There are a finite number of these if and if only if the language is regular.",
                    "label": 0
                },
                {
                    "sent": "Now when you learn the minimal DFA for each state, you can define the language generated from that state.",
                    "label": 1
                },
                {
                    "sent": "So here we have a state of Q and we're saying the language generate from the state is the set of strings such that when you start from that state Q, you end up in accepting an accepting state.",
                    "label": 1
                },
                {
                    "sent": "Now the minimal DFA has a property, there is a bijection.",
                    "label": 0
                },
                {
                    "sent": "OK, between these two things.",
                    "label": 0
                },
                {
                    "sent": "There's an exact correspondence between the residual languages OK, which is defined purely in terms of the language.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look at the definition residual language, there's no reference to the state or anything like that is just the language itself and the.",
                    "label": 0
                },
                {
                    "sent": "And the states here correspond precisely to those.",
                    "label": 0
                },
                {
                    "sent": "OK, so it is in a sort of special sense when you think of being objective.",
                    "label": 0
                },
                {
                    "sent": "OK, the states correspond directly to some well defined sets of strings in the language.",
                    "label": 0
                },
                {
                    "sent": "OK. And is that which allows DFA's to be learnable?",
                    "label": 0
                },
                {
                    "sent": "So the natural question.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you know, how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we extend this to context free inference?",
                    "label": 0
                },
                {
                    "sent": "So the slogan here is I guess sort of in Paris is model empiricist is a slightly annoying term.",
                    "label": 0
                },
                {
                    "sent": "What I mean is you know empiricism is a philosophical doctrine.",
                    "label": 0
                },
                {
                    "sent": "Is that the knowledge is based on experience so linguistic in person with the idea that our linguistic knowledge is based on linguistic experience.",
                    "label": 0
                },
                {
                    "sent": "So these models really had this impressive property that the.",
                    "label": 0
                },
                {
                    "sent": "Structured representation should be based on the structure of the language, not something you arbitrarily imposed from outside.",
                    "label": 1
                },
                {
                    "sent": "And this is, I think, a very proud.",
                    "label": 0
                },
                {
                    "sent": "I hope this seems like a very reasonable natural assumption.",
                    "label": 0
                },
                {
                    "sent": "We're just saying the structure or representation should be based on the structure of the data, but it's worth realizing this completely rules out the standard Chomsky representations.",
                    "label": 0
                },
                {
                    "sent": "They do not have this property, so the research program.",
                    "label": 1
                },
                {
                    "sent": "Then as you identify some structure in language.",
                    "label": 0
                },
                {
                    "sent": "You construct representation which is based on that structure.",
                    "label": 1
                },
                {
                    "sent": "An richer structure give you more powerful representations.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So normal direction.",
                    "label": 0
                },
                {
                    "sent": "So when you did your when you learned about context free grammars, you define a grammar.",
                    "label": 1
                },
                {
                    "sent": "You define a derivation relationship and then you show how this grammar here can define a context free language.",
                    "label": 1
                },
                {
                    "sent": "Here we're putting we're putting learnability first, so we're going to go back.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's right, we're going to go.",
                    "label": 0
                },
                {
                    "sent": "We won't say whenever function from the language.",
                    "label": 1
                },
                {
                    "sent": "The representation we're going to start from a set of strings, and they're going to map that onto some representational primitive of the formalism.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "So are there any questions on this part so far?",
                    "label": 0
                },
                {
                    "sent": "Probably a good moment to take sort of any general questions about linguistics before we get into more technical stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll start talking now about.",
                    "label": 0
                },
                {
                    "sent": "Does.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Traditional learning.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be building up to is local distribution lattice grammars which are.",
                    "label": 0
                },
                {
                    "sent": "A richly structured, context sensitive representation that seemed to have a very good match.",
                    "label": 1
                },
                {
                    "sent": "The class of natural languages, and we have efficient correct algorithms for these, which are based on distribution learning.",
                    "label": 1
                },
                {
                    "sent": "We have a theoretical foundation in the theorization lapses that I'm not going to talk very much about, and we also have the fact these form results were going to be using this symbolic learning paradigm where we assume that we have a source of positive data and we can ask queries.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I think there's a natural skepticism I get from people is when I say I talk about child.",
                    "label": 0
                },
                {
                    "sent": "I talk about language acquisition.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "People always assume that I spend all of my time in a lab.",
                    "label": 0
                },
                {
                    "sent": "Clipping electrodes to babies, heads and things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, and they were shocked when I say, well, actually I just sit, you know, sit at home in my arm chair proving theorems.",
                    "label": 0
                },
                {
                    "sent": "I think The thing is really we can use data to distinguish between competing theories, but remember, there simply aren't any competing theories.",
                    "label": 1
                },
                {
                    "sent": "There are no theories that satisfy the most sort of minimal requirements.",
                    "label": 0
                },
                {
                    "sent": "There are, so two rough candidates.",
                    "label": 0
                },
                {
                    "sent": "One is, I guess, construction grammar to Mike Tomasello Annadell Goldbergs, but it's.",
                    "label": 0
                },
                {
                    "sent": "It's not really a theory of acquisition, they have a.",
                    "label": 0
                },
                {
                    "sent": "They simply don't talk about acquisition in a formal way, and the principles and parameters model, which is the other competing model, doesn't really have a solution, so.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to be doing here is proving things and giving some examples, and I think I want to contrast the exams I'm giving with how other people maybe use examples exams.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be very very simple.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I have proof that these algorithms work and those proofs that I'm not going to present them should convince you what I'm going to give you.",
                    "label": 0
                },
                {
                    "sent": "These little examples is just to illustrate to illuminate they're not meant to convince you so.",
                    "label": 0
                },
                {
                    "sent": "One of the problems I have is this.",
                    "label": 0
                },
                {
                    "sent": "This work is is, I think, a lot of the research I'm doing should really been done in the 1960s and wasn't so this whole field have a slight 1960s filter.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be using very small examples.",
                    "label": 0
                },
                {
                    "sent": "This works on very large examples, but I'm going to be giving you just the simplest ones.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic assumptions here is that we have some finite set of symbols.",
                    "label": 0
                },
                {
                    "sent": "OK, so Sigma is for example the set of words in English say the a cat dog is Sigma star.",
                    "label": 1
                },
                {
                    "sent": "Here is a set of all finite strings.",
                    "label": 1
                },
                {
                    "sent": "An L is just a subset of grammatical sentences, so it's those strings which are grammatically well formed.",
                    "label": 0
                },
                {
                    "sent": "The cat is dead, I run away, etc.",
                    "label": 0
                },
                {
                    "sent": "And the remaining strings are the ungrammatical ones, which are.",
                    "label": 0
                },
                {
                    "sent": "Some small, numerous things, like the other cat pattern helicopters, so random strings that have no no meaning and this is very crude.",
                    "label": 0
                },
                {
                    "sent": "It's much better to have some sort of distribution over signal over signal starts a more natural way, but as I said, I'm working with a non probabilistic learning paradigm.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus every simple sort of formal example here is that we have a.",
                    "label": 0
                },
                {
                    "sent": "We abstract it and just consider some very simple formal language like Sigma equals where we have a alphabet.",
                    "label": 0
                },
                {
                    "sent": "Just have two symbols A&B.",
                    "label": 0
                },
                {
                    "sent": "We have some language which consists of any number of days followed by the same number of bees, which is a classic example of a non regular context free language.",
                    "label": 0
                },
                {
                    "sent": "And Sigma star minus L is just every other string.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to be using predominantly these sorts of very simple examples as a proxy for the more complicated ones.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the learning problem is basically that were given some information about L about the language, the set of strings and we want to construct a representation G such that G defines the language L. So typically have a sequence of examples W1W2 we assume that all of those examples are in the language, and at every example of language will eventually occur.",
                    "label": 1
                },
                {
                    "sent": "This is a very weak constraint, and so we're going to compensate by allowing the learner to query whether a W as in L and we wanted to converge to right answer.",
                    "label": 0
                },
                {
                    "sent": "So this is a very weak source of information.",
                    "label": 0
                },
                {
                    "sent": "We don't have any context.",
                    "label": 0
                },
                {
                    "sent": "We can't.",
                    "label": 0
                },
                {
                    "sent": "We don't have any interaction, we just passively receive these examples.",
                    "label": 0
                },
                {
                    "sent": "But we compensate for this by allowing ourselves to to query.",
                    "label": 0
                },
                {
                    "sent": "Now what I think is extraordinary about the field, right?",
                    "label": 0
                },
                {
                    "sent": "Is that this is a fairly straightforward problem and there are almost no algorithms for doing this.",
                    "label": 0
                },
                {
                    "sent": "So which I find which I I will come back to, why this is?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "While this is interesting, so distributional learning, I think if people think as far as cognitive science thinks about language acquisition, the default assumptions, broadly speaking, that you store some set of examples and then you generalize in somewhere an it's very natural algorithm idea.",
                    "label": 0
                },
                {
                    "sent": "Particularly, look at child directed speech is that you have the child is confronted with the settings like look at the doggie look at the car, look at the biscuit, look at the blue car.",
                    "label": 1
                },
                {
                    "sent": "The doggie is over there.",
                    "label": 1
                },
                {
                    "sent": "The biscuit is over there, and seeing this data.",
                    "label": 0
                },
                {
                    "sent": "You naturally think well, be obvious there's something like.",
                    "label": 0
                },
                {
                    "sent": "You know doggy car biscuit and blue car or similar.",
                    "label": 0
                },
                {
                    "sent": "We can substitute them one for the other, and that's a very natural algorithmic idea.",
                    "label": 0
                },
                {
                    "sent": "It's been around for a long time, and the question really is what class of languages can we learn using this approach?",
                    "label": 0
                },
                {
                    "sent": "OK. We obviously can't learn every possible language, but there must be some way of characterizing sorts of languages that we can learn using this approach, and I guess the subsidiary question is how does that class of languages correspond to the class of natural languages?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are some problems with this naive approach, which Chomsky was one of the first to point out.",
                    "label": 0
                },
                {
                    "sent": "Johnson is very much reacting to distribution learning.",
                    "label": 0
                },
                {
                    "sent": "Zelek Harris was Chomsky supervisor.",
                    "label": 0
                },
                {
                    "sent": "One of the examples of this is John is easy to please and John is eager to please.",
                    "label": 1
                },
                {
                    "sent": "OK, so these two sentences superficially very similar.",
                    "label": 0
                },
                {
                    "sent": "But if you think about from him, you realize there structurally very different in that in the first one John is easy to please.",
                    "label": 0
                },
                {
                    "sent": "Jaune is the person that is being pleased.",
                    "label": 0
                },
                {
                    "sent": "OK, in some sense is the object of the verb please.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the second sentence John is eager to please.",
                    "label": 0
                },
                {
                    "sent": "Jaune is in fact the one that wants to isn't going to be doing the prison, so he is the subject of the thing so.",
                    "label": 0
                },
                {
                    "sent": "This very simple, so we have these two sentences that appear superficially to be very, very similar and yet have very different interpretations.",
                    "label": 0
                },
                {
                    "sent": "So how can a simple distribution a learner figure out these different interpretations?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can see that there different by considering a sentence like this, they are ready to eat again.",
                    "label": 1
                },
                {
                    "sent": "This illustrates a problem that simple structuralist models had.",
                    "label": 0
                },
                {
                    "sent": "But there's no real way to account for the ambiguity of this sentence.",
                    "label": 0
                },
                {
                    "sent": "So they are ready to eat can mean two things that could mean you know the hot dogs are ready to eat, or it could be that the students are ready to eat.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are two different interpretations here.",
                    "label": 0
                },
                {
                    "sent": "I thought about putting up some non PC joke about you know cannibalism, but I'll, I'll leave that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you also have displaced constituents, so you know this is the book that John said that Mary had.",
                    "label": 1
                },
                {
                    "sent": "You know what verb can go there?",
                    "label": 0
                },
                {
                    "sent": "You know it's gotta be something like red.",
                    "label": 0
                },
                {
                    "sent": "OK, something that can take the book as an object.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here we have a case where there is a dependency between book there and it's an unbounded dependency because you can have as many John said that Mary had told Jaune or whatever you connect.",
                    "label": 0
                },
                {
                    "sent": "You can extend these as long as you like, so these are sorts of problems that.",
                    "label": 0
                },
                {
                    "sent": "Many people found convincing as attacks on on these sort of naive distribution.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So let me just define a bit more formally here.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a simple sentence that man over there is bothering me.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can split it into two parts we can consider take any substring, say man over Anna context that blank there is bothering me.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to find a context really just being a pair of strings.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "A pair L which occurs the left an abit are which occurs to the right and we can combine a context LR with a symbol year with a substring you to get LUR and clearly got a special context here which is Lambda Lambda Lambda.",
                    "label": 0
                },
                {
                    "sent": "Here is the empty string.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm using languages refer to empty string, so the special context Lambda Lambda is sometimes like the identity context.",
                    "label": 0
                },
                {
                    "sent": "Given a language L, we can define the distribution of a string here just to be this.",
                    "label": 1
                },
                {
                    "sent": "Here is the set of all contexts.",
                    "label": 0
                },
                {
                    "sent": "That a particular string you will have OK in the language so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make that explicit, let's say we have.",
                    "label": 0
                },
                {
                    "sent": "Simple form example here, GmbH then.",
                    "label": 0
                },
                {
                    "sent": "Then we have that the context, the distribution of a consists of the strings that the context, Lambda, BABBAABB and so on.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the distribution of AAB we see it is almost the same, but not exactly the same.",
                    "label": 1
                },
                {
                    "sent": "OK Becausw, you may notice that a can occur in the last one of those contexts, a AVB but a B.",
                    "label": 0
                },
                {
                    "sent": "Cannot we stick AB into the last context of the distribution of a?",
                    "label": 0
                },
                {
                    "sent": "We get a string that is not in the grammar.",
                    "label": 0
                },
                {
                    "sent": "But we can see that a ABB is does have exactly the same distribution as a B. OK answer me in English, if we look at the word cat, we might find a lot of sentence.",
                    "label": 0
                },
                {
                    "sent": "Look at the blank.",
                    "label": 0
                },
                {
                    "sent": "The blank is on the mat and seeing with dog however, justice in this game result.",
                    "label": 1
                },
                {
                    "sent": "It's quite easy to come up with contexts where cat can occur, but dog can't.",
                    "label": 0
                },
                {
                    "sent": "So what's what's an example of a?",
                    "label": 0
                },
                {
                    "sent": "Contacts that context would differentiate Cat and dog.",
                    "label": 0
                },
                {
                    "sent": "So I mean, you know I have a cat flap, but in my door but you don't have a dog flat.",
                    "label": 0
                },
                {
                    "sent": "I'm going to dog his footsteps, but you didn't catch his footsteps.",
                    "label": 0
                },
                {
                    "sent": "You know there are a bunch of ways in which cat subtly differs from dog, so when you're dealing with when you're doing this very, very simple artificial example like this, this sort of congruence.",
                    "label": 0
                },
                {
                    "sent": "Exact identity of distribution seems like a reasonable thing to do when you look at sort of real language.",
                    "label": 0
                },
                {
                    "sent": "In fact, exact congruences is quite rare, but nonetheless there's a great deal of similar.",
                    "label": 0
                },
                {
                    "sent": "I mean, the fact that nobody instant laptop with dozens of examples is good, because in fact you know the distribution.",
                    "label": 0
                },
                {
                    "sent": "Can dog are very, very similar in many respects, and we can exploit that similarity when we write.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now there are.",
                    "label": 0
                },
                {
                    "sent": "There are several reasons to take distribution learning seriously, so one is it is cognitively plausible.",
                    "label": 1
                },
                {
                    "sent": "So there's work by Saffron aslund, Newport that show that newborn children are sensitive to properties of the distribution.",
                    "label": 0
                },
                {
                    "sent": "At least an artificial grammar learning experiments.",
                    "label": 1
                },
                {
                    "sent": "It works in practice, so large scale lexical induction, like James Curran Works, is a staple of statistical natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Linguists used as constituent structure tests.",
                    "label": 1
                },
                {
                    "sent": "Recent book by Andrew Connie called constituent structure.",
                    "label": 0
                },
                {
                    "sent": "You know, argues for for distribution, learning an historically phrase structure.",
                    "label": 0
                },
                {
                    "sent": "Grammars were meant to be learnable.",
                    "label": 0
                },
                {
                    "sent": "Using these approaches, I mean this is quite striking.",
                    "label": 0
                },
                {
                    "sent": "Chomsky designed the context free grammar OK college program is not some.",
                    "label": 0
                },
                {
                    "sent": "It's an invention of man.",
                    "label": 0
                },
                {
                    "sent": "He created it with the intention that it would be learnable alright, so.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to do here is is fixed some technical details and context free grammars to make them making reliable.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the distribution learning here what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "The foundational idea here is that what we're going to do is try to predict the distribution of strengths.",
                    "label": 0
                },
                {
                    "sent": "OK, so CL here is just the distribution of string.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn some finite representation G. This is going to define a function from.",
                    "label": 1
                },
                {
                    "sent": "We're going to try to model A function from you to the distribution of you.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if we can do that.",
                    "label": 0
                },
                {
                    "sent": "Then we've learned a language becausw.",
                    "label": 0
                },
                {
                    "sent": "If Lambda Landers in the distribution of you, then you, as in L. So if we predict the distribution, then we will implicitly have defined language OK.",
                    "label": 0
                },
                {
                    "sent": "So there are two problems with doing this.",
                    "label": 0
                },
                {
                    "sent": "First is that the distribution is normally infinite, so I'm going to need some representation, and the 2nd is there going to be Norman infinite number of strings.",
                    "label": 1
                },
                {
                    "sent": "You in Sigma star, so we're going to do is do a standard sort of recursive decomposition, and we're going to try to predict the distribution of UV from our estimates of the distribution of you and the distribution of E. So it's again a sort of fairly classic cognitive idea.",
                    "label": 0
                },
                {
                    "sent": "We have a small number of primitive elements.",
                    "label": 0
                },
                {
                    "sent": "We have some rule for combining this primitive elements and we can use that to recursively define something over an infinite set.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic idea is we are going to take a finite set of Contacts F which are going to include Lambda Lambda.",
                    "label": 1
                },
                {
                    "sent": "We're going to take a finite set of substrings which are going to include the primitive elements which have got to be at a minimum, the words the elements of Sigma, and we're going to take any language L OK, and we're just going to be considering a small part of this data, so the particular, but we're going to be talking looking at is this F rap KK.",
                    "label": 1
                },
                {
                    "sent": "So this just means we take.",
                    "label": 0
                },
                {
                    "sent": "Any element from K any two item case sticking together and wrap them in any element from F. OK, so this here is polynomially bounded set OK. And what the data actually look at is just the grammatical elements of this set.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have three elements.",
                    "label": 0
                },
                {
                    "sent": "We have all sort of examples K. We have our set of context.",
                    "label": 1
                },
                {
                    "sent": "We're going to use to model the distribution, and then we have our data.",
                    "label": 0
                },
                {
                    "sent": "If you like D, which is represents a finite set of of the language.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st way you could do this OK?",
                    "label": 0
                },
                {
                    "sent": "And this is the most natural way to do it is to partition all of our strings into congruence classes.",
                    "label": 1
                },
                {
                    "sent": "So we're going to have exact identity of distribution.",
                    "label": 0
                },
                {
                    "sent": "So here this congruent sign there we're going to have consider each string you are going to consider the set of all strings that are exactly distribution identical.",
                    "label": 0
                },
                {
                    "sent": "So that means we can swap completely freely any you for a V without changing dramatic County.",
                    "label": 1
                },
                {
                    "sent": "So using our simple example here, we have.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 1
                },
                {
                    "sent": "There's only one string with that precise distribution.",
                    "label": 0
                },
                {
                    "sent": "We have a B which is infinite AAB, which is infinite and then we have a few slightly weird ones.",
                    "label": 0
                },
                {
                    "sent": "The empty string is is unique, and then we have another class of all things that have an empty distribution, so these are substrings that don't occur anywhere in our language.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a partition of all the strings of Sigma star into these congruence classes.",
                    "label": 1
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, let's say we wanted to build a context free grammar.",
                    "label": 1
                },
                {
                    "sent": "OK, now I'm.",
                    "label": 0
                },
                {
                    "sent": "This is meant to just illustrate the 'cause.",
                    "label": 0
                },
                {
                    "sent": "I assume people if people are familiar with context free grammars, then this will be, I think, a good way of understand what's going on here so we can consider for a non tunnel and the set of all strings are generated from that non terminal right?",
                    "label": 0
                },
                {
                    "sent": "And suppose you have a rule an goes to PQ, then clearly the set of strings generated from N is going to include the concatenation of the set cell of PLQ, right?",
                    "label": 1
                },
                {
                    "sent": "This is almost the defining property of water context free grammar is.",
                    "label": 0
                },
                {
                    "sent": "So if we go.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Backwards, let's say we define a set of strings XY, zed, and suppose it happens to be that X contains Y. Zed, then maybe we can add a rule X goes to.",
                    "label": 1
                },
                {
                    "sent": "Why is that OK?",
                    "label": 0
                },
                {
                    "sent": "So we're just taking we just in line with this basic idea of running things backwards.",
                    "label": 0
                },
                {
                    "sent": "Here we start off by defining what are nonterminals going to kind of refer to, what they're going to generate, and then from that we can then infer the rules so.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now Congress cause haven't had one very nice property, right?",
                    "label": 0
                },
                {
                    "sent": "Which is that if we take the Congress cause of you and the current cost of V and we concatenate them, then it is always a subset of the Congress class of UV.",
                    "label": 0
                },
                {
                    "sent": "So that means we can always add a rule.",
                    "label": 0
                },
                {
                    "sent": "From the comments, cost of UV to the common cause of you and the comments of E. OK, so this is very, very fundamental.",
                    "label": 0
                },
                {
                    "sent": "What this is saying is that we can essentially read off the structure of our representation from the structure of these congruence classes.",
                    "label": 0
                },
                {
                    "sent": "There's what's called the syntactic monoid, and we can just simply define a natural representation and natural context free grammar based on.",
                    "label": 0
                },
                {
                    "sent": "On these symbols, the problem is it's kind of hard to tell whether you is congruent to V because we have to tell whether you is congruent to V. We need to test infinitely many.",
                    "label": 1
                },
                {
                    "sent": "Infinitely many context mean to check that we need to have precise equality of distribution.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just give an example of this.",
                    "label": 0
                },
                {
                    "sent": "So again, using the simple example here, we have the common cause of a, which is just the string.",
                    "label": 0
                },
                {
                    "sent": "A further cons cause of ABB.",
                    "label": 0
                },
                {
                    "sent": "If we concatenate the common cause of a with the concept of a B, then we have the input set of strings a ABB ABB which are all.",
                    "label": 1
                },
                {
                    "sent": "In the Congress costs ABB, which is equal to the current source AP, which is in the language.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means that we can simply write down a grammar.",
                    "label": 0
                },
                {
                    "sent": "We start off by having our stock symbol S an we.",
                    "label": 0
                },
                {
                    "sent": "We go to the two congruence classes that it contains.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have some other trivial rules here.",
                    "label": 0
                },
                {
                    "sent": "We can just say that the columns cost of a can be rewritten as a common customer.",
                    "label": 0
                },
                {
                    "sent": "A console should be can be rewritten as B.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have some slightly more interesting rules.",
                    "label": 0
                },
                {
                    "sent": "Here we have that baby can go to ABB.",
                    "label": 0
                },
                {
                    "sent": "Baby can also go to A&BAB can go to a ABB and so on.",
                    "label": 0
                },
                {
                    "sent": "So The thing is we don't have.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no search here.",
                    "label": 0
                },
                {
                    "sent": "There's no computation right?",
                    "label": 0
                },
                {
                    "sent": "Once we define the reference of this thing then the inference problem simply dissolves OK.",
                    "label": 0
                },
                {
                    "sent": "If something becomes trivial.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you also have a few other trivial rules which you don't need to worry about.",
                    "label": 0
                },
                {
                    "sent": "Perhaps he is example reasonably clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we actually?",
                    "label": 0
                },
                {
                    "sent": "No, what we're doing is basically the strategy here is defined some chunks of strings distributionally, so the this way we define the strings to be congruence classes, and this is in some sense as small as possible's classes.",
                    "label": 0
                },
                {
                    "sent": "We can consider OK if two strings are absolutely identical, then there's no way you can distinguish but absolutely identical distribution.",
                    "label": 0
                },
                {
                    "sent": "There's no way you can distinguish them, so this is the most fine grain possible representation we could have.",
                    "label": 0
                },
                {
                    "sent": "An alternative way of sort of jewel way would be to take a particular context and consider all strings that occur in that context.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first one comments causes a set of strings that share every single.",
                    "label": 0
                },
                {
                    "sent": "Context these here are just simply sets of strings that have one context in common OK, and if there's a certain particular case which is when these two coincide, which is when they're called substitutable.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is no concept.",
                    "label": 0
                },
                {
                    "sent": "I recently found a paper by John Myhill who's the guy who counted Myhill Nerode theorem in 1950.",
                    "label": 0
                },
                {
                    "sent": "He wrote a letter to the journalist symbolic Logic, so I don't feel bad about not having found it until recently, but he defines a particular concept which calls regular.",
                    "label": 0
                },
                {
                    "sent": "Obviously, now we use regular to refer to something different, but it's very natural concept now in 2005.",
                    "label": 0
                },
                {
                    "sent": "We came up with a polynomial learning which is kind of the first, the most primitive elementary result.",
                    "label": 0
                },
                {
                    "sent": "The context for inference, which is based on precisely that property.",
                    "label": 0
                },
                {
                    "sent": "OK, mathematically, identical definition.",
                    "label": 0
                },
                {
                    "sent": "So natural question is, you know, while the delay OK, Why was there basically a 50 year gap between this?",
                    "label": 0
                },
                {
                    "sent": "This property being hypothesized as being something learnable and this is in the context of, you know, a discussion about distributional learning.",
                    "label": 0
                },
                {
                    "sent": "Why's there 50 year gap between that and the first formal result in the field?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll answer that later on, but that is I think that's if I'm saying, you know, I'm here saying, oh, there's this wonderful new idea called distribution.",
                    "label": 0
                },
                {
                    "sent": "Learning is not enough.",
                    "label": 0
                },
                {
                    "sent": "I could explain why is such an obvious idea being neglected for so long.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this paper, you will see that it is really quite trivial and the Journal version is a bit long because the review is basically didn't believe the result they wanted.",
                    "label": 0
                },
                {
                    "sent": "You know I kept on having to expand every every lemma right, but the basic result is very trivial.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic results here that from if we just have positive data then we get this substitutable result.",
                    "label": 0
                },
                {
                    "sent": "We polynomial result from positive data alone.",
                    "label": 1
                },
                {
                    "sent": "For a small class are substitutable languages if the data is generated from a probabilistic context free grammar, then we have a a result on PAC Learning a particular class of languages.",
                    "label": 0
                },
                {
                    "sent": "This is slightly incomplete.",
                    "label": 1
                },
                {
                    "sent": "And then this year I have a paper on the submission which is showing an efficient membership query algorithm where you pick a finite set of contexts and you test for congruence using whether you.",
                    "label": 0
                },
                {
                    "sent": "Whether the basically rather testing an infinite set of of context you test a finite set OK, this gives you a polynomial query result for a very large class of context free grammars.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is progress, but.",
                    "label": 0
                },
                {
                    "sent": "One symbol per congruence class is is very natural, but it's just not going to work for natural language is OK 'cause you have very, very many congruence classes and they are very close together.",
                    "label": 1
                },
                {
                    "sent": "Exact substitutability is very, very rare, as the cat dog example.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think Tuesday is probably substitutable for Wednesday, but beyond that is very very difficult to find, you know.",
                    "label": 0
                },
                {
                    "sent": "Beyond this very efficient, it's very difficult to find words that exactly.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Substitutable.",
                    "label": 0
                },
                {
                    "sent": "And the land, were you using really assumes that either there, either 2.",
                    "label": 1
                },
                {
                    "sent": "Either 2 strings are completely identical, or they're completely unrelated, and that's too crude.",
                    "label": 1
                },
                {
                    "sent": "We need to have a representation that can represent the structure of the congruence classes, and there's another problem, which is the languages aren't context free.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you're using a context free representation, we know right away that we're we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not looking the right place.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we gonna do is I'll just describe this using.",
                    "label": 0
                },
                {
                    "sent": "What kind of simple observation table?",
                    "label": 1
                },
                {
                    "sent": "So here we have a table and down the left.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to have a sequence of strings from K K1 up to Kate and along the top we're going to label these with Contacts F1F up to F-10.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we can do is you can just fill this in with membership queries, right?",
                    "label": 1
                },
                {
                    "sent": "So what we're going to do is we want to say this is going to be filled in.",
                    "label": 0
                },
                {
                    "sent": "If this string, combined with that context is grammatical alright.",
                    "label": 0
                },
                {
                    "sent": "So the question really is, given this sort of information about a language, can we construct representation?",
                    "label": 0
                },
                {
                    "sent": "Can we learn grammar for this?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, substitutable grammars are precisely the ones where these form into blocks like this.",
                    "label": 0
                },
                {
                    "sent": "Again, in this case, it's pretty easy to learn these things.",
                    "label": 0
                },
                {
                    "sent": "We just take one non tonal feature these blocks and read off the grammar and if we have enough data, we're going to, we're going to get it right.",
                    "label": 0
                },
                {
                    "sent": "But even in very.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very simple examples.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get this, so we're going to run looking at blocks.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at kind of maximal rectangles here, so maximal rectangle here, which were coloring in red is a rectangle, so it's an area which is completely filled in, and its maximal in the sense you can't increase either the set of strings or the set of contexts without violating the fact that it's all filled in.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one of those things where it's very easy to just show you.",
                    "label": 0
                },
                {
                    "sent": "Which set of maximal rectangles are?",
                    "label": 1
                },
                {
                    "sent": "OK, so these are the maximum rectangles.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are these all the maximum rectangles.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what's the relation to context free grammars?",
                    "label": 1
                },
                {
                    "sent": "Again, I'm not really interesting context free grammars, 'cause I think they're wrong, but it's a good way of explaining it.",
                    "label": 1
                },
                {
                    "sent": "Let's say we have a context free grammar for each non time.",
                    "label": 0
                },
                {
                    "sent": "We can consider the yield right?",
                    "label": 0
                },
                {
                    "sent": "The set of strings that you can derive.",
                    "label": 0
                },
                {
                    "sent": "From that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to the context of that non terminal which is a set of context you can derive from that.",
                    "label": 0
                }
            ]
        }
    }
}