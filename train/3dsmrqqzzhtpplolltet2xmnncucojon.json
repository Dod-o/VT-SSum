{
    "id": "3dsmrqqzzhtpplolltet2xmnncucojon",
    "title": "Machine Learning for the Web: A Unified View",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Web Mining",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bsciw08_domingos_mlwuv/",
    "segmentation": [
        [
            "Machine learning for the web, which is of course a very large and ever growing field.",
            "So much of this is based on work that I've done at UW with these people.",
            "So here's an overview of the talk.",
            "I will begin with some motivation and then some."
        ],
        [
            "Background I will.",
            "I will introduce this language called Markov logic, which is a language is going to allow us to do such a unified view of the various learning algorithms for the web.",
            "Talk about how to do inference and learning, mention some of the open source software that's available to do this and then look at some of the applications that we can do, and in particular I will go into one application in more detail to basically give people a more concrete sense of what can be done and conclude with some discussion."
        ],
        [
            "So motivation.",
            "The web is an amazing machine for generating machine learning problems.",
            "It's really, you know.",
            "This is a partial list, right?",
            "And you know, many of these topics that actually quite recent.",
            "All of these are important things that you want to do on the web.",
            "All of them call for machine learning, hypertext classification, search ranking, personalization, recommender systems, Rep reduction, information extraction, information integration, extracting information from the deep web, making the semantic web actually work, ad placement, content selection.",
            "Yeah.",
            "So the deep web for those of you not familiar with it, is this idea that most of the information on the web is in the form of databases with the web front end in front of them.",
            "They are not web pages, so if you just do ordinary crawling and indexing you actually won't capture the deep web.",
            "But that's actually a lot of the information is, so some of the machine learning problems on the deep web are can I learn what information it is that a deep web source provides?",
            "And can I figure out how do we extract that information and then combine it with others?",
            "Can I learn how to make plans for how to get the answers that I want?",
            "Right, it's sort of like a very interesting learning problem, 'cause a source might be higher quality, but be less available.",
            "For example, right?",
            "So this is the problem with the deep web.",
            "Anne.",
            "Figuring out you know how much to be there in auction.",
            "Social networks are very popular topic these days that I'll actually use as a running example in some of the explanation.",
            "Mass collaboration, spam filtering, where the kinds of spam that you want to filter our ever growing right.",
            "There's there's of course email spam, but then there's like spam.",
            "There's you know, click spam, there's you know.",
            "Ever growing new kinds of spam that you want to fight with machine learning, reputation systems.",
            "Performance optimization is actually maybe one of the less visible applications.",
            "Machine learning on the web.",
            "It's very important right on the web.",
            "You want other things on the very large scale, you can use machine learning to optimize the performance of your disk accesses your crawlers, your networking, etc etc.",
            "So an ever growing set of of learning problems that the web."
        ],
        [
            "Provides.",
            "On the other side, there's also an ever growing set of machine learning solutions.",
            "And again, here's a very partial list of them.",
            "Right?",
            "Pick your own favorite machine learning technique, naive Bayes, logistic regression, you know vision networks you know, log in your models, Hmm's, conditional, random fields, SVM.",
            "You know there's hundreds of them and you know.",
            "Wait another.",
            "Well, there's probably thousands of them, and every year you'll see you know 100 more.",
            "You know a few 100 more appear, right?",
            "This is very nice, but it creates a very big problem, which is, if you're somebody who works on web problems."
        ],
        [
            "And you know actually needs to solve them.",
            "What do you do?",
            "Do you have to learn all the algorithms that I just listed?",
            "God forbid, right?",
            "I mean even we machine learning researchers can't keep up with all of it and then do they figure out you know which one to use each time, right?",
            "How do you do that right?",
            "You know 100 machine learning algorithms and which one to use for each problem and which variations of that will be try.",
            "And how do you frame the problem that you have?",
            "Is machine learning right?",
            "This is often one of the hardest parts.",
            "And how do we incorporate the knowledge that you have about the problem into your machine learning algorithm is actually often the most important thing that someone in a particular application does incorporate their knowledge, but this is actually not very easy, right?",
            "How to incorporate your knowledge into an SVM or into a neural network, right?",
            "Not very clear.",
            "And how do you then glue all the pieces that you've built?",
            "How do you build them altogether?",
            "And you have to start from scratch each time, right this?",
            "Unfortunately, the sad state of machine learning on the web today is that pretty much every time you have to start from scratch, you build a CRF to extract one kind of information.",
            "Or if you know information from one source, tomorrow you build another CRF to extract information from another source and you start from scratch and at the end of the day you have 500 CRF's, none of which share any code, and none of which you know actually combine.",
            "So there must be a better way.",
            "There better be a better way, or, you know, we're going to be limited in what we can do with machine learning on the web."
        ],
        [
            "So what what, what, what would such a better way be?",
            "Well, let's look at what some of the interesting characteristics of web problems are, right that actually set them apart from many other problems that people have traditionally dealt with in machine learning.",
            "Well, here's a very big one on the web.",
            "Your samples are not IID.",
            "Right ID means independent and identically distributed.",
            "This is almost always the assumption that's made in statistical learning.",
            "This means that one object doesn't tell me anything about another object, right?",
            "This would be like, for example, in the old days of text classification, I predict the topic of a web page from the words on that page, and you know that's not a bad thing to do.",
            "But the interesting thing is that if I use the links.",
            "I get much better results because if this page is on topic A and it points to page 2, you know that page that page is probably also on topic, a right?",
            "So there's a lot of power in this and this is really what the web is all about.",
            "It's the links, right?",
            "But at this point the machine learning problem has suddenly gotten a lot harder because you don't have value data anymore.",
            "You have a big network of connected pages with their properties affecting each other.",
            "So we need methods that actually handle this.",
            "The other one is that objects on the web have a lot of structure.",
            "Most machine learning algorithms are designed to deal with vectors, attributes, vectors, feature vectors, right?",
            "But objects on the web are things like webpages, webpages, structure.",
            "They have links.",
            "They have anchor text, they have.",
            "You know the header.",
            "And then there's things like XML that have even more structure, right?",
            "You know, think of like you know product pages.",
            "For example, they have a lot of structure.",
            "They have visual structure.",
            "They have two D structure or they have no structure at all.",
            "Like a lot of the information on the web is text, right?",
            "And you know, we can treat text as a bag of words, but the truth is, that's a very impoverished representation, right?",
            "There's a lot more going on in text, so we need to handle that case as well.",
            "And here's another one that makes life complicated, which is when you look at some of these problems, right?",
            "Often."
        ],
        [
            "What you have in a particular application situation is that multiple of these problems are tight together.",
            "You wanna you know you have a recommender system you need to manage reputations.",
            "You need to deal with spam.",
            "It has social network and mass collaboration aspects.",
            "Maybe we're going to extract some information to actually populate some of this for people, right?",
            "These problems are often, you know, there's more than one problem type together.",
            "So what we'd like to be able to do is compose the solutions.",
            "Another, of course salient care."
        ],
        [
            "Stick of the web is that there are massive amounts of data and this is good, right?",
            "Because it means we can let that day to do a lot of the work.",
            "For us, this is probably more true on the web than anywhere else in the notes.",
            "Of course, a general exam as an has changed its HTML somehow or you know the spammers come up with new tricks and whatnot, and the problem with this rapid changes that it doesn't cut it anymore to have to spend six months developing machine learning application because you know, by six months later the spammers will have run away with it or your website that's supposed to get there.",
            "That information is actually only gathering broken links, so we need to have more rapid development of machine learning albums and more active update of the models.",
            "And finally, you know when I talk to people in industry, you know, like Google and Yahoo and Amazon and whatnot, I'm always struck by this is that.",
            "They don't know what to do because there are so many things that you could do.",
            "There's like no end of places where you could apply machine learning.",
            "Knowing the ways in which you could apply them, and you know they only have 100 people doing machine learning, they would have 1000 if they could, but you know there aren't enough to go around, and so the problem is that we have to make the machine learning experts more productive.",
            "Right, we need to automate things as much as possible such that you know with your number of people that you have to work on machine learning problems.",
            "They can do 10 times more.",
            "They can explore 10 times more opportunities, or maybe try 100 different variations on automatically evaluate them and so forth.",
            "So we what we need is a machine learning framework that actually takes all of these things into account.",
            "Now of course, unfortunately there is no such thing available today, but I think that what we need to do is work towards that, right?",
            "In both academia and industry and what I'm going to describe, here is 1 possible approach to trying to do this.",
            "You know again, these things are all work in progress, but I think there's a number of useful ideas that we can hope for."
        ],
        [
            "Pick up here.",
            "So.",
            "I would argue that one of the first things that we need is a language.",
            "We need the language that allows us to easily define the standard types of models that we just saw, right without a lot of work without programming them from scratch in Java and C++, that provides a common framework in which to combine these algorithms do various things with them that is automatically compiled into learning and inference.",
            "Code that executes sufficiently right?",
            "I want to be able to define my machine learning solutions, compositions of models and what not very compactly.",
            "Very easy, very elegantly, but then this has to compile into code that sufficient.",
            "Otherwise, you know it's it's not really that useful at the end of the day.",
            "And also I wanted to be easy to encode the knowledge of the practitioners, the knowledge of the problem into the solution.",
            "Yeah, so would something like infer.net be relevant to what you're talking about?",
            "No, absolutely.",
            "So there's like I said, there's a number of different works in progress towards doing this, right?",
            "'cause it's not like you know, nobody else is notice these problems, right?",
            "So yeah.",
            "And also there allows models to be composed and reused, right?",
            "I mean, in software engineering and programming this is this is this is a given and unfortunately it isn't.",
            "Usually you can't compose your models and you can't reuse your model, so we need the language that actually allows us to do this.",
            "So my proposal for one."
        ],
        [
            "Such language is called Markov logic.",
            "And the basic idea, Markov logic is the following.",
            "We're going to use first order logic as the language for specifying our models, so we actually not going to invent a new language.",
            "What has actually been a surprise to us is that first order logic, which has been around for a long time, is actually such a really good match to the problem of specifying and manipulating and combining, and we're using machine learning models with one little addition is weights on the formulas.",
            "Right 'cause we're going to have, you know, parameters numeric parameters to learn those are going to come in the form of weights on the formulas and in some sense the unified view that I'm going to try to present here is how a whole bunch of these machine learning statistical learning algorithms can all be easily cast in this form.",
            "And then we can in some sense write programs that combine a logistic regression with the CRF with a mixture, model etc etc and play with that.",
            "The meaning of these formulas is that the 1st order formulas are going to act as templates for constructing Markov networks.",
            "And the Markov net was in some sense the most flexible, most powerful representation for probabilistic models that we have.",
            "So it's a natural thing to target.",
            "So this is actually what we're going to be doing is we're going to be specifying these very large models, potentially very very large, very compactly using first order logic.",
            "And then of course, you know this.",
            "This is only useful if we have efficient inference and learning algorithms.",
            "There's actually a whole bunch of algorithms that have been developed for machine for Markov logic by now.",
            "The one that I'm going to talk about here is something called lifted belief propagation, which is a more efficient form of belief, belief propagation, and various learning algorithms like voted perceptrons pseudo likelihood, inductive logic programming.",
            "Many of these algorithms, percent don't actually skill to websites, but if so, I'm going to talk about some generic software that has this algorithms and truth in advertising.",
            "You can't run it on the billing pages yet, but in some cases you can run it on things in the millions and you know the progress is being made at the rate of a couple of orders of magnitude per year.",
            "So I think we hopefully will get there.",
            "So as I mentioned that these things are available in some open source software, the package that we have is called alchemy.",
            "I'll put up the URL and talk a little bit about it a little bit later, and you know there have been a whole slew of applications of Markov logic to a web problems like information extraction, text mining, social networks, etc etc.",
            "The one example here that I will go into into into more detail is information extraction, both because it's a very important and challenging problem and one that I think illustrates well what we can do in this framework."
        ],
        [
            "OK, so before I describe Markov logic and the algorithms in the applications, let me just give a little bit of background."
        ],
        [
            "This piece of background is Markov networks.",
            "Markov networks are models of the joint distribution of a set of variables and their graphical models, so there's a note in the graph for each variable, and there's an arc and directed arc between two variables that directly depend on each other.",
            "And what the graph captures is independence.",
            "Conditional independence is envision networks.",
            "In particular, variable is conditionally independent of the rest of the network given its neighbors.",
            "So for example, smoking is conditionally dependent of cough given cancer and asthma.",
            "So this is the structure of the model.",
            "The parameters of the model come in the form of water called potential functions.",
            "There's a potential function for every maximal clique in the graph, so here there are two.",
            "There's the smoking cancer click and there's the cancer, asthma cough click and the potential function has a value.",
            "For each possible state of the click and the value is any non negative real number.",
            "So for example, here's the Smoking cancer click.",
            "Assuming there Boolean, there's four States and if you notice the intuitive meaning of the potential function values, is that higher values correspond to more preferred states.",
            "So what this little toy example here is doing is basically saying that the state smoking true and cancer false is less likely than the others, and there isn't.",
            "It's less likely that, yeah, this is that of course smoking causes cancer.",
            "And then the probability of a complete state is just the product of the values of all the potential functions normalized to sum to one.",
            "OK, so this is.",
            "This is very nice, but it has a problem in that it doesn't scale very well and you know scaling is going to be very important for us and the reason it doesn't scale very well is that if you notice if you notice the number of states of a potential function.",
            "And therefore the number of parameters that you need to deal with is exponential in the number of nodes in the click.",
            "OK, so if all you have are small clicks, then life is good.",
            "But if you have a click even of size 50 or 100, you know this is like you know, now you have two to the 100 states.",
            "So what can you do?"
        ],
        [
            "Well, what you can do is use this alternate form.",
            "Very popular with statisticians called the log linear or an exponential model.",
            "And the idea is that instead of having a product of factors, what we're going to have is an exponentiated sum of terms, right?",
            "And you can always convert from one to the other, because you know the log of a product is the sum, and the terms are products of features, which is just arbitrary Boolean functions and their weights.",
            "And I can always convert from potential form to log linear form by creating a feature for each state of each.",
            "Which potential function of each click and letting this corresponding white be the log.",
            "Of the corresponding value of the potential.",
            "Now the important thing though is that I don't need to have one feature per state of a click.",
            "If I have a click with a Brazilian states, but I know there's only 10 important features of that click, then I define those.",
            "I assign them weights and I can actually handle a very large click.",
            "You know, in a in a compact amount of space and not having to learn too many parameters.",
            "So for example.",
            "If we define this function, you know this feature, that's one.",
            "If you don't smoke or have cancer, and 0 otherwise, and give it away to 1.5, this actually realizes the same model as the is what we had on the previous slide.",
            "So the other small piece of background is on 1st order logic.",
            "So infer."
        ],
        [
            "So logically make logical statements about the world in the form of formulas.",
            "The formulas are built up out of four kinds of symbols, constants, variables, functions and predicates.",
            "Constants like Anna object.",
            "They represent objects in the domain.",
            "Variables range over those objects.",
            "Say you know X functions that take objects and return objects like mother of X and predicates like friends, XY that represent properties of objects or relations between objects and then out of these we build formulas using.",
            "The usual logical connectives like you know conjunction, disjunction, implication quantification etc.",
            "And I'm going to call a grounding of a predicate or the formula the result of replacing all the variables by constants.",
            "So for example, if one of the predicates in my domain is friends and two of my constants are N and Bob one possible grounding of friends is friends Anna Bob.",
            "When we call this the ground Atom and the ground Atom is just a Boolean variable.",
            "It's true if and Bob are friends in this case and it's false if they're not OK and I'm going to call a world and assignment of truth values to all the ground items.",
            "I is just one big Boolean vector.",
            "So what we're going to be talking about here is probability distributions over this very large Boolean vectors that represent you know everything of interest in your world.",
            "Like you know the words on the page and what pages connect to what pages and what not.",
            "OK so here."
        ],
        [
            "Here's an example that I will use as a running example from the.",
            "You know from the domain of social networks, which is of course a very popular thing these days.",
            "So what we have here is, you know the results of a study on smoking and you know people quitting smoking.",
            "So this is in 1970 and this is in 2000.",
            "And this is a social network and you know these represent how many cigarettes people smoke.",
            "And the good news is that many fewer people are smoking than before, right?",
            "And that's good because you know smoking causes cancer and you want people to stop smoking etc.",
            "But the other interesting thing is that.",
            "People don't stop smoking randomly.",
            "They stop smoking in clumps.",
            "If your friends stop smoking, that makes you more likely to stop smoking as well.",
            "And if you're trying to stop smoking and all your friends smoke, you know good luck.",
            "It's going to be very difficult.",
            "So how might we model something like this, right?",
            "We're looking at a social network that people in the social network, their properties, how they interact.",
            "So let's try to do that in logic, right?",
            "Which of course is not going to work very well, but it's going to, you know."
        ],
        [
            "At this stage, for what we're going to do next, right?",
            "Let's start by saying, you know, a couple of things in English that we know to be true.",
            "For example, smoking causes cancer and friends have similar smoking habits.",
            "Now."
        ],
        [
            "In logic, you can write this as follows.",
            "For every X smokes of X implies cancer of X and for every XY friends of XY implies smokes of X is equivalent to smokes of, why?",
            "Now this is a very nice and easy exercise, but the problem is that this isn't very useful, because if you notice, these two statements were true.",
            "And these two are false.",
            "The first because not everybody who smokes gets cancer and not all pairs of friends have similar smoking habits.",
            "So this logic by itself doesn't give us everything that we need, but you know we're going to see how to get everything that we need.",
            "So this is a good way to."
        ],
        [
            "Introduce Markov logic."
        ],
        [
            "So what's the idea?",
            "Markov logic?",
            "Well, the reason that previous example didn't work is that each formula was a hard constraint on the possible worlds.",
            "If you violate even one formula, if there's even one person in your world who's a smoker and doesn't get cancer now, the world becomes impossible, right?",
            "And so this isn't going to work right?",
            "Because you know, the world is full of noise.",
            "So how about instead of hard constraints, will let the formulas be soft constraints.",
            "Meaning that when the world violates a formula, when a smoker doesn't get cancer, the world doesn't become impossible, it just becomes less probable and we're going to give it formula weight that represents how strong of a constraint it is.",
            "So if we really believe in the formula, then we give it a high weight.",
            "Then you pay big penalty for violating it and vice versa, and then the probability of a world is just the log linear model of the form that we saw before I eat.",
            "That's the normalized exponentiated some of the weights of the formulas that the world satisfies.",
            "And the more formulas it satisfies, the more the higher weight they are, the more probable the world becomes, and you know this is exactly the behavior that we want.",
            "So here."
        ],
        [
            "A more precise definition I'm going to call a Markov logic network or MLN for short.",
            "A set of pairs FW where F is a formula in first order logic and W is a real number, positive or negative.",
            "This is the syntax.",
            "The semantics of this language is.",
            "The following is that, together with a set of constants representing objects in the domain and MLN, defines a Markov network as follows.",
            "It's going to have one node for each grounding of each predicate in the MLN, and it's going to have one feature for each grounding of each formula in the MLN with the corresponding weight.",
            "So here's our friends and smokers example.",
            "Right now we can assign weights to these formulas and now we actually are back to something that's useful.",
            "Right, these things are back to being what they should have been all along, which is statistical regularity's."
        ],
        [
            "And the reason this formula has a higher weight in this one is that you know this is a stronger regularity.",
            "So what does this MLN mean, right?",
            "What is the statistical model that I have built here?",
            "As it turns out, as much as this looks like a toy example, this is actually a pretty state of the art social network model.",
            "Believe it or not."
        ],
        [
            "So let's suppose for simplicity, to fit on a slide that we have only two constants in the domain N and Bob.",
            "So the rule is that I'm going to have a ground Atom for each for a variable in the Markov network for each grounding of each."
        ],
        [
            "Cricket with each with each constant.",
            "So I'm going to have cancer, Anna right Boolean variable.",
            "True friend has cancer false if she doesn't same for smokes N and same for Bob."
        ],
        [
            "And the same thing for friends Anna Bob.",
            "OK so I'm going to have friends in above and friends Babanin notice that the two things are different, right?",
            "Friendship is asymmetric, as sociologists will tell you.",
            "And this actually happens a lot in real social networks like Bob, for example, could be a much better friend of Interventies of Bob.",
            "And then there's like these degenerate cases of like friends and and and friends, Bob, Bob, that you know, maybe you have to do their self esteem or something.",
            "So at this point what we have is a large number of Boolean variables, and we're going to build a distribution over these variables.",
            "So how do we build that distribution?",
            "Well, the rule is that we're going to have one feature for every grounding of each formula.",
            "OK, so for example, we're going."
        ],
        [
            "I have a feature smokes Ann implies cancer in which is going to lead to having an edge in the graph here and the same thing for Bob.",
            "OK, now this formula has two predicates in it, so it leads to a two way click.",
            "This formula has three, so it's going to lead to a 3 way click IE."
        ],
        [
            "The triangle, so here are the various triangles, so this is the structure of the model, right?"
        ],
        [
            "So at this point, we've completely defined the model.",
            "The probability of a world right is going to be the normalized exponentiate some overall.",
            "The 1st Order formulas of the weight of the Formula Times the number of true groundings of the formula in the world.",
            "And notice that the MLN does not just represent the single distribution, it actually represents a whole family distributions.",
            "And that's where a lot of its power lights they may learn is really a template for constructing ground Markov networks.",
            "And depending on the domain I could get very different networks.",
            "In particular, some could be much larger than others, but they're all repetitions of the same templates.",
            "Now of course something that has already occurred to you that this all sounds very nice, but it doesn't sound remotely practical, right?",
            "Because I'm getting I'm going to get a combinatorially explosion when I try to create the ground atoms and features.",
            "And you know this isn't going to fit in memory, and I'm not even going to get off the ground.",
            "And indeed, if we haven't solved that problem we had, we don't really have something that we can use.",
            "Of course, I wouldn't be talking to you about it here if we hadn't at least two enough of a degree.",
            "That's practical.",
            "But before we look at some of the algorithms, one thing that you can do right off the bat that is very, very useful is to just have typed variables and constants.",
            "OK.",
            "So if you have a predicate like works for XY, it only makes sense to replace X by people.",
            "And why by organisations, right?",
            "Anything else is a waste of time, and they're already vastly cuts down on the number of atoms that you're going to have to deal with now in first order logic you can actually handle the full range of constructs in logic functions, existential quantifiers, infinitely continuous domains.",
            "I'm going to.",
            "I'm not going to talk about that here, 'cause it's not the most relevant, but certainly."
        ],
        [
            "If somebody is interested, I can give you the pointers.",
            "So how does this relate to the statistical models that we want to be able to build easily?",
            "You know, combine reuse etc.",
            "Well, the nice thing right?",
            "What makes Markov logic so useful is that all of these models?",
            "Are simple special cases of Markov logic, meaning I can define them very easily using just a few formulas and then a result like you know, I can also combine them just by sharing predicates, right?",
            "If I have, you know, hmm, and the logistic regression that share some predicate and I have a model that is a combined HMM and logistic regression for example.",
            "And the other thing, of course, is that Markov logic allows the objects to be non IID.",
            "Right as we saw in the social network example, whether I smoke depends on whether or not my friends smoke.",
            "OK, so in Markov logic, setting this up is not a problem.",
            "How does this relate to 1st order logic?",
            "Well in?"
        ],
        [
            "Limit of infinite weights.",
            "You get first order logic back right?",
            "That makes sense, right?",
            "If you paint infinite penalty then you become valuating a formula then you become impossible and in fact there's a theorem that says you can answer every entailment Queen 1st order logic by computing conditional probabilities in a Markov logic network that you get by assigning a weight of Infinity to every formula.",
            "That's the infinite wait case.",
            "Of course.",
            "The interesting case is the finite weight case.",
            "In that case, we can still say something interesting, which is that if the knowledge base is satisfiable, meaning you can make all the formulas true at the same time.",
            "And although it's are positive, which you can always do by flipping, you know the signs of negating formulas with negative weights and making the weights positive.",
            "Then the satisfying assignments are the modes of the distribution.",
            "Meaning that the world's that first order logic likes are still embedded in there.",
            "There's the peaks of the distribution is just that probability has leaked away from them which which is which is exactly what we want.",
            "You know, like the neighboring worlds, also have some probability.",
            "But the most important thing is that Markov logic allows contradictions between formulas.",
            "Think of the semantic web right.",
            "The dream of the semantic Web is that everybody contributes knowledge, right?",
            "They write their knowledge in all, or, you know it's translated in two hours.",
            "Or you know an RDF triples and what not, and then you know somehow magically you can do inference over this, right?",
            "But first order logic inference isn't.",
            "You know, these things are all special cases of 1st logic, but that's not going to cut it right because it's going to be full of contradictions just like the real web.",
            "Like the you know, the text web is full of contradictions.",
            "The semantic web is going to be full of contradictions as well.",
            "Right?",
            "So in logic, if you have a contradiction, either directly or indirectly between formulas by their consequences, then you can prove anything.",
            "Anything follows from a contradiction.",
            "In Markov logic, having contradictions between formulas is not a problem.",
            "If there's something that says that A and something that is not a in Michael logic, what happens is you do the inference.",
            "You weigh the evidence on both sides, and you produce the probability that is true.",
            "OK, so it's something like Markov logic.",
            "I think something like the semantic web actually becomes more feasible.",
            "So.",
            "That's the basic language."
        ],
        [
            "Right now, in order to make this language useful for solving the kinds of problems that we saw before, we need efficient inference and learning algorithms and justice.",
            "The language had all these models of special cases.",
            "Not surprisingly, these inference and learning algorithms are going to be based on the kinds of infants and learning algorithms that people have developed before.",
            "But now you can see them all in a nice integrated framework, where instead of having to learn 500 things, maybe you just have to learn you know half a dozen."
        ],
        [
            "So let's look at some of those algorithms.",
            "So there's two main kinds of inference that you typically want to do.",
            "The first one is what's called M AP for maximum a posteriori or MPE.",
            "For most probable explanation inference.",
            "This is inferring the most likely values of your query variables given your observed variables.",
            "For example, I may want to infer the topics of web pages given their words on them, and the links between them, right?",
            "This is an example of MLP inference.",
            "And for this, there's a bunch of algorithms.",
            "They include Max Walk set, which is just a weighted satisfiability solver.",
            "Because the modes of the distribution are the states that satisfy the most formulas, we can just use satisfiability testers to solve the problem, and this is a very highly developed technology, right?",
            "Notes solvers like Walksat can solve problems with millions of variables in minutes.",
            "And problems that are hard but many set problems are easily can solve hard problems in minutes with things like hundreds of thousands and millions of variables.",
            "So you know, this is pretty scalable.",
            "Lazy said is a more efficient version of Max Walk set that deals with the problem of the combinatorial explosion and you, ideally this set is that you are not going to create all the ground atoms.",
            "Because you don't need to think of friends ex.",
            "Wife for example.",
            "There's 6 billion people in the world, so there's 36 billion billion possible groundings of friends, right?",
            "But the vast majority of them are false, right?",
            "'cause most people have?",
            "Maybe only a few dozen friends or a few 100, maybe a few 1000.",
            "If you're on Facebook, but that's you know, that's the maximum.",
            "So what we can do is we can assume that atoms are false until the infant says that they're true, and we only explicitly represent the true atoms, and correspondingly most of the clauses are going to be true because their preconditions never fire, right?",
            "If we're not friends, we don't need to worry about whether you're smoking habits, influence might, right, and so for most cases I had met him not going to have to drag out the closet, so this kind of his influences again, you know that."
        ],
        [
            "Is a sub by many orders of magnitude.",
            "The other thing that you often want to do is compute marginal and conditional probabilities.",
            "And in principle you could do this just by checking in each possible world whether your formula holds or not.",
            "And then you sum the probabilities of those worlds and you know how to compute them from the formula, right?",
            "The problem, of course is that there's an exponential number of them, right?",
            "So this isn't going to work.",
            "What you can do though, is you can use.",
            "You know, sampling, right?",
            "You can use MCMC to basically generate worlds at random, and you count the fraction of those samples in which your formula holds, and that's your estimate of the probability.",
            "And you can do this using your standard.",
            "You know Gibbs sampler, but unfortunately give samples really suck when you try to do things on a large scale and within within with strong dependencies.",
            "So we have developed a better algorithm called MC set, which is an MCMC algorithm that uses a satisfiability solver to generate a new sample.",
            "And that we can jump very quickly between different modes in the distribution, which is, which is what you need to have fast inference.",
            "There's also another thing that you can do which is called knowledge based model construction, and this is don't construct the entire network.",
            "Just construct the subset of the network that is needed to answer your question right again, think of the semantic web, right?",
            "I have all this knowledge out there and now you're asking a question, but probably out of all the rules and facts that people contributed, there's only a very few that actually relevant to the query.",
            "So I'm going to construct the network only of those and do probably influence on that.",
            "So even this even, even though the semantic web may contain bazillions of rules and facts, hopefully the probabilistic inference that you have to do it in today, which is the most expensive part, will be done on a fairly small network.",
            "And the other thing that you can do that I'm going to talk about in a little more detail is lifted belief propagation.",
            "So belief propagation, as some of you may know."
        ],
        [
            "Is a very general inference algorithm that has lots of things that special cases like you know with Serbian Univision networks.",
            "You can do inference on them using with propagation etc etc.",
            "So it's a nice general argument they have, but what we're going to look at here is a lifted version of the propagation and by lifted I mean that it's lifted as in logic you can do lifted inference.",
            "I'm going to do inference for a whole sets of objects at once.",
            "So the cost of my inference is actually going at the end of the day be independent of the number of objects that I have.",
            "It is actually only depends on the number of sets of objects that behave differently from each other, and again that is potentially going to bias very big scale UPS.",
            "So what's the idea?",
            "Well I guess already."
        ],
        [
            "Said this, so here's a brief review of belief propagation for those of you not familiar with it, the best way to think of blue propagation is in terms of what's called the factor graph.",
            "Affecter graph is a bipartite graph where on the one side you have your variables, your nodes and on the other side you have your features and each node is connected to all the features that it appears in, and the way belief propagation works is that you what we're going to do is that.",
            "Each node is going to maintain what is in essence and approximate an approximation of its marginal probability.",
            "And it just starts out at one because, you know, we don't know anything in the beginning.",
            "And then we're going to iterate this process of passing messages from nodes to features and back until it converges and the message.",
            "So the message there are no sense to feature is, you know.",
            "Roughly speaking, it's marginal probability what a feature computes is this thing here, which is.",
            "What the feature does is it pretends that all all the all the nodes in it are independent, right?",
            "And computes the probability of each state right, which is the value of the potential times the marginals sums out all the other variables except the one that is sending the message to, so the message that the features seem to the node is basically what the future thinks.",
            "The margin of the node should be, and then we just do this until convergence.",
            "Now the problem that we're going to have here is that we want to do this on networks with millions of nodes and billions of features.",
            "OK, so just standard belief propagation isn't going to work.",
            "You know, it probably won't even fit in memory.",
            "So what can we do?",
            "What we can do is no."
        ],
        [
            "Notice that most of the time there's going to be whole sets of nodes that throughout belief propagation always send the same messages.",
            "Right?",
            "And same thing for features.",
            "And so if I notice that I can divide these nodes into these blocks, that's in the same message.",
            "Now instead of sending it, you know, look at this block on this block instead of sending a message from every node in here to every feature in here."
        ],
        [
            "I can just send one.",
            "OK. And now suddenly my belief propagation became a lot faster and the memory requirements also became a lot smaller because instead of having this many nodes, I just have as many nodes as there are rectangles here, and I'm going to call these rectangles super nodes in this rectangle, super features, and So what we're going to do is we're going to construct the lifted network that's made up of supernode and super features that each stands for, you know, maybe millions or quadrillions of nodes, right?",
            "So the network that I'm doing belief propagation on is small or hopefully small in the worst case, it will still be larger."
        ],
        [
            "OK."
        ],
        [
            "So, but of course the whole question is how do we do that?",
            "Right, how do we form the listed network?",
            "Well, here's the idea.",
            "Again, I already said this.",
            "You know super node is going to be a set of ground items that will send and receive the same messages throughout BP.",
            "Similar for super features.",
            "And then we're just going to run blue propagation on this network.",
            "How do we form the lifted network?",
            "Well, here's the idea.",
            "If you have no evidence, if you think about it, all groundings of a predicate will behave exactly the same way, right?",
            "'cause?"
        ],
        [
            "You know, we've said the same thing about them, and all groundings of each formula would also behave the same way, right?",
            "The thing that complicates life is of course evidence.",
            "You know, like for example what the words on the page are right?",
            "You know, without evidence you can do anything useful.",
            "So the question becomes, how do I propagate the evidence through the belief propagation algorithm and figure out what are the sets of atoms that still behave the same way?",
            "Well, here's how you can do that.",
            "You start out by forming your initial supernotes, and if you don't have evidence about a particular predicate, then there's just one right?",
            "'cause all groundings are unknown.",
            "If you have evidence, then you're going to break up the predicate into three super nodes, one containing all the true groundings, right?",
            "One containing all the false groundings, and one containing all the unknown groundings, right?",
            "So at this point I've divided the evidence predicates into super nodes, but nothing else.",
            "But now the question is, given that these predicates have been divided into super nodes, not what that was that what that is?",
            "What does that cause in the features that those nodes appearing right now if you think about it, if a feature involves two predicates and each one of them can be in the true, false or unknown states right?",
            "Then there are six cases for the feature, right?",
            "True true true false, true Anon, etc.",
            "And they may not all occur.",
            "But you know, at most there are those six.",
            "And So what we're going to do is we're going to form those super features.",
            "And if you think about it, the way they formed is by doing joints of the Super nodes in the database sense of joint, right?",
            "If one of my predicates ispu XY and the other predicate is in RYZ, and I have a feature that's a conjunction of those right, then what I need is to join those two on Y to get something over XY and Z. OK, so I create my super features by doing joins of the Super nodes that appear in them, and now I need to go back and say now that I've subdivided the Super features.",
            "What subdivisions that does this induce in the other non evidence predicates that appeared in them, right?",
            "So now if you think about it, what's going to happen is that I have to project my features back to the nodes, right?",
            "If I if I have a feature over XYZ and my note is only over XYI need to project out Z and Now the key question is how many projections does each node receive two nodes that receive the same number of projections from the same features are going to receive the same messages and therefore they can be put together in the same supernode.",
            "And so this is what?"
        ],
        [
            "I'm going to do at this point.",
            "I have a new generation of supernodes, so this will in turn generate some subdivisions in some for the features and I'm going to keep on doing this until it converges.",
            "And then you can prove.",
            "That it converges that there's a unique minimal lifted network, and that this algorithm finds it, and then when you do inference on that network, you get the same results as if you're doing inference on the whole humongous network with bazillions of nodes.",
            "So this is a version of live propagation that is just vastly more scalable than the standard belief propagation.",
            "When there is this kind of repeated structure that you can exploit."
        ],
        [
            "There is one important aspect that I glossed over so far which is the following.",
            "The lifted network that you get into that they may be very small and the belief propagation runs on that network, so hopefully it will be fast.",
            "But how do I represent the supernodes and super features while I'm building the network?",
            "The easy way is to just represent them as database relations and to use standard database joins and projections.",
            "The problem with that is that that itself may run out of memory, right?",
            "So what can I do?",
            "Well, something that I can do, taking an idea from logic is user resolution like representation, where instead of enumerating values I say for example what I allow here is all values, But these two or you know this has to be equal to way or a or equal to be right and now as long as I figure out how to do joins and projections in this more condensed language, I can still do the algorithm, but using much less memory.",
            "And more generally, you can just think of this as the problem of forming clusters of atoms that behave the same.",
            "And forming, you know the same thing for features, right?",
            "And there's no single language that is always going to be the best one.",
            "You know.",
            "Indeed, finding the best language is not surprisingly, an NP complete problem, but this is something that you know could buy you a lot, and we have some preliminary results where by forming classes in more interesting ways you get much bigger scale ups.",
            "And you can also form approximate clusters, which will you know you cluster together atoms.",
            "That's in approximately the same messages, right?",
            "And that will in turn give you a lot more scale up.",
            "OK."
        ],
        [
            "So let's talk about learning."
        ],
        [
            "So learning in this framework for learning your data is a relational database, right?",
            "So what if you have if what you have is something else, then you need to convert it to a relational database, right?",
            "But you can write.",
            "You can convert text by saying you know, having a credit that says what tokens appear, etc.",
            "And here in this talk I'm going to make the clothes world assumption, which is that everything in the night in your database is false.",
            "If you don't want to make that assumption, then you need em.",
            "Versions of the algorithms that I'm going to describe, but you know, those are available too, and now you can learn parameters and formulas.",
            "The parameters you can learn, generative Lior, discriminatively and I'm going to just very briefly."
        ],
        [
            "It's on each one of those.",
            "So generative with learning this is.",
            "This is like the standard statistical learning problem.",
            "I want to find the weights that make the database as likely as possible.",
            "There's no closed form solution, but it is a convex problem, so there's a single global optimum.",
            "You can find it using.",
            "You know gradient descent or quasi Newton standard methods.",
            "The gradient has a very intuitive form.",
            "The derivative of the log likelihood with respect to a clause.",
            "It is with the weight is just the difference between the number of ground true groundings of their claws in the data and the number predicted by the model.",
            "So if the model predicts that it's true less often than it should be the way it needs to go up if it produces often is true.",
            "More often they choose, it needs to go down, and once they all line up, you've reached the maximum likelihood point.",
            "Now, of course, this requires inference at each step, right?",
            "To compute this expectations.",
            "So this could be slow, something that you can do that's more approximate."
        ],
        [
            "But a lot faster is to use instead something called the pseudo likelihood.",
            "The pseudo likelihood is just the product over all the variables of their conditional probability given the state of its neighbors in the data, and so computing it doesn't require inference.",
            "And if you use it with something like LB FGS, it's actually pretty fast, and it's a consistent estimator.",
            "The problem is that sometimes the parameters that you learn are not so good.",
            "So what else can you do?",
            "You can do discriminate."
        ],
        [
            "Of learning and you know this is again what you do when you do logistic regression or conditional random fields etc etc.",
            "Here the idea is that you're just going to if you know in advance what variables are going to be query and what verbs are going to be evidence, you just maximize the conditional likelihood of the query variables given the evidence variables and the form of the gradient is still the same.",
            "But now what I can do is approximate this expectation just by the counts in the most likely state of Y given X, right?",
            "What makes these inference problems hard is that there are many many modes.",
            "But when I condition on evidence, the mode start to disappear.",
            "And ideally, there's one that dominates.",
            "In fact, in the limit there has to be one that dominates.",
            "So instead of counting over next minimal number of states, I can just find the most likely one under the accounts there, and in fact this idea was."
        ],
        [
            "Originally proposed for training HMMS discriminatively by by Michael Collins and he called it the voted Perceptron.",
            "Based on some earlier ideas by.",
            "Rob Shapiro and and others, and this is very much like a perceptron algorithm.",
            "I initialize the weights to zero and then I do this some number of times.",
            "I find the most likely."
        ],
        [
            "State of Y given X in the case of hmm, you can do using the Viterbi algorithm and then I subtract the counts in that state from the accounts in the data multiplied by a learning rate after the weights.",
            "And that's my update, and then I return the average of these, which is good for combating overfitting, and that's why it's called over separate.",
            "What we want, of course, is to do this on an arbitrary network, not just a mark of an HMM, but we can do that by replacing the Turbo with the Max Walksat algorithm that I mentioned before.",
            "OK, so again, you don't actually need very new algorithms to do this, you just need.",
            "The combination of algorithms that were available before you."
        ],
        [
            "Also do structure.",
            "Learning time is running a little short, so let me skip over that.",
            "The idea here is that you can actually learn the formulas or refine some formulas that you have."
        ],
        [
            "By looking at data."
        ],
        [
            "So let."
        ],
        [
            "As I mentioned, this is all available in open source software called Alchemy.",
            "I will put up this URL again at the end.",
            "Alchemy is really a kind of programming language that you can use to build a statistical models and combine them and reuse them."
        ],
        [
            "And so forth.",
            "And you know what?"
        ],
        [
            "Going to see now is an example of what you can do with this.",
            "So as already mentioned, many web applications and also non web applications have been done with Markov logic so far.",
            "Let me as an example.",
            "I think there's nothing like seeing a concrete example, so the example that."
        ],
        [
            "Users information extraction.",
            "So information extraction is the problem of taking some unstructured or semi structured data like you know web pages or in this case the citation list and extract databases from them."
        ],
        [
            "OK.",
            "So this is this is the problem that things like Google Scholar and Citeseer solve is go to the reference list in papers and extracted database of publications and there's really 2 problems that you have to solve here.",
            "The first one is segmentation, figuring out that, for example Tripoli, 06 is a venue and PROC single is an author."
        ],
        [
            "The next one isn't the resolution.",
            "You need to figure out that Tripoli, 06 and the National Conference on Artificial Intelligence are the same thing.",
            "Otherwise, you're going to end up with duplicates.",
            "And once you've duplicated duplicated the field you want, of course."
        ],
        [
            "Duplicate the records."
        ],
        [
            "Now the state of the art for this is to use an HMM or CRF to assign each took into a field and then to use a classifier like typical logistic regression or naive Bayes to do the Inter resolution by taking each pair of observations.",
            "In this case fields are citations and predicting whether the same or not.",
            "And then you need to enforce transitivity because they face the same as B&B is the same as.",
            "See then is the same as C, and is often done in some clustering or other heuristic way.",
            "Now, if you were to build and you know people build these things by the dozen these days, you know to extract, you know all kinds of information.",
            "If you were to build this, you know by hand using a language like Java or C++ today it would probably run you 10s of thousands of lines of code and you know take few weeks of programming and debugging to do.",
            "In alchemy, this whole thing can be done in just seven formulas that actually fit on a single slide.",
            "So this is what I'm going to put up next.",
            "So in MLN is diff."
        ],
        [
            "Find in an MLN file using three things.",
            "The first one is type declarations.",
            "The second one is predicate declarations.",
            "In the third one is formulas.",
            "The type declarations are actually optional because you can just infer them from data, and that's what we do."
        ],
        [
            "All the time, but here for clarity, I'm actually going to make them explicit.",
            "So we're going to use four types here.",
            "One is token that you know is the actual words that appear on pages.",
            "The other one is the fields that you want to extract, and citations, and."
        ],
        [
            "Sessions and I'm going to have my evidence predicate.",
            "This is the predicate that says that this token appears in this position in this."
        ],
        [
            "Dictation.",
            "And my query predicates this one in field.",
            "Does the segmentation it says that this position is part of this field in the citation.",
            "Like for example, the first position in the citation C one is part of the author field and these two do the anti resolution right?",
            "This one means that the field say author field in citations one and two are the same and this one that these two citations are the same?"
        ],
        [
            "So here's the MLN, using seven formulas that's that.",
            "Does the whole problem for you.",
            "These first three formulas are in, HMM, that the segmentation and these four are Inter resolution system.",
            "OK so how does?"
        ],
        [
            "This work?",
            "Free variables are implicitly universally quantified, as people usually do in logic programming languages, so you know there's no point in repeating the quantifiers all the time and this notation.",
            "This plus.",
            "Means that we want alchemy to learn one weight for each grounding of that formula, with each constant of that type.",
            "So what this formula here says is that if this token appears in this position, then this position is a part of this field.",
            "So for example, you know this what an be 'cause?",
            "I'm going to have a plus on the token on the field.",
            "What I'm going to learn here is a matrix of tokens by Hills, which is very precisely the observation matrix of an HMM where the observations are tokens and the hidden states are fields.",
            "And So what I'm going to learn here is the correlation between tokens and fields, like for example Smith is predictive of author conferences."
        ],
        [
            "Adictiva, venue, etc.",
            "The next formula captures the transition matrix, right?",
            "If I'm in this field right now in the next position, I'm going to be in this field.",
            "This one is a detail that I'm going to ignore here now.",
            "How do we do the entire resolution?",
            "Well, here's the."
        ],
        [
            "Formula that does most of the work it."
        ],
        [
            "This basically says that fields are the same if they have a lot of tokens in common.",
            "OK and of course some tokens are more informative than others, so they're going to have a higher weight and I have a plus for token and field as before.",
            "So what's going on here?",
            "Is a logistic regression where this is the predicted variable and and these formulas you know are constructing the features.",
            "Another way to look at this is that we're doing a similarity comparison between the fields.",
            "This."
        ],
        [
            "Formula says that if the fields are the same, then the citations are the same and vice versa.",
            "Figuring out their Tripoli, I and the National Conference on Artificial Intelligence are the same, just based on string comparison as people traditionally do is hopeless.",
            "Right, but what happens here is that if the authors are the same and the tiles are the same, and because of that, I figure out that the citations are the same, then it follows that the venues must be the same.",
            "So at this point I figured out that triple AI and the national conference rooms are the same thing, and this becomes available to use elsewhere, so there's a lot of power in this, because now you can do a lot more with a lot less work on your part.",
            "And these last two formulas are just."
        ],
        [
            "The transitivity, so for example, this formula just says that you citation sees this embassy primacy, primacy, prime prime, then sees the semesi prime prime.",
            "And again, contrast this with, for example, the paper that Macalmon Welner published here at NIPS a few years ago, where what they did was to add transitive iti to a CRF model for Inter resolution, and it was a difficult feat that they perform because they had to invent.",
            "Knew you know, inference and learning algorithms to do this right very, very widely cited paper in this framework, all they have to do is add this one formula and this form is just the transitivity axiom for equality, right?",
            "And then you know.",
            "The inference and learning algorithms that we already have under the hood do the necessary."
        ],
        [
            "For you now this thing is actually a fairly good into resolution model.",
            "This one is not such a great segmentation model and the reason is that it has trouble deciding exactly where the field boundaries are and people.",
            "Information extraction notice, and in fact that's what's behind the whole field of wrapper induction is you write rules that detect where field begins.",
            "Now it's this you know HTML tag fold by.",
            "That means that this is the start of the price of the CD.",
            "But the question is how do you combine that with an HMM or CRF?",
            "Well, in our case it's very simple because you can just go and refine these rules, in particular, for example in the case of citations, we know that there's a very good field boundary marker, which is a period, right?",
            "Most fields end with periods, So what we can do is we introduce a new condition in this rule that says neighboring fields."
        ],
        [
            "It could be the same naming positions like to be in the same field only if one of them is not a period, so the period stops the propagation.",
            "And with this we suddenly get much better results, so these are results on quarter, which is a benchmark.",
            "It's a database of computer science papers.",
            "This is precision recall.",
            "We want to be up here.",
            "This is the model without the period, right?",
            "And when we add the period, very simple change.",
            "Suddenly we get much better and then if we add a comment we get even better and so forth.",
            "So now it becomes very easy to come right.",
            "We just combine here in HMM with the logistic regression with the transitivity closure model with some additions to the rules and it was all quite easy and search for it.",
            "And the network that I showed you is not the state of the art solution for this problem, but we have developed one that still has only a couple dozen formulas, and it's the best.",
            "Currently the best state of the art solution for things like you know the core benchmarks on the Sightseer benchmarks, and more importantly, it took a lot less time to develop than the previous solutions that."
        ],
        [
            "That people had.",
            "OK, so to conclude.",
            "The web is amazing in terms of being a domain from machine learn."
        ],
        [
            "Right, it provides never ending problems.",
            "You know, every year new kinds of applications, machine learning come up.",
            "Conversely, machine learning provides a plethora of solutions.",
            "But you know this.",
            "This creates a very big problem, right?",
            "Which solutions that we use for which problems and how to figure out what to do on how to combine them.",
            "What I would argue is that we need the unifying language to do this.",
            "Markov logic is such as one possible such unified language.",
            "It just uses weighted 1st order logic to define statistical models.",
            "And now with this we can combine them or use them etc.",
            "There are efficient inference and learning algorithms as I mentioned before.",
            "If you want to do something on the web scale today using Alchemy will probably not cut it, but you can then refine you can define.",
            "You can prototype your solution in alchemy and then when you want to do it on a large scale then you can do some specialized coding and this is what a number of people have done at this point and you know it works quite well.",
            "So there have been many successful applications like for example the information extraction one that I showed you.",
            "This is all available in open source software and you know there's a website.",
            "Here's here's the URL.",
            "It has the software, the documentation, tutorials, APIs, Merlins, datasets, you know everything that you need to play.",
            "Thank you."
        ],
        [
            "Any questions?",
            "So you have a nice list of characteristics of web learning problems, right?",
            "One that wasn't there or maybe was there implicitly was that most run web problems are online, right?",
            "There's a constant stream Aquarius new web pages.",
            "You possible results?",
            "Does that fit into kind of modeling?",
            "Talk about?",
            "Yeah, that's that's a very good point.",
            "In fact, a lot of the web problems are online and very much we want to address that.",
            "And I didn't talk about that here, but we do have online versions of some of the alchemy algorithms, and we're developing more.",
            "It's something that's not as much as some of the rest it, but definitely I think that's very important and ultimately.",
            "We want everything to work online and I think we can do that.",
            "How big is it and how long does it take to train?",
            "For example, this simple seven former model on it and this sophisticated using formulas?",
            "So the formula.",
            "So the core database, the original cord that bases, I think hundreds of thousands of papers, the core database that's used as a benchmark for citation matching is small.",
            "It's in the order of thousands and how long does it take to train on it?",
            "It takes on the order of maybe minutes to an hour depending on algorithm.",
            "If you pseudo likelihood and the objects it takes a few minutes, but here's an important thing is that these algorithms scale linearly in the size of the data.",
            "Right, so if instead of 1000, right?",
            "So this is very important.",
            "Most machine learning algorithms have bad scale up, but algorithms that are based on just gathering these sufficient statistics, like you know, like we're seeing here the scale up, you know you just need to run once over the data.",
            "Collecting the sufficient statistics.",
            "Number of examples?",
            "Or do you need a number of notes here?",
            "I mean the number of in this case, for example, the number of papers, right?",
            "The cost of getting this fish sticks is of the order of the size of the database.",
            "Then the cost of the inference might be alot alot higher, but that's all the size or the size of the MLN and not of the size of the data.",
            "There's a need for language.",
            "As an example, exactly.",
            "Absolutely, so it's interesting 'cause I just give a talk in the probabilistic programming Workshop where this was.",
            "This was my last slide was, you know, the talk was what we can do with Markov logic today, but the last slide was we can't do with Markov logic today right?",
            "And the single most important thing that we can do with Markov logic today is that these algorithms are not push button yet.",
            "Push button, it's easy and it's a lot of fun to write down your model in Markov logic.",
            "That will take, you know, maybe all of five minutes I've literally given talks where people while I'm giving the talk, they download alchemy and they define their model in Markov logic and then come ask me questions afterwards.",
            "Right?",
            "It's that easy.",
            "The thing that's not always easy is that all of these algorithms have parameter settings.",
            "You know they don't necessarily work with the default parameter settings when you don't get the results that you want.",
            "It's not necessarily clear why you're not getting them right, and the problem is that.",
            "We're inheriting the state of the art in statistical learning and probabilistic inference, and all these things right?",
            "And the state of the art is not at the point where these things can be used by non experts.",
            "What we really like to have is somebody who only knows about the domain.",
            "You know, rights, you know writes formulas in Alchemy, tries things on their domain, but doesn't have to know how you know Elegs works or Max walk set, and we're not there yet, but I think you know with.",
            "This summer you've got a great language for talking about models.",
            "You've got some technology generating algorithms models.",
            "One of the attractions of what you've done is it's declarative.",
            "Make sure what you really want this push button, then declarative system at the end user.",
            "Well, I mean this is a declarative system.",
            "At the end, use level.",
            "But then what happens is that.",
            "But the other models of the problems, right?",
            "That promise of declarative programming, right, is that you can just say what you know and that will give you the results that you want.",
            "And that is true here.",
            "Up to the point that you have to fiddle with the parameters of the algorithm.",
            "Yeah, I think you're stuck here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Machine learning for the web, which is of course a very large and ever growing field.",
                    "label": 1
                },
                {
                    "sent": "So much of this is based on work that I've done at UW with these people.",
                    "label": 0
                },
                {
                    "sent": "So here's an overview of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will begin with some motivation and then some.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Background I will.",
                    "label": 0
                },
                {
                    "sent": "I will introduce this language called Markov logic, which is a language is going to allow us to do such a unified view of the various learning algorithms for the web.",
                    "label": 1
                },
                {
                    "sent": "Talk about how to do inference and learning, mention some of the open source software that's available to do this and then look at some of the applications that we can do, and in particular I will go into one application in more detail to basically give people a more concrete sense of what can be done and conclude with some discussion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So motivation.",
                    "label": 0
                },
                {
                    "sent": "The web is an amazing machine for generating machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "It's really, you know.",
                    "label": 0
                },
                {
                    "sent": "This is a partial list, right?",
                    "label": 0
                },
                {
                    "sent": "And you know, many of these topics that actually quite recent.",
                    "label": 0
                },
                {
                    "sent": "All of these are important things that you want to do on the web.",
                    "label": 0
                },
                {
                    "sent": "All of them call for machine learning, hypertext classification, search ranking, personalization, recommender systems, Rep reduction, information extraction, information integration, extracting information from the deep web, making the semantic web actually work, ad placement, content selection.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the deep web for those of you not familiar with it, is this idea that most of the information on the web is in the form of databases with the web front end in front of them.",
                    "label": 0
                },
                {
                    "sent": "They are not web pages, so if you just do ordinary crawling and indexing you actually won't capture the deep web.",
                    "label": 0
                },
                {
                    "sent": "But that's actually a lot of the information is, so some of the machine learning problems on the deep web are can I learn what information it is that a deep web source provides?",
                    "label": 0
                },
                {
                    "sent": "And can I figure out how do we extract that information and then combine it with others?",
                    "label": 0
                },
                {
                    "sent": "Can I learn how to make plans for how to get the answers that I want?",
                    "label": 0
                },
                {
                    "sent": "Right, it's sort of like a very interesting learning problem, 'cause a source might be higher quality, but be less available.",
                    "label": 0
                },
                {
                    "sent": "For example, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the problem with the deep web.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Figuring out you know how much to be there in auction.",
                    "label": 0
                },
                {
                    "sent": "Social networks are very popular topic these days that I'll actually use as a running example in some of the explanation.",
                    "label": 0
                },
                {
                    "sent": "Mass collaboration, spam filtering, where the kinds of spam that you want to filter our ever growing right.",
                    "label": 0
                },
                {
                    "sent": "There's there's of course email spam, but then there's like spam.",
                    "label": 0
                },
                {
                    "sent": "There's you know, click spam, there's you know.",
                    "label": 0
                },
                {
                    "sent": "Ever growing new kinds of spam that you want to fight with machine learning, reputation systems.",
                    "label": 0
                },
                {
                    "sent": "Performance optimization is actually maybe one of the less visible applications.",
                    "label": 0
                },
                {
                    "sent": "Machine learning on the web.",
                    "label": 0
                },
                {
                    "sent": "It's very important right on the web.",
                    "label": 0
                },
                {
                    "sent": "You want other things on the very large scale, you can use machine learning to optimize the performance of your disk accesses your crawlers, your networking, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So an ever growing set of of learning problems that the web.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Provides.",
                    "label": 0
                },
                {
                    "sent": "On the other side, there's also an ever growing set of machine learning solutions.",
                    "label": 0
                },
                {
                    "sent": "And again, here's a very partial list of them.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Pick your own favorite machine learning technique, naive Bayes, logistic regression, you know vision networks you know, log in your models, Hmm's, conditional, random fields, SVM.",
                    "label": 1
                },
                {
                    "sent": "You know there's hundreds of them and you know.",
                    "label": 0
                },
                {
                    "sent": "Wait another.",
                    "label": 0
                },
                {
                    "sent": "Well, there's probably thousands of them, and every year you'll see you know 100 more.",
                    "label": 0
                },
                {
                    "sent": "You know a few 100 more appear, right?",
                    "label": 0
                },
                {
                    "sent": "This is very nice, but it creates a very big problem, which is, if you're somebody who works on web problems.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you know actually needs to solve them.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                },
                {
                    "sent": "Do you have to learn all the algorithms that I just listed?",
                    "label": 0
                },
                {
                    "sent": "God forbid, right?",
                    "label": 0
                },
                {
                    "sent": "I mean even we machine learning researchers can't keep up with all of it and then do they figure out you know which one to use each time, right?",
                    "label": 0
                },
                {
                    "sent": "How do you do that right?",
                    "label": 0
                },
                {
                    "sent": "You know 100 machine learning algorithms and which one to use for each problem and which variations of that will be try.",
                    "label": 1
                },
                {
                    "sent": "And how do you frame the problem that you have?",
                    "label": 0
                },
                {
                    "sent": "Is machine learning right?",
                    "label": 0
                },
                {
                    "sent": "This is often one of the hardest parts.",
                    "label": 0
                },
                {
                    "sent": "And how do we incorporate the knowledge that you have about the problem into your machine learning algorithm is actually often the most important thing that someone in a particular application does incorporate their knowledge, but this is actually not very easy, right?",
                    "label": 0
                },
                {
                    "sent": "How to incorporate your knowledge into an SVM or into a neural network, right?",
                    "label": 0
                },
                {
                    "sent": "Not very clear.",
                    "label": 0
                },
                {
                    "sent": "And how do you then glue all the pieces that you've built?",
                    "label": 0
                },
                {
                    "sent": "How do you build them altogether?",
                    "label": 0
                },
                {
                    "sent": "And you have to start from scratch each time, right this?",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, the sad state of machine learning on the web today is that pretty much every time you have to start from scratch, you build a CRF to extract one kind of information.",
                    "label": 0
                },
                {
                    "sent": "Or if you know information from one source, tomorrow you build another CRF to extract information from another source and you start from scratch and at the end of the day you have 500 CRF's, none of which share any code, and none of which you know actually combine.",
                    "label": 0
                },
                {
                    "sent": "So there must be a better way.",
                    "label": 1
                },
                {
                    "sent": "There better be a better way, or, you know, we're going to be limited in what we can do with machine learning on the web.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what what, what, what would such a better way be?",
                    "label": 0
                },
                {
                    "sent": "Well, let's look at what some of the interesting characteristics of web problems are, right that actually set them apart from many other problems that people have traditionally dealt with in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Well, here's a very big one on the web.",
                    "label": 0
                },
                {
                    "sent": "Your samples are not IID.",
                    "label": 1
                },
                {
                    "sent": "Right ID means independent and identically distributed.",
                    "label": 0
                },
                {
                    "sent": "This is almost always the assumption that's made in statistical learning.",
                    "label": 0
                },
                {
                    "sent": "This means that one object doesn't tell me anything about another object, right?",
                    "label": 0
                },
                {
                    "sent": "This would be like, for example, in the old days of text classification, I predict the topic of a web page from the words on that page, and you know that's not a bad thing to do.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing is that if I use the links.",
                    "label": 0
                },
                {
                    "sent": "I get much better results because if this page is on topic A and it points to page 2, you know that page that page is probably also on topic, a right?",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of power in this and this is really what the web is all about.",
                    "label": 0
                },
                {
                    "sent": "It's the links, right?",
                    "label": 0
                },
                {
                    "sent": "But at this point the machine learning problem has suddenly gotten a lot harder because you don't have value data anymore.",
                    "label": 0
                },
                {
                    "sent": "You have a big network of connected pages with their properties affecting each other.",
                    "label": 0
                },
                {
                    "sent": "So we need methods that actually handle this.",
                    "label": 0
                },
                {
                    "sent": "The other one is that objects on the web have a lot of structure.",
                    "label": 0
                },
                {
                    "sent": "Most machine learning algorithms are designed to deal with vectors, attributes, vectors, feature vectors, right?",
                    "label": 0
                },
                {
                    "sent": "But objects on the web are things like webpages, webpages, structure.",
                    "label": 0
                },
                {
                    "sent": "They have links.",
                    "label": 0
                },
                {
                    "sent": "They have anchor text, they have.",
                    "label": 0
                },
                {
                    "sent": "You know the header.",
                    "label": 0
                },
                {
                    "sent": "And then there's things like XML that have even more structure, right?",
                    "label": 1
                },
                {
                    "sent": "You know, think of like you know product pages.",
                    "label": 0
                },
                {
                    "sent": "For example, they have a lot of structure.",
                    "label": 1
                },
                {
                    "sent": "They have visual structure.",
                    "label": 0
                },
                {
                    "sent": "They have two D structure or they have no structure at all.",
                    "label": 0
                },
                {
                    "sent": "Like a lot of the information on the web is text, right?",
                    "label": 0
                },
                {
                    "sent": "And you know, we can treat text as a bag of words, but the truth is, that's a very impoverished representation, right?",
                    "label": 0
                },
                {
                    "sent": "There's a lot more going on in text, so we need to handle that case as well.",
                    "label": 0
                },
                {
                    "sent": "And here's another one that makes life complicated, which is when you look at some of these problems, right?",
                    "label": 0
                },
                {
                    "sent": "Often.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you have in a particular application situation is that multiple of these problems are tight together.",
                    "label": 0
                },
                {
                    "sent": "You wanna you know you have a recommender system you need to manage reputations.",
                    "label": 0
                },
                {
                    "sent": "You need to deal with spam.",
                    "label": 0
                },
                {
                    "sent": "It has social network and mass collaboration aspects.",
                    "label": 0
                },
                {
                    "sent": "Maybe we're going to extract some information to actually populate some of this for people, right?",
                    "label": 0
                },
                {
                    "sent": "These problems are often, you know, there's more than one problem type together.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to be able to do is compose the solutions.",
                    "label": 0
                },
                {
                    "sent": "Another, of course salient care.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stick of the web is that there are massive amounts of data and this is good, right?",
                    "label": 1
                },
                {
                    "sent": "Because it means we can let that day to do a lot of the work.",
                    "label": 0
                },
                {
                    "sent": "For us, this is probably more true on the web than anywhere else in the notes.",
                    "label": 0
                },
                {
                    "sent": "Of course, a general exam as an has changed its HTML somehow or you know the spammers come up with new tricks and whatnot, and the problem with this rapid changes that it doesn't cut it anymore to have to spend six months developing machine learning application because you know, by six months later the spammers will have run away with it or your website that's supposed to get there.",
                    "label": 0
                },
                {
                    "sent": "That information is actually only gathering broken links, so we need to have more rapid development of machine learning albums and more active update of the models.",
                    "label": 0
                },
                {
                    "sent": "And finally, you know when I talk to people in industry, you know, like Google and Yahoo and Amazon and whatnot, I'm always struck by this is that.",
                    "label": 0
                },
                {
                    "sent": "They don't know what to do because there are so many things that you could do.",
                    "label": 0
                },
                {
                    "sent": "There's like no end of places where you could apply machine learning.",
                    "label": 0
                },
                {
                    "sent": "Knowing the ways in which you could apply them, and you know they only have 100 people doing machine learning, they would have 1000 if they could, but you know there aren't enough to go around, and so the problem is that we have to make the machine learning experts more productive.",
                    "label": 0
                },
                {
                    "sent": "Right, we need to automate things as much as possible such that you know with your number of people that you have to work on machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "They can do 10 times more.",
                    "label": 0
                },
                {
                    "sent": "They can explore 10 times more opportunities, or maybe try 100 different variations on automatically evaluate them and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we what we need is a machine learning framework that actually takes all of these things into account.",
                    "label": 0
                },
                {
                    "sent": "Now of course, unfortunately there is no such thing available today, but I think that what we need to do is work towards that, right?",
                    "label": 0
                },
                {
                    "sent": "In both academia and industry and what I'm going to describe, here is 1 possible approach to trying to do this.",
                    "label": 0
                },
                {
                    "sent": "You know again, these things are all work in progress, but I think there's a number of useful ideas that we can hope for.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pick up here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I would argue that one of the first things that we need is a language.",
                    "label": 0
                },
                {
                    "sent": "We need the language that allows us to easily define the standard types of models that we just saw, right without a lot of work without programming them from scratch in Java and C++, that provides a common framework in which to combine these algorithms do various things with them that is automatically compiled into learning and inference.",
                    "label": 1
                },
                {
                    "sent": "Code that executes sufficiently right?",
                    "label": 0
                },
                {
                    "sent": "I want to be able to define my machine learning solutions, compositions of models and what not very compactly.",
                    "label": 0
                },
                {
                    "sent": "Very easy, very elegantly, but then this has to compile into code that sufficient.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, you know it's it's not really that useful at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "And also I wanted to be easy to encode the knowledge of the practitioners, the knowledge of the problem into the solution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so would something like infer.net be relevant to what you're talking about?",
                    "label": 0
                },
                {
                    "sent": "No, absolutely.",
                    "label": 0
                },
                {
                    "sent": "So there's like I said, there's a number of different works in progress towards doing this, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it's not like you know, nobody else is notice these problems, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 1
                },
                {
                    "sent": "And also there allows models to be composed and reused, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, in software engineering and programming this is this is this is a given and unfortunately it isn't.",
                    "label": 0
                },
                {
                    "sent": "Usually you can't compose your models and you can't reuse your model, so we need the language that actually allows us to do this.",
                    "label": 0
                },
                {
                    "sent": "So my proposal for one.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such language is called Markov logic.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea, Markov logic is the following.",
                    "label": 0
                },
                {
                    "sent": "We're going to use first order logic as the language for specifying our models, so we actually not going to invent a new language.",
                    "label": 0
                },
                {
                    "sent": "What has actually been a surprise to us is that first order logic, which has been around for a long time, is actually such a really good match to the problem of specifying and manipulating and combining, and we're using machine learning models with one little addition is weights on the formulas.",
                    "label": 0
                },
                {
                    "sent": "Right 'cause we're going to have, you know, parameters numeric parameters to learn those are going to come in the form of weights on the formulas and in some sense the unified view that I'm going to try to present here is how a whole bunch of these machine learning statistical learning algorithms can all be easily cast in this form.",
                    "label": 0
                },
                {
                    "sent": "And then we can in some sense write programs that combine a logistic regression with the CRF with a mixture, model etc etc and play with that.",
                    "label": 0
                },
                {
                    "sent": "The meaning of these formulas is that the 1st order formulas are going to act as templates for constructing Markov networks.",
                    "label": 0
                },
                {
                    "sent": "And the Markov net was in some sense the most flexible, most powerful representation for probabilistic models that we have.",
                    "label": 0
                },
                {
                    "sent": "So it's a natural thing to target.",
                    "label": 0
                },
                {
                    "sent": "So this is actually what we're going to be doing is we're going to be specifying these very large models, potentially very very large, very compactly using first order logic.",
                    "label": 0
                },
                {
                    "sent": "And then of course, you know this.",
                    "label": 0
                },
                {
                    "sent": "This is only useful if we have efficient inference and learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's actually a whole bunch of algorithms that have been developed for machine for Markov logic by now.",
                    "label": 0
                },
                {
                    "sent": "The one that I'm going to talk about here is something called lifted belief propagation, which is a more efficient form of belief, belief propagation, and various learning algorithms like voted perceptrons pseudo likelihood, inductive logic programming.",
                    "label": 0
                },
                {
                    "sent": "Many of these algorithms, percent don't actually skill to websites, but if so, I'm going to talk about some generic software that has this algorithms and truth in advertising.",
                    "label": 0
                },
                {
                    "sent": "You can't run it on the billing pages yet, but in some cases you can run it on things in the millions and you know the progress is being made at the rate of a couple of orders of magnitude per year.",
                    "label": 0
                },
                {
                    "sent": "So I think we hopefully will get there.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned that these things are available in some open source software, the package that we have is called alchemy.",
                    "label": 0
                },
                {
                    "sent": "I'll put up the URL and talk a little bit about it a little bit later, and you know there have been a whole slew of applications of Markov logic to a web problems like information extraction, text mining, social networks, etc etc.",
                    "label": 0
                },
                {
                    "sent": "The one example here that I will go into into into more detail is information extraction, both because it's a very important and challenging problem and one that I think illustrates well what we can do in this framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before I describe Markov logic and the algorithms in the applications, let me just give a little bit of background.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This piece of background is Markov networks.",
                    "label": 0
                },
                {
                    "sent": "Markov networks are models of the joint distribution of a set of variables and their graphical models, so there's a note in the graph for each variable, and there's an arc and directed arc between two variables that directly depend on each other.",
                    "label": 0
                },
                {
                    "sent": "And what the graph captures is independence.",
                    "label": 0
                },
                {
                    "sent": "Conditional independence is envision networks.",
                    "label": 0
                },
                {
                    "sent": "In particular, variable is conditionally independent of the rest of the network given its neighbors.",
                    "label": 0
                },
                {
                    "sent": "So for example, smoking is conditionally dependent of cough given cancer and asthma.",
                    "label": 0
                },
                {
                    "sent": "So this is the structure of the model.",
                    "label": 0
                },
                {
                    "sent": "The parameters of the model come in the form of water called potential functions.",
                    "label": 0
                },
                {
                    "sent": "There's a potential function for every maximal clique in the graph, so here there are two.",
                    "label": 0
                },
                {
                    "sent": "There's the smoking cancer click and there's the cancer, asthma cough click and the potential function has a value.",
                    "label": 0
                },
                {
                    "sent": "For each possible state of the click and the value is any non negative real number.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's the Smoking cancer click.",
                    "label": 0
                },
                {
                    "sent": "Assuming there Boolean, there's four States and if you notice the intuitive meaning of the potential function values, is that higher values correspond to more preferred states.",
                    "label": 0
                },
                {
                    "sent": "So what this little toy example here is doing is basically saying that the state smoking true and cancer false is less likely than the others, and there isn't.",
                    "label": 0
                },
                {
                    "sent": "It's less likely that, yeah, this is that of course smoking causes cancer.",
                    "label": 0
                },
                {
                    "sent": "And then the probability of a complete state is just the product of the values of all the potential functions normalized to sum to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is very nice, but it has a problem in that it doesn't scale very well and you know scaling is going to be very important for us and the reason it doesn't scale very well is that if you notice if you notice the number of states of a potential function.",
                    "label": 0
                },
                {
                    "sent": "And therefore the number of parameters that you need to deal with is exponential in the number of nodes in the click.",
                    "label": 0
                },
                {
                    "sent": "OK, so if all you have are small clicks, then life is good.",
                    "label": 0
                },
                {
                    "sent": "But if you have a click even of size 50 or 100, you know this is like you know, now you have two to the 100 states.",
                    "label": 0
                },
                {
                    "sent": "So what can you do?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what you can do is use this alternate form.",
                    "label": 0
                },
                {
                    "sent": "Very popular with statisticians called the log linear or an exponential model.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that instead of having a product of factors, what we're going to have is an exponentiated sum of terms, right?",
                    "label": 0
                },
                {
                    "sent": "And you can always convert from one to the other, because you know the log of a product is the sum, and the terms are products of features, which is just arbitrary Boolean functions and their weights.",
                    "label": 0
                },
                {
                    "sent": "And I can always convert from potential form to log linear form by creating a feature for each state of each.",
                    "label": 0
                },
                {
                    "sent": "Which potential function of each click and letting this corresponding white be the log.",
                    "label": 0
                },
                {
                    "sent": "Of the corresponding value of the potential.",
                    "label": 0
                },
                {
                    "sent": "Now the important thing though is that I don't need to have one feature per state of a click.",
                    "label": 0
                },
                {
                    "sent": "If I have a click with a Brazilian states, but I know there's only 10 important features of that click, then I define those.",
                    "label": 0
                },
                {
                    "sent": "I assign them weights and I can actually handle a very large click.",
                    "label": 0
                },
                {
                    "sent": "You know, in a in a compact amount of space and not having to learn too many parameters.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If we define this function, you know this feature, that's one.",
                    "label": 0
                },
                {
                    "sent": "If you don't smoke or have cancer, and 0 otherwise, and give it away to 1.5, this actually realizes the same model as the is what we had on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So the other small piece of background is on 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "So infer.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So logically make logical statements about the world in the form of formulas.",
                    "label": 0
                },
                {
                    "sent": "The formulas are built up out of four kinds of symbols, constants, variables, functions and predicates.",
                    "label": 1
                },
                {
                    "sent": "Constants like Anna object.",
                    "label": 0
                },
                {
                    "sent": "They represent objects in the domain.",
                    "label": 0
                },
                {
                    "sent": "Variables range over those objects.",
                    "label": 0
                },
                {
                    "sent": "Say you know X functions that take objects and return objects like mother of X and predicates like friends, XY that represent properties of objects or relations between objects and then out of these we build formulas using.",
                    "label": 0
                },
                {
                    "sent": "The usual logical connectives like you know conjunction, disjunction, implication quantification etc.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to call a grounding of a predicate or the formula the result of replacing all the variables by constants.",
                    "label": 0
                },
                {
                    "sent": "So for example, if one of the predicates in my domain is friends and two of my constants are N and Bob one possible grounding of friends is friends Anna Bob.",
                    "label": 0
                },
                {
                    "sent": "When we call this the ground Atom and the ground Atom is just a Boolean variable.",
                    "label": 1
                },
                {
                    "sent": "It's true if and Bob are friends in this case and it's false if they're not OK and I'm going to call a world and assignment of truth values to all the ground items.",
                    "label": 0
                },
                {
                    "sent": "I is just one big Boolean vector.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to be talking about here is probability distributions over this very large Boolean vectors that represent you know everything of interest in your world.",
                    "label": 0
                },
                {
                    "sent": "Like you know the words on the page and what pages connect to what pages and what not.",
                    "label": 0
                },
                {
                    "sent": "OK so here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example that I will use as a running example from the.",
                    "label": 0
                },
                {
                    "sent": "You know from the domain of social networks, which is of course a very popular thing these days.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is, you know the results of a study on smoking and you know people quitting smoking.",
                    "label": 0
                },
                {
                    "sent": "So this is in 1970 and this is in 2000.",
                    "label": 0
                },
                {
                    "sent": "And this is a social network and you know these represent how many cigarettes people smoke.",
                    "label": 0
                },
                {
                    "sent": "And the good news is that many fewer people are smoking than before, right?",
                    "label": 0
                },
                {
                    "sent": "And that's good because you know smoking causes cancer and you want people to stop smoking etc.",
                    "label": 0
                },
                {
                    "sent": "But the other interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "People don't stop smoking randomly.",
                    "label": 0
                },
                {
                    "sent": "They stop smoking in clumps.",
                    "label": 0
                },
                {
                    "sent": "If your friends stop smoking, that makes you more likely to stop smoking as well.",
                    "label": 0
                },
                {
                    "sent": "And if you're trying to stop smoking and all your friends smoke, you know good luck.",
                    "label": 0
                },
                {
                    "sent": "It's going to be very difficult.",
                    "label": 0
                },
                {
                    "sent": "So how might we model something like this, right?",
                    "label": 0
                },
                {
                    "sent": "We're looking at a social network that people in the social network, their properties, how they interact.",
                    "label": 0
                },
                {
                    "sent": "So let's try to do that in logic, right?",
                    "label": 0
                },
                {
                    "sent": "Which of course is not going to work very well, but it's going to, you know.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this stage, for what we're going to do next, right?",
                    "label": 0
                },
                {
                    "sent": "Let's start by saying, you know, a couple of things in English that we know to be true.",
                    "label": 0
                },
                {
                    "sent": "For example, smoking causes cancer and friends have similar smoking habits.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In logic, you can write this as follows.",
                    "label": 0
                },
                {
                    "sent": "For every X smokes of X implies cancer of X and for every XY friends of XY implies smokes of X is equivalent to smokes of, why?",
                    "label": 0
                },
                {
                    "sent": "Now this is a very nice and easy exercise, but the problem is that this isn't very useful, because if you notice, these two statements were true.",
                    "label": 0
                },
                {
                    "sent": "And these two are false.",
                    "label": 0
                },
                {
                    "sent": "The first because not everybody who smokes gets cancer and not all pairs of friends have similar smoking habits.",
                    "label": 0
                },
                {
                    "sent": "So this logic by itself doesn't give us everything that we need, but you know we're going to see how to get everything that we need.",
                    "label": 0
                },
                {
                    "sent": "So this is a good way to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduce Markov logic.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Markov logic?",
                    "label": 0
                },
                {
                    "sent": "Well, the reason that previous example didn't work is that each formula was a hard constraint on the possible worlds.",
                    "label": 1
                },
                {
                    "sent": "If you violate even one formula, if there's even one person in your world who's a smoker and doesn't get cancer now, the world becomes impossible, right?",
                    "label": 0
                },
                {
                    "sent": "And so this isn't going to work right?",
                    "label": 0
                },
                {
                    "sent": "Because you know, the world is full of noise.",
                    "label": 0
                },
                {
                    "sent": "So how about instead of hard constraints, will let the formulas be soft constraints.",
                    "label": 1
                },
                {
                    "sent": "Meaning that when the world violates a formula, when a smoker doesn't get cancer, the world doesn't become impossible, it just becomes less probable and we're going to give it formula weight that represents how strong of a constraint it is.",
                    "label": 1
                },
                {
                    "sent": "So if we really believe in the formula, then we give it a high weight.",
                    "label": 0
                },
                {
                    "sent": "Then you pay big penalty for violating it and vice versa, and then the probability of a world is just the log linear model of the form that we saw before I eat.",
                    "label": 0
                },
                {
                    "sent": "That's the normalized exponentiated some of the weights of the formulas that the world satisfies.",
                    "label": 0
                },
                {
                    "sent": "And the more formulas it satisfies, the more the higher weight they are, the more probable the world becomes, and you know this is exactly the behavior that we want.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A more precise definition I'm going to call a Markov logic network or MLN for short.",
                    "label": 0
                },
                {
                    "sent": "A set of pairs FW where F is a formula in first order logic and W is a real number, positive or negative.",
                    "label": 1
                },
                {
                    "sent": "This is the syntax.",
                    "label": 0
                },
                {
                    "sent": "The semantics of this language is.",
                    "label": 0
                },
                {
                    "sent": "The following is that, together with a set of constants representing objects in the domain and MLN, defines a Markov network as follows.",
                    "label": 0
                },
                {
                    "sent": "It's going to have one node for each grounding of each predicate in the MLN, and it's going to have one feature for each grounding of each formula in the MLN with the corresponding weight.",
                    "label": 1
                },
                {
                    "sent": "So here's our friends and smokers example.",
                    "label": 0
                },
                {
                    "sent": "Right now we can assign weights to these formulas and now we actually are back to something that's useful.",
                    "label": 0
                },
                {
                    "sent": "Right, these things are back to being what they should have been all along, which is statistical regularity's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the reason this formula has a higher weight in this one is that you know this is a stronger regularity.",
                    "label": 0
                },
                {
                    "sent": "So what does this MLN mean, right?",
                    "label": 0
                },
                {
                    "sent": "What is the statistical model that I have built here?",
                    "label": 0
                },
                {
                    "sent": "As it turns out, as much as this looks like a toy example, this is actually a pretty state of the art social network model.",
                    "label": 0
                },
                {
                    "sent": "Believe it or not.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's suppose for simplicity, to fit on a slide that we have only two constants in the domain N and Bob.",
                    "label": 0
                },
                {
                    "sent": "So the rule is that I'm going to have a ground Atom for each for a variable in the Markov network for each grounding of each.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cricket with each with each constant.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to have cancer, Anna right Boolean variable.",
                    "label": 0
                },
                {
                    "sent": "True friend has cancer false if she doesn't same for smokes N and same for Bob.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the same thing for friends Anna Bob.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm going to have friends in above and friends Babanin notice that the two things are different, right?",
                    "label": 0
                },
                {
                    "sent": "Friendship is asymmetric, as sociologists will tell you.",
                    "label": 0
                },
                {
                    "sent": "And this actually happens a lot in real social networks like Bob, for example, could be a much better friend of Interventies of Bob.",
                    "label": 0
                },
                {
                    "sent": "And then there's like these degenerate cases of like friends and and and friends, Bob, Bob, that you know, maybe you have to do their self esteem or something.",
                    "label": 0
                },
                {
                    "sent": "So at this point what we have is a large number of Boolean variables, and we're going to build a distribution over these variables.",
                    "label": 0
                },
                {
                    "sent": "So how do we build that distribution?",
                    "label": 0
                },
                {
                    "sent": "Well, the rule is that we're going to have one feature for every grounding of each formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, we're going.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a feature smokes Ann implies cancer in which is going to lead to having an edge in the graph here and the same thing for Bob.",
                    "label": 0
                },
                {
                    "sent": "OK, now this formula has two predicates in it, so it leads to a two way click.",
                    "label": 0
                },
                {
                    "sent": "This formula has three, so it's going to lead to a 3 way click IE.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The triangle, so here are the various triangles, so this is the structure of the model, right?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at this point, we've completely defined the model.",
                    "label": 0
                },
                {
                    "sent": "The probability of a world right is going to be the normalized exponentiate some overall.",
                    "label": 1
                },
                {
                    "sent": "The 1st Order formulas of the weight of the Formula Times the number of true groundings of the formula in the world.",
                    "label": 1
                },
                {
                    "sent": "And notice that the MLN does not just represent the single distribution, it actually represents a whole family distributions.",
                    "label": 0
                },
                {
                    "sent": "And that's where a lot of its power lights they may learn is really a template for constructing ground Markov networks.",
                    "label": 0
                },
                {
                    "sent": "And depending on the domain I could get very different networks.",
                    "label": 0
                },
                {
                    "sent": "In particular, some could be much larger than others, but they're all repetitions of the same templates.",
                    "label": 0
                },
                {
                    "sent": "Now of course something that has already occurred to you that this all sounds very nice, but it doesn't sound remotely practical, right?",
                    "label": 0
                },
                {
                    "sent": "Because I'm getting I'm going to get a combinatorially explosion when I try to create the ground atoms and features.",
                    "label": 0
                },
                {
                    "sent": "And you know this isn't going to fit in memory, and I'm not even going to get off the ground.",
                    "label": 0
                },
                {
                    "sent": "And indeed, if we haven't solved that problem we had, we don't really have something that we can use.",
                    "label": 0
                },
                {
                    "sent": "Of course, I wouldn't be talking to you about it here if we hadn't at least two enough of a degree.",
                    "label": 1
                },
                {
                    "sent": "That's practical.",
                    "label": 0
                },
                {
                    "sent": "But before we look at some of the algorithms, one thing that you can do right off the bat that is very, very useful is to just have typed variables and constants.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you have a predicate like works for XY, it only makes sense to replace X by people.",
                    "label": 0
                },
                {
                    "sent": "And why by organisations, right?",
                    "label": 0
                },
                {
                    "sent": "Anything else is a waste of time, and they're already vastly cuts down on the number of atoms that you're going to have to deal with now in first order logic you can actually handle the full range of constructs in logic functions, existential quantifiers, infinitely continuous domains.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that here, 'cause it's not the most relevant, but certainly.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If somebody is interested, I can give you the pointers.",
                    "label": 0
                },
                {
                    "sent": "So how does this relate to the statistical models that we want to be able to build easily?",
                    "label": 0
                },
                {
                    "sent": "You know, combine reuse etc.",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing right?",
                    "label": 0
                },
                {
                    "sent": "What makes Markov logic so useful is that all of these models?",
                    "label": 1
                },
                {
                    "sent": "Are simple special cases of Markov logic, meaning I can define them very easily using just a few formulas and then a result like you know, I can also combine them just by sharing predicates, right?",
                    "label": 0
                },
                {
                    "sent": "If I have, you know, hmm, and the logistic regression that share some predicate and I have a model that is a combined HMM and logistic regression for example.",
                    "label": 0
                },
                {
                    "sent": "And the other thing, of course, is that Markov logic allows the objects to be non IID.",
                    "label": 1
                },
                {
                    "sent": "Right as we saw in the social network example, whether I smoke depends on whether or not my friends smoke.",
                    "label": 0
                },
                {
                    "sent": "OK, so in Markov logic, setting this up is not a problem.",
                    "label": 0
                },
                {
                    "sent": "How does this relate to 1st order logic?",
                    "label": 0
                },
                {
                    "sent": "Well in?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Limit of infinite weights.",
                    "label": 0
                },
                {
                    "sent": "You get first order logic back right?",
                    "label": 0
                },
                {
                    "sent": "That makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "If you paint infinite penalty then you become valuating a formula then you become impossible and in fact there's a theorem that says you can answer every entailment Queen 1st order logic by computing conditional probabilities in a Markov logic network that you get by assigning a weight of Infinity to every formula.",
                    "label": 0
                },
                {
                    "sent": "That's the infinite wait case.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "The interesting case is the finite weight case.",
                    "label": 0
                },
                {
                    "sent": "In that case, we can still say something interesting, which is that if the knowledge base is satisfiable, meaning you can make all the formulas true at the same time.",
                    "label": 0
                },
                {
                    "sent": "And although it's are positive, which you can always do by flipping, you know the signs of negating formulas with negative weights and making the weights positive.",
                    "label": 0
                },
                {
                    "sent": "Then the satisfying assignments are the modes of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Meaning that the world's that first order logic likes are still embedded in there.",
                    "label": 0
                },
                {
                    "sent": "There's the peaks of the distribution is just that probability has leaked away from them which which is which is exactly what we want.",
                    "label": 0
                },
                {
                    "sent": "You know, like the neighboring worlds, also have some probability.",
                    "label": 0
                },
                {
                    "sent": "But the most important thing is that Markov logic allows contradictions between formulas.",
                    "label": 1
                },
                {
                    "sent": "Think of the semantic web right.",
                    "label": 0
                },
                {
                    "sent": "The dream of the semantic Web is that everybody contributes knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "They write their knowledge in all, or, you know it's translated in two hours.",
                    "label": 0
                },
                {
                    "sent": "Or you know an RDF triples and what not, and then you know somehow magically you can do inference over this, right?",
                    "label": 0
                },
                {
                    "sent": "But first order logic inference isn't.",
                    "label": 0
                },
                {
                    "sent": "You know, these things are all special cases of 1st logic, but that's not going to cut it right because it's going to be full of contradictions just like the real web.",
                    "label": 0
                },
                {
                    "sent": "Like the you know, the text web is full of contradictions.",
                    "label": 0
                },
                {
                    "sent": "The semantic web is going to be full of contradictions as well.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So in logic, if you have a contradiction, either directly or indirectly between formulas by their consequences, then you can prove anything.",
                    "label": 0
                },
                {
                    "sent": "Anything follows from a contradiction.",
                    "label": 0
                },
                {
                    "sent": "In Markov logic, having contradictions between formulas is not a problem.",
                    "label": 0
                },
                {
                    "sent": "If there's something that says that A and something that is not a in Michael logic, what happens is you do the inference.",
                    "label": 0
                },
                {
                    "sent": "You weigh the evidence on both sides, and you produce the probability that is true.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's something like Markov logic.",
                    "label": 0
                },
                {
                    "sent": "I think something like the semantic web actually becomes more feasible.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's the basic language.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right now, in order to make this language useful for solving the kinds of problems that we saw before, we need efficient inference and learning algorithms and justice.",
                    "label": 0
                },
                {
                    "sent": "The language had all these models of special cases.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, these inference and learning algorithms are going to be based on the kinds of infants and learning algorithms that people have developed before.",
                    "label": 0
                },
                {
                    "sent": "But now you can see them all in a nice integrated framework, where instead of having to learn 500 things, maybe you just have to learn you know half a dozen.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at some of those algorithms.",
                    "label": 0
                },
                {
                    "sent": "So there's two main kinds of inference that you typically want to do.",
                    "label": 0
                },
                {
                    "sent": "The first one is what's called M AP for maximum a posteriori or MPE.",
                    "label": 0
                },
                {
                    "sent": "For most probable explanation inference.",
                    "label": 0
                },
                {
                    "sent": "This is inferring the most likely values of your query variables given your observed variables.",
                    "label": 0
                },
                {
                    "sent": "For example, I may want to infer the topics of web pages given their words on them, and the links between them, right?",
                    "label": 0
                },
                {
                    "sent": "This is an example of MLP inference.",
                    "label": 0
                },
                {
                    "sent": "And for this, there's a bunch of algorithms.",
                    "label": 0
                },
                {
                    "sent": "They include Max Walk set, which is just a weighted satisfiability solver.",
                    "label": 0
                },
                {
                    "sent": "Because the modes of the distribution are the states that satisfy the most formulas, we can just use satisfiability testers to solve the problem, and this is a very highly developed technology, right?",
                    "label": 0
                },
                {
                    "sent": "Notes solvers like Walksat can solve problems with millions of variables in minutes.",
                    "label": 0
                },
                {
                    "sent": "And problems that are hard but many set problems are easily can solve hard problems in minutes with things like hundreds of thousands and millions of variables.",
                    "label": 0
                },
                {
                    "sent": "So you know, this is pretty scalable.",
                    "label": 0
                },
                {
                    "sent": "Lazy said is a more efficient version of Max Walk set that deals with the problem of the combinatorial explosion and you, ideally this set is that you are not going to create all the ground atoms.",
                    "label": 0
                },
                {
                    "sent": "Because you don't need to think of friends ex.",
                    "label": 0
                },
                {
                    "sent": "Wife for example.",
                    "label": 0
                },
                {
                    "sent": "There's 6 billion people in the world, so there's 36 billion billion possible groundings of friends, right?",
                    "label": 0
                },
                {
                    "sent": "But the vast majority of them are false, right?",
                    "label": 0
                },
                {
                    "sent": "'cause most people have?",
                    "label": 0
                },
                {
                    "sent": "Maybe only a few dozen friends or a few 100, maybe a few 1000.",
                    "label": 0
                },
                {
                    "sent": "If you're on Facebook, but that's you know, that's the maximum.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can assume that atoms are false until the infant says that they're true, and we only explicitly represent the true atoms, and correspondingly most of the clauses are going to be true because their preconditions never fire, right?",
                    "label": 0
                },
                {
                    "sent": "If we're not friends, we don't need to worry about whether you're smoking habits, influence might, right, and so for most cases I had met him not going to have to drag out the closet, so this kind of his influences again, you know that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a sub by many orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you often want to do is compute marginal and conditional probabilities.",
                    "label": 1
                },
                {
                    "sent": "And in principle you could do this just by checking in each possible world whether your formula holds or not.",
                    "label": 0
                },
                {
                    "sent": "And then you sum the probabilities of those worlds and you know how to compute them from the formula, right?",
                    "label": 0
                },
                {
                    "sent": "The problem, of course is that there's an exponential number of them, right?",
                    "label": 0
                },
                {
                    "sent": "So this isn't going to work.",
                    "label": 0
                },
                {
                    "sent": "What you can do though, is you can use.",
                    "label": 0
                },
                {
                    "sent": "You know, sampling, right?",
                    "label": 0
                },
                {
                    "sent": "You can use MCMC to basically generate worlds at random, and you count the fraction of those samples in which your formula holds, and that's your estimate of the probability.",
                    "label": 0
                },
                {
                    "sent": "And you can do this using your standard.",
                    "label": 0
                },
                {
                    "sent": "You know Gibbs sampler, but unfortunately give samples really suck when you try to do things on a large scale and within within with strong dependencies.",
                    "label": 0
                },
                {
                    "sent": "So we have developed a better algorithm called MC set, which is an MCMC algorithm that uses a satisfiability solver to generate a new sample.",
                    "label": 0
                },
                {
                    "sent": "And that we can jump very quickly between different modes in the distribution, which is, which is what you need to have fast inference.",
                    "label": 0
                },
                {
                    "sent": "There's also another thing that you can do which is called knowledge based model construction, and this is don't construct the entire network.",
                    "label": 0
                },
                {
                    "sent": "Just construct the subset of the network that is needed to answer your question right again, think of the semantic web, right?",
                    "label": 0
                },
                {
                    "sent": "I have all this knowledge out there and now you're asking a question, but probably out of all the rules and facts that people contributed, there's only a very few that actually relevant to the query.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to construct the network only of those and do probably influence on that.",
                    "label": 0
                },
                {
                    "sent": "So even this even, even though the semantic web may contain bazillions of rules and facts, hopefully the probabilistic inference that you have to do it in today, which is the most expensive part, will be done on a fairly small network.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that you can do that I'm going to talk about in a little more detail is lifted belief propagation.",
                    "label": 1
                },
                {
                    "sent": "So belief propagation, as some of you may know.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a very general inference algorithm that has lots of things that special cases like you know with Serbian Univision networks.",
                    "label": 0
                },
                {
                    "sent": "You can do inference on them using with propagation etc etc.",
                    "label": 1
                },
                {
                    "sent": "So it's a nice general argument they have, but what we're going to look at here is a lifted version of the propagation and by lifted I mean that it's lifted as in logic you can do lifted inference.",
                    "label": 1
                },
                {
                    "sent": "I'm going to do inference for a whole sets of objects at once.",
                    "label": 0
                },
                {
                    "sent": "So the cost of my inference is actually going at the end of the day be independent of the number of objects that I have.",
                    "label": 0
                },
                {
                    "sent": "It is actually only depends on the number of sets of objects that behave differently from each other, and again that is potentially going to bias very big scale UPS.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Well I guess already.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Said this, so here's a brief review of belief propagation for those of you not familiar with it, the best way to think of blue propagation is in terms of what's called the factor graph.",
                    "label": 0
                },
                {
                    "sent": "Affecter graph is a bipartite graph where on the one side you have your variables, your nodes and on the other side you have your features and each node is connected to all the features that it appears in, and the way belief propagation works is that you what we're going to do is that.",
                    "label": 0
                },
                {
                    "sent": "Each node is going to maintain what is in essence and approximate an approximation of its marginal probability.",
                    "label": 0
                },
                {
                    "sent": "And it just starts out at one because, you know, we don't know anything in the beginning.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to iterate this process of passing messages from nodes to features and back until it converges and the message.",
                    "label": 0
                },
                {
                    "sent": "So the message there are no sense to feature is, you know.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, it's marginal probability what a feature computes is this thing here, which is.",
                    "label": 0
                },
                {
                    "sent": "What the feature does is it pretends that all all the all the nodes in it are independent, right?",
                    "label": 0
                },
                {
                    "sent": "And computes the probability of each state right, which is the value of the potential times the marginals sums out all the other variables except the one that is sending the message to, so the message that the features seem to the node is basically what the future thinks.",
                    "label": 0
                },
                {
                    "sent": "The margin of the node should be, and then we just do this until convergence.",
                    "label": 0
                },
                {
                    "sent": "Now the problem that we're going to have here is that we want to do this on networks with millions of nodes and billions of features.",
                    "label": 0
                },
                {
                    "sent": "OK, so just standard belief propagation isn't going to work.",
                    "label": 1
                },
                {
                    "sent": "You know, it probably won't even fit in memory.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "What we can do is no.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice that most of the time there's going to be whole sets of nodes that throughout belief propagation always send the same messages.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And same thing for features.",
                    "label": 0
                },
                {
                    "sent": "And so if I notice that I can divide these nodes into these blocks, that's in the same message.",
                    "label": 0
                },
                {
                    "sent": "Now instead of sending it, you know, look at this block on this block instead of sending a message from every node in here to every feature in here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can just send one.",
                    "label": 0
                },
                {
                    "sent": "OK. And now suddenly my belief propagation became a lot faster and the memory requirements also became a lot smaller because instead of having this many nodes, I just have as many nodes as there are rectangles here, and I'm going to call these rectangles super nodes in this rectangle, super features, and So what we're going to do is we're going to construct the lifted network that's made up of supernode and super features that each stands for, you know, maybe millions or quadrillions of nodes, right?",
                    "label": 0
                },
                {
                    "sent": "So the network that I'm doing belief propagation on is small or hopefully small in the worst case, it will still be larger.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but of course the whole question is how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Right, how do we form the listed network?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Again, I already said this.",
                    "label": 0
                },
                {
                    "sent": "You know super node is going to be a set of ground items that will send and receive the same messages throughout BP.",
                    "label": 0
                },
                {
                    "sent": "Similar for super features.",
                    "label": 0
                },
                {
                    "sent": "And then we're just going to run blue propagation on this network.",
                    "label": 0
                },
                {
                    "sent": "How do we form the lifted network?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the idea.",
                    "label": 0
                },
                {
                    "sent": "If you have no evidence, if you think about it, all groundings of a predicate will behave exactly the same way, right?",
                    "label": 0
                },
                {
                    "sent": "'cause?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, we've said the same thing about them, and all groundings of each formula would also behave the same way, right?",
                    "label": 1
                },
                {
                    "sent": "The thing that complicates life is of course evidence.",
                    "label": 0
                },
                {
                    "sent": "You know, like for example what the words on the page are right?",
                    "label": 0
                },
                {
                    "sent": "You know, without evidence you can do anything useful.",
                    "label": 0
                },
                {
                    "sent": "So the question becomes, how do I propagate the evidence through the belief propagation algorithm and figure out what are the sets of atoms that still behave the same way?",
                    "label": 0
                },
                {
                    "sent": "Well, here's how you can do that.",
                    "label": 0
                },
                {
                    "sent": "You start out by forming your initial supernotes, and if you don't have evidence about a particular predicate, then there's just one right?",
                    "label": 0
                },
                {
                    "sent": "'cause all groundings are unknown.",
                    "label": 0
                },
                {
                    "sent": "If you have evidence, then you're going to break up the predicate into three super nodes, one containing all the true groundings, right?",
                    "label": 0
                },
                {
                    "sent": "One containing all the false groundings, and one containing all the unknown groundings, right?",
                    "label": 0
                },
                {
                    "sent": "So at this point I've divided the evidence predicates into super nodes, but nothing else.",
                    "label": 0
                },
                {
                    "sent": "But now the question is, given that these predicates have been divided into super nodes, not what that was that what that is?",
                    "label": 0
                },
                {
                    "sent": "What does that cause in the features that those nodes appearing right now if you think about it, if a feature involves two predicates and each one of them can be in the true, false or unknown states right?",
                    "label": 0
                },
                {
                    "sent": "Then there are six cases for the feature, right?",
                    "label": 0
                },
                {
                    "sent": "True true true false, true Anon, etc.",
                    "label": 1
                },
                {
                    "sent": "And they may not all occur.",
                    "label": 0
                },
                {
                    "sent": "But you know, at most there are those six.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is we're going to form those super features.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, the way they formed is by doing joints of the Super nodes in the database sense of joint, right?",
                    "label": 0
                },
                {
                    "sent": "If one of my predicates ispu XY and the other predicate is in RYZ, and I have a feature that's a conjunction of those right, then what I need is to join those two on Y to get something over XY and Z. OK, so I create my super features by doing joins of the Super nodes that appear in them, and now I need to go back and say now that I've subdivided the Super features.",
                    "label": 0
                },
                {
                    "sent": "What subdivisions that does this induce in the other non evidence predicates that appeared in them, right?",
                    "label": 0
                },
                {
                    "sent": "So now if you think about it, what's going to happen is that I have to project my features back to the nodes, right?",
                    "label": 0
                },
                {
                    "sent": "If I if I have a feature over XYZ and my note is only over XYI need to project out Z and Now the key question is how many projections does each node receive two nodes that receive the same number of projections from the same features are going to receive the same messages and therefore they can be put together in the same supernode.",
                    "label": 1
                },
                {
                    "sent": "And so this is what?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to do at this point.",
                    "label": 0
                },
                {
                    "sent": "I have a new generation of supernodes, so this will in turn generate some subdivisions in some for the features and I'm going to keep on doing this until it converges.",
                    "label": 0
                },
                {
                    "sent": "And then you can prove.",
                    "label": 0
                },
                {
                    "sent": "That it converges that there's a unique minimal lifted network, and that this algorithm finds it, and then when you do inference on that network, you get the same results as if you're doing inference on the whole humongous network with bazillions of nodes.",
                    "label": 1
                },
                {
                    "sent": "So this is a version of live propagation that is just vastly more scalable than the standard belief propagation.",
                    "label": 0
                },
                {
                    "sent": "When there is this kind of repeated structure that you can exploit.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is one important aspect that I glossed over so far which is the following.",
                    "label": 0
                },
                {
                    "sent": "The lifted network that you get into that they may be very small and the belief propagation runs on that network, so hopefully it will be fast.",
                    "label": 0
                },
                {
                    "sent": "But how do I represent the supernodes and super features while I'm building the network?",
                    "label": 1
                },
                {
                    "sent": "The easy way is to just represent them as database relations and to use standard database joins and projections.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is that that itself may run out of memory, right?",
                    "label": 0
                },
                {
                    "sent": "So what can I do?",
                    "label": 0
                },
                {
                    "sent": "Well, something that I can do, taking an idea from logic is user resolution like representation, where instead of enumerating values I say for example what I allow here is all values, But these two or you know this has to be equal to way or a or equal to be right and now as long as I figure out how to do joins and projections in this more condensed language, I can still do the algorithm, but using much less memory.",
                    "label": 0
                },
                {
                    "sent": "And more generally, you can just think of this as the problem of forming clusters of atoms that behave the same.",
                    "label": 0
                },
                {
                    "sent": "And forming, you know the same thing for features, right?",
                    "label": 0
                },
                {
                    "sent": "And there's no single language that is always going to be the best one.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Indeed, finding the best language is not surprisingly, an NP complete problem, but this is something that you know could buy you a lot, and we have some preliminary results where by forming classes in more interesting ways you get much bigger scale ups.",
                    "label": 0
                },
                {
                    "sent": "And you can also form approximate clusters, which will you know you cluster together atoms.",
                    "label": 0
                },
                {
                    "sent": "That's in approximately the same messages, right?",
                    "label": 0
                },
                {
                    "sent": "And that will in turn give you a lot more scale up.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about learning.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So learning in this framework for learning your data is a relational database, right?",
                    "label": 1
                },
                {
                    "sent": "So what if you have if what you have is something else, then you need to convert it to a relational database, right?",
                    "label": 0
                },
                {
                    "sent": "But you can write.",
                    "label": 0
                },
                {
                    "sent": "You can convert text by saying you know, having a credit that says what tokens appear, etc.",
                    "label": 0
                },
                {
                    "sent": "And here in this talk I'm going to make the clothes world assumption, which is that everything in the night in your database is false.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to make that assumption, then you need em.",
                    "label": 0
                },
                {
                    "sent": "Versions of the algorithms that I'm going to describe, but you know, those are available too, and now you can learn parameters and formulas.",
                    "label": 0
                },
                {
                    "sent": "The parameters you can learn, generative Lior, discriminatively and I'm going to just very briefly.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's on each one of those.",
                    "label": 0
                },
                {
                    "sent": "So generative with learning this is.",
                    "label": 0
                },
                {
                    "sent": "This is like the standard statistical learning problem.",
                    "label": 0
                },
                {
                    "sent": "I want to find the weights that make the database as likely as possible.",
                    "label": 0
                },
                {
                    "sent": "There's no closed form solution, but it is a convex problem, so there's a single global optimum.",
                    "label": 0
                },
                {
                    "sent": "You can find it using.",
                    "label": 0
                },
                {
                    "sent": "You know gradient descent or quasi Newton standard methods.",
                    "label": 0
                },
                {
                    "sent": "The gradient has a very intuitive form.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the log likelihood with respect to a clause.",
                    "label": 0
                },
                {
                    "sent": "It is with the weight is just the difference between the number of ground true groundings of their claws in the data and the number predicted by the model.",
                    "label": 0
                },
                {
                    "sent": "So if the model predicts that it's true less often than it should be the way it needs to go up if it produces often is true.",
                    "label": 0
                },
                {
                    "sent": "More often they choose, it needs to go down, and once they all line up, you've reached the maximum likelihood point.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, this requires inference at each step, right?",
                    "label": 1
                },
                {
                    "sent": "To compute this expectations.",
                    "label": 0
                },
                {
                    "sent": "So this could be slow, something that you can do that's more approximate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But a lot faster is to use instead something called the pseudo likelihood.",
                    "label": 0
                },
                {
                    "sent": "The pseudo likelihood is just the product over all the variables of their conditional probability given the state of its neighbors in the data, and so computing it doesn't require inference.",
                    "label": 1
                },
                {
                    "sent": "And if you use it with something like LB FGS, it's actually pretty fast, and it's a consistent estimator.",
                    "label": 0
                },
                {
                    "sent": "The problem is that sometimes the parameters that you learn are not so good.",
                    "label": 0
                },
                {
                    "sent": "So what else can you do?",
                    "label": 0
                },
                {
                    "sent": "You can do discriminate.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of learning and you know this is again what you do when you do logistic regression or conditional random fields etc etc.",
                    "label": 0
                },
                {
                    "sent": "Here the idea is that you're just going to if you know in advance what variables are going to be query and what verbs are going to be evidence, you just maximize the conditional likelihood of the query variables given the evidence variables and the form of the gradient is still the same.",
                    "label": 0
                },
                {
                    "sent": "But now what I can do is approximate this expectation just by the counts in the most likely state of Y given X, right?",
                    "label": 1
                },
                {
                    "sent": "What makes these inference problems hard is that there are many many modes.",
                    "label": 0
                },
                {
                    "sent": "But when I condition on evidence, the mode start to disappear.",
                    "label": 0
                },
                {
                    "sent": "And ideally, there's one that dominates.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the limit there has to be one that dominates.",
                    "label": 0
                },
                {
                    "sent": "So instead of counting over next minimal number of states, I can just find the most likely one under the accounts there, and in fact this idea was.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Originally proposed for training HMMS discriminatively by by Michael Collins and he called it the voted Perceptron.",
                    "label": 1
                },
                {
                    "sent": "Based on some earlier ideas by.",
                    "label": 0
                },
                {
                    "sent": "Rob Shapiro and and others, and this is very much like a perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "I initialize the weights to zero and then I do this some number of times.",
                    "label": 0
                },
                {
                    "sent": "I find the most likely.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "State of Y given X in the case of hmm, you can do using the Viterbi algorithm and then I subtract the counts in that state from the accounts in the data multiplied by a learning rate after the weights.",
                    "label": 0
                },
                {
                    "sent": "And that's my update, and then I return the average of these, which is good for combating overfitting, and that's why it's called over separate.",
                    "label": 0
                },
                {
                    "sent": "What we want, of course, is to do this on an arbitrary network, not just a mark of an HMM, but we can do that by replacing the Turbo with the Max Walksat algorithm that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, you don't actually need very new algorithms to do this, you just need.",
                    "label": 0
                },
                {
                    "sent": "The combination of algorithms that were available before you.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also do structure.",
                    "label": 0
                },
                {
                    "sent": "Learning time is running a little short, so let me skip over that.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that you can actually learn the formulas or refine some formulas that you have.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By looking at data.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I mentioned, this is all available in open source software called Alchemy.",
                    "label": 0
                },
                {
                    "sent": "I will put up this URL again at the end.",
                    "label": 0
                },
                {
                    "sent": "Alchemy is really a kind of programming language that you can use to build a statistical models and combine them and reuse them.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "And you know what?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to see now is an example of what you can do with this.",
                    "label": 0
                },
                {
                    "sent": "So as already mentioned, many web applications and also non web applications have been done with Markov logic so far.",
                    "label": 0
                },
                {
                    "sent": "Let me as an example.",
                    "label": 0
                },
                {
                    "sent": "I think there's nothing like seeing a concrete example, so the example that.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Users information extraction.",
                    "label": 0
                },
                {
                    "sent": "So information extraction is the problem of taking some unstructured or semi structured data like you know web pages or in this case the citation list and extract databases from them.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the problem that things like Google Scholar and Citeseer solve is go to the reference list in papers and extracted database of publications and there's really 2 problems that you have to solve here.",
                    "label": 0
                },
                {
                    "sent": "The first one is segmentation, figuring out that, for example Tripoli, 06 is a venue and PROC single is an author.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next one isn't the resolution.",
                    "label": 0
                },
                {
                    "sent": "You need to figure out that Tripoli, 06 and the National Conference on Artificial Intelligence are the same thing.",
                    "label": 1
                },
                {
                    "sent": "Otherwise, you're going to end up with duplicates.",
                    "label": 0
                },
                {
                    "sent": "And once you've duplicated duplicated the field you want, of course.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Duplicate the records.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the state of the art for this is to use an HMM or CRF to assign each took into a field and then to use a classifier like typical logistic regression or naive Bayes to do the Inter resolution by taking each pair of observations.",
                    "label": 1
                },
                {
                    "sent": "In this case fields are citations and predicting whether the same or not.",
                    "label": 0
                },
                {
                    "sent": "And then you need to enforce transitivity because they face the same as B&B is the same as.",
                    "label": 0
                },
                {
                    "sent": "See then is the same as C, and is often done in some clustering or other heuristic way.",
                    "label": 0
                },
                {
                    "sent": "Now, if you were to build and you know people build these things by the dozen these days, you know to extract, you know all kinds of information.",
                    "label": 0
                },
                {
                    "sent": "If you were to build this, you know by hand using a language like Java or C++ today it would probably run you 10s of thousands of lines of code and you know take few weeks of programming and debugging to do.",
                    "label": 0
                },
                {
                    "sent": "In alchemy, this whole thing can be done in just seven formulas that actually fit on a single slide.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm going to put up next.",
                    "label": 0
                },
                {
                    "sent": "So in MLN is diff.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find in an MLN file using three things.",
                    "label": 0
                },
                {
                    "sent": "The first one is type declarations.",
                    "label": 0
                },
                {
                    "sent": "The second one is predicate declarations.",
                    "label": 0
                },
                {
                    "sent": "In the third one is formulas.",
                    "label": 0
                },
                {
                    "sent": "The type declarations are actually optional because you can just infer them from data, and that's what we do.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the time, but here for clarity, I'm actually going to make them explicit.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use four types here.",
                    "label": 0
                },
                {
                    "sent": "One is token that you know is the actual words that appear on pages.",
                    "label": 0
                },
                {
                    "sent": "The other one is the fields that you want to extract, and citations, and.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sessions and I'm going to have my evidence predicate.",
                    "label": 0
                },
                {
                    "sent": "This is the predicate that says that this token appears in this position in this.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dictation.",
                    "label": 0
                },
                {
                    "sent": "And my query predicates this one in field.",
                    "label": 0
                },
                {
                    "sent": "Does the segmentation it says that this position is part of this field in the citation.",
                    "label": 0
                },
                {
                    "sent": "Like for example, the first position in the citation C one is part of the author field and these two do the anti resolution right?",
                    "label": 0
                },
                {
                    "sent": "This one means that the field say author field in citations one and two are the same and this one that these two citations are the same?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the MLN, using seven formulas that's that.",
                    "label": 0
                },
                {
                    "sent": "Does the whole problem for you.",
                    "label": 0
                },
                {
                    "sent": "These first three formulas are in, HMM, that the segmentation and these four are Inter resolution system.",
                    "label": 0
                },
                {
                    "sent": "OK so how does?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This work?",
                    "label": 0
                },
                {
                    "sent": "Free variables are implicitly universally quantified, as people usually do in logic programming languages, so you know there's no point in repeating the quantifiers all the time and this notation.",
                    "label": 0
                },
                {
                    "sent": "This plus.",
                    "label": 0
                },
                {
                    "sent": "Means that we want alchemy to learn one weight for each grounding of that formula, with each constant of that type.",
                    "label": 0
                },
                {
                    "sent": "So what this formula here says is that if this token appears in this position, then this position is a part of this field.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know this what an be 'cause?",
                    "label": 0
                },
                {
                    "sent": "I'm going to have a plus on the token on the field.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to learn here is a matrix of tokens by Hills, which is very precisely the observation matrix of an HMM where the observations are tokens and the hidden states are fields.",
                    "label": 0
                },
                {
                    "sent": "And So what I'm going to learn here is the correlation between tokens and fields, like for example Smith is predictive of author conferences.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adictiva, venue, etc.",
                    "label": 0
                },
                {
                    "sent": "The next formula captures the transition matrix, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm in this field right now in the next position, I'm going to be in this field.",
                    "label": 0
                },
                {
                    "sent": "This one is a detail that I'm going to ignore here now.",
                    "label": 0
                },
                {
                    "sent": "How do we do the entire resolution?",
                    "label": 0
                },
                {
                    "sent": "Well, here's the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formula that does most of the work it.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This basically says that fields are the same if they have a lot of tokens in common.",
                    "label": 0
                },
                {
                    "sent": "OK and of course some tokens are more informative than others, so they're going to have a higher weight and I have a plus for token and field as before.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Is a logistic regression where this is the predicted variable and and these formulas you know are constructing the features.",
                    "label": 0
                },
                {
                    "sent": "Another way to look at this is that we're doing a similarity comparison between the fields.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formula says that if the fields are the same, then the citations are the same and vice versa.",
                    "label": 0
                },
                {
                    "sent": "Figuring out their Tripoli, I and the National Conference on Artificial Intelligence are the same, just based on string comparison as people traditionally do is hopeless.",
                    "label": 0
                },
                {
                    "sent": "Right, but what happens here is that if the authors are the same and the tiles are the same, and because of that, I figure out that the citations are the same, then it follows that the venues must be the same.",
                    "label": 0
                },
                {
                    "sent": "So at this point I figured out that triple AI and the national conference rooms are the same thing, and this becomes available to use elsewhere, so there's a lot of power in this, because now you can do a lot more with a lot less work on your part.",
                    "label": 0
                },
                {
                    "sent": "And these last two formulas are just.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The transitivity, so for example, this formula just says that you citation sees this embassy primacy, primacy, prime prime, then sees the semesi prime prime.",
                    "label": 0
                },
                {
                    "sent": "And again, contrast this with, for example, the paper that Macalmon Welner published here at NIPS a few years ago, where what they did was to add transitive iti to a CRF model for Inter resolution, and it was a difficult feat that they perform because they had to invent.",
                    "label": 0
                },
                {
                    "sent": "Knew you know, inference and learning algorithms to do this right very, very widely cited paper in this framework, all they have to do is add this one formula and this form is just the transitivity axiom for equality, right?",
                    "label": 0
                },
                {
                    "sent": "And then you know.",
                    "label": 0
                },
                {
                    "sent": "The inference and learning algorithms that we already have under the hood do the necessary.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For you now this thing is actually a fairly good into resolution model.",
                    "label": 0
                },
                {
                    "sent": "This one is not such a great segmentation model and the reason is that it has trouble deciding exactly where the field boundaries are and people.",
                    "label": 0
                },
                {
                    "sent": "Information extraction notice, and in fact that's what's behind the whole field of wrapper induction is you write rules that detect where field begins.",
                    "label": 0
                },
                {
                    "sent": "Now it's this you know HTML tag fold by.",
                    "label": 0
                },
                {
                    "sent": "That means that this is the start of the price of the CD.",
                    "label": 0
                },
                {
                    "sent": "But the question is how do you combine that with an HMM or CRF?",
                    "label": 0
                },
                {
                    "sent": "Well, in our case it's very simple because you can just go and refine these rules, in particular, for example in the case of citations, we know that there's a very good field boundary marker, which is a period, right?",
                    "label": 0
                },
                {
                    "sent": "Most fields end with periods, So what we can do is we introduce a new condition in this rule that says neighboring fields.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It could be the same naming positions like to be in the same field only if one of them is not a period, so the period stops the propagation.",
                    "label": 0
                },
                {
                    "sent": "And with this we suddenly get much better results, so these are results on quarter, which is a benchmark.",
                    "label": 0
                },
                {
                    "sent": "It's a database of computer science papers.",
                    "label": 0
                },
                {
                    "sent": "This is precision recall.",
                    "label": 0
                },
                {
                    "sent": "We want to be up here.",
                    "label": 0
                },
                {
                    "sent": "This is the model without the period, right?",
                    "label": 0
                },
                {
                    "sent": "And when we add the period, very simple change.",
                    "label": 0
                },
                {
                    "sent": "Suddenly we get much better and then if we add a comment we get even better and so forth.",
                    "label": 0
                },
                {
                    "sent": "So now it becomes very easy to come right.",
                    "label": 0
                },
                {
                    "sent": "We just combine here in HMM with the logistic regression with the transitivity closure model with some additions to the rules and it was all quite easy and search for it.",
                    "label": 0
                },
                {
                    "sent": "And the network that I showed you is not the state of the art solution for this problem, but we have developed one that still has only a couple dozen formulas, and it's the best.",
                    "label": 0
                },
                {
                    "sent": "Currently the best state of the art solution for things like you know the core benchmarks on the Sightseer benchmarks, and more importantly, it took a lot less time to develop than the previous solutions that.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That people had.",
                    "label": 0
                },
                {
                    "sent": "OK, so to conclude.",
                    "label": 0
                },
                {
                    "sent": "The web is amazing in terms of being a domain from machine learn.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, it provides never ending problems.",
                    "label": 0
                },
                {
                    "sent": "You know, every year new kinds of applications, machine learning come up.",
                    "label": 0
                },
                {
                    "sent": "Conversely, machine learning provides a plethora of solutions.",
                    "label": 1
                },
                {
                    "sent": "But you know this.",
                    "label": 0
                },
                {
                    "sent": "This creates a very big problem, right?",
                    "label": 0
                },
                {
                    "sent": "Which solutions that we use for which problems and how to figure out what to do on how to combine them.",
                    "label": 0
                },
                {
                    "sent": "What I would argue is that we need the unifying language to do this.",
                    "label": 0
                },
                {
                    "sent": "Markov logic is such as one possible such unified language.",
                    "label": 1
                },
                {
                    "sent": "It just uses weighted 1st order logic to define statistical models.",
                    "label": 1
                },
                {
                    "sent": "And now with this we can combine them or use them etc.",
                    "label": 0
                },
                {
                    "sent": "There are efficient inference and learning algorithms as I mentioned before.",
                    "label": 1
                },
                {
                    "sent": "If you want to do something on the web scale today using Alchemy will probably not cut it, but you can then refine you can define.",
                    "label": 0
                },
                {
                    "sent": "You can prototype your solution in alchemy and then when you want to do it on a large scale then you can do some specialized coding and this is what a number of people have done at this point and you know it works quite well.",
                    "label": 0
                },
                {
                    "sent": "So there have been many successful applications like for example the information extraction one that I showed you.",
                    "label": 0
                },
                {
                    "sent": "This is all available in open source software and you know there's a website.",
                    "label": 0
                },
                {
                    "sent": "Here's here's the URL.",
                    "label": 0
                },
                {
                    "sent": "It has the software, the documentation, tutorials, APIs, Merlins, datasets, you know everything that you need to play.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So you have a nice list of characteristics of web learning problems, right?",
                    "label": 1
                },
                {
                    "sent": "One that wasn't there or maybe was there implicitly was that most run web problems are online, right?",
                    "label": 0
                },
                {
                    "sent": "There's a constant stream Aquarius new web pages.",
                    "label": 0
                },
                {
                    "sent": "You possible results?",
                    "label": 0
                },
                {
                    "sent": "Does that fit into kind of modeling?",
                    "label": 0
                },
                {
                    "sent": "Talk about?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "In fact, a lot of the web problems are online and very much we want to address that.",
                    "label": 0
                },
                {
                    "sent": "And I didn't talk about that here, but we do have online versions of some of the alchemy algorithms, and we're developing more.",
                    "label": 0
                },
                {
                    "sent": "It's something that's not as much as some of the rest it, but definitely I think that's very important and ultimately.",
                    "label": 0
                },
                {
                    "sent": "We want everything to work online and I think we can do that.",
                    "label": 0
                },
                {
                    "sent": "How big is it and how long does it take to train?",
                    "label": 0
                },
                {
                    "sent": "For example, this simple seven former model on it and this sophisticated using formulas?",
                    "label": 0
                },
                {
                    "sent": "So the formula.",
                    "label": 0
                },
                {
                    "sent": "So the core database, the original cord that bases, I think hundreds of thousands of papers, the core database that's used as a benchmark for citation matching is small.",
                    "label": 0
                },
                {
                    "sent": "It's in the order of thousands and how long does it take to train on it?",
                    "label": 0
                },
                {
                    "sent": "It takes on the order of maybe minutes to an hour depending on algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you pseudo likelihood and the objects it takes a few minutes, but here's an important thing is that these algorithms scale linearly in the size of the data.",
                    "label": 0
                },
                {
                    "sent": "Right, so if instead of 1000, right?",
                    "label": 0
                },
                {
                    "sent": "So this is very important.",
                    "label": 0
                },
                {
                    "sent": "Most machine learning algorithms have bad scale up, but algorithms that are based on just gathering these sufficient statistics, like you know, like we're seeing here the scale up, you know you just need to run once over the data.",
                    "label": 1
                },
                {
                    "sent": "Collecting the sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "Number of examples?",
                    "label": 1
                },
                {
                    "sent": "Or do you need a number of notes here?",
                    "label": 0
                },
                {
                    "sent": "I mean the number of in this case, for example, the number of papers, right?",
                    "label": 0
                },
                {
                    "sent": "The cost of getting this fish sticks is of the order of the size of the database.",
                    "label": 0
                },
                {
                    "sent": "Then the cost of the inference might be alot alot higher, but that's all the size or the size of the MLN and not of the size of the data.",
                    "label": 0
                },
                {
                    "sent": "There's a need for language.",
                    "label": 0
                },
                {
                    "sent": "As an example, exactly.",
                    "label": 0
                },
                {
                    "sent": "Absolutely, so it's interesting 'cause I just give a talk in the probabilistic programming Workshop where this was.",
                    "label": 1
                },
                {
                    "sent": "This was my last slide was, you know, the talk was what we can do with Markov logic today, but the last slide was we can't do with Markov logic today right?",
                    "label": 0
                },
                {
                    "sent": "And the single most important thing that we can do with Markov logic today is that these algorithms are not push button yet.",
                    "label": 0
                },
                {
                    "sent": "Push button, it's easy and it's a lot of fun to write down your model in Markov logic.",
                    "label": 0
                },
                {
                    "sent": "That will take, you know, maybe all of five minutes I've literally given talks where people while I'm giving the talk, they download alchemy and they define their model in Markov logic and then come ask me questions afterwards.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "It's that easy.",
                    "label": 0
                },
                {
                    "sent": "The thing that's not always easy is that all of these algorithms have parameter settings.",
                    "label": 0
                },
                {
                    "sent": "You know they don't necessarily work with the default parameter settings when you don't get the results that you want.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily clear why you're not getting them right, and the problem is that.",
                    "label": 0
                },
                {
                    "sent": "We're inheriting the state of the art in statistical learning and probabilistic inference, and all these things right?",
                    "label": 0
                },
                {
                    "sent": "And the state of the art is not at the point where these things can be used by non experts.",
                    "label": 0
                },
                {
                    "sent": "What we really like to have is somebody who only knows about the domain.",
                    "label": 0
                },
                {
                    "sent": "You know, rights, you know writes formulas in Alchemy, tries things on their domain, but doesn't have to know how you know Elegs works or Max walk set, and we're not there yet, but I think you know with.",
                    "label": 0
                },
                {
                    "sent": "This summer you've got a great language for talking about models.",
                    "label": 0
                },
                {
                    "sent": "You've got some technology generating algorithms models.",
                    "label": 0
                },
                {
                    "sent": "One of the attractions of what you've done is it's declarative.",
                    "label": 0
                },
                {
                    "sent": "Make sure what you really want this push button, then declarative system at the end user.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean this is a declarative system.",
                    "label": 0
                },
                {
                    "sent": "At the end, use level.",
                    "label": 0
                },
                {
                    "sent": "But then what happens is that.",
                    "label": 0
                },
                {
                    "sent": "But the other models of the problems, right?",
                    "label": 0
                },
                {
                    "sent": "That promise of declarative programming, right, is that you can just say what you know and that will give you the results that you want.",
                    "label": 0
                },
                {
                    "sent": "And that is true here.",
                    "label": 0
                },
                {
                    "sent": "Up to the point that you have to fiddle with the parameters of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think you're stuck here.",
                    "label": 0
                }
            ]
        }
    }
}