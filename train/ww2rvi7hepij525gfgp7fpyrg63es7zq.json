{
    "id": "ww2rvi7hepij525gfgp7fpyrg63es7zq",
    "title": "Treeler: Open-source Structured Prediction for NLP",
    "info": {
        "author": [
            "Xavier Carreras, Universitat Polit\u00e8cnica de Catalunya"
        ],
        "published": "Nov. 11, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/wapa2011_carreras_treeler/",
    "segmentation": [
        [
            "So yeah as well he said I'm chubby carretas.",
            "I'm from the University Polytechnic Cataluna and I'm going to present just an overview of this library we are releasing which is called trailer, which kind of stands for three learning approach approximately and it's an open source library for structured prediction in natural language processing.",
            "And now before starting I would like to thank our colleagues from my stay at MIT.",
            "Michael Collins.",
            "Fantastical where this is.",
            "Where actually this all these methods behind this library originated and we did a lot of work there in the context of machine learning of parsing, and now in the context of the Pascal Harvest program, our goal is to release this code so that.",
            "These methods can be used by other people that so I'd like also to thank master students who, thanks to the Pascal Harvest program, they came to our University to spend the summer with us and work on this so they are not allowed to and pranayama just So what is?"
        ],
        [
            "Killer thriller is an open source package for for a structured prediction.",
            "I'll tell about you, know what we mean by structure prediction.",
            "Basically, it's the problem of given some input, predict and now put with this structure.",
            "So this is server library and is released, and the GPL license by gene.",
            "You and our main focus here all the time will be on natural language processing problems.",
            "So why?",
            "Well, basically because that's our own bias, but but.",
            "LP is a very important area of application of structured prediction methods.",
            "Basically be cause everything you represent of language has the form of some sort of structure, either sequence and mostly trees.",
            "So many things that you want to represent in text and language take the form of a tree structure, so another characteristic of NLP is that everything is large from just the vocabularies of words.",
            "Many words, many tiny concepts and.",
            "And also the number of possible structures you can put on a sentence.",
            "It grows very very fast and so performance is critical.",
            "And so it's not.",
            "It's not for the people who work here.",
            "It's not.",
            "Typical.",
            "Thing is that when you want to learn model for parsing something like that, it will take several hours or maybe several days.",
            "But depending on the implementation it can easily then take even several months, which is infeasible tool.",
            "So performance is is very critical in this in these domains, and the other characteristic of thing is that if you want to learn a model to predict sequences or trees, there's a high overlap.",
            "And the components that are involved in this in this in this in this model links.",
            "So you would really like to take advantage of this overlap and having some form of polymorphisms that allows you to reuse different components for models that in principle are quite different.",
            "So that's those are the main goals of the library.",
            "And so, as I said, the oranges of this really come from from from work with it at MIT cell where we were trying to.",
            "Support machine learning techniques that were working for things like classification and sequence prediction.",
            "2 problems which are larger scale related to parsing.",
            "So and now we have redesigned and and our goal is that this will be this will be able to be used in.",
            "By other people.",
            "So the libraries in C++ and and so most of the design.",
            "So what we want to achieve is polymorphism and the way we achieve that is by exploiting the template mechanism that the C++ offers.",
            "So I will give some ideas of how that works."
        ],
        [
            "But let's let me try to convince you of why you would like to use this kind of this kind of thing.",
            "So let's imagine that you want to build some application, and so I'd imagine that maybe you want to do some data mining and you want to do some textual data mining, since since you know the web is possibly the source of.",
            "You know that the why the source of information but that information is a form of Texan isn't structure.",
            "So say that you want to do some application, but for that you required actually to have to extract some sort of relations between things.",
            "So let's say that you want to extract financial relations, and so maybe you have crawler or something that keeps looking for sentences.",
            "And maybe you get so that would be an example of a positive relation which Mr. Wayne bought shares of Acme Corporation.",
            "So that's an example of for example in an acquisition relation between Mr. Wayne and this company so.",
            "So imagine you build this application.",
            "It reads text from from the web and then comes."
        ],
        [
            "New text, so the first thing you would like to do since there's a lot of type of information in the in the web, so maybe you want to classify these new tags according to whether it's it's in your target or not.",
            "Whether it's a financial text or not.",
            "So for that you know how to do it.",
            "You just take a binary classification algorithm you possibly exploit back of word representations.",
            "An if it says yes or no.",
            "So if a tax is not financial you just throw it away.",
            "And if the taxes were financed, there's probably a relation you want to find that so you know you have to find the relation so."
        ],
        [
            "The second step, maybe you just want to detect the name entities in there, so you want to have some sort of model that for any word, it tells you whether that word is part of a person of an organization, so that's already first form of structure prediction, because now for the sentence you are predicting a sequence.",
            "So we'd like to do that now.",
            "You want you know that there's a person, and there's some organization, so there may be a relation here, but you still still still need to see how they are connected, right?",
            "So you may exploit some shallow features of of the sentence, but all our intuition says that if there is an acquisition relation between this so the syntactic structure should tell us that."
        ],
        [
            "So the first, the first step that you would like to do is to parse the text and you retain some tree structure that is a syntactic structure of this sentence.",
            "So in this in this case the form of this is in the form of of dependencies relations, which basically every word points to the other words that modify their meaning, and so maybe so we would like to use some probabilistic dependency parser.",
            "You would input the sentence, it would have put that.",
            "And then given that, and given that you have two entities you would like to compute, you would like to extract what's the syntactic relation between these two entities.",
            "So you would like to extract some sort of path of the syntactic edges that link one thing to the other and be cause you know parsing is a difficult task, so maybe you would like to have some weight of the probability of that path is correct.",
            "You would like to have some sort of representation of this sort, so once you have this.",
            "Now you can.",
            "You can still need to classify it so."
        ],
        [
            "Well, you have two entities that the candidate and you can use multiclass classification.",
            "Deciding on the type of relations, but because if you have access to step three, you can use grammatical relations as features.",
            "So if your parser is good and you know how to do features out of that, you probably have.",
            "A lot more success in classifying the relation that just using shallow features which are approximations of the syntactic relation."
        ],
        [
            "So you want to you need to do for steps in this list in this application and thriller.",
            "The library provides core algorithms for learning and using classifiers, taggers and parsers.",
            "So I'd want to point that what I mean by core algorithms so trailer is not a library for building an application.",
            "It's not a library that is designed to build this pipeline and build the application for you.",
            "Instead, what is design is to give you the basic components that you will need to build one of these steps.",
            "So building then the application is an additional complexity and there are some tools in there, so our hope is that trailer is a library.",
            "Can that actually can be integrated in one of the other tools, so we'll start to see."
        ],
        [
            "The what is linear prediction or linear structure prediction in the in this three types of problems.",
            "So our goal here will be to headlight what is common about this and what is different so that that will give us some clue about."
        ],
        [
            "The design, so let's start with something that is not really structured prediction, because there's no structure involved, so classification or multiclass classification guess that everyone is familiar with that.",
            "So we have some domain X, which is the input domain and don't from the main Y, which is a set of labels.",
            "So the goal is to predict the label for a point.",
            "We can assume that the input domain is already.",
            "Every point is a collection of features.",
            "If not, their methods for building features, let's assume that.",
            "And so if we want to learn a linear classifier will go by the finding weight vector for each label WL.",
            "That's in the same dimensionality and then given a new point, will do something like.",
            "This will do this computation.",
            "So we take dinner dinner product of the input point with every weight vector and we just output the class that is that attains better score.",
            "So that's really the standard form.",
            "So basically it's a linear form where you choose among anything so.",
            "That's quite a standard and very well known our in for training these things.",
            "Perceptron SVM, Max margin you name it.",
            "So there they all train with the estimate await vector for this sort of classifier.",
            "Uh."
        ],
        [
            "It starts next task structure prediction for sequence tagging.",
            "So here now this is structured task where the sentence was the input X in the examples.",
            "Maybe a sentence and the output now is a sequence.",
            "So that's part of the sequence you want to predict that sequence.",
            "So if you don't know but structure prediction, let me just give you a brief overview so you don't know about that.",
            "What you may so the first approach we may think of is well, well, here.",
            "Basically, I have to predict.",
            "Yes, I have to play sequence, but I just need to predict one label for each word on top of each word.",
            "So I may just go by training what we call local classifiers.",
            "That is basically multiclass classifier that for every word it predicts.",
            "What is the best label.",
            "So so that's what I want to focus is this has the same for us before, but the feature function here.",
            "Now it takes the position and some label.",
            "I will compute some representation.",
            "Of features of of this.",
            "Of putting this label here.",
            "OK so it will for for for this decision it will represent this context, and so there are many methods to do that.",
            "Now if you want to output the full sequence, you trivially just concatenate the best prediction at this point.",
            "And that's fine.",
            "That's that's a sequence prediction model that actually in certain cases it works quite well.",
            "And how it works?",
            "It will depend on the presentation on what you can put in this feature, so you know if if the if this label can be predicted just by looking at features of the input.",
            "That's going to be fine, but for some tasks you'll see that you start building more and more features, and at some point you will not improve and you will figure out that.",
            "Well, this is too limited in context 'cause I can only predict features for one level at the time, and maybe it would be very easy here if.",
            "If, when it will be very convenient if when I'm predicting person here, I also know that actually the previous thing is a person.",
            "So if you have access to the the two labels.",
            "That would be kind of a very good feature to be able to say that, yeah, Jack London corresponds thing so.",
            "So this this feature function is too limited.",
            "You can think of a second approach where you have global classifier now, so you just."
        ],
        [
            "Instead of predicting just one tag, you just so your classes now are full task sequences.",
            "You just predict all at once.",
            "So so you're wise are full task sequences and so you're going to have a representation of an entire X with an entire why?",
            "So here there are no restrictions on features.",
            "You can extract everything you want, and that's very powerful.",
            "But importantly, you if you do that, you're not going to be able to solve the argmax why?",
            "Because there are as many, so you have to compute the score for every sequence, and there are as many sequences.",
            "Grows exponentially so it's too expensive.",
            "It's infeasible to do that unless we are."
        ],
        [
            "Doing something so the third approach, which is the proper structure prediction, is to assume some factorization of the sequences.",
            "So the easiest and perhaps more standard way of doing that is is saying well instead of justice, computing features of 1 label or all the labels at the time, I'm just going to compute features of every two labels or by grams of tags.",
            "That's why I'm in here.",
            "So now your feature function is going to take some position while I and it's going to look at.",
            "Two labels at the time, so that's all that will allow you to, you know.",
            "Device dependent dependencies that exist in your output structure, right?",
            "So so you.",
            "So here you have that you have an extended locality of features by extending the scope of engrams so well, this can be changed and it turns out that these computations we can do them we can do them using well known.",
            "Now we're in suggest in this case the Viterbi algorithm.",
            "So that's something that can work 'cause it's very expensive and we can do computation.",
            "So we can actually train these things with methods such as.",
            "Conditional random fields or other versions for doing that.",
            "So now let's"
        ],
        [
            "Look at another task, parsing, so again you have the input.",
            "Now is a sentence and the output is this tree.",
            "And well, so this is this form of parsing is dependency parsing is the the three that you're predicting is built with these dependencies, which are arcs that connect somewhat with the modifier.",
            "So this means that so having this arc here means that share is a modifier of bike.",
            "And also Wayne is modifier about and Mr is modified, or when it kind of it affects their meaning."
        ],
        [
            "So it turns out that you can port the same techniques that we use for tagging, where we take a sequence and we break it into small parts and we compute features there so you can do the same the same thing for parsing.",
            "Now that you're going to break your instead of breaking a sequence, now you have to break a tree, so that's the natural way to break this kind of tree just to look at art individually.",
            "So that's what's called our factorization, so that's going to be the form of your predictor you are looking for.",
            "43 these are trees, but then you're looking at at the dependencies.",
            "So instead of looking at 1 three at a time, you're just going to look at one dependency at the time.",
            "So dependency here is qualified by two indices.",
            "And so you're going to extract features of that dependency assigned to dispositions in this sentence, and you can evaluate that so you can compute that that there are certain features.",
            "So it turns out that this simple factorization you can actually encode very rich features in here, and it turns out that this is actually very successful model, and so it's good in terms of expression an.",
            "It's also good in terms of computation 'cause they are destructible inference, acceptable way to compute this so this.",
            "With variance of the CKY algorithm not going to give details about that, but it's successful so."
        ],
        [
            "What we just saw is a number of linear prediction tasks.",
            "They are linearly 'cause we have a weight vector and we're always doing some score of every candidate that we need to output is given by some linear computation between a vector and some feature representation.",
            "But what is different in every application is that for classification, is you directly to the inner product with the representation of the output, but for other things in order to this to be.",
            "Computational tractable, you need to consider factorizations.",
            "So these factorizations take really very different forms because they're really task dependent, but so it would like to exploit is the what is common in this task and what is different.",
            "And do a design that allows us to do this in a polymorphic way.",
            "So the way will go is by saying by having this notion of factorizations.",
            "So we'll say that in general for any structure prediction task we have that the structures that compose into parts.",
            "And so we're going to build.",
            "We're going to have.",
            "This is a structure.",
            "This is a part of the structure, and we're going.",
            "We're going to have a representation of features that looks at one part at a time, and then we're going to have this type of story.",
            "And so that.",
            "Note that because we're summing over linear scores, the full thing is also a linear model.",
            "So everything is linear here.",
            "OK, so that's linear factor structure prediction."
        ],
        [
            "So yeah, so this is what?",
            "So this is the basic framework.",
            "We're going to assume in an input domain X, and I put them in Y that we assume that these structures.",
            "So from now we're going to try to be abstract.",
            "We're going to.",
            "We're going to try to commit as little as possible to the actual structures with, so that will give us ideas for doing a design for a library.",
            "So one commitment is that we need to do some factorization of the structures in.",
            "Why so we're going to consider for any task where we need to map X to Y, we're going to have to supply some sort of factorization that allows us to the compose structures Y, and we also need to supply their representation.",
            "So we're going to model is going to mean a choice of factorization.",
            "Anna Model choice of features, then that will define our linear prediction model and so then there are two problems that.",
            "For anymore we need to build, we need to be able to to solve also.",
            "So the first is inference.",
            "How do we solve this so that it's something that it's going to be dependent on the factorization, so ahead of time we cannot say anything about that, so you need to supply the factorization so that inference distracts also that you will need to supply depending on your choice of this, you're going to need to supply.",
            "How do you do this?",
            "The other thing is learning.",
            "So how do I obtain W from data set?",
            "So the good thing is that so you know if it's classification, we know that these methods, the nice part about this, is that these methods actually can be ported to a generic type factorization.",
            "So without committing to any particular form of X&Y, we can have, say, in SVM training algorithm that trains that up with machine as long as certain things of our factorization are met, which are basically computational things.",
            "OK, so that's.",
            "That's going to be the main the main ideas behind the design of Warframe."
        ],
        [
            "So now I'm going to show you how we work in this genetic terms.",
            "I'm going to try not to give too many details here, but I do want to give some.",
            "So."
        ],
        [
            "Of the obstructions, so it's going to be of the form of pseudo language for for that.",
            "After that you can try to implement that using concept of templating.",
            "So we're going to assume so.",
            "The goal here is to learn a classifier that predicts wise for X, so we're going to assume this that we have this to say generic types X&Y and so the first thing we need to provide is some type R, which is a factorization.",
            "We call this R. And so this this this, this factorization needs to provide at least three things.",
            "The first thing is a basic type for for representing a part.",
            "And then it is supplied to functions, the most important being this one that given two pairs of XY, it enumerates the set of parts.",
            "It actually kind of unrolls the part that are there OK. Also have another function that sometimes it's necessary that for a given X we want to list all the possible parts that can be assigned there.",
            "OK, so if we want to."
        ],
        [
            "Be grounded on somewhere, so here's to type of factorizations.",
            "We can consider one for tagging, therefore parsing.",
            "So let's say that we want to do tagging with bigram factorization.",
            "So then with the background position, how do we set this?",
            "These are components while you first need to define how do you represent the part.",
            "So part is a position together with the bigram of tax, so it's represented by an integer, which is a position.",
            "And to tax, and basically that part means that position I you have the bigram assigned AB OK. And so the path functions.",
            "Once you define that, it's typically very easy to animate the parts of a solution.",
            "So we have this thing.",
            "Basically, for every possible position here will have a bigram that when we just copy so from multiple airport from pair to pair, this one from Puerto, nothing, nothing to nothing, and nothing to location OK, you just enumerate what's in there, and here it would donate anything that you can assign anywhere, so it's just everything.",
            "So let's see how parts look like in passing, so parsing you have trees and we say that a good factorization is just to look at the arts.",
            "So how do you represent an arc?",
            "In part three?",
            "We just need two indices.",
            "So this we call H sense for hat MSN for modifier.",
            "So we call these dependencies so that the two words that are in the dependency we call them the head and the modifier is just imply the role of that.",
            "So you just need to positions.",
            "We are in the form of interger and so if you want to represent so you just want to list the parts that are in this tree, you just go by that.",
            "So this represents a special route.",
            "So the root points 2, three and three points to two points to one 3.2, two points to one and three point 1.",
            "So here also you will have for every word in a certain Y in a certain three will have a part for every word in this case, but in general you also want to be able to control all the possible parts that are assignable somewhere.",
            "So now we have a mechanism where we can have an abstraction of how to enumerate part of the solution."
        ],
        [
            "So the first, the first thing we can do is try to score them, so we're going to again think of this abstract component where now, so here I'm using the notation of templates, so I'm defining this component, and by this I mean that this component, the actual form of that, depends on X&R, which I assume they exist.",
            "OK, so this means that for every value I put here, and these values are types for every type value input.",
            "Here I will have a different component, so that's that's a way of thinking about polymorphism.",
            "Plus, so let's think abstractly, that we want the scores for parts.",
            "So then we have this component that you know, given an X and some path, it gives the score for that part.",
            "OK, so if we have such a such a component, we can define this toy generic algorithm which scores password solution.",
            "So we can define this.",
            "Our hymn score.",
            "Now you supply something on input and output and some scores of all the parts for that.",
            "And so we will just enumerate the parts in the solution and summit.",
            "OK, so that if we supply everything we can write this component once and for anything that is well defined it will.",
            "It will work OK, so that's the type of things we want to reach for complex algorithms."
        ],
        [
            "So as course come from some from scores come from somewhere and they come from doing this inner product with between wait San features.",
            "So let's look at what our features.",
            "So again features we cannot define them ahead of time.",
            "We can define what we need from the feature so so a feature feature extraction component will in general depend on X&R OK on the inputs and apart.",
            "And if we provide a feature vector representation, so you have this, we have some type, which is how you know the feature vector itself and some function that actually returns given a certain X Anna some part it returns the feature representation.",
            "So then, on top of that we can define so we can we can redefine the scoring component to be a score that is actually implemented with parameter vector and some features.",
            "So now this will produce this course for parts of X, but using some feature component.",
            "OK, and so this is just an implementation of the component we saw before in the sense that it provides scorings for parts.",
            "But this is Cordings will be computed with feature with features and weights and so this particular thing will also define a type which is a type of the W, right?",
            "So once you instantiate one of these times you will have a W and a feature vector and you can do the inner product right?",
            "So So what is what is attractive about this thing?",
            "Is that the actual form of this?",
            "Can be tailored to the types so I can.",
            "I can for example I can have representations of the W that are sparse or dense in the way they represent the weight vector, and more generally I can have polymorphic inner product.",
            "So in practice what happens is that we have some some basic components, so we have a different implementations of inner products and then depending on the type of.",
            "Scoring that we're doing on what?",
            "What things are we scoring?",
            "We're going to select ones or the others and that will be the form of that will be determined by the structure."
        ],
        [
            "Let's look at the third component.",
            "For.",
            "For doing structure prediction.",
            "It's the inference component.",
            "It again it depends on X.",
            "It also depends on Y&R, so the inference it will be the component that will allow us to map.",
            "Things factor structures into non factor structures right?",
            "So the computations we need to do always assume some sort of scoring of parts, so let's let's assume that S is a scoring, and so maybe you were going to be interested in this hour in Mac, so that returns the maximum structure for some input, so it computes this thing.",
            "OK, so as you score so, it gives these things and it computes this thing.",
            "So the form of this will really depend on the on the actual.",
            "Problem we're solving with mapping.",
            "If we're doing some sort of sequence tagging or parsing or what, what type of parsing?",
            "What our part?",
            "So this will be.",
            "The implementation of this will be hidden under this, so when we do structure prediction and we want to train certain types of models, it turns out that there are other components that learning algorithms.",
            "Also you also need so in general, inference will be any algorithm that deals with.",
            "Maximizing or summing over all a space of why so?",
            "These are spaces, exponential incised.",
            "So what is necessary is that this algorithm gives us a polynomial algorithm for computing these quantities.",
            "Officially so, as I said, the actual implementation of these things really depends on so some extent on expert really depends on the relation of how we how do you have you have decomposed wise into ours."
        ],
        [
            "Alright, so let's now look at what is a learner.",
            "So learning is something that just you know you provide all these components and it has a dysfunction that you supply a training set and some parameters of the actual learner.",
            "It lends a weight vector.",
            "Right, so so we have several implementations of learning algorithms, but in any case, the learner.",
            "Of the certain problem you are trying to learn, so it will use the following components.",
            "You will use features.",
            "It will use a scoring mechanism based on features and will use inference, so these components.",
            "Are used by the learner, but the form of it is defined by these things.",
            "OK, so the learner is going to work with this irrespective of what is the form of these things so available methods that we have our perception much margin look linear."
        ],
        [
            "Let me just give you some similar code for perception, so I just want to highlight that everything here is is generic with respect to the structures.",
            "So the first thing is to say so perception will train a parameter vector so that what is the form of that parameter vector.",
            "So so the first thing is that.",
            "So we can.",
            "We can define a type which can be implicitly defined, which is the type of the scoring which that depends on.",
            "XR and the features you are trying to extract so that by setting values here you obtain what is the type of your.",
            "Of your weight vector of your scoring mechanism and that scoring mechanism should define a weight vector.",
            "So you can you can work with this abstraction and so perceptron goes by initializing things to zero and then keeps visiting examples, so that's easy.",
            "Everything is easy to do in a general way.",
            "So given an example what it's going to do is create a scoring.",
            "This type so that will score parts for X using W is the current parameter vector.",
            "Then will call inference or will compute the Max solution that will give us a solution and then we'll do the perception update.",
            "So everything here is using this generic form of the component."
        ],
        [
            "So here's how.",
            "So that's that's the abstraction that's the concept of a learner right then.",
            "Then on top on below this concept we actually implement algorithms such as perception.",
            "So we have perceptor, and while this is a concept, is an actual implementation of something.",
            "So here's perceptron.",
            "So different audience will have different requirements on things, so Perceptron requires to be able to do Max in the inference.",
            "So we also maybe you know about big asses in a very fast way of training SVM's in the primal so that the grass is always requires Max from from inference we doing the work at MIT.",
            "We worked a lot on optimisers based on exponentiated gradient.",
            "So I worked in the dual and that's with that.",
            "We can train linear models and maximizing model so so that there's also type of learners that we can use.",
            "Now this learners will use Max but also some.",
            "So if you want to actually use this thing where your model you need to provide the sum function in addition to the Max.",
            "So I'm focusing here on the relation between models and inference.",
            "But if you just are interested in learning and you are interested in in.",
            "In implementing them and using extraction so you'll see that actually look at these two algorithms.",
            "Actually they share a lot, they belong, they use the same optimiser.",
            "What changes between this and this is the type of objective you are doing, but the way you optimize that objective is the same.",
            "So you can also develop behind whatever implementation inside here.",
            "You can also start doing abstractions and generalizations in order to obtain.",
            "Compact and readable code, so these things are not actually NOT implementations are just instantiations of an actual implementation of something that that takes the objective as a as a parameter.",
            "So you can incite everything.",
            "There's always some structure that you can use in order to enforce software reuse."
        ],
        [
            "So that's for learning.",
            "And those are the type of models that we have now in the library.",
            "So we have models for classification here.",
            "It's just a trivial case for for structure prediction.",
            "So eggs are feature vectors.",
            "Why's are labels?",
            "The factorization is a trivial one so.",
            "A label component decomposes into labels.",
            "There's no factorization, and so inferences one versus all.",
            "So maybe you want to do some classification that is a pairwise classification so that you have a prediction for every two labels.",
            "So you could also implement this.",
            "Actually we don't have it, but we believe that the the abstraction we've built is good also for developing those type of structure prediction inside classification.",
            "So typically here you can imagine this in classification.",
            "Typically numerate L is not too big, so you can enumerate it.",
            "So you can do Max and somebody explicitly and going over all of them, you don't need to do any implicit.",
            "Think so.",
            "Most interesting is taking and passing for tagging, we use bigram decompositions.",
            "For that we use battery for the Max and we use forward backward to some overall sequences.",
            "You can extend instead of decomposing into bigrams, you can compose into trigrams, and so then there exists generalizations of battery that can do so.",
            "For tagging, we can actually work with different.",
            "We can think of different types of decomposing solutions, so same thing for parsing.",
            "For passing, also I'd like to point out that there are different ways that people define possible parse trees.",
            "So independency parsing we have the set of projective trees and the set of non projective trees so non productive trees is a is a product is included in non productive trees.",
            "I'm not going to find out why the subtlety behind there, but even if you use the same factorization into works so the type of the class of possible trees.",
            "Has an implication of the type of inference you have to do right?",
            "So the the inference here is based on dynamic programming and based on CKY the inference here has quite a different nature, more graph based.",
            "So there are different things that so depending on what we do, we're going to have to supply our needs which are on different form as much as we know them, of course.",
            "So the same thing you can do here for tagging where you extend from bigrams, trigrams.",
            "You can do it.",
            "For projective trees you can, instead of factoring for one arc, you can factor for several arcs at the time.",
            "That gives you bigger features, and sometimes it gives you much better performance and it's easy to extend this thing so.",
            "So basically our library provides all these abstract mechanisms and we define all these components and actually implement them so that you can use them.",
            "So that's the abstraction in order to have this and make A and have a real parser.",
            "You also need to supply feature functions and you method for reading and writing data.",
            "And all these things.",
            "So all those things are also there.",
            "We try to make it as flexible and usable and standard as possible.",
            "I also have some.",
            "The software itself is a library.",
            "We also provide script for actually developing things, but we also provide certain models, so that's basically AM."
        ],
        [
            "The library let me present some experiments so that you can see that."
        ],
        [
            "This thing really runs.",
            "So these are the type of experiments we typically one parsing nothing in the side of actually doing applications, but I think it's interesting to see what's the possible combinations we can do.",
            "So the first thing is like, OK, let's take one model for dependency parsing and let's try different learners because it's a matter of selecting each of them.",
            "So we take as a model.",
            "We take dependency parsing, mapping sentences to projective trees with decomposing two arcs, and we use this inference algorithms, and we're going to compare.",
            "Perception and log linear models, CRF's and media models SVM's.",
            "So the data we use is the standard data used for parsing the Penn treebank, which is a collection of sentences with syntactic trees.",
            "So we can this plots number of epochs, number of times you have looked over the training set and this is validation accuracy.",
            "So percent.",
            "Well actually you can see the curiosity.",
            "So perception is the blue curve perspective.",
            "It turns out that in NLP turns out to be an hour in.",
            "That really works well, especially the average perceptron.",
            "So it works well.",
            "He goes.",
            "It achieves us as many as good results on the other ones that are simple to implement.",
            "And actually it only takes three or four books to to run.",
            "But that's just the gravity of this data set anything.",
            "So perhaps in other problems you really find benefits in training as VM's or or or cigarettes, or depending on your application you may like to actually train a CRF, which is a probabilistic classifier.",
            "You can do lots of things with that.",
            "So that's one type of graphic employee."
        ],
        [
            "Another thing you can do is I'm going to just fix the learner, but I'm going to I'm going to try different factorizations so so the problem is the same.",
            "I only change here the factorization, so here I consider another factorization and here consider something that stands for two hours.",
            "That really can boost the results, so that can buy you a couple of points which is difficult to achieve so that Kite likes that it's very.",
            "So by changing the factorization we can get an improvement, so we really it's been researching the last years people trying to come up with different factorizations that exploit higher things, and so it's when you're doing this type of things that you say, well I cannot re implement everything every time, I cannot just cut and paste the code and do the changes.",
            "I need some structure in my library so that I can do these things in a clean way.",
            "So inside the parsing algorithms you see that I have.",
            "These things 1 two.",
            "That means that actually the actual implementation behind this can also be parameterized by certain things that you know of your factorization.",
            "So, so we're providing.",
            "Also, we're trying to provide this under development.",
            "Allegations of this via via notions of States and notions of of semirings where actually this in this is actually the same implementation, just changing little things."
        ],
        [
            "So that's the final experiment.",
            "Does this data set from the kernel 1007 shader task?",
            "With this, all these 10 or 11 three ones about dependency parsing on different languages, so so in blue, is the is the results obtained by thriller what we have obtained right now, and so the other performance is the best performance for each language of any system.",
            "So we're comparing against the best for each.",
            "Thing is, it's not the best system.",
            "It's the best system at it for its language.",
            "So we're doing pretty good so far.",
            "So the idea is to have to release the library where you actually can can obtain results that are in the state of the art and you can transpile use it and reproduce it and change it.",
            "And all these things.",
            "So I think that's about it."
        ],
        [
            "Just as a summary, it's an open source library for structured prediction.",
            "Here's the website where we are doing the development.",
            "You can download it from there.",
            "You can also download some models that are already trained for you.",
            "And so the focus is on tagging and parsing problems in natural language processing.",
            "We think that we've worked with this abstraction between modeling and learning, and I think that that hopefully can be good for extending the library.",
            "So there's two extensions you can do.",
            "You could think of a new model you want.",
            "I have this new structure prediction problem, so as long as you implement the framework, which we think you'll have to do it somehow because structure prediction requires features and inference.",
            "So if you do it with us then maybe you can easily block all the platter of learners that we have, right?",
            "So that's that's good for research in NLP or instruction prediction that wants to focus on representations so that you may be like a machine learning researcher doing research on structured prediction that doesn't know the details of NLP, right?",
            "So how can you show that your algorithm works on this very large scale and interesting problems so you can build a new learning learner?",
            "And if your learner requires the same interface, which is likely.",
            "So then you'll be able to apply your learning into all these tasks by just instantiating them, so you don't.",
            "You can.",
            "You can do parsing without knowing how a parser works, as long as your algorithm only requires the computations were giving, so that's that's what we would like.",
            "And so in order to achieve this with you, 70 plus plus templates, which we think is a good mechanism for Poly polymorphisms that doesn't compromise the efficiency.",
            "So that's about it.",
            "Thank you.",
            "Yes, is everything.",
            "Is there an API or anything people do know?",
            "We'd like to would like to so, but that we're very, very early stage, so the main focus now has been on setting up all these interfaces.",
            "So this is documented, but we'd like to have.",
            "We know that people so.",
            "Not a language where productivity is super high.",
            "It's quite slow to do that, so we'd like to possibly have interfaces.",
            "To Python, Matlab, whatever people is.",
            "So we're open to talking to people that can provide those those buildings.",
            "This relates to the feeling right?",
            "So yeah, it relates to feeling so if you don't know Freiling is a is a suite of NLP tools that we have in our group.",
            "You can, you can check it feeling and so it relates in the sense that I tried to hide like in the first slides where I said through there is not a system for doing pipelines for doing applications.",
            "It's a system for building components, so this will going to bring.",
            "Statistical components into failing components that you can train and things like that.",
            "So once we once we see all look this parser.",
            "Worked very well for these languages, so will package it as a thrilling model and then you can integrate that model with all the frieling components that actually builds a full NLP pipeline so that that is more useful is useful for building those applications to some extent.",
            "House documentation doing that so the obstructions we have then clear and then the actual quote we are working on that.",
            "But yeah, we have a working release.",
            "Thank you Yep."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah as well he said I'm chubby carretas.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University Polytechnic Cataluna and I'm going to present just an overview of this library we are releasing which is called trailer, which kind of stands for three learning approach approximately and it's an open source library for structured prediction in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "And now before starting I would like to thank our colleagues from my stay at MIT.",
                    "label": 0
                },
                {
                    "sent": "Michael Collins.",
                    "label": 0
                },
                {
                    "sent": "Fantastical where this is.",
                    "label": 0
                },
                {
                    "sent": "Where actually this all these methods behind this library originated and we did a lot of work there in the context of machine learning of parsing, and now in the context of the Pascal Harvest program, our goal is to release this code so that.",
                    "label": 0
                },
                {
                    "sent": "These methods can be used by other people that so I'd like also to thank master students who, thanks to the Pascal Harvest program, they came to our University to spend the summer with us and work on this so they are not allowed to and pranayama just So what is?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Killer thriller is an open source package for for a structured prediction.",
                    "label": 1
                },
                {
                    "sent": "I'll tell about you, know what we mean by structure prediction.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's the problem of given some input, predict and now put with this structure.",
                    "label": 0
                },
                {
                    "sent": "So this is server library and is released, and the GPL license by gene.",
                    "label": 0
                },
                {
                    "sent": "You and our main focus here all the time will be on natural language processing problems.",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Well, basically because that's our own bias, but but.",
                    "label": 0
                },
                {
                    "sent": "LP is a very important area of application of structured prediction methods.",
                    "label": 0
                },
                {
                    "sent": "Basically be cause everything you represent of language has the form of some sort of structure, either sequence and mostly trees.",
                    "label": 0
                },
                {
                    "sent": "So many things that you want to represent in text and language take the form of a tree structure, so another characteristic of NLP is that everything is large from just the vocabularies of words.",
                    "label": 0
                },
                {
                    "sent": "Many words, many tiny concepts and.",
                    "label": 0
                },
                {
                    "sent": "And also the number of possible structures you can put on a sentence.",
                    "label": 0
                },
                {
                    "sent": "It grows very very fast and so performance is critical.",
                    "label": 1
                },
                {
                    "sent": "And so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not for the people who work here.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Typical.",
                    "label": 0
                },
                {
                    "sent": "Thing is that when you want to learn model for parsing something like that, it will take several hours or maybe several days.",
                    "label": 0
                },
                {
                    "sent": "But depending on the implementation it can easily then take even several months, which is infeasible tool.",
                    "label": 0
                },
                {
                    "sent": "So performance is is very critical in this in these domains, and the other characteristic of thing is that if you want to learn a model to predict sequences or trees, there's a high overlap.",
                    "label": 0
                },
                {
                    "sent": "And the components that are involved in this in this in this in this model links.",
                    "label": 0
                },
                {
                    "sent": "So you would really like to take advantage of this overlap and having some form of polymorphisms that allows you to reuse different components for models that in principle are quite different.",
                    "label": 0
                },
                {
                    "sent": "So that's those are the main goals of the library.",
                    "label": 0
                },
                {
                    "sent": "And so, as I said, the oranges of this really come from from from work with it at MIT cell where we were trying to.",
                    "label": 0
                },
                {
                    "sent": "Support machine learning techniques that were working for things like classification and sequence prediction.",
                    "label": 0
                },
                {
                    "sent": "2 problems which are larger scale related to parsing.",
                    "label": 0
                },
                {
                    "sent": "So and now we have redesigned and and our goal is that this will be this will be able to be used in.",
                    "label": 0
                },
                {
                    "sent": "By other people.",
                    "label": 0
                },
                {
                    "sent": "So the libraries in C++ and and so most of the design.",
                    "label": 0
                },
                {
                    "sent": "So what we want to achieve is polymorphism and the way we achieve that is by exploiting the template mechanism that the C++ offers.",
                    "label": 0
                },
                {
                    "sent": "So I will give some ideas of how that works.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let's let me try to convince you of why you would like to use this kind of this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine that you want to build some application, and so I'd imagine that maybe you want to do some data mining and you want to do some textual data mining, since since you know the web is possibly the source of.",
                    "label": 0
                },
                {
                    "sent": "You know that the why the source of information but that information is a form of Texan isn't structure.",
                    "label": 0
                },
                {
                    "sent": "So say that you want to do some application, but for that you required actually to have to extract some sort of relations between things.",
                    "label": 0
                },
                {
                    "sent": "So let's say that you want to extract financial relations, and so maybe you have crawler or something that keeps looking for sentences.",
                    "label": 0
                },
                {
                    "sent": "And maybe you get so that would be an example of a positive relation which Mr. Wayne bought shares of Acme Corporation.",
                    "label": 1
                },
                {
                    "sent": "So that's an example of for example in an acquisition relation between Mr. Wayne and this company so.",
                    "label": 0
                },
                {
                    "sent": "So imagine you build this application.",
                    "label": 0
                },
                {
                    "sent": "It reads text from from the web and then comes.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "New text, so the first thing you would like to do since there's a lot of type of information in the in the web, so maybe you want to classify these new tags according to whether it's it's in your target or not.",
                    "label": 1
                },
                {
                    "sent": "Whether it's a financial text or not.",
                    "label": 0
                },
                {
                    "sent": "So for that you know how to do it.",
                    "label": 0
                },
                {
                    "sent": "You just take a binary classification algorithm you possibly exploit back of word representations.",
                    "label": 0
                },
                {
                    "sent": "An if it says yes or no.",
                    "label": 0
                },
                {
                    "sent": "So if a tax is not financial you just throw it away.",
                    "label": 0
                },
                {
                    "sent": "And if the taxes were financed, there's probably a relation you want to find that so you know you have to find the relation so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second step, maybe you just want to detect the name entities in there, so you want to have some sort of model that for any word, it tells you whether that word is part of a person of an organization, so that's already first form of structure prediction, because now for the sentence you are predicting a sequence.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to do that now.",
                    "label": 0
                },
                {
                    "sent": "You want you know that there's a person, and there's some organization, so there may be a relation here, but you still still still need to see how they are connected, right?",
                    "label": 0
                },
                {
                    "sent": "So you may exploit some shallow features of of the sentence, but all our intuition says that if there is an acquisition relation between this so the syntactic structure should tell us that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first, the first step that you would like to do is to parse the text and you retain some tree structure that is a syntactic structure of this sentence.",
                    "label": 0
                },
                {
                    "sent": "So in this in this case the form of this is in the form of of dependencies relations, which basically every word points to the other words that modify their meaning, and so maybe so we would like to use some probabilistic dependency parser.",
                    "label": 0
                },
                {
                    "sent": "You would input the sentence, it would have put that.",
                    "label": 0
                },
                {
                    "sent": "And then given that, and given that you have two entities you would like to compute, you would like to extract what's the syntactic relation between these two entities.",
                    "label": 0
                },
                {
                    "sent": "So you would like to extract some sort of path of the syntactic edges that link one thing to the other and be cause you know parsing is a difficult task, so maybe you would like to have some weight of the probability of that path is correct.",
                    "label": 0
                },
                {
                    "sent": "You would like to have some sort of representation of this sort, so once you have this.",
                    "label": 0
                },
                {
                    "sent": "Now you can.",
                    "label": 0
                },
                {
                    "sent": "You can still need to classify it so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, you have two entities that the candidate and you can use multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "Deciding on the type of relations, but because if you have access to step three, you can use grammatical relations as features.",
                    "label": 1
                },
                {
                    "sent": "So if your parser is good and you know how to do features out of that, you probably have.",
                    "label": 0
                },
                {
                    "sent": "A lot more success in classifying the relation that just using shallow features which are approximations of the syntactic relation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you want to you need to do for steps in this list in this application and thriller.",
                    "label": 0
                },
                {
                    "sent": "The library provides core algorithms for learning and using classifiers, taggers and parsers.",
                    "label": 1
                },
                {
                    "sent": "So I'd want to point that what I mean by core algorithms so trailer is not a library for building an application.",
                    "label": 0
                },
                {
                    "sent": "It's not a library that is designed to build this pipeline and build the application for you.",
                    "label": 0
                },
                {
                    "sent": "Instead, what is design is to give you the basic components that you will need to build one of these steps.",
                    "label": 0
                },
                {
                    "sent": "So building then the application is an additional complexity and there are some tools in there, so our hope is that trailer is a library.",
                    "label": 0
                },
                {
                    "sent": "Can that actually can be integrated in one of the other tools, so we'll start to see.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The what is linear prediction or linear structure prediction in the in this three types of problems.",
                    "label": 0
                },
                {
                    "sent": "So our goal here will be to headlight what is common about this and what is different so that that will give us some clue about.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The design, so let's start with something that is not really structured prediction, because there's no structure involved, so classification or multiclass classification guess that everyone is familiar with that.",
                    "label": 1
                },
                {
                    "sent": "So we have some domain X, which is the input domain and don't from the main Y, which is a set of labels.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to predict the label for a point.",
                    "label": 0
                },
                {
                    "sent": "We can assume that the input domain is already.",
                    "label": 0
                },
                {
                    "sent": "Every point is a collection of features.",
                    "label": 0
                },
                {
                    "sent": "If not, their methods for building features, let's assume that.",
                    "label": 0
                },
                {
                    "sent": "And so if we want to learn a linear classifier will go by the finding weight vector for each label WL.",
                    "label": 0
                },
                {
                    "sent": "That's in the same dimensionality and then given a new point, will do something like.",
                    "label": 0
                },
                {
                    "sent": "This will do this computation.",
                    "label": 0
                },
                {
                    "sent": "So we take dinner dinner product of the input point with every weight vector and we just output the class that is that attains better score.",
                    "label": 0
                },
                {
                    "sent": "So that's really the standard form.",
                    "label": 0
                },
                {
                    "sent": "So basically it's a linear form where you choose among anything so.",
                    "label": 1
                },
                {
                    "sent": "That's quite a standard and very well known our in for training these things.",
                    "label": 0
                },
                {
                    "sent": "Perceptron SVM, Max margin you name it.",
                    "label": 0
                },
                {
                    "sent": "So there they all train with the estimate await vector for this sort of classifier.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It starts next task structure prediction for sequence tagging.",
                    "label": 1
                },
                {
                    "sent": "So here now this is structured task where the sentence was the input X in the examples.",
                    "label": 0
                },
                {
                    "sent": "Maybe a sentence and the output now is a sequence.",
                    "label": 0
                },
                {
                    "sent": "So that's part of the sequence you want to predict that sequence.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know but structure prediction, let me just give you a brief overview so you don't know about that.",
                    "label": 0
                },
                {
                    "sent": "What you may so the first approach we may think of is well, well, here.",
                    "label": 0
                },
                {
                    "sent": "Basically, I have to predict.",
                    "label": 1
                },
                {
                    "sent": "Yes, I have to play sequence, but I just need to predict one label for each word on top of each word.",
                    "label": 1
                },
                {
                    "sent": "So I may just go by training what we call local classifiers.",
                    "label": 0
                },
                {
                    "sent": "That is basically multiclass classifier that for every word it predicts.",
                    "label": 0
                },
                {
                    "sent": "What is the best label.",
                    "label": 0
                },
                {
                    "sent": "So so that's what I want to focus is this has the same for us before, but the feature function here.",
                    "label": 0
                },
                {
                    "sent": "Now it takes the position and some label.",
                    "label": 0
                },
                {
                    "sent": "I will compute some representation.",
                    "label": 0
                },
                {
                    "sent": "Of features of of this.",
                    "label": 0
                },
                {
                    "sent": "Of putting this label here.",
                    "label": 0
                },
                {
                    "sent": "OK so it will for for for this decision it will represent this context, and so there are many methods to do that.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to output the full sequence, you trivially just concatenate the best prediction at this point.",
                    "label": 0
                },
                {
                    "sent": "And that's fine.",
                    "label": 0
                },
                {
                    "sent": "That's that's a sequence prediction model that actually in certain cases it works quite well.",
                    "label": 0
                },
                {
                    "sent": "And how it works?",
                    "label": 0
                },
                {
                    "sent": "It will depend on the presentation on what you can put in this feature, so you know if if the if this label can be predicted just by looking at features of the input.",
                    "label": 0
                },
                {
                    "sent": "That's going to be fine, but for some tasks you'll see that you start building more and more features, and at some point you will not improve and you will figure out that.",
                    "label": 0
                },
                {
                    "sent": "Well, this is too limited in context 'cause I can only predict features for one level at the time, and maybe it would be very easy here if.",
                    "label": 0
                },
                {
                    "sent": "If, when it will be very convenient if when I'm predicting person here, I also know that actually the previous thing is a person.",
                    "label": 0
                },
                {
                    "sent": "So if you have access to the the two labels.",
                    "label": 0
                },
                {
                    "sent": "That would be kind of a very good feature to be able to say that, yeah, Jack London corresponds thing so.",
                    "label": 0
                },
                {
                    "sent": "So this this feature function is too limited.",
                    "label": 0
                },
                {
                    "sent": "You can think of a second approach where you have global classifier now, so you just.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of predicting just one tag, you just so your classes now are full task sequences.",
                    "label": 0
                },
                {
                    "sent": "You just predict all at once.",
                    "label": 0
                },
                {
                    "sent": "So so you're wise are full task sequences and so you're going to have a representation of an entire X with an entire why?",
                    "label": 0
                },
                {
                    "sent": "So here there are no restrictions on features.",
                    "label": 0
                },
                {
                    "sent": "You can extract everything you want, and that's very powerful.",
                    "label": 0
                },
                {
                    "sent": "But importantly, you if you do that, you're not going to be able to solve the argmax why?",
                    "label": 0
                },
                {
                    "sent": "Because there are as many, so you have to compute the score for every sequence, and there are as many sequences.",
                    "label": 0
                },
                {
                    "sent": "Grows exponentially so it's too expensive.",
                    "label": 0
                },
                {
                    "sent": "It's infeasible to do that unless we are.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing something so the third approach, which is the proper structure prediction, is to assume some factorization of the sequences.",
                    "label": 0
                },
                {
                    "sent": "So the easiest and perhaps more standard way of doing that is is saying well instead of justice, computing features of 1 label or all the labels at the time, I'm just going to compute features of every two labels or by grams of tags.",
                    "label": 0
                },
                {
                    "sent": "That's why I'm in here.",
                    "label": 0
                },
                {
                    "sent": "So now your feature function is going to take some position while I and it's going to look at.",
                    "label": 0
                },
                {
                    "sent": "Two labels at the time, so that's all that will allow you to, you know.",
                    "label": 0
                },
                {
                    "sent": "Device dependent dependencies that exist in your output structure, right?",
                    "label": 0
                },
                {
                    "sent": "So so you.",
                    "label": 0
                },
                {
                    "sent": "So here you have that you have an extended locality of features by extending the scope of engrams so well, this can be changed and it turns out that these computations we can do them we can do them using well known.",
                    "label": 1
                },
                {
                    "sent": "Now we're in suggest in this case the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "So that's something that can work 'cause it's very expensive and we can do computation.",
                    "label": 0
                },
                {
                    "sent": "So we can actually train these things with methods such as.",
                    "label": 0
                },
                {
                    "sent": "Conditional random fields or other versions for doing that.",
                    "label": 0
                },
                {
                    "sent": "So now let's",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at another task, parsing, so again you have the input.",
                    "label": 0
                },
                {
                    "sent": "Now is a sentence and the output is this tree.",
                    "label": 0
                },
                {
                    "sent": "And well, so this is this form of parsing is dependency parsing is the the three that you're predicting is built with these dependencies, which are arcs that connect somewhat with the modifier.",
                    "label": 0
                },
                {
                    "sent": "So this means that so having this arc here means that share is a modifier of bike.",
                    "label": 0
                },
                {
                    "sent": "And also Wayne is modifier about and Mr is modified, or when it kind of it affects their meaning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out that you can port the same techniques that we use for tagging, where we take a sequence and we break it into small parts and we compute features there so you can do the same the same thing for parsing.",
                    "label": 0
                },
                {
                    "sent": "Now that you're going to break your instead of breaking a sequence, now you have to break a tree, so that's the natural way to break this kind of tree just to look at art individually.",
                    "label": 0
                },
                {
                    "sent": "So that's what's called our factorization, so that's going to be the form of your predictor you are looking for.",
                    "label": 0
                },
                {
                    "sent": "43 these are trees, but then you're looking at at the dependencies.",
                    "label": 0
                },
                {
                    "sent": "So instead of looking at 1 three at a time, you're just going to look at one dependency at the time.",
                    "label": 0
                },
                {
                    "sent": "So dependency here is qualified by two indices.",
                    "label": 0
                },
                {
                    "sent": "And so you're going to extract features of that dependency assigned to dispositions in this sentence, and you can evaluate that so you can compute that that there are certain features.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this simple factorization you can actually encode very rich features in here, and it turns out that this is actually very successful model, and so it's good in terms of expression an.",
                    "label": 0
                },
                {
                    "sent": "It's also good in terms of computation 'cause they are destructible inference, acceptable way to compute this so this.",
                    "label": 0
                },
                {
                    "sent": "With variance of the CKY algorithm not going to give details about that, but it's successful so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we just saw is a number of linear prediction tasks.",
                    "label": 0
                },
                {
                    "sent": "They are linearly 'cause we have a weight vector and we're always doing some score of every candidate that we need to output is given by some linear computation between a vector and some feature representation.",
                    "label": 0
                },
                {
                    "sent": "But what is different in every application is that for classification, is you directly to the inner product with the representation of the output, but for other things in order to this to be.",
                    "label": 0
                },
                {
                    "sent": "Computational tractable, you need to consider factorizations.",
                    "label": 0
                },
                {
                    "sent": "So these factorizations take really very different forms because they're really task dependent, but so it would like to exploit is the what is common in this task and what is different.",
                    "label": 0
                },
                {
                    "sent": "And do a design that allows us to do this in a polymorphic way.",
                    "label": 0
                },
                {
                    "sent": "So the way will go is by saying by having this notion of factorizations.",
                    "label": 0
                },
                {
                    "sent": "So we'll say that in general for any structure prediction task we have that the structures that compose into parts.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to build.",
                    "label": 0
                },
                {
                    "sent": "We're going to have.",
                    "label": 0
                },
                {
                    "sent": "This is a structure.",
                    "label": 0
                },
                {
                    "sent": "This is a part of the structure, and we're going.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a representation of features that looks at one part at a time, and then we're going to have this type of story.",
                    "label": 0
                },
                {
                    "sent": "And so that.",
                    "label": 0
                },
                {
                    "sent": "Note that because we're summing over linear scores, the full thing is also a linear model.",
                    "label": 0
                },
                {
                    "sent": "So everything is linear here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's linear factor structure prediction.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, so this is what?",
                    "label": 0
                },
                {
                    "sent": "So this is the basic framework.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume in an input domain X, and I put them in Y that we assume that these structures.",
                    "label": 1
                },
                {
                    "sent": "So from now we're going to try to be abstract.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to commit as little as possible to the actual structures with, so that will give us ideas for doing a design for a library.",
                    "label": 0
                },
                {
                    "sent": "So one commitment is that we need to do some factorization of the structures in.",
                    "label": 0
                },
                {
                    "sent": "Why so we're going to consider for any task where we need to map X to Y, we're going to have to supply some sort of factorization that allows us to the compose structures Y, and we also need to supply their representation.",
                    "label": 0
                },
                {
                    "sent": "So we're going to model is going to mean a choice of factorization.",
                    "label": 1
                },
                {
                    "sent": "Anna Model choice of features, then that will define our linear prediction model and so then there are two problems that.",
                    "label": 0
                },
                {
                    "sent": "For anymore we need to build, we need to be able to to solve also.",
                    "label": 0
                },
                {
                    "sent": "So the first is inference.",
                    "label": 0
                },
                {
                    "sent": "How do we solve this so that it's something that it's going to be dependent on the factorization, so ahead of time we cannot say anything about that, so you need to supply the factorization so that inference distracts also that you will need to supply depending on your choice of this, you're going to need to supply.",
                    "label": 0
                },
                {
                    "sent": "How do you do this?",
                    "label": 0
                },
                {
                    "sent": "The other thing is learning.",
                    "label": 0
                },
                {
                    "sent": "So how do I obtain W from data set?",
                    "label": 0
                },
                {
                    "sent": "So the good thing is that so you know if it's classification, we know that these methods, the nice part about this, is that these methods actually can be ported to a generic type factorization.",
                    "label": 0
                },
                {
                    "sent": "So without committing to any particular form of X&Y, we can have, say, in SVM training algorithm that trains that up with machine as long as certain things of our factorization are met, which are basically computational things.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the main the main ideas behind the design of Warframe.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to show you how we work in this genetic terms.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try not to give too many details here, but I do want to give some.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the obstructions, so it's going to be of the form of pseudo language for for that.",
                    "label": 0
                },
                {
                    "sent": "After that you can try to implement that using concept of templating.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume so.",
                    "label": 0
                },
                {
                    "sent": "The goal here is to learn a classifier that predicts wise for X, so we're going to assume this that we have this to say generic types X&Y and so the first thing we need to provide is some type R, which is a factorization.",
                    "label": 0
                },
                {
                    "sent": "We call this R. And so this this this, this factorization needs to provide at least three things.",
                    "label": 0
                },
                {
                    "sent": "The first thing is a basic type for for representing a part.",
                    "label": 1
                },
                {
                    "sent": "And then it is supplied to functions, the most important being this one that given two pairs of XY, it enumerates the set of parts.",
                    "label": 1
                },
                {
                    "sent": "It actually kind of unrolls the part that are there OK. Also have another function that sometimes it's necessary that for a given X we want to list all the possible parts that can be assigned there.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we want to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be grounded on somewhere, so here's to type of factorizations.",
                    "label": 0
                },
                {
                    "sent": "We can consider one for tagging, therefore parsing.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we want to do tagging with bigram factorization.",
                    "label": 0
                },
                {
                    "sent": "So then with the background position, how do we set this?",
                    "label": 0
                },
                {
                    "sent": "These are components while you first need to define how do you represent the part.",
                    "label": 0
                },
                {
                    "sent": "So part is a position together with the bigram of tax, so it's represented by an integer, which is a position.",
                    "label": 0
                },
                {
                    "sent": "And to tax, and basically that part means that position I you have the bigram assigned AB OK. And so the path functions.",
                    "label": 0
                },
                {
                    "sent": "Once you define that, it's typically very easy to animate the parts of a solution.",
                    "label": 0
                },
                {
                    "sent": "So we have this thing.",
                    "label": 0
                },
                {
                    "sent": "Basically, for every possible position here will have a bigram that when we just copy so from multiple airport from pair to pair, this one from Puerto, nothing, nothing to nothing, and nothing to location OK, you just enumerate what's in there, and here it would donate anything that you can assign anywhere, so it's just everything.",
                    "label": 0
                },
                {
                    "sent": "So let's see how parts look like in passing, so parsing you have trees and we say that a good factorization is just to look at the arts.",
                    "label": 0
                },
                {
                    "sent": "So how do you represent an arc?",
                    "label": 0
                },
                {
                    "sent": "In part three?",
                    "label": 0
                },
                {
                    "sent": "We just need two indices.",
                    "label": 0
                },
                {
                    "sent": "So this we call H sense for hat MSN for modifier.",
                    "label": 0
                },
                {
                    "sent": "So we call these dependencies so that the two words that are in the dependency we call them the head and the modifier is just imply the role of that.",
                    "label": 0
                },
                {
                    "sent": "So you just need to positions.",
                    "label": 0
                },
                {
                    "sent": "We are in the form of interger and so if you want to represent so you just want to list the parts that are in this tree, you just go by that.",
                    "label": 0
                },
                {
                    "sent": "So this represents a special route.",
                    "label": 0
                },
                {
                    "sent": "So the root points 2, three and three points to two points to one 3.2, two points to one and three point 1.",
                    "label": 0
                },
                {
                    "sent": "So here also you will have for every word in a certain Y in a certain three will have a part for every word in this case, but in general you also want to be able to control all the possible parts that are assignable somewhere.",
                    "label": 0
                },
                {
                    "sent": "So now we have a mechanism where we can have an abstraction of how to enumerate part of the solution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first, the first thing we can do is try to score them, so we're going to again think of this abstract component where now, so here I'm using the notation of templates, so I'm defining this component, and by this I mean that this component, the actual form of that, depends on X&R, which I assume they exist.",
                    "label": 0
                },
                {
                    "sent": "OK, so this means that for every value I put here, and these values are types for every type value input.",
                    "label": 0
                },
                {
                    "sent": "Here I will have a different component, so that's that's a way of thinking about polymorphism.",
                    "label": 0
                },
                {
                    "sent": "Plus, so let's think abstractly, that we want the scores for parts.",
                    "label": 1
                },
                {
                    "sent": "So then we have this component that you know, given an X and some path, it gives the score for that part.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we have such a such a component, we can define this toy generic algorithm which scores password solution.",
                    "label": 1
                },
                {
                    "sent": "So we can define this.",
                    "label": 0
                },
                {
                    "sent": "Our hymn score.",
                    "label": 0
                },
                {
                    "sent": "Now you supply something on input and output and some scores of all the parts for that.",
                    "label": 0
                },
                {
                    "sent": "And so we will just enumerate the parts in the solution and summit.",
                    "label": 0
                },
                {
                    "sent": "OK, so that if we supply everything we can write this component once and for anything that is well defined it will.",
                    "label": 0
                },
                {
                    "sent": "It will work OK, so that's the type of things we want to reach for complex algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as course come from some from scores come from somewhere and they come from doing this inner product with between wait San features.",
                    "label": 0
                },
                {
                    "sent": "So let's look at what our features.",
                    "label": 0
                },
                {
                    "sent": "So again features we cannot define them ahead of time.",
                    "label": 0
                },
                {
                    "sent": "We can define what we need from the feature so so a feature feature extraction component will in general depend on X&R OK on the inputs and apart.",
                    "label": 0
                },
                {
                    "sent": "And if we provide a feature vector representation, so you have this, we have some type, which is how you know the feature vector itself and some function that actually returns given a certain X Anna some part it returns the feature representation.",
                    "label": 0
                },
                {
                    "sent": "So then, on top of that we can define so we can we can redefine the scoring component to be a score that is actually implemented with parameter vector and some features.",
                    "label": 0
                },
                {
                    "sent": "So now this will produce this course for parts of X, but using some feature component.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is just an implementation of the component we saw before in the sense that it provides scorings for parts.",
                    "label": 0
                },
                {
                    "sent": "But this is Cordings will be computed with feature with features and weights and so this particular thing will also define a type which is a type of the W, right?",
                    "label": 0
                },
                {
                    "sent": "So once you instantiate one of these times you will have a W and a feature vector and you can do the inner product right?",
                    "label": 0
                },
                {
                    "sent": "So So what is what is attractive about this thing?",
                    "label": 0
                },
                {
                    "sent": "Is that the actual form of this?",
                    "label": 0
                },
                {
                    "sent": "Can be tailored to the types so I can.",
                    "label": 1
                },
                {
                    "sent": "I can for example I can have representations of the W that are sparse or dense in the way they represent the weight vector, and more generally I can have polymorphic inner product.",
                    "label": 1
                },
                {
                    "sent": "So in practice what happens is that we have some some basic components, so we have a different implementations of inner products and then depending on the type of.",
                    "label": 0
                },
                {
                    "sent": "Scoring that we're doing on what?",
                    "label": 0
                },
                {
                    "sent": "What things are we scoring?",
                    "label": 0
                },
                {
                    "sent": "We're going to select ones or the others and that will be the form of that will be determined by the structure.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at the third component.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For doing structure prediction.",
                    "label": 0
                },
                {
                    "sent": "It's the inference component.",
                    "label": 0
                },
                {
                    "sent": "It again it depends on X.",
                    "label": 0
                },
                {
                    "sent": "It also depends on Y&R, so the inference it will be the component that will allow us to map.",
                    "label": 0
                },
                {
                    "sent": "Things factor structures into non factor structures right?",
                    "label": 0
                },
                {
                    "sent": "So the computations we need to do always assume some sort of scoring of parts, so let's let's assume that S is a scoring, and so maybe you were going to be interested in this hour in Mac, so that returns the maximum structure for some input, so it computes this thing.",
                    "label": 1
                },
                {
                    "sent": "OK, so as you score so, it gives these things and it computes this thing.",
                    "label": 1
                },
                {
                    "sent": "So the form of this will really depend on the on the actual.",
                    "label": 0
                },
                {
                    "sent": "Problem we're solving with mapping.",
                    "label": 0
                },
                {
                    "sent": "If we're doing some sort of sequence tagging or parsing or what, what type of parsing?",
                    "label": 0
                },
                {
                    "sent": "What our part?",
                    "label": 0
                },
                {
                    "sent": "So this will be.",
                    "label": 0
                },
                {
                    "sent": "The implementation of this will be hidden under this, so when we do structure prediction and we want to train certain types of models, it turns out that there are other components that learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Also you also need so in general, inference will be any algorithm that deals with.",
                    "label": 0
                },
                {
                    "sent": "Maximizing or summing over all a space of why so?",
                    "label": 0
                },
                {
                    "sent": "These are spaces, exponential incised.",
                    "label": 0
                },
                {
                    "sent": "So what is necessary is that this algorithm gives us a polynomial algorithm for computing these quantities.",
                    "label": 0
                },
                {
                    "sent": "Officially so, as I said, the actual implementation of these things really depends on so some extent on expert really depends on the relation of how we how do you have you have decomposed wise into ours.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's now look at what is a learner.",
                    "label": 0
                },
                {
                    "sent": "So learning is something that just you know you provide all these components and it has a dysfunction that you supply a training set and some parameters of the actual learner.",
                    "label": 0
                },
                {
                    "sent": "It lends a weight vector.",
                    "label": 1
                },
                {
                    "sent": "Right, so so we have several implementations of learning algorithms, but in any case, the learner.",
                    "label": 0
                },
                {
                    "sent": "Of the certain problem you are trying to learn, so it will use the following components.",
                    "label": 1
                },
                {
                    "sent": "You will use features.",
                    "label": 0
                },
                {
                    "sent": "It will use a scoring mechanism based on features and will use inference, so these components.",
                    "label": 0
                },
                {
                    "sent": "Are used by the learner, but the form of it is defined by these things.",
                    "label": 0
                },
                {
                    "sent": "OK, so the learner is going to work with this irrespective of what is the form of these things so available methods that we have our perception much margin look linear.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me just give you some similar code for perception, so I just want to highlight that everything here is is generic with respect to the structures.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is to say so perception will train a parameter vector so that what is the form of that parameter vector.",
                    "label": 0
                },
                {
                    "sent": "So so the first thing is that.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "We can define a type which can be implicitly defined, which is the type of the scoring which that depends on.",
                    "label": 0
                },
                {
                    "sent": "XR and the features you are trying to extract so that by setting values here you obtain what is the type of your.",
                    "label": 0
                },
                {
                    "sent": "Of your weight vector of your scoring mechanism and that scoring mechanism should define a weight vector.",
                    "label": 0
                },
                {
                    "sent": "So you can you can work with this abstraction and so perceptron goes by initializing things to zero and then keeps visiting examples, so that's easy.",
                    "label": 0
                },
                {
                    "sent": "Everything is easy to do in a general way.",
                    "label": 0
                },
                {
                    "sent": "So given an example what it's going to do is create a scoring.",
                    "label": 0
                },
                {
                    "sent": "This type so that will score parts for X using W is the current parameter vector.",
                    "label": 1
                },
                {
                    "sent": "Then will call inference or will compute the Max solution that will give us a solution and then we'll do the perception update.",
                    "label": 0
                },
                {
                    "sent": "So everything here is using this generic form of the component.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the abstraction that's the concept of a learner right then.",
                    "label": 0
                },
                {
                    "sent": "Then on top on below this concept we actually implement algorithms such as perception.",
                    "label": 0
                },
                {
                    "sent": "So we have perceptor, and while this is a concept, is an actual implementation of something.",
                    "label": 0
                },
                {
                    "sent": "So here's perceptron.",
                    "label": 0
                },
                {
                    "sent": "So different audience will have different requirements on things, so Perceptron requires to be able to do Max in the inference.",
                    "label": 0
                },
                {
                    "sent": "So we also maybe you know about big asses in a very fast way of training SVM's in the primal so that the grass is always requires Max from from inference we doing the work at MIT.",
                    "label": 0
                },
                {
                    "sent": "We worked a lot on optimisers based on exponentiated gradient.",
                    "label": 0
                },
                {
                    "sent": "So I worked in the dual and that's with that.",
                    "label": 0
                },
                {
                    "sent": "We can train linear models and maximizing model so so that there's also type of learners that we can use.",
                    "label": 0
                },
                {
                    "sent": "Now this learners will use Max but also some.",
                    "label": 0
                },
                {
                    "sent": "So if you want to actually use this thing where your model you need to provide the sum function in addition to the Max.",
                    "label": 0
                },
                {
                    "sent": "So I'm focusing here on the relation between models and inference.",
                    "label": 0
                },
                {
                    "sent": "But if you just are interested in learning and you are interested in in.",
                    "label": 0
                },
                {
                    "sent": "In implementing them and using extraction so you'll see that actually look at these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "Actually they share a lot, they belong, they use the same optimiser.",
                    "label": 0
                },
                {
                    "sent": "What changes between this and this is the type of objective you are doing, but the way you optimize that objective is the same.",
                    "label": 0
                },
                {
                    "sent": "So you can also develop behind whatever implementation inside here.",
                    "label": 0
                },
                {
                    "sent": "You can also start doing abstractions and generalizations in order to obtain.",
                    "label": 0
                },
                {
                    "sent": "Compact and readable code, so these things are not actually NOT implementations are just instantiations of an actual implementation of something that that takes the objective as a as a parameter.",
                    "label": 0
                },
                {
                    "sent": "So you can incite everything.",
                    "label": 0
                },
                {
                    "sent": "There's always some structure that you can use in order to enforce software reuse.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's for learning.",
                    "label": 0
                },
                {
                    "sent": "And those are the type of models that we have now in the library.",
                    "label": 0
                },
                {
                    "sent": "So we have models for classification here.",
                    "label": 0
                },
                {
                    "sent": "It's just a trivial case for for structure prediction.",
                    "label": 0
                },
                {
                    "sent": "So eggs are feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Why's are labels?",
                    "label": 0
                },
                {
                    "sent": "The factorization is a trivial one so.",
                    "label": 0
                },
                {
                    "sent": "A label component decomposes into labels.",
                    "label": 0
                },
                {
                    "sent": "There's no factorization, and so inferences one versus all.",
                    "label": 0
                },
                {
                    "sent": "So maybe you want to do some classification that is a pairwise classification so that you have a prediction for every two labels.",
                    "label": 0
                },
                {
                    "sent": "So you could also implement this.",
                    "label": 0
                },
                {
                    "sent": "Actually we don't have it, but we believe that the the abstraction we've built is good also for developing those type of structure prediction inside classification.",
                    "label": 0
                },
                {
                    "sent": "So typically here you can imagine this in classification.",
                    "label": 0
                },
                {
                    "sent": "Typically numerate L is not too big, so you can enumerate it.",
                    "label": 0
                },
                {
                    "sent": "So you can do Max and somebody explicitly and going over all of them, you don't need to do any implicit.",
                    "label": 0
                },
                {
                    "sent": "Think so.",
                    "label": 0
                },
                {
                    "sent": "Most interesting is taking and passing for tagging, we use bigram decompositions.",
                    "label": 0
                },
                {
                    "sent": "For that we use battery for the Max and we use forward backward to some overall sequences.",
                    "label": 0
                },
                {
                    "sent": "You can extend instead of decomposing into bigrams, you can compose into trigrams, and so then there exists generalizations of battery that can do so.",
                    "label": 0
                },
                {
                    "sent": "For tagging, we can actually work with different.",
                    "label": 0
                },
                {
                    "sent": "We can think of different types of decomposing solutions, so same thing for parsing.",
                    "label": 0
                },
                {
                    "sent": "For passing, also I'd like to point out that there are different ways that people define possible parse trees.",
                    "label": 0
                },
                {
                    "sent": "So independency parsing we have the set of projective trees and the set of non projective trees so non productive trees is a is a product is included in non productive trees.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to find out why the subtlety behind there, but even if you use the same factorization into works so the type of the class of possible trees.",
                    "label": 0
                },
                {
                    "sent": "Has an implication of the type of inference you have to do right?",
                    "label": 0
                },
                {
                    "sent": "So the the inference here is based on dynamic programming and based on CKY the inference here has quite a different nature, more graph based.",
                    "label": 0
                },
                {
                    "sent": "So there are different things that so depending on what we do, we're going to have to supply our needs which are on different form as much as we know them, of course.",
                    "label": 0
                },
                {
                    "sent": "So the same thing you can do here for tagging where you extend from bigrams, trigrams.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "For projective trees you can, instead of factoring for one arc, you can factor for several arcs at the time.",
                    "label": 0
                },
                {
                    "sent": "That gives you bigger features, and sometimes it gives you much better performance and it's easy to extend this thing so.",
                    "label": 0
                },
                {
                    "sent": "So basically our library provides all these abstract mechanisms and we define all these components and actually implement them so that you can use them.",
                    "label": 0
                },
                {
                    "sent": "So that's the abstraction in order to have this and make A and have a real parser.",
                    "label": 0
                },
                {
                    "sent": "You also need to supply feature functions and you method for reading and writing data.",
                    "label": 0
                },
                {
                    "sent": "And all these things.",
                    "label": 0
                },
                {
                    "sent": "So all those things are also there.",
                    "label": 0
                },
                {
                    "sent": "We try to make it as flexible and usable and standard as possible.",
                    "label": 0
                },
                {
                    "sent": "I also have some.",
                    "label": 0
                },
                {
                    "sent": "The software itself is a library.",
                    "label": 0
                },
                {
                    "sent": "We also provide script for actually developing things, but we also provide certain models, so that's basically AM.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The library let me present some experiments so that you can see that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This thing really runs.",
                    "label": 0
                },
                {
                    "sent": "So these are the type of experiments we typically one parsing nothing in the side of actually doing applications, but I think it's interesting to see what's the possible combinations we can do.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is like, OK, let's take one model for dependency parsing and let's try different learners because it's a matter of selecting each of them.",
                    "label": 1
                },
                {
                    "sent": "So we take as a model.",
                    "label": 0
                },
                {
                    "sent": "We take dependency parsing, mapping sentences to projective trees with decomposing two arcs, and we use this inference algorithms, and we're going to compare.",
                    "label": 0
                },
                {
                    "sent": "Perception and log linear models, CRF's and media models SVM's.",
                    "label": 1
                },
                {
                    "sent": "So the data we use is the standard data used for parsing the Penn treebank, which is a collection of sentences with syntactic trees.",
                    "label": 1
                },
                {
                    "sent": "So we can this plots number of epochs, number of times you have looked over the training set and this is validation accuracy.",
                    "label": 0
                },
                {
                    "sent": "So percent.",
                    "label": 0
                },
                {
                    "sent": "Well actually you can see the curiosity.",
                    "label": 0
                },
                {
                    "sent": "So perception is the blue curve perspective.",
                    "label": 0
                },
                {
                    "sent": "It turns out that in NLP turns out to be an hour in.",
                    "label": 0
                },
                {
                    "sent": "That really works well, especially the average perceptron.",
                    "label": 0
                },
                {
                    "sent": "So it works well.",
                    "label": 0
                },
                {
                    "sent": "He goes.",
                    "label": 0
                },
                {
                    "sent": "It achieves us as many as good results on the other ones that are simple to implement.",
                    "label": 0
                },
                {
                    "sent": "And actually it only takes three or four books to to run.",
                    "label": 0
                },
                {
                    "sent": "But that's just the gravity of this data set anything.",
                    "label": 0
                },
                {
                    "sent": "So perhaps in other problems you really find benefits in training as VM's or or or cigarettes, or depending on your application you may like to actually train a CRF, which is a probabilistic classifier.",
                    "label": 0
                },
                {
                    "sent": "You can do lots of things with that.",
                    "label": 0
                },
                {
                    "sent": "So that's one type of graphic employee.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing you can do is I'm going to just fix the learner, but I'm going to I'm going to try different factorizations so so the problem is the same.",
                    "label": 0
                },
                {
                    "sent": "I only change here the factorization, so here I consider another factorization and here consider something that stands for two hours.",
                    "label": 0
                },
                {
                    "sent": "That really can boost the results, so that can buy you a couple of points which is difficult to achieve so that Kite likes that it's very.",
                    "label": 0
                },
                {
                    "sent": "So by changing the factorization we can get an improvement, so we really it's been researching the last years people trying to come up with different factorizations that exploit higher things, and so it's when you're doing this type of things that you say, well I cannot re implement everything every time, I cannot just cut and paste the code and do the changes.",
                    "label": 0
                },
                {
                    "sent": "I need some structure in my library so that I can do these things in a clean way.",
                    "label": 0
                },
                {
                    "sent": "So inside the parsing algorithms you see that I have.",
                    "label": 0
                },
                {
                    "sent": "These things 1 two.",
                    "label": 0
                },
                {
                    "sent": "That means that actually the actual implementation behind this can also be parameterized by certain things that you know of your factorization.",
                    "label": 0
                },
                {
                    "sent": "So, so we're providing.",
                    "label": 0
                },
                {
                    "sent": "Also, we're trying to provide this under development.",
                    "label": 0
                },
                {
                    "sent": "Allegations of this via via notions of States and notions of of semirings where actually this in this is actually the same implementation, just changing little things.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the final experiment.",
                    "label": 0
                },
                {
                    "sent": "Does this data set from the kernel 1007 shader task?",
                    "label": 0
                },
                {
                    "sent": "With this, all these 10 or 11 three ones about dependency parsing on different languages, so so in blue, is the is the results obtained by thriller what we have obtained right now, and so the other performance is the best performance for each language of any system.",
                    "label": 0
                },
                {
                    "sent": "So we're comparing against the best for each.",
                    "label": 0
                },
                {
                    "sent": "Thing is, it's not the best system.",
                    "label": 0
                },
                {
                    "sent": "It's the best system at it for its language.",
                    "label": 0
                },
                {
                    "sent": "So we're doing pretty good so far.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to have to release the library where you actually can can obtain results that are in the state of the art and you can transpile use it and reproduce it and change it.",
                    "label": 0
                },
                {
                    "sent": "And all these things.",
                    "label": 0
                },
                {
                    "sent": "So I think that's about it.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just as a summary, it's an open source library for structured prediction.",
                    "label": 1
                },
                {
                    "sent": "Here's the website where we are doing the development.",
                    "label": 0
                },
                {
                    "sent": "You can download it from there.",
                    "label": 0
                },
                {
                    "sent": "You can also download some models that are already trained for you.",
                    "label": 1
                },
                {
                    "sent": "And so the focus is on tagging and parsing problems in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "We think that we've worked with this abstraction between modeling and learning, and I think that that hopefully can be good for extending the library.",
                    "label": 0
                },
                {
                    "sent": "So there's two extensions you can do.",
                    "label": 0
                },
                {
                    "sent": "You could think of a new model you want.",
                    "label": 0
                },
                {
                    "sent": "I have this new structure prediction problem, so as long as you implement the framework, which we think you'll have to do it somehow because structure prediction requires features and inference.",
                    "label": 0
                },
                {
                    "sent": "So if you do it with us then maybe you can easily block all the platter of learners that we have, right?",
                    "label": 0
                },
                {
                    "sent": "So that's that's good for research in NLP or instruction prediction that wants to focus on representations so that you may be like a machine learning researcher doing research on structured prediction that doesn't know the details of NLP, right?",
                    "label": 0
                },
                {
                    "sent": "So how can you show that your algorithm works on this very large scale and interesting problems so you can build a new learning learner?",
                    "label": 0
                },
                {
                    "sent": "And if your learner requires the same interface, which is likely.",
                    "label": 0
                },
                {
                    "sent": "So then you'll be able to apply your learning into all these tasks by just instantiating them, so you don't.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can do parsing without knowing how a parser works, as long as your algorithm only requires the computations were giving, so that's that's what we would like.",
                    "label": 0
                },
                {
                    "sent": "And so in order to achieve this with you, 70 plus plus templates, which we think is a good mechanism for Poly polymorphisms that doesn't compromise the efficiency.",
                    "label": 0
                },
                {
                    "sent": "So that's about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, is everything.",
                    "label": 0
                },
                {
                    "sent": "Is there an API or anything people do know?",
                    "label": 0
                },
                {
                    "sent": "We'd like to would like to so, but that we're very, very early stage, so the main focus now has been on setting up all these interfaces.",
                    "label": 0
                },
                {
                    "sent": "So this is documented, but we'd like to have.",
                    "label": 0
                },
                {
                    "sent": "We know that people so.",
                    "label": 0
                },
                {
                    "sent": "Not a language where productivity is super high.",
                    "label": 0
                },
                {
                    "sent": "It's quite slow to do that, so we'd like to possibly have interfaces.",
                    "label": 0
                },
                {
                    "sent": "To Python, Matlab, whatever people is.",
                    "label": 0
                },
                {
                    "sent": "So we're open to talking to people that can provide those those buildings.",
                    "label": 0
                },
                {
                    "sent": "This relates to the feeling right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, it relates to feeling so if you don't know Freiling is a is a suite of NLP tools that we have in our group.",
                    "label": 0
                },
                {
                    "sent": "You can, you can check it feeling and so it relates in the sense that I tried to hide like in the first slides where I said through there is not a system for doing pipelines for doing applications.",
                    "label": 0
                },
                {
                    "sent": "It's a system for building components, so this will going to bring.",
                    "label": 0
                },
                {
                    "sent": "Statistical components into failing components that you can train and things like that.",
                    "label": 0
                },
                {
                    "sent": "So once we once we see all look this parser.",
                    "label": 0
                },
                {
                    "sent": "Worked very well for these languages, so will package it as a thrilling model and then you can integrate that model with all the frieling components that actually builds a full NLP pipeline so that that is more useful is useful for building those applications to some extent.",
                    "label": 0
                },
                {
                    "sent": "House documentation doing that so the obstructions we have then clear and then the actual quote we are working on that.",
                    "label": 0
                },
                {
                    "sent": "But yeah, we have a working release.",
                    "label": 0
                },
                {
                    "sent": "Thank you Yep.",
                    "label": 0
                }
            ]
        }
    }
}