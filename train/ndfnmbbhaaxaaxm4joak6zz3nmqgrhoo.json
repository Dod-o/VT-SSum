{
    "id": "ndfnmbbhaaxaaxm4joak6zz3nmqgrhoo",
    "title": "Mixability is Bayes Risk Curvature Relative to Log Loss",
    "info": {
        "author": [
            "Robert C. Williamson, Australian National University"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_williamson_risk/",
    "segmentation": [
        [
            "It's about mix ability.",
            "That's a mixer, and in the antepenultimate talk of the conference."
        ],
        [
            "If you fall asleep after the title slide, that's OK, because the one result in this paper is in the title, right?",
            "That's all you have to remember.",
            "So if you want to leave now, that's fine.",
            "OK, you want more?",
            "This is with Tim and Mark photos their teams."
        ],
        [
            "The audience marks back in camera, so mix ability is a property of a loss function.",
            "I've been looking at loss functions the last couple of years and I got interested in this question of characterizing mix ability and I'll explain why it's important because it precisely characterizes the performance of aggregated estimators, so this seems like a pretty fundamental quantity, and it would be nice to understand it and characterize it.",
            "There are some known results on mix ability of certain losses in the binary case, and at least for two losses in the multiclass case.",
            "But at least to me, there is next to no intuition from those formulas, and I'll show you one of them.",
            "In this talk, the result is a complete and at least intuitive to me.",
            "Now geometric characterization of mix ability in the multiclass case.",
            "And the end.",
            "I'll relate it to some other recent work that relates statistical performance to the geometry of the."
        ],
        [
            "Space risk.",
            "So the set up there's a outcome space of N possible labels.",
            "The learner makes some predictions.",
            "You measure the performance with respect to a loss function, and you then accumulate that loss.",
            "So the cumulative loss loss T is that the price that you're going to pay.",
            "The learner has access to predictions from N experts E1 to EN that are trying to predict the same sequence and the goal of the learner is to predict nearly as well as the best expert.",
            "So this is."
        ],
        [
            "Expanded and then a merging strategy tries to combine the predictions of those experts.",
            "That's what you want to do.",
            "Jane, this is standard.",
            "And after T rounds then you have the loss of the merging strategy is just the cumulative loss of the predictions that the merging strategy gave.",
            "The loss of the individual expert is just the loss made by their prediction."
        ],
        [
            "And the key result due to involve some time ago is that there is this algorithm called the aggregating algorithm and this magic.",
            "Parameter called beta mix ability, which I'll explain shortly, implies that for all times T and for all experts, the loss of this merging strategy is less than the loss of the expert plus this constant.",
            "The thing to note about this, it holds for all experts, so in particular for the best one, there's no constant in front of the bound of the loss of the expert, and you've just got two key parameters on the right hand side, the number of experts in.",
            "And this parameter beta.",
            "So if you knew what this parameter beta was, you would know everything you can say about this learning problem as set up here.",
            "Furthermore, if this loss is not beta mixable for any beta, you can't predict as well as the best expert up to an additive constant, right?",
            "So this mix ability constant is is crucial here.",
            "Not every loss is mixable.",
            "For those that are what I'm going to show you is how to calculate that mix ability constant in terms of geometric parameters of the loss."
        ],
        [
            "Auction.",
            "So I'm looking at the multiclass case so the motivation is multiclass losses for class probability estimation.",
            "So this is when you've got impossible labels and your job is to predict the probability of a certain class.",
            "Now, Interestingly, the setup that I just gave you is not probabilistic, and proper losses are defined probabilistically, which is intriguing and I don't have a deep explanation of what's going on here, but there you have it.",
            "These multiclass losses being things for predicting probabilities, are naturally defined on the simplex Delta and is just the simplex and so now I'm going to write a loss function in this slightly different way.",
            "If you paying attention is different to a couple of slides ago, you think of the losses of vector valued function, so it takes a prediction and it outputs a vector and the vector is just the partial losses.",
            "OK, so that's the loss that you would incur, so L sub I.",
            "Is the loss that you would incur on your prediction if the true label was I just another way of running, it just turns out to be convenient.",
            "So the Elsa buys are the partial losses.",
            "That's what I just said.",
            "If the outcomes are distributed with a particular probability P, that's the true probability, then the risk for predicting Q is just the expected loss.",
            "It's just a standard statistical setting and writing the loss function in this form that expected that risk is just the dot product between P&L.",
            "QKP is a vector and L of Q is a vector."
        ],
        [
            "A key quantity in the statistical setting is the Bayes risk.",
            "The Bayes risk is the minimal achievable risk for that outcome distribution, right?",
            "So you consider all possible predictors Q and you take the smallest value of the risk right?",
            "So that's L underbar P the underbar supposed to signify that it's as low as it can go.",
            "It's known that ALB.",
            "RFP is always concave.",
            "And a loss is called proper, so these things in the statistic literature called proper scoring rules.",
            "But I'm going to call improper losses whenever the minimal risk is always achieved by predicting the true outcome distribution.",
            "So this is a nice kind of loss to get if you minimize the loss then you get it right.",
            "And a proper loss is strictly proper.",
            "If there's no other prediction that you could make that would do this.",
            "OK, so you have to remember proper and strictly proper.",
            "And Furthermore, it's known for a long time that just given L bar tells you which proper loss you've got, so there's a wonder one relationship.",
            "Elba is the Bayes risk, it's a concave function on the simplex that is, given any concave function on this simplex that corresponds to a proper loss, any proper loss has that concave function concave function you also called."
        ],
        [
            "Entropy.",
            "Now to do what we ended up doing, one of the technical headaches we have is that proper losses are defined on the simplex.",
            "That's a submanifold and we want to do derivatives.",
            "So you could try and get fancy and do differential calculus on manifolds.",
            "Or you could take the engineers approach and just re parameterise it back to things that work well, which is all we've done here.",
            "And so all you need to do is to recognize that the N dimensional simplex is actually an N -- 1 dimensional set.",
            "Just work with that.",
            "So we called at the bottom of the simplex and you just think if you've got a probability vector P1 to PN, I don't need to carry PN around because the last entry of the vector is just one minus the sum of all of the others.",
            "All that's going on here.",
            "So all of the spaces and stuff that we work with, we just keep that in mind and the only thing you really need to remember from this diagram is that the twiddles denote when we're working in minus one dimensions and in minus one dimensions you can just take derivatives in the good old fashioned way."
        ],
        [
            "So.",
            "So the another key thing that you can do with proper losses when you assume that they are differentiable is from the definition of properness that the loss is minimized when you predict the correct value, then this just standard calculus that you get this thing called the stationarity condition so.",
            "The P times the derivative of the loss is just the zero vector.",
            "A lot of the.",
            "The various functions that we work with have not properties like they're invertible.",
            "This Pi Lambda is just this projection function, so you can go between these N -- 1 and dimensions, so you can you want to solve a problem in dimensions.",
            "You can do it all down in minus one, and if you think of this, set Lambda.",
            "So you define, your loss is defined on the simplex, so L of Delta in is the image of the simplex under the loss.",
            "So that's a manifold, and if.",
            "You have a particular probability vector P. That's in this same space.",
            "Then it will define a hyperplane that touches the this manifold at the point parameterized by P. So there's a nice geometric picture to give you."
        ],
        [
            "Insight here.",
            "So a key thing that you need to define this notion of mix ability is what Valve calls the Super prediction set.",
            "So the two curves there are just the image of the simplex under the loss and the Super prediction set is just that region to the North East of the set.",
            "OK, so it's defined mathematically there, but if you just keep this picture in mind and there's two curves there for two particular proper losses, the Brier score, or squared loss and log."
        ],
        [
            "Plus so there's a key operator that we use called the beta exponential operator.",
            "It's just a matter of taking exponentials of these losses, and the crucial fact that we use this is not knew this was known by Valk.",
            "Data loss is beta mixable.",
            "When this set is convex.",
            "So all you need to do to workout when it losses Mixable is to determine when this exponentially distorted image of the Super prediction set is convex.",
            "That's all you have to do.",
            "And the mix ability constant is just the largest beta such that that set is convex."
        ],
        [
            "OK, so you can do that and the way you do it is you define this set E beater of SL and it ends up this is the boundary of it.",
            "So if you look at these curves here you think of these as increasing beta.",
            "Think of these curves right?",
            "You think of the set down to the left when you take that super prediction set, put it through that E beta.",
            "Map.",
            "The Gray area gets mapped down to the lower region, right?",
            "'cause the better thing is monotone decreasing, so as you increase the beta in the beta map.",
            "You get this family of curves, right?",
            "So that's what's going on."
        ],
        [
            "Sorry, you can't get to see the movie.",
            "Now for differentiable binary losses, there's a known formula that does tell you what the mixability constant is for binary losses, so it gives you this intuitive form here, which gives you a lot of insight into what's going on right now.",
            "The first observation we made was that if you have a proper loss, then the.",
            "The fact that it's proper imposes constraints between the derivatives of the partial losses, so that's what the stationarity condition gives you here OK. And if we introduce this single the wait function, which is the negative second derivative of the Bayes risk."
        ],
        [
            "And you just go and plug all of this in and then hey presto, it turns into this nice red formula which looks a lot nicer.",
            "But we can go a little further still, if you take the Bayes risk for log.",
            "Lausanne observed that it looks like that in you calculate the derivatives of that, then you get this even nicer formula down the bottom right, so that shows you that the mix ability constant for a binary differentiable proper loss is governed by this minimum of the ratio of.",
            "The second derivative of the Bayes risk log loss to the second derivative of the Bayes risk for the loss are interested in.",
            "Alright, that was very easy.",
            "All of the heavy lifting in the paper is generalizing that too in dimensional losses."
        ],
        [
            "And I'm not going to give you the gory details of that.",
            "There's a couple of key tricks that we end up using, so one of them is that you can express the derivative of the NTH partial loss in terms of all of the other partial losses.",
            "And another one is that the formula in red down the bottom there is crucial that the Hessian of the Bayes risk, so the Bayes risk here is a function from N -- 1 dimensions to the reels, so the Hessian is an N -- 1 by N -- 1 matrix and I can express that as a matrix times the derivative of L. Twiddle L Twitter is the 1st in minus one components of the loss, so that's crucial crucial connection and HL bar of P is negative definite an invertible and you can.",
            "Express it in this form.",
            "Sorry for log loss.",
            "There's a particularly nice form."
        ],
        [
            "OK, so.",
            "So the key result is that the mix ability constant.",
            "So for general N multiclass losses, if you assume it's twice differentiable and strictly proper, then the mixability constant is the largest beta such that this condition holds, right?",
            "This is saying that Beta HFL bar minus H develop dialogue is positive definite.",
            "This funny.",
            "Greater than or equal sing is the positive definite ordering.",
            "OK, and you can do a little more work on."
        ],
        [
            "And get it into this form.",
            "So nice form that we've got it too.",
            "So the mix ability constant satisfies this.",
            "It's now the minimum of the maximum minimum over the simplex.",
            "Of the maximum eigenvalue of the inverse of the Bayes risk but scaled by the Hessian of the Bayes risk for log loss.",
            "And if you compare that to the result in the binary case, you see that it'll make sense, right?",
            "Because in the binary case the Hessian will be a one by one matrix, right?",
            "Its maximum eigenvalue is at single entry.",
            "OK, the proof is the chain rule, right?",
            "But it takes about 8 pages and it's just gory, but."
        ],
        [
            "That's all I'm telling you about the proof.",
            "Now this was all for proper losses, but it turns out that is not such a restriction because you can always, not always.",
            "For many non proper losses you can calibrate them with a link function.",
            "So if you've got an improper loss then.",
            "I can define this thing which is the reference link.",
            "So remember if a loss is improper, then it's minimized for value of P not equal to the true P. So then all you have to do is to have some translation function that says, OK, we'll just map it back again.",
            "That's all a link function does, right?",
            "So you do that and it's easy to see that if you go and define this reference link in this fashion, then you can go and construct a loss from the loss that you were given.",
            "That is proper appeal to the result you've got, so."
        ],
        [
            "Properness in itself doesn't seem such a restriction.",
            "The last thing I want to mention is a connection to a paper by Abernethy, Bartlett, Rachlin and Agarwal in Colt two years ago where they were looking at these statistical performance in terms of the geometry of the Bayes risk they.",
            "Use this notion of Alpha flatness and strong convexity.",
            "The way to me, an easy way to understand Alpha flatness is that a function is Alpha flat if F minus Alpha times the norm squared is concave.",
            "So to equivalent way of saying it.",
            "If you use that then you can see that their their result affectively requires the Hessian of the Bayes risk to be dominated by.",
            "The identity matrix.",
            "So if you compare that to the result that we've got, so we have mix ability in terms they there is is in terms of the Hessian of the Bayes risk of the loss you're interested in.",
            "Our result is in terms of the Hessian of the Bayes risk of the loss you're interested in, normalized by the Hessian for log loss, so low, and it's not surprising that log loss somehow plays a crucial part here.",
            "If you think the aggregating algorithm is just Bayesian conditioning.",
            "When you've got the log loss so it kind of makes sense."
        ],
        [
            "So to conclude, the real result of this paper is I'll slow down then.",
            "The I got 2 minutes the the.",
            "This formula is the main result of the paper.",
            "So that it's in terms of the Bayes risk relative to log loss.",
            "You can use LINQ functions to extend it to a wider class of loss functions.",
            "That's all I've got to say, plus an ad.",
            "The comparison to the result of Peter Bartlett.",
            "So I didn't.",
            "I didn't really get that.",
            "Can you get back?",
            "Go back to that?",
            "I didn't really."
        ],
        [
            "Till it, that's why you don't write, because does this imply that your that your condition is strictly weaker condition?",
            "Well, I don't know what their results, so that's what is their result so.",
            "To state their result in detail, I did not have enough time right, but the high level story is that they bound the performance of a prediction problem in terms of the geometry of the Bayes risk of the loss and the crucial thing comes down to the curvature of the Bayes risk of the loss.",
            "The specific form that they get is like it's just the curvature of that.",
            "The difference is that in our result it's the curvature of that, but you kind of normalize it relative.",
            "To the curvature of log loss, right now they're not exactly the same problem, right?",
            "So that's why I'm not, I'm not saying that you know our result subsumes there's right, but this is more of a known WAVY, suggestive thing, and it may well be that in their setting, which is slightly different, you can get a stronger result that is normalized by log loss.",
            "I just don't know.",
            "OK, let's clear.",
            "Any other questions?",
            "Alright, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's about mix ability.",
                    "label": 0
                },
                {
                    "sent": "That's a mixer, and in the antepenultimate talk of the conference.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you fall asleep after the title slide, that's OK, because the one result in this paper is in the title, right?",
                    "label": 0
                },
                {
                    "sent": "That's all you have to remember.",
                    "label": 0
                },
                {
                    "sent": "So if you want to leave now, that's fine.",
                    "label": 0
                },
                {
                    "sent": "OK, you want more?",
                    "label": 0
                },
                {
                    "sent": "This is with Tim and Mark photos their teams.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The audience marks back in camera, so mix ability is a property of a loss function.",
                    "label": 1
                },
                {
                    "sent": "I've been looking at loss functions the last couple of years and I got interested in this question of characterizing mix ability and I'll explain why it's important because it precisely characterizes the performance of aggregated estimators, so this seems like a pretty fundamental quantity, and it would be nice to understand it and characterize it.",
                    "label": 1
                },
                {
                    "sent": "There are some known results on mix ability of certain losses in the binary case, and at least for two losses in the multiclass case.",
                    "label": 1
                },
                {
                    "sent": "But at least to me, there is next to no intuition from those formulas, and I'll show you one of them.",
                    "label": 0
                },
                {
                    "sent": "In this talk, the result is a complete and at least intuitive to me.",
                    "label": 0
                },
                {
                    "sent": "Now geometric characterization of mix ability in the multiclass case.",
                    "label": 0
                },
                {
                    "sent": "And the end.",
                    "label": 0
                },
                {
                    "sent": "I'll relate it to some other recent work that relates statistical performance to the geometry of the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space risk.",
                    "label": 0
                },
                {
                    "sent": "So the set up there's a outcome space of N possible labels.",
                    "label": 0
                },
                {
                    "sent": "The learner makes some predictions.",
                    "label": 0
                },
                {
                    "sent": "You measure the performance with respect to a loss function, and you then accumulate that loss.",
                    "label": 0
                },
                {
                    "sent": "So the cumulative loss loss T is that the price that you're going to pay.",
                    "label": 0
                },
                {
                    "sent": "The learner has access to predictions from N experts E1 to EN that are trying to predict the same sequence and the goal of the learner is to predict nearly as well as the best expert.",
                    "label": 1
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expanded and then a merging strategy tries to combine the predictions of those experts.",
                    "label": 0
                },
                {
                    "sent": "That's what you want to do.",
                    "label": 0
                },
                {
                    "sent": "Jane, this is standard.",
                    "label": 0
                },
                {
                    "sent": "And after T rounds then you have the loss of the merging strategy is just the cumulative loss of the predictions that the merging strategy gave.",
                    "label": 1
                },
                {
                    "sent": "The loss of the individual expert is just the loss made by their prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the key result due to involve some time ago is that there is this algorithm called the aggregating algorithm and this magic.",
                    "label": 0
                },
                {
                    "sent": "Parameter called beta mix ability, which I'll explain shortly, implies that for all times T and for all experts, the loss of this merging strategy is less than the loss of the expert plus this constant.",
                    "label": 0
                },
                {
                    "sent": "The thing to note about this, it holds for all experts, so in particular for the best one, there's no constant in front of the bound of the loss of the expert, and you've just got two key parameters on the right hand side, the number of experts in.",
                    "label": 0
                },
                {
                    "sent": "And this parameter beta.",
                    "label": 0
                },
                {
                    "sent": "So if you knew what this parameter beta was, you would know everything you can say about this learning problem as set up here.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, if this loss is not beta mixable for any beta, you can't predict as well as the best expert up to an additive constant, right?",
                    "label": 1
                },
                {
                    "sent": "So this mix ability constant is is crucial here.",
                    "label": 0
                },
                {
                    "sent": "Not every loss is mixable.",
                    "label": 0
                },
                {
                    "sent": "For those that are what I'm going to show you is how to calculate that mix ability constant in terms of geometric parameters of the loss.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Auction.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking at the multiclass case so the motivation is multiclass losses for class probability estimation.",
                    "label": 0
                },
                {
                    "sent": "So this is when you've got impossible labels and your job is to predict the probability of a certain class.",
                    "label": 0
                },
                {
                    "sent": "Now, Interestingly, the setup that I just gave you is not probabilistic, and proper losses are defined probabilistically, which is intriguing and I don't have a deep explanation of what's going on here, but there you have it.",
                    "label": 0
                },
                {
                    "sent": "These multiclass losses being things for predicting probabilities, are naturally defined on the simplex Delta and is just the simplex and so now I'm going to write a loss function in this slightly different way.",
                    "label": 0
                },
                {
                    "sent": "If you paying attention is different to a couple of slides ago, you think of the losses of vector valued function, so it takes a prediction and it outputs a vector and the vector is just the partial losses.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the loss that you would incur, so L sub I.",
                    "label": 0
                },
                {
                    "sent": "Is the loss that you would incur on your prediction if the true label was I just another way of running, it just turns out to be convenient.",
                    "label": 0
                },
                {
                    "sent": "So the Elsa buys are the partial losses.",
                    "label": 0
                },
                {
                    "sent": "That's what I just said.",
                    "label": 0
                },
                {
                    "sent": "If the outcomes are distributed with a particular probability P, that's the true probability, then the risk for predicting Q is just the expected loss.",
                    "label": 1
                },
                {
                    "sent": "It's just a standard statistical setting and writing the loss function in this form that expected that risk is just the dot product between P&L.",
                    "label": 0
                },
                {
                    "sent": "QKP is a vector and L of Q is a vector.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A key quantity in the statistical setting is the Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "The Bayes risk is the minimal achievable risk for that outcome distribution, right?",
                    "label": 1
                },
                {
                    "sent": "So you consider all possible predictors Q and you take the smallest value of the risk right?",
                    "label": 0
                },
                {
                    "sent": "So that's L underbar P the underbar supposed to signify that it's as low as it can go.",
                    "label": 0
                },
                {
                    "sent": "It's known that ALB.",
                    "label": 0
                },
                {
                    "sent": "RFP is always concave.",
                    "label": 0
                },
                {
                    "sent": "And a loss is called proper, so these things in the statistic literature called proper scoring rules.",
                    "label": 0
                },
                {
                    "sent": "But I'm going to call improper losses whenever the minimal risk is always achieved by predicting the true outcome distribution.",
                    "label": 1
                },
                {
                    "sent": "So this is a nice kind of loss to get if you minimize the loss then you get it right.",
                    "label": 0
                },
                {
                    "sent": "And a proper loss is strictly proper.",
                    "label": 0
                },
                {
                    "sent": "If there's no other prediction that you could make that would do this.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have to remember proper and strictly proper.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, it's known for a long time that just given L bar tells you which proper loss you've got, so there's a wonder one relationship.",
                    "label": 0
                },
                {
                    "sent": "Elba is the Bayes risk, it's a concave function on the simplex that is, given any concave function on this simplex that corresponds to a proper loss, any proper loss has that concave function concave function you also called.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Entropy.",
                    "label": 0
                },
                {
                    "sent": "Now to do what we ended up doing, one of the technical headaches we have is that proper losses are defined on the simplex.",
                    "label": 1
                },
                {
                    "sent": "That's a submanifold and we want to do derivatives.",
                    "label": 0
                },
                {
                    "sent": "So you could try and get fancy and do differential calculus on manifolds.",
                    "label": 0
                },
                {
                    "sent": "Or you could take the engineers approach and just re parameterise it back to things that work well, which is all we've done here.",
                    "label": 1
                },
                {
                    "sent": "And so all you need to do is to recognize that the N dimensional simplex is actually an N -- 1 dimensional set.",
                    "label": 0
                },
                {
                    "sent": "Just work with that.",
                    "label": 0
                },
                {
                    "sent": "So we called at the bottom of the simplex and you just think if you've got a probability vector P1 to PN, I don't need to carry PN around because the last entry of the vector is just one minus the sum of all of the others.",
                    "label": 0
                },
                {
                    "sent": "All that's going on here.",
                    "label": 0
                },
                {
                    "sent": "So all of the spaces and stuff that we work with, we just keep that in mind and the only thing you really need to remember from this diagram is that the twiddles denote when we're working in minus one dimensions and in minus one dimensions you can just take derivatives in the good old fashioned way.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the another key thing that you can do with proper losses when you assume that they are differentiable is from the definition of properness that the loss is minimized when you predict the correct value, then this just standard calculus that you get this thing called the stationarity condition so.",
                    "label": 1
                },
                {
                    "sent": "The P times the derivative of the loss is just the zero vector.",
                    "label": 0
                },
                {
                    "sent": "A lot of the.",
                    "label": 0
                },
                {
                    "sent": "The various functions that we work with have not properties like they're invertible.",
                    "label": 0
                },
                {
                    "sent": "This Pi Lambda is just this projection function, so you can go between these N -- 1 and dimensions, so you can you want to solve a problem in dimensions.",
                    "label": 0
                },
                {
                    "sent": "You can do it all down in minus one, and if you think of this, set Lambda.",
                    "label": 0
                },
                {
                    "sent": "So you define, your loss is defined on the simplex, so L of Delta in is the image of the simplex under the loss.",
                    "label": 0
                },
                {
                    "sent": "So that's a manifold, and if.",
                    "label": 1
                },
                {
                    "sent": "You have a particular probability vector P. That's in this same space.",
                    "label": 0
                },
                {
                    "sent": "Then it will define a hyperplane that touches the this manifold at the point parameterized by P. So there's a nice geometric picture to give you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Insight here.",
                    "label": 0
                },
                {
                    "sent": "So a key thing that you need to define this notion of mix ability is what Valve calls the Super prediction set.",
                    "label": 0
                },
                {
                    "sent": "So the two curves there are just the image of the simplex under the loss and the Super prediction set is just that region to the North East of the set.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's defined mathematically there, but if you just keep this picture in mind and there's two curves there for two particular proper losses, the Brier score, or squared loss and log.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plus so there's a key operator that we use called the beta exponential operator.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of taking exponentials of these losses, and the crucial fact that we use this is not knew this was known by Valk.",
                    "label": 0
                },
                {
                    "sent": "Data loss is beta mixable.",
                    "label": 0
                },
                {
                    "sent": "When this set is convex.",
                    "label": 0
                },
                {
                    "sent": "So all you need to do to workout when it losses Mixable is to determine when this exponentially distorted image of the Super prediction set is convex.",
                    "label": 0
                },
                {
                    "sent": "That's all you have to do.",
                    "label": 0
                },
                {
                    "sent": "And the mix ability constant is just the largest beta such that that set is convex.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you can do that and the way you do it is you define this set E beater of SL and it ends up this is the boundary of it.",
                    "label": 0
                },
                {
                    "sent": "So if you look at these curves here you think of these as increasing beta.",
                    "label": 0
                },
                {
                    "sent": "Think of these curves right?",
                    "label": 0
                },
                {
                    "sent": "You think of the set down to the left when you take that super prediction set, put it through that E beta.",
                    "label": 0
                },
                {
                    "sent": "Map.",
                    "label": 0
                },
                {
                    "sent": "The Gray area gets mapped down to the lower region, right?",
                    "label": 0
                },
                {
                    "sent": "'cause the better thing is monotone decreasing, so as you increase the beta in the beta map.",
                    "label": 0
                },
                {
                    "sent": "You get this family of curves, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, you can't get to see the movie.",
                    "label": 0
                },
                {
                    "sent": "Now for differentiable binary losses, there's a known formula that does tell you what the mixability constant is for binary losses, so it gives you this intuitive form here, which gives you a lot of insight into what's going on right now.",
                    "label": 1
                },
                {
                    "sent": "The first observation we made was that if you have a proper loss, then the.",
                    "label": 1
                },
                {
                    "sent": "The fact that it's proper imposes constraints between the derivatives of the partial losses, so that's what the stationarity condition gives you here OK. And if we introduce this single the wait function, which is the negative second derivative of the Bayes risk.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you just go and plug all of this in and then hey presto, it turns into this nice red formula which looks a lot nicer.",
                    "label": 0
                },
                {
                    "sent": "But we can go a little further still, if you take the Bayes risk for log.",
                    "label": 0
                },
                {
                    "sent": "Lausanne observed that it looks like that in you calculate the derivatives of that, then you get this even nicer formula down the bottom right, so that shows you that the mix ability constant for a binary differentiable proper loss is governed by this minimum of the ratio of.",
                    "label": 0
                },
                {
                    "sent": "The second derivative of the Bayes risk log loss to the second derivative of the Bayes risk for the loss are interested in.",
                    "label": 0
                },
                {
                    "sent": "Alright, that was very easy.",
                    "label": 0
                },
                {
                    "sent": "All of the heavy lifting in the paper is generalizing that too in dimensional losses.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm not going to give you the gory details of that.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of key tricks that we end up using, so one of them is that you can express the derivative of the NTH partial loss in terms of all of the other partial losses.",
                    "label": 0
                },
                {
                    "sent": "And another one is that the formula in red down the bottom there is crucial that the Hessian of the Bayes risk, so the Bayes risk here is a function from N -- 1 dimensions to the reels, so the Hessian is an N -- 1 by N -- 1 matrix and I can express that as a matrix times the derivative of L. Twiddle L Twitter is the 1st in minus one components of the loss, so that's crucial crucial connection and HL bar of P is negative definite an invertible and you can.",
                    "label": 0
                },
                {
                    "sent": "Express it in this form.",
                    "label": 0
                },
                {
                    "sent": "Sorry for log loss.",
                    "label": 0
                },
                {
                    "sent": "There's a particularly nice form.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the key result is that the mix ability constant.",
                    "label": 0
                },
                {
                    "sent": "So for general N multiclass losses, if you assume it's twice differentiable and strictly proper, then the mixability constant is the largest beta such that this condition holds, right?",
                    "label": 1
                },
                {
                    "sent": "This is saying that Beta HFL bar minus H develop dialogue is positive definite.",
                    "label": 0
                },
                {
                    "sent": "This funny.",
                    "label": 1
                },
                {
                    "sent": "Greater than or equal sing is the positive definite ordering.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can do a little more work on.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And get it into this form.",
                    "label": 0
                },
                {
                    "sent": "So nice form that we've got it too.",
                    "label": 0
                },
                {
                    "sent": "So the mix ability constant satisfies this.",
                    "label": 0
                },
                {
                    "sent": "It's now the minimum of the maximum minimum over the simplex.",
                    "label": 0
                },
                {
                    "sent": "Of the maximum eigenvalue of the inverse of the Bayes risk but scaled by the Hessian of the Bayes risk for log loss.",
                    "label": 0
                },
                {
                    "sent": "And if you compare that to the result in the binary case, you see that it'll make sense, right?",
                    "label": 0
                },
                {
                    "sent": "Because in the binary case the Hessian will be a one by one matrix, right?",
                    "label": 1
                },
                {
                    "sent": "Its maximum eigenvalue is at single entry.",
                    "label": 0
                },
                {
                    "sent": "OK, the proof is the chain rule, right?",
                    "label": 1
                },
                {
                    "sent": "But it takes about 8 pages and it's just gory, but.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's all I'm telling you about the proof.",
                    "label": 0
                },
                {
                    "sent": "Now this was all for proper losses, but it turns out that is not such a restriction because you can always, not always.",
                    "label": 0
                },
                {
                    "sent": "For many non proper losses you can calibrate them with a link function.",
                    "label": 1
                },
                {
                    "sent": "So if you've got an improper loss then.",
                    "label": 1
                },
                {
                    "sent": "I can define this thing which is the reference link.",
                    "label": 0
                },
                {
                    "sent": "So remember if a loss is improper, then it's minimized for value of P not equal to the true P. So then all you have to do is to have some translation function that says, OK, we'll just map it back again.",
                    "label": 0
                },
                {
                    "sent": "That's all a link function does, right?",
                    "label": 1
                },
                {
                    "sent": "So you do that and it's easy to see that if you go and define this reference link in this fashion, then you can go and construct a loss from the loss that you were given.",
                    "label": 0
                },
                {
                    "sent": "That is proper appeal to the result you've got, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Properness in itself doesn't seem such a restriction.",
                    "label": 0
                },
                {
                    "sent": "The last thing I want to mention is a connection to a paper by Abernethy, Bartlett, Rachlin and Agarwal in Colt two years ago where they were looking at these statistical performance in terms of the geometry of the Bayes risk they.",
                    "label": 0
                },
                {
                    "sent": "Use this notion of Alpha flatness and strong convexity.",
                    "label": 0
                },
                {
                    "sent": "The way to me, an easy way to understand Alpha flatness is that a function is Alpha flat if F minus Alpha times the norm squared is concave.",
                    "label": 1
                },
                {
                    "sent": "So to equivalent way of saying it.",
                    "label": 0
                },
                {
                    "sent": "If you use that then you can see that their their result affectively requires the Hessian of the Bayes risk to be dominated by.",
                    "label": 0
                },
                {
                    "sent": "The identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you compare that to the result that we've got, so we have mix ability in terms they there is is in terms of the Hessian of the Bayes risk of the loss you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Our result is in terms of the Hessian of the Bayes risk of the loss you're interested in, normalized by the Hessian for log loss, so low, and it's not surprising that log loss somehow plays a crucial part here.",
                    "label": 1
                },
                {
                    "sent": "If you think the aggregating algorithm is just Bayesian conditioning.",
                    "label": 0
                },
                {
                    "sent": "When you've got the log loss so it kind of makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, the real result of this paper is I'll slow down then.",
                    "label": 0
                },
                {
                    "sent": "The I got 2 minutes the the.",
                    "label": 0
                },
                {
                    "sent": "This formula is the main result of the paper.",
                    "label": 0
                },
                {
                    "sent": "So that it's in terms of the Bayes risk relative to log loss.",
                    "label": 1
                },
                {
                    "sent": "You can use LINQ functions to extend it to a wider class of loss functions.",
                    "label": 0
                },
                {
                    "sent": "That's all I've got to say, plus an ad.",
                    "label": 0
                },
                {
                    "sent": "The comparison to the result of Peter Bartlett.",
                    "label": 0
                },
                {
                    "sent": "So I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't really get that.",
                    "label": 0
                },
                {
                    "sent": "Can you get back?",
                    "label": 0
                },
                {
                    "sent": "Go back to that?",
                    "label": 0
                },
                {
                    "sent": "I didn't really.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Till it, that's why you don't write, because does this imply that your that your condition is strictly weaker condition?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know what their results, so that's what is their result so.",
                    "label": 0
                },
                {
                    "sent": "To state their result in detail, I did not have enough time right, but the high level story is that they bound the performance of a prediction problem in terms of the geometry of the Bayes risk of the loss and the crucial thing comes down to the curvature of the Bayes risk of the loss.",
                    "label": 1
                },
                {
                    "sent": "The specific form that they get is like it's just the curvature of that.",
                    "label": 1
                },
                {
                    "sent": "The difference is that in our result it's the curvature of that, but you kind of normalize it relative.",
                    "label": 0
                },
                {
                    "sent": "To the curvature of log loss, right now they're not exactly the same problem, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why I'm not, I'm not saying that you know our result subsumes there's right, but this is more of a known WAVY, suggestive thing, and it may well be that in their setting, which is slightly different, you can get a stronger result that is normalized by log loss.",
                    "label": 0
                },
                {
                    "sent": "I just don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, let's clear.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}