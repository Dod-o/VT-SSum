{
    "id": "373e4summmpvidtemjgqc7ikjezaumr5",
    "title": "Bridging the Structured Un-Structured Gap",
    "info": {
        "author": [
            "Soumen Chakrabarti, Department of Computer Science and Engineering, Indian Institute of Technology Bombay"
        ],
        "published": "Oct. 12, 2010",
        "recorded": "February 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm2010_chakrabarti_bsus/",
    "segmentation": [
        [
            "OK, good morning.",
            "Welcome to the conference.",
            "I'm going to talk about building bridges across the gap between structured data and unstructured web data."
        ],
        [
            "So certainly have come a long way since we started seeing around mid 90s from very brittle ranking functions near duplicate pages.",
            "Spam attacks to fairly successful spam filtering link assisted ranking result diversification and detecting when you want to Geo sensitive answer and many many other features.",
            "There's even some limited type awareness in certain verticals.",
            "You can directly ask today one cagey equals how many pounds or the distance between Rome and Venice or hotels near Brooklyn Bridge.",
            "Those things work really well.",
            "However, there remain information needs where the cognitive burden is still fairly large, and that's not directly a problem with the search engine is just because the tasks are somewhat more complicated."
        ],
        [
            "Here are some example queries and I have checked most of them until recently, so there should still be difficult artists who got Oscars for both acting and direction.",
            "And I'm not even asking that the same movie or not typical price of certain kinds of motherboards with certain features.",
            "Or is the number of Oscars 1 by a movie directly related to its production budget?",
            "Say or how many justices serve in the International Criminal Court.",
            "What was the cleanup cost of some environmental disaster?",
            "And so on.",
            "And the reason why these queries are difficult?"
        ],
        [
            "Is that we can do various things yet with search engines.",
            "And indulge me for a moment and don't don't protest that these are not things that end users want.",
            "Because they probably do at least some of them.",
            "Search engines provide very good low level access methods to pages, but they don't give you any variables and how complicated queries can you ask without variables.",
            "I cannot, for example, say that is both an actor and director.",
            "I can't pin that down in my query.",
            "There are no types, or at least the types are well hidden inside the search engine.",
            "I cannot say M stands for a motherboard or P stands for a money amount.",
            "And then ask where is about them.",
            "I cannot relate variables using predicates.",
            "I cannot say that M cells for P or M costs P. And finally I cannot do aggregates.",
            "I cannot build fairly robust ideas about distributions on money amounts.",
            "If you go to the web today, you can fairly successfully collect hundreds of web pages talking about the exam Valdez event and then scan the pages manually to find money amounts, but it'll take you 15 to 20 minutes to build a good.",
            "Reliable statistical impression of what that money amount was a single pages and single snippets onto it for you.",
            "So that's the kind of queries I'm going to talk about today.",
            "What kind of support do you need to impose?",
            "Some kind of structural view on the web so that these queries become hopefully commonplace?"
        ],
        [
            "What if you could ask queries with support?",
            "All these things, for example M is a French movie or A is a number.",
            "B is a money amount or C1 and C2R snippet contexts.",
            "I'll come back toward a snippet.",
            "Context exactly is in a few minutes, but roughly speaking it's a sequence of tokens with support some kind of hypothesis.",
            "So I'd like to ask, is there a context C1 in which a French movie F and number A appear in close juxtaposition together with compulsory word Oscar?",
            "An optional word one?",
            "You'd think this would be sort of easy to do, but surprisingly you still cannot have a search engine do this for you.",
            "Or for the second half of my query, I might be interested in asking if there's a context C2 where the French movie F and a production cost P appear together with the phrase production cost or budget, and then I want to notice that F is a shared variable across the subquery, so that's what's implementing the join.",
            "And finally, I'd like to aggregate out in some sense the context C1 and C2, thereby getting a single table with a movie name missing of F&M.",
            "Here, a number of awards won and the production budget be.",
            "And now I'm free to run my standard lab tools or whatever statistical analysis I want to do to figure out if the budget is correlated with the number of awards.",
            "But today I can't do that because all these steps are out of our reach.",
            "So I'd like to I'd like you to notice, specially the red font over here.",
            "So one is in plus.",
            "So that means M is of category French movie transitively, so there may be finer subdivisions of French movies.",
            "I'm not interested in those, I want to bridge across those and just detect the fact that M is a French movie The second Red font is in in context, and that's something we'll spend some time on, which is can I get snippet segments or token segments where there's this?",
            "Evidence of connection between two or more free variables in the query.",
            "OK, like F and A or F&P.",
            "The last one is aggregate, which is the most dangerous of all, which is given a bunch of snippets where this kind of evidence appears in a noisy fashion.",
            "How are we going to put together the evidence from multiple snippets and rank answers in this case?",
            "Unit of answer is MA&B, so what's the score of a couple like that?",
            "For single items?",
            "We do have some answers, but for multiple column tables like this, it's very unclear how to order the rows in the final game."
        ],
        [
            "So a few disclaimers are in order here, so clearly I'm talking about queries which look sort of esoteric as against mainstream queries.",
            "If you look into any commercial search engines query logs, you'll see that those queries look nowhere close to what I'm talking about, and that might sort of bring forth protest that this is.",
            "This is a niche thing to do, and very few people care about it.",
            "Um?",
            "Becaused you know the last bullet goes commercial interests are focused on traffic and revenue, whereas at this conference at least we are also interested in ideas and prototypes.",
            "So it's more like what if we did this somewhat speculative, as against we already have a broad user base which supports this kind of queries and I'll be talking about public domain work being an academic outside the search companies, so there might be identical or even better things going on inside the search.",
            "Companies have no idea about that, and you know.",
            "In Standard search we know about things which already work.",
            "We use them dozens of times a day.",
            "What I'm going to talk about today may not work all that well immediately, but that's where we want to go eventually."
        ],
        [
            "So of course this is not entirely out of the blue.",
            "There is a lot of supporting work and background and influences coming into the proposal for searching the semantic.",
            "The annotated web.",
            "Then of course, the very well known earlier projects, Web K. We know it all, and web reading in CMU and UMass in and you wash, there's the NLP communities working with question answering NLP, part of speech tagging, entity resolution.",
            "Works into the immigration more recent machine learning and statistical information extraction coming up to the right side.",
            "Here a lot of database work on XML query languages, XML search, RDF, SPARQL and so on, and then another set of database projects on uncertain or probabilistic databases, which I expect to get into action a lot more in the coming months and years.",
            "And then of course the whole thing is closed by having your idea there, which might bring all of this even."
        ],
        [
            "Better Together.",
            "So here's sort of my.",
            "Picture of the puzzle and I'll go over some of the pieces one by one in this talk.",
            "So of course we have a raw corpus.",
            "The first thing you have to do to realize this vision of the annotated web is to actually annotate it and the way you do that is in two steps.",
            "One is to find spots or snippets contexts where you suspect that a real world entity is being mentioned and the disambiguate are where you actually figure out which entity it is after your done with that, you end up with.",
            "A bunch of annotations.",
            "In a database.",
            "And then the original corpus, as well as the annotations go into an indexer which is number 2 on my list and that forms a Composite Index between the the unstructured text and the annotations to the structured knowledge base.",
            "So here is the structure knowledge base as the centerpiece of the story.",
            "It has a catalog of types and entities and I don't want to define two precisely what I mean by that.",
            "You could think of something like word net with a lot more entities hanging off it.",
            "Or you could think of something a little more chaotic, like Wikipedia with this category graph, which is not even a DAG, and then have a lot of entities again connected to these categories.",
            "This might be maintained by experts, as in the case of ordnet, or it might be done socially as in Wikipedia.",
            "Ideally we would like to use to harness social energy in maintaining this catalog of types and entities, because otherwise it won't scale with the evolution of the web.",
            "After that, in the last step here I'm talking about query processing, which uses the Composite Index between the corpus and the structured or semi structured knowledge base.",
            "But one thing that's hidden here is that query processing will now inherently involve both aggregation and ranking in a big way.",
            "Becausw, no matter how good you make it, this part of the design will never be perfect.",
            "You will be linking up things which don't make sense.",
            "And then the user will ask queries which again have.",
            "Both recall and precision guiding problems.",
            "You have to somehow tide over both of these through the mechanisms of aggregation and ranking, and those are very, very integral parts of the block.",
            "Number 3 here.",
            "So at this point I'll dive into the three parts of the."
        ],
        [
            "Lock so the first thing to do is to spot where entities may be lurking and then figure out which entities they are.",
            "So here's a snippet of random text from the web.",
            "The lack of memory and time efficient libraries, etc.",
            "So there are certain tokens which are worth connecting things in Wikipedia or Wordnet, and there are some tokens which are not worth attaching.",
            "Eventually that's really driven by queries.",
            "We think that linking Dar in this context is meaningless or linking and is meaningless, but if you look up Wikipedia might be surprised because you know if it's the name of an island.",
            "There is an island called if in Wikipedia, so nothing is a stop word necessarily.",
            "And so while you're scanning to this corpus, mention can be any token segment like the red ones, which is worthwhile linking to Wikipedia like memory in this context doesn't mean human memory, it means computer memory and that is immigration might help you out someday when someone is asking a query.",
            "Similarly for libraries, libraries have very many meanings that we'll see in the next slide.",
            "So any of these red tokens is a potential mention, and sequences of red tokens may also be a mention.",
            "It's just a compound.",
            "And the mention, together with some left context in the right context is called a spot.",
            "A spot is vaguely some minimal segment of text which gives you reasonable clue about what entity it might be linking.",
            "Mentioned this works may overlap.",
            "For example when people talk about hashing, there's a meaning to hashing which is independent from perfect hashing.",
            "You might want to link both of them.",
            "Because different queries might look for aggregation across different granularities or in some cases you may want to make one tagging hide the other tagging.",
            "If you're sure we're talking about New York, you do not want to link York, and these are sort of well understood things from an LP, but it's never been done at this scale before."
        ],
        [
            "Spotting basically involves a massive similarity join between segments in the corpus and your knowledge base, so they're saying as you scan this particular document from left to right, you see New York Times what existential E it could be.",
            "Three different things.",
            "It could be York, it could be New York.",
            "It could be New York Times and depending on how you look this up, you'll find that York has.",
            "Cured matches with various levels in your Wikipedia entity catalogs.",
            "York University, Duke of York, New York, has matches with New York State City, York University.",
            "Again because there's one word overlap with smaller overlap this time, and if you look at New York Times, the whole thing you again get the newspaper and maybe other things which are partially overlapping.",
            "So and then you look up library and you find in Wikipedia huge collection of articles with library in their title, a collection of books.",
            "Which is the right meaning in this context but also library in computing library in Windows 7 which is different from library and computing as well as Library of Congress library or Transit Station.",
            "In Utah and so on.",
            "So Wikipedia other snapshot sometime back had about 2 and half million entities with 2.8 million Limassol.",
            "Emma is a token sequence describing an entity, so a single thing may be described by multiple limoz and the number of total unique tokens in all the layers is about 7 million.",
            "So to pull this off for every token you scan in your 10 billion page web corpus, you need to be looking up the 7 million key database.",
            "And then do some scoring in the scoring is non trivial as I've already hinted.",
            "How can you decide between York and New York and New York Times?",
            "In this case, the three yellow rectangles and various things come into play.",
            "The idea of the inverse document frequency of each token comes into play.",
            "The gaps you're missing between the label and as you're scanning the corpus comes into play.",
            "What are the tokens you're missing?",
            "What is the idea of the missing tokens?",
            "What's the token length of the candidate lemma and the token length of your segments?",
            "Man, the ratio of that actually is a very important feature.",
            "So all of these?",
            "Make the going more difficult and today the some sort of University level code we can spot about.",
            "5200 documents per second on A8 core node.",
            "So is that as good as indexing something with Lucy or Notch?",
            "Is it much slower?",
            "Is it much faster so it can be much faster?",
            "So how much are you slowing down the works?",
            "Because we want to do this similarity search as you're going along.",
            "So next step after that is disambiguation.",
            "Once I've figured out the.",
            "The red segments.",
            "Here I'd like to find out which entity those are talking about."
        ],
        [
            "And that's actually more well defined problem.",
            "So let S be a spot with the mentioned somewhere in it of some entity and some surrounding context and let gamma sub S with a set of candidate entities for S. So library had those candidate sets below it.",
            "Let small gamma in gamma SP One candidate entity.",
            "And we have remembered that is my best be left unconnected, so most John Smiths mentioned on the web are no John Smith in Wikipedia.",
            "And here's a small footnote here, which is if you see a person name on the web, chances are they're not in Wikipedia, whereas if you see a car model name or the name of a constellation in space, chances are they're already in Wikipedia.",
            "So here is already a situation where our world knowledge gives us a nice shift in prior about whether we want to link up or you don't want to link.",
            "So embedding these in learning programs are still not done.",
            "Might be very nice to see how that works, but coming back here so no attachment or Na is a valid class and it's a very frequent class, so you decide something in the spot, but you claim confidently that there is no counterparty.",
            "Wikipedia that's important too as well.",
            "Otherwise your later queries are going to be get totally found in a bad position, even if the recall is high.",
            "This is a generalization of works integration LP and Natural language processing, but it's not quite the same.",
            "Becausw NLP typically deals with closed common spaces of words, senses of multiple words, and typically there's a lot of training data available.",
            "Within the scope of your corpus, whereas here it's like open domain disambiguation, you can never have enough training data for every entity in Wikipedia.",
            "This off work even within the context of open domain disambiguation.",
            "So the very influential paper, some tag and seeker in the web conference around 2003.",
            "Recent papers called Wikify paper by UNESCO and password using tree kernels for doing this kind of matching into immigration and several others.",
            "So I'm going to talk about the last paper which we wrote last year, which seems to improve on many of the numbers published."
        ],
        [
            "The previous papers.",
            "So what kind of information can we use to decide on the disambiguation?",
            "If you see this snippet of text, it's clear to us there is talking about the car, and that's clear from the green works automotive tech, red backlight on the engine start button.",
            "However, there are confounding words which are marked in red pulses and heartbeat and so on.",
            "In spite of that, it's fairly clear from this local context that this is talking about Jaguar, the car, not the Jacksonville Jaguars or Jaguar.",
            "Dan and I represent backed by this kind of a funnel diagram here, so the document has a bunch of spots, the red, the black circle is the mention of the center of the spot, roughly.",
            "And S can resolve to any entity out of this set, gamma service and small gamma happens to be one of them.",
            "And the decisions you're making would be independent across parts.",
            "You decide on what Jaguar means up there.",
            "And you decide on engine down there in our independent way for the lower fan engine.",
            "In this case don't mean a search engine.",
            "It will mean a car engine and so on, right?",
            "So do this demonstration."
        ],
        [
            "Independently.",
            "How do you do that so the standard machine learning for it and what we do is to declare a vector of features called F sub gamma of SFS of gamma.",
            "Which is composed of a bunch of feature elements, each element in that feature vector is a function of the textual content of the spot S and some metadata or description associated with gamma, and you can use a bunch of things you could use, say the TF IDF cosine match between the full Wikipedia page down below there and the snippet or the anchor text of known links into the Wikipedia page and the surrounding text in the snippet.",
            "And so on.",
            "And each of them will give you 1 feature.",
            "But we can learn to combine them using standard machine learning and that would give us some kind of a model W during inference or during application of the model.",
            "You would look for that gamma which maximizes W, transpose, FS, gamma.",
            "So we know how to wear these things carefully and in experiments have found this to be much better than heuristics or even single features.",
            "So if we combine all these features, suddenly we get accuracy which is far more than any of them.",
            "That's quite surprising for us.",
            "That's what the word gap."
        ],
        [
            "Was.",
            "And then we can start exploiting collective information before getting into this example, suppose you see the word Michael Jordan on a webpage.",
            "Prior probability suggests is the basketball player.",
            "But if you see both Michael Jordan and Stuart Russell on the same page, then they're most likely both Berkeley professors, so somehow cooccurrence sway is your opinion on disambiguation alot.",
            "And here's.",
            "The same example, the second different context.",
            "So Michael Jordan could be either the Berkeley professor or the basketball player, or even an actor.",
            "And Air Jordan could be the Jordan Airlines, or it could be like issues.",
            "So in each case I could make independent decisions, but if I made the right decision in each case, suddenly everything would fall together.",
            "Would somehow get more goodness out of the page than if you link things at random.",
            "So let Y sub S be the variable which represents the Entity label for spot S. So why text values over gammas?",
            "We want to pick all the wise together, somehow optimizing a global objective.",
            "And what should the global objective be?"
        ],
        [
            "So here's a better diagram where suddenly the gamma and gamma prime are connected.",
            "So on the left is.",
            "This is the old spot to label compatibility considerations, but on the right you see that entities are now being embedded in some space G through a function G. So gamma goes to the vector G of gamma and gamma prime goes to the vector G of gamma prime, and then we can measure some sort of cosine similarity or distance in that space.",
            "So because I'm just giving an overview in this talk, I'm going to skip how to define G. We have some definitions which work well, but we're by no means close the story so.",
            "Better designs are possible and the overall objective is going to maximize some combination of local compatibility as well as global coherence pairwise between all the decisions you make on a page.",
            "First of all we need to validate.",
            "Is this making sense?",
            "In other words, if we design this global objective when we try to maximize it, does it actually improve the quality or accuracy?"
        ],
        [
            "Your annotations and answer is very strong, yes.",
            "So on 6 arbitrary sample documents we ran a local Hill climbing optimization to improve the collective objective, and then at every step of the way, we compared the current labeling against hand labeled ground truth annotations, and you find that there's a very high positive correlation as you increase your objective there phone accuracy off the actual annotations go up as well, and this is important.",
            "Verify before you run, you know.",
            "High power algorithms to optimize whatever objective design."
        ],
        [
            "But once we actually optimized the final objective also get good results.",
            "So prior art is around here in recall and precision and around there the lowest curve over here.",
            "Now this is over about 20 two 20,000 spots that we manually labeled on web documents local, which is this curve is just training W. Prior is where local is biased.",
            "Using Wikipedia as distributions of linkage.",
            "So if you look at the name Intel, Intel is the Chipmaking giant, but it's also a 1960s cartel in a BBC commercial serial, so the second sense is very obscure and you can use the ratio of linkage frequencies to those two meanings to guide your local inference, and that's that gives us a huge boost, almost as good as it gets.",
            "But then there's still about a 5% increase in F1 score at the edge where we're really trying to push recall and precision in the green curve, which is to use.",
            "Linear programming, relaxation of an integer program which represents this collective inference problem.",
            "And then when you round it and use that for assigning the labels, we get the best accuracy overall in annotating the text.",
            "And this does include studies on whether you do know attachment correctly or not and how that goes, so that's that's nice.",
            "But there are various lose and still."
        ],
        [
            "Remaining.",
            "We designed G of gamma by hand, so we picked good embeddings off the entities in Wikipedia by hand.",
            "I really want to learn that.",
            "There are certain machine learning tools to try to do that, but nothing at the scale of Wikipedia with 2.5 million elements.",
            "So that's something I'd like to discuss offline.",
            "However, applying the model should remain fast.",
            "Remember, this is being done over every token of the input corpus, billions of pages, so whereas my embedding may become more complicated, I don't want that to slow down my spotting and annotation.",
            "What is the cost of spotting and annotation compared to basic indexing?",
            "When we are reading a page, we already have a topic model of what this page is about.",
            "If I think of your New York City approving certain budget for school libraries.",
            "The whole document totally drives out weird meanings like software libraries out of my mind, and so even though the Entity dictionary is 7 million 7 million large, I don't want to be looking of looking up all of them.",
            "They are scanning a particular document, so can we use page or site level features to drastically prune the number of candidates we actually have to consider Puerto can?",
            "Can we use that to speed up the accuracy and also improved accuracy?",
            "Finally, this cannot be done reliably unless this becomes some sort of social presents.",
            "Can we use some sort of active learning together with social tagging to improve the system?",
            "So today you see a certain kind of tagging over the whole web, you go into content that you're interested in.",
            "You see taggings that look wrong, and you fix it, and in the next cycle that's taken into account to improve the learner, right?",
            "So it's sort of like translation tools on the web, so when you get a translated page, you can fix the translation and that goes back to improve the translator.",
            "So similar to that, can we build an annotated web by having people actually fix it?",
            "As time goes by, source of lifelong learning setting."
        ],
        [
            "So that finishes what I'm going to talk about.",
            "The first module, which is spotting plus disambiguation, and now I'm going to talk about some work in how to index those things.",
            "And there are some challenges in doing that.",
            "So feel free to interrupt anytime you have a question."
        ],
        [
            "So the second stage we're worried about that in context part of queries.",
            "So remember like I want.",
            "A motherboard and opted on in the same context.",
            "So how do I solve that?",
            "So let's pick another query scientist who studied Wales.",
            "So I might like to say S is a scientist, but if I'm better informed I might want to say S is a marine biologist.",
            "There's the usual recall precision tradeoff in doing that, and it also differs a lot based on who is asking the query.",
            "Remember that magic box in my very early slide, where the end user is going to the Magic box and that's turning their end user queries into this status of this kind of queries so.",
            "Um?",
            "There may be some uncertainty about which of scientists and marine biologist is the best fit for my information need, and then I ask things like in a context CS and the following words should appear in this word should improve the score of the snippet.",
            "So studies or study dorweiler Wales.",
            "I might plus one of will or wills.",
            "Or it could be stamped.",
            "So the first option to solving this is quite impractical, so you do query expansion.",
            "Do able to ask which scientist studied whales you ask?",
            "Did Einstein study?",
            "Who else did Niels Bohr study ways to draw the food study waves?",
            "In other words, you only have a ground constant index on the whole corpus and then you know what all entities scientist expands to try to verify which hypothesis holds.",
            "And that's totally impractical, because even Wordnet knows about 650 scientists and 160 cities.",
            "If you go to the Wikipedia game is totally over."
        ],
        [
            "So the other option, which is the only practical option, seems to be index expansion, which means that anytime you see the word cluster in a particular web page, you expand out to scientist, person Organism, living thing entity.",
            "And hope that one of these will be queried by users later on.",
            "In this case scientists.",
            "And then pretend that all these tokens appear wherever Cousteau does in their index.",
            "This works OK for small type sets like maybe 5 to 10 broad types.",
            "So project called Entity Tank from Arbana Kevin Shanks Group did something like that about 15 types.",
            "But for open domain work, that's not enough, because word NET has 15,000 internal noun types and 80,000 total leaf plus internal noun types, and Wikipedia has over 250 thousand categories.",
            "So if you really wanted to index all of these types, that would explore the index unacceptably."
        ],
        [
            "So one fix we propose a few years back is to index, not the entire set of answer types.",
            "A button index, some carefully chosen subset R, and then the query paradigm would be sort of a pre generalizing post filter paradigm.",
            "Suppose the query answer type is scientist as I said, but you find that unfortunately the scientist which is a query answer type is not indexed.",
            "You walk up to some clothes indexed answer type which is living thing.",
            "G. The generalization, and then you ask find me living things near Wayland study.",
            "And you'd end up with more answers because all of them have a loss of precision.",
            "So both Cousteau and will appear to be living things.",
            "And in the post filter stage, you need to figure out that as far as we know, outside the world of Hitchhiker's Guide to the Galaxy, you know whales are not scientists as far as we know and therefore will have to be eliminated from the answer set.",
            "And this takes.",
            "This requires using a so-called forward index and reachability index, which I won't go into the details of, but then some number survive this pruning step, and if enough don't survive year to restart the pipeline with larger value of K prime, which is expensive, But that's the main parroting.",
            "Very simple query processing paradigm, but the important question is OK.",
            "This is how you process queries.",
            "How would you pick R, that's the."
        ],
        [
            "Request so the cost is the space taken by R. And how much space can you save in the index?",
            "Because you chose our instead of the whole of a.",
            "The problem is we can't try out an exponential number of subsets of A and the benefit is or other inner space saved is the benefit and the cost is how much your queries are slowing down because you have to do this generalization and post filter and that depends on the query workload and we can't afford to test it on too many queries.",
            "So in 2006, paper in the web conference, we give some algorithms for doing this balancing."
        ],
        [
            "Between.",
            "Index isan query slow down.",
            "And this is what we found is a very small scale experiment with a track corpus only about 5.7 gigabytes.",
            "The GZIP corpus, however, which is some indication of information content, is about 1.3 gigabytes.",
            "The STEM index was about the same.",
            "So it's a good compression if we index the full type system in Word net you take 4.3 gigabytes, which is much larger than the gzip corpus and the same index.",
            "However, if you carefully chose the subset that you want to index with the track query workload, which is what we used, you could cut down to an index of only 520 megabytes, which is less than the inverted index size, and your queries would slow down only by a factor of 1.8.",
            "But the important question is, does this scale to the web?",
            "How do you actually hunt around for the small set R amidst a sea of Wikipedia categories?",
            "Hopefully most of them are pretty obscure, but it does have very obscure categories.",
            "But still, it's kind of unclear as to whether you can pull off this kind of a trick with the whole web corpus and Wikipedia.",
            "That's something that we're working."
        ],
        [
            "So that that's all I have time for.",
            "For the second part.",
            "And finally I'm going to.",
            "Go over to how to do query processing, specifically aggregation of evidence.",
            "Once you get access to this kind of indices."
        ],
        [
            "The basic picture to have in your mind is that the query now has four simple kinds of queries as a one type specification, which motherboard and words would be like opted on?",
            "Or PCI Express slots?",
            "And the green words match green words directly in various snippets.",
            "The pink circles in the snippets are possible mentions of entities which have the given type.",
            "So as you can see, the first entity one second is also even.",
            "Sorry about that.",
            "The first entity you want is supported by only one sniper, but there's lots of keywords near it.",
            "The second entity is supported by three snippets, but each of them have fewer keywords matching it.",
            "So how do you play this off?",
            "Is there established theory for scoring and ranking entities in this setting?",
            "And this is fairly ripe for trying to use something like personalized page rank.",
            "We have not tried it, but it seems plausible if you do it right.",
            "That you can inject some sort of teleporter energy into each of your words.",
            "Those spread out over snipers.",
            "Then they collect into the entities and see how they rank.",
            "Anyway, but clearly describe something a little different so you know this context is a candidate because it mentioned an entity of a target Type 4 score for context, and then if you have multiple contexts like the three for the second entity, how do you aggregate that into?"
        ],
        [
            "Evidence.",
            "So for textual matches here some proposal.",
            "So the query is in context C, something called P, which is a person together with the stem, invent and television.",
            "So who invented television?",
            "That's sort of the goal, and I want to aggregate over context to get a ranked list of people who might have invented the television.",
            "I hope that the right one is at the top.",
            "So there is considerations about.",
            "The activation or energy at the red node, given the matches at the green and blue nodes.",
            "The reality of the matches.",
            "So if you see in the corpus the television is rather than invent, then television should pump in more energy into the red node as compared to invent.",
            "And that's shown by the higher general profile of the green curve as compared to the blue curve.",
            "If there are multiple occurrences of a certain query word, you might want to pick the closest one.",
            "Or some other aggregation of them.",
            "And then from multiple selectors, if you want to combine their effect, maybe you can do some, but maybe you can do some sort of softmax.",
            "So learning these kinds of functions are also very open.",
            "I mean I've not seen any work on sort of a generic framework for learning proximity functions, although cigar last year had a couple of papers and approximately language models, that's a area to watch, would know what's happening.",
            "And also we don't really know that increasing distance from a match monotonically decreases evidence at a point as.",
            "Hinted by the red and the green and blue curves, it may not be monotonic are experiential.",
            "It's not one.",
            "In fact for English.",
            "Anyway, but then there's this question of, you know, if I see John Baird on one document and just bear down another document.",
            "Those should somehow conspire to improve the score of John Bird.",
            "And how can."
        ],
        [
            "That be done.",
            "So the nice couple of papers, one from CMU in Cigar 2007 and another one in www.two 1008 which does roughly what might be interpreted as Laplacian smoothing or Laplacian scoring.",
            "We represent each snippet with the feature vector XI and the local score of a snippet is some WXI in the usual linear scoring model and then one defines this notion of affinity between mentions in snippets.",
            "For example, if one snippet says Andrew McCallum, another snippet says.",
            "AK McCallum.",
            "Then you suspect there the same, and AIG represents that affinity in case of quantities.",
            "That's a more trickier game.",
            "What's the height of a giraffe?",
            "One document says 18 feet, another says 19 feet, third one says 3 to 4 meters.",
            "Someone to recognize that those are very similar as far as Heights of giraffes and animals is concerned, but you know 3.5 seconds and 3.5 two seconds are very different when it comes to Olympic records.",
            "So what scale you pick for smoothing things out is fairly query dependent.",
            "Anyway, so Laplacian scoring proceeds like this, you try to fit a global score for every node.",
            "I called FI so that.",
            "Your global score and your local score don't deviate much.",
            "So that you pay the reviewer too much and the second thing is the Laplacian term which says that across edges in the graph.",
            "If there is a fact edge between I&J, namely inj look very similar as in the example above, you don't want their final scores to be two different either right?",
            "So you play off these two considerations.",
            "Local score versus global smoothness across the matrix or edges of the graph and then you fit.",
            "If I to get the final scores during training you actually use a very similar formulation with the addition that you now want to fit W instead of F, But you only have partial orders and if during training you say.",
            "The answer to the television inventing question is John Baird and not other things.",
            "So the score of John Barrett should exceed that of any other entity, and we use that together with this expression to fit both W&F in the training examples."
        ],
        [
            "However, local scores can be very, very unreliable.",
            "For example, in the height of a giraffe example with say foot as the unit you wanted in here are three snippets picked not uniformly, but three snippets pick from the top 50 of Google.",
            "First one has the right answer.",
            "It says large enough was a small, was approximately 11 feet because she is still young or full grown giraffe can reach a height of 18 feet, so that's not too bad.",
            "The Green one is right, the yellow one is sort of OK, but the second one says, Gee Rephotography users are telescopic.",
            "Mast to elaborate and eight megapixel camera to a height of 50 feet.",
            "That's wrong answer.",
            "It's in the right unit.",
            "It has the words heightened giraffe on it.",
            "OK, so there's not too much to distinguish it from the first one, unless you're doing real NLP there, right?",
            "And the third one says that record height for a giraffe unicycle is about 100 feet.",
            "So all of these snippets content, all your query keywords.",
            "They have the answer in the right unit.",
            "So you never discriminate between them.",
            "That seems very difficult unless you understand what's being said.",
            "The third one is even more interesting.",
            "How far should a raccoon be located in Miles?",
            "Look at the 2nd 116 deer 2 Fox is 1 skunk and two raccoons are cited during 135 mile Drive.",
            "So it seems like an impossible problem.",
            "Can you really avoid deep NLP while trying to aggregate out these kinds of noises?",
            "Get the right answer.",
            "The surprising answer is yes, we can actually use some sort of wisdom of the crowds if you will to actually hunt for."
        ],
        [
            "Right kind of intervals.",
            "So somehow the problem changes from scoring documents and snippets into hunting rectangles over a 2D space and this is what I mean.",
            "So each of this for one query, the X axis is the quantity for uniformity, normalized between something like zero and one.",
            "the Y axis is the local snippet score which I got out of training scoring model.",
            "So more score means it looks like a more promising snippet.",
            "Less score means it's less promising snippet, and if OK and the pluses are delivering snippets which actually have evidence of your answer.",
            "And the circles are irrelevant secrets which have some of the query keywords and even a quantity in the right unit.",
            "But they're not correct.",
            "So as you can see, there's hardly any connection between the Y coordinate and correctness.",
            "If your learning of W are perfect, you'd actually in each case have some horizontal line so that all the pluses would lie above the line and all the circles would lie below the line.",
            "But that never happens because without doing an LP these snippets all look the same to a standard IR ISH search engine.",
            "So as you can see.",
            "What saves you is the observation that correct answers are in, by definition, tightly packed rectangles.",
            "And then everyone rectangles.",
            "For example, if you ask about cordless phone frequencies, there are two broad clusters.",
            "As you'd expect.",
            "Tufts actually is a very interesting case.",
            "This is the 18 foot cluster that's a 6 foot cluster.",
            "The six foot cluster comes out of the fact that when a giraffe is born, it drops to the ground through a height of 6 feet.",
            "And still lives.",
            "So that's gonna be interesting.",
            "You might even argue that that's not a bad second answer to give just based on truth by democracy, but we need to learn that this is the best cluster, the right big rectangle over here to the right is that is the correct cluster.",
            "So this is how rectangle."
        ],
        [
            "Something comes into the picture, and here's sort of the schematic.",
            "Hopefully what you have is kind of a sea of or cloud of relevant clusters with a few relevant green rectangles sticking out of it, and there should be some feature which lets you pull out the green rectangle out of the red mess.",
            "So the question now is how to detect and rank these so-called conscientious rectangles.",
            "The challenging problem is that the position and shape or width and height of all these rectangles vary from query to query.",
            "Necessarily some queries may have multiple rectangles, some kind of a single rectangle there in different places, so you cannot use a standard non linear discriminant approach to solve these kinds of problems.",
            "You cannot throw standard machine learning techniques at it.",
            "So this is actually more related to what's called Special scan statistics type work in databases and data mining."
        ],
        [
            "So skip the details of how it is done, but here the results there.",
            "That's really speak for themselves.",
            "If we use rank SVM to score each snippet independently, get the lower baseline, which is understandable.",
            "You have no idea whether 18 feet in 19 feet are similar.",
            "If we use the Laplacian technique that I mentioned before you get a slightly higher result.",
            "On the X axis, the X axis is the tolerance percentage that the user specifies with the query, so I'd like to know the height of a giraffe to within 20%.",
            "The distance between the Sun and Pluto is a much more uncertain affair and changes all the time through a large factor.",
            "So you certainly the distance between Sun and Pluto within 50%, but you were trying to figure out Olympic records.",
            "You'd like to know the answer to within 1%.",
            "So that's something that's essentially application dependent and at the moment we don't see a.",
            "Big where to second guess the users needs, but our algorithm is also very insensitive to what particular into pick.",
            "All the curves are fairly flat, so the tolerance percentage is between 0 and 10% here, and if we learn new ways to score intervals on the X axis properly, then we get the third line which is this.",
            "Pink line over here and then.",
            "Finally we actually exploit collective features across all the snippets content in each interval and get.",
            "This gives us the best accuracy of all, which is really large in information retrieval terms compared to the baselines.",
            "So this appeared in Cigar last year.",
            "Go look at that, but again, this is not really a closed question.",
            "We know how to we now know how to point out single intervals in response to queries talking about quantities.",
            "But what if I want a table of things?",
            "Can I use the synergy between two quantities to decide that?",
            "Give me Michael Answers which are more accurate than if I was ranking for every each column separately or if I'm asking for the height of.",
            "Land mammals.",
            "Can I somehow shrink it?",
            "The information to broader categories in Wikipedia to decide the dancer can never be 2 miles.",
            "When we scan documents for quantities, we always use our prior knowledge over the world to scan numbers properly.",
            "Even without deep linguistic attention to the page, we will never participate and see the weight of a digital camera is 5 KGS.",
            "So we somehow use broader notions of object categories in Wikipedia ordinate or in our minds, to decide what plausible quantities can be.",
            "And that can help a lot, so we haven't really."
        ],
        [
            "That so to round up the talk.",
            "We have explored how to open up new information pathways between documents and between documents and semi structured knowledge bases.",
            "So if you thought the web world was complicated, had 10 billion documents, it just got more complicated.",
            "'cause now the number of links is no longer the number of macroscopic hyperlinks between pages, but it's equal to the number of this microscopic annotation.",
            "Links between pages and Wikipedia.",
            "So the number of links have just gone up to maybe 100 billion right?",
            "And we need to search across this graph which has become.",
            "More dense as compared to the web graph.",
            "It probably has properties which are quite different from the social network properties that we usually come to expect.",
            "Ranking in these graphs is a totally unexplored area.",
            "And how to really know access into this richer information representation is still quite unclear.",
            "I'll just proposed the beginnings of 1 query language somewhere between you know really structured SQL and the completely unstructured text query languages.",
            "Where is the sweet spot that will require a lot of participation to decide.",
            "I would however really lobby for trying that out because for several years now we seem to have kind of cost the interaction.",
            "Mode in concrete and so that no matter what you do about web search engine research, the end interaction is going to be the same.",
            "It seems like you know breaking through that restriction can open up more things to do.",
            "Right, so if only we could agree on certain important constructs of the next generation query languages, maybe there will be more fun research to do.",
            "So we had a panel at World Wide Web last year and there was quite some agreement that people want for specific vertical applications.",
            "They want entities and types for sure.",
            "We've been trying not to talk about all this in a vacuum, so we are prototyping with about half a billion pages and about 40 hosts.",
            "So we sort of roughly know what's going on, where the performance pinches, and what are the important considerations of recall versus precision and so on.",
            "And you know the right question to ask is probably not what will end.",
            "Users are up today.",
            "Will they start using this overnight?",
            "But really, you know how to fill up that initial box of magic, which is, how can we help end users take advantage of this new composite representation where complicated social organic knowledge network connects to the unstructured corpus and this is done in noisy, unreliable ways.",
            "And yet we managed to distill correct answers out of it because of the redundancy involved because of the diversity involved in the corpus, so that's sort of the broad proposal I'm trying to pitch.",
            "Any questions?"
        ],
        [
            "Any questions?",
            "But some of the questions are raised.",
            "So.",
            "We waited for longer sentences, but there are some things.",
            "Yes.",
            "Right?",
            "Sure, so.",
            "It's a couple of.",
            "Differences first is that if you look at the traditional question answering community.",
            "The query is hardly ever posed in a very structured or semi structured way, so it's sort of the the job of the system to somehow understand natural language in the question and break it up into the type specifiers, the qualifiers, and so on, and that limits the complexity of queries which you can ask later generations of question answering are trying to become more sophisticated.",
            "In particular, database people recognize joins and aggregations in some of the queries today, but there is no direct pathway for.",
            "The question to turn into that kind of structured query.",
            "That's first thing, a lot of question answering teams of course.",
            "Use word net as the annotation mechanism.",
            "I've not kept in touch with the QA community for the last one or two years, so I don't know how much they are using Wikipedia.",
            "Evidence aggregation in question answering is a sparse space.",
            "There were these two or three nice papers and associative ranking of the answers, but there's not that much in terms of evidence aggregation.",
            "And finally, if your answer is not a single entity, it's not entity ranking or expert finding, but the expected answer is like a bunch of couples out of a database, then question answering has very little to say.",
            "Studer first of all, I love your vision.",
            "I think it's terrific and very exciting.",
            "It's a big vision and the question I want to ask is where do you think you talk about a lot of pieces and how they might be done, both given you thought about this problem, where do you think the hardest subparts of the problem are?",
            "Where do you think are the weakest link in your own argument about this issue?",
            "The two weak points, the first is too.",
            "Have annotation be accurate enough.",
            "And the reason why that's difficult is because it's eventually goal oriented.",
            "You'd like to answer queries properly with it.",
            "It doesn't matter if you.",
            "Loste recall on 100 million tokens.",
            "If they don't affect any query unfortunately don't know what queries are today, so it's a chicken and egg problem.",
            "Right, so in fact, in a partial answer to the previous query, to try to benchmark our systems, we actually went to trick.",
            "QA tracks and we collected queries from there and transcribe them by hand to see that we can answer at least those nicely, right, but it ends there and we don't have a rich set of queries.",
            "That is comparable anywhere.",
            "Tour 2, search engine query logs the second week spot is.",
            "Taking ranking beyond single snipers and single entities into really using the.",
            "It's still growing literature on certain databases to have formally interesting things to say about ranking of couples in the output.",
            "So if I'm asking for a three column table and there are interesting correlation structures across those, what's my confidence in every row that I'm emitting?",
            "So if you have looked at some of the uncertain demonstration device community, all kinds of nice algorithm kinda structure ideas, but hardly anyone knows how the public numbers are stuck in the table in the 1st place, very little known about that.",
            "So when I say I looked at a 5 million tokens or 5 million snippets on the web.",
            "And here's my belief in a certain fact.",
            "How do you even define that?",
            "Or is it that probabilistic belief is to be ruled out from the very beginning and we should just proceed?",
            "Discriminatively you know?",
            "So my eventual goal is to just get the answer to the top.",
            "Don't ask him out about any quantitative belief.",
            "So which path will be better is very open at the moment.",
            "So this sort of depends on what the query language.",
            "I'm just looking like, but I was wondering if you were to take sort of a state of the art traditional IR technique to answer these queries.",
            "And then you take your answers.",
            "What sort of percentage of the top of the IR results are going to match the results you return?",
            "I like.",
            "I mean, the idea is if your queries are expensive, execute.",
            "Can you do a pre query stage in traditional IR?",
            "Just reduce the number of documents that we're really talking about returning over.",
            "Right so.",
            "Some part of the aggregation cannot be pushed to preprocessing unless you're basically caching frequent query answers already.",
            "So, so where do?",
            "Standard error indices and access methods really fail if you're trying to do these kinds of things.",
            "I think the very first step should be to index types as soon as your.",
            "Index can bring forth preferentially snippets with just don't have your credit key words in it, but also something of the given type that will give you a huge boost.",
            "So in the quantity search example, we actually used Google Snippets and we got a certain level of accuracy.",
            "But in Google you cannot really say from the end user API.",
            "Give me a snippet, witches, giraffe and height and something that actually looks like a linear distance quantity.",
            "Right, it just happens that for this particular query Google is great, but especially for number of queries like counted queries, you don't have any control on Google to say this.",
            "Looks like a number and.",
            "So if you cannot control the fact that the snippet should have your answer type on it, then you have lost half the battle, so that's the first thing that indexing and should do.",
            "So we did other experiments where we actually indexed our tiny corpus, half a billion page corpus with types, but it's not comparable with what you're getting.",
            "Google because Google has many more documents in a different ranking function, right?",
            "So that's why it's difficult to kind of give a. Apple to Apple comparison there, but it's really nice to have the identical corpus with really state of the art ranking functions and then say if you could index this, this is more how much better do I get that we did study?",
            "Any questions?",
            "So your your effort does take a certain amount of engineering knowledge engineering to identify the things you want to have you in various stages of process.",
            "The most successful Trek QA systems.",
            "So the closest thing we have to what you're talking about the measure, also have a significant amount of knowledge engineering and it only engineering to make work.",
            "I don't know how much you're familiar with those systems that can you say something about the degree of engineering required to get reasonable performance on some my mom.",
            "So yeah, as you said, I mean, I've not kept track of exactly what trick you does in the last couple of years, but.",
            "As per the last counting, I did the degree of ambiguity of the average mentioned in track is much smaller than the degree of ambiguity of any particular token in the Wikipedia dictionary, so accuracy can fall and as has been known, in standard IR and disambiguation, unless you're doing this immigration well enough, you don't want to use it in higher actually.",
            "So you need to reach a certain threshold of quality before you can use aggregation over annotations properly.",
            "If I'm pulling in arbitrary garbage at every spot and saying this might link to that guy, you know I might bring up wrong solutions a lot of times.",
            "So Interactive works the corpus recently.",
            "Right, so that's.",
            "Really specialize in our system storage.",
            "You got served.",
            "What's the relative scale?",
            "So having experienced what it takes to even look up a 7 million key dictionary for every token and then do score aggregation across snippets of segments of size 5 or less, which is what you need for Wikipedia most of the time.",
            "It seems like you know even dependency parsing of every sentence on the web is something you would like to avoid at all cost.",
            "At least as of today.",
            "The.",
            "So what can you do?",
            "What else from that community can you import?",
            "OK. OK, I I would also agree with Tom that this vision, I guess, can I please interrupt and go back to one.",
            "On one hand you don't want to exploit natural language.",
            "Sentence is if that takes too much time.",
            "But on the web there are other languages.",
            "OK, it's not.",
            "It's not NLP.",
            "It's not English, but there's the language of lists, the language of tables.",
            "There is a language of sight organization.",
            "Those things can help a lot.",
            "I mean, there have been a couple of papers on table mining which have really improved.",
            "So my comment would be in this direction.",
            "So mainly you were just presenting methods which still have pretty much materialized knowledge, so there's no reasoning in the sense that.",
            "You would get facts or connections on the fly which are not even within an index level.",
            "Well, some of the joint queries I'm talking about are sort of like that.",
            "So today I'm kind of restricting to equal joins if you will.",
            "But you can assert other arithmetic predicates on it and so on, right?",
            "It will definitely ethnic Inequality's and see what comes out right.",
            "Motherboards with price at most $400, so those things can be done.",
            "It's unclear what the formal semantics would be of response, rose.",
            "But let's leave out that for the moment.",
            "So I guess I don't really know what you mean by lightweight and what do you mean by reasoning so lightweight means that you would have some heavy deductive reasoning, which, let's say lead you to this psych style of producing effects.",
            "So that way it would be more like using not indexing.",
            "Let's say all connections within sort of willed model, which occasionally you would have to understand.",
            "Your data better, so it's always a question.",
            "How much pre structure you introduce into the data you want to deal with, right?",
            "So so I guess that's really the core of the proposal, which is the preprocessing and the earlier preparation of the data.",
            "I'm just proposing you materialize possible mentioned links.",
            "And that you recognize entities and recognize quantities.",
            "So what the sweet spot?",
            "How can we get 80% of the remaining queries with the first step, right so?",
            "What other forms of logic do you need to impose on this?",
            "For example, frame netlists, about 35 basic relationships in any natural language?",
            "Do I need?",
            "Do I want to tag every occurrence of religion in frame net or my home corpus right?",
            "So maybe that's overkill?",
            "Maybe maybe out of the 35 we can pick 10 and who knows?",
            "So things might work gradually in that direction, but even at the current scale of aggregation it's hard to see all of these queries being interactive.",
            "Maybe the ones that are not interactive will be declared as views and updated once in awhile.",
            "That's another thing I didn't reflect on, which is maybe some of these queries can be declared as views and then maintain so that.",
            "Smaller queries can be quickly composed out of the views.",
            "So I would like to ask, how do you see your vision as compared to the vision of semantic web where the goal is to return the webbing for gigantic knowledge database, perhaps in terms of sophisticated ventilation ship routes like those you think this is perhaps a more practical way to attack on this very, very difficult.",
            "Yeah, there's a little political.",
            "I mean semantically wants to.",
            "Build order from the ground up right.",
            "Where is this like cleaning up stuff which already happened?",
            "To get this running within six months to a year, this may be the only way.",
            "But there's a lot of synergy there, I mean.",
            "Semantic structuring.",
            "When it is done.",
            "Relatively organically and without central control can scale a lot, and Wikipedia knows about new new important entities in the world within hours.",
            "And so if there are more semantic knowledge bases which are maintained thus.",
            "Those should be brought into the fold.",
            "Right, so think of defining an abstraction over semi structured world knowledge and then plugging that into this system.",
            "While understanding that that knowledge base will never engulf the whole corpus because the schema will never be complete.",
            "I think for the sake of time we have to sort of end this session and let's thank the speaker again.",
            "So next session will start in 20 minutes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good morning.",
                    "label": 0
                },
                {
                    "sent": "Welcome to the conference.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about building bridges across the gap between structured data and unstructured web data.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So certainly have come a long way since we started seeing around mid 90s from very brittle ranking functions near duplicate pages.",
                    "label": 0
                },
                {
                    "sent": "Spam attacks to fairly successful spam filtering link assisted ranking result diversification and detecting when you want to Geo sensitive answer and many many other features.",
                    "label": 0
                },
                {
                    "sent": "There's even some limited type awareness in certain verticals.",
                    "label": 0
                },
                {
                    "sent": "You can directly ask today one cagey equals how many pounds or the distance between Rome and Venice or hotels near Brooklyn Bridge.",
                    "label": 0
                },
                {
                    "sent": "Those things work really well.",
                    "label": 0
                },
                {
                    "sent": "However, there remain information needs where the cognitive burden is still fairly large, and that's not directly a problem with the search engine is just because the tasks are somewhat more complicated.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some example queries and I have checked most of them until recently, so there should still be difficult artists who got Oscars for both acting and direction.",
                    "label": 1
                },
                {
                    "sent": "And I'm not even asking that the same movie or not typical price of certain kinds of motherboards with certain features.",
                    "label": 0
                },
                {
                    "sent": "Or is the number of Oscars 1 by a movie directly related to its production budget?",
                    "label": 1
                },
                {
                    "sent": "Say or how many justices serve in the International Criminal Court.",
                    "label": 1
                },
                {
                    "sent": "What was the cleanup cost of some environmental disaster?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And the reason why these queries are difficult?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that we can do various things yet with search engines.",
                    "label": 0
                },
                {
                    "sent": "And indulge me for a moment and don't don't protest that these are not things that end users want.",
                    "label": 0
                },
                {
                    "sent": "Because they probably do at least some of them.",
                    "label": 0
                },
                {
                    "sent": "Search engines provide very good low level access methods to pages, but they don't give you any variables and how complicated queries can you ask without variables.",
                    "label": 1
                },
                {
                    "sent": "I cannot, for example, say that is both an actor and director.",
                    "label": 0
                },
                {
                    "sent": "I can't pin that down in my query.",
                    "label": 0
                },
                {
                    "sent": "There are no types, or at least the types are well hidden inside the search engine.",
                    "label": 0
                },
                {
                    "sent": "I cannot say M stands for a motherboard or P stands for a money amount.",
                    "label": 0
                },
                {
                    "sent": "And then ask where is about them.",
                    "label": 0
                },
                {
                    "sent": "I cannot relate variables using predicates.",
                    "label": 0
                },
                {
                    "sent": "I cannot say that M cells for P or M costs P. And finally I cannot do aggregates.",
                    "label": 0
                },
                {
                    "sent": "I cannot build fairly robust ideas about distributions on money amounts.",
                    "label": 0
                },
                {
                    "sent": "If you go to the web today, you can fairly successfully collect hundreds of web pages talking about the exam Valdez event and then scan the pages manually to find money amounts, but it'll take you 15 to 20 minutes to build a good.",
                    "label": 0
                },
                {
                    "sent": "Reliable statistical impression of what that money amount was a single pages and single snippets onto it for you.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of queries I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "What kind of support do you need to impose?",
                    "label": 0
                },
                {
                    "sent": "Some kind of structural view on the web so that these queries become hopefully commonplace?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What if you could ask queries with support?",
                    "label": 1
                },
                {
                    "sent": "All these things, for example M is a French movie or A is a number.",
                    "label": 0
                },
                {
                    "sent": "B is a money amount or C1 and C2R snippet contexts.",
                    "label": 0
                },
                {
                    "sent": "I'll come back toward a snippet.",
                    "label": 0
                },
                {
                    "sent": "Context exactly is in a few minutes, but roughly speaking it's a sequence of tokens with support some kind of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to ask, is there a context C1 in which a French movie F and number A appear in close juxtaposition together with compulsory word Oscar?",
                    "label": 0
                },
                {
                    "sent": "An optional word one?",
                    "label": 0
                },
                {
                    "sent": "You'd think this would be sort of easy to do, but surprisingly you still cannot have a search engine do this for you.",
                    "label": 0
                },
                {
                    "sent": "Or for the second half of my query, I might be interested in asking if there's a context C2 where the French movie F and a production cost P appear together with the phrase production cost or budget, and then I want to notice that F is a shared variable across the subquery, so that's what's implementing the join.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'd like to aggregate out in some sense the context C1 and C2, thereby getting a single table with a movie name missing of F&M.",
                    "label": 0
                },
                {
                    "sent": "Here, a number of awards won and the production budget be.",
                    "label": 0
                },
                {
                    "sent": "And now I'm free to run my standard lab tools or whatever statistical analysis I want to do to figure out if the budget is correlated with the number of awards.",
                    "label": 0
                },
                {
                    "sent": "But today I can't do that because all these steps are out of our reach.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to I'd like you to notice, specially the red font over here.",
                    "label": 0
                },
                {
                    "sent": "So one is in plus.",
                    "label": 0
                },
                {
                    "sent": "So that means M is of category French movie transitively, so there may be finer subdivisions of French movies.",
                    "label": 0
                },
                {
                    "sent": "I'm not interested in those, I want to bridge across those and just detect the fact that M is a French movie The second Red font is in in context, and that's something we'll spend some time on, which is can I get snippet segments or token segments where there's this?",
                    "label": 0
                },
                {
                    "sent": "Evidence of connection between two or more free variables in the query.",
                    "label": 0
                },
                {
                    "sent": "OK, like F and A or F&P.",
                    "label": 0
                },
                {
                    "sent": "The last one is aggregate, which is the most dangerous of all, which is given a bunch of snippets where this kind of evidence appears in a noisy fashion.",
                    "label": 0
                },
                {
                    "sent": "How are we going to put together the evidence from multiple snippets and rank answers in this case?",
                    "label": 0
                },
                {
                    "sent": "Unit of answer is MA&B, so what's the score of a couple like that?",
                    "label": 0
                },
                {
                    "sent": "For single items?",
                    "label": 0
                },
                {
                    "sent": "We do have some answers, but for multiple column tables like this, it's very unclear how to order the rows in the final game.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a few disclaimers are in order here, so clearly I'm talking about queries which look sort of esoteric as against mainstream queries.",
                    "label": 0
                },
                {
                    "sent": "If you look into any commercial search engines query logs, you'll see that those queries look nowhere close to what I'm talking about, and that might sort of bring forth protest that this is.",
                    "label": 0
                },
                {
                    "sent": "This is a niche thing to do, and very few people care about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Becaused you know the last bullet goes commercial interests are focused on traffic and revenue, whereas at this conference at least we are also interested in ideas and prototypes.",
                    "label": 0
                },
                {
                    "sent": "So it's more like what if we did this somewhat speculative, as against we already have a broad user base which supports this kind of queries and I'll be talking about public domain work being an academic outside the search companies, so there might be identical or even better things going on inside the search.",
                    "label": 1
                },
                {
                    "sent": "Companies have no idea about that, and you know.",
                    "label": 0
                },
                {
                    "sent": "In Standard search we know about things which already work.",
                    "label": 0
                },
                {
                    "sent": "We use them dozens of times a day.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to talk about today may not work all that well immediately, but that's where we want to go eventually.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So of course this is not entirely out of the blue.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of supporting work and background and influences coming into the proposal for searching the semantic.",
                    "label": 0
                },
                {
                    "sent": "The annotated web.",
                    "label": 0
                },
                {
                    "sent": "Then of course, the very well known earlier projects, Web K. We know it all, and web reading in CMU and UMass in and you wash, there's the NLP communities working with question answering NLP, part of speech tagging, entity resolution.",
                    "label": 0
                },
                {
                    "sent": "Works into the immigration more recent machine learning and statistical information extraction coming up to the right side.",
                    "label": 0
                },
                {
                    "sent": "Here a lot of database work on XML query languages, XML search, RDF, SPARQL and so on, and then another set of database projects on uncertain or probabilistic databases, which I expect to get into action a lot more in the coming months and years.",
                    "label": 1
                },
                {
                    "sent": "And then of course the whole thing is closed by having your idea there, which might bring all of this even.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better Together.",
                    "label": 0
                },
                {
                    "sent": "So here's sort of my.",
                    "label": 0
                },
                {
                    "sent": "Picture of the puzzle and I'll go over some of the pieces one by one in this talk.",
                    "label": 0
                },
                {
                    "sent": "So of course we have a raw corpus.",
                    "label": 1
                },
                {
                    "sent": "The first thing you have to do to realize this vision of the annotated web is to actually annotate it and the way you do that is in two steps.",
                    "label": 0
                },
                {
                    "sent": "One is to find spots or snippets contexts where you suspect that a real world entity is being mentioned and the disambiguate are where you actually figure out which entity it is after your done with that, you end up with.",
                    "label": 0
                },
                {
                    "sent": "A bunch of annotations.",
                    "label": 0
                },
                {
                    "sent": "In a database.",
                    "label": 0
                },
                {
                    "sent": "And then the original corpus, as well as the annotations go into an indexer which is number 2 on my list and that forms a Composite Index between the the unstructured text and the annotations to the structured knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So here is the structure knowledge base as the centerpiece of the story.",
                    "label": 0
                },
                {
                    "sent": "It has a catalog of types and entities and I don't want to define two precisely what I mean by that.",
                    "label": 1
                },
                {
                    "sent": "You could think of something like word net with a lot more entities hanging off it.",
                    "label": 0
                },
                {
                    "sent": "Or you could think of something a little more chaotic, like Wikipedia with this category graph, which is not even a DAG, and then have a lot of entities again connected to these categories.",
                    "label": 0
                },
                {
                    "sent": "This might be maintained by experts, as in the case of ordnet, or it might be done socially as in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would like to use to harness social energy in maintaining this catalog of types and entities, because otherwise it won't scale with the evolution of the web.",
                    "label": 0
                },
                {
                    "sent": "After that, in the last step here I'm talking about query processing, which uses the Composite Index between the corpus and the structured or semi structured knowledge base.",
                    "label": 0
                },
                {
                    "sent": "But one thing that's hidden here is that query processing will now inherently involve both aggregation and ranking in a big way.",
                    "label": 0
                },
                {
                    "sent": "Becausw, no matter how good you make it, this part of the design will never be perfect.",
                    "label": 0
                },
                {
                    "sent": "You will be linking up things which don't make sense.",
                    "label": 0
                },
                {
                    "sent": "And then the user will ask queries which again have.",
                    "label": 0
                },
                {
                    "sent": "Both recall and precision guiding problems.",
                    "label": 0
                },
                {
                    "sent": "You have to somehow tide over both of these through the mechanisms of aggregation and ranking, and those are very, very integral parts of the block.",
                    "label": 0
                },
                {
                    "sent": "Number 3 here.",
                    "label": 0
                },
                {
                    "sent": "So at this point I'll dive into the three parts of the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lock so the first thing to do is to spot where entities may be lurking and then figure out which entities they are.",
                    "label": 0
                },
                {
                    "sent": "So here's a snippet of random text from the web.",
                    "label": 0
                },
                {
                    "sent": "The lack of memory and time efficient libraries, etc.",
                    "label": 1
                },
                {
                    "sent": "So there are certain tokens which are worth connecting things in Wikipedia or Wordnet, and there are some tokens which are not worth attaching.",
                    "label": 0
                },
                {
                    "sent": "Eventually that's really driven by queries.",
                    "label": 0
                },
                {
                    "sent": "We think that linking Dar in this context is meaningless or linking and is meaningless, but if you look up Wikipedia might be surprised because you know if it's the name of an island.",
                    "label": 0
                },
                {
                    "sent": "There is an island called if in Wikipedia, so nothing is a stop word necessarily.",
                    "label": 0
                },
                {
                    "sent": "And so while you're scanning to this corpus, mention can be any token segment like the red ones, which is worthwhile linking to Wikipedia like memory in this context doesn't mean human memory, it means computer memory and that is immigration might help you out someday when someone is asking a query.",
                    "label": 0
                },
                {
                    "sent": "Similarly for libraries, libraries have very many meanings that we'll see in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So any of these red tokens is a potential mention, and sequences of red tokens may also be a mention.",
                    "label": 0
                },
                {
                    "sent": "It's just a compound.",
                    "label": 0
                },
                {
                    "sent": "And the mention, together with some left context in the right context is called a spot.",
                    "label": 0
                },
                {
                    "sent": "A spot is vaguely some minimal segment of text which gives you reasonable clue about what entity it might be linking.",
                    "label": 0
                },
                {
                    "sent": "Mentioned this works may overlap.",
                    "label": 0
                },
                {
                    "sent": "For example when people talk about hashing, there's a meaning to hashing which is independent from perfect hashing.",
                    "label": 0
                },
                {
                    "sent": "You might want to link both of them.",
                    "label": 0
                },
                {
                    "sent": "Because different queries might look for aggregation across different granularities or in some cases you may want to make one tagging hide the other tagging.",
                    "label": 0
                },
                {
                    "sent": "If you're sure we're talking about New York, you do not want to link York, and these are sort of well understood things from an LP, but it's never been done at this scale before.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spotting basically involves a massive similarity join between segments in the corpus and your knowledge base, so they're saying as you scan this particular document from left to right, you see New York Times what existential E it could be.",
                    "label": 0
                },
                {
                    "sent": "Three different things.",
                    "label": 0
                },
                {
                    "sent": "It could be York, it could be New York.",
                    "label": 0
                },
                {
                    "sent": "It could be New York Times and depending on how you look this up, you'll find that York has.",
                    "label": 0
                },
                {
                    "sent": "Cured matches with various levels in your Wikipedia entity catalogs.",
                    "label": 0
                },
                {
                    "sent": "York University, Duke of York, New York, has matches with New York State City, York University.",
                    "label": 1
                },
                {
                    "sent": "Again because there's one word overlap with smaller overlap this time, and if you look at New York Times, the whole thing you again get the newspaper and maybe other things which are partially overlapping.",
                    "label": 0
                },
                {
                    "sent": "So and then you look up library and you find in Wikipedia huge collection of articles with library in their title, a collection of books.",
                    "label": 0
                },
                {
                    "sent": "Which is the right meaning in this context but also library in computing library in Windows 7 which is different from library and computing as well as Library of Congress library or Transit Station.",
                    "label": 0
                },
                {
                    "sent": "In Utah and so on.",
                    "label": 0
                },
                {
                    "sent": "So Wikipedia other snapshot sometime back had about 2 and half million entities with 2.8 million Limassol.",
                    "label": 0
                },
                {
                    "sent": "Emma is a token sequence describing an entity, so a single thing may be described by multiple limoz and the number of total unique tokens in all the layers is about 7 million.",
                    "label": 0
                },
                {
                    "sent": "So to pull this off for every token you scan in your 10 billion page web corpus, you need to be looking up the 7 million key database.",
                    "label": 0
                },
                {
                    "sent": "And then do some scoring in the scoring is non trivial as I've already hinted.",
                    "label": 0
                },
                {
                    "sent": "How can you decide between York and New York and New York Times?",
                    "label": 0
                },
                {
                    "sent": "In this case, the three yellow rectangles and various things come into play.",
                    "label": 0
                },
                {
                    "sent": "The idea of the inverse document frequency of each token comes into play.",
                    "label": 0
                },
                {
                    "sent": "The gaps you're missing between the label and as you're scanning the corpus comes into play.",
                    "label": 0
                },
                {
                    "sent": "What are the tokens you're missing?",
                    "label": 0
                },
                {
                    "sent": "What is the idea of the missing tokens?",
                    "label": 0
                },
                {
                    "sent": "What's the token length of the candidate lemma and the token length of your segments?",
                    "label": 0
                },
                {
                    "sent": "Man, the ratio of that actually is a very important feature.",
                    "label": 0
                },
                {
                    "sent": "So all of these?",
                    "label": 0
                },
                {
                    "sent": "Make the going more difficult and today the some sort of University level code we can spot about.",
                    "label": 0
                },
                {
                    "sent": "5200 documents per second on A8 core node.",
                    "label": 0
                },
                {
                    "sent": "So is that as good as indexing something with Lucy or Notch?",
                    "label": 0
                },
                {
                    "sent": "Is it much slower?",
                    "label": 0
                },
                {
                    "sent": "Is it much faster so it can be much faster?",
                    "label": 0
                },
                {
                    "sent": "So how much are you slowing down the works?",
                    "label": 0
                },
                {
                    "sent": "Because we want to do this similarity search as you're going along.",
                    "label": 0
                },
                {
                    "sent": "So next step after that is disambiguation.",
                    "label": 0
                },
                {
                    "sent": "Once I've figured out the.",
                    "label": 0
                },
                {
                    "sent": "The red segments.",
                    "label": 0
                },
                {
                    "sent": "Here I'd like to find out which entity those are talking about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's actually more well defined problem.",
                    "label": 0
                },
                {
                    "sent": "So let S be a spot with the mentioned somewhere in it of some entity and some surrounding context and let gamma sub S with a set of candidate entities for S. So library had those candidate sets below it.",
                    "label": 1
                },
                {
                    "sent": "Let small gamma in gamma SP One candidate entity.",
                    "label": 1
                },
                {
                    "sent": "And we have remembered that is my best be left unconnected, so most John Smiths mentioned on the web are no John Smith in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And here's a small footnote here, which is if you see a person name on the web, chances are they're not in Wikipedia, whereas if you see a car model name or the name of a constellation in space, chances are they're already in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So here is already a situation where our world knowledge gives us a nice shift in prior about whether we want to link up or you don't want to link.",
                    "label": 0
                },
                {
                    "sent": "So embedding these in learning programs are still not done.",
                    "label": 0
                },
                {
                    "sent": "Might be very nice to see how that works, but coming back here so no attachment or Na is a valid class and it's a very frequent class, so you decide something in the spot, but you claim confidently that there is no counterparty.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia that's important too as well.",
                    "label": 0
                },
                {
                    "sent": "Otherwise your later queries are going to be get totally found in a bad position, even if the recall is high.",
                    "label": 0
                },
                {
                    "sent": "This is a generalization of works integration LP and Natural language processing, but it's not quite the same.",
                    "label": 0
                },
                {
                    "sent": "Becausw NLP typically deals with closed common spaces of words, senses of multiple words, and typically there's a lot of training data available.",
                    "label": 0
                },
                {
                    "sent": "Within the scope of your corpus, whereas here it's like open domain disambiguation, you can never have enough training data for every entity in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "This off work even within the context of open domain disambiguation.",
                    "label": 0
                },
                {
                    "sent": "So the very influential paper, some tag and seeker in the web conference around 2003.",
                    "label": 0
                },
                {
                    "sent": "Recent papers called Wikify paper by UNESCO and password using tree kernels for doing this kind of matching into immigration and several others.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about the last paper which we wrote last year, which seems to improve on many of the numbers published.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The previous papers.",
                    "label": 0
                },
                {
                    "sent": "So what kind of information can we use to decide on the disambiguation?",
                    "label": 0
                },
                {
                    "sent": "If you see this snippet of text, it's clear to us there is talking about the car, and that's clear from the green works automotive tech, red backlight on the engine start button.",
                    "label": 1
                },
                {
                    "sent": "However, there are confounding words which are marked in red pulses and heartbeat and so on.",
                    "label": 0
                },
                {
                    "sent": "In spite of that, it's fairly clear from this local context that this is talking about Jaguar, the car, not the Jacksonville Jaguars or Jaguar.",
                    "label": 0
                },
                {
                    "sent": "Dan and I represent backed by this kind of a funnel diagram here, so the document has a bunch of spots, the red, the black circle is the mention of the center of the spot, roughly.",
                    "label": 0
                },
                {
                    "sent": "And S can resolve to any entity out of this set, gamma service and small gamma happens to be one of them.",
                    "label": 0
                },
                {
                    "sent": "And the decisions you're making would be independent across parts.",
                    "label": 0
                },
                {
                    "sent": "You decide on what Jaguar means up there.",
                    "label": 0
                },
                {
                    "sent": "And you decide on engine down there in our independent way for the lower fan engine.",
                    "label": 0
                },
                {
                    "sent": "In this case don't mean a search engine.",
                    "label": 0
                },
                {
                    "sent": "It will mean a car engine and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So do this demonstration.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Independently.",
                    "label": 0
                },
                {
                    "sent": "How do you do that so the standard machine learning for it and what we do is to declare a vector of features called F sub gamma of SFS of gamma.",
                    "label": 1
                },
                {
                    "sent": "Which is composed of a bunch of feature elements, each element in that feature vector is a function of the textual content of the spot S and some metadata or description associated with gamma, and you can use a bunch of things you could use, say the TF IDF cosine match between the full Wikipedia page down below there and the snippet or the anchor text of known links into the Wikipedia page and the surrounding text in the snippet.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And each of them will give you 1 feature.",
                    "label": 0
                },
                {
                    "sent": "But we can learn to combine them using standard machine learning and that would give us some kind of a model W during inference or during application of the model.",
                    "label": 0
                },
                {
                    "sent": "You would look for that gamma which maximizes W, transpose, FS, gamma.",
                    "label": 0
                },
                {
                    "sent": "So we know how to wear these things carefully and in experiments have found this to be much better than heuristics or even single features.",
                    "label": 0
                },
                {
                    "sent": "So if we combine all these features, suddenly we get accuracy which is far more than any of them.",
                    "label": 0
                },
                {
                    "sent": "That's quite surprising for us.",
                    "label": 0
                },
                {
                    "sent": "That's what the word gap.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Was.",
                    "label": 0
                },
                {
                    "sent": "And then we can start exploiting collective information before getting into this example, suppose you see the word Michael Jordan on a webpage.",
                    "label": 0
                },
                {
                    "sent": "Prior probability suggests is the basketball player.",
                    "label": 0
                },
                {
                    "sent": "But if you see both Michael Jordan and Stuart Russell on the same page, then they're most likely both Berkeley professors, so somehow cooccurrence sway is your opinion on disambiguation alot.",
                    "label": 0
                },
                {
                    "sent": "And here's.",
                    "label": 0
                },
                {
                    "sent": "The same example, the second different context.",
                    "label": 0
                },
                {
                    "sent": "So Michael Jordan could be either the Berkeley professor or the basketball player, or even an actor.",
                    "label": 0
                },
                {
                    "sent": "And Air Jordan could be the Jordan Airlines, or it could be like issues.",
                    "label": 0
                },
                {
                    "sent": "So in each case I could make independent decisions, but if I made the right decision in each case, suddenly everything would fall together.",
                    "label": 0
                },
                {
                    "sent": "Would somehow get more goodness out of the page than if you link things at random.",
                    "label": 0
                },
                {
                    "sent": "So let Y sub S be the variable which represents the Entity label for spot S. So why text values over gammas?",
                    "label": 1
                },
                {
                    "sent": "We want to pick all the wise together, somehow optimizing a global objective.",
                    "label": 0
                },
                {
                    "sent": "And what should the global objective be?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a better diagram where suddenly the gamma and gamma prime are connected.",
                    "label": 0
                },
                {
                    "sent": "So on the left is.",
                    "label": 0
                },
                {
                    "sent": "This is the old spot to label compatibility considerations, but on the right you see that entities are now being embedded in some space G through a function G. So gamma goes to the vector G of gamma and gamma prime goes to the vector G of gamma prime, and then we can measure some sort of cosine similarity or distance in that space.",
                    "label": 0
                },
                {
                    "sent": "So because I'm just giving an overview in this talk, I'm going to skip how to define G. We have some definitions which work well, but we're by no means close the story so.",
                    "label": 0
                },
                {
                    "sent": "Better designs are possible and the overall objective is going to maximize some combination of local compatibility as well as global coherence pairwise between all the decisions you make on a page.",
                    "label": 1
                },
                {
                    "sent": "First of all we need to validate.",
                    "label": 0
                },
                {
                    "sent": "Is this making sense?",
                    "label": 0
                },
                {
                    "sent": "In other words, if we design this global objective when we try to maximize it, does it actually improve the quality or accuracy?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your annotations and answer is very strong, yes.",
                    "label": 0
                },
                {
                    "sent": "So on 6 arbitrary sample documents we ran a local Hill climbing optimization to improve the collective objective, and then at every step of the way, we compared the current labeling against hand labeled ground truth annotations, and you find that there's a very high positive correlation as you increase your objective there phone accuracy off the actual annotations go up as well, and this is important.",
                    "label": 1
                },
                {
                    "sent": "Verify before you run, you know.",
                    "label": 0
                },
                {
                    "sent": "High power algorithms to optimize whatever objective design.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But once we actually optimized the final objective also get good results.",
                    "label": 0
                },
                {
                    "sent": "So prior art is around here in recall and precision and around there the lowest curve over here.",
                    "label": 0
                },
                {
                    "sent": "Now this is over about 20 two 20,000 spots that we manually labeled on web documents local, which is this curve is just training W. Prior is where local is biased.",
                    "label": 0
                },
                {
                    "sent": "Using Wikipedia as distributions of linkage.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the name Intel, Intel is the Chipmaking giant, but it's also a 1960s cartel in a BBC commercial serial, so the second sense is very obscure and you can use the ratio of linkage frequencies to those two meanings to guide your local inference, and that's that gives us a huge boost, almost as good as it gets.",
                    "label": 0
                },
                {
                    "sent": "But then there's still about a 5% increase in F1 score at the edge where we're really trying to push recall and precision in the green curve, which is to use.",
                    "label": 0
                },
                {
                    "sent": "Linear programming, relaxation of an integer program which represents this collective inference problem.",
                    "label": 0
                },
                {
                    "sent": "And then when you round it and use that for assigning the labels, we get the best accuracy overall in annotating the text.",
                    "label": 0
                },
                {
                    "sent": "And this does include studies on whether you do know attachment correctly or not and how that goes, so that's that's nice.",
                    "label": 0
                },
                {
                    "sent": "But there are various lose and still.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Remaining.",
                    "label": 0
                },
                {
                    "sent": "We designed G of gamma by hand, so we picked good embeddings off the entities in Wikipedia by hand.",
                    "label": 0
                },
                {
                    "sent": "I really want to learn that.",
                    "label": 0
                },
                {
                    "sent": "There are certain machine learning tools to try to do that, but nothing at the scale of Wikipedia with 2.5 million elements.",
                    "label": 0
                },
                {
                    "sent": "So that's something I'd like to discuss offline.",
                    "label": 0
                },
                {
                    "sent": "However, applying the model should remain fast.",
                    "label": 1
                },
                {
                    "sent": "Remember, this is being done over every token of the input corpus, billions of pages, so whereas my embedding may become more complicated, I don't want that to slow down my spotting and annotation.",
                    "label": 1
                },
                {
                    "sent": "What is the cost of spotting and annotation compared to basic indexing?",
                    "label": 0
                },
                {
                    "sent": "When we are reading a page, we already have a topic model of what this page is about.",
                    "label": 0
                },
                {
                    "sent": "If I think of your New York City approving certain budget for school libraries.",
                    "label": 0
                },
                {
                    "sent": "The whole document totally drives out weird meanings like software libraries out of my mind, and so even though the Entity dictionary is 7 million 7 million large, I don't want to be looking of looking up all of them.",
                    "label": 0
                },
                {
                    "sent": "They are scanning a particular document, so can we use page or site level features to drastically prune the number of candidates we actually have to consider Puerto can?",
                    "label": 0
                },
                {
                    "sent": "Can we use that to speed up the accuracy and also improved accuracy?",
                    "label": 0
                },
                {
                    "sent": "Finally, this cannot be done reliably unless this becomes some sort of social presents.",
                    "label": 0
                },
                {
                    "sent": "Can we use some sort of active learning together with social tagging to improve the system?",
                    "label": 0
                },
                {
                    "sent": "So today you see a certain kind of tagging over the whole web, you go into content that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "You see taggings that look wrong, and you fix it, and in the next cycle that's taken into account to improve the learner, right?",
                    "label": 0
                },
                {
                    "sent": "So it's sort of like translation tools on the web, so when you get a translated page, you can fix the translation and that goes back to improve the translator.",
                    "label": 0
                },
                {
                    "sent": "So similar to that, can we build an annotated web by having people actually fix it?",
                    "label": 0
                },
                {
                    "sent": "As time goes by, source of lifelong learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that finishes what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "The first module, which is spotting plus disambiguation, and now I'm going to talk about some work in how to index those things.",
                    "label": 0
                },
                {
                    "sent": "And there are some challenges in doing that.",
                    "label": 0
                },
                {
                    "sent": "So feel free to interrupt anytime you have a question.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second stage we're worried about that in context part of queries.",
                    "label": 0
                },
                {
                    "sent": "So remember like I want.",
                    "label": 0
                },
                {
                    "sent": "A motherboard and opted on in the same context.",
                    "label": 0
                },
                {
                    "sent": "So how do I solve that?",
                    "label": 0
                },
                {
                    "sent": "So let's pick another query scientist who studied Wales.",
                    "label": 1
                },
                {
                    "sent": "So I might like to say S is a scientist, but if I'm better informed I might want to say S is a marine biologist.",
                    "label": 0
                },
                {
                    "sent": "There's the usual recall precision tradeoff in doing that, and it also differs a lot based on who is asking the query.",
                    "label": 0
                },
                {
                    "sent": "Remember that magic box in my very early slide, where the end user is going to the Magic box and that's turning their end user queries into this status of this kind of queries so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There may be some uncertainty about which of scientists and marine biologist is the best fit for my information need, and then I ask things like in a context CS and the following words should appear in this word should improve the score of the snippet.",
                    "label": 0
                },
                {
                    "sent": "So studies or study dorweiler Wales.",
                    "label": 0
                },
                {
                    "sent": "I might plus one of will or wills.",
                    "label": 0
                },
                {
                    "sent": "Or it could be stamped.",
                    "label": 0
                },
                {
                    "sent": "So the first option to solving this is quite impractical, so you do query expansion.",
                    "label": 1
                },
                {
                    "sent": "Do able to ask which scientist studied whales you ask?",
                    "label": 0
                },
                {
                    "sent": "Did Einstein study?",
                    "label": 0
                },
                {
                    "sent": "Who else did Niels Bohr study ways to draw the food study waves?",
                    "label": 1
                },
                {
                    "sent": "In other words, you only have a ground constant index on the whole corpus and then you know what all entities scientist expands to try to verify which hypothesis holds.",
                    "label": 0
                },
                {
                    "sent": "And that's totally impractical, because even Wordnet knows about 650 scientists and 160 cities.",
                    "label": 0
                },
                {
                    "sent": "If you go to the Wikipedia game is totally over.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other option, which is the only practical option, seems to be index expansion, which means that anytime you see the word cluster in a particular web page, you expand out to scientist, person Organism, living thing entity.",
                    "label": 0
                },
                {
                    "sent": "And hope that one of these will be queried by users later on.",
                    "label": 0
                },
                {
                    "sent": "In this case scientists.",
                    "label": 0
                },
                {
                    "sent": "And then pretend that all these tokens appear wherever Cousteau does in their index.",
                    "label": 1
                },
                {
                    "sent": "This works OK for small type sets like maybe 5 to 10 broad types.",
                    "label": 0
                },
                {
                    "sent": "So project called Entity Tank from Arbana Kevin Shanks Group did something like that about 15 types.",
                    "label": 0
                },
                {
                    "sent": "But for open domain work, that's not enough, because word NET has 15,000 internal noun types and 80,000 total leaf plus internal noun types, and Wikipedia has over 250 thousand categories.",
                    "label": 0
                },
                {
                    "sent": "So if you really wanted to index all of these types, that would explore the index unacceptably.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one fix we propose a few years back is to index, not the entire set of answer types.",
                    "label": 0
                },
                {
                    "sent": "A button index, some carefully chosen subset R, and then the query paradigm would be sort of a pre generalizing post filter paradigm.",
                    "label": 0
                },
                {
                    "sent": "Suppose the query answer type is scientist as I said, but you find that unfortunately the scientist which is a query answer type is not indexed.",
                    "label": 0
                },
                {
                    "sent": "You walk up to some clothes indexed answer type which is living thing.",
                    "label": 1
                },
                {
                    "sent": "G. The generalization, and then you ask find me living things near Wayland study.",
                    "label": 0
                },
                {
                    "sent": "And you'd end up with more answers because all of them have a loss of precision.",
                    "label": 0
                },
                {
                    "sent": "So both Cousteau and will appear to be living things.",
                    "label": 0
                },
                {
                    "sent": "And in the post filter stage, you need to figure out that as far as we know, outside the world of Hitchhiker's Guide to the Galaxy, you know whales are not scientists as far as we know and therefore will have to be eliminated from the answer set.",
                    "label": 0
                },
                {
                    "sent": "And this takes.",
                    "label": 0
                },
                {
                    "sent": "This requires using a so-called forward index and reachability index, which I won't go into the details of, but then some number survive this pruning step, and if enough don't survive year to restart the pipeline with larger value of K prime, which is expensive, But that's the main parroting.",
                    "label": 1
                },
                {
                    "sent": "Very simple query processing paradigm, but the important question is OK.",
                    "label": 0
                },
                {
                    "sent": "This is how you process queries.",
                    "label": 0
                },
                {
                    "sent": "How would you pick R, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Request so the cost is the space taken by R. And how much space can you save in the index?",
                    "label": 0
                },
                {
                    "sent": "Because you chose our instead of the whole of a.",
                    "label": 1
                },
                {
                    "sent": "The problem is we can't try out an exponential number of subsets of A and the benefit is or other inner space saved is the benefit and the cost is how much your queries are slowing down because you have to do this generalization and post filter and that depends on the query workload and we can't afford to test it on too many queries.",
                    "label": 1
                },
                {
                    "sent": "So in 2006, paper in the web conference, we give some algorithms for doing this balancing.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between.",
                    "label": 0
                },
                {
                    "sent": "Index isan query slow down.",
                    "label": 0
                },
                {
                    "sent": "And this is what we found is a very small scale experiment with a track corpus only about 5.7 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "The GZIP corpus, however, which is some indication of information content, is about 1.3 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "The STEM index was about the same.",
                    "label": 0
                },
                {
                    "sent": "So it's a good compression if we index the full type system in Word net you take 4.3 gigabytes, which is much larger than the gzip corpus and the same index.",
                    "label": 0
                },
                {
                    "sent": "However, if you carefully chose the subset that you want to index with the track query workload, which is what we used, you could cut down to an index of only 520 megabytes, which is less than the inverted index size, and your queries would slow down only by a factor of 1.8.",
                    "label": 0
                },
                {
                    "sent": "But the important question is, does this scale to the web?",
                    "label": 0
                },
                {
                    "sent": "How do you actually hunt around for the small set R amidst a sea of Wikipedia categories?",
                    "label": 0
                },
                {
                    "sent": "Hopefully most of them are pretty obscure, but it does have very obscure categories.",
                    "label": 0
                },
                {
                    "sent": "But still, it's kind of unclear as to whether you can pull off this kind of a trick with the whole web corpus and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "That's something that we're working.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that that's all I have time for.",
                    "label": 0
                },
                {
                    "sent": "For the second part.",
                    "label": 0
                },
                {
                    "sent": "And finally I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Go over to how to do query processing, specifically aggregation of evidence.",
                    "label": 0
                },
                {
                    "sent": "Once you get access to this kind of indices.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic picture to have in your mind is that the query now has four simple kinds of queries as a one type specification, which motherboard and words would be like opted on?",
                    "label": 0
                },
                {
                    "sent": "Or PCI Express slots?",
                    "label": 0
                },
                {
                    "sent": "And the green words match green words directly in various snippets.",
                    "label": 0
                },
                {
                    "sent": "The pink circles in the snippets are possible mentions of entities which have the given type.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, the first entity one second is also even.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "The first entity you want is supported by only one sniper, but there's lots of keywords near it.",
                    "label": 0
                },
                {
                    "sent": "The second entity is supported by three snippets, but each of them have fewer keywords matching it.",
                    "label": 0
                },
                {
                    "sent": "So how do you play this off?",
                    "label": 0
                },
                {
                    "sent": "Is there established theory for scoring and ranking entities in this setting?",
                    "label": 0
                },
                {
                    "sent": "And this is fairly ripe for trying to use something like personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "We have not tried it, but it seems plausible if you do it right.",
                    "label": 0
                },
                {
                    "sent": "That you can inject some sort of teleporter energy into each of your words.",
                    "label": 0
                },
                {
                    "sent": "Those spread out over snipers.",
                    "label": 0
                },
                {
                    "sent": "Then they collect into the entities and see how they rank.",
                    "label": 0
                },
                {
                    "sent": "Anyway, but clearly describe something a little different so you know this context is a candidate because it mentioned an entity of a target Type 4 score for context, and then if you have multiple contexts like the three for the second entity, how do you aggregate that into?",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evidence.",
                    "label": 0
                },
                {
                    "sent": "So for textual matches here some proposal.",
                    "label": 0
                },
                {
                    "sent": "So the query is in context C, something called P, which is a person together with the stem, invent and television.",
                    "label": 1
                },
                {
                    "sent": "So who invented television?",
                    "label": 0
                },
                {
                    "sent": "That's sort of the goal, and I want to aggregate over context to get a ranked list of people who might have invented the television.",
                    "label": 0
                },
                {
                    "sent": "I hope that the right one is at the top.",
                    "label": 0
                },
                {
                    "sent": "So there is considerations about.",
                    "label": 0
                },
                {
                    "sent": "The activation or energy at the red node, given the matches at the green and blue nodes.",
                    "label": 0
                },
                {
                    "sent": "The reality of the matches.",
                    "label": 0
                },
                {
                    "sent": "So if you see in the corpus the television is rather than invent, then television should pump in more energy into the red node as compared to invent.",
                    "label": 0
                },
                {
                    "sent": "And that's shown by the higher general profile of the green curve as compared to the blue curve.",
                    "label": 0
                },
                {
                    "sent": "If there are multiple occurrences of a certain query word, you might want to pick the closest one.",
                    "label": 1
                },
                {
                    "sent": "Or some other aggregation of them.",
                    "label": 0
                },
                {
                    "sent": "And then from multiple selectors, if you want to combine their effect, maybe you can do some, but maybe you can do some sort of softmax.",
                    "label": 0
                },
                {
                    "sent": "So learning these kinds of functions are also very open.",
                    "label": 0
                },
                {
                    "sent": "I mean I've not seen any work on sort of a generic framework for learning proximity functions, although cigar last year had a couple of papers and approximately language models, that's a area to watch, would know what's happening.",
                    "label": 1
                },
                {
                    "sent": "And also we don't really know that increasing distance from a match monotonically decreases evidence at a point as.",
                    "label": 0
                },
                {
                    "sent": "Hinted by the red and the green and blue curves, it may not be monotonic are experiential.",
                    "label": 1
                },
                {
                    "sent": "It's not one.",
                    "label": 0
                },
                {
                    "sent": "In fact for English.",
                    "label": 0
                },
                {
                    "sent": "Anyway, but then there's this question of, you know, if I see John Baird on one document and just bear down another document.",
                    "label": 0
                },
                {
                    "sent": "Those should somehow conspire to improve the score of John Bird.",
                    "label": 0
                },
                {
                    "sent": "And how can.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That be done.",
                    "label": 0
                },
                {
                    "sent": "So the nice couple of papers, one from CMU in Cigar 2007 and another one in www.two 1008 which does roughly what might be interpreted as Laplacian smoothing or Laplacian scoring.",
                    "label": 0
                },
                {
                    "sent": "We represent each snippet with the feature vector XI and the local score of a snippet is some WXI in the usual linear scoring model and then one defines this notion of affinity between mentions in snippets.",
                    "label": 1
                },
                {
                    "sent": "For example, if one snippet says Andrew McCallum, another snippet says.",
                    "label": 0
                },
                {
                    "sent": "AK McCallum.",
                    "label": 0
                },
                {
                    "sent": "Then you suspect there the same, and AIG represents that affinity in case of quantities.",
                    "label": 0
                },
                {
                    "sent": "That's a more trickier game.",
                    "label": 0
                },
                {
                    "sent": "What's the height of a giraffe?",
                    "label": 0
                },
                {
                    "sent": "One document says 18 feet, another says 19 feet, third one says 3 to 4 meters.",
                    "label": 0
                },
                {
                    "sent": "Someone to recognize that those are very similar as far as Heights of giraffes and animals is concerned, but you know 3.5 seconds and 3.5 two seconds are very different when it comes to Olympic records.",
                    "label": 0
                },
                {
                    "sent": "So what scale you pick for smoothing things out is fairly query dependent.",
                    "label": 1
                },
                {
                    "sent": "Anyway, so Laplacian scoring proceeds like this, you try to fit a global score for every node.",
                    "label": 0
                },
                {
                    "sent": "I called FI so that.",
                    "label": 0
                },
                {
                    "sent": "Your global score and your local score don't deviate much.",
                    "label": 0
                },
                {
                    "sent": "So that you pay the reviewer too much and the second thing is the Laplacian term which says that across edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "If there is a fact edge between I&J, namely inj look very similar as in the example above, you don't want their final scores to be two different either right?",
                    "label": 0
                },
                {
                    "sent": "So you play off these two considerations.",
                    "label": 0
                },
                {
                    "sent": "Local score versus global smoothness across the matrix or edges of the graph and then you fit.",
                    "label": 0
                },
                {
                    "sent": "If I to get the final scores during training you actually use a very similar formulation with the addition that you now want to fit W instead of F, But you only have partial orders and if during training you say.",
                    "label": 0
                },
                {
                    "sent": "The answer to the television inventing question is John Baird and not other things.",
                    "label": 0
                },
                {
                    "sent": "So the score of John Barrett should exceed that of any other entity, and we use that together with this expression to fit both W&F in the training examples.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, local scores can be very, very unreliable.",
                    "label": 0
                },
                {
                    "sent": "For example, in the height of a giraffe example with say foot as the unit you wanted in here are three snippets picked not uniformly, but three snippets pick from the top 50 of Google.",
                    "label": 0
                },
                {
                    "sent": "First one has the right answer.",
                    "label": 0
                },
                {
                    "sent": "It says large enough was a small, was approximately 11 feet because she is still young or full grown giraffe can reach a height of 18 feet, so that's not too bad.",
                    "label": 0
                },
                {
                    "sent": "The Green one is right, the yellow one is sort of OK, but the second one says, Gee Rephotography users are telescopic.",
                    "label": 0
                },
                {
                    "sent": "Mast to elaborate and eight megapixel camera to a height of 50 feet.",
                    "label": 0
                },
                {
                    "sent": "That's wrong answer.",
                    "label": 0
                },
                {
                    "sent": "It's in the right unit.",
                    "label": 0
                },
                {
                    "sent": "It has the words heightened giraffe on it.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's not too much to distinguish it from the first one, unless you're doing real NLP there, right?",
                    "label": 0
                },
                {
                    "sent": "And the third one says that record height for a giraffe unicycle is about 100 feet.",
                    "label": 0
                },
                {
                    "sent": "So all of these snippets content, all your query keywords.",
                    "label": 0
                },
                {
                    "sent": "They have the answer in the right unit.",
                    "label": 0
                },
                {
                    "sent": "So you never discriminate between them.",
                    "label": 0
                },
                {
                    "sent": "That seems very difficult unless you understand what's being said.",
                    "label": 0
                },
                {
                    "sent": "The third one is even more interesting.",
                    "label": 0
                },
                {
                    "sent": "How far should a raccoon be located in Miles?",
                    "label": 0
                },
                {
                    "sent": "Look at the 2nd 116 deer 2 Fox is 1 skunk and two raccoons are cited during 135 mile Drive.",
                    "label": 0
                },
                {
                    "sent": "So it seems like an impossible problem.",
                    "label": 0
                },
                {
                    "sent": "Can you really avoid deep NLP while trying to aggregate out these kinds of noises?",
                    "label": 1
                },
                {
                    "sent": "Get the right answer.",
                    "label": 0
                },
                {
                    "sent": "The surprising answer is yes, we can actually use some sort of wisdom of the crowds if you will to actually hunt for.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right kind of intervals.",
                    "label": 0
                },
                {
                    "sent": "So somehow the problem changes from scoring documents and snippets into hunting rectangles over a 2D space and this is what I mean.",
                    "label": 0
                },
                {
                    "sent": "So each of this for one query, the X axis is the quantity for uniformity, normalized between something like zero and one.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the local snippet score which I got out of training scoring model.",
                    "label": 0
                },
                {
                    "sent": "So more score means it looks like a more promising snippet.",
                    "label": 0
                },
                {
                    "sent": "Less score means it's less promising snippet, and if OK and the pluses are delivering snippets which actually have evidence of your answer.",
                    "label": 0
                },
                {
                    "sent": "And the circles are irrelevant secrets which have some of the query keywords and even a quantity in the right unit.",
                    "label": 0
                },
                {
                    "sent": "But they're not correct.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, there's hardly any connection between the Y coordinate and correctness.",
                    "label": 0
                },
                {
                    "sent": "If your learning of W are perfect, you'd actually in each case have some horizontal line so that all the pluses would lie above the line and all the circles would lie below the line.",
                    "label": 0
                },
                {
                    "sent": "But that never happens because without doing an LP these snippets all look the same to a standard IR ISH search engine.",
                    "label": 0
                },
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "What saves you is the observation that correct answers are in, by definition, tightly packed rectangles.",
                    "label": 0
                },
                {
                    "sent": "And then everyone rectangles.",
                    "label": 0
                },
                {
                    "sent": "For example, if you ask about cordless phone frequencies, there are two broad clusters.",
                    "label": 0
                },
                {
                    "sent": "As you'd expect.",
                    "label": 0
                },
                {
                    "sent": "Tufts actually is a very interesting case.",
                    "label": 0
                },
                {
                    "sent": "This is the 18 foot cluster that's a 6 foot cluster.",
                    "label": 0
                },
                {
                    "sent": "The six foot cluster comes out of the fact that when a giraffe is born, it drops to the ground through a height of 6 feet.",
                    "label": 0
                },
                {
                    "sent": "And still lives.",
                    "label": 0
                },
                {
                    "sent": "So that's gonna be interesting.",
                    "label": 0
                },
                {
                    "sent": "You might even argue that that's not a bad second answer to give just based on truth by democracy, but we need to learn that this is the best cluster, the right big rectangle over here to the right is that is the correct cluster.",
                    "label": 0
                },
                {
                    "sent": "So this is how rectangle.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something comes into the picture, and here's sort of the schematic.",
                    "label": 0
                },
                {
                    "sent": "Hopefully what you have is kind of a sea of or cloud of relevant clusters with a few relevant green rectangles sticking out of it, and there should be some feature which lets you pull out the green rectangle out of the red mess.",
                    "label": 0
                },
                {
                    "sent": "So the question now is how to detect and rank these so-called conscientious rectangles.",
                    "label": 1
                },
                {
                    "sent": "The challenging problem is that the position and shape or width and height of all these rectangles vary from query to query.",
                    "label": 0
                },
                {
                    "sent": "Necessarily some queries may have multiple rectangles, some kind of a single rectangle there in different places, so you cannot use a standard non linear discriminant approach to solve these kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "You cannot throw standard machine learning techniques at it.",
                    "label": 0
                },
                {
                    "sent": "So this is actually more related to what's called Special scan statistics type work in databases and data mining.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So skip the details of how it is done, but here the results there.",
                    "label": 0
                },
                {
                    "sent": "That's really speak for themselves.",
                    "label": 0
                },
                {
                    "sent": "If we use rank SVM to score each snippet independently, get the lower baseline, which is understandable.",
                    "label": 0
                },
                {
                    "sent": "You have no idea whether 18 feet in 19 feet are similar.",
                    "label": 0
                },
                {
                    "sent": "If we use the Laplacian technique that I mentioned before you get a slightly higher result.",
                    "label": 0
                },
                {
                    "sent": "On the X axis, the X axis is the tolerance percentage that the user specifies with the query, so I'd like to know the height of a giraffe to within 20%.",
                    "label": 0
                },
                {
                    "sent": "The distance between the Sun and Pluto is a much more uncertain affair and changes all the time through a large factor.",
                    "label": 0
                },
                {
                    "sent": "So you certainly the distance between Sun and Pluto within 50%, but you were trying to figure out Olympic records.",
                    "label": 0
                },
                {
                    "sent": "You'd like to know the answer to within 1%.",
                    "label": 0
                },
                {
                    "sent": "So that's something that's essentially application dependent and at the moment we don't see a.",
                    "label": 0
                },
                {
                    "sent": "Big where to second guess the users needs, but our algorithm is also very insensitive to what particular into pick.",
                    "label": 0
                },
                {
                    "sent": "All the curves are fairly flat, so the tolerance percentage is between 0 and 10% here, and if we learn new ways to score intervals on the X axis properly, then we get the third line which is this.",
                    "label": 0
                },
                {
                    "sent": "Pink line over here and then.",
                    "label": 0
                },
                {
                    "sent": "Finally we actually exploit collective features across all the snippets content in each interval and get.",
                    "label": 1
                },
                {
                    "sent": "This gives us the best accuracy of all, which is really large in information retrieval terms compared to the baselines.",
                    "label": 0
                },
                {
                    "sent": "So this appeared in Cigar last year.",
                    "label": 0
                },
                {
                    "sent": "Go look at that, but again, this is not really a closed question.",
                    "label": 0
                },
                {
                    "sent": "We know how to we now know how to point out single intervals in response to queries talking about quantities.",
                    "label": 0
                },
                {
                    "sent": "But what if I want a table of things?",
                    "label": 0
                },
                {
                    "sent": "Can I use the synergy between two quantities to decide that?",
                    "label": 0
                },
                {
                    "sent": "Give me Michael Answers which are more accurate than if I was ranking for every each column separately or if I'm asking for the height of.",
                    "label": 0
                },
                {
                    "sent": "Land mammals.",
                    "label": 0
                },
                {
                    "sent": "Can I somehow shrink it?",
                    "label": 0
                },
                {
                    "sent": "The information to broader categories in Wikipedia to decide the dancer can never be 2 miles.",
                    "label": 0
                },
                {
                    "sent": "When we scan documents for quantities, we always use our prior knowledge over the world to scan numbers properly.",
                    "label": 0
                },
                {
                    "sent": "Even without deep linguistic attention to the page, we will never participate and see the weight of a digital camera is 5 KGS.",
                    "label": 0
                },
                {
                    "sent": "So we somehow use broader notions of object categories in Wikipedia ordinate or in our minds, to decide what plausible quantities can be.",
                    "label": 0
                },
                {
                    "sent": "And that can help a lot, so we haven't really.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That so to round up the talk.",
                    "label": 0
                },
                {
                    "sent": "We have explored how to open up new information pathways between documents and between documents and semi structured knowledge bases.",
                    "label": 1
                },
                {
                    "sent": "So if you thought the web world was complicated, had 10 billion documents, it just got more complicated.",
                    "label": 0
                },
                {
                    "sent": "'cause now the number of links is no longer the number of macroscopic hyperlinks between pages, but it's equal to the number of this microscopic annotation.",
                    "label": 0
                },
                {
                    "sent": "Links between pages and Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So the number of links have just gone up to maybe 100 billion right?",
                    "label": 0
                },
                {
                    "sent": "And we need to search across this graph which has become.",
                    "label": 0
                },
                {
                    "sent": "More dense as compared to the web graph.",
                    "label": 0
                },
                {
                    "sent": "It probably has properties which are quite different from the social network properties that we usually come to expect.",
                    "label": 0
                },
                {
                    "sent": "Ranking in these graphs is a totally unexplored area.",
                    "label": 1
                },
                {
                    "sent": "And how to really know access into this richer information representation is still quite unclear.",
                    "label": 0
                },
                {
                    "sent": "I'll just proposed the beginnings of 1 query language somewhere between you know really structured SQL and the completely unstructured text query languages.",
                    "label": 0
                },
                {
                    "sent": "Where is the sweet spot that will require a lot of participation to decide.",
                    "label": 0
                },
                {
                    "sent": "I would however really lobby for trying that out because for several years now we seem to have kind of cost the interaction.",
                    "label": 0
                },
                {
                    "sent": "Mode in concrete and so that no matter what you do about web search engine research, the end interaction is going to be the same.",
                    "label": 0
                },
                {
                    "sent": "It seems like you know breaking through that restriction can open up more things to do.",
                    "label": 0
                },
                {
                    "sent": "Right, so if only we could agree on certain important constructs of the next generation query languages, maybe there will be more fun research to do.",
                    "label": 0
                },
                {
                    "sent": "So we had a panel at World Wide Web last year and there was quite some agreement that people want for specific vertical applications.",
                    "label": 0
                },
                {
                    "sent": "They want entities and types for sure.",
                    "label": 0
                },
                {
                    "sent": "We've been trying not to talk about all this in a vacuum, so we are prototyping with about half a billion pages and about 40 hosts.",
                    "label": 0
                },
                {
                    "sent": "So we sort of roughly know what's going on, where the performance pinches, and what are the important considerations of recall versus precision and so on.",
                    "label": 0
                },
                {
                    "sent": "And you know the right question to ask is probably not what will end.",
                    "label": 0
                },
                {
                    "sent": "Users are up today.",
                    "label": 0
                },
                {
                    "sent": "Will they start using this overnight?",
                    "label": 0
                },
                {
                    "sent": "But really, you know how to fill up that initial box of magic, which is, how can we help end users take advantage of this new composite representation where complicated social organic knowledge network connects to the unstructured corpus and this is done in noisy, unreliable ways.",
                    "label": 0
                },
                {
                    "sent": "And yet we managed to distill correct answers out of it because of the redundancy involved because of the diversity involved in the corpus, so that's sort of the broad proposal I'm trying to pitch.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "But some of the questions are raised.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We waited for longer sentences, but there are some things.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Sure, so.",
                    "label": 0
                },
                {
                    "sent": "It's a couple of.",
                    "label": 0
                },
                {
                    "sent": "Differences first is that if you look at the traditional question answering community.",
                    "label": 0
                },
                {
                    "sent": "The query is hardly ever posed in a very structured or semi structured way, so it's sort of the the job of the system to somehow understand natural language in the question and break it up into the type specifiers, the qualifiers, and so on, and that limits the complexity of queries which you can ask later generations of question answering are trying to become more sophisticated.",
                    "label": 0
                },
                {
                    "sent": "In particular, database people recognize joins and aggregations in some of the queries today, but there is no direct pathway for.",
                    "label": 0
                },
                {
                    "sent": "The question to turn into that kind of structured query.",
                    "label": 0
                },
                {
                    "sent": "That's first thing, a lot of question answering teams of course.",
                    "label": 0
                },
                {
                    "sent": "Use word net as the annotation mechanism.",
                    "label": 0
                },
                {
                    "sent": "I've not kept in touch with the QA community for the last one or two years, so I don't know how much they are using Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Evidence aggregation in question answering is a sparse space.",
                    "label": 0
                },
                {
                    "sent": "There were these two or three nice papers and associative ranking of the answers, but there's not that much in terms of evidence aggregation.",
                    "label": 0
                },
                {
                    "sent": "And finally, if your answer is not a single entity, it's not entity ranking or expert finding, but the expected answer is like a bunch of couples out of a database, then question answering has very little to say.",
                    "label": 0
                },
                {
                    "sent": "Studer first of all, I love your vision.",
                    "label": 0
                },
                {
                    "sent": "I think it's terrific and very exciting.",
                    "label": 0
                },
                {
                    "sent": "It's a big vision and the question I want to ask is where do you think you talk about a lot of pieces and how they might be done, both given you thought about this problem, where do you think the hardest subparts of the problem are?",
                    "label": 0
                },
                {
                    "sent": "Where do you think are the weakest link in your own argument about this issue?",
                    "label": 0
                },
                {
                    "sent": "The two weak points, the first is too.",
                    "label": 0
                },
                {
                    "sent": "Have annotation be accurate enough.",
                    "label": 0
                },
                {
                    "sent": "And the reason why that's difficult is because it's eventually goal oriented.",
                    "label": 0
                },
                {
                    "sent": "You'd like to answer queries properly with it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter if you.",
                    "label": 0
                },
                {
                    "sent": "Loste recall on 100 million tokens.",
                    "label": 0
                },
                {
                    "sent": "If they don't affect any query unfortunately don't know what queries are today, so it's a chicken and egg problem.",
                    "label": 0
                },
                {
                    "sent": "Right, so in fact, in a partial answer to the previous query, to try to benchmark our systems, we actually went to trick.",
                    "label": 0
                },
                {
                    "sent": "QA tracks and we collected queries from there and transcribe them by hand to see that we can answer at least those nicely, right, but it ends there and we don't have a rich set of queries.",
                    "label": 0
                },
                {
                    "sent": "That is comparable anywhere.",
                    "label": 0
                },
                {
                    "sent": "Tour 2, search engine query logs the second week spot is.",
                    "label": 0
                },
                {
                    "sent": "Taking ranking beyond single snipers and single entities into really using the.",
                    "label": 0
                },
                {
                    "sent": "It's still growing literature on certain databases to have formally interesting things to say about ranking of couples in the output.",
                    "label": 0
                },
                {
                    "sent": "So if I'm asking for a three column table and there are interesting correlation structures across those, what's my confidence in every row that I'm emitting?",
                    "label": 0
                },
                {
                    "sent": "So if you have looked at some of the uncertain demonstration device community, all kinds of nice algorithm kinda structure ideas, but hardly anyone knows how the public numbers are stuck in the table in the 1st place, very little known about that.",
                    "label": 0
                },
                {
                    "sent": "So when I say I looked at a 5 million tokens or 5 million snippets on the web.",
                    "label": 0
                },
                {
                    "sent": "And here's my belief in a certain fact.",
                    "label": 0
                },
                {
                    "sent": "How do you even define that?",
                    "label": 0
                },
                {
                    "sent": "Or is it that probabilistic belief is to be ruled out from the very beginning and we should just proceed?",
                    "label": 0
                },
                {
                    "sent": "Discriminatively you know?",
                    "label": 0
                },
                {
                    "sent": "So my eventual goal is to just get the answer to the top.",
                    "label": 0
                },
                {
                    "sent": "Don't ask him out about any quantitative belief.",
                    "label": 0
                },
                {
                    "sent": "So which path will be better is very open at the moment.",
                    "label": 0
                },
                {
                    "sent": "So this sort of depends on what the query language.",
                    "label": 0
                },
                {
                    "sent": "I'm just looking like, but I was wondering if you were to take sort of a state of the art traditional IR technique to answer these queries.",
                    "label": 0
                },
                {
                    "sent": "And then you take your answers.",
                    "label": 0
                },
                {
                    "sent": "What sort of percentage of the top of the IR results are going to match the results you return?",
                    "label": 0
                },
                {
                    "sent": "I like.",
                    "label": 0
                },
                {
                    "sent": "I mean, the idea is if your queries are expensive, execute.",
                    "label": 0
                },
                {
                    "sent": "Can you do a pre query stage in traditional IR?",
                    "label": 0
                },
                {
                    "sent": "Just reduce the number of documents that we're really talking about returning over.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Some part of the aggregation cannot be pushed to preprocessing unless you're basically caching frequent query answers already.",
                    "label": 0
                },
                {
                    "sent": "So, so where do?",
                    "label": 0
                },
                {
                    "sent": "Standard error indices and access methods really fail if you're trying to do these kinds of things.",
                    "label": 0
                },
                {
                    "sent": "I think the very first step should be to index types as soon as your.",
                    "label": 0
                },
                {
                    "sent": "Index can bring forth preferentially snippets with just don't have your credit key words in it, but also something of the given type that will give you a huge boost.",
                    "label": 0
                },
                {
                    "sent": "So in the quantity search example, we actually used Google Snippets and we got a certain level of accuracy.",
                    "label": 0
                },
                {
                    "sent": "But in Google you cannot really say from the end user API.",
                    "label": 0
                },
                {
                    "sent": "Give me a snippet, witches, giraffe and height and something that actually looks like a linear distance quantity.",
                    "label": 0
                },
                {
                    "sent": "Right, it just happens that for this particular query Google is great, but especially for number of queries like counted queries, you don't have any control on Google to say this.",
                    "label": 0
                },
                {
                    "sent": "Looks like a number and.",
                    "label": 0
                },
                {
                    "sent": "So if you cannot control the fact that the snippet should have your answer type on it, then you have lost half the battle, so that's the first thing that indexing and should do.",
                    "label": 0
                },
                {
                    "sent": "So we did other experiments where we actually indexed our tiny corpus, half a billion page corpus with types, but it's not comparable with what you're getting.",
                    "label": 0
                },
                {
                    "sent": "Google because Google has many more documents in a different ranking function, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why it's difficult to kind of give a. Apple to Apple comparison there, but it's really nice to have the identical corpus with really state of the art ranking functions and then say if you could index this, this is more how much better do I get that we did study?",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So your your effort does take a certain amount of engineering knowledge engineering to identify the things you want to have you in various stages of process.",
                    "label": 0
                },
                {
                    "sent": "The most successful Trek QA systems.",
                    "label": 0
                },
                {
                    "sent": "So the closest thing we have to what you're talking about the measure, also have a significant amount of knowledge engineering and it only engineering to make work.",
                    "label": 0
                },
                {
                    "sent": "I don't know how much you're familiar with those systems that can you say something about the degree of engineering required to get reasonable performance on some my mom.",
                    "label": 0
                },
                {
                    "sent": "So yeah, as you said, I mean, I've not kept track of exactly what trick you does in the last couple of years, but.",
                    "label": 0
                },
                {
                    "sent": "As per the last counting, I did the degree of ambiguity of the average mentioned in track is much smaller than the degree of ambiguity of any particular token in the Wikipedia dictionary, so accuracy can fall and as has been known, in standard IR and disambiguation, unless you're doing this immigration well enough, you don't want to use it in higher actually.",
                    "label": 0
                },
                {
                    "sent": "So you need to reach a certain threshold of quality before you can use aggregation over annotations properly.",
                    "label": 0
                },
                {
                    "sent": "If I'm pulling in arbitrary garbage at every spot and saying this might link to that guy, you know I might bring up wrong solutions a lot of times.",
                    "label": 0
                },
                {
                    "sent": "So Interactive works the corpus recently.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's.",
                    "label": 0
                },
                {
                    "sent": "Really specialize in our system storage.",
                    "label": 0
                },
                {
                    "sent": "You got served.",
                    "label": 0
                },
                {
                    "sent": "What's the relative scale?",
                    "label": 0
                },
                {
                    "sent": "So having experienced what it takes to even look up a 7 million key dictionary for every token and then do score aggregation across snippets of segments of size 5 or less, which is what you need for Wikipedia most of the time.",
                    "label": 0
                },
                {
                    "sent": "It seems like you know even dependency parsing of every sentence on the web is something you would like to avoid at all cost.",
                    "label": 0
                },
                {
                    "sent": "At least as of today.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "So what can you do?",
                    "label": 0
                },
                {
                    "sent": "What else from that community can you import?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, I I would also agree with Tom that this vision, I guess, can I please interrupt and go back to one.",
                    "label": 0
                },
                {
                    "sent": "On one hand you don't want to exploit natural language.",
                    "label": 0
                },
                {
                    "sent": "Sentence is if that takes too much time.",
                    "label": 0
                },
                {
                    "sent": "But on the web there are other languages.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not NLP.",
                    "label": 0
                },
                {
                    "sent": "It's not English, but there's the language of lists, the language of tables.",
                    "label": 0
                },
                {
                    "sent": "There is a language of sight organization.",
                    "label": 0
                },
                {
                    "sent": "Those things can help a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, there have been a couple of papers on table mining which have really improved.",
                    "label": 0
                },
                {
                    "sent": "So my comment would be in this direction.",
                    "label": 0
                },
                {
                    "sent": "So mainly you were just presenting methods which still have pretty much materialized knowledge, so there's no reasoning in the sense that.",
                    "label": 0
                },
                {
                    "sent": "You would get facts or connections on the fly which are not even within an index level.",
                    "label": 0
                },
                {
                    "sent": "Well, some of the joint queries I'm talking about are sort of like that.",
                    "label": 0
                },
                {
                    "sent": "So today I'm kind of restricting to equal joins if you will.",
                    "label": 0
                },
                {
                    "sent": "But you can assert other arithmetic predicates on it and so on, right?",
                    "label": 0
                },
                {
                    "sent": "It will definitely ethnic Inequality's and see what comes out right.",
                    "label": 0
                },
                {
                    "sent": "Motherboards with price at most $400, so those things can be done.",
                    "label": 0
                },
                {
                    "sent": "It's unclear what the formal semantics would be of response, rose.",
                    "label": 0
                },
                {
                    "sent": "But let's leave out that for the moment.",
                    "label": 0
                },
                {
                    "sent": "So I guess I don't really know what you mean by lightweight and what do you mean by reasoning so lightweight means that you would have some heavy deductive reasoning, which, let's say lead you to this psych style of producing effects.",
                    "label": 0
                },
                {
                    "sent": "So that way it would be more like using not indexing.",
                    "label": 0
                },
                {
                    "sent": "Let's say all connections within sort of willed model, which occasionally you would have to understand.",
                    "label": 0
                },
                {
                    "sent": "Your data better, so it's always a question.",
                    "label": 0
                },
                {
                    "sent": "How much pre structure you introduce into the data you want to deal with, right?",
                    "label": 0
                },
                {
                    "sent": "So so I guess that's really the core of the proposal, which is the preprocessing and the earlier preparation of the data.",
                    "label": 0
                },
                {
                    "sent": "I'm just proposing you materialize possible mentioned links.",
                    "label": 0
                },
                {
                    "sent": "And that you recognize entities and recognize quantities.",
                    "label": 0
                },
                {
                    "sent": "So what the sweet spot?",
                    "label": 0
                },
                {
                    "sent": "How can we get 80% of the remaining queries with the first step, right so?",
                    "label": 0
                },
                {
                    "sent": "What other forms of logic do you need to impose on this?",
                    "label": 0
                },
                {
                    "sent": "For example, frame netlists, about 35 basic relationships in any natural language?",
                    "label": 0
                },
                {
                    "sent": "Do I need?",
                    "label": 0
                },
                {
                    "sent": "Do I want to tag every occurrence of religion in frame net or my home corpus right?",
                    "label": 0
                },
                {
                    "sent": "So maybe that's overkill?",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe out of the 35 we can pick 10 and who knows?",
                    "label": 0
                },
                {
                    "sent": "So things might work gradually in that direction, but even at the current scale of aggregation it's hard to see all of these queries being interactive.",
                    "label": 0
                },
                {
                    "sent": "Maybe the ones that are not interactive will be declared as views and updated once in awhile.",
                    "label": 0
                },
                {
                    "sent": "That's another thing I didn't reflect on, which is maybe some of these queries can be declared as views and then maintain so that.",
                    "label": 0
                },
                {
                    "sent": "Smaller queries can be quickly composed out of the views.",
                    "label": 0
                },
                {
                    "sent": "So I would like to ask, how do you see your vision as compared to the vision of semantic web where the goal is to return the webbing for gigantic knowledge database, perhaps in terms of sophisticated ventilation ship routes like those you think this is perhaps a more practical way to attack on this very, very difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a little political.",
                    "label": 0
                },
                {
                    "sent": "I mean semantically wants to.",
                    "label": 0
                },
                {
                    "sent": "Build order from the ground up right.",
                    "label": 0
                },
                {
                    "sent": "Where is this like cleaning up stuff which already happened?",
                    "label": 0
                },
                {
                    "sent": "To get this running within six months to a year, this may be the only way.",
                    "label": 0
                },
                {
                    "sent": "But there's a lot of synergy there, I mean.",
                    "label": 0
                },
                {
                    "sent": "Semantic structuring.",
                    "label": 0
                },
                {
                    "sent": "When it is done.",
                    "label": 0
                },
                {
                    "sent": "Relatively organically and without central control can scale a lot, and Wikipedia knows about new new important entities in the world within hours.",
                    "label": 0
                },
                {
                    "sent": "And so if there are more semantic knowledge bases which are maintained thus.",
                    "label": 0
                },
                {
                    "sent": "Those should be brought into the fold.",
                    "label": 0
                },
                {
                    "sent": "Right, so think of defining an abstraction over semi structured world knowledge and then plugging that into this system.",
                    "label": 0
                },
                {
                    "sent": "While understanding that that knowledge base will never engulf the whole corpus because the schema will never be complete.",
                    "label": 0
                },
                {
                    "sent": "I think for the sake of time we have to sort of end this session and let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "So next session will start in 20 minutes.",
                    "label": 0
                }
            ]
        }
    }
}