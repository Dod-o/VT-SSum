{
    "id": "a2qm5tkho7bnswadpzh7nbzisu2hbyzg",
    "title": "Minimax rates for memory-bounded sparse linear regression",
    "info": {
        "author": [
            "Jacob Steinhardt, Computer Science Department, Stanford University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_steinhardt_linear_regression/",
    "segmentation": [
        [
            "OK, so I'd like to stay."
        ],
        [
            "By asking a question that has become increasingly pertinent in recent years.",
            "Namely, how do we solve statistical learning problems with limited computational resources, and this has been studied in many settings.",
            "The most obvious perhaps is to consider the resource of running time which has sparked a good deal of work, including notably the best paper at Colt two years ago by birthday and regularly.",
            "But we can also consider other computational resource constraints, such As for instance, privacy.",
            "And recently communication and memory constraints have spawned a lot of recent work.",
            "I'd like to note in particular the work of Ohad Shamir, whose framework is very similar to the framework that we will consider in this work, and I will be studying the case of sparse linear regression in a memory in communication constraints setting, this is a problem which, despite all of this work above, has not yet had well characterized minimax rates.",
            "And in this work, we'll go most of the way towards closing this gap of getting matching upper and lower bounds."
        ],
        [
            "So just to start, I'll give you the setting that we consider, so it's a fairly standard sparse linear regression set up.",
            "We're doing regression over D dimensional real space and why is function of XA linear function given by W star plus some additive noise, and we'll assume that the true parameters W star are very sparse, so the number of non zero entries is K, where typically will assume K is much less than D. Although our analysis will actually hold for any key all the way down to one or all the way up to the dimension D. And the thing that will sort of add A twist to our framework is that will have a memory constraint.",
            "So one way to formalize this is that we'll assume that our observations X&Y are observed as a read only stream, so we can see these, but we can't really use that as memory to write to and instead the only memory we get is.",
            "We can write up to be bits of state to a variable that will denote ZI, and that's the only way we can really transmit information.",
            "Between samples, so before we see the next sample we have to write all over information to our current memory state and then the sample disappears.",
            "We can make this more formal by considering the following graphical model, so we have the true parameters W star.",
            "We see our first input, covariant X1 and X1 and W star together and generate some output Y one and then we can write to our memory state Z.",
            "We then see our next observation in the next output, and now we write a new memory state Z2, which can only depend on Z1 in the current observations, and there's at most B bits in either of these States and then we just continue this and at the end we need to produce some estimate of the true parameters and you can also consider certain communication constraint versions of this as well, but I'll just focus on the memory constrained case for the purposes of this talk."
        ],
        [
            "And so a fundamental question we can ask is if we have enough memory to represent the answer, can we also efficiently learn the answer?",
            "So to represent the answer, it's just, you know.",
            "K dimensional set of real numbers.",
            "So it's going to take about K log D bits to represent.",
            "We might wonder, is that enough bits to actually also learn the answer efficiently?",
            "Or is there some fundamental barrier beyond just this sort of representation lower bound?"
        ],
        [
            "And so to get out this more precisely, we can ask how much data we need to obtain a certain error.",
            "A certain error on the OR a certain error level for the mean squared error will call that level epsilon.",
            "And in the classical case, this is very well studied.",
            "Several people have characterized the minimax rate, basically optimally in particular, Wainwright in 2009 showed that the answer is K. Log D over epsilon up to constant factors.",
            "However, with memory constraints, the answer changes quite a bit, so there's this log D here.",
            "In the classical case, when we have memory constraints, this log D becomes a D / B.",
            "And so this is actually an exponential increase in the sample complexity, unless B is quite close to the dimension D. And I should note that this is this only holds in the regime where B is between K log D&D Sobel OK logged, the answer is just you can't get vanish in error and above D it's possible to essentially achieve this rate, so there's various stochastic gradient algorithms that can achieve this optimal rate without do log bits of memory.",
            "And I'm also hiding just a couple more caveats.",
            "I'm hiding some structural assumptions on the design matrix needed for this upper bound, and there's of course a little bit of a gap, so there's still this epsilon versus epsilon squared gap, so there's still some remaining questions to answer here, but at least in terms of the dependence on the dimension and the memory constraint, we have pretty much the answer nailed down."
        ],
        [
            "So I'm just going to start with an overview of our proof techniques, and then later we'll delve into a few of the more interesting ideas in a bit more technical detail.",
            "So the lower bound is basically information theoretic in nature.",
            "It relies on what's called a strong data processing inequality and to give a bit of intuition about this, let's just imagine the case where we make a single observation.",
            "So there's a single pair XY that we observe.",
            "We can then imagine the following Markov chain.",
            "Where we have the true parameters W star based on those, we generate an observation XY.",
            "And if we had no memory constraints at all.",
            "Then we could just store everything and that would take about D bits of memory because X is a D dimensional vector and Y is a scalar.",
            "If we store all of that information.",
            "Then we expect to get about a constant number of bits of information about W star because we've stored one linear relation.",
            "Of course, the actual number of bits will depend on the signal to noise ratio of our problem, but at least for a cartoon picture we can we can sort of take these numbers, take these numbers, and say that if we stored the bits, we get about 1 bit of information about the answer.",
            "Now, in reality we have a memory constraint of only B bits, where V is less than D. And so we have to sort of choose what information we're going to store.",
            "An you might expect.",
            "Well, if we scale down the amount of information from D to B, then the amount of information we get about the answer should also scale down from one to be over D and so since we need cave, it's total to learn K real numbers or over on the order of K bits, then we should need about KD over V samples, which is the lower bound we get.",
            "And the main challenge here beyond sort of existing work on these strong data processing inequality's, is to handle the dependence between X&Y, so most of the work that I alluded to on the previous slide relies on strong data processing inequality.",
            "Is that exploit independence of lots of random variables.",
            "But here in regression X&Y are necessarily dependent and so we need some way of dealing with that and I'll highlight that later.",
            "For the upper bound, we can use in some sense much simpler techniques, so we basically use a data structure from compressed sensing called accountant sketch data structure together with an L1 regularize dual averaging algorithm.",
            "So this is basically a variant of stochastic gradient descent.",
            "And the key idea.",
            "Is that the higher the degree of L1 regularization, the easier it is to distinguish coordinates?",
            "That should be 0 from coordinates that should be non zero.",
            "In particular this shows up in the memory requirements of the Count, Min sketch structure needed to accurately recover the support of the answer.",
            "OK, I'd like to delve in a bit more into the lower."
        ],
        [
            "In construction, so to prove the lower bound we need to exhibit some distribution which is hard on average, and this distribution will actually be quite simple, so we need to split the coordinates into K blocks of equal size.",
            "And we want something case bars.",
            "So within each of these K blocks will randomly choose one coordinate to be non zero and will give it uniformly random sign.",
            "So will either be plus Delta with some probability or minus Delta.",
            "Plus Delta with probability 1/2 minus Delta with probability 1/2 and Delta is a constant that we need to tune to get the optimal bound.",
            "And so we get something like this, where each block there's a single nonzero coordinate J.",
            "Using what's called the direct sum argument, we can actually reduce to only considering the case when K = 1.",
            "So when we have only a single non zero coordinate, this argument is actually fairly nontrivial, but for the purposes of this talk, I'm going to gloss over it and just assume that we can just restrict to the case when we only have one non zero coordinate.",
            "And then we can use sort of a standard estimation to test inbound showing that sense each of these correspond to well separated parameters.",
            "They're all at least Euclidean distance Delta squared apart.",
            "The estimation error is lower bounded by a multiple of the testing error, and so we just have to show that it's hard to figure out which coordinate is non zero.",
            "And we can use standard information theoretic techniques based on bounding KL divergences to do this.",
            "So this is in some sense a fairly.",
            "Standard construction approach.",
            "Aside from this direct sum argument and so on, the next slide I'll go into what specific properties of the problem we actually use."
        ],
        [
            "So first of all, it's important that our covariates are actually random sign vectors.",
            "To get the lower bound we want.",
            "And for a bit of notation, will let piece of J denote the distribution conditioned on the non zero coordinate being J will assume without loss of generality that it's positive and this is really the distribution over everything, so I've put Z here, But we can really consider any of XYZ or their joint distribution, and in particular I'd like to point to this graph here of the marginal probability density of Y.",
            "Conditioned on a given value of XJ.",
            "So if we condition on XJ depending on whether it's positive or negative, will either get a probability density that's shifted to the right by Delta or shifted to the left by Delta.",
            "For any other value of X other than J, this shift won't happen because that coordinate of W is zero and so will just get Y being independent of all of those other credits.",
            "And in fact, we'll define a base distribution where Y is actually independent of every single coordinate, so there's no coordinate Jay that affects the answer.",
            "And then we just get this marginal distribution shown here in green.",
            "And to prove our lower bound will use what's called.",
            "Also odds method which is a bound on the on the testing error in terms of this average Cal divergance.",
            "Highlighted in red here so.",
            "The intuition, or sorry a key fact, is that Y&XJ together are independent of all of the other coordinates under PJ.",
            "And why is this?",
            "Well, it's just this thing I said before that all of the coordinates other than J just don't affect the output because the parameters are zero and all of those coordinates.",
            "And so before we prove these strong data processing inequality is we needed sort of everything to be independent.",
            "Here, we're going to exploit this slightly weaker independence property that these two variables are independent of everything else conditioned on a particular distribution.",
            "And so the intuition is then that really the only thing that can tell us anything about PJ is the coordinated XJ, because for any other coordinate we just get something that looks exactly like P0.",
            "We just get this green sort of spread out mass, and so the only way to make the kale divergent between P0 and PJ large is to store information about XJ.",
            "And so unless we store information about the majority of the X is.",
            "The coordinates of X, the average KL divergences going to be pretty small, and since we only have B bits and there's D coordinates of X to store, we're basically going to be out of luck.",
            "And that's going to give us our lower bound."
        ],
        [
            "And so formally we have the following result, which is our strong data processing inequality will focus on a single index, so a single sample I can look at the memory state at time I conditioned on all of the previous memory states.",
            "In this case, we have the following formula result, but the Cal Divergance between P0 and PJ at this point in time is bounded by basically Delta squared times the mutual information between XJ and the memory state conditioned on the output Y.",
            "And we can further upper bound this by just moving the conditioned Y into the mutual information.",
            "So it's bounded by the mutual information between XJ and the memory state and the output.",
            "And the important thing here is this only depends on the single coordinate XJ rather than all of X, and this is going to give us a very useful tensor isation inequality as shown here.",
            "So recall that we care about is bound in the average KL divergent.",
            "We can bound that using this by the sum of the mutual informations.",
            "And then using the fact that the ex general independent, we can further bound this by just the total mutual information.",
            "Which is further bounded by the entropy of ZNYZ as a bee bit vector.",
            "So now we use our memory constraint.",
            "It only has be bits of entropy.",
            "And why is a scalar?",
            "So it only has a constant number of additional bits of entropy and you can see here how we got around our original problem of having dependency between X&Y.",
            "We've done this by basically treating Y as additional memory, so why shows up on the right hand side with the memory?",
            "And now once we have that, all of our other variables X are all independent of each other?",
            "And we can get these sorts of tensor isation bounds that we want to prove our data processing inequality's."
        ],
        [
            "And then we get the answer.",
            "We want that we have roughly be over D bits per round.",
            "Now I'll just quickly go over the upper bound.",
            "As I said, it's much simpler.",
            "We just need to solve this standard L1 regularize dual averaging problem.",
            "For those of you who have seen this before, it will be familiar for those of you haven't.",
            "We're basically minimizing a linearized version of our loss in an online fashion with an L1 regularization term in an L2 regularization term.",
            "And the hard part is going to be determining the support of W. So in particular we need to figure out when Theta will be bigger than Lambda.",
            "Rooten theaters are cumulative gradients.",
            "And so.",
            "The case that we need to distinguish from is if Theta is just sort of according, it doesn't matter, the gradients will be a random walk.",
            "They'll have order root NR regularization is Lambda Rutan.",
            "If Lambda is much bigger than one, it should be easy to distinguish these cases and in fact you can show formally that if you use Count min sketch to try to tell these apart then the memory usage to do this is about D log D over Lambda squared, so you get this nice result where increased regularization helps you computationally, which is actually been seen in a few cases before in the L2 case.",
            "Namely, for the Sdca algorithm and also by Brewer at all who show that regularization makes projection steps easier and certain optimization problems."
        ],
        [
            "So to summarize, we've proved almost matching upper and lower bounds on sparse linear regression for the lower bound, we extended data processing inequality to handle covariates for the upper bound, we use an L1 regularizer to reduce to a sketching problem.",
            "For future work, it would be pretty nice to close this gap between epsilon and epsilon squared.",
            "And also our upper bound assumptions require fairly stringent conditions on the design matrix for the regression problem.",
            "So it would be nice to weaken those assumptions for the upper bound."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'd like to stay.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By asking a question that has become increasingly pertinent in recent years.",
                    "label": 0
                },
                {
                    "sent": "Namely, how do we solve statistical learning problems with limited computational resources, and this has been studied in many settings.",
                    "label": 1
                },
                {
                    "sent": "The most obvious perhaps is to consider the resource of running time which has sparked a good deal of work, including notably the best paper at Colt two years ago by birthday and regularly.",
                    "label": 0
                },
                {
                    "sent": "But we can also consider other computational resource constraints, such As for instance, privacy.",
                    "label": 0
                },
                {
                    "sent": "And recently communication and memory constraints have spawned a lot of recent work.",
                    "label": 0
                },
                {
                    "sent": "I'd like to note in particular the work of Ohad Shamir, whose framework is very similar to the framework that we will consider in this work, and I will be studying the case of sparse linear regression in a memory in communication constraints setting, this is a problem which, despite all of this work above, has not yet had well characterized minimax rates.",
                    "label": 0
                },
                {
                    "sent": "And in this work, we'll go most of the way towards closing this gap of getting matching upper and lower bounds.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to start, I'll give you the setting that we consider, so it's a fairly standard sparse linear regression set up.",
                    "label": 1
                },
                {
                    "sent": "We're doing regression over D dimensional real space and why is function of XA linear function given by W star plus some additive noise, and we'll assume that the true parameters W star are very sparse, so the number of non zero entries is K, where typically will assume K is much less than D. Although our analysis will actually hold for any key all the way down to one or all the way up to the dimension D. And the thing that will sort of add A twist to our framework is that will have a memory constraint.",
                    "label": 0
                },
                {
                    "sent": "So one way to formalize this is that we'll assume that our observations X&Y are observed as a read only stream, so we can see these, but we can't really use that as memory to write to and instead the only memory we get is.",
                    "label": 1
                },
                {
                    "sent": "We can write up to be bits of state to a variable that will denote ZI, and that's the only way we can really transmit information.",
                    "label": 0
                },
                {
                    "sent": "Between samples, so before we see the next sample we have to write all over information to our current memory state and then the sample disappears.",
                    "label": 0
                },
                {
                    "sent": "We can make this more formal by considering the following graphical model, so we have the true parameters W star.",
                    "label": 0
                },
                {
                    "sent": "We see our first input, covariant X1 and X1 and W star together and generate some output Y one and then we can write to our memory state Z.",
                    "label": 0
                },
                {
                    "sent": "We then see our next observation in the next output, and now we write a new memory state Z2, which can only depend on Z1 in the current observations, and there's at most B bits in either of these States and then we just continue this and at the end we need to produce some estimate of the true parameters and you can also consider certain communication constraint versions of this as well, but I'll just focus on the memory constrained case for the purposes of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so a fundamental question we can ask is if we have enough memory to represent the answer, can we also efficiently learn the answer?",
                    "label": 1
                },
                {
                    "sent": "So to represent the answer, it's just, you know.",
                    "label": 0
                },
                {
                    "sent": "K dimensional set of real numbers.",
                    "label": 0
                },
                {
                    "sent": "So it's going to take about K log D bits to represent.",
                    "label": 0
                },
                {
                    "sent": "We might wonder, is that enough bits to actually also learn the answer efficiently?",
                    "label": 0
                },
                {
                    "sent": "Or is there some fundamental barrier beyond just this sort of representation lower bound?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so to get out this more precisely, we can ask how much data we need to obtain a certain error.",
                    "label": 1
                },
                {
                    "sent": "A certain error on the OR a certain error level for the mean squared error will call that level epsilon.",
                    "label": 0
                },
                {
                    "sent": "And in the classical case, this is very well studied.",
                    "label": 0
                },
                {
                    "sent": "Several people have characterized the minimax rate, basically optimally in particular, Wainwright in 2009 showed that the answer is K. Log D over epsilon up to constant factors.",
                    "label": 1
                },
                {
                    "sent": "However, with memory constraints, the answer changes quite a bit, so there's this log D here.",
                    "label": 0
                },
                {
                    "sent": "In the classical case, when we have memory constraints, this log D becomes a D / B.",
                    "label": 1
                },
                {
                    "sent": "And so this is actually an exponential increase in the sample complexity, unless B is quite close to the dimension D. And I should note that this is this only holds in the regime where B is between K log D&D Sobel OK logged, the answer is just you can't get vanish in error and above D it's possible to essentially achieve this rate, so there's various stochastic gradient algorithms that can achieve this optimal rate without do log bits of memory.",
                    "label": 0
                },
                {
                    "sent": "And I'm also hiding just a couple more caveats.",
                    "label": 0
                },
                {
                    "sent": "I'm hiding some structural assumptions on the design matrix needed for this upper bound, and there's of course a little bit of a gap, so there's still this epsilon versus epsilon squared gap, so there's still some remaining questions to answer here, but at least in terms of the dependence on the dimension and the memory constraint, we have pretty much the answer nailed down.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm just going to start with an overview of our proof techniques, and then later we'll delve into a few of the more interesting ideas in a bit more technical detail.",
                    "label": 0
                },
                {
                    "sent": "So the lower bound is basically information theoretic in nature.",
                    "label": 0
                },
                {
                    "sent": "It relies on what's called a strong data processing inequality and to give a bit of intuition about this, let's just imagine the case where we make a single observation.",
                    "label": 0
                },
                {
                    "sent": "So there's a single pair XY that we observe.",
                    "label": 0
                },
                {
                    "sent": "We can then imagine the following Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Where we have the true parameters W star based on those, we generate an observation XY.",
                    "label": 0
                },
                {
                    "sent": "And if we had no memory constraints at all.",
                    "label": 0
                },
                {
                    "sent": "Then we could just store everything and that would take about D bits of memory because X is a D dimensional vector and Y is a scalar.",
                    "label": 0
                },
                {
                    "sent": "If we store all of that information.",
                    "label": 0
                },
                {
                    "sent": "Then we expect to get about a constant number of bits of information about W star because we've stored one linear relation.",
                    "label": 0
                },
                {
                    "sent": "Of course, the actual number of bits will depend on the signal to noise ratio of our problem, but at least for a cartoon picture we can we can sort of take these numbers, take these numbers, and say that if we stored the bits, we get about 1 bit of information about the answer.",
                    "label": 0
                },
                {
                    "sent": "Now, in reality we have a memory constraint of only B bits, where V is less than D. And so we have to sort of choose what information we're going to store.",
                    "label": 0
                },
                {
                    "sent": "An you might expect.",
                    "label": 0
                },
                {
                    "sent": "Well, if we scale down the amount of information from D to B, then the amount of information we get about the answer should also scale down from one to be over D and so since we need cave, it's total to learn K real numbers or over on the order of K bits, then we should need about KD over V samples, which is the lower bound we get.",
                    "label": 0
                },
                {
                    "sent": "And the main challenge here beyond sort of existing work on these strong data processing inequality's, is to handle the dependence between X&Y, so most of the work that I alluded to on the previous slide relies on strong data processing inequality.",
                    "label": 1
                },
                {
                    "sent": "Is that exploit independence of lots of random variables.",
                    "label": 0
                },
                {
                    "sent": "But here in regression X&Y are necessarily dependent and so we need some way of dealing with that and I'll highlight that later.",
                    "label": 0
                },
                {
                    "sent": "For the upper bound, we can use in some sense much simpler techniques, so we basically use a data structure from compressed sensing called accountant sketch data structure together with an L1 regularize dual averaging algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a variant of stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And the key idea.",
                    "label": 0
                },
                {
                    "sent": "Is that the higher the degree of L1 regularization, the easier it is to distinguish coordinates?",
                    "label": 0
                },
                {
                    "sent": "That should be 0 from coordinates that should be non zero.",
                    "label": 0
                },
                {
                    "sent": "In particular this shows up in the memory requirements of the Count, Min sketch structure needed to accurately recover the support of the answer.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd like to delve in a bit more into the lower.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In construction, so to prove the lower bound we need to exhibit some distribution which is hard on average, and this distribution will actually be quite simple, so we need to split the coordinates into K blocks of equal size.",
                    "label": 1
                },
                {
                    "sent": "And we want something case bars.",
                    "label": 0
                },
                {
                    "sent": "So within each of these K blocks will randomly choose one coordinate to be non zero and will give it uniformly random sign.",
                    "label": 0
                },
                {
                    "sent": "So will either be plus Delta with some probability or minus Delta.",
                    "label": 0
                },
                {
                    "sent": "Plus Delta with probability 1/2 minus Delta with probability 1/2 and Delta is a constant that we need to tune to get the optimal bound.",
                    "label": 0
                },
                {
                    "sent": "And so we get something like this, where each block there's a single nonzero coordinate J.",
                    "label": 1
                },
                {
                    "sent": "Using what's called the direct sum argument, we can actually reduce to only considering the case when K = 1.",
                    "label": 0
                },
                {
                    "sent": "So when we have only a single non zero coordinate, this argument is actually fairly nontrivial, but for the purposes of this talk, I'm going to gloss over it and just assume that we can just restrict to the case when we only have one non zero coordinate.",
                    "label": 0
                },
                {
                    "sent": "And then we can use sort of a standard estimation to test inbound showing that sense each of these correspond to well separated parameters.",
                    "label": 0
                },
                {
                    "sent": "They're all at least Euclidean distance Delta squared apart.",
                    "label": 0
                },
                {
                    "sent": "The estimation error is lower bounded by a multiple of the testing error, and so we just have to show that it's hard to figure out which coordinate is non zero.",
                    "label": 0
                },
                {
                    "sent": "And we can use standard information theoretic techniques based on bounding KL divergences to do this.",
                    "label": 0
                },
                {
                    "sent": "So this is in some sense a fairly.",
                    "label": 0
                },
                {
                    "sent": "Standard construction approach.",
                    "label": 0
                },
                {
                    "sent": "Aside from this direct sum argument and so on, the next slide I'll go into what specific properties of the problem we actually use.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, it's important that our covariates are actually random sign vectors.",
                    "label": 0
                },
                {
                    "sent": "To get the lower bound we want.",
                    "label": 0
                },
                {
                    "sent": "And for a bit of notation, will let piece of J denote the distribution conditioned on the non zero coordinate being J will assume without loss of generality that it's positive and this is really the distribution over everything, so I've put Z here, But we can really consider any of XYZ or their joint distribution, and in particular I'd like to point to this graph here of the marginal probability density of Y.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on a given value of XJ.",
                    "label": 1
                },
                {
                    "sent": "So if we condition on XJ depending on whether it's positive or negative, will either get a probability density that's shifted to the right by Delta or shifted to the left by Delta.",
                    "label": 0
                },
                {
                    "sent": "For any other value of X other than J, this shift won't happen because that coordinate of W is zero and so will just get Y being independent of all of those other credits.",
                    "label": 0
                },
                {
                    "sent": "And in fact, we'll define a base distribution where Y is actually independent of every single coordinate, so there's no coordinate Jay that affects the answer.",
                    "label": 0
                },
                {
                    "sent": "And then we just get this marginal distribution shown here in green.",
                    "label": 0
                },
                {
                    "sent": "And to prove our lower bound will use what's called.",
                    "label": 0
                },
                {
                    "sent": "Also odds method which is a bound on the on the testing error in terms of this average Cal divergance.",
                    "label": 0
                },
                {
                    "sent": "Highlighted in red here so.",
                    "label": 0
                },
                {
                    "sent": "The intuition, or sorry a key fact, is that Y&XJ together are independent of all of the other coordinates under PJ.",
                    "label": 1
                },
                {
                    "sent": "And why is this?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just this thing I said before that all of the coordinates other than J just don't affect the output because the parameters are zero and all of those coordinates.",
                    "label": 0
                },
                {
                    "sent": "And so before we prove these strong data processing inequality is we needed sort of everything to be independent.",
                    "label": 0
                },
                {
                    "sent": "Here, we're going to exploit this slightly weaker independence property that these two variables are independent of everything else conditioned on a particular distribution.",
                    "label": 0
                },
                {
                    "sent": "And so the intuition is then that really the only thing that can tell us anything about PJ is the coordinated XJ, because for any other coordinate we just get something that looks exactly like P0.",
                    "label": 1
                },
                {
                    "sent": "We just get this green sort of spread out mass, and so the only way to make the kale divergent between P0 and PJ large is to store information about XJ.",
                    "label": 0
                },
                {
                    "sent": "And so unless we store information about the majority of the X is.",
                    "label": 0
                },
                {
                    "sent": "The coordinates of X, the average KL divergences going to be pretty small, and since we only have B bits and there's D coordinates of X to store, we're basically going to be out of luck.",
                    "label": 0
                },
                {
                    "sent": "And that's going to give us our lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so formally we have the following result, which is our strong data processing inequality will focus on a single index, so a single sample I can look at the memory state at time I conditioned on all of the previous memory states.",
                    "label": 1
                },
                {
                    "sent": "In this case, we have the following formula result, but the Cal Divergance between P0 and PJ at this point in time is bounded by basically Delta squared times the mutual information between XJ and the memory state conditioned on the output Y.",
                    "label": 0
                },
                {
                    "sent": "And we can further upper bound this by just moving the conditioned Y into the mutual information.",
                    "label": 0
                },
                {
                    "sent": "So it's bounded by the mutual information between XJ and the memory state and the output.",
                    "label": 0
                },
                {
                    "sent": "And the important thing here is this only depends on the single coordinate XJ rather than all of X, and this is going to give us a very useful tensor isation inequality as shown here.",
                    "label": 0
                },
                {
                    "sent": "So recall that we care about is bound in the average KL divergent.",
                    "label": 0
                },
                {
                    "sent": "We can bound that using this by the sum of the mutual informations.",
                    "label": 0
                },
                {
                    "sent": "And then using the fact that the ex general independent, we can further bound this by just the total mutual information.",
                    "label": 0
                },
                {
                    "sent": "Which is further bounded by the entropy of ZNYZ as a bee bit vector.",
                    "label": 0
                },
                {
                    "sent": "So now we use our memory constraint.",
                    "label": 0
                },
                {
                    "sent": "It only has be bits of entropy.",
                    "label": 0
                },
                {
                    "sent": "And why is a scalar?",
                    "label": 0
                },
                {
                    "sent": "So it only has a constant number of additional bits of entropy and you can see here how we got around our original problem of having dependency between X&Y.",
                    "label": 0
                },
                {
                    "sent": "We've done this by basically treating Y as additional memory, so why shows up on the right hand side with the memory?",
                    "label": 0
                },
                {
                    "sent": "And now once we have that, all of our other variables X are all independent of each other?",
                    "label": 0
                },
                {
                    "sent": "And we can get these sorts of tensor isation bounds that we want to prove our data processing inequality's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we get the answer.",
                    "label": 0
                },
                {
                    "sent": "We want that we have roughly be over D bits per round.",
                    "label": 0
                },
                {
                    "sent": "Now I'll just quickly go over the upper bound.",
                    "label": 1
                },
                {
                    "sent": "As I said, it's much simpler.",
                    "label": 0
                },
                {
                    "sent": "We just need to solve this standard L1 regularize dual averaging problem.",
                    "label": 1
                },
                {
                    "sent": "For those of you who have seen this before, it will be familiar for those of you haven't.",
                    "label": 0
                },
                {
                    "sent": "We're basically minimizing a linearized version of our loss in an online fashion with an L1 regularization term in an L2 regularization term.",
                    "label": 1
                },
                {
                    "sent": "And the hard part is going to be determining the support of W. So in particular we need to figure out when Theta will be bigger than Lambda.",
                    "label": 0
                },
                {
                    "sent": "Rooten theaters are cumulative gradients.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "The case that we need to distinguish from is if Theta is just sort of according, it doesn't matter, the gradients will be a random walk.",
                    "label": 0
                },
                {
                    "sent": "They'll have order root NR regularization is Lambda Rutan.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is much bigger than one, it should be easy to distinguish these cases and in fact you can show formally that if you use Count min sketch to try to tell these apart then the memory usage to do this is about D log D over Lambda squared, so you get this nice result where increased regularization helps you computationally, which is actually been seen in a few cases before in the L2 case.",
                    "label": 0
                },
                {
                    "sent": "Namely, for the Sdca algorithm and also by Brewer at all who show that regularization makes projection steps easier and certain optimization problems.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we've proved almost matching upper and lower bounds on sparse linear regression for the lower bound, we extended data processing inequality to handle covariates for the upper bound, we use an L1 regularizer to reduce to a sketching problem.",
                    "label": 1
                },
                {
                    "sent": "For future work, it would be pretty nice to close this gap between epsilon and epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "And also our upper bound assumptions require fairly stringent conditions on the design matrix for the regression problem.",
                    "label": 0
                },
                {
                    "sent": "So it would be nice to weaken those assumptions for the upper bound.",
                    "label": 0
                }
            ]
        }
    }
}