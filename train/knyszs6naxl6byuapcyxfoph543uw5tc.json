{
    "id": "knyszs6naxl6byuapcyxfoph543uw5tc",
    "title": "Scaled Gradients on Grassmann Manifolds for Matrix Completion",
    "info": {
        "author": [
            "Thanh Ngo, Department of Computer Science and Engineering, University of Minnesota"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Collaborative Filtering",
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/machine_ngo_matrix_completion/",
    "segmentation": [
        [
            "Hi, the problem is many of you might know is to recover low rank matrix from the subset of entries that you can observe and the common application is recommended system."
        ],
        [
            "Current methods to solve this problem can be categorized into 2 main approaches.",
            "The first one is nuclear norm minimization methods, and with this formulation the problem is convex and they have nice recovery guarantees.",
            "As far as I know, methods to algorithms to solve this problem, either if they use the singular value decomposition, specifically the singular value thresholding operator and hence they are quite expensive.",
            "Another line of work, which is also our focus, is the fixed wing method.",
            "With this formulation, that problem is nonconvex but algorithm to solve it are usually faster nuclear norm minimization, because the SVD computation can be avoid this despite the non convexity case of an O and Montanari have established recovery results in their paper with the up space method based on the minimization algorithm on the Grassmann manifold.",
            "Another interesting work is L Moffett.",
            "Which is a subspace iteration like method.",
            "This method already has local convergence, but it is very efficient and it works well in practice.",
            "So there is a gap here between the theory and practical implementation of fixed wing methods and our."
        ],
        [
            "Contribution is to narrow this gap by establishing a connection between op space and Elma fit by so that we can obtain faster method in our methods.",
            "Also work well with you condition matrix and also we can still maintain exactly coverage results derived from up space.",
            "The main idea is pretty simple.",
            "We use a non Canonical metric in a product under Grassmann manifold and this inner product we call scaled inner product.",
            "It reflects the level set, the shape of the level curves by concurrent approximation of the singular values that we already have in each iterations and based on this we can derive 1st order method which converge fast and is efficiently in each iteration."
        ],
        [
            "And this graph shows that our method in blue outperforms several other methods for a 10,000 by 10,000 rank, 10 matrix and a singular value spectrum is from 1000 to 10,000.",
            "An here are some interesting feature work that we're interested in.",
            "First, it's to the lower bound for recovery from up space depends on the condition number of the matrix to the power 6, and we intend to.",
            "We cannot dependency.",
            "The second thing is to slap recovery guarantees for fast variants.",
            "An finally regularization for those fast firing methods.",
            "Thanks, last but not least, I'll post is tonight and the number is the lucky #30.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, the problem is many of you might know is to recover low rank matrix from the subset of entries that you can observe and the common application is recommended system.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Current methods to solve this problem can be categorized into 2 main approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one is nuclear norm minimization methods, and with this formulation the problem is convex and they have nice recovery guarantees.",
                    "label": 1
                },
                {
                    "sent": "As far as I know, methods to algorithms to solve this problem, either if they use the singular value decomposition, specifically the singular value thresholding operator and hence they are quite expensive.",
                    "label": 0
                },
                {
                    "sent": "Another line of work, which is also our focus, is the fixed wing method.",
                    "label": 1
                },
                {
                    "sent": "With this formulation, that problem is nonconvex but algorithm to solve it are usually faster nuclear norm minimization, because the SVD computation can be avoid this despite the non convexity case of an O and Montanari have established recovery results in their paper with the up space method based on the minimization algorithm on the Grassmann manifold.",
                    "label": 0
                },
                {
                    "sent": "Another interesting work is L Moffett.",
                    "label": 1
                },
                {
                    "sent": "Which is a subspace iteration like method.",
                    "label": 0
                },
                {
                    "sent": "This method already has local convergence, but it is very efficient and it works well in practice.",
                    "label": 0
                },
                {
                    "sent": "So there is a gap here between the theory and practical implementation of fixed wing methods and our.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contribution is to narrow this gap by establishing a connection between op space and Elma fit by so that we can obtain faster method in our methods.",
                    "label": 0
                },
                {
                    "sent": "Also work well with you condition matrix and also we can still maintain exactly coverage results derived from up space.",
                    "label": 1
                },
                {
                    "sent": "The main idea is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "We use a non Canonical metric in a product under Grassmann manifold and this inner product we call scaled inner product.",
                    "label": 0
                },
                {
                    "sent": "It reflects the level set, the shape of the level curves by concurrent approximation of the singular values that we already have in each iterations and based on this we can derive 1st order method which converge fast and is efficiently in each iteration.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this graph shows that our method in blue outperforms several other methods for a 10,000 by 10,000 rank, 10 matrix and a singular value spectrum is from 1000 to 10,000.",
                    "label": 0
                },
                {
                    "sent": "An here are some interesting feature work that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "First, it's to the lower bound for recovery from up space depends on the condition number of the matrix to the power 6, and we intend to.",
                    "label": 1
                },
                {
                    "sent": "We cannot dependency.",
                    "label": 1
                },
                {
                    "sent": "The second thing is to slap recovery guarantees for fast variants.",
                    "label": 0
                },
                {
                    "sent": "An finally regularization for those fast firing methods.",
                    "label": 0
                },
                {
                    "sent": "Thanks, last but not least, I'll post is tonight and the number is the lucky #30.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}