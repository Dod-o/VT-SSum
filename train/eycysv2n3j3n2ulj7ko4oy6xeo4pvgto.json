{
    "id": "eycysv2n3j3n2ulj7ko4oy6xeo4pvgto",
    "title": "The oracle Complexity of Smooth Convex Optimization in Nonstandard Settings",
    "info": {
        "author": [
            "Crist\u00f3bal Guzm\u00e1n, University of Chile"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_guzman_nonstandard_settings/",
    "segmentation": [
        [
            "So this talk is about Oracle complexity of convex optimization.",
            "I've been working in the last few years alot on this like proving lower bounds for 1st order methods and we came up with this gaps on upper lower complexity bounds that I think are really exciting and more people should know about this.",
            "So OK let's get started."
        ],
        [
            "So first of all, we consider the problem of minimization of classes of convex functions that we parameterized in some way, and the way we do it here.",
            "It's considering what we call capelis smooth convex functions.",
            "OK, so there's this paramita recap here.",
            "Which for capitals one is essentially in a smooth case like Lipschitz continuous convex functions, capitals two corresponds to this move case.",
            "As you can see, like the standard condition or lipstick continuous gradients.",
            "But we also have intermediate degrees of smoothness, so that's one part of the story we consider a certain optimization domain, which is symmetric convex body and will assume it's simple enough so we can perform projections and other simple operations here.",
            "The most important ingredient here is the interaction with an Oracle, so an algorithm doesn't have an explicit access to the objective function.",
            "Rather, it can query points in the domain and the Oracle replies some local information.",
            "So there's a slightly more general definition of what this means.",
            "But for the sake of this talk, let's just think about it as giving you know the value of the function and the gradient at that point.",
            "OK."
        ],
        [
            "So our measure of performance is these algorithms will perform queries to the Oracle.",
            "The Oracle will reply this this information an after T potentially adaptive Oracle queries.",
            "The algorithm gives an output.",
            "And we measure the performance of the algorithm in terms of minimax or worst case Oracle complexity.",
            "Meaning the worst on the class of functions of the gap between the actual optimal solution and what the algorithm outputs.",
            "So this is like the classical notion that people use in convex optimization."
        ],
        [
            "What I'm calling here about this, like the standard setting, means the fact that the optimization domain is a symmetric convex body.",
            "This guy in the Sonoran and we used that known for measuring smoothness up here.",
            "OK.",
            "So this open problems related to what happens if we consider other norms for measuring this business here.",
            "OK, and the question is why you should care about this.",
            "Well, simply example is considering regression models like standard linear regression."
        ],
        [
            "Here, for instance, we are looking for a predictor by structural assumptions in the model.",
            "You can consider your optimization domain to be a certain LP wall in and dimensions an you would have access to, you know, random input output output examples an again.",
            "By the way, you obtain these observations, they turn out you have a certain other Q storm.",
            "Northbound and you start means you were taking the conjugative Q which is between one and 50 OK.",
            "So if you have this."
        ],
        [
            "This data and you want to obtain this predictor.",
            "What you would usually do is just consider a linear regression model where you would put the constraint here on your predictor.",
            "But your and this is another people, but the smoothness in the function naturally can be quantified in.",
            "LQ Norm is set, so we have this sort of mismatch between the geometry of the domain and the geometry of the objective function.",
            "So what happens here?",
            "Well, we're able to prove you know nontrivial lower bounds on the number of grading computations you need to perform in this model.",
            "And I don't have the time to go deeper into what these lower bounds mean, but what is important here is that in the case when P is larger than Q.",
            "What we obtain is nearly optimal, so it turns out that the lower bounds will attain match certain versions of Nesterov Accelerative method.",
            "However, when P is smaller than Q, are lower bounds are not tytan we get significant gaps with existing algorithms.",
            "And this open question is precisely about, you know, closing these gaps.",
            "These gaps are actually worsen for larger smoothness parameters, so in this most cases when it becomes more important.",
            "Also, our technique for proving lower bounds do not only work for these like LP LQ setup, but in fact also apply to the metrics Schatten norm setup.",
            "So also like if you're doing matrix completion or things like that.",
            "Lower bounds, actually their analysts."
        ],
        [
            "OK, so finally just to show you that this is not a purely theoretical question.",
            "Like the case B equals one key equals to sort of coincides with what you would do for when you're looking for sparse solution.",
            "For complex sensing, an.",
            "What happens here is best.",
            "Existing algorithm is nestors method which gives you a convergence rate of 1 or 2 ^2.",
            "The strongest are lower bounds, can go is up to 1 / T Q.",
            "So there's a question of whether you can actually improve over natural performance an our conjecture here is that in fact you should be able to improve this convergence rate.",
            "So with this I will stop.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk is about Oracle complexity of convex optimization.",
                    "label": 0
                },
                {
                    "sent": "I've been working in the last few years alot on this like proving lower bounds for 1st order methods and we came up with this gaps on upper lower complexity bounds that I think are really exciting and more people should know about this.",
                    "label": 0
                },
                {
                    "sent": "So OK let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, we consider the problem of minimization of classes of convex functions that we parameterized in some way, and the way we do it here.",
                    "label": 0
                },
                {
                    "sent": "It's considering what we call capelis smooth convex functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's this paramita recap here.",
                    "label": 0
                },
                {
                    "sent": "Which for capitals one is essentially in a smooth case like Lipschitz continuous convex functions, capitals two corresponds to this move case.",
                    "label": 0
                },
                {
                    "sent": "As you can see, like the standard condition or lipstick continuous gradients.",
                    "label": 0
                },
                {
                    "sent": "But we also have intermediate degrees of smoothness, so that's one part of the story we consider a certain optimization domain, which is symmetric convex body and will assume it's simple enough so we can perform projections and other simple operations here.",
                    "label": 0
                },
                {
                    "sent": "The most important ingredient here is the interaction with an Oracle, so an algorithm doesn't have an explicit access to the objective function.",
                    "label": 0
                },
                {
                    "sent": "Rather, it can query points in the domain and the Oracle replies some local information.",
                    "label": 0
                },
                {
                    "sent": "So there's a slightly more general definition of what this means.",
                    "label": 0
                },
                {
                    "sent": "But for the sake of this talk, let's just think about it as giving you know the value of the function and the gradient at that point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our measure of performance is these algorithms will perform queries to the Oracle.",
                    "label": 0
                },
                {
                    "sent": "The Oracle will reply this this information an after T potentially adaptive Oracle queries.",
                    "label": 0
                },
                {
                    "sent": "The algorithm gives an output.",
                    "label": 0
                },
                {
                    "sent": "And we measure the performance of the algorithm in terms of minimax or worst case Oracle complexity.",
                    "label": 0
                },
                {
                    "sent": "Meaning the worst on the class of functions of the gap between the actual optimal solution and what the algorithm outputs.",
                    "label": 0
                },
                {
                    "sent": "So this is like the classical notion that people use in convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm calling here about this, like the standard setting, means the fact that the optimization domain is a symmetric convex body.",
                    "label": 1
                },
                {
                    "sent": "This guy in the Sonoran and we used that known for measuring smoothness up here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this open problems related to what happens if we consider other norms for measuring this business here.",
                    "label": 0
                },
                {
                    "sent": "OK, and the question is why you should care about this.",
                    "label": 0
                },
                {
                    "sent": "Well, simply example is considering regression models like standard linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, for instance, we are looking for a predictor by structural assumptions in the model.",
                    "label": 0
                },
                {
                    "sent": "You can consider your optimization domain to be a certain LP wall in and dimensions an you would have access to, you know, random input output output examples an again.",
                    "label": 0
                },
                {
                    "sent": "By the way, you obtain these observations, they turn out you have a certain other Q storm.",
                    "label": 0
                },
                {
                    "sent": "Northbound and you start means you were taking the conjugative Q which is between one and 50 OK.",
                    "label": 0
                },
                {
                    "sent": "So if you have this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data and you want to obtain this predictor.",
                    "label": 0
                },
                {
                    "sent": "What you would usually do is just consider a linear regression model where you would put the constraint here on your predictor.",
                    "label": 0
                },
                {
                    "sent": "But your and this is another people, but the smoothness in the function naturally can be quantified in.",
                    "label": 0
                },
                {
                    "sent": "LQ Norm is set, so we have this sort of mismatch between the geometry of the domain and the geometry of the objective function.",
                    "label": 0
                },
                {
                    "sent": "So what happens here?",
                    "label": 0
                },
                {
                    "sent": "Well, we're able to prove you know nontrivial lower bounds on the number of grading computations you need to perform in this model.",
                    "label": 0
                },
                {
                    "sent": "And I don't have the time to go deeper into what these lower bounds mean, but what is important here is that in the case when P is larger than Q.",
                    "label": 0
                },
                {
                    "sent": "What we obtain is nearly optimal, so it turns out that the lower bounds will attain match certain versions of Nesterov Accelerative method.",
                    "label": 0
                },
                {
                    "sent": "However, when P is smaller than Q, are lower bounds are not tytan we get significant gaps with existing algorithms.",
                    "label": 0
                },
                {
                    "sent": "And this open question is precisely about, you know, closing these gaps.",
                    "label": 0
                },
                {
                    "sent": "These gaps are actually worsen for larger smoothness parameters, so in this most cases when it becomes more important.",
                    "label": 0
                },
                {
                    "sent": "Also, our technique for proving lower bounds do not only work for these like LP LQ setup, but in fact also apply to the metrics Schatten norm setup.",
                    "label": 0
                },
                {
                    "sent": "So also like if you're doing matrix completion or things like that.",
                    "label": 0
                },
                {
                    "sent": "Lower bounds, actually their analysts.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so finally just to show you that this is not a purely theoretical question.",
                    "label": 0
                },
                {
                    "sent": "Like the case B equals one key equals to sort of coincides with what you would do for when you're looking for sparse solution.",
                    "label": 0
                },
                {
                    "sent": "For complex sensing, an.",
                    "label": 0
                },
                {
                    "sent": "What happens here is best.",
                    "label": 0
                },
                {
                    "sent": "Existing algorithm is nestors method which gives you a convergence rate of 1 or 2 ^2.",
                    "label": 0
                },
                {
                    "sent": "The strongest are lower bounds, can go is up to 1 / T Q.",
                    "label": 0
                },
                {
                    "sent": "So there's a question of whether you can actually improve over natural performance an our conjecture here is that in fact you should be able to improve this convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So with this I will stop.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}