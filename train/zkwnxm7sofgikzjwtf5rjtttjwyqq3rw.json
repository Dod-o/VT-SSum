{
    "id": "zkwnxm7sofgikzjwtf5rjtttjwyqq3rw",
    "title": "How to Teach Support Vector Machine to Learn Vector Outputs",
    "info": {
        "author": [
            "Sandor Szedmak, School of Electronics and Computer Science, University of Southampton"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/oh06_szedmak_htsvm/",
    "segmentation": [
        [
            "This presentation will be apart of raw TV long tutorials that is behind this I selected some slides which are really relating to the middle class learning multitask learning and.",
            "By it can be useful for everybody.",
            "This application is.",
            "Computationally senses are very, very cheap and very efficient.",
            "And I'm going immediately into the main part to avoid any delay.",
            "You know the origonal SVM only.",
            "I would like to record the base ID's we are looking for a maximum margin between the positive and negative cases and for this we have a parameter W. We have a parameter W which interpreted as a normal vector of the separating hyperplane."
        ],
        [
            "And."
        ],
        [
            "So the original was binary output space in normal vector.",
            "Let's redefine everything by the arbitrary outputs.",
            "Maybe we can assume it is embedded into a. Vector linear vector space that I assume this is Hilbert space and we can say the W the transposes the W is really a linear operator, arbitrary linear operator which projecting the input space into the out with space.",
            "And aim to find a higher similarity between the input and output.",
            "In the special case in the original SVM, the output space is 1 dimensional space and values are normalized to plus one and minus one."
        ],
        [
            "And I'm good."
        ],
        [
            "Immediately to compare that it claims a very small modification, really.",
            "In the objective function in originally we compute.",
            "The square norm of the W of the normal vector.",
            "It is transformed because we have a linear operator.",
            "Let's use the Frobenius norm instead of, which is very straightforward generalization of the original.",
            "This happens in a lot of papers.",
            "Koby Krammer and singer multiclass classification in your paper as well, there's a lot of cases use this step.",
            "The difference is really on the constraint, the constraint they changed.",
            "The original data boards.",
            "We have the normal vector we have the feature vector and some who bias and multiply by the label 'cause it's changed to general vector that may be embedded into our.",
            "Output vector space.",
            "We have a linear operator and we have the input vector as similarity that the original and we have the inner product defined in the space of the outputs.",
            "This is only which is more defined in the system.",
            "All of their things, the flex are the same and the corresponding penalty term in the objective is the same.",
            "That is, changes really.",
            "Somehow the interpretation of the W. If we have a very generous space, we can have a very general space.",
            "Maybe this dimension can be infinite in both sides we have.",
            "We can consider functions is not really matters.",
            "Yes, if we have infinite dimensional space, we need some regularity on this space to compute this.",
            "Which is matters.",
            "The dwarf solution of this."
        ],
        [
            "Is practically the same that the original binary asphere.",
            "We have a kernel computed component by Component Vice product between the input and output.",
            "It was in the grammar and single paper very similarly, but they assume a very large kernel scaling with the size of the number of classes.",
            "Here we have the same size.",
            "And we have a constraint which contains explicita representation of the embedded on the outputs.",
            "I don't want to go to the details, but it's a very trivial it can be transformed into another constraint with the same feasibility domain with implicit representation.",
            "This constraints.",
            "That it happens if.",
            "And.",
            "Which one?",
            "Then you just press it OK?",
            "These constraints can happen if you have a bias in the original, the original.",
            "Prime this bias."
        ],
        [
            "Is a translation we have somewhere linear transformation.",
            "With this bias we have affine transformation in the.",
            "Define the input space is transformed the input into the output space.",
            "That we really need here is only a linear function on the W. It maybe not the inner product.",
            "In general we don't need to assume it is a Hilbert space because to derive the dual we need the courage contactor condition which.",
            "Doesn't require the.",
            "The Hilbert space.",
            "So this is the base configuration that we can solve especially efficiently if we have node.",
            "Spa"
        ],
        [
            "Should biased case.",
            "Otherwise, we have this objective function bunker malfunction defining this way and we have a box constraint.",
            "This is maybe the most trivial optimization problem that any whole somebody can solve.",
            "A very simple.",
            "Coordinate descent method can solve very efficiently this in very large scale.",
            "The corresponding Matlab code which solves the full problem with the bias and with the box constraint you can find on the Internet.",
            "It is modular.",
            "Put some people downloaded and used.",
            "And the question, how can we use this?",
            "One is the problem.",
            "How can we get the prediction?",
            "The prediction looks like that.",
            "If the output is somehow embedded into a feature space we have made."
        ],
        [
            "There's trouble because we have the.",
            "Implicit representation of the output.",
            "And the solution generally that the people present."
        ],
        [
            "To compute the argmax relative to the other possible puts, it isn't happens in very lot of papers.",
            "Covering similar problems.",
            "But in some cases relatively a lot of cases we can simplify this problem.",
            "One is when, let's say we have the multiclass classification problem.",
            "Sorry.",
            "In the multiclass classification prob."
        ],
        [
            "This vectors can be explicitly computed by indicator vectors, or some who using a hyper tetrahedron vectors.",
            "This performs the better."
        ],
        [
            "This is the computation of the vectors.",
            "It seems probably the best solution for representing multi classes in the Hilbert space.",
            "I'm sorry I should be very fast because I have I don't know maybe 10 minutes or five.",
            "It is somewhere very.",
            "Something to give impression about the solution and to show the efficiency.",
            "Let's go to the.",
            "Hi rocky, learning.",
            "The hierarchy learning means we have a multiclass problem several."
        ],
        [
            "Classes and somehow that is hierarchy defined on this classes.",
            "And the output karnadi ride in a very simple way somehow, considering the shortest paths in this hierarchy.",
            "If the classes are closed in the hierarchy, the corresponding kernel value is large and the colors more or less covered.",
            "This thread are the highest value and dark blue the lowest.",
            "This is our output Carnell and in the Viper data set the number of classes in the high rack is 187.",
            "I don't 386 or 87, it means really very large.",
            "The data set size is 1500 something 500 around.",
            "And only a result."
        ],
        [
            "To compare some plot SVM, no advisors VM which is completed which is a Bianchi and others has a regressive solution.",
            "You are also present depressive tomorrow.",
            "The approach behind the HM 3 and we have this MMR solution.",
            "Benomar stands for the maximum margin robot.",
            "Give something.",
            "Stupid enough name?",
            "And the result they are the.",
            "The test errors."
        ],
        [
            "You can see the testers are relatively in general very large.",
            "In 2006 it was I found later than this table was created chokachi this and Hoffman and you are in paper.",
            "In that paper the error was 72.",
            "72 something.",
            "Here they are in our MMR gives 47.",
            "You can see surprisingly works very well the other which is very important to compute.",
            "This takes hours generally the simple SVM takes 2 hours.",
            "In our case.",
            "Device."
        ],
        [
            "Apple said that in our corner is 1.9 second in the polynomial 1.2 and the Gaussian less than one second.",
            "It is computed by pure Matlab code.",
            "To be adaptive card now takes 5 or 6 seconds.",
            "It is clearly the solution of the optimization problem.",
            "In our case, is somehow not exactly explicit the output vectors, But the computation of data structure is relatively trivialities.",
            "Seconds for all of the test set.",
            "It means we have completely a solution within 10 seconds, and the optimization takes only one.",
            "Point, it has a huge advantage if you have a lot of parameters and structural learning too.",
            "In the validation to find the optimal value in the very large Lane range of the.",
            "Possible parameters in this case is.",
            "Is a reliable and feasible approach and in multi task learning.",
            "Maybe we can use."
        ],
        [
            "These two approaches money is somehow we can say additive approach.",
            "We can put together.",
            "Let's see if we have a binary multi task learning and with the assume we assume the domain are the same.",
            "And in this case we have several views and we have several labels and a very simple case to present in this way the problem.",
            "The concept vans in the dual space.",
            "The kernel is a pointwise product of the input kernel and some of the output kernels.",
            "It means the base to our problem.",
            "Under that are some of the input panels.",
            "Some of the input gardeners and output canals is point was product here so.",
            "The other view when we would like to get something more complex, I used here."
        ],
        [
            "But it can be extended for several others.",
            "If you compute here the tensor product between the two views, the corresponding do.",
            "A kernel is nothing else but the pointwise product of the corresponding.",
            "Feels.",
            "And we have the same dual.",
            "We can compute this prediction.",
            "And solve this problem is really reasonable very fast.",
            "And so the question in the experiments, did you have to be included as a very?",
            "This is that I how much time that I have.",
            "Now.",
            "I don't like him.",
            "So very sad.",
            "Because what is the problem here immediately in the applications?"
        ],
        [
            "One is the problem if the norm of device for this are very different, because this kind of constraints cannot be really satisfied in this way.",
            "So one is we need to somehow normalize and why otherwise it doesn't work correctly.",
            "And if, let's see if you have a really wonderful both Debian the Axis 1 dimensional.",
            "I hope it helps.",
            "Very simple case X&Y.",
            "I'm sorry.",
            "That I missed here before how it looks like in the practice.",
            "This constraint can be rewritten."
        ],
        [
            "The steps are not so important.",
            "At the end we arrived to a bond class SVM defined in the tensor product space of the input and output.",
            "This is by the complexity independent on the output dimension.",
            "So if we have this one class SVM that we solve really for any kind of input output, and if he had one dimensional."
        ],
        [
            "Items we have the points.",
            "Let's say we have a regression problem to predict the by using text.",
            "If you have one class problem, this is the origin.",
            "This is the hyperplane, something completely useless.",
            "And how can we make a regression?",
            "In general, we need to exploit the geometric structure of this that I mentioned.",
            "We have somewhere in our operator there the solution."
        ],
        [
            "I'm sorry very fastly.",
            "Who's the stereographic projection?",
            "Here?",
            "We have the X.",
            "We have a bowl.",
            "We have a North Pole and the image of the X is on the intersection of the line and the circle.",
            "Here every images has known one.",
            "And after when we computed the regression problem, we can project back because it is explicit and invertible representation and we receive something like this.",
            "So it because here we have no the same problem.",
            "It is not a distance minimisation but inner product maximization not is really Max images maximization.",
            "We would like to get sufficiently large in that product between the output and input.",
            "And here in the bias.",
            "If he had this kind of representation, but the norms are.",
            "One, we don't need a bias.",
            "And this kind of what is?",
            "It is almost the same that the Gaussian kernel.",
            "Only the difference, the Gaussian kernel not invertible.",
            "This is invertable, the Gaussian kernel.",
            "Every items is normalized.",
            "It means if, let's say if you use Gaussian kernel, we have a normalized input or a normalized output.",
            "But because of the explicit representation is missing, we need the special operator find.",
            "Here we can do much complex embedded into the ball.",
            "Pit is the simplest and it can.",
            "It can happen in the kernel space.",
            "Independent under Canada presentation.",
            "OK, I think I'm.",
            "I may be able to fast, but I wanted to finish because what is the time?",
            "I think I time.",
            "Questions yes.",
            "So just make sure I understand.",
            "Sorry I couldn't hear you follow Jesus.",
            "I assume you are in the multiclass study, yes, so you have this kernel.",
            "At some point you projected it, slide about yes.",
            "One moment I'm going there.",
            "You can ask you up.",
            "Yes, yes, sorry.",
            "The hierarchical.",
            "Sure, but.",
            "Yes, how do you feel that Colonel on the right?"
        ],
        [
            "The Colonel was in the very concrete case, you know.",
            "I mean, how can you set the value of the function on the?",
            "But this is I would like to say, let's say if you have these two points.",
            "The similarity somehow that we can use the in the distance is here to in the graph here for, let's compute the inverse.",
            "It is a very simple case.",
            "And in their practices."
        ],
        [
            "Really value is 1 parameter which somehow saying nothing else but edge."
        ],
        [
            "Far from the root is more important than the closer, but it was a parameter.",
            "It is wait.",
            "Maybe you carry scale and the edge close to the root is more important.",
            "It was the only parameter that was used in the Bible data set.",
            "This parameter was zero point 5.",
            "It means bait is 1, it is 0.5 and square of 0.5.",
            "Voice in the Bible.",
            "The choice of this these values actually affect the results.",
            "The accuracy results that you are.",
            "If I'm changing this weight.",
            "Yes, it influences in the.",
            "For example, if I use only the lower level, it means all others are.",
            "It means here the weights are one, but maybe others are zero.",
            "It means a multiclass problem is very in here it was 53.",
            "I don't remember exactly."
        ],
        [
            "Petite influences it was banned on this parameter.",
            "There was an optimization to get the best weight which dominates the edges on the path to the root and it was 47.",
            "Using the Gaussian kernels 46.",
            "The corresponding Matlab code is on the Internet, or I can send.",
            "Choose this convenience, not just cause some similarities is supported because in the optimized opprobrious known that you can trivially derivate.",
            "Yeah, you can use something else but the question in the computational sense, the Frobenius norm.",
            "If you compute the derivative of the probing, I'm sorry.",
            "Because the derivative of this function is now."
        ],
        [
            "Think about W. If you have a complex, more complex function.",
            "The question the dual problem, the complexity of the dual problem.",
            "Here it is most trivial choice for W. This function is purposeful for cleaning.",
            "Doing this kind of stuff, sorry.",
            "One yesterday.",
            "Choosing more contracts.",
            "So.",
            "For the purpose of doing that.",
            "Is really the purpose?",
            "Yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes.",
            "In a special cases, maybe it's important to you.",
            "Is not this known, but there let's see the maximum eigenvalues of the maximum.",
            "Angular value because it's not square.",
            "And it gives some who here the margin is nothing else but the inverse of the singular value.",
            "We have one here, some who the margins because we have several margins, are the inverses of the singular values and we would like to minimize of the sum of the square of the singular values.",
            "Typically.",
            "There were four other other things funny kind of classification, because out of these constraints, for example, this is the sum of probabilities.",
            "What those methods?",
            "They hopefully the outcome would be in the similar range, like more like the progression type of problem.",
            "Instead of this kind of problem.",
            "I am using at this moment for regression.",
            "Very pure regression is weather prediction for several input on several output variables.",
            "I need the normalization that I mentioned is very important without a special kind of normalization doesn't work but after.",
            "You have really the regression problem.",
            "Apps.",
            "The classification he."
        ],
        [
            "Multiclass?",
            "You explicitly have the features to compute the not, not, not really.",
            "If you use this."
        ],
        [
            "This formula you don't need because here you have the kernels in.",
            "If the number of classes is relatively small in the bipod is 186.",
            "The computation is very trivial in this case.",
            "Sorry.",
            "More questions.",
            "OK, so it's like."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This presentation will be apart of raw TV long tutorials that is behind this I selected some slides which are really relating to the middle class learning multitask learning and.",
                    "label": 0
                },
                {
                    "sent": "By it can be useful for everybody.",
                    "label": 0
                },
                {
                    "sent": "This application is.",
                    "label": 0
                },
                {
                    "sent": "Computationally senses are very, very cheap and very efficient.",
                    "label": 0
                },
                {
                    "sent": "And I'm going immediately into the main part to avoid any delay.",
                    "label": 0
                },
                {
                    "sent": "You know the origonal SVM only.",
                    "label": 0
                },
                {
                    "sent": "I would like to record the base ID's we are looking for a maximum margin between the positive and negative cases and for this we have a parameter W. We have a parameter W which interpreted as a normal vector of the separating hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the original was binary output space in normal vector.",
                    "label": 1
                },
                {
                    "sent": "Let's redefine everything by the arbitrary outputs.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can assume it is embedded into a. Vector linear vector space that I assume this is Hilbert space and we can say the W the transposes the W is really a linear operator, arbitrary linear operator which projecting the input space into the out with space.",
                    "label": 1
                },
                {
                    "sent": "And aim to find a higher similarity between the input and output.",
                    "label": 0
                },
                {
                    "sent": "In the special case in the original SVM, the output space is 1 dimensional space and values are normalized to plus one and minus one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Immediately to compare that it claims a very small modification, really.",
                    "label": 0
                },
                {
                    "sent": "In the objective function in originally we compute.",
                    "label": 0
                },
                {
                    "sent": "The square norm of the W of the normal vector.",
                    "label": 0
                },
                {
                    "sent": "It is transformed because we have a linear operator.",
                    "label": 0
                },
                {
                    "sent": "Let's use the Frobenius norm instead of, which is very straightforward generalization of the original.",
                    "label": 0
                },
                {
                    "sent": "This happens in a lot of papers.",
                    "label": 0
                },
                {
                    "sent": "Koby Krammer and singer multiclass classification in your paper as well, there's a lot of cases use this step.",
                    "label": 0
                },
                {
                    "sent": "The difference is really on the constraint, the constraint they changed.",
                    "label": 0
                },
                {
                    "sent": "The original data boards.",
                    "label": 0
                },
                {
                    "sent": "We have the normal vector we have the feature vector and some who bias and multiply by the label 'cause it's changed to general vector that may be embedded into our.",
                    "label": 0
                },
                {
                    "sent": "Output vector space.",
                    "label": 0
                },
                {
                    "sent": "We have a linear operator and we have the input vector as similarity that the original and we have the inner product defined in the space of the outputs.",
                    "label": 0
                },
                {
                    "sent": "This is only which is more defined in the system.",
                    "label": 0
                },
                {
                    "sent": "All of their things, the flex are the same and the corresponding penalty term in the objective is the same.",
                    "label": 0
                },
                {
                    "sent": "That is, changes really.",
                    "label": 0
                },
                {
                    "sent": "Somehow the interpretation of the W. If we have a very generous space, we can have a very general space.",
                    "label": 0
                },
                {
                    "sent": "Maybe this dimension can be infinite in both sides we have.",
                    "label": 0
                },
                {
                    "sent": "We can consider functions is not really matters.",
                    "label": 0
                },
                {
                    "sent": "Yes, if we have infinite dimensional space, we need some regularity on this space to compute this.",
                    "label": 0
                },
                {
                    "sent": "Which is matters.",
                    "label": 0
                },
                {
                    "sent": "The dwarf solution of this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is practically the same that the original binary asphere.",
                    "label": 0
                },
                {
                    "sent": "We have a kernel computed component by Component Vice product between the input and output.",
                    "label": 1
                },
                {
                    "sent": "It was in the grammar and single paper very similarly, but they assume a very large kernel scaling with the size of the number of classes.",
                    "label": 0
                },
                {
                    "sent": "Here we have the same size.",
                    "label": 1
                },
                {
                    "sent": "And we have a constraint which contains explicita representation of the embedded on the outputs.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go to the details, but it's a very trivial it can be transformed into another constraint with the same feasibility domain with implicit representation.",
                    "label": 0
                },
                {
                    "sent": "This constraints.",
                    "label": 0
                },
                {
                    "sent": "That it happens if.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Which one?",
                    "label": 0
                },
                {
                    "sent": "Then you just press it OK?",
                    "label": 0
                },
                {
                    "sent": "These constraints can happen if you have a bias in the original, the original.",
                    "label": 0
                },
                {
                    "sent": "Prime this bias.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a translation we have somewhere linear transformation.",
                    "label": 0
                },
                {
                    "sent": "With this bias we have affine transformation in the.",
                    "label": 0
                },
                {
                    "sent": "Define the input space is transformed the input into the output space.",
                    "label": 0
                },
                {
                    "sent": "That we really need here is only a linear function on the W. It maybe not the inner product.",
                    "label": 0
                },
                {
                    "sent": "In general we don't need to assume it is a Hilbert space because to derive the dual we need the courage contactor condition which.",
                    "label": 0
                },
                {
                    "sent": "Doesn't require the.",
                    "label": 0
                },
                {
                    "sent": "The Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So this is the base configuration that we can solve especially efficiently if we have node.",
                    "label": 0
                },
                {
                    "sent": "Spa",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should biased case.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we have this objective function bunker malfunction defining this way and we have a box constraint.",
                    "label": 0
                },
                {
                    "sent": "This is maybe the most trivial optimization problem that any whole somebody can solve.",
                    "label": 0
                },
                {
                    "sent": "A very simple.",
                    "label": 0
                },
                {
                    "sent": "Coordinate descent method can solve very efficiently this in very large scale.",
                    "label": 0
                },
                {
                    "sent": "The corresponding Matlab code which solves the full problem with the bias and with the box constraint you can find on the Internet.",
                    "label": 0
                },
                {
                    "sent": "It is modular.",
                    "label": 0
                },
                {
                    "sent": "Put some people downloaded and used.",
                    "label": 0
                },
                {
                    "sent": "And the question, how can we use this?",
                    "label": 0
                },
                {
                    "sent": "One is the problem.",
                    "label": 0
                },
                {
                    "sent": "How can we get the prediction?",
                    "label": 0
                },
                {
                    "sent": "The prediction looks like that.",
                    "label": 0
                },
                {
                    "sent": "If the output is somehow embedded into a feature space we have made.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's trouble because we have the.",
                    "label": 0
                },
                {
                    "sent": "Implicit representation of the output.",
                    "label": 0
                },
                {
                    "sent": "And the solution generally that the people present.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compute the argmax relative to the other possible puts, it isn't happens in very lot of papers.",
                    "label": 0
                },
                {
                    "sent": "Covering similar problems.",
                    "label": 0
                },
                {
                    "sent": "But in some cases relatively a lot of cases we can simplify this problem.",
                    "label": 0
                },
                {
                    "sent": "One is when, let's say we have the multiclass classification problem.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "In the multiclass classification prob.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This vectors can be explicitly computed by indicator vectors, or some who using a hyper tetrahedron vectors.",
                    "label": 0
                },
                {
                    "sent": "This performs the better.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the computation of the vectors.",
                    "label": 0
                },
                {
                    "sent": "It seems probably the best solution for representing multi classes in the Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I should be very fast because I have I don't know maybe 10 minutes or five.",
                    "label": 0
                },
                {
                    "sent": "It is somewhere very.",
                    "label": 0
                },
                {
                    "sent": "Something to give impression about the solution and to show the efficiency.",
                    "label": 0
                },
                {
                    "sent": "Let's go to the.",
                    "label": 0
                },
                {
                    "sent": "Hi rocky, learning.",
                    "label": 0
                },
                {
                    "sent": "The hierarchy learning means we have a multiclass problem several.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classes and somehow that is hierarchy defined on this classes.",
                    "label": 0
                },
                {
                    "sent": "And the output karnadi ride in a very simple way somehow, considering the shortest paths in this hierarchy.",
                    "label": 0
                },
                {
                    "sent": "If the classes are closed in the hierarchy, the corresponding kernel value is large and the colors more or less covered.",
                    "label": 0
                },
                {
                    "sent": "This thread are the highest value and dark blue the lowest.",
                    "label": 0
                },
                {
                    "sent": "This is our output Carnell and in the Viper data set the number of classes in the high rack is 187.",
                    "label": 0
                },
                {
                    "sent": "I don't 386 or 87, it means really very large.",
                    "label": 0
                },
                {
                    "sent": "The data set size is 1500 something 500 around.",
                    "label": 0
                },
                {
                    "sent": "And only a result.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compare some plot SVM, no advisors VM which is completed which is a Bianchi and others has a regressive solution.",
                    "label": 0
                },
                {
                    "sent": "You are also present depressive tomorrow.",
                    "label": 0
                },
                {
                    "sent": "The approach behind the HM 3 and we have this MMR solution.",
                    "label": 0
                },
                {
                    "sent": "Benomar stands for the maximum margin robot.",
                    "label": 0
                },
                {
                    "sent": "Give something.",
                    "label": 0
                },
                {
                    "sent": "Stupid enough name?",
                    "label": 0
                },
                {
                    "sent": "And the result they are the.",
                    "label": 0
                },
                {
                    "sent": "The test errors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see the testers are relatively in general very large.",
                    "label": 0
                },
                {
                    "sent": "In 2006 it was I found later than this table was created chokachi this and Hoffman and you are in paper.",
                    "label": 0
                },
                {
                    "sent": "In that paper the error was 72.",
                    "label": 0
                },
                {
                    "sent": "72 something.",
                    "label": 0
                },
                {
                    "sent": "Here they are in our MMR gives 47.",
                    "label": 0
                },
                {
                    "sent": "You can see surprisingly works very well the other which is very important to compute.",
                    "label": 0
                },
                {
                    "sent": "This takes hours generally the simple SVM takes 2 hours.",
                    "label": 0
                },
                {
                    "sent": "In our case.",
                    "label": 0
                },
                {
                    "sent": "Device.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple said that in our corner is 1.9 second in the polynomial 1.2 and the Gaussian less than one second.",
                    "label": 0
                },
                {
                    "sent": "It is computed by pure Matlab code.",
                    "label": 1
                },
                {
                    "sent": "To be adaptive card now takes 5 or 6 seconds.",
                    "label": 0
                },
                {
                    "sent": "It is clearly the solution of the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "In our case, is somehow not exactly explicit the output vectors, But the computation of data structure is relatively trivialities.",
                    "label": 0
                },
                {
                    "sent": "Seconds for all of the test set.",
                    "label": 0
                },
                {
                    "sent": "It means we have completely a solution within 10 seconds, and the optimization takes only one.",
                    "label": 0
                },
                {
                    "sent": "Point, it has a huge advantage if you have a lot of parameters and structural learning too.",
                    "label": 0
                },
                {
                    "sent": "In the validation to find the optimal value in the very large Lane range of the.",
                    "label": 0
                },
                {
                    "sent": "Possible parameters in this case is.",
                    "label": 0
                },
                {
                    "sent": "Is a reliable and feasible approach and in multi task learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can use.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These two approaches money is somehow we can say additive approach.",
                    "label": 0
                },
                {
                    "sent": "We can put together.",
                    "label": 0
                },
                {
                    "sent": "Let's see if we have a binary multi task learning and with the assume we assume the domain are the same.",
                    "label": 0
                },
                {
                    "sent": "And in this case we have several views and we have several labels and a very simple case to present in this way the problem.",
                    "label": 0
                },
                {
                    "sent": "The concept vans in the dual space.",
                    "label": 0
                },
                {
                    "sent": "The kernel is a pointwise product of the input kernel and some of the output kernels.",
                    "label": 0
                },
                {
                    "sent": "It means the base to our problem.",
                    "label": 0
                },
                {
                    "sent": "Under that are some of the input panels.",
                    "label": 0
                },
                {
                    "sent": "Some of the input gardeners and output canals is point was product here so.",
                    "label": 0
                },
                {
                    "sent": "The other view when we would like to get something more complex, I used here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it can be extended for several others.",
                    "label": 0
                },
                {
                    "sent": "If you compute here the tensor product between the two views, the corresponding do.",
                    "label": 0
                },
                {
                    "sent": "A kernel is nothing else but the pointwise product of the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Feels.",
                    "label": 0
                },
                {
                    "sent": "And we have the same dual.",
                    "label": 0
                },
                {
                    "sent": "We can compute this prediction.",
                    "label": 0
                },
                {
                    "sent": "And solve this problem is really reasonable very fast.",
                    "label": 0
                },
                {
                    "sent": "And so the question in the experiments, did you have to be included as a very?",
                    "label": 0
                },
                {
                    "sent": "This is that I how much time that I have.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "I don't like him.",
                    "label": 0
                },
                {
                    "sent": "So very sad.",
                    "label": 0
                },
                {
                    "sent": "Because what is the problem here immediately in the applications?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is the problem if the norm of device for this are very different, because this kind of constraints cannot be really satisfied in this way.",
                    "label": 0
                },
                {
                    "sent": "So one is we need to somehow normalize and why otherwise it doesn't work correctly.",
                    "label": 0
                },
                {
                    "sent": "And if, let's see if you have a really wonderful both Debian the Axis 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "I hope it helps.",
                    "label": 0
                },
                {
                    "sent": "Very simple case X&Y.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "That I missed here before how it looks like in the practice.",
                    "label": 0
                },
                {
                    "sent": "This constraint can be rewritten.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The steps are not so important.",
                    "label": 0
                },
                {
                    "sent": "At the end we arrived to a bond class SVM defined in the tensor product space of the input and output.",
                    "label": 1
                },
                {
                    "sent": "This is by the complexity independent on the output dimension.",
                    "label": 0
                },
                {
                    "sent": "So if we have this one class SVM that we solve really for any kind of input output, and if he had one dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Items we have the points.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have a regression problem to predict the by using text.",
                    "label": 0
                },
                {
                    "sent": "If you have one class problem, this is the origin.",
                    "label": 0
                },
                {
                    "sent": "This is the hyperplane, something completely useless.",
                    "label": 0
                },
                {
                    "sent": "And how can we make a regression?",
                    "label": 0
                },
                {
                    "sent": "In general, we need to exploit the geometric structure of this that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "We have somewhere in our operator there the solution.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry very fastly.",
                    "label": 0
                },
                {
                    "sent": "Who's the stereographic projection?",
                    "label": 0
                },
                {
                    "sent": "Here?",
                    "label": 0
                },
                {
                    "sent": "We have the X.",
                    "label": 0
                },
                {
                    "sent": "We have a bowl.",
                    "label": 0
                },
                {
                    "sent": "We have a North Pole and the image of the X is on the intersection of the line and the circle.",
                    "label": 0
                },
                {
                    "sent": "Here every images has known one.",
                    "label": 0
                },
                {
                    "sent": "And after when we computed the regression problem, we can project back because it is explicit and invertible representation and we receive something like this.",
                    "label": 0
                },
                {
                    "sent": "So it because here we have no the same problem.",
                    "label": 0
                },
                {
                    "sent": "It is not a distance minimisation but inner product maximization not is really Max images maximization.",
                    "label": 0
                },
                {
                    "sent": "We would like to get sufficiently large in that product between the output and input.",
                    "label": 0
                },
                {
                    "sent": "And here in the bias.",
                    "label": 0
                },
                {
                    "sent": "If he had this kind of representation, but the norms are.",
                    "label": 0
                },
                {
                    "sent": "One, we don't need a bias.",
                    "label": 0
                },
                {
                    "sent": "And this kind of what is?",
                    "label": 0
                },
                {
                    "sent": "It is almost the same that the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Only the difference, the Gaussian kernel not invertible.",
                    "label": 0
                },
                {
                    "sent": "This is invertable, the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Every items is normalized.",
                    "label": 0
                },
                {
                    "sent": "It means if, let's say if you use Gaussian kernel, we have a normalized input or a normalized output.",
                    "label": 0
                },
                {
                    "sent": "But because of the explicit representation is missing, we need the special operator find.",
                    "label": 0
                },
                {
                    "sent": "Here we can do much complex embedded into the ball.",
                    "label": 0
                },
                {
                    "sent": "Pit is the simplest and it can.",
                    "label": 0
                },
                {
                    "sent": "It can happen in the kernel space.",
                    "label": 0
                },
                {
                    "sent": "Independent under Canada presentation.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm.",
                    "label": 0
                },
                {
                    "sent": "I may be able to fast, but I wanted to finish because what is the time?",
                    "label": 0
                },
                {
                    "sent": "I think I time.",
                    "label": 0
                },
                {
                    "sent": "Questions yes.",
                    "label": 0
                },
                {
                    "sent": "So just make sure I understand.",
                    "label": 0
                },
                {
                    "sent": "Sorry I couldn't hear you follow Jesus.",
                    "label": 0
                },
                {
                    "sent": "I assume you are in the multiclass study, yes, so you have this kernel.",
                    "label": 0
                },
                {
                    "sent": "At some point you projected it, slide about yes.",
                    "label": 0
                },
                {
                    "sent": "One moment I'm going there.",
                    "label": 0
                },
                {
                    "sent": "You can ask you up.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "The hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Sure, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, how do you feel that Colonel on the right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Colonel was in the very concrete case, you know.",
                    "label": 0
                },
                {
                    "sent": "I mean, how can you set the value of the function on the?",
                    "label": 0
                },
                {
                    "sent": "But this is I would like to say, let's say if you have these two points.",
                    "label": 0
                },
                {
                    "sent": "The similarity somehow that we can use the in the distance is here to in the graph here for, let's compute the inverse.",
                    "label": 0
                },
                {
                    "sent": "It is a very simple case.",
                    "label": 0
                },
                {
                    "sent": "And in their practices.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really value is 1 parameter which somehow saying nothing else but edge.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Far from the root is more important than the closer, but it was a parameter.",
                    "label": 0
                },
                {
                    "sent": "It is wait.",
                    "label": 0
                },
                {
                    "sent": "Maybe you carry scale and the edge close to the root is more important.",
                    "label": 0
                },
                {
                    "sent": "It was the only parameter that was used in the Bible data set.",
                    "label": 0
                },
                {
                    "sent": "This parameter was zero point 5.",
                    "label": 0
                },
                {
                    "sent": "It means bait is 1, it is 0.5 and square of 0.5.",
                    "label": 0
                },
                {
                    "sent": "Voice in the Bible.",
                    "label": 0
                },
                {
                    "sent": "The choice of this these values actually affect the results.",
                    "label": 0
                },
                {
                    "sent": "The accuracy results that you are.",
                    "label": 0
                },
                {
                    "sent": "If I'm changing this weight.",
                    "label": 0
                },
                {
                    "sent": "Yes, it influences in the.",
                    "label": 0
                },
                {
                    "sent": "For example, if I use only the lower level, it means all others are.",
                    "label": 0
                },
                {
                    "sent": "It means here the weights are one, but maybe others are zero.",
                    "label": 0
                },
                {
                    "sent": "It means a multiclass problem is very in here it was 53.",
                    "label": 0
                },
                {
                    "sent": "I don't remember exactly.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Petite influences it was banned on this parameter.",
                    "label": 0
                },
                {
                    "sent": "There was an optimization to get the best weight which dominates the edges on the path to the root and it was 47.",
                    "label": 0
                },
                {
                    "sent": "Using the Gaussian kernels 46.",
                    "label": 0
                },
                {
                    "sent": "The corresponding Matlab code is on the Internet, or I can send.",
                    "label": 0
                },
                {
                    "sent": "Choose this convenience, not just cause some similarities is supported because in the optimized opprobrious known that you can trivially derivate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can use something else but the question in the computational sense, the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "If you compute the derivative of the probing, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Because the derivative of this function is now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think about W. If you have a complex, more complex function.",
                    "label": 0
                },
                {
                    "sent": "The question the dual problem, the complexity of the dual problem.",
                    "label": 0
                },
                {
                    "sent": "Here it is most trivial choice for W. This function is purposeful for cleaning.",
                    "label": 0
                },
                {
                    "sent": "Doing this kind of stuff, sorry.",
                    "label": 0
                },
                {
                    "sent": "One yesterday.",
                    "label": 0
                },
                {
                    "sent": "Choosing more contracts.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For the purpose of doing that.",
                    "label": 0
                },
                {
                    "sent": "Is really the purpose?",
                    "label": 0
                },
                {
                    "sent": "Yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "In a special cases, maybe it's important to you.",
                    "label": 0
                },
                {
                    "sent": "Is not this known, but there let's see the maximum eigenvalues of the maximum.",
                    "label": 0
                },
                {
                    "sent": "Angular value because it's not square.",
                    "label": 0
                },
                {
                    "sent": "And it gives some who here the margin is nothing else but the inverse of the singular value.",
                    "label": 0
                },
                {
                    "sent": "We have one here, some who the margins because we have several margins, are the inverses of the singular values and we would like to minimize of the sum of the square of the singular values.",
                    "label": 0
                },
                {
                    "sent": "Typically.",
                    "label": 0
                },
                {
                    "sent": "There were four other other things funny kind of classification, because out of these constraints, for example, this is the sum of probabilities.",
                    "label": 0
                },
                {
                    "sent": "What those methods?",
                    "label": 0
                },
                {
                    "sent": "They hopefully the outcome would be in the similar range, like more like the progression type of problem.",
                    "label": 0
                },
                {
                    "sent": "Instead of this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "I am using at this moment for regression.",
                    "label": 0
                },
                {
                    "sent": "Very pure regression is weather prediction for several input on several output variables.",
                    "label": 0
                },
                {
                    "sent": "I need the normalization that I mentioned is very important without a special kind of normalization doesn't work but after.",
                    "label": 0
                },
                {
                    "sent": "You have really the regression problem.",
                    "label": 0
                },
                {
                    "sent": "Apps.",
                    "label": 0
                },
                {
                    "sent": "The classification he.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Multiclass?",
                    "label": 0
                },
                {
                    "sent": "You explicitly have the features to compute the not, not, not really.",
                    "label": 0
                },
                {
                    "sent": "If you use this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This formula you don't need because here you have the kernels in.",
                    "label": 0
                },
                {
                    "sent": "If the number of classes is relatively small in the bipod is 186.",
                    "label": 0
                },
                {
                    "sent": "The computation is very trivial in this case.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's like.",
                    "label": 0
                }
            ]
        }
    }
}