{
    "id": "h5isiqbvhlwo4wparag7urkx4xuaibfl",
    "title": "Deep Learning 2",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Department of Statistical Sciences, University of Toronto"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_salakhutdinov_deep_learning_2/",
    "segmentation": [
        [
            "Here's what I'm going to talk about, so I'm going to talk about several different things.",
            "So the first part of the talk is going to be for digital is going to be a little bit more and empirical side.",
            "I'm going to show you some interesting things we can do.",
            "The second part is going to be a little bit more technical, so I'll try to look at the two so I'd like to talk about zero shot learning, sort of trying to combine images in text.",
            "I'm going to talk about caption generation work, something that I've shown you before.",
            "Then a little bit more on the technical side, I'd like to just give you some intuitions about learning recurrent attention models.",
            "And then I'll finish off by.",
            "The recent work on learning skipped vectors.",
            "OK, so."
        ],
        [
            "Let me first give you a little bit of a zero shot learning type of setup, which is I think it's an interesting setup.",
            "It's a very simple idea, but I just wanted to share it with you and this is joint work with Jimmy Barkan is Fairsky and Sonia Fiddler, but you know credit goes to."
        ],
        [
            "These two guys is the students who did the work.",
            "OK. Can you solve 0 short learning for me?",
            "So that's a test for you.",
            "Let's say I give you this Wikipedia article right, and I ask you, can you classify?",
            "So it talks about Canada, Canada warbler.",
            "The question for you is that can you actually find the right class?",
            "Can you solve this problem?",
            "How many of you think that that's the right answer?",
            "What about this one?",
            "How about this one?",
            "One good pretty much most of you probably guess that that's the right answer, right?",
            "How come, well, they sent something about have black forehead and black necklace, and I think that's that's the bird that has it.",
            "Right, so you able to solve this problem even though yeah.",
            "It's working with.",
            "Maybe?",
            "Turns out that that's Canada."
        ],
        [
            "This one is a yellow world which is a different one.",
            "No, but OK.",
            "So so to be fair, what I should have shown you is I should.",
            "I should have shown you maybe 20 Wikipedia articles about 20 different classes and then you could make your mind figuring out which one is which.",
            "Um?",
            "Could be could be, but I can tell you that people are very good at solving this problem.",
            "Like on the kinds of images that you see, you pretty good.",
            "What about this one?"
        ],
        [
            "It's a bit tougher one, but still.",
            "The answer is this guy here.",
            "Right, so one question, one question is that can you actually do these kinds of things with machines?"
        ],
        [
            "So let me give you."
        ],
        [
            "Very very simple model.",
            "Very simple idea of how we can do that.",
            "Suppose we have a simple set up.",
            "We're trying to do solve one versus all classifier.",
            "Right, so we building you can think of these weights as a vector weights for a particular class.",
            "Alright, and this is sort of a standard problem that let's say we're trying to solve doesn't necessarily have to be linear model, could be convolutional neural network or whatever your favorite model is.",
            "So how can you deal with previously unseen classes?",
            "OK, well, we can't really do it in that particular formulation.",
            "So what can we do?",
            "Suppose we have an additional text features T OK. And suppose they line some space RP.",
            "And the very simple idea is to do the following week instead of using a static weight vector W, we can actually use the text features to predict the weights of the classifier.",
            "Right, so we're going to be parameterising our textual features to say, well, can you actually predict the weight of the classifier?",
            "And we can use it to predict the output weights of a client of a classifier for convolutional networks and such.",
            "It doesn't really matter.",
            "So that's that's the idea.",
            "Very simple idea.",
            "So what the model looks like is the following."
        ],
        [
            "You have.",
            "It's a Wikipedia article, right?",
            "And we're doing very primitive way of treating.",
            "The Wikipedia articles at this stage it's very early work, obviously can do something much more intelligent like using recurrent neural Nets.",
            "Something that I'm going to talk about.",
            "At this tutorial and then you pushing it through, let's say, some kind of nonlinear multilayer perception."
        ],
        [
            "In getting these features so we have C classes and these are can dimensional features right?",
            "You take an image, you get some feature representation of the image and then you basically taking the dot product of the two.",
            "Right, and then you're trying to predict which class that is.",
            "So that way you can think about this the following and that's the idea that's been explored a lot in the machine learning community, and you'll see when people are trying to work with multimodal data.",
            "This is basically what happens is that you take the Wikipedia article you embedded in some latent space, you take images and embed in the same space.",
            "Right, well, you take the same story goes on, so you're trying to basically find this joint space right?",
            "And essentially this model does something something very similar, and then you can minimize cross entropy or hinge loss, whatever you object if you want to do you trying to solve classification problem?",
            "Right, so it's a very, very simple setup.",
            "An initially when we started looking at the setup, I was basically saying no way we can solve this particular problem.",
            "It seems like a very hard problem, and in fact we can't.",
            "But we can make fairly good improvements and given the fact that we're just using simple like TF IDF Deco type of features so we can improve upon that.",
            "But let's let me look at the set up."
        ],
        [
            "The setup is quite interesting.",
            "Let's say you have an images as well as the associated class labels, right?",
            "So you have see distinct classes.",
            "Think about, you know, maybe having 200 different classes and then during the test time.",
            "You get.",
            "An N sub zero number of previously unseen classes so you've never seen these classes before.",
            "Right, so think of it that you know at the training time.",
            "I give you Wikipedia article about about Tigers and you know everything about Tigers and I give you multiple images of Tigers so you see how images correlate with text and at the test time I give you Wikipedia article about Siberian Tigers right and my hope is that maybe you figure out Siberia has to be snow.",
            "So maybe it's like a Tiger in the snow, right?",
            "And that would be Siberian Tiger, for example.",
            "The goal is to do well on previously unseen classes as well as previously seen classes.",
            "So basically trying to learn a very good similarity.",
            "Kernel between images and.",
            "Articles Wikipedia articles.",
            "Right so with."
        ],
        [
            "That just with this very simple idea, we've looked at a couple of datasets.",
            "The interesting set up here is that it is only single Wikipedia article for each of the one of these bird classes you have about 200 classes.",
            "Um?",
            "And this is coming from Caltech Bird data set.",
            "It's not a very big data set, sort of reasonable out of 200 classes you have about 40 classes, defined as unseen and 160 classes defined as seen and the same setup happens for the Oxford Flowers data set, right?",
            "You have about 102 classes.",
            "So in this case 20 classes are used for being as unseen, 82 classes being is treated as seen.",
            "So what would you expect from this?",
            "How many of you think you can do much better than random?",
            "Good, that's it's very inspiring, because when I looked at this problem, I thought you barely can do better than random.",
            "But here is some reason."
        ],
        [
            "Also, this is our OC's curves.",
            "This is the previously published work.",
            "This is what inspired us to basically look at the same problem.",
            "D he stands for domain adaptation.",
            "And the interesting thing here is that if you look at what you can do, you know .8 two point 8 zero D0C.",
            "These are not perfect, but they actually pretty good.",
            "The interesting thing is that if you look at the scene classes like things that you have data for, you pretty much nailing it.",
            "Right, so the surprising thing is that, and that's an interesting kind of setup, because you never see images, you just everything is based on just the articles themselves.",
            "So I think it's an interesting problem.",
            "Now if we go to the flower data."
        ],
        [
            "But things become a little bit more worse, like the numbers go down, like .7 is not is not that good, so there's still a lot of room for improvement.",
            "Somehow Flowers is much more difficult to differentiate and to be honest, I looked at this data set myself and I tried to do the classification.",
            "It's very hard, specially when it comes down to Flowers.",
            "You look at specific descriptions is very hard, so humans will probably do better than this, but probably not.",
            "Too much better, yeah?",
            "No, so for these datasets.",
            "For these datasets, for each class you have a Wikipedia article.",
            "Right?",
            "So if the test time I only give you Wikipedia article, So what you're doing is you're taking Wikipedia article projecting into the semantic space, you're predicting what the classification weights should be, and then using those weights you classifying.",
            "Right, or you can think of it the other way.",
            "You can think about it is you projecting it down to the semantic space and you're looking for images.",
            "That are closest to it and the ones that are closest to you classify as that particular class.",
            "So here is."
        ],
        [
            "You can also do interesting things like attribute discovery, so if I give you this Wikipedia article, these are the images that the system retrieves, so it confuses Scarlet tanager with summer tennager and these other birds, and it's pretty hard right?",
            "I mean you have to solve the visual problem as well.",
            "But everything that you can do.",
            "Yes, so in this case Wikipedia articles of which class is projecting into the future space, and you're looking for nearest neighbors.",
            "But the other thing you can do is you can look at the word sensitivities, you can say.",
            "If you wiggle some of the words which words matter the most for predicting that particular class?",
            "And it picks up these ones, so these are the most important words.",
            "When the system believes that it should be bought Scarlet Tanger so it picks up Tennager picks up Scarlett.",
            "Obviously Wikipedia article talks about Scala tennager.",
            "Then these words become very important.",
            "It also picks up these ones and I have no idea what these guys are.",
            "So I looked on the Wikipedia, apparently this thing.",
            "Identifies groups of birds that has to do with teenagers.",
            "So if it takes these interesting kinds of features, features from words and this, this whole thing is kind of done in.",
            "This is for the unseen class, so this is a class that we've never seen before.",
            "Same thing happens for."
        ],
        [
            "Or you know images of Flowers and the word sensitivities for the.",
            "For example, for bearded irises, I don't know what that is.",
            "Freezing iris, apparently.",
            "You know these these plants can freeze that compost and such, so it's a reason why I'm bringing this up is.",
            "I think that five years ago in the vision community, there's a hole.",
            "There's a whole movement of basically trying to find attributes right?",
            "And people were constructing these attributes for different classes like.",
            "If this class is about cats and I can predict that it's fluffy and it's you know it has certain attributes, then I can do better when dealing with new class because foreign you classify predict those attributes.",
            "I can do well so we were hoping that we can actually try to find whether the relevant attributes are for the unseen classes.",
            "So this is just just the intro so.",
            "But let me move on to the caption generation."
        ],
        [
            "Which sort of has a similar type of flavor, but it's trying to solve a much more complex problem, and this is work by Ryan.",
            "Curious who's sitting over there.",
            "I can't find a picture of Ryan on Google like you, just not on the web.",
            "Man find the picture, but this is the work done by Ryan Curious."
        ],
        [
            "So let me think about the following challenging problem, OK?",
            "We've seen that we can generate tags.",
            "Maybe we can embed articles and images into some space and try to reason about that, but how about a much more complex problem where I show you this input and I want to generate the sentence?",
            "OK, how can I do that?",
            "And it's interesting because in the last year there's been a lot of progress in that space, and some of it is actually very surprising.",
            "The things that we can do, nobody expected that these models could work so well.",
            "At least I didn't expect them to work so well.",
            "So it's it's.",
            "It's great, so."
        ],
        [
            "One of the things that we the way we can think about these kinds of models is we can think about them in terms of encoder decoder framework.",
            "Right, so think of it as you know, the way you're trying to translate from English to French, you can think of it as trying to translate from images to English.",
            "Right, you have some image.",
            "You have some form of encoder and then given some representation you decoding it back.",
            "OK, so I'm going to structure this talking in two steps.",
            "I'm going to talk about the encoder first where using convolutional neural Nets as well as their current type of models to get the representation of sentences and images.",
            "And then there is going to be decoded part and the decoder part in for this particular talk I'm going to be focusing on probably signal language models.",
            "Even though you know lately people are also using LSD M. So recurrent neural Nets for to do the same pretty much the same thing.",
            "So let me just first focus on the encoder.",
            "Let's ask the question how can we embed images and sentences into the latent space into the same space?",
            "Suppose that I'm trying to solve retrieval problem.",
            "Given an image, can you find me a sentence that goes well with that particular image so we can formulate it as a treatment as a retrieval problem?"
        ],
        [
            "OK, and the key idea and you've seen this, you probably seen this from Chris Stock is the key idea that underlines a lot of these models is the idea that you want to be representing words as some dimensional real valued vectors right?",
            "Instead of trying to find the semantic space?",
            "So these are word embeddings and there's been tons of work here on doing that, and the idea is that maybe you can sort of figure out the tables and chair semantically mean the same thing so we can find sort of them to be closer together in dolphin and whales and such.",
            "Right so."
        ],
        [
            "So what are we going to do here well?",
            "Let's say that we're going to be using some form of a convolutional next terministic system to embed images into some space, and then we're going to try to learn adjoining bedding of images in text, right?",
            "So, in principle, you know here you can basically embed anything, right?",
            "You can embed images, words, phrases, Wikipedia articles.",
            "And there's a very natural definition of a scoring function.",
            "You're just looking at the inner product in that space.",
            "Right, so how we?"
        ],
        [
            "Embedding sentences just to highlight here if you have a sequence of three words, you're getting some representation and you're building a recurrent neural network or LS TM type of model.",
            "Adam was giving you gave you a little bit of preview about recurrent Nets, so it's pretty much the same model, and I'm going to talk about this in the last part of this tutorial, something that Ryan's been working on skipped vectors, so we have a way of trying to basically get.",
            "Interesting representation from sentences.",
            "I'm going to try to convince you that they're actually pretty good representation of sentences for the images we can just use a convolutional neural network as these kinds of models.",
            "They work the best for images and these are the both representations that we're getting right, so we're using LSD estimates to embed sentences, and we're using convolutional neural Nets to embed images.",
            "Right, so this becomes your image representation."
        ],
        [
            "So what's the idea?",
            "The idea is extremely simple.",
            "What we'd like to do is we'd like to say that this image and this sentence get mapped to the same point.",
            "In the latent space, or in the semantic space, we also want to say that this image and that sentence get embed.",
            "They get embedded into the points that are close to each other.",
            "Right and so what's the objective?",
            "Well, you can use a ranking like objective people have done.",
            "I've used these kinds of objective functions for a very long time.",
            "It's a very simple objective.",
            "It's very easy to optimize and has the following structure.",
            "So here S you can think of it as just looking at the product or the cosine between.",
            "Images and text right?",
            "So you basically saying I want the cosine between these two vectors to be high.",
            "And I want the cosine between this image and this random sentence.",
            "To be small.",
            "I want to push them apart, so I want to make sure that these two points are close to each other and I want to make sure that this point in this point further away from each other by some margin Alpha and Alpha can be set to one.",
            "For example, it doesn't really matter and you can do the same thing for text, right?",
            "So you can obviously train this system.",
            "It's a deterministic system.",
            "You can back propagate through the whole thing.",
            "You can back propagate through the convolutional model, can back propagate through the current network model so the entire system becomes basically deterministic system.",
            "So how?"
        ],
        [
            "How well does it do?",
            "Here's what it does.",
            "These are test images.",
            "So I show you this image and I basically embed this image into the latent space and I just look for nearest neighbors in that space.",
            "Anne, these are the nearest neighbors.",
            "It's pretty interesting what it does right?",
            "I mean, in a lot of times it actually works pretty well.",
            "Now obviously this system you know I'm not telling you how we are generating sentences, which is a much more challenging task.",
            "This is just the first part to tell you how we can retrieve sentences given the images.",
            "Yeah, there's a question.",
            "So for this case, I think we were using Flickr data set and there was one caption for proper image in the Coco data set.",
            "I think there are five captions per image, so we can use that as well.",
            "Flickr also has five, so yes, you can use 5.",
            "Retrieve an image.",
            "It's it's a test, so this is it."
        ],
        [
            "Yes, this is a test image.",
            "And you retrieving training sentences.",
            "I presume.",
            "I mean, there's multiple combinations you can do right?",
            "Or given a test sentence, you can retrieve from a database the images.",
            "I think Ryan for these things, user test images in an sentence is.",
            "That's right.",
            "But again, this task is the question.",
            "Sorry what.",
            "Yes yes.",
            "So in this case again I want to emphasize this is not a generation thing.",
            "This is just the retrieval thing.",
            "Think of it as the following.",
            "Give you an image and you based on some data set of sentences you're trying to retrieve.",
            "The most relevant one, and this is what is being shown here.",
            "Yeah, yeah, sure.",
            "Yes, so the way the way to test this instance quantitatively like, if you want the answer, then for every single test image you have the correct caption, right?",
            "So what you can do is you can rank all the captions and then see where this correct cache caption lens and that will give you some notion of precision.",
            "Yes, yes that's true.",
            "I mean it will.",
            "It will depend on that, yes, but there is sort of standard datasets, right?",
            "I mean there is standard test set so everybody benchmarks off the same thing.",
            "Yeah, because I'm unfortunately, yeah, but let me write.",
            "Let me get to the Jenner."
        ],
        [
            "Asian part, so the other thing that you can do here is you can also retrieve words, not necessarily sentences, right?",
            "So given those images, these are words that the system retrieves or the other way around.",
            "I've shown you this before.",
            "Given the word you just look for nearby sentences and actually works pretty well.",
            "You can also do these funny things like.",
            "Travel with adjectives, right?",
            "If I give you the embedding of the word fluffy, you can look for similar images.",
            "Sort of captures these things even though you're not specifying you're not training the system, you don't.",
            "You don't tell the system that this is fluffy or this is delicious.",
            "It's just that whenever you look at the sentences most of the time, you know for these images, fluffy appears and delicious appears right.",
            "You can also do fun things, so this is."
        ],
        [
            "These kinds of embeddings.",
            "There's a little bit of caveat how we've done it, but maybe I can just take it offline, but one of the things that you can do is you can embed this image.",
            "You can embed the get representation of the word blue and red, and you can do vector multiplications right?",
            "So you can say in image minus blue plus red and then you look for nearby images and get these things.",
            "Right now mine is blue plus yellow, get yellow cars.",
            "Right?"
        ],
        [
            "Or you can say airplane minus flying, fly, sail and get these sailboats.",
            "I like this one is like kittens minus bowl plus box to get kids in the box or you get this kitten you say minus box plus bowl you get kittens in the bowl right?",
            "It's actually so this thing here is a failure right?",
            "I think that's a duck.",
            "Right, right now this is just.",
            "It has nothing to do with quantitative evaluation, is just kind of like fun things you can do so for this thing.",
            "Actually we were not.",
            "We were not using STM so we were using.",
            "Basically it's a way of just summing the representation of the words but but you can talk to Ryan about it, specifically how it was done, but.",
            "As background, so I think that one you were using Flickr database right?",
            "For this, yeah.",
            "Stony Brook University.",
            "Yeah."
        ],
        [
            "So, but how about a more challenging problem?",
            "How about the problem of generating sentences given the inputs right?",
            "And so one of the one of the things that I'm going to talk about is I'm going to talk about neural language model for doing that.",
            "And maybe I can give you a little bit of intuition how these models are working."
        ],
        [
            "So you can think of neural language models of feedforward neural networks with a single linear layer.",
            "So each work here W is represented as a K dimensional real valued vector.",
            "So you basically representing them as.",
            "Embedding essentially is the same as embedding words in the K dimensional space.",
            "So our here I'm going to be denoting R is a V by K matrix of words.",
            "These the vocabulary, size case, the representation, the latent representation of the word.",
            "Um?",
            "And then you can do the following.",
            "You can use this following very simple idea.",
            "You can say, well, let's look at the top of minus one words where N -- 1 is the context size right?",
            "And then the next word representation becomes just basically take a linear combination.",
            "So take these representations.",
            "The next representation for the next word you just looking at the linear combination of the previous representations of the words.",
            "So these matrices are K by K parameter matrices that you have to learn.",
            "And if you structure it becomes basically.",
            "A feedforward neural network with a linear layer.",
            "And in this case, the conditional probability of the next word is going to be given by this expression, which is a softmax.",
            "And I think that this particular model was made popular in the machine learning community back in 2003 by Yoshua.",
            "So there's a paper on probably single language models.",
            "Um?",
            "So this is sort of a standard model that's been around."
        ],
        [
            "Or for a long time now.",
            "Typically this is something that could be expensive to compute because V is can be high dimensional.",
            "Let's say if you're working with.",
            "You know 10,000 or 100,000 words.",
            "This is something that you have to you have to do, but there are ways around speeding this up.",
            "So this is a standard probably signal language model."
        ],
        [
            "OK, So what we're going to do is we're going to be following.",
            "We're going to be looking at multiplicative models, right?",
            "And we're going to be dealing with tensors, so let's say we're going to be representing words extensors, so we're going to say that this tensor is going to be a science V by K by G, where G is going to be 10 slices, and you can think of.",
            "Maybe you have some attribute vector U.",
            "For example, these could be representing image features, right?",
            "If you're trying to go from images to text, let's say you here is representing image features.",
            "Then you can compute these gate at word representations as follows.",
            "So you're getting these representations.",
            "By using image features.",
            "And one of the things when you're dealing with tensors, what makes it a little bit challenging is the size.",
            "Of this matrix of this tensor, right?",
            "Imagine that if these 100,000 K maybe 300 dimensional or 500 dimensional and geez, you know typically have 4000 features could be very very high dimensional object."
        ],
        [
            "And what you can do is you can basically do low rank decomposition.",
            "You can say well, instead of representing this tense, I'm going to be representing it in terms of these three low rank matrices.",
            "And if you write it up, it just has that particular presentation, right?",
            "So in the back of your mind you should be just thinking that instead of dealing with full tensors, you're going to be dealing with low rank approximation, lowering decomposition of those tensors.",
            "F. Here is the number of pre chosen factors you can think of them as maybe being like 20 dimensional factors of 50 dimensional factors and such, so you can reduce.",
            "The number of parameters that you need to fit.",
            "So what is this system does?",
            "It's basically very much the same as a neural language model, but we with these multiplicative connections, right?",
            "So let's say E. It's going to be K by the matrix the same as before.",
            "But you can see it's has a low rank approximation and then the predicted representation for the next word.",
            "You have the same story here.",
            "So far we haven't done much except for just had the low rank approximation of our matrix and then the next word representation given the representation of the next word.",
            "The factor outputs.",
            "This is where the gating happens, and this is componentwise product.",
            "So what happens here is that essentially image features can gate.",
            "The.",
            "The probabilities for predicting the next word.",
            "Um?"
        ],
        [
            "So in effect what happens is that the conditional probability of the next word has the same form, but essentially, but here you have these additional features that could be, for example, image features or any other kind of sign information that affects the prediction of the next word.",
            "And may."
        ],
        [
            "I can just give you intuitively what is happening here.",
            "Suppose I give you this image and I tell you, steamship in.",
            "And then there are a bunch of words that you can predict steamship in the waters deep ship in the factory steamship in my house.",
            "You know, if you just look at the language model, there's alot of things that you can predict.",
            "Right just from the language model, but because you're looking at this image, this image has certain features like.",
            "This is a steamship, I think it's in the water or at the dock.",
            "Right, So what this model is doing?",
            "This gating model is doing is essentially it will shut off irrelevant words like will push on the probabilities of the word like steamship in a factory for example, and will push up on the probability water so that you can generate the next step ship in the water.",
            "Right, yeah?",
            "So we've done, yes, so we've done it.",
            "So one way you can do it is you can use it as a bias into the model.",
            "You can basically say you know just have this additional thing that goes in and then predicts you can do that.",
            "And to be honest, initially when we've looked at these things when Ryan looked at these things, the bias model was doing about the same as a gating.",
            "But I think on more recent on bigger datasets we do see these gating things to actually give us better results.",
            "So empirically we do see that.",
            "The previous the context.",
            "Yes, yes exactly.",
            "You can put the previous words into the gating and this is something that we haven't done.",
            "I think that will work better.",
            "Because I think that the gating.",
            "So you can think about the following right?",
            "If I you basically just the way to think about this is that you just the probability of predicting the next word, and you can implicitly shut down certain things.",
            "So Chris, maybe I can show you a slightly different example where it might convince you why it's more more relevant.",
            "So let me get to that.",
            "Up yes.",
            "So.",
            "Right, but but let me let me show you one other example which might make it a little bit more convincing.",
            "So the other thing you can condition on its part of speech tags.",
            "Right, so whenever you're generating a sentence, you can say.",
            "What if I give you a template for part of speech tax in terms of the sentence that I want to generate?",
            "An that sort of gives a soft way of finding relevant words.",
            "So for example."
        ],
        [
            "Say I'm trying to predict the next word given the previous word, and given this image an given, you know I want to generate the sentence that has this particular structure.",
            "I don't want to impose hard structure.",
            "In other words, the model can still choose to do generate what it wants to generate, but I want it to be consistent with without representation.",
            "And again, these parts of speech tags I eighting is a gating because it essentially the way you can think about this is that in the language model, when you predict the next word, there's a whole distribution of possible words, right?",
            "But when you when you condition on a particular parts of speech, you essentially having a.",
            "It's a software you're shutting down certain words, so words that are verbs will go down.",
            "Words that are nouns will go up."
        ],
        [
            "So for example, whenever you are doing that, you can generate A and then you say, well, generating a noun."
        ],
        [
            "Icicle generate me."
        ],
        [
            "Verb."
        ],
        [
            "Generate me.",
            "On there and on there and so forth.",
            "So that gave us a little bit better way of trying to generate something that syntactically coherent.",
            "I guess that was done that was done with the multiplicative model, right, right?",
            "The bias model in this case, how well does it work if you do the bias?",
            "If you just use these parts of speech is advice."
        ],
        [
            "So these are the I've shown you this before.",
            "These are sentences you can generate.",
            "That is all good sentences.",
            "Everybody shows good sentences, but I am going to show you some failed exam."
        ],
        [
            "These are failed examples.",
            "So this is the Turbo that trying to be seen in the water.",
            "Or we can generate things like apart car while driving down the road.",
            "So one thing that I want to point out here is that because we are trying to at least forcing the model to generate different things.",
            "Obviously, you know something like this.",
            "A human would not generate, right?"
        ],
        [
            "Here is another one.",
            "The handlebars are trying to ride.",
            "Bike rack so you know.",
            "So there's there's been a lot of concern in these models.",
            "Is that maybe what they do?",
            "Is they just sort of learn to just copy the training captions?",
            "And that's true to some extent.",
            "Maybe just a matter of getting the caption from the training set and reducing it.",
            "But in this case, at least, we're able to generate something novel."
        ],
        [
            "Right, which is always great.",
            "So you can also do these things where you filling in the blanks.",
            "You can say you know.",
            "Generate me now generates a cat and so forth."
        ],
        [
            "The cute cat is in the box or there is a bus and then parked.",
            "There is a car behind the bus or there is 3 on the bus and such.",
            "So.",
            "So I guess it should be near the bus or something like that, but they choose chosen on the bus so you can do it."
        ],
        [
            "Can do these things.",
            "You can also do fun things like seeing how these systems work in the real data.",
            "So this is a year wise, and David fleet that the NIPS workshop and this is what it generates, clicks.",
            "Entrepreneurs somehow generates busboys and waiters.",
            "I'm not sure why.",
            "And these are samples from the Model 2 men in the room talking on the table to many sitting next to each other, and so forth, right?",
            "So you can see I mean, these are simplistic sentences, so I'm not trying to claim that we actually generating something.",
            "Amazing, it sort of does does a reason."
        ],
        [
            "Good job, here's another example.",
            "How many of you know who that is?",
            "Good good good speaker typist computer and this is what generates man working the computer man sitting down with the laptop trying typing on a computer and so forth.",
            "Unfortunately, you know these systems are not very good.",
            "I wish it could generate something like a Facebook director.",
            "Is writing a Facebook post right?"
        ],
        [
            "Here's another example.",
            "This is this is.",
            "This is theory.",
            "There is no ski.",
            "I think he actually.",
            "Set up nips, right.",
            "He was the founder of NIPS.",
            "Um?",
            "This picture of NIPS.",
            "So it does.",
            "That's reasonable job theory, particularly like this sentence.",
            "Several young people sit in front of the laptop.",
            "So there's certain things you can do, and then there was a failed case, so I wanted to show you failed case."
        ],
        [
            "This is this is a failed case.",
            "This is Rob Fergus.",
            "And if you look at, you know it's completely off, right?",
            "A woman stands in the middle of an art Gallery when standing up off the wall, or a painting of a woman leaning up against the wall or young woman painting a picture of a wall in the middle of an art Gallery, woman evolution.",
            "So these are these actual things like this is we have an app on the website which we're going to get up and running.",
            "So if you actually upload the picture, it will generate sentences for you.",
            "So these are.",
            "Are these?"
        ],
        [
            "Sentences.",
            "Intel, in terms of results, these kinds of models they do well, but let me just maybe step back a little bit and say that there's also was a follow up work joint work between University, Montreal and Toronto on looking at the visual attention.",
            "So going a little bit beyond just the simple caption generation.",
            "And the idea here is the following.",
            "You have an in."
        ],
        [
            "Each and then."
        ],
        [
            "When you generating a sentences, you also paying attention to different parts of the image, right?",
            "So, for example, whenever you generating a woman tends to focus on the woman and also focus on the child in the park.",
            "Is throwing a Frisbee and when it generates the word Frisbee, it focuses on the Frisbee.",
            "In the park and when it generates the word park, it sort of looks around right?",
            "So it's kind of a way of trying to see how can you build these models that just not only taking the full image and generating the sentence, but also looking at different parts of the image.",
            "And generating them.",
            "Um?",
            "And that part is."
        ],
        [
            "The model I guess somebody from Montreal is probably going to be talking about this, I assume.",
            "So, so this particular model was very exciting and heads up for for the students in Montreal and Toronto who put this together.",
            "'cause it's a pretty challenging challenging problem.",
            "Microsoft had the competition set up and one of the things about these kinds of models is very hard to score them.",
            "It's very hard.",
            "I give you sentence.",
            "How do you know the good sentence of that sentence?",
            "There are a lot of different metrics that you can use.",
            "But none of them really correlate with human level performance, so Microsoft guys, they set up a competition and then they actually did Mechanical Turk experiments.",
            "So they've took the sentences generating generated by machines versus sentences generated by humans and ask people to differentiate between the two.",
            "And this is the metric that was used and one thing I want to point out.",
            "So Google took the first place, then Microsoft, then Toronto and such.",
            "Um?",
            "And this is nearest neighbor, clever nearest neighbor, I should say, but it's nearest neighbor 1.",
            "Interesting thing what I want to point out here is that there is a huge gap between human performance and that you know the best system.",
            "So this is someone someone is saying that we still have some ways to go.",
            "I don't think we can claim that we've solved the caption generation problem at this stage.",
            "At least I don't believe we have.",
            "Um?",
            "Now."
        ],
        [
            "Let me step back a little bit and start focusing on attention models 'cause I think it's a very exciting area of research.",
            "It's been around for some time, but you know more recently we beginning to see that it actually works, and if you see caption generation for caption Generation, it actually worked pretty well, given that we were just trailing a little bit behind Google and Microsoft.",
            "So in so for example, in the case of video applications, that might be very relevant, right?",
            "Instead of you have video frames instead of processing every single frame.",
            "Maybe you can focus on relevant pieces and process only the necessary pieces, right?",
            "So for example, one thing you can do is you can say, well, how about you do the attention?",
            "If I."
        ],
        [
            "Show you this video and let's see if it works.",
            "Right, she's sort of attending.",
            "You tend to attend.",
            "Um?",
            "To the ball right?",
            "Or these guys riding a bike so it sort of focuses on the wheels or.",
            "Try to focus on the horse.",
            "This is horseback riding and then there are some fairly examples, so here you know it incorrectly.",
            "Classifieds this as basketball shooting and notice what happens here.",
            "These guys are playing soccer.",
            "On the field, but the system, which is basically ignores them and just pays attention to the background and basically figures out must be basketball court, so I don't even need to look at what's happening inside, and that's a failure case, yeah?",
            "So here for example, it doesn't really pay attention to people who are moving.",
            "Right?",
            "I mean could be could be.",
            "Well, maybe maybe I'll get to the formulation of how this is done, and it will probably make more sense once I actually formulate the problem yet.",
            "Yep yeah, yeah.",
            "So you can do these things like for example if I say just look at these guys over here.",
            "It will classify this as a basketball shooting or it will classify it as a soccer juggling.",
            "If I tend to the right piece, if I know which what to attend to, I will do the correct classification.",
            "The trick is how do you find what the right thing to attend?",
            "And that's a big problem."
        ],
        [
            "We talk about recent work with with Jimmy and Roger on how can we learn these attention models?",
            "And there's been some work in machine learning community where basically people have been looking at attention models for a very long time, and now we sort of getting to the stage where we can actually do something useful on real data.",
            "Um?"
        ],
        [
            "So let me try to give you a little bit of intuition what these models are doing.",
            "Suppose you have some course image right?",
            "You get some representation of the image, and then you're going to what you're going to do is you're going to be modeling the action.",
            "And you can think of this probability as defining.",
            "Where to look at right?",
            "So let's say for simplicity.",
            "Let's say we partition this image into blocks an this a here is is telling us which block to attend to, or a could be real valued number XY coordinate and a could tell us where do you want to attend to.",
            "Right, so let's say you sample and you say look at this part of the image, so that becomes your input, you go on.",
            "You get some representation.",
            "Notice that you are trying to keep the whole history and then second action you sample the second action you go on, and that's the second piece you attending to, and so forth.",
            "You look at the third action and you repeat it for.",
            "The number of steps of fixed number of steps.",
            "And then you want to classify what you see in the image.",
            "OK, so a couple of things to note here is that you can think about this whole system.",
            "As the saying can you tell me which parts of the image I want to attend to such that the probability of classifying the correct or giving the correct classification is as high as possible?",
            "Right?",
            "Or during caption Generation you can say where I should be looking in the image such that the probability of producing the correct caption is as high as possible.",
            "That's the training signal, but these actions here, stochastic right?",
            "You don't know where you want to attend to, so you want to build some kind of system that tells you where you should be attending to, so that in the end you can classify things correctly OK.",
            "So there are different types of models.",
            "There is hard attention models.",
            "This is what I'm describing here where you choosing where you want to look at, and then there's soft attention models where you're taking expectations instead of doing sampling.",
            "And I'll talk about briefly.",
            "I mostly will focus on the hard attention model and briefly mentioned something about soft attention models.",
            "Fixed.",
            "Yes, that can vary in size.",
            "You can look at different resolutions.",
            "You can look at scale.",
            "You can look at location.",
            "I'll mention that everything will come in here.",
            "You can think of this is not a single random variable, but multiple random variables.",
            "Maybe specifying location specifying size specified which you want to look at, and you can obviously build these complex system, but then learning these things become becomes much much harder task.",
            "So what's the?"
        ],
        [
            "Here, here, so let's just for simplicity.",
            "Assume that we have a supervised learning problem.",
            "We have an image, we have the labels.",
            "Our goal is to learn a good attention policy, like trying to figure out where I should be attending to and the best locations to attend to are the ones that lead to the lead the model to predict the correct class.",
            "So you want to basically figure out which locations you want to look at so that you can predict the correct class.",
            "That's the task.",
            "Obviously if somebody told you where to look at then it would be just a fully supervised learning task, right?",
            "If somebody told you a step one, look here at Step 2, look here at step three look here, you can just build these independent regressors and say you know just predict where to look at.",
            "But you don't know it.",
            "So you can think of these latent variables, or these latent gaze locations as latent.",
            "You don't know what they are.",
            "Right, so for example here if this boys playing with the bull you may maybe you want to focus focus on the ball and that actually that sequence of locations you're looking at leads to correct classification.",
            "So let."
        ],
        [
            "We formulate the model.",
            "It turns out there is actually very good connection to learning with latent variables and we sort of know how to do learning with latent variables.",
            "So here's here's how it goes.",
            "So let's say we want to maximize the probability of the correct class by marginalizing out over the latent gaze locations, right?",
            "So we want to sum over all possible paths over all possible locations to look at right?",
            "And that's the objective that we want to optimize.",
            "So W here the parameters say these parameters of the model would be recurrent.",
            "Neural Network is a set of actions, right?",
            "Let's just for simplicity.",
            "Think of them as gaze locations where they are.",
            "X is the input, so we always conditionally input for image, or if it's a video frame and such.",
            "Um?"
        ],
        [
            "So let's look at the previous approaches.",
            "There's been a lot of interesting previous approaches for doing that, and one way of doing that would be to look at the variational bound so we can do the following week and say look at this low probability here.",
            "Obviously this sum is exponential.",
            "Right, we have to.",
            "You know, if we have T steps at every single step, you know there K locations we need to look at.",
            "Then there are TTC possible configurations, right?",
            "OK, to the possible configurations right?",
            "And as you notice in machine learning.",
            "One of the fundamental problems is how do you do sums over exponentially many things.",
            "A lot of problems can be reduced to that, right?",
            "So in the end, all we're trying to do is we just basically trying to figure out how do we compute sums over huge spaces.",
            "So here what we can do is we can look at the variational inference, something that I talked about before Q.",
            "Here is some approximation to the posterior over these locations OK?",
            "So Q is something that we have to specify.",
            "In the case we accused the prior.",
            "So what we can do is we can say instead of Q we just.",
            "Use it as a prior.",
            "The price, you know you basically doing sampling forward in the model.",
            "In"
        ],
        [
            "This case, the variational bound takes this form, and this is something that was done by deep mind and by my student Jimmy when he was a deep mind.",
            "So they've explored that particular formulation.",
            "OK, so this."
        ],
        [
            "Is something that is abound.",
            "You can take derivatives.",
            "Nice thing about these models.",
            "You can differentiate and notice if you differentiate with respect to parameters you have these two terms, so that's coming from differentiating this log plus and you have this.",
            "This representation, sorry the first term coming from differentiating this guy the second term coming from differentiating this guy.",
            "So there's a little bit of algebra involved, but believe me, that's that's that's the derivative.",
            "Now, what happens with these variational arguments is that this term is a very bad term.",
            "The reason why is because it's unbounded.",
            "Right?",
            "If the probability of the correct class is very, very small.",
            "Right then this log will become very large.",
            "And so you if you want to do optimization that way once in awhile the variance becomes, you know you have high variance and then introduces problems.",
            "So what people have done before is you replacing with term with something like a 01 term.",
            "You basically saying that if you hit the right class, it's one.",
            "If you hit the wrong class at 0.",
            "OK, you can sample from hearing you say is that the right class?",
            "Then it's one.",
            "Is it wrong?",
            "Class Zero and sort of relates to reinforce algorithm people have have looked at that.",
            "But this becomes a little bit of problematic.",
            "You have to be careful how you optimizing these things.",
            "OK. Now, in principle you can just do stochastic estimator for that and you can do the following.",
            "You can say draw EM samples from the prior.",
            "Once you draw EM samples from the pride and notice what happens here, this is just an expectation and this is just an expectation and then by joining samples from the prior you can just.",
            "Do the Monte Carlo approximation OK?",
            "So you can get someone to color approximation of that.",
            "You have to be careful how you do it and such so you can look at the papers.",
            "There are a few tricks that you have to be very careful about how you do it, but in principle you can.",
            "Um?"
        ],
        [
            "OK, and how does sampling from the prior looks like?",
            "Well, the sampling from the Prius actually you just run in that look forward.",
            "You start with the data.",
            "You know you sample the location.",
            "Given that you sample the location and so forth.",
            "And once you sample, these are sampled locations and then you can do Monte Carlo approximation based on that.",
            "You basically running the network forward and then you saying how good is my prediction for the class.",
            "If I happen to choose the right locations to look at, then the probability of the correct classes is going to be high.",
            "This is great if I happen to choose sort of the wrong locations.",
            "The probability of the class in the wrong class is going to be.",
            "Small, and that's how you do it.",
            "OK."
        ],
        [
            "One of the key observations in that setting is that you can actually maximize the marginals close low probabilities directly.",
            "Can you do that?",
            "Right without actually looking at the variational bounds.",
            "And how can you do that?",
            "Well, this is the definition of the of the marginal like log likelihood, and then you can say why don't I just differentiate this thing directly?",
            "OK, and you can.",
            "And you have this normalizing constant, and if you look at this expression, here is a little bit of math you can derive.",
            "It's not very difficult.",
            "This is essentially the posterior.",
            "Right, so this is essentially this is where the variational argument comes in, because if you don't have access to the posterior, you can approximate it with the prior, let's say, But in this in this setting this is something that we cannot compute right if we could compute, it would be like in M type of algorithm, which we know how to how to solve.",
            "But we can't.",
            "But on the other hand, what we can do is we can use important sampling to estimate these expectations.",
            "OK, so let's see how we do that.",
            "Let Q be some approximation to the posterior right?",
            "So Q is approximately going to be approximation to the posterior and then using importance sampling we can do the following, right?",
            "We can say look at this importance weight sample from the queue.",
            "So we have to build some kind of inference.",
            "Network is a Q network.",
            "I'll tell you how we do that.",
            "You sample from Q.",
            "Notice that Q depends on Y&X.",
            "So it depends on the label and then you have these important weights and then you can actually get the estimates.",
            "This is the classic estimate of the marginal probability.",
            "So in principle, in principle you can do that.",
            "Let me just go back."
        ],
        [
            "Little bit.",
            "Right?"
        ],
        [
            "So let's look at the two different estimators.",
            "So this is the estimator of the variational bound.",
            "This is an estimated stochastic estimator of the marginal likelihood.",
            "And remember this is quite bad term, right?",
            "But if you look at the two estimators.",
            "Look at this these terms.",
            "They're basically the same.",
            "And look at these terms.",
            "They're basically the same.",
            "So it turns out that the two estimators are kind of very similar to each other with one exception.",
            "You have this bad term, and here you have these importance weights, which is effectively telling you how good your trajectory is, right?",
            "Things that are good, the weights will be high.",
            "Things that are bad.",
            "The weights will be low and these are normalized weights, so they sum up to one.",
            "Obviously, the performance here gains of the important sampling is going to be heavily reliant on the choice of the proposal distribution Q.",
            "How do you choose so?",
            "Imagine that I'm trying to solve the following system.",
            "If I tell you I show you an image, and I tell you there is a cow in this image.",
            "Right?",
            "The hope is that your proposal distribution will sell well.",
            "These are the locations you should be looking at.",
            "To you know, there's probably some distribution of where the cows should be in the image.",
            "If I show, you know there's an image of an airplane, maybe there's certain locations where the airplane should be right instead of just sampling it from the prior you actually trying to build the inference network, you that can predict better where the true locations are given what you're trying to predict.",
            "And if the approximate posterior Q is equal to the prior.",
            "Then this is something that Charlie did back in 2013, and if you also looking at the Q distribution, this is also similar to the related Wake sleep algorithm of.",
            "Bon, Shannon and Yoshua.",
            "Which is very recent 2015.",
            "Um?"
        ],
        [
            "OK, there is another key observations that I want to point here and I promise you that's the last technical piece.",
            "If you have finite number of samples.",
            "Then we can show that the important sampling is basically gradient descent on this objective function.",
            "Right, this is the definition of the marginal likelihood and expectation in expectation.",
            "That's what we're trying to do now using Jensen's inequality.",
            "We can show that whatever we optimizing is actually lower bound on the marginal log probability.",
            "So effectively, if you're using finite number of samples.",
            "In expectation we get a lower bound and a low probability right as the number of samples goes to Infinity.",
            "Then we actually optimizing the marginal likelihood.",
            "But we are looking at the lower bound, so now you can say what's the point?",
            "You've already when people worked on the variational bounds, but I ever we optimizing a lower bound on the marginal likelihood, although the variance can be high.",
            "And the bond becomes tighter as we increase M, so we can show that as number of samples increases the bound becomes tighter but."
        ],
        [
            "Interesting thing is that this bound is at least as accurate as the variational bound in expectation.",
            "So this is the variational bound and this is what you optimizing which is smaller.",
            "Which would you or we are optimizing, which is tighter than?",
            "Then the variational bound right?",
            "So the variational bound is looser.",
            "So what this is saying that?",
            "You know, as you increase the number of samples, we actually optimizing a better bound, but it's interesting.",
            "Kind of connection that.",
            "You know, in expectation we are looking at at the bounds.",
            "Of course, you know these kinds of arguments are a little bit tougher to make because sometimes variational works better.",
            "Sometimes sampling was better and the reason why is because if we actually have high variance in sampling that can hurt us.",
            "So we have to control for the variance."
        ],
        [
            "Little bit.",
            "So let's just come back and say this is what we want to optimize.",
            "This is what we do and these are stochastic units.",
            "But if you look at this particular formulation here, you can just think of this entire system as a neural network with some stochastic units of some deterministic units, right?",
            "That's all it is.",
            "You know you have this specific structure, but it's just the neural network with some stochastic units and deterministic units.",
            "Um?"
        ],
        [
            "And the interesting thing is that you can also think of this as a conditional Helmholtz machine.",
            "Right so Jeff Hinton back in 90s.",
            "Worked on Helmholtz machines and these are stochastic directed models where you have stochastic hidden variables and this is the data you're trying to model that and if you know there's been a lot of work on variational autoencoders and sigmoid belief Nets and trying to build these models.",
            "Models based on wake sleep algorithm and reweighted wake sleep algorithm and such.",
            "These are all algorithms student to this thing here.",
            "And there is nothing different from the attention models and these these kinds of models.",
            "They are very much related to each other.",
            "So in principle you can use wake sleep.",
            "You can use your weighted wake sleep, which is what we do.",
            "You can use variational turn, chorus and various kinds of things you can explore.",
            "So that connection allows you to sort of connect.",
            "Connect the two."
        ],
        [
            "OK, so now what we can do is we can learn both Nets.",
            "We can learn recognition model and we can learn.",
            "The generative model.",
            "So what happens here?",
            "This is our generative network.",
            "Given the input which is generating the targets.",
            "And this is our inference network, right?",
            "And notice that the inference networks also takes Y as its input.",
            "So this is our Q network.",
            "So notice that the train."
        ],
        [
            "Inference network you basically want to predict glimpses given the observations as well as class labels, right?",
            "'cause every time you try to do inference given the data and the targets you're trying to predict, what's the best distribution of the locations I should be looking at?",
            "An we can parameterise it this way.",
            "There's a particular choice that we're making.",
            "There's probably many other choices you can do.",
            "This is just the first attempt, so this distribution is in langle and again the same as a prior except for each decision.",
            "You also taking label.",
            "Why into account, which is probably the simplest way to do it.",
            "And then to train AQ in that way."
        ],
        [
            "But you can just look at the KL divergences.",
            "The definition of the KL divergent's you have this particular expression and you can use important sampling with the following stochastic estimate of the gradient and that's you estimate of the gradient, which is the same as was done in wake sleep algorithm related wake sleep algorithm.",
            "So in fact you can use important ways computing for the attention model, so the whole thing you know you don't have to do these extra work.",
            "You can reuse the samples we actually train the queue network.",
            "But again, the step that I want to take back is, you know, to tell you that in these kinds of models and attention models, I think it's important to learn the generative piece as well as the recognition piece.",
            "So this is, this is what's done."
        ],
        [
            "These are just some examples.",
            "You know, I haven't talked about.",
            "There's something very important when you're looking at these models, you have to define so-called control variants, techniques to reduce the variance.",
            "So it's becomes a little bit more technical.",
            "You can look at the paper.",
            "It's going to be online very soon, but let me show you."
        ],
        [
            "This is done on on the captions data set.",
            "You know this is variational approximation.",
            "This is the recurrent attention model weeks Leprechaun intention models and you know we sort of get a little bit better numbers, but not much.",
            "So we still have to push on the performance of these models.",
            "You know, but when we look at the negative log likelihood, you know we are doing better compared to variational arguments."
        ],
        [
            "Here's one thing that you can do, which I think is pretty interesting.",
            "So this is a very toyish example what you're trying to do is you're trying to do digit recognition.",
            "All right, and then you have two random variables.",
            "One of them is predicting the location where you want to look, the other one is predicting scale.",
            "Do you want to do?",
            "You want to look at?",
            "You want to choose between three different scales?",
            "You're looking at the image.",
            "And it turns out that dealing with scale is very hard, so you have these two random variables, right?",
            "One is 1 is softmax, the other one is Gaussian.",
            "Ann, looking at the variational approximations very hard to make these systems work in the variational domain.",
            "But you can make them work in the hard attention domain.",
            "At least that was our experience.",
            "So this is what the system does, right so?",
            "So it sort of focuses on.",
            "You know pieces of like look at this four right?",
            "Like if I if I run it again.",
            "So focuses on like middle of the five.",
            "It focuses on the four words, has a little bit of across and such.",
            "This is a different scale, you know, so it's sort of finds what the thing what the thing should be, and then you know if you really have a little for here, you know it sort of tends to, you know, these are just examples, but it does for the caption generation we're trying to do the system where.",
            "You can attend to different layers in the convolutional model, right?",
            "So if you intend to the high level, you seeing a little bit you have, you look at the bigger part of the image.",
            "If you're attending to different levels in the convolutional model, you can actually get better results.",
            "OK, so this is.",
            "This is the attention, I think it's a.",
            "It's an exciting area, is a lot of people trying to look at this thing and the hope is that potentially maybe these systems can take over convolutions and be able to sort of attend two different pieces and maybe replace convolutional neural Nets."
        ],
        [
            "OK, now let me just finally point out that you know if you look at this entire systems here.",
            "Instead of sampling, you can take expectations right?",
            "And this is work that was done in University of Montreal, in particular for machine translation was very successful, right where you can take the expectations and I think Alex also did this work back in 2003.",
            "So some version of it.",
            "Right?"
        ],
        [
            "So what's the difference?",
            "Well, the soft attention models they are computationally expensive.",
            "The reason why is because you have to examine every single image location.",
            "Now you can try to use some tricks to cut.",
            "Cut that computation down, but in principle you have to.",
            "But the nice thing about these soft models is that they are deterministic.",
            "You never need to sample the locations.",
            "You basically taking expectations, so you smoothing it.",
            "The heart attention models are believe a computational much more efficient because you just choosing particular thing to look at and you just processing that one piece.",
            "But there stochastic so they require some form of sampling because they have to make discrete choices.",
            "So you know research is basically taking in both ends.",
            "I know that people are now looking trying to basically look at translation models and trying to look at hard attention models to see where they can use them instead of soft attention.",
            "But again, the jury is still out.",
            "The research takes place in both directions.",
            "Yes.",
            "Could be there is a lot of research that could be done in that area.",
            "Yes, it's.",
            "So."
        ],
        [
            "Let me finish off by.",
            "You know I've talked about caption Generation 0 short learning, trying to look at attention models.",
            "You know, we've seen that there's been a lot of successes using attention model soft attention models for caption Generation.",
            "But let me talk about finally on about the model on trying to one sentence representations right?",
            "Because these are very important, very important problem.",
            "Wherever you're going to use them for generating sentences or.",
            "Captions and such.",
            "So this is learning skip thought vectors.",
            "This is by Ryan Curious and Bunch of other people and yukun, but credit goes to Ryan 'cause he really.",
            "Pull it together."
        ],
        [
            "So sequence to sequence modeling.",
            "There's been a lot of work on sequence to sequence modeling, right?",
            "You have some input sequence.",
            "You encode it using RN ends recurrent Nets.",
            "And then maybe you decode it.",
            "Right, so maybe you want to translate English to French.",
            "And there's been a lot of work done in that space, and you can think of this as your representation, right?",
            "You can apply to video sequences.",
            "You can apply it to language.",
            "Kind of nice nice formulation."
        ],
        [
            "So what you can do is you can do that.",
            "You can use the following very simple idea, extremely simple idea.",
            "Let's say you have three sentences, a couple of sentences, continuous sentences.",
            "Now you using this middle sentence and you encode it using LTM.",
            "It's a recurrent neural net.",
            "And then you use this sentence to reconstruct the previous sentence and the next sentence you trying to reconstruct the previous and the next.",
            "So for example, the input is the triplet I got back home.",
            "I could see the cat on the steps.",
            "This was strange.",
            "So encoding the middle sentence and you try to predict the context around it.",
            "Trying to predict this sentence in that sentence."
        ],
        [
            "Alright, so what's what's the idea here?",
            "Is that again?",
            "You encoding a sentence using recurrent net?",
            "And then you are decoding it back the previous sentence and you're decoding the 4th sentence OK."
        ],
        [
            "So what's the objective function?",
            "The sum?",
            "The objective function is basically the sum of the log probability of the next and the previous sentence conditioned on the representation of the encoder.",
            "Right and you can compute these these probabilities.",
            "So this is just goes over the forward sentence.",
            "These are all the words in the forward sentence.",
            "These are all the words in the previous sentence and.",
            "And that's basically the objective function.",
            "The entire thing is differentiable, so you can just do stochastic gradient descent.",
            "Um?",
            "Right, and that's the representation of the encoder."
        ],
        [
            "Now, if you use this thing on a very large data set so this is books 11K data set, you have about 11,000 books.",
            "You have about 1,000,000 words and about.",
            "Not a million words about a billion words, about million unique words.",
            "And so here's here's, let me show you some examples.",
            "So you have a query sentence along with its nearest neighbors from 500 sentences.",
            "Using just cosine similarity and learn representations.",
            "So here's the sentence.",
            "He ran his hand inside his code, double checking that the unopened letter was still there.",
            "It's pretty complex sentence, right?",
            "And this is the nearest sentence.",
            "He slipped his hand between his code in his shirt, where the folded copies laying Brown envelope.",
            "You know, kind of similar.",
            "This is some other examples."
        ],
        [
            "Um?",
            "Yes, so she said aggressively, so you know if you look through these examples, in many cases sort of finds these interesting similar similar sentences.",
            "Um?"
        ],
        [
            "But let me show you some some examples of how well is it works, so this was a semantic evaluation task.",
            "You basically looking at the semantic relatedness between two sentences.",
            "So I have two sentences and you have to produce a score.",
            "How semantically similar these sentences are based on scores one to five and the data set counts with a predefined split of training and tests and such.",
            "And then you can use these representation of these vectors and then what?",
            "But you can essentially do is just building a simple linear regression model to predict the semantic relatedness.",
            "Right, so getting the representations from these two sentences, you know.",
            "Putting componentwise features between pairs something very simple, like looking at the difference between the two and based on that you just building a simple linear regression model.",
            "Now for a lot of these tasks we did not back propagate through the entire system, because the goal here was to try to see how good these embeddings are.",
            "Right across a variety of different tasks, so we just want to say, given these representations, how good they are for all of these tasks."
        ],
        [
            "OK, and this is what the system does.",
            "Just wanted to point out here is that these are evaluations coming in 2014.",
            "Um?",
            "This is mean squared error versus also doing some correlations.",
            "These are all results reported in the paper by title.",
            "Guess that's coming from Stanford and this is ours.",
            "Right?",
            "So there are various different combinations.",
            "I think that Ryan was using the models where you using sort of bidirectional, so these are called by skipping is using bidirectional STM.",
            "You combining units keep with bioscape manure, folding in the Coco data set so that is sort of like slight different extensions, but what's interesting is that if you look at the dependency tree LTM model here, you pretty much on par, just slightly below the state of the art, but you are on par.",
            "But notice we're not doing anything complicated here.",
            "Right?"
        ],
        [
            "And that's interesting, but this is fun to look at.",
            "So this is the case where you look at the two sentences and you say how semantically similar they are.",
            "So for example, here a girl is looking at the woman costume an A young girl is looking at women in costume right?",
            "The ground truth is 4.7.",
            "And the predicted is 4.5.",
            "So the reason why it's 4.7 is because I think that they ask 10 people or so like some number of people to write these things.",
            "So and then they take the average.",
            "So a man is driving a car.",
            "The car is being driven by a man.",
            "Write 54.9.",
            "Here's some failure.",
            "Cases at person is performing acrobatics on the motorcycle.",
            "Person is performing tricks on a motorcycle.",
            "You know the ground truth is 4.3.",
            "They are related.",
            "Model predicts 4.4.",
            "Here's a failure case.",
            "A person is performing tricks on a motorcycle.",
            "The performer is tricking a personal motorcycle.",
            "Now the ground truth is 2.6, right?",
            "But the model gets it is 4.4, so there is some room for improvement or this one.",
            "Someone is pouring ingredients into report.",
            "A man is removing vegetables from a pot.",
            "Or nobody is pouring ingredients into the pot.",
            "Someone is pouring rain into the pot, so you know you get you get sort of consistency there, right?",
            "So obviously you know there are some cases where these systems can fail, but on average they're doing."
        ],
        [
            "Really, really well then you can also look at the paraphrase detection.",
            "So for example, this is the Microsoft paraphrase corpus.",
            "Again, for two sentences you have to predict whether or not they are paraphrases of each other, and these results.",
            "These are all recursive autoencoders.",
            "This is the best published results.",
            "There's a lot of work being done in a natural language and LP communities, and this is our model, so you can see you know these numbers are better than ours numbers, but.",
            "You know one thing I do want to emphasize is that when we're not tuning these sentence representations for a particular task, you just taking these representations and then you see how well do they work.",
            "So it's kind of like trying to build a sentence two VEC representation compatible to work.",
            "The vector presentation right?"
        ],
        [
            "This is another example.",
            "You know you have five different datasets.",
            "Trying to do a lot of different things.",
            "These are bag of words representations.",
            "These are supervised models and this is ours and then you can see that.",
            "You know?",
            "These particular kind of models they're doing a little bit better than than what we're doing, but we sort of up there.",
            "Yes, sorry.",
            "Yes.",
            "Yes, so I'll come back to that at the very end.",
            "I'll come back to that.",
            "I'll try to answer that question at the very end.",
            "I have an answer for that.",
            "So basically the short answer is that this is something that we've started exploring, particularly something that Ryan started exploring, and there's a lot of more things that you can do, and I'll come back to that.",
            "Good question."
        ],
        [
            "Oh, this is summer.",
            "In fact, I'm coming back to that question so.",
            "So this particular model, it only looks at, you know, only scratches.",
            "The surface of all possible things you can do, right.",
            "So for example, a lot of different possibilities can be explored, including what you are suggesting.",
            "Do you really need this huge representation?",
            "Maybe you can go with a smaller one and achieve similar results.",
            "Deep auto encoders and decoders right now.",
            "You just I think Ryan is just using the 1st, just a layer less than with a single layer.",
            "Maybe you can improve upon that.",
            "You can look at the larger context windows.",
            "We just predicting the previous on the next sentence.",
            "You can try to predict things around looking at the context, encoding, decoding paragraphs.",
            "Maybe you want to decode the entire paragraph as opposed just the sentences that or looking at other other encoders.",
            "Um?",
            "I think that there is a lot of exploration we haven't.",
            "You know to be honest.",
            "We haven't done exhaustive search of what's the best.",
            "Right, and I think about these models.",
            "Is that and Ryan can correct me if I'm wrong.",
            "Is going through this entire I think.",
            "Going through the entire data set takes like a day or so or even longer than that right on the GPU.",
            "It takes 2 weeks.",
            "There you go, so just going through a single Passover, the data takes about 2 weeks.",
            "It's it's.",
            "It's a very big corpus, and so again a lot of things.",
            "Can be explored.",
            "And also I should point out that the code and data are available online.",
            "So if you want to crank take a crack at it.",
            "The data is online through 11,000.",
            "Books are online and there is also code the encoding of sentences is also in line.",
            "So if you working with sentences and you want to get the vector representation of the sentences and see how good they are and test how good they are, the code is also online.",
            "I think it's on GitHub.",
            "You can just use it as a word to vector this sentence to back and see see see how good these representations are.",
            "OK."
        ],
        [
            "On that point, I should probably stop and thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's what I'm going to talk about, so I'm going to talk about several different things.",
                    "label": 0
                },
                {
                    "sent": "So the first part of the talk is going to be for digital is going to be a little bit more and empirical side.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you some interesting things we can do.",
                    "label": 0
                },
                {
                    "sent": "The second part is going to be a little bit more technical, so I'll try to look at the two so I'd like to talk about zero shot learning, sort of trying to combine images in text.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about caption generation work, something that I've shown you before.",
                    "label": 0
                },
                {
                    "sent": "Then a little bit more on the technical side, I'd like to just give you some intuitions about learning recurrent attention models.",
                    "label": 0
                },
                {
                    "sent": "And then I'll finish off by.",
                    "label": 0
                },
                {
                    "sent": "The recent work on learning skipped vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me first give you a little bit of a zero shot learning type of setup, which is I think it's an interesting setup.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple idea, but I just wanted to share it with you and this is joint work with Jimmy Barkan is Fairsky and Sonia Fiddler, but you know credit goes to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These two guys is the students who did the work.",
                    "label": 0
                },
                {
                    "sent": "OK. Can you solve 0 short learning for me?",
                    "label": 0
                },
                {
                    "sent": "So that's a test for you.",
                    "label": 0
                },
                {
                    "sent": "Let's say I give you this Wikipedia article right, and I ask you, can you classify?",
                    "label": 0
                },
                {
                    "sent": "So it talks about Canada, Canada warbler.",
                    "label": 0
                },
                {
                    "sent": "The question for you is that can you actually find the right class?",
                    "label": 0
                },
                {
                    "sent": "Can you solve this problem?",
                    "label": 1
                },
                {
                    "sent": "How many of you think that that's the right answer?",
                    "label": 0
                },
                {
                    "sent": "What about this one?",
                    "label": 0
                },
                {
                    "sent": "How about this one?",
                    "label": 0
                },
                {
                    "sent": "One good pretty much most of you probably guess that that's the right answer, right?",
                    "label": 0
                },
                {
                    "sent": "How come, well, they sent something about have black forehead and black necklace, and I think that's that's the bird that has it.",
                    "label": 0
                },
                {
                    "sent": "Right, so you able to solve this problem even though yeah.",
                    "label": 0
                },
                {
                    "sent": "It's working with.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Turns out that that's Canada.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is a yellow world which is a different one.",
                    "label": 0
                },
                {
                    "sent": "No, but OK.",
                    "label": 0
                },
                {
                    "sent": "So so to be fair, what I should have shown you is I should.",
                    "label": 0
                },
                {
                    "sent": "I should have shown you maybe 20 Wikipedia articles about 20 different classes and then you could make your mind figuring out which one is which.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Could be could be, but I can tell you that people are very good at solving this problem.",
                    "label": 0
                },
                {
                    "sent": "Like on the kinds of images that you see, you pretty good.",
                    "label": 0
                },
                {
                    "sent": "What about this one?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a bit tougher one, but still.",
                    "label": 0
                },
                {
                    "sent": "The answer is this guy here.",
                    "label": 0
                },
                {
                    "sent": "Right, so one question, one question is that can you actually do these kinds of things with machines?",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very very simple model.",
                    "label": 0
                },
                {
                    "sent": "Very simple idea of how we can do that.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a simple set up.",
                    "label": 0
                },
                {
                    "sent": "We're trying to do solve one versus all classifier.",
                    "label": 0
                },
                {
                    "sent": "Right, so we building you can think of these weights as a vector weights for a particular class.",
                    "label": 0
                },
                {
                    "sent": "Alright, and this is sort of a standard problem that let's say we're trying to solve doesn't necessarily have to be linear model, could be convolutional neural network or whatever your favorite model is.",
                    "label": 0
                },
                {
                    "sent": "So how can you deal with previously unseen classes?",
                    "label": 0
                },
                {
                    "sent": "OK, well, we can't really do it in that particular formulation.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "Suppose we have an additional text features T OK. And suppose they line some space RP.",
                    "label": 0
                },
                {
                    "sent": "And the very simple idea is to do the following week instead of using a static weight vector W, we can actually use the text features to predict the weights of the classifier.",
                    "label": 1
                },
                {
                    "sent": "Right, so we're going to be parameterising our textual features to say, well, can you actually predict the weight of the classifier?",
                    "label": 0
                },
                {
                    "sent": "And we can use it to predict the output weights of a client of a classifier for convolutional networks and such.",
                    "label": 1
                },
                {
                    "sent": "It doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Very simple idea.",
                    "label": 0
                },
                {
                    "sent": "So what the model looks like is the following.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "It's a Wikipedia article, right?",
                    "label": 0
                },
                {
                    "sent": "And we're doing very primitive way of treating.",
                    "label": 0
                },
                {
                    "sent": "The Wikipedia articles at this stage it's very early work, obviously can do something much more intelligent like using recurrent neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Something that I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "At this tutorial and then you pushing it through, let's say, some kind of nonlinear multilayer perception.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In getting these features so we have C classes and these are can dimensional features right?",
                    "label": 0
                },
                {
                    "sent": "You take an image, you get some feature representation of the image and then you basically taking the dot product of the two.",
                    "label": 0
                },
                {
                    "sent": "Right, and then you're trying to predict which class that is.",
                    "label": 0
                },
                {
                    "sent": "So that way you can think about this the following and that's the idea that's been explored a lot in the machine learning community, and you'll see when people are trying to work with multimodal data.",
                    "label": 0
                },
                {
                    "sent": "This is basically what happens is that you take the Wikipedia article you embedded in some latent space, you take images and embed in the same space.",
                    "label": 0
                },
                {
                    "sent": "Right, well, you take the same story goes on, so you're trying to basically find this joint space right?",
                    "label": 0
                },
                {
                    "sent": "And essentially this model does something something very similar, and then you can minimize cross entropy or hinge loss, whatever you object if you want to do you trying to solve classification problem?",
                    "label": 0
                },
                {
                    "sent": "Right, so it's a very, very simple setup.",
                    "label": 0
                },
                {
                    "sent": "An initially when we started looking at the setup, I was basically saying no way we can solve this particular problem.",
                    "label": 0
                },
                {
                    "sent": "It seems like a very hard problem, and in fact we can't.",
                    "label": 0
                },
                {
                    "sent": "But we can make fairly good improvements and given the fact that we're just using simple like TF IDF Deco type of features so we can improve upon that.",
                    "label": 0
                },
                {
                    "sent": "But let's let me look at the set up.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setup is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have an images as well as the associated class labels, right?",
                    "label": 1
                },
                {
                    "sent": "So you have see distinct classes.",
                    "label": 0
                },
                {
                    "sent": "Think about, you know, maybe having 200 different classes and then during the test time.",
                    "label": 0
                },
                {
                    "sent": "You get.",
                    "label": 0
                },
                {
                    "sent": "An N sub zero number of previously unseen classes so you've never seen these classes before.",
                    "label": 1
                },
                {
                    "sent": "Right, so think of it that you know at the training time.",
                    "label": 0
                },
                {
                    "sent": "I give you Wikipedia article about about Tigers and you know everything about Tigers and I give you multiple images of Tigers so you see how images correlate with text and at the test time I give you Wikipedia article about Siberian Tigers right and my hope is that maybe you figure out Siberia has to be snow.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's like a Tiger in the snow, right?",
                    "label": 0
                },
                {
                    "sent": "And that would be Siberian Tiger, for example.",
                    "label": 0
                },
                {
                    "sent": "The goal is to do well on previously unseen classes as well as previously seen classes.",
                    "label": 1
                },
                {
                    "sent": "So basically trying to learn a very good similarity.",
                    "label": 0
                },
                {
                    "sent": "Kernel between images and.",
                    "label": 0
                },
                {
                    "sent": "Articles Wikipedia articles.",
                    "label": 0
                },
                {
                    "sent": "Right so with.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That just with this very simple idea, we've looked at a couple of datasets.",
                    "label": 0
                },
                {
                    "sent": "The interesting set up here is that it is only single Wikipedia article for each of the one of these bird classes you have about 200 classes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this is coming from Caltech Bird data set.",
                    "label": 0
                },
                {
                    "sent": "It's not a very big data set, sort of reasonable out of 200 classes you have about 40 classes, defined as unseen and 160 classes defined as seen and the same setup happens for the Oxford Flowers data set, right?",
                    "label": 1
                },
                {
                    "sent": "You have about 102 classes.",
                    "label": 1
                },
                {
                    "sent": "So in this case 20 classes are used for being as unseen, 82 classes being is treated as seen.",
                    "label": 0
                },
                {
                    "sent": "So what would you expect from this?",
                    "label": 0
                },
                {
                    "sent": "How many of you think you can do much better than random?",
                    "label": 0
                },
                {
                    "sent": "Good, that's it's very inspiring, because when I looked at this problem, I thought you barely can do better than random.",
                    "label": 0
                },
                {
                    "sent": "But here is some reason.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, this is our OC's curves.",
                    "label": 0
                },
                {
                    "sent": "This is the previously published work.",
                    "label": 0
                },
                {
                    "sent": "This is what inspired us to basically look at the same problem.",
                    "label": 0
                },
                {
                    "sent": "D he stands for domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing here is that if you look at what you can do, you know .8 two point 8 zero D0C.",
                    "label": 0
                },
                {
                    "sent": "These are not perfect, but they actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that if you look at the scene classes like things that you have data for, you pretty much nailing it.",
                    "label": 0
                },
                {
                    "sent": "Right, so the surprising thing is that, and that's an interesting kind of setup, because you never see images, you just everything is based on just the articles themselves.",
                    "label": 0
                },
                {
                    "sent": "So I think it's an interesting problem.",
                    "label": 0
                },
                {
                    "sent": "Now if we go to the flower data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But things become a little bit more worse, like the numbers go down, like .7 is not is not that good, so there's still a lot of room for improvement.",
                    "label": 0
                },
                {
                    "sent": "Somehow Flowers is much more difficult to differentiate and to be honest, I looked at this data set myself and I tried to do the classification.",
                    "label": 0
                },
                {
                    "sent": "It's very hard, specially when it comes down to Flowers.",
                    "label": 0
                },
                {
                    "sent": "You look at specific descriptions is very hard, so humans will probably do better than this, but probably not.",
                    "label": 0
                },
                {
                    "sent": "Too much better, yeah?",
                    "label": 0
                },
                {
                    "sent": "No, so for these datasets.",
                    "label": 0
                },
                {
                    "sent": "For these datasets, for each class you have a Wikipedia article.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So if the test time I only give you Wikipedia article, So what you're doing is you're taking Wikipedia article projecting into the semantic space, you're predicting what the classification weights should be, and then using those weights you classifying.",
                    "label": 0
                },
                {
                    "sent": "Right, or you can think of it the other way.",
                    "label": 0
                },
                {
                    "sent": "You can think about it is you projecting it down to the semantic space and you're looking for images.",
                    "label": 0
                },
                {
                    "sent": "That are closest to it and the ones that are closest to you classify as that particular class.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can also do interesting things like attribute discovery, so if I give you this Wikipedia article, these are the images that the system retrieves, so it confuses Scarlet tanager with summer tennager and these other birds, and it's pretty hard right?",
                    "label": 0
                },
                {
                    "sent": "I mean you have to solve the visual problem as well.",
                    "label": 0
                },
                {
                    "sent": "But everything that you can do.",
                    "label": 0
                },
                {
                    "sent": "Yes, so in this case Wikipedia articles of which class is projecting into the future space, and you're looking for nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "But the other thing you can do is you can look at the word sensitivities, you can say.",
                    "label": 0
                },
                {
                    "sent": "If you wiggle some of the words which words matter the most for predicting that particular class?",
                    "label": 0
                },
                {
                    "sent": "And it picks up these ones, so these are the most important words.",
                    "label": 0
                },
                {
                    "sent": "When the system believes that it should be bought Scarlet Tanger so it picks up Tennager picks up Scarlett.",
                    "label": 0
                },
                {
                    "sent": "Obviously Wikipedia article talks about Scala tennager.",
                    "label": 0
                },
                {
                    "sent": "Then these words become very important.",
                    "label": 0
                },
                {
                    "sent": "It also picks up these ones and I have no idea what these guys are.",
                    "label": 0
                },
                {
                    "sent": "So I looked on the Wikipedia, apparently this thing.",
                    "label": 0
                },
                {
                    "sent": "Identifies groups of birds that has to do with teenagers.",
                    "label": 0
                },
                {
                    "sent": "So if it takes these interesting kinds of features, features from words and this, this whole thing is kind of done in.",
                    "label": 0
                },
                {
                    "sent": "This is for the unseen class, so this is a class that we've never seen before.",
                    "label": 0
                },
                {
                    "sent": "Same thing happens for.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or you know images of Flowers and the word sensitivities for the.",
                    "label": 1
                },
                {
                    "sent": "For example, for bearded irises, I don't know what that is.",
                    "label": 0
                },
                {
                    "sent": "Freezing iris, apparently.",
                    "label": 0
                },
                {
                    "sent": "You know these these plants can freeze that compost and such, so it's a reason why I'm bringing this up is.",
                    "label": 0
                },
                {
                    "sent": "I think that five years ago in the vision community, there's a hole.",
                    "label": 0
                },
                {
                    "sent": "There's a whole movement of basically trying to find attributes right?",
                    "label": 0
                },
                {
                    "sent": "And people were constructing these attributes for different classes like.",
                    "label": 0
                },
                {
                    "sent": "If this class is about cats and I can predict that it's fluffy and it's you know it has certain attributes, then I can do better when dealing with new class because foreign you classify predict those attributes.",
                    "label": 1
                },
                {
                    "sent": "I can do well so we were hoping that we can actually try to find whether the relevant attributes are for the unseen classes.",
                    "label": 0
                },
                {
                    "sent": "So this is just just the intro so.",
                    "label": 0
                },
                {
                    "sent": "But let me move on to the caption generation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which sort of has a similar type of flavor, but it's trying to solve a much more complex problem, and this is work by Ryan.",
                    "label": 0
                },
                {
                    "sent": "Curious who's sitting over there.",
                    "label": 0
                },
                {
                    "sent": "I can't find a picture of Ryan on Google like you, just not on the web.",
                    "label": 0
                },
                {
                    "sent": "Man find the picture, but this is the work done by Ryan Curious.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me think about the following challenging problem, OK?",
                    "label": 1
                },
                {
                    "sent": "We've seen that we can generate tags.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can embed articles and images into some space and try to reason about that, but how about a much more complex problem where I show you this input and I want to generate the sentence?",
                    "label": 0
                },
                {
                    "sent": "OK, how can I do that?",
                    "label": 1
                },
                {
                    "sent": "And it's interesting because in the last year there's been a lot of progress in that space, and some of it is actually very surprising.",
                    "label": 0
                },
                {
                    "sent": "The things that we can do, nobody expected that these models could work so well.",
                    "label": 0
                },
                {
                    "sent": "At least I didn't expect them to work so well.",
                    "label": 0
                },
                {
                    "sent": "So it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's great, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the things that we the way we can think about these kinds of models is we can think about them in terms of encoder decoder framework.",
                    "label": 0
                },
                {
                    "sent": "Right, so think of it as you know, the way you're trying to translate from English to French, you can think of it as trying to translate from images to English.",
                    "label": 0
                },
                {
                    "sent": "Right, you have some image.",
                    "label": 0
                },
                {
                    "sent": "You have some form of encoder and then given some representation you decoding it back.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to structure this talking in two steps.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the encoder first where using convolutional neural Nets as well as their current type of models to get the representation of sentences and images.",
                    "label": 0
                },
                {
                    "sent": "And then there is going to be decoded part and the decoder part in for this particular talk I'm going to be focusing on probably signal language models.",
                    "label": 0
                },
                {
                    "sent": "Even though you know lately people are also using LSD M. So recurrent neural Nets for to do the same pretty much the same thing.",
                    "label": 0
                },
                {
                    "sent": "So let me just first focus on the encoder.",
                    "label": 0
                },
                {
                    "sent": "Let's ask the question how can we embed images and sentences into the latent space into the same space?",
                    "label": 0
                },
                {
                    "sent": "Suppose that I'm trying to solve retrieval problem.",
                    "label": 0
                },
                {
                    "sent": "Given an image, can you find me a sentence that goes well with that particular image so we can formulate it as a treatment as a retrieval problem?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the key idea and you've seen this, you probably seen this from Chris Stock is the key idea that underlines a lot of these models is the idea that you want to be representing words as some dimensional real valued vectors right?",
                    "label": 0
                },
                {
                    "sent": "Instead of trying to find the semantic space?",
                    "label": 0
                },
                {
                    "sent": "So these are word embeddings and there's been tons of work here on doing that, and the idea is that maybe you can sort of figure out the tables and chair semantically mean the same thing so we can find sort of them to be closer together in dolphin and whales and such.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are we going to do here well?",
                    "label": 0
                },
                {
                    "sent": "Let's say that we're going to be using some form of a convolutional next terministic system to embed images into some space, and then we're going to try to learn adjoining bedding of images in text, right?",
                    "label": 0
                },
                {
                    "sent": "So, in principle, you know here you can basically embed anything, right?",
                    "label": 0
                },
                {
                    "sent": "You can embed images, words, phrases, Wikipedia articles.",
                    "label": 1
                },
                {
                    "sent": "And there's a very natural definition of a scoring function.",
                    "label": 0
                },
                {
                    "sent": "You're just looking at the inner product in that space.",
                    "label": 0
                },
                {
                    "sent": "Right, so how we?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Embedding sentences just to highlight here if you have a sequence of three words, you're getting some representation and you're building a recurrent neural network or LS TM type of model.",
                    "label": 1
                },
                {
                    "sent": "Adam was giving you gave you a little bit of preview about recurrent Nets, so it's pretty much the same model, and I'm going to talk about this in the last part of this tutorial, something that Ryan's been working on skipped vectors, so we have a way of trying to basically get.",
                    "label": 0
                },
                {
                    "sent": "Interesting representation from sentences.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to convince you that they're actually pretty good representation of sentences for the images we can just use a convolutional neural network as these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "They work the best for images and these are the both representations that we're getting right, so we're using LSD estimates to embed sentences, and we're using convolutional neural Nets to embed images.",
                    "label": 0
                },
                {
                    "sent": "Right, so this becomes your image representation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "The idea is extremely simple.",
                    "label": 0
                },
                {
                    "sent": "What we'd like to do is we'd like to say that this image and this sentence get mapped to the same point.",
                    "label": 0
                },
                {
                    "sent": "In the latent space, or in the semantic space, we also want to say that this image and that sentence get embed.",
                    "label": 1
                },
                {
                    "sent": "They get embedded into the points that are close to each other.",
                    "label": 0
                },
                {
                    "sent": "Right and so what's the objective?",
                    "label": 0
                },
                {
                    "sent": "Well, you can use a ranking like objective people have done.",
                    "label": 0
                },
                {
                    "sent": "I've used these kinds of objective functions for a very long time.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple objective.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to optimize and has the following structure.",
                    "label": 0
                },
                {
                    "sent": "So here S you can think of it as just looking at the product or the cosine between.",
                    "label": 0
                },
                {
                    "sent": "Images and text right?",
                    "label": 0
                },
                {
                    "sent": "So you basically saying I want the cosine between these two vectors to be high.",
                    "label": 0
                },
                {
                    "sent": "And I want the cosine between this image and this random sentence.",
                    "label": 0
                },
                {
                    "sent": "To be small.",
                    "label": 0
                },
                {
                    "sent": "I want to push them apart, so I want to make sure that these two points are close to each other and I want to make sure that this point in this point further away from each other by some margin Alpha and Alpha can be set to one.",
                    "label": 0
                },
                {
                    "sent": "For example, it doesn't really matter and you can do the same thing for text, right?",
                    "label": 0
                },
                {
                    "sent": "So you can obviously train this system.",
                    "label": 0
                },
                {
                    "sent": "It's a deterministic system.",
                    "label": 0
                },
                {
                    "sent": "You can back propagate through the whole thing.",
                    "label": 0
                },
                {
                    "sent": "You can back propagate through the convolutional model, can back propagate through the current network model so the entire system becomes basically deterministic system.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How well does it do?",
                    "label": 0
                },
                {
                    "sent": "Here's what it does.",
                    "label": 0
                },
                {
                    "sent": "These are test images.",
                    "label": 0
                },
                {
                    "sent": "So I show you this image and I basically embed this image into the latent space and I just look for nearest neighbors in that space.",
                    "label": 0
                },
                {
                    "sent": "Anne, these are the nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "It's pretty interesting what it does right?",
                    "label": 0
                },
                {
                    "sent": "I mean, in a lot of times it actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "Now obviously this system you know I'm not telling you how we are generating sentences, which is a much more challenging task.",
                    "label": 0
                },
                {
                    "sent": "This is just the first part to tell you how we can retrieve sentences given the images.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a question.",
                    "label": 0
                },
                {
                    "sent": "So for this case, I think we were using Flickr data set and there was one caption for proper image in the Coco data set.",
                    "label": 0
                },
                {
                    "sent": "I think there are five captions per image, so we can use that as well.",
                    "label": 0
                },
                {
                    "sent": "Flickr also has five, so yes, you can use 5.",
                    "label": 0
                },
                {
                    "sent": "Retrieve an image.",
                    "label": 0
                },
                {
                    "sent": "It's it's a test, so this is it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, this is a test image.",
                    "label": 0
                },
                {
                    "sent": "And you retrieving training sentences.",
                    "label": 0
                },
                {
                    "sent": "I presume.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's multiple combinations you can do right?",
                    "label": 0
                },
                {
                    "sent": "Or given a test sentence, you can retrieve from a database the images.",
                    "label": 0
                },
                {
                    "sent": "I think Ryan for these things, user test images in an sentence is.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "But again, this task is the question.",
                    "label": 0
                },
                {
                    "sent": "Sorry what.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "So in this case again I want to emphasize this is not a generation thing.",
                    "label": 0
                },
                {
                    "sent": "This is just the retrieval thing.",
                    "label": 0
                },
                {
                    "sent": "Think of it as the following.",
                    "label": 0
                },
                {
                    "sent": "Give you an image and you based on some data set of sentences you're trying to retrieve.",
                    "label": 0
                },
                {
                    "sent": "The most relevant one, and this is what is being shown here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Yes, so the way the way to test this instance quantitatively like, if you want the answer, then for every single test image you have the correct caption, right?",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can rank all the captions and then see where this correct cache caption lens and that will give you some notion of precision.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes that's true.",
                    "label": 0
                },
                {
                    "sent": "I mean it will.",
                    "label": 0
                },
                {
                    "sent": "It will depend on that, yes, but there is sort of standard datasets, right?",
                    "label": 0
                },
                {
                    "sent": "I mean there is standard test set so everybody benchmarks off the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because I'm unfortunately, yeah, but let me write.",
                    "label": 0
                },
                {
                    "sent": "Let me get to the Jenner.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian part, so the other thing that you can do here is you can also retrieve words, not necessarily sentences, right?",
                    "label": 0
                },
                {
                    "sent": "So given those images, these are words that the system retrieves or the other way around.",
                    "label": 0
                },
                {
                    "sent": "I've shown you this before.",
                    "label": 0
                },
                {
                    "sent": "Given the word you just look for nearby sentences and actually works pretty well.",
                    "label": 0
                },
                {
                    "sent": "You can also do these funny things like.",
                    "label": 0
                },
                {
                    "sent": "Travel with adjectives, right?",
                    "label": 0
                },
                {
                    "sent": "If I give you the embedding of the word fluffy, you can look for similar images.",
                    "label": 0
                },
                {
                    "sent": "Sort of captures these things even though you're not specifying you're not training the system, you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't tell the system that this is fluffy or this is delicious.",
                    "label": 0
                },
                {
                    "sent": "It's just that whenever you look at the sentences most of the time, you know for these images, fluffy appears and delicious appears right.",
                    "label": 0
                },
                {
                    "sent": "You can also do fun things, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These kinds of embeddings.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of caveat how we've done it, but maybe I can just take it offline, but one of the things that you can do is you can embed this image.",
                    "label": 0
                },
                {
                    "sent": "You can embed the get representation of the word blue and red, and you can do vector multiplications right?",
                    "label": 0
                },
                {
                    "sent": "So you can say in image minus blue plus red and then you look for nearby images and get these things.",
                    "label": 0
                },
                {
                    "sent": "Right now mine is blue plus yellow, get yellow cars.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you can say airplane minus flying, fly, sail and get these sailboats.",
                    "label": 0
                },
                {
                    "sent": "I like this one is like kittens minus bowl plus box to get kids in the box or you get this kitten you say minus box plus bowl you get kittens in the bowl right?",
                    "label": 0
                },
                {
                    "sent": "It's actually so this thing here is a failure right?",
                    "label": 0
                },
                {
                    "sent": "I think that's a duck.",
                    "label": 0
                },
                {
                    "sent": "Right, right now this is just.",
                    "label": 0
                },
                {
                    "sent": "It has nothing to do with quantitative evaluation, is just kind of like fun things you can do so for this thing.",
                    "label": 0
                },
                {
                    "sent": "Actually we were not.",
                    "label": 0
                },
                {
                    "sent": "We were not using STM so we were using.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a way of just summing the representation of the words but but you can talk to Ryan about it, specifically how it was done, but.",
                    "label": 0
                },
                {
                    "sent": "As background, so I think that one you were using Flickr database right?",
                    "label": 0
                },
                {
                    "sent": "For this, yeah.",
                    "label": 0
                },
                {
                    "sent": "Stony Brook University.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but how about a more challenging problem?",
                    "label": 1
                },
                {
                    "sent": "How about the problem of generating sentences given the inputs right?",
                    "label": 0
                },
                {
                    "sent": "And so one of the one of the things that I'm going to talk about is I'm going to talk about neural language model for doing that.",
                    "label": 0
                },
                {
                    "sent": "And maybe I can give you a little bit of intuition how these models are working.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can think of neural language models of feedforward neural networks with a single linear layer.",
                    "label": 1
                },
                {
                    "sent": "So each work here W is represented as a K dimensional real valued vector.",
                    "label": 1
                },
                {
                    "sent": "So you basically representing them as.",
                    "label": 1
                },
                {
                    "sent": "Embedding essentially is the same as embedding words in the K dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So our here I'm going to be denoting R is a V by K matrix of words.",
                    "label": 0
                },
                {
                    "sent": "These the vocabulary, size case, the representation, the latent representation of the word.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then you can do the following.",
                    "label": 1
                },
                {
                    "sent": "You can use this following very simple idea.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, let's look at the top of minus one words where N -- 1 is the context size right?",
                    "label": 0
                },
                {
                    "sent": "And then the next word representation becomes just basically take a linear combination.",
                    "label": 0
                },
                {
                    "sent": "So take these representations.",
                    "label": 0
                },
                {
                    "sent": "The next representation for the next word you just looking at the linear combination of the previous representations of the words.",
                    "label": 1
                },
                {
                    "sent": "So these matrices are K by K parameter matrices that you have to learn.",
                    "label": 0
                },
                {
                    "sent": "And if you structure it becomes basically.",
                    "label": 0
                },
                {
                    "sent": "A feedforward neural network with a linear layer.",
                    "label": 0
                },
                {
                    "sent": "And in this case, the conditional probability of the next word is going to be given by this expression, which is a softmax.",
                    "label": 0
                },
                {
                    "sent": "And I think that this particular model was made popular in the machine learning community back in 2003 by Yoshua.",
                    "label": 0
                },
                {
                    "sent": "So there's a paper on probably single language models.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a standard model that's been around.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or for a long time now.",
                    "label": 0
                },
                {
                    "sent": "Typically this is something that could be expensive to compute because V is can be high dimensional.",
                    "label": 1
                },
                {
                    "sent": "Let's say if you're working with.",
                    "label": 0
                },
                {
                    "sent": "You know 10,000 or 100,000 words.",
                    "label": 0
                },
                {
                    "sent": "This is something that you have to you have to do, but there are ways around speeding this up.",
                    "label": 1
                },
                {
                    "sent": "So this is a standard probably signal language model.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what we're going to do is we're going to be following.",
                    "label": 0
                },
                {
                    "sent": "We're going to be looking at multiplicative models, right?",
                    "label": 0
                },
                {
                    "sent": "And we're going to be dealing with tensors, so let's say we're going to be representing words extensors, so we're going to say that this tensor is going to be a science V by K by G, where G is going to be 10 slices, and you can think of.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have some attribute vector U.",
                    "label": 0
                },
                {
                    "sent": "For example, these could be representing image features, right?",
                    "label": 0
                },
                {
                    "sent": "If you're trying to go from images to text, let's say you here is representing image features.",
                    "label": 0
                },
                {
                    "sent": "Then you can compute these gate at word representations as follows.",
                    "label": 0
                },
                {
                    "sent": "So you're getting these representations.",
                    "label": 0
                },
                {
                    "sent": "By using image features.",
                    "label": 0
                },
                {
                    "sent": "And one of the things when you're dealing with tensors, what makes it a little bit challenging is the size.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix of this tensor, right?",
                    "label": 0
                },
                {
                    "sent": "Imagine that if these 100,000 K maybe 300 dimensional or 500 dimensional and geez, you know typically have 4000 features could be very very high dimensional object.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what you can do is you can basically do low rank decomposition.",
                    "label": 1
                },
                {
                    "sent": "You can say well, instead of representing this tense, I'm going to be representing it in terms of these three low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "And if you write it up, it just has that particular presentation, right?",
                    "label": 0
                },
                {
                    "sent": "So in the back of your mind you should be just thinking that instead of dealing with full tensors, you're going to be dealing with low rank approximation, lowering decomposition of those tensors.",
                    "label": 0
                },
                {
                    "sent": "F. Here is the number of pre chosen factors you can think of them as maybe being like 20 dimensional factors of 50 dimensional factors and such, so you can reduce.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters that you need to fit.",
                    "label": 0
                },
                {
                    "sent": "So what is this system does?",
                    "label": 0
                },
                {
                    "sent": "It's basically very much the same as a neural language model, but we with these multiplicative connections, right?",
                    "label": 0
                },
                {
                    "sent": "So let's say E. It's going to be K by the matrix the same as before.",
                    "label": 0
                },
                {
                    "sent": "But you can see it's has a low rank approximation and then the predicted representation for the next word.",
                    "label": 1
                },
                {
                    "sent": "You have the same story here.",
                    "label": 0
                },
                {
                    "sent": "So far we haven't done much except for just had the low rank approximation of our matrix and then the next word representation given the representation of the next word.",
                    "label": 0
                },
                {
                    "sent": "The factor outputs.",
                    "label": 1
                },
                {
                    "sent": "This is where the gating happens, and this is componentwise product.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is that essentially image features can gate.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 1
                },
                {
                    "sent": "The probabilities for predicting the next word.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in effect what happens is that the conditional probability of the next word has the same form, but essentially, but here you have these additional features that could be, for example, image features or any other kind of sign information that affects the prediction of the next word.",
                    "label": 0
                },
                {
                    "sent": "And may.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I can just give you intuitively what is happening here.",
                    "label": 0
                },
                {
                    "sent": "Suppose I give you this image and I tell you, steamship in.",
                    "label": 0
                },
                {
                    "sent": "And then there are a bunch of words that you can predict steamship in the waters deep ship in the factory steamship in my house.",
                    "label": 0
                },
                {
                    "sent": "You know, if you just look at the language model, there's alot of things that you can predict.",
                    "label": 1
                },
                {
                    "sent": "Right just from the language model, but because you're looking at this image, this image has certain features like.",
                    "label": 0
                },
                {
                    "sent": "This is a steamship, I think it's in the water or at the dock.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this model is doing?",
                    "label": 0
                },
                {
                    "sent": "This gating model is doing is essentially it will shut off irrelevant words like will push on the probabilities of the word like steamship in a factory for example, and will push up on the probability water so that you can generate the next step ship in the water.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "So we've done, yes, so we've done it.",
                    "label": 0
                },
                {
                    "sent": "So one way you can do it is you can use it as a bias into the model.",
                    "label": 0
                },
                {
                    "sent": "You can basically say you know just have this additional thing that goes in and then predicts you can do that.",
                    "label": 0
                },
                {
                    "sent": "And to be honest, initially when we've looked at these things when Ryan looked at these things, the bias model was doing about the same as a gating.",
                    "label": 0
                },
                {
                    "sent": "But I think on more recent on bigger datasets we do see these gating things to actually give us better results.",
                    "label": 0
                },
                {
                    "sent": "So empirically we do see that.",
                    "label": 0
                },
                {
                    "sent": "The previous the context.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes exactly.",
                    "label": 0
                },
                {
                    "sent": "You can put the previous words into the gating and this is something that we haven't done.",
                    "label": 0
                },
                {
                    "sent": "I think that will work better.",
                    "label": 0
                },
                {
                    "sent": "Because I think that the gating.",
                    "label": 0
                },
                {
                    "sent": "So you can think about the following right?",
                    "label": 0
                },
                {
                    "sent": "If I you basically just the way to think about this is that you just the probability of predicting the next word, and you can implicitly shut down certain things.",
                    "label": 1
                },
                {
                    "sent": "So Chris, maybe I can show you a slightly different example where it might convince you why it's more more relevant.",
                    "label": 0
                },
                {
                    "sent": "So let me get to that.",
                    "label": 0
                },
                {
                    "sent": "Up yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, but but let me let me show you one other example which might make it a little bit more convincing.",
                    "label": 0
                },
                {
                    "sent": "So the other thing you can condition on its part of speech tags.",
                    "label": 1
                },
                {
                    "sent": "Right, so whenever you're generating a sentence, you can say.",
                    "label": 0
                },
                {
                    "sent": "What if I give you a template for part of speech tax in terms of the sentence that I want to generate?",
                    "label": 0
                },
                {
                    "sent": "An that sort of gives a soft way of finding relevant words.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say I'm trying to predict the next word given the previous word, and given this image an given, you know I want to generate the sentence that has this particular structure.",
                    "label": 0
                },
                {
                    "sent": "I don't want to impose hard structure.",
                    "label": 0
                },
                {
                    "sent": "In other words, the model can still choose to do generate what it wants to generate, but I want it to be consistent with without representation.",
                    "label": 0
                },
                {
                    "sent": "And again, these parts of speech tags I eighting is a gating because it essentially the way you can think about this is that in the language model, when you predict the next word, there's a whole distribution of possible words, right?",
                    "label": 0
                },
                {
                    "sent": "But when you when you condition on a particular parts of speech, you essentially having a.",
                    "label": 0
                },
                {
                    "sent": "It's a software you're shutting down certain words, so words that are verbs will go down.",
                    "label": 0
                },
                {
                    "sent": "Words that are nouns will go up.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, whenever you are doing that, you can generate A and then you say, well, generating a noun.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Icicle generate me.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Verb.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generate me.",
                    "label": 0
                },
                {
                    "sent": "On there and on there and so forth.",
                    "label": 0
                },
                {
                    "sent": "So that gave us a little bit better way of trying to generate something that syntactically coherent.",
                    "label": 0
                },
                {
                    "sent": "I guess that was done that was done with the multiplicative model, right, right?",
                    "label": 0
                },
                {
                    "sent": "The bias model in this case, how well does it work if you do the bias?",
                    "label": 0
                },
                {
                    "sent": "If you just use these parts of speech is advice.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the I've shown you this before.",
                    "label": 0
                },
                {
                    "sent": "These are sentences you can generate.",
                    "label": 0
                },
                {
                    "sent": "That is all good sentences.",
                    "label": 0
                },
                {
                    "sent": "Everybody shows good sentences, but I am going to show you some failed exam.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are failed examples.",
                    "label": 0
                },
                {
                    "sent": "So this is the Turbo that trying to be seen in the water.",
                    "label": 0
                },
                {
                    "sent": "Or we can generate things like apart car while driving down the road.",
                    "label": 0
                },
                {
                    "sent": "So one thing that I want to point out here is that because we are trying to at least forcing the model to generate different things.",
                    "label": 0
                },
                {
                    "sent": "Obviously, you know something like this.",
                    "label": 0
                },
                {
                    "sent": "A human would not generate, right?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another one.",
                    "label": 0
                },
                {
                    "sent": "The handlebars are trying to ride.",
                    "label": 0
                },
                {
                    "sent": "Bike rack so you know.",
                    "label": 0
                },
                {
                    "sent": "So there's there's been a lot of concern in these models.",
                    "label": 0
                },
                {
                    "sent": "Is that maybe what they do?",
                    "label": 0
                },
                {
                    "sent": "Is they just sort of learn to just copy the training captions?",
                    "label": 0
                },
                {
                    "sent": "And that's true to some extent.",
                    "label": 0
                },
                {
                    "sent": "Maybe just a matter of getting the caption from the training set and reducing it.",
                    "label": 0
                },
                {
                    "sent": "But in this case, at least, we're able to generate something novel.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, which is always great.",
                    "label": 0
                },
                {
                    "sent": "So you can also do these things where you filling in the blanks.",
                    "label": 0
                },
                {
                    "sent": "You can say you know.",
                    "label": 0
                },
                {
                    "sent": "Generate me now generates a cat and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The cute cat is in the box or there is a bus and then parked.",
                    "label": 0
                },
                {
                    "sent": "There is a car behind the bus or there is 3 on the bus and such.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I guess it should be near the bus or something like that, but they choose chosen on the bus so you can do it.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can do these things.",
                    "label": 0
                },
                {
                    "sent": "You can also do fun things like seeing how these systems work in the real data.",
                    "label": 0
                },
                {
                    "sent": "So this is a year wise, and David fleet that the NIPS workshop and this is what it generates, clicks.",
                    "label": 0
                },
                {
                    "sent": "Entrepreneurs somehow generates busboys and waiters.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure why.",
                    "label": 0
                },
                {
                    "sent": "And these are samples from the Model 2 men in the room talking on the table to many sitting next to each other, and so forth, right?",
                    "label": 1
                },
                {
                    "sent": "So you can see I mean, these are simplistic sentences, so I'm not trying to claim that we actually generating something.",
                    "label": 0
                },
                {
                    "sent": "Amazing, it sort of does does a reason.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good job, here's another example.",
                    "label": 0
                },
                {
                    "sent": "How many of you know who that is?",
                    "label": 0
                },
                {
                    "sent": "Good good good speaker typist computer and this is what generates man working the computer man sitting down with the laptop trying typing on a computer and so forth.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, you know these systems are not very good.",
                    "label": 0
                },
                {
                    "sent": "I wish it could generate something like a Facebook director.",
                    "label": 0
                },
                {
                    "sent": "Is writing a Facebook post right?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another example.",
                    "label": 0
                },
                {
                    "sent": "This is this is.",
                    "label": 0
                },
                {
                    "sent": "This is theory.",
                    "label": 0
                },
                {
                    "sent": "There is no ski.",
                    "label": 0
                },
                {
                    "sent": "I think he actually.",
                    "label": 0
                },
                {
                    "sent": "Set up nips, right.",
                    "label": 0
                },
                {
                    "sent": "He was the founder of NIPS.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This picture of NIPS.",
                    "label": 0
                },
                {
                    "sent": "So it does.",
                    "label": 0
                },
                {
                    "sent": "That's reasonable job theory, particularly like this sentence.",
                    "label": 0
                },
                {
                    "sent": "Several young people sit in front of the laptop.",
                    "label": 1
                },
                {
                    "sent": "So there's certain things you can do, and then there was a failed case, so I wanted to show you failed case.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is this is a failed case.",
                    "label": 0
                },
                {
                    "sent": "This is Rob Fergus.",
                    "label": 0
                },
                {
                    "sent": "And if you look at, you know it's completely off, right?",
                    "label": 0
                },
                {
                    "sent": "A woman stands in the middle of an art Gallery when standing up off the wall, or a painting of a woman leaning up against the wall or young woman painting a picture of a wall in the middle of an art Gallery, woman evolution.",
                    "label": 1
                },
                {
                    "sent": "So these are these actual things like this is we have an app on the website which we're going to get up and running.",
                    "label": 0
                },
                {
                    "sent": "So if you actually upload the picture, it will generate sentences for you.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "Are these?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sentences.",
                    "label": 0
                },
                {
                    "sent": "Intel, in terms of results, these kinds of models they do well, but let me just maybe step back a little bit and say that there's also was a follow up work joint work between University, Montreal and Toronto on looking at the visual attention.",
                    "label": 0
                },
                {
                    "sent": "So going a little bit beyond just the simple caption generation.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is the following.",
                    "label": 1
                },
                {
                    "sent": "You have an in.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each and then.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you generating a sentences, you also paying attention to different parts of the image, right?",
                    "label": 0
                },
                {
                    "sent": "So, for example, whenever you generating a woman tends to focus on the woman and also focus on the child in the park.",
                    "label": 0
                },
                {
                    "sent": "Is throwing a Frisbee and when it generates the word Frisbee, it focuses on the Frisbee.",
                    "label": 1
                },
                {
                    "sent": "In the park and when it generates the word park, it sort of looks around right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a way of trying to see how can you build these models that just not only taking the full image and generating the sentence, but also looking at different parts of the image.",
                    "label": 0
                },
                {
                    "sent": "And generating them.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And that part is.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model I guess somebody from Montreal is probably going to be talking about this, I assume.",
                    "label": 0
                },
                {
                    "sent": "So, so this particular model was very exciting and heads up for for the students in Montreal and Toronto who put this together.",
                    "label": 0
                },
                {
                    "sent": "'cause it's a pretty challenging challenging problem.",
                    "label": 0
                },
                {
                    "sent": "Microsoft had the competition set up and one of the things about these kinds of models is very hard to score them.",
                    "label": 0
                },
                {
                    "sent": "It's very hard.",
                    "label": 0
                },
                {
                    "sent": "I give you sentence.",
                    "label": 0
                },
                {
                    "sent": "How do you know the good sentence of that sentence?",
                    "label": 0
                },
                {
                    "sent": "There are a lot of different metrics that you can use.",
                    "label": 0
                },
                {
                    "sent": "But none of them really correlate with human level performance, so Microsoft guys, they set up a competition and then they actually did Mechanical Turk experiments.",
                    "label": 0
                },
                {
                    "sent": "So they've took the sentences generating generated by machines versus sentences generated by humans and ask people to differentiate between the two.",
                    "label": 0
                },
                {
                    "sent": "And this is the metric that was used and one thing I want to point out.",
                    "label": 0
                },
                {
                    "sent": "So Google took the first place, then Microsoft, then Toronto and such.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this is nearest neighbor, clever nearest neighbor, I should say, but it's nearest neighbor 1.",
                    "label": 0
                },
                {
                    "sent": "Interesting thing what I want to point out here is that there is a huge gap between human performance and that you know the best system.",
                    "label": 0
                },
                {
                    "sent": "So this is someone someone is saying that we still have some ways to go.",
                    "label": 0
                },
                {
                    "sent": "I don't think we can claim that we've solved the caption generation problem at this stage.",
                    "label": 0
                },
                {
                    "sent": "At least I don't believe we have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me step back a little bit and start focusing on attention models 'cause I think it's a very exciting area of research.",
                    "label": 0
                },
                {
                    "sent": "It's been around for some time, but you know more recently we beginning to see that it actually works, and if you see caption generation for caption Generation, it actually worked pretty well, given that we were just trailing a little bit behind Google and Microsoft.",
                    "label": 0
                },
                {
                    "sent": "So in so for example, in the case of video applications, that might be very relevant, right?",
                    "label": 0
                },
                {
                    "sent": "Instead of you have video frames instead of processing every single frame.",
                    "label": 1
                },
                {
                    "sent": "Maybe you can focus on relevant pieces and process only the necessary pieces, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, one thing you can do is you can say, well, how about you do the attention?",
                    "label": 0
                },
                {
                    "sent": "If I.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show you this video and let's see if it works.",
                    "label": 0
                },
                {
                    "sent": "Right, she's sort of attending.",
                    "label": 0
                },
                {
                    "sent": "You tend to attend.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To the ball right?",
                    "label": 0
                },
                {
                    "sent": "Or these guys riding a bike so it sort of focuses on the wheels or.",
                    "label": 0
                },
                {
                    "sent": "Try to focus on the horse.",
                    "label": 0
                },
                {
                    "sent": "This is horseback riding and then there are some fairly examples, so here you know it incorrectly.",
                    "label": 0
                },
                {
                    "sent": "Classifieds this as basketball shooting and notice what happens here.",
                    "label": 0
                },
                {
                    "sent": "These guys are playing soccer.",
                    "label": 0
                },
                {
                    "sent": "On the field, but the system, which is basically ignores them and just pays attention to the background and basically figures out must be basketball court, so I don't even need to look at what's happening inside, and that's a failure case, yeah?",
                    "label": 0
                },
                {
                    "sent": "So here for example, it doesn't really pay attention to people who are moving.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I mean could be could be.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe maybe I'll get to the formulation of how this is done, and it will probably make more sense once I actually formulate the problem yet.",
                    "label": 0
                },
                {
                    "sent": "Yep yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So you can do these things like for example if I say just look at these guys over here.",
                    "label": 0
                },
                {
                    "sent": "It will classify this as a basketball shooting or it will classify it as a soccer juggling.",
                    "label": 1
                },
                {
                    "sent": "If I tend to the right piece, if I know which what to attend to, I will do the correct classification.",
                    "label": 0
                },
                {
                    "sent": "The trick is how do you find what the right thing to attend?",
                    "label": 0
                },
                {
                    "sent": "And that's a big problem.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We talk about recent work with with Jimmy and Roger on how can we learn these attention models?",
                    "label": 0
                },
                {
                    "sent": "And there's been some work in machine learning community where basically people have been looking at attention models for a very long time, and now we sort of getting to the stage where we can actually do something useful on real data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me try to give you a little bit of intuition what these models are doing.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have some course image right?",
                    "label": 0
                },
                {
                    "sent": "You get some representation of the image, and then you're going to what you're going to do is you're going to be modeling the action.",
                    "label": 0
                },
                {
                    "sent": "And you can think of this probability as defining.",
                    "label": 0
                },
                {
                    "sent": "Where to look at right?",
                    "label": 0
                },
                {
                    "sent": "So let's say for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Let's say we partition this image into blocks an this a here is is telling us which block to attend to, or a could be real valued number XY coordinate and a could tell us where do you want to attend to.",
                    "label": 0
                },
                {
                    "sent": "Right, so let's say you sample and you say look at this part of the image, so that becomes your input, you go on.",
                    "label": 0
                },
                {
                    "sent": "You get some representation.",
                    "label": 0
                },
                {
                    "sent": "Notice that you are trying to keep the whole history and then second action you sample the second action you go on, and that's the second piece you attending to, and so forth.",
                    "label": 0
                },
                {
                    "sent": "You look at the third action and you repeat it for.",
                    "label": 0
                },
                {
                    "sent": "The number of steps of fixed number of steps.",
                    "label": 0
                },
                {
                    "sent": "And then you want to classify what you see in the image.",
                    "label": 0
                },
                {
                    "sent": "OK, so a couple of things to note here is that you can think about this whole system.",
                    "label": 0
                },
                {
                    "sent": "As the saying can you tell me which parts of the image I want to attend to such that the probability of classifying the correct or giving the correct classification is as high as possible?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Or during caption Generation you can say where I should be looking in the image such that the probability of producing the correct caption is as high as possible.",
                    "label": 0
                },
                {
                    "sent": "That's the training signal, but these actions here, stochastic right?",
                    "label": 0
                },
                {
                    "sent": "You don't know where you want to attend to, so you want to build some kind of system that tells you where you should be attending to, so that in the end you can classify things correctly OK.",
                    "label": 0
                },
                {
                    "sent": "So there are different types of models.",
                    "label": 0
                },
                {
                    "sent": "There is hard attention models.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm describing here where you choosing where you want to look at, and then there's soft attention models where you're taking expectations instead of doing sampling.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about briefly.",
                    "label": 0
                },
                {
                    "sent": "I mostly will focus on the hard attention model and briefly mentioned something about soft attention models.",
                    "label": 0
                },
                {
                    "sent": "Fixed.",
                    "label": 0
                },
                {
                    "sent": "Yes, that can vary in size.",
                    "label": 0
                },
                {
                    "sent": "You can look at different resolutions.",
                    "label": 0
                },
                {
                    "sent": "You can look at scale.",
                    "label": 0
                },
                {
                    "sent": "You can look at location.",
                    "label": 0
                },
                {
                    "sent": "I'll mention that everything will come in here.",
                    "label": 0
                },
                {
                    "sent": "You can think of this is not a single random variable, but multiple random variables.",
                    "label": 0
                },
                {
                    "sent": "Maybe specifying location specifying size specified which you want to look at, and you can obviously build these complex system, but then learning these things become becomes much much harder task.",
                    "label": 0
                },
                {
                    "sent": "So what's the?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, here, so let's just for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Assume that we have a supervised learning problem.",
                    "label": 1
                },
                {
                    "sent": "We have an image, we have the labels.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to learn a good attention policy, like trying to figure out where I should be attending to and the best locations to attend to are the ones that lead to the lead the model to predict the correct class.",
                    "label": 1
                },
                {
                    "sent": "So you want to basically figure out which locations you want to look at so that you can predict the correct class.",
                    "label": 0
                },
                {
                    "sent": "That's the task.",
                    "label": 0
                },
                {
                    "sent": "Obviously if somebody told you where to look at then it would be just a fully supervised learning task, right?",
                    "label": 0
                },
                {
                    "sent": "If somebody told you a step one, look here at Step 2, look here at step three look here, you can just build these independent regressors and say you know just predict where to look at.",
                    "label": 0
                },
                {
                    "sent": "But you don't know it.",
                    "label": 0
                },
                {
                    "sent": "So you can think of these latent variables, or these latent gaze locations as latent.",
                    "label": 0
                },
                {
                    "sent": "You don't know what they are.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example here if this boys playing with the bull you may maybe you want to focus focus on the ball and that actually that sequence of locations you're looking at leads to correct classification.",
                    "label": 0
                },
                {
                    "sent": "So let.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We formulate the model.",
                    "label": 0
                },
                {
                    "sent": "It turns out there is actually very good connection to learning with latent variables and we sort of know how to do learning with latent variables.",
                    "label": 0
                },
                {
                    "sent": "So here's here's how it goes.",
                    "label": 0
                },
                {
                    "sent": "So let's say we want to maximize the probability of the correct class by marginalizing out over the latent gaze locations, right?",
                    "label": 1
                },
                {
                    "sent": "So we want to sum over all possible paths over all possible locations to look at right?",
                    "label": 0
                },
                {
                    "sent": "And that's the objective that we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "So W here the parameters say these parameters of the model would be recurrent.",
                    "label": 1
                },
                {
                    "sent": "Neural Network is a set of actions, right?",
                    "label": 0
                },
                {
                    "sent": "Let's just for simplicity.",
                    "label": 1
                },
                {
                    "sent": "Think of them as gaze locations where they are.",
                    "label": 0
                },
                {
                    "sent": "X is the input, so we always conditionally input for image, or if it's a video frame and such.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the previous approaches.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of interesting previous approaches for doing that, and one way of doing that would be to look at the variational bound so we can do the following week and say look at this low probability here.",
                    "label": 0
                },
                {
                    "sent": "Obviously this sum is exponential.",
                    "label": 0
                },
                {
                    "sent": "Right, we have to.",
                    "label": 0
                },
                {
                    "sent": "You know, if we have T steps at every single step, you know there K locations we need to look at.",
                    "label": 0
                },
                {
                    "sent": "Then there are TTC possible configurations, right?",
                    "label": 0
                },
                {
                    "sent": "OK, to the possible configurations right?",
                    "label": 0
                },
                {
                    "sent": "And as you notice in machine learning.",
                    "label": 0
                },
                {
                    "sent": "One of the fundamental problems is how do you do sums over exponentially many things.",
                    "label": 0
                },
                {
                    "sent": "A lot of problems can be reduced to that, right?",
                    "label": 0
                },
                {
                    "sent": "So in the end, all we're trying to do is we just basically trying to figure out how do we compute sums over huge spaces.",
                    "label": 0
                },
                {
                    "sent": "So here what we can do is we can look at the variational inference, something that I talked about before Q.",
                    "label": 0
                },
                {
                    "sent": "Here is some approximation to the posterior over these locations OK?",
                    "label": 1
                },
                {
                    "sent": "So Q is something that we have to specify.",
                    "label": 0
                },
                {
                    "sent": "In the case we accused the prior.",
                    "label": 1
                },
                {
                    "sent": "So what we can do is we can say instead of Q we just.",
                    "label": 0
                },
                {
                    "sent": "Use it as a prior.",
                    "label": 0
                },
                {
                    "sent": "The price, you know you basically doing sampling forward in the model.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case, the variational bound takes this form, and this is something that was done by deep mind and by my student Jimmy when he was a deep mind.",
                    "label": 0
                },
                {
                    "sent": "So they've explored that particular formulation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is something that is abound.",
                    "label": 0
                },
                {
                    "sent": "You can take derivatives.",
                    "label": 0
                },
                {
                    "sent": "Nice thing about these models.",
                    "label": 0
                },
                {
                    "sent": "You can differentiate and notice if you differentiate with respect to parameters you have these two terms, so that's coming from differentiating this log plus and you have this.",
                    "label": 0
                },
                {
                    "sent": "This representation, sorry the first term coming from differentiating this guy the second term coming from differentiating this guy.",
                    "label": 0
                },
                {
                    "sent": "So there's a little bit of algebra involved, but believe me, that's that's that's the derivative.",
                    "label": 0
                },
                {
                    "sent": "Now, what happens with these variational arguments is that this term is a very bad term.",
                    "label": 0
                },
                {
                    "sent": "The reason why is because it's unbounded.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "If the probability of the correct class is very, very small.",
                    "label": 0
                },
                {
                    "sent": "Right then this log will become very large.",
                    "label": 0
                },
                {
                    "sent": "And so you if you want to do optimization that way once in awhile the variance becomes, you know you have high variance and then introduces problems.",
                    "label": 0
                },
                {
                    "sent": "So what people have done before is you replacing with term with something like a 01 term.",
                    "label": 0
                },
                {
                    "sent": "You basically saying that if you hit the right class, it's one.",
                    "label": 0
                },
                {
                    "sent": "If you hit the wrong class at 0.",
                    "label": 0
                },
                {
                    "sent": "OK, you can sample from hearing you say is that the right class?",
                    "label": 0
                },
                {
                    "sent": "Then it's one.",
                    "label": 0
                },
                {
                    "sent": "Is it wrong?",
                    "label": 0
                },
                {
                    "sent": "Class Zero and sort of relates to reinforce algorithm people have have looked at that.",
                    "label": 0
                },
                {
                    "sent": "But this becomes a little bit of problematic.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful how you optimizing these things.",
                    "label": 0
                },
                {
                    "sent": "OK. Now, in principle you can just do stochastic estimator for that and you can do the following.",
                    "label": 0
                },
                {
                    "sent": "You can say draw EM samples from the prior.",
                    "label": 1
                },
                {
                    "sent": "Once you draw EM samples from the pride and notice what happens here, this is just an expectation and this is just an expectation and then by joining samples from the prior you can just.",
                    "label": 0
                },
                {
                    "sent": "Do the Monte Carlo approximation OK?",
                    "label": 0
                },
                {
                    "sent": "So you can get someone to color approximation of that.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful how you do it and such so you can look at the papers.",
                    "label": 0
                },
                {
                    "sent": "There are a few tricks that you have to be very careful about how you do it, but in principle you can.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and how does sampling from the prior looks like?",
                    "label": 1
                },
                {
                    "sent": "Well, the sampling from the Prius actually you just run in that look forward.",
                    "label": 0
                },
                {
                    "sent": "You start with the data.",
                    "label": 0
                },
                {
                    "sent": "You know you sample the location.",
                    "label": 0
                },
                {
                    "sent": "Given that you sample the location and so forth.",
                    "label": 0
                },
                {
                    "sent": "And once you sample, these are sampled locations and then you can do Monte Carlo approximation based on that.",
                    "label": 0
                },
                {
                    "sent": "You basically running the network forward and then you saying how good is my prediction for the class.",
                    "label": 0
                },
                {
                    "sent": "If I happen to choose the right locations to look at, then the probability of the correct classes is going to be high.",
                    "label": 0
                },
                {
                    "sent": "This is great if I happen to choose sort of the wrong locations.",
                    "label": 0
                },
                {
                    "sent": "The probability of the class in the wrong class is going to be.",
                    "label": 0
                },
                {
                    "sent": "Small, and that's how you do it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the key observations in that setting is that you can actually maximize the marginals close low probabilities directly.",
                    "label": 0
                },
                {
                    "sent": "Can you do that?",
                    "label": 0
                },
                {
                    "sent": "Right without actually looking at the variational bounds.",
                    "label": 0
                },
                {
                    "sent": "And how can you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the definition of the of the marginal like log likelihood, and then you can say why don't I just differentiate this thing directly?",
                    "label": 0
                },
                {
                    "sent": "OK, and you can.",
                    "label": 0
                },
                {
                    "sent": "And you have this normalizing constant, and if you look at this expression, here is a little bit of math you can derive.",
                    "label": 0
                },
                {
                    "sent": "It's not very difficult.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the posterior.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is essentially this is where the variational argument comes in, because if you don't have access to the posterior, you can approximate it with the prior, let's say, But in this in this setting this is something that we cannot compute right if we could compute, it would be like in M type of algorithm, which we know how to how to solve.",
                    "label": 0
                },
                {
                    "sent": "But we can't.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, what we can do is we can use important sampling to estimate these expectations.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's see how we do that.",
                    "label": 1
                },
                {
                    "sent": "Let Q be some approximation to the posterior right?",
                    "label": 0
                },
                {
                    "sent": "So Q is approximately going to be approximation to the posterior and then using importance sampling we can do the following, right?",
                    "label": 0
                },
                {
                    "sent": "We can say look at this importance weight sample from the queue.",
                    "label": 0
                },
                {
                    "sent": "So we have to build some kind of inference.",
                    "label": 0
                },
                {
                    "sent": "Network is a Q network.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you how we do that.",
                    "label": 0
                },
                {
                    "sent": "You sample from Q.",
                    "label": 0
                },
                {
                    "sent": "Notice that Q depends on Y&X.",
                    "label": 1
                },
                {
                    "sent": "So it depends on the label and then you have these important weights and then you can actually get the estimates.",
                    "label": 0
                },
                {
                    "sent": "This is the classic estimate of the marginal probability.",
                    "label": 0
                },
                {
                    "sent": "So in principle, in principle you can do that.",
                    "label": 0
                },
                {
                    "sent": "Let me just go back.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the two different estimators.",
                    "label": 1
                },
                {
                    "sent": "So this is the estimator of the variational bound.",
                    "label": 1
                },
                {
                    "sent": "This is an estimated stochastic estimator of the marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "And remember this is quite bad term, right?",
                    "label": 0
                },
                {
                    "sent": "But if you look at the two estimators.",
                    "label": 0
                },
                {
                    "sent": "Look at this these terms.",
                    "label": 0
                },
                {
                    "sent": "They're basically the same.",
                    "label": 0
                },
                {
                    "sent": "And look at these terms.",
                    "label": 0
                },
                {
                    "sent": "They're basically the same.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the two estimators are kind of very similar to each other with one exception.",
                    "label": 0
                },
                {
                    "sent": "You have this bad term, and here you have these importance weights, which is effectively telling you how good your trajectory is, right?",
                    "label": 0
                },
                {
                    "sent": "Things that are good, the weights will be high.",
                    "label": 0
                },
                {
                    "sent": "Things that are bad.",
                    "label": 0
                },
                {
                    "sent": "The weights will be low and these are normalized weights, so they sum up to one.",
                    "label": 1
                },
                {
                    "sent": "Obviously, the performance here gains of the important sampling is going to be heavily reliant on the choice of the proposal distribution Q.",
                    "label": 0
                },
                {
                    "sent": "How do you choose so?",
                    "label": 0
                },
                {
                    "sent": "Imagine that I'm trying to solve the following system.",
                    "label": 0
                },
                {
                    "sent": "If I tell you I show you an image, and I tell you there is a cow in this image.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "The hope is that your proposal distribution will sell well.",
                    "label": 0
                },
                {
                    "sent": "These are the locations you should be looking at.",
                    "label": 0
                },
                {
                    "sent": "To you know, there's probably some distribution of where the cows should be in the image.",
                    "label": 0
                },
                {
                    "sent": "If I show, you know there's an image of an airplane, maybe there's certain locations where the airplane should be right instead of just sampling it from the prior you actually trying to build the inference network, you that can predict better where the true locations are given what you're trying to predict.",
                    "label": 1
                },
                {
                    "sent": "And if the approximate posterior Q is equal to the prior.",
                    "label": 1
                },
                {
                    "sent": "Then this is something that Charlie did back in 2013, and if you also looking at the Q distribution, this is also similar to the related Wake sleep algorithm of.",
                    "label": 0
                },
                {
                    "sent": "Bon, Shannon and Yoshua.",
                    "label": 0
                },
                {
                    "sent": "Which is very recent 2015.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there is another key observations that I want to point here and I promise you that's the last technical piece.",
                    "label": 0
                },
                {
                    "sent": "If you have finite number of samples.",
                    "label": 0
                },
                {
                    "sent": "Then we can show that the important sampling is basically gradient descent on this objective function.",
                    "label": 0
                },
                {
                    "sent": "Right, this is the definition of the marginal likelihood and expectation in expectation.",
                    "label": 0
                },
                {
                    "sent": "That's what we're trying to do now using Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "We can show that whatever we optimizing is actually lower bound on the marginal log probability.",
                    "label": 0
                },
                {
                    "sent": "So effectively, if you're using finite number of samples.",
                    "label": 0
                },
                {
                    "sent": "In expectation we get a lower bound and a low probability right as the number of samples goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then we actually optimizing the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "But we are looking at the lower bound, so now you can say what's the point?",
                    "label": 0
                },
                {
                    "sent": "You've already when people worked on the variational bounds, but I ever we optimizing a lower bound on the marginal likelihood, although the variance can be high.",
                    "label": 1
                },
                {
                    "sent": "And the bond becomes tighter as we increase M, so we can show that as number of samples increases the bound becomes tighter but.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting thing is that this bound is at least as accurate as the variational bound in expectation.",
                    "label": 1
                },
                {
                    "sent": "So this is the variational bound and this is what you optimizing which is smaller.",
                    "label": 0
                },
                {
                    "sent": "Which would you or we are optimizing, which is tighter than?",
                    "label": 0
                },
                {
                    "sent": "Then the variational bound right?",
                    "label": 0
                },
                {
                    "sent": "So the variational bound is looser.",
                    "label": 0
                },
                {
                    "sent": "So what this is saying that?",
                    "label": 0
                },
                {
                    "sent": "You know, as you increase the number of samples, we actually optimizing a better bound, but it's interesting.",
                    "label": 0
                },
                {
                    "sent": "Kind of connection that.",
                    "label": 0
                },
                {
                    "sent": "You know, in expectation we are looking at at the bounds.",
                    "label": 0
                },
                {
                    "sent": "Of course, you know these kinds of arguments are a little bit tougher to make because sometimes variational works better.",
                    "label": 0
                },
                {
                    "sent": "Sometimes sampling was better and the reason why is because if we actually have high variance in sampling that can hurt us.",
                    "label": 0
                },
                {
                    "sent": "So we have to control for the variance.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little bit.",
                    "label": 0
                },
                {
                    "sent": "So let's just come back and say this is what we want to optimize.",
                    "label": 0
                },
                {
                    "sent": "This is what we do and these are stochastic units.",
                    "label": 0
                },
                {
                    "sent": "But if you look at this particular formulation here, you can just think of this entire system as a neural network with some stochastic units of some deterministic units, right?",
                    "label": 0
                },
                {
                    "sent": "That's all it is.",
                    "label": 0
                },
                {
                    "sent": "You know you have this specific structure, but it's just the neural network with some stochastic units and deterministic units.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the interesting thing is that you can also think of this as a conditional Helmholtz machine.",
                    "label": 1
                },
                {
                    "sent": "Right so Jeff Hinton back in 90s.",
                    "label": 0
                },
                {
                    "sent": "Worked on Helmholtz machines and these are stochastic directed models where you have stochastic hidden variables and this is the data you're trying to model that and if you know there's been a lot of work on variational autoencoders and sigmoid belief Nets and trying to build these models.",
                    "label": 0
                },
                {
                    "sent": "Models based on wake sleep algorithm and reweighted wake sleep algorithm and such.",
                    "label": 0
                },
                {
                    "sent": "These are all algorithms student to this thing here.",
                    "label": 0
                },
                {
                    "sent": "And there is nothing different from the attention models and these these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "They are very much related to each other.",
                    "label": 0
                },
                {
                    "sent": "So in principle you can use wake sleep.",
                    "label": 1
                },
                {
                    "sent": "You can use your weighted wake sleep, which is what we do.",
                    "label": 0
                },
                {
                    "sent": "You can use variational turn, chorus and various kinds of things you can explore.",
                    "label": 0
                },
                {
                    "sent": "So that connection allows you to sort of connect.",
                    "label": 0
                },
                {
                    "sent": "Connect the two.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now what we can do is we can learn both Nets.",
                    "label": 1
                },
                {
                    "sent": "We can learn recognition model and we can learn.",
                    "label": 0
                },
                {
                    "sent": "The generative model.",
                    "label": 0
                },
                {
                    "sent": "So what happens here?",
                    "label": 0
                },
                {
                    "sent": "This is our generative network.",
                    "label": 0
                },
                {
                    "sent": "Given the input which is generating the targets.",
                    "label": 0
                },
                {
                    "sent": "And this is our inference network, right?",
                    "label": 0
                },
                {
                    "sent": "And notice that the inference networks also takes Y as its input.",
                    "label": 1
                },
                {
                    "sent": "So this is our Q network.",
                    "label": 0
                },
                {
                    "sent": "So notice that the train.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inference network you basically want to predict glimpses given the observations as well as class labels, right?",
                    "label": 1
                },
                {
                    "sent": "'cause every time you try to do inference given the data and the targets you're trying to predict, what's the best distribution of the locations I should be looking at?",
                    "label": 0
                },
                {
                    "sent": "An we can parameterise it this way.",
                    "label": 0
                },
                {
                    "sent": "There's a particular choice that we're making.",
                    "label": 0
                },
                {
                    "sent": "There's probably many other choices you can do.",
                    "label": 0
                },
                {
                    "sent": "This is just the first attempt, so this distribution is in langle and again the same as a prior except for each decision.",
                    "label": 0
                },
                {
                    "sent": "You also taking label.",
                    "label": 0
                },
                {
                    "sent": "Why into account, which is probably the simplest way to do it.",
                    "label": 0
                },
                {
                    "sent": "And then to train AQ in that way.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you can just look at the KL divergences.",
                    "label": 0
                },
                {
                    "sent": "The definition of the KL divergent's you have this particular expression and you can use important sampling with the following stochastic estimate of the gradient and that's you estimate of the gradient, which is the same as was done in wake sleep algorithm related wake sleep algorithm.",
                    "label": 1
                },
                {
                    "sent": "So in fact you can use important ways computing for the attention model, so the whole thing you know you don't have to do these extra work.",
                    "label": 1
                },
                {
                    "sent": "You can reuse the samples we actually train the queue network.",
                    "label": 1
                },
                {
                    "sent": "But again, the step that I want to take back is, you know, to tell you that in these kinds of models and attention models, I think it's important to learn the generative piece as well as the recognition piece.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what's done.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are just some examples.",
                    "label": 0
                },
                {
                    "sent": "You know, I haven't talked about.",
                    "label": 0
                },
                {
                    "sent": "There's something very important when you're looking at these models, you have to define so-called control variants, techniques to reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "So it's becomes a little bit more technical.",
                    "label": 0
                },
                {
                    "sent": "You can look at the paper.",
                    "label": 0
                },
                {
                    "sent": "It's going to be online very soon, but let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is done on on the captions data set.",
                    "label": 0
                },
                {
                    "sent": "You know this is variational approximation.",
                    "label": 0
                },
                {
                    "sent": "This is the recurrent attention model weeks Leprechaun intention models and you know we sort of get a little bit better numbers, but not much.",
                    "label": 0
                },
                {
                    "sent": "So we still have to push on the performance of these models.",
                    "label": 0
                },
                {
                    "sent": "You know, but when we look at the negative log likelihood, you know we are doing better compared to variational arguments.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's one thing that you can do, which I think is pretty interesting.",
                    "label": 0
                },
                {
                    "sent": "So this is a very toyish example what you're trying to do is you're trying to do digit recognition.",
                    "label": 0
                },
                {
                    "sent": "All right, and then you have two random variables.",
                    "label": 0
                },
                {
                    "sent": "One of them is predicting the location where you want to look, the other one is predicting scale.",
                    "label": 0
                },
                {
                    "sent": "Do you want to do?",
                    "label": 0
                },
                {
                    "sent": "You want to look at?",
                    "label": 0
                },
                {
                    "sent": "You want to choose between three different scales?",
                    "label": 1
                },
                {
                    "sent": "You're looking at the image.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that dealing with scale is very hard, so you have these two random variables, right?",
                    "label": 0
                },
                {
                    "sent": "One is 1 is softmax, the other one is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Ann, looking at the variational approximations very hard to make these systems work in the variational domain.",
                    "label": 0
                },
                {
                    "sent": "But you can make them work in the hard attention domain.",
                    "label": 0
                },
                {
                    "sent": "At least that was our experience.",
                    "label": 0
                },
                {
                    "sent": "So this is what the system does, right so?",
                    "label": 0
                },
                {
                    "sent": "So it sort of focuses on.",
                    "label": 0
                },
                {
                    "sent": "You know pieces of like look at this four right?",
                    "label": 0
                },
                {
                    "sent": "Like if I if I run it again.",
                    "label": 0
                },
                {
                    "sent": "So focuses on like middle of the five.",
                    "label": 0
                },
                {
                    "sent": "It focuses on the four words, has a little bit of across and such.",
                    "label": 0
                },
                {
                    "sent": "This is a different scale, you know, so it's sort of finds what the thing what the thing should be, and then you know if you really have a little for here, you know it sort of tends to, you know, these are just examples, but it does for the caption generation we're trying to do the system where.",
                    "label": 0
                },
                {
                    "sent": "You can attend to different layers in the convolutional model, right?",
                    "label": 0
                },
                {
                    "sent": "So if you intend to the high level, you seeing a little bit you have, you look at the bigger part of the image.",
                    "label": 0
                },
                {
                    "sent": "If you're attending to different levels in the convolutional model, you can actually get better results.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is the attention, I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's an exciting area, is a lot of people trying to look at this thing and the hope is that potentially maybe these systems can take over convolutions and be able to sort of attend two different pieces and maybe replace convolutional neural Nets.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now let me just finally point out that you know if you look at this entire systems here.",
                    "label": 0
                },
                {
                    "sent": "Instead of sampling, you can take expectations right?",
                    "label": 1
                },
                {
                    "sent": "And this is work that was done in University of Montreal, in particular for machine translation was very successful, right where you can take the expectations and I think Alex also did this work back in 2003.",
                    "label": 0
                },
                {
                    "sent": "So some version of it.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the difference?",
                    "label": 0
                },
                {
                    "sent": "Well, the soft attention models they are computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "The reason why is because you have to examine every single image location.",
                    "label": 1
                },
                {
                    "sent": "Now you can try to use some tricks to cut.",
                    "label": 0
                },
                {
                    "sent": "Cut that computation down, but in principle you have to.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about these soft models is that they are deterministic.",
                    "label": 1
                },
                {
                    "sent": "You never need to sample the locations.",
                    "label": 0
                },
                {
                    "sent": "You basically taking expectations, so you smoothing it.",
                    "label": 0
                },
                {
                    "sent": "The heart attention models are believe a computational much more efficient because you just choosing particular thing to look at and you just processing that one piece.",
                    "label": 0
                },
                {
                    "sent": "But there stochastic so they require some form of sampling because they have to make discrete choices.",
                    "label": 1
                },
                {
                    "sent": "So you know research is basically taking in both ends.",
                    "label": 0
                },
                {
                    "sent": "I know that people are now looking trying to basically look at translation models and trying to look at hard attention models to see where they can use them instead of soft attention.",
                    "label": 0
                },
                {
                    "sent": "But again, the jury is still out.",
                    "label": 0
                },
                {
                    "sent": "The research takes place in both directions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Could be there is a lot of research that could be done in that area.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me finish off by.",
                    "label": 0
                },
                {
                    "sent": "You know I've talked about caption Generation 0 short learning, trying to look at attention models.",
                    "label": 0
                },
                {
                    "sent": "You know, we've seen that there's been a lot of successes using attention model soft attention models for caption Generation.",
                    "label": 0
                },
                {
                    "sent": "But let me talk about finally on about the model on trying to one sentence representations right?",
                    "label": 0
                },
                {
                    "sent": "Because these are very important, very important problem.",
                    "label": 0
                },
                {
                    "sent": "Wherever you're going to use them for generating sentences or.",
                    "label": 0
                },
                {
                    "sent": "Captions and such.",
                    "label": 0
                },
                {
                    "sent": "So this is learning skip thought vectors.",
                    "label": 0
                },
                {
                    "sent": "This is by Ryan Curious and Bunch of other people and yukun, but credit goes to Ryan 'cause he really.",
                    "label": 0
                },
                {
                    "sent": "Pull it together.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So sequence to sequence modeling.",
                    "label": 1
                },
                {
                    "sent": "There's been a lot of work on sequence to sequence modeling, right?",
                    "label": 1
                },
                {
                    "sent": "You have some input sequence.",
                    "label": 0
                },
                {
                    "sent": "You encode it using RN ends recurrent Nets.",
                    "label": 0
                },
                {
                    "sent": "And then maybe you decode it.",
                    "label": 0
                },
                {
                    "sent": "Right, so maybe you want to translate English to French.",
                    "label": 0
                },
                {
                    "sent": "And there's been a lot of work done in that space, and you can think of this as your representation, right?",
                    "label": 0
                },
                {
                    "sent": "You can apply to video sequences.",
                    "label": 0
                },
                {
                    "sent": "You can apply it to language.",
                    "label": 0
                },
                {
                    "sent": "Kind of nice nice formulation.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you can do is you can do that.",
                    "label": 0
                },
                {
                    "sent": "You can use the following very simple idea, extremely simple idea.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have three sentences, a couple of sentences, continuous sentences.",
                    "label": 0
                },
                {
                    "sent": "Now you using this middle sentence and you encode it using LTM.",
                    "label": 0
                },
                {
                    "sent": "It's a recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "And then you use this sentence to reconstruct the previous sentence and the next sentence you trying to reconstruct the previous and the next.",
                    "label": 0
                },
                {
                    "sent": "So for example, the input is the triplet I got back home.",
                    "label": 1
                },
                {
                    "sent": "I could see the cat on the steps.",
                    "label": 1
                },
                {
                    "sent": "This was strange.",
                    "label": 0
                },
                {
                    "sent": "So encoding the middle sentence and you try to predict the context around it.",
                    "label": 0
                },
                {
                    "sent": "Trying to predict this sentence in that sentence.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so what's what's the idea here?",
                    "label": 0
                },
                {
                    "sent": "Is that again?",
                    "label": 0
                },
                {
                    "sent": "You encoding a sentence using recurrent net?",
                    "label": 0
                },
                {
                    "sent": "And then you are decoding it back the previous sentence and you're decoding the 4th sentence OK.",
                    "label": 1
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the objective function?",
                    "label": 0
                },
                {
                    "sent": "The sum?",
                    "label": 0
                },
                {
                    "sent": "The objective function is basically the sum of the log probability of the next and the previous sentence conditioned on the representation of the encoder.",
                    "label": 1
                },
                {
                    "sent": "Right and you can compute these these probabilities.",
                    "label": 0
                },
                {
                    "sent": "So this is just goes over the forward sentence.",
                    "label": 0
                },
                {
                    "sent": "These are all the words in the forward sentence.",
                    "label": 0
                },
                {
                    "sent": "These are all the words in the previous sentence and.",
                    "label": 0
                },
                {
                    "sent": "And that's basically the objective function.",
                    "label": 0
                },
                {
                    "sent": "The entire thing is differentiable, so you can just do stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, and that's the representation of the encoder.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you use this thing on a very large data set so this is books 11K data set, you have about 11,000 books.",
                    "label": 0
                },
                {
                    "sent": "You have about 1,000,000 words and about.",
                    "label": 0
                },
                {
                    "sent": "Not a million words about a billion words, about million unique words.",
                    "label": 0
                },
                {
                    "sent": "And so here's here's, let me show you some examples.",
                    "label": 0
                },
                {
                    "sent": "So you have a query sentence along with its nearest neighbors from 500 sentences.",
                    "label": 1
                },
                {
                    "sent": "Using just cosine similarity and learn representations.",
                    "label": 0
                },
                {
                    "sent": "So here's the sentence.",
                    "label": 0
                },
                {
                    "sent": "He ran his hand inside his code, double checking that the unopened letter was still there.",
                    "label": 1
                },
                {
                    "sent": "It's pretty complex sentence, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the nearest sentence.",
                    "label": 0
                },
                {
                    "sent": "He slipped his hand between his code in his shirt, where the folded copies laying Brown envelope.",
                    "label": 1
                },
                {
                    "sent": "You know, kind of similar.",
                    "label": 0
                },
                {
                    "sent": "This is some other examples.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, so she said aggressively, so you know if you look through these examples, in many cases sort of finds these interesting similar similar sentences.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let me show you some some examples of how well is it works, so this was a semantic evaluation task.",
                    "label": 0
                },
                {
                    "sent": "You basically looking at the semantic relatedness between two sentences.",
                    "label": 0
                },
                {
                    "sent": "So I have two sentences and you have to produce a score.",
                    "label": 1
                },
                {
                    "sent": "How semantically similar these sentences are based on scores one to five and the data set counts with a predefined split of training and tests and such.",
                    "label": 1
                },
                {
                    "sent": "And then you can use these representation of these vectors and then what?",
                    "label": 0
                },
                {
                    "sent": "But you can essentially do is just building a simple linear regression model to predict the semantic relatedness.",
                    "label": 1
                },
                {
                    "sent": "Right, so getting the representations from these two sentences, you know.",
                    "label": 0
                },
                {
                    "sent": "Putting componentwise features between pairs something very simple, like looking at the difference between the two and based on that you just building a simple linear regression model.",
                    "label": 0
                },
                {
                    "sent": "Now for a lot of these tasks we did not back propagate through the entire system, because the goal here was to try to see how good these embeddings are.",
                    "label": 0
                },
                {
                    "sent": "Right across a variety of different tasks, so we just want to say, given these representations, how good they are for all of these tasks.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and this is what the system does.",
                    "label": 1
                },
                {
                    "sent": "Just wanted to point out here is that these are evaluations coming in 2014.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is mean squared error versus also doing some correlations.",
                    "label": 1
                },
                {
                    "sent": "These are all results reported in the paper by title.",
                    "label": 0
                },
                {
                    "sent": "Guess that's coming from Stanford and this is ours.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So there are various different combinations.",
                    "label": 0
                },
                {
                    "sent": "I think that Ryan was using the models where you using sort of bidirectional, so these are called by skipping is using bidirectional STM.",
                    "label": 0
                },
                {
                    "sent": "You combining units keep with bioscape manure, folding in the Coco data set so that is sort of like slight different extensions, but what's interesting is that if you look at the dependency tree LTM model here, you pretty much on par, just slightly below the state of the art, but you are on par.",
                    "label": 0
                },
                {
                    "sent": "But notice we're not doing anything complicated here.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's interesting, but this is fun to look at.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where you look at the two sentences and you say how semantically similar they are.",
                    "label": 0
                },
                {
                    "sent": "So for example, here a girl is looking at the woman costume an A young girl is looking at women in costume right?",
                    "label": 0
                },
                {
                    "sent": "The ground truth is 4.7.",
                    "label": 1
                },
                {
                    "sent": "And the predicted is 4.5.",
                    "label": 0
                },
                {
                    "sent": "So the reason why it's 4.7 is because I think that they ask 10 people or so like some number of people to write these things.",
                    "label": 0
                },
                {
                    "sent": "So and then they take the average.",
                    "label": 0
                },
                {
                    "sent": "So a man is driving a car.",
                    "label": 0
                },
                {
                    "sent": "The car is being driven by a man.",
                    "label": 0
                },
                {
                    "sent": "Write 54.9.",
                    "label": 0
                },
                {
                    "sent": "Here's some failure.",
                    "label": 0
                },
                {
                    "sent": "Cases at person is performing acrobatics on the motorcycle.",
                    "label": 0
                },
                {
                    "sent": "Person is performing tricks on a motorcycle.",
                    "label": 0
                },
                {
                    "sent": "You know the ground truth is 4.3.",
                    "label": 0
                },
                {
                    "sent": "They are related.",
                    "label": 0
                },
                {
                    "sent": "Model predicts 4.4.",
                    "label": 0
                },
                {
                    "sent": "Here's a failure case.",
                    "label": 0
                },
                {
                    "sent": "A person is performing tricks on a motorcycle.",
                    "label": 0
                },
                {
                    "sent": "The performer is tricking a personal motorcycle.",
                    "label": 0
                },
                {
                    "sent": "Now the ground truth is 2.6, right?",
                    "label": 0
                },
                {
                    "sent": "But the model gets it is 4.4, so there is some room for improvement or this one.",
                    "label": 0
                },
                {
                    "sent": "Someone is pouring ingredients into report.",
                    "label": 0
                },
                {
                    "sent": "A man is removing vegetables from a pot.",
                    "label": 0
                },
                {
                    "sent": "Or nobody is pouring ingredients into the pot.",
                    "label": 0
                },
                {
                    "sent": "Someone is pouring rain into the pot, so you know you get you get sort of consistency there, right?",
                    "label": 0
                },
                {
                    "sent": "So obviously you know there are some cases where these systems can fail, but on average they're doing.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, really well then you can also look at the paraphrase detection.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is the Microsoft paraphrase corpus.",
                    "label": 0
                },
                {
                    "sent": "Again, for two sentences you have to predict whether or not they are paraphrases of each other, and these results.",
                    "label": 1
                },
                {
                    "sent": "These are all recursive autoencoders.",
                    "label": 1
                },
                {
                    "sent": "This is the best published results.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work being done in a natural language and LP communities, and this is our model, so you can see you know these numbers are better than ours numbers, but.",
                    "label": 0
                },
                {
                    "sent": "You know one thing I do want to emphasize is that when we're not tuning these sentence representations for a particular task, you just taking these representations and then you see how well do they work.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like trying to build a sentence two VEC representation compatible to work.",
                    "label": 0
                },
                {
                    "sent": "The vector presentation right?",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another example.",
                    "label": 0
                },
                {
                    "sent": "You know you have five different datasets.",
                    "label": 0
                },
                {
                    "sent": "Trying to do a lot of different things.",
                    "label": 0
                },
                {
                    "sent": "These are bag of words representations.",
                    "label": 0
                },
                {
                    "sent": "These are supervised models and this is ours and then you can see that.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "These particular kind of models they're doing a little bit better than than what we're doing, but we sort of up there.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I'll come back to that at the very end.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "I'll try to answer that question at the very end.",
                    "label": 0
                },
                {
                    "sent": "I have an answer for that.",
                    "label": 0
                },
                {
                    "sent": "So basically the short answer is that this is something that we've started exploring, particularly something that Ryan started exploring, and there's a lot of more things that you can do, and I'll come back to that.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, this is summer.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm coming back to that question so.",
                    "label": 0
                },
                {
                    "sent": "So this particular model, it only looks at, you know, only scratches.",
                    "label": 0
                },
                {
                    "sent": "The surface of all possible things you can do, right.",
                    "label": 1
                },
                {
                    "sent": "So for example, a lot of different possibilities can be explored, including what you are suggesting.",
                    "label": 0
                },
                {
                    "sent": "Do you really need this huge representation?",
                    "label": 0
                },
                {
                    "sent": "Maybe you can go with a smaller one and achieve similar results.",
                    "label": 0
                },
                {
                    "sent": "Deep auto encoders and decoders right now.",
                    "label": 1
                },
                {
                    "sent": "You just I think Ryan is just using the 1st, just a layer less than with a single layer.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can improve upon that.",
                    "label": 1
                },
                {
                    "sent": "You can look at the larger context windows.",
                    "label": 0
                },
                {
                    "sent": "We just predicting the previous on the next sentence.",
                    "label": 0
                },
                {
                    "sent": "You can try to predict things around looking at the context, encoding, decoding paragraphs.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to decode the entire paragraph as opposed just the sentences that or looking at other other encoders.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I think that there is a lot of exploration we haven't.",
                    "label": 0
                },
                {
                    "sent": "You know to be honest.",
                    "label": 0
                },
                {
                    "sent": "We haven't done exhaustive search of what's the best.",
                    "label": 0
                },
                {
                    "sent": "Right, and I think about these models.",
                    "label": 0
                },
                {
                    "sent": "Is that and Ryan can correct me if I'm wrong.",
                    "label": 0
                },
                {
                    "sent": "Is going through this entire I think.",
                    "label": 0
                },
                {
                    "sent": "Going through the entire data set takes like a day or so or even longer than that right on the GPU.",
                    "label": 0
                },
                {
                    "sent": "It takes 2 weeks.",
                    "label": 0
                },
                {
                    "sent": "There you go, so just going through a single Passover, the data takes about 2 weeks.",
                    "label": 0
                },
                {
                    "sent": "It's it's.",
                    "label": 0
                },
                {
                    "sent": "It's a very big corpus, and so again a lot of things.",
                    "label": 0
                },
                {
                    "sent": "Can be explored.",
                    "label": 0
                },
                {
                    "sent": "And also I should point out that the code and data are available online.",
                    "label": 1
                },
                {
                    "sent": "So if you want to crank take a crack at it.",
                    "label": 0
                },
                {
                    "sent": "The data is online through 11,000.",
                    "label": 0
                },
                {
                    "sent": "Books are online and there is also code the encoding of sentences is also in line.",
                    "label": 0
                },
                {
                    "sent": "So if you working with sentences and you want to get the vector representation of the sentences and see how good they are and test how good they are, the code is also online.",
                    "label": 0
                },
                {
                    "sent": "I think it's on GitHub.",
                    "label": 0
                },
                {
                    "sent": "You can just use it as a word to vector this sentence to back and see see see how good these representations are.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On that point, I should probably stop and thank you.",
                    "label": 0
                }
            ]
        }
    }
}