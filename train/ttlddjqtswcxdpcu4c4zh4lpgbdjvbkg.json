{
    "id": "ttlddjqtswcxdpcu4c4zh4lpgbdjvbkg",
    "title": "Robust Textual Inference Using Diverse Knowledge Sources",
    "info": {
        "author": [
            "Rajat Raina, Computer Science Department, Stanford University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "April 2005",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/pcw05_raina_rtiud/",
    "segmentation": [
        [
            "OK, I would like to.",
            "Welcome their jobs from Stanford.",
            "Presenting which is quite sophisticated, another kind of component, so it might be interesting through the following Taylor.",
            "So I present the system which is work done with all these people and Chris Cox is settings like the rest of the company."
        ],
        [
            "The basic approach of ours is similar to what we've seen from many people.",
            "That we first take the sentence, we find the syntactic dependencies, and we use this as our base representation.",
            "To which will add some semantic annotations.",
            "An idea is that some kind of language variability will try to model within the representation itself.",
            "For example, actives and passives will try to collapse with the same representation.",
            "Then given this representation, we want to perform semantic inference using.",
            "All kinds of annotations and different sources of knowledge that we might use.",
            "And both the infants approaches and I'll describe their work in the same way.",
            "Same end goal that that is the producer cost for inferring the hypothesis from the text.",
            "And the idea being that a low cost implies that the hypothesis should be entailed.",
            "So."
        ],
        [
            "The rest of the talk I'll first show that.",
            "How we take the input sentence and represent it?",
            "Then I then I'll point out the two different infinite inference methods that we use.",
            "The first one does graph matching over dependency trees, which is somewhat similar to what has been described before, but.",
            "I think the particular way in which we oppose it is slightly different.",
            "And the second method will build on the some of the logical inference we've seen and.",
            "I think the reason it's different from previous presentations is that instead of using logical axioms, we actually use abductive axioms.",
            "So for each axiom you have to pay a cost and then a proof always has a cost associated with it.",
            "Instead of always having a 0 cost, it can have any arbitrary cost.",
            "Finally, I'll present a combined system and show some results."
        ],
        [
            "So I start with the representation part.",
            "We start out with the English sentence before it's with the standard we CFG parser, we just use a small number of tricks to deal with dirty data set, because typically these parsers are trained on data which is 10 or 15 years old.",
            "So just some regular expressions and some extra sentences from recent news, because the data set seems to be derived from that.",
            "Then, to augment this basic parse tree, what we did was we wanted to use the named entity Recognition system to identify that.",
            "Certain.",
            "Sequences of strings might be names of names of persons or organisations or something like that.",
            "But to make our final representation consistent, what we do is that.",
            "We first performed limited recognition.",
            "And then for some of the tags, namely person, location and organization, we first the parser to name them to tag them as proper nouns.",
            "This is just to ensure that after this is all done, it is.",
            "It won't be the case that the parse tree will say something else and the named entities will say something else.",
            "So for this example.",
            "American Ministry of Foreign Affairs announced that Russia so and so, so if the named entity recognition System recognizes that American Ministry of Foreign Affairs is a. Organisations say.",
            "Then it will collapse this into one one token and it will force the parser to.",
            "Label it as a proper noun."
        ],
        [
            "Just some words about the named entity recognizer.",
            "Used some some kind of conditional random field model which was implemented by us and.",
            "Another trick we added was to deal with examples like this where the text says conducted 6643 full face to face interviews and the hypothesis says more than 60,000 adults.",
            "So you need to know that 60 this number can be implied by more than 60,000, but not the other way around, for example an.",
            "There are several tricks in longer.",
            "So what we do is that whenever something is marked as a number by the named entity recognition system, we look for regular expressions of this form.",
            "So this might this might map to 6.2 billion.",
            "This might map to.",
            "Arrange more than 60,000.",
            "This might map to approximate number.",
            "So we recognize these kinds of regular expressions and also we normalize the other name entities, money, date, percent and time into a Canonical Canonical value.",
            "So you can reason with them."
        ],
        [
            "Also, given this parse tree, we do some kind of post processing on it.",
            "One particular kind is that we recognize collocations.",
            "Using word net.",
            "For example, if you look at this sentence trick to rank up $92,000,000 that in some particular sense Rang up, we want to deal it and deal with as one.",
            "As one component instead of being ranking up because Rang up has a particular meaning which does not.",
            "Match with the meanings of rank order.",
            "So we collapse.",
            "If we can collapse them.",
            "And in this particular example, I also show how money values might you might be represented if 92 million is marked as a money entity, then you might store Canonical value for that."
        ],
        [
            "So now we have a parse tree and as I promised, we try to find the dependencies in this parse tree.",
            "And we find dependencies from the constituency parts rather than using Mini Park as many other people have used.",
            "One of the things we try to do is we try to collapse several dependencies and to try to find.",
            "The types of dependencies more accurately, so for example, from this case, bills mother walked to the grocery store.",
            "This is the kind of representation that you might get.",
            "You might say that worked as a subject to mother and possessive.",
            "Possessive dependency between mother and Bill.",
            "There is a two dependency from walk to store and then there's a noun, dependencies and so on.",
            "So you can imagine that this can be converted into a graph representation, which also in the next slide, in which each of these words will be a node and the dependencies between them denotes the type of the relation between them.",
            "And the other way to write this is that you can write the same, almost the same thing in a logical formula.",
            "So you can have.",
            "The way to read this logical formula is that A is a mother bees bill.",
            "There is a possessive relation between BNA sees the grocery store and walked.",
            "A walk to see an Easter event of the working and I removed some existential quantifiers just to be."
        ],
        [
            "So let's look at these representations in more detail.",
            "So I I drew out this simple sentence is a graph and I wrote out the logical formula and the thing to add now is that.",
            "We can make the representation much richer than just these because we know things like worked as a verb.",
            "Will is a person.",
            "We might know that and then store is the destination of the world.",
            "So to capture these things, we might annotate these representations in different ways.",
            "I'll just show some of these annotations.",
            "For example, you might annotate in the dependency graph representation you might annotate.",
            "That the node walked is a past tense word.",
            "That's the part of speech tag.",
            "You might annotate.",
            "That bill is a person.",
            "This node similarly, you might also add annotations to edges.",
            "For example, you might say that this edge.",
            "Is a location modified?",
            "This is the way you write a location modify and similarly in the logical form representation.",
            "Also you can add these annotations so you can save work.",
            "The predicate worked is past tense word.",
            "The predicate build is a person and corresponding to the annotations on the edges you get annotations on the arguments here, but this argument sees the location modifier."
        ],
        [
            "So I want to describe some of the annotations we used because we found that these linguistic features actually turn out to be more important than.",
            "Many of the other things we do.",
            "So I already showed that we compute parts of speech and named entities.",
            "We also compute semantic rules, so here's an example which shows that semantic roles are necessary in some way to for the RT problem.",
            "So this the text says that C&D Technologies announced.",
            "Disclose the acquisition of data and then there are actually two hypothesis which says CMD acquired detail or detail acquired CMD and to differentiate between these two.",
            "We think that it's necessary that you consider the semantic role of this required work.",
            "So we label all the arguments to avert using the semantic role labeler."
        ],
        [
            "History.",
            "Another annotation we might add is coreference, so here's an example.",
            "Since its permission in 1948 Israel.",
            "Blah blah blah.",
            "And then you need to know that it's related to Israel.",
            "To infer that the hypothesis is.",
            "Some kind of correct conditional random field model again for coreference detection.",
            "And each of the annotations each of the representations can be annotated with these extra information.",
            "And I also want to point out that the positive references which were.",
            "Which seems to be pretty useful on this task, but rejected, but by a different mechanism.",
            "It because they could be detected just from the parse tree heuristically.",
            "So for example.",
            "Influences like this can be bad.",
            "And there were some other annotations which are not that interesting.",
            "For example, you can add word stems, you can advert senses, but many of these were not useful for us in the end."
        ],
        [
            "So there was this huge Stickley used to find investments which is also.",
            "Which also shows up in Nominalizations as it was discussed earlier.",
            "For example, if you have.",
            "This next sentence witnessed the murder of police commander.",
            "And then you have to prove that police officer killed.",
            "So because of the particular way that the inference mechanisms work, they don't know that murder.",
            "Represents an event of murdering in which the person that was murdered was the police commander.",
            "Because if you look at the logical formula, for example.",
            "Then it looks like this.",
            "So there is a murderer and then police commander and there is no link between them through event work.",
            "It just shows that the model and the way we augment the representation is.",
            "That the.",
            "For all the nouns in this text logical formula we find.",
            "Verb forms using word net television links, and will augment this representation with a term like this which says that.",
            "Murder isn't event.",
            "the M is the murder and he's the police commander which it finds.",
            "So this is just."
        ],
        [
            "So at this stage we have come to the.",
            "Finish the representation part of this talk and I'll now describe each of the inference algorithms 1 by 1."
        ],
        [
            "1st, I'll start with the graph matching approach, which works with the syntactic dependencies.",
            "So let's skip some of the motivation, because we've already seen this quite a bit of detail, so here's a toy example.",
            "I'll use that John Border deal BMW and it's supposed to entail.",
            "John purchased the car, so here are some.",
            "Dependency graphs for these.",
            "And each of the vertices will be words or phrases, phrases because they thought they might be collapsed collocations or named entities.",
            "And it is a label dependencies.",
            "The neighbors might be semantic roles as they are shown here, or they might be subject object.",
            "And our goal is that we want to produce a cost of matching edge to T. Because I said our systems work like that."
        ],
        [
            "That's the idea that we want to align vertices of edge to vertices of T. So just to define, we call a matching a mapping of vertices from HTT.",
            "So for every vertex and edge you should map to a vertex in T. And this these set of links correspond to a matching.",
            "And the overall idea would be that we want to find.",
            "A matching and say that the hypothesis is entailed if and only if we can find a matching that seems to preserve the relations between the vertices and edges.",
            "So just to make it more formal."
        ],
        [
            "Will define the cost of a matching.",
            "To measure the quality of a matching in terms of two components, we have a vertex cost, which measures how correspondingly similar.",
            "The vertices are and we have a relation costs which will measure how correspondingly similar relations are.",
            "So for example.",
            "In this case.",
            "The vertex cost might look at how similar is joined to Jaunpur chased aboard car to BMW.",
            "And the relation cost might look at.",
            "So you map purchased a botton car to BMW.",
            "So you are mapping this relation to this solution.",
            "How similar are these relations?",
            "And we'll Add all these contributions from the vertices and the relations in a.",
            "Innovative way so the final cost will say is the vertex cost plus the relation cost.",
            "Yeah, and I should also mention that the overall idea then would be that given this model and given a text in hypothesis fair, the final goal is to find a matching which minimizes this match cost.",
            "And if that match cost has a low cost, then you say that the hypothesis isn't in otherwise not.",
            "So I'll describe some of the details now of the vertex cost in relation cost.",
            "How you could put in features there."
        ],
        [
            "So for the vertex cost, what we're looking at is that we have a vertex in edge and a vertex it is mapped to in T. Then the questions you could ask her, do these vertex vertices have the same stem or the same part of speech?",
            "Are they hypernyms of each other?",
            "Are they similar phrases?",
            "And the way the cost is built up is that for each of these there is a corresponding cost, and these costs are added up throughout to give the overall vertex cost.",
            "I want to take a digression here to discuss this thing about phrase similarity, because it seems to be very similar for.",
            "Very important for this.",
            "So the."
        ],
        [
            "Several kinds of phrase similarity, the first one being just measures based on word net, which we've seen.",
            "Then we use measures based on distributional similarity becausw.",
            "Take this example of running marathon.",
            "There they seem to be related, but they don't occur in any.",
            "Very similarly inverted.",
            "They're not part of the is Iraqi.",
            "And so we use latent semantic analysis to discover words that are similar, and we also used some.",
            "We tried some websites business measures and we tried some.",
            "Some measures based on learning paraphrases, which is similar to previous work but didn't don't help, so we didn't use them in the final system.",
            "And the last thing was also that we found that cases like the system was supposed to another CEO response to Chief Executive Officer.",
            "It might be supposed to move in.",
            "Philippines is the country in Filipino is a person.",
            "So we actually found.",
            "Fill in the forms.",
            "And we can also add common facts that we didn't do that."
        ],
        [
            "So returning back I just described the vertex plus 10 for the relation cost.",
            "If we do something similar, you could say that if there is an edge really prime in edge just now to some agenti.",
            "Then our parents are relations in edge.",
            "Indeed.",
            "So."
        ],
        [
            "Match this match is found to be the minimum cost matching this exam.",
            "But this vertex match there is an exact match does not cost for this vertex match.",
            "It discovers these are synonyms, so there's some conspiracy knowledge.",
            "So the vertex cost turns out to be original, please just .2.",
            "Duration cost is 0 because all the edges are preserved.",
            "The cost is lower."
        ],
        [
            "So I finished talking about the graph matching.",
            "I'm joining."
        ],
        [
            "So the idea of the inductive inferences that we represent extra hypothesis as a logical family and the basic idea is.",
            "That I believe this can be inferred from the text if and only the logical formula can be proved from the text.",
            "So we saw something similar about this before.",
            "Text, John.",
            "And what we wanted to do was to prove this is given.",
            "You need to know that a BMW is a car.",
            "We need to know that buying something in game purchasing.",
            "It does not seem to be including Axiom Sports.",
            "At the same time we start.",
            "Possible words to use your.",
            "Possibility to also check.",
            "Assumptions in the group assumptions correspond to axioms in the normal setting, but we also have a cost associated.",
            "Zoom.",
            "What is a purchase only?",
            "The task that remains to find.",
            "Search costs for assumptions like this and then given such a cost model to find the minimum cost to fund hypothesis.",
            "So we have all the assumption cost that you use and then you say that OK, the minimum cost is 3 for this because this assumption in this assumption and then based on that person."
        ],
        [
            "Just trying to say.",
            "This is a logical term piece of predicate.",
            "Who is a predicate?",
            "If you want to know the arguments and we want to say.",
            "What is the cost for assuming that this is like this?",
            "So the way to do it is that we allow predicates to match and then allow arguments to match it.",
            "The reason we relax the ordering of the argument system.",
            "Some arguments might correspond multiple arguments diagnostics.",
            "It might be that.",
            "I'm not.",
            "And then once you assign such a matching between the predicate and the arguments, we add them and we had the costs associated with each.",
            "So we have a match for possible matching predicates.",
            "Look at other features.",
            "Location, location.",
            "There is something else going on Cincinnati cost which.",
            "Construct logical.",
            "Each other."
        ],
        [
            "Because I'm supervising potential group Staten."
        ],
        [
            "One last thing is that I just write this way of.",
            "Assumption costs based on these individual assumption costs, right?",
            "Of course, for free similarities in their ability.",
            "So it turns out that we can frame a learning algorithm which are automatically learns fast for these individual assumptions based on.",
            "In addition, is just that.",
            "Examples and lower costs.",
            "Describe some other."
        ],
        [
            "Another thing to do is.",
            "John Dixon charged.",
            "It does not match."
        ],
        [
            "System."
        ],
        [
            "Cost to score.",
            "Systems classified in one submission."
        ],
        [
            "There are other jobs.",
            "So at this time."
        ],
        [
            "Like us it seems to be doing well."
        ],
        [
            "There are some results based on confidence, So what I did was I looked at.",
            "50%.",
            "Only only on the top 50% confident examples.",
            "If you only need to stop them from 50% if you go to one of the curves and you find accuracy there.",
            "So this means that if I were to do abortion submission with only 50% of the bells.",
            "Coverage.",
            "Interesting things out there.",
            "This curve is the better it is.",
            "30% coverage then you can get elected.",
            "And you can also draw similar coverage and confidence with school clubs.",
            "So you say that you find cover only 50% of the examples.",
            "What complaints?"
        ],
        [
            "Here are some interesting issues.",
            "Saying that something is of any of the Ryder Cup does not always mean is the host.",
            "For example, you might say London is the running of the limits.",
            "Responsibility for restoration."
        ],
        [
            "I think this one is interesting, but."
        ],
        [
            "Question.",
            "I was.",
            "Suppose I say that for the top 30% examples and classify and for the other side, just in case it turns out that system does better than ours, around 3% very close to the best.",
            "So we have some logic.",
            "On this stage.",
            "Using an assumption based model for instance.",
            "This song.",
            "Automatically generated using this.",
            "So it's like a search procedure that you're starting out with.",
            "Using resolution definition you have you starting out with this kind of.",
            "Individual.",
            "And then you want to find the minimum cost path to the number.",
            "So there are some cases in which you might have.",
            "For example, if you have good evidence, then you might have a link between.",
            "So I didn't mention that, but finding the minimum matching cost is not known to be attractive.",
            "Because the minimizing relation costs that I mentioned this in some class which is not known.",
            "So what we do actually is that we minimize the vertex costs because that is tractable, and then we do a local search around that process.",
            "I didn't mention it.",
            "Versus low carb.",
            "So do you have any?",
            "Insights about comparing the two, which ones?",
            "Is it really necessary value?",
            "Something to move into large representation?",
            "Turns out that both of the systems perform about equally India, but we haven't done internal system.",
            "Which one does better?",
            "So I'm easier than the other.",
            "It might be that the graph matching."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I would like to.",
                    "label": 0
                },
                {
                    "sent": "Welcome their jobs from Stanford.",
                    "label": 0
                },
                {
                    "sent": "Presenting which is quite sophisticated, another kind of component, so it might be interesting through the following Taylor.",
                    "label": 0
                },
                {
                    "sent": "So I present the system which is work done with all these people and Chris Cox is settings like the rest of the company.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic approach of ours is similar to what we've seen from many people.",
                    "label": 0
                },
                {
                    "sent": "That we first take the sentence, we find the syntactic dependencies, and we use this as our base representation.",
                    "label": 0
                },
                {
                    "sent": "To which will add some semantic annotations.",
                    "label": 1
                },
                {
                    "sent": "An idea is that some kind of language variability will try to model within the representation itself.",
                    "label": 1
                },
                {
                    "sent": "For example, actives and passives will try to collapse with the same representation.",
                    "label": 0
                },
                {
                    "sent": "Then given this representation, we want to perform semantic inference using.",
                    "label": 1
                },
                {
                    "sent": "All kinds of annotations and different sources of knowledge that we might use.",
                    "label": 0
                },
                {
                    "sent": "And both the infants approaches and I'll describe their work in the same way.",
                    "label": 1
                },
                {
                    "sent": "Same end goal that that is the producer cost for inferring the hypothesis from the text.",
                    "label": 0
                },
                {
                    "sent": "And the idea being that a low cost implies that the hypothesis should be entailed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rest of the talk I'll first show that.",
                    "label": 0
                },
                {
                    "sent": "How we take the input sentence and represent it?",
                    "label": 0
                },
                {
                    "sent": "Then I then I'll point out the two different infinite inference methods that we use.",
                    "label": 0
                },
                {
                    "sent": "The first one does graph matching over dependency trees, which is somewhat similar to what has been described before, but.",
                    "label": 0
                },
                {
                    "sent": "I think the particular way in which we oppose it is slightly different.",
                    "label": 0
                },
                {
                    "sent": "And the second method will build on the some of the logical inference we've seen and.",
                    "label": 1
                },
                {
                    "sent": "I think the reason it's different from previous presentations is that instead of using logical axioms, we actually use abductive axioms.",
                    "label": 0
                },
                {
                    "sent": "So for each axiom you have to pay a cost and then a proof always has a cost associated with it.",
                    "label": 0
                },
                {
                    "sent": "Instead of always having a 0 cost, it can have any arbitrary cost.",
                    "label": 0
                },
                {
                    "sent": "Finally, I'll present a combined system and show some results.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I start with the representation part.",
                    "label": 0
                },
                {
                    "sent": "We start out with the English sentence before it's with the standard we CFG parser, we just use a small number of tricks to deal with dirty data set, because typically these parsers are trained on data which is 10 or 15 years old.",
                    "label": 0
                },
                {
                    "sent": "So just some regular expressions and some extra sentences from recent news, because the data set seems to be derived from that.",
                    "label": 0
                },
                {
                    "sent": "Then, to augment this basic parse tree, what we did was we wanted to use the named entity Recognition system to identify that.",
                    "label": 0
                },
                {
                    "sent": "Certain.",
                    "label": 0
                },
                {
                    "sent": "Sequences of strings might be names of names of persons or organisations or something like that.",
                    "label": 0
                },
                {
                    "sent": "But to make our final representation consistent, what we do is that.",
                    "label": 0
                },
                {
                    "sent": "We first performed limited recognition.",
                    "label": 0
                },
                {
                    "sent": "And then for some of the tags, namely person, location and organization, we first the parser to name them to tag them as proper nouns.",
                    "label": 0
                },
                {
                    "sent": "This is just to ensure that after this is all done, it is.",
                    "label": 0
                },
                {
                    "sent": "It won't be the case that the parse tree will say something else and the named entities will say something else.",
                    "label": 0
                },
                {
                    "sent": "So for this example.",
                    "label": 0
                },
                {
                    "sent": "American Ministry of Foreign Affairs announced that Russia so and so, so if the named entity recognition System recognizes that American Ministry of Foreign Affairs is a. Organisations say.",
                    "label": 1
                },
                {
                    "sent": "Then it will collapse this into one one token and it will force the parser to.",
                    "label": 0
                },
                {
                    "sent": "Label it as a proper noun.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just some words about the named entity recognizer.",
                    "label": 1
                },
                {
                    "sent": "Used some some kind of conditional random field model which was implemented by us and.",
                    "label": 1
                },
                {
                    "sent": "Another trick we added was to deal with examples like this where the text says conducted 6643 full face to face interviews and the hypothesis says more than 60,000 adults.",
                    "label": 0
                },
                {
                    "sent": "So you need to know that 60 this number can be implied by more than 60,000, but not the other way around, for example an.",
                    "label": 0
                },
                {
                    "sent": "There are several tricks in longer.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that whenever something is marked as a number by the named entity recognition system, we look for regular expressions of this form.",
                    "label": 0
                },
                {
                    "sent": "So this might this might map to 6.2 billion.",
                    "label": 0
                },
                {
                    "sent": "This might map to.",
                    "label": 0
                },
                {
                    "sent": "Arrange more than 60,000.",
                    "label": 0
                },
                {
                    "sent": "This might map to approximate number.",
                    "label": 0
                },
                {
                    "sent": "So we recognize these kinds of regular expressions and also we normalize the other name entities, money, date, percent and time into a Canonical Canonical value.",
                    "label": 0
                },
                {
                    "sent": "So you can reason with them.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, given this parse tree, we do some kind of post processing on it.",
                    "label": 1
                },
                {
                    "sent": "One particular kind is that we recognize collocations.",
                    "label": 1
                },
                {
                    "sent": "Using word net.",
                    "label": 1
                },
                {
                    "sent": "For example, if you look at this sentence trick to rank up $92,000,000 that in some particular sense Rang up, we want to deal it and deal with as one.",
                    "label": 0
                },
                {
                    "sent": "As one component instead of being ranking up because Rang up has a particular meaning which does not.",
                    "label": 0
                },
                {
                    "sent": "Match with the meanings of rank order.",
                    "label": 0
                },
                {
                    "sent": "So we collapse.",
                    "label": 0
                },
                {
                    "sent": "If we can collapse them.",
                    "label": 0
                },
                {
                    "sent": "And in this particular example, I also show how money values might you might be represented if 92 million is marked as a money entity, then you might store Canonical value for that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have a parse tree and as I promised, we try to find the dependencies in this parse tree.",
                    "label": 1
                },
                {
                    "sent": "And we find dependencies from the constituency parts rather than using Mini Park as many other people have used.",
                    "label": 0
                },
                {
                    "sent": "One of the things we try to do is we try to collapse several dependencies and to try to find.",
                    "label": 0
                },
                {
                    "sent": "The types of dependencies more accurately, so for example, from this case, bills mother walked to the grocery store.",
                    "label": 1
                },
                {
                    "sent": "This is the kind of representation that you might get.",
                    "label": 0
                },
                {
                    "sent": "You might say that worked as a subject to mother and possessive.",
                    "label": 0
                },
                {
                    "sent": "Possessive dependency between mother and Bill.",
                    "label": 0
                },
                {
                    "sent": "There is a two dependency from walk to store and then there's a noun, dependencies and so on.",
                    "label": 1
                },
                {
                    "sent": "So you can imagine that this can be converted into a graph representation, which also in the next slide, in which each of these words will be a node and the dependencies between them denotes the type of the relation between them.",
                    "label": 0
                },
                {
                    "sent": "And the other way to write this is that you can write the same, almost the same thing in a logical formula.",
                    "label": 0
                },
                {
                    "sent": "So you can have.",
                    "label": 0
                },
                {
                    "sent": "The way to read this logical formula is that A is a mother bees bill.",
                    "label": 0
                },
                {
                    "sent": "There is a possessive relation between BNA sees the grocery store and walked.",
                    "label": 0
                },
                {
                    "sent": "A walk to see an Easter event of the working and I removed some existential quantifiers just to be.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at these representations in more detail.",
                    "label": 0
                },
                {
                    "sent": "So I I drew out this simple sentence is a graph and I wrote out the logical formula and the thing to add now is that.",
                    "label": 0
                },
                {
                    "sent": "We can make the representation much richer than just these because we know things like worked as a verb.",
                    "label": 1
                },
                {
                    "sent": "Will is a person.",
                    "label": 0
                },
                {
                    "sent": "We might know that and then store is the destination of the world.",
                    "label": 1
                },
                {
                    "sent": "So to capture these things, we might annotate these representations in different ways.",
                    "label": 0
                },
                {
                    "sent": "I'll just show some of these annotations.",
                    "label": 0
                },
                {
                    "sent": "For example, you might annotate in the dependency graph representation you might annotate.",
                    "label": 0
                },
                {
                    "sent": "That the node walked is a past tense word.",
                    "label": 1
                },
                {
                    "sent": "That's the part of speech tag.",
                    "label": 0
                },
                {
                    "sent": "You might annotate.",
                    "label": 0
                },
                {
                    "sent": "That bill is a person.",
                    "label": 1
                },
                {
                    "sent": "This node similarly, you might also add annotations to edges.",
                    "label": 0
                },
                {
                    "sent": "For example, you might say that this edge.",
                    "label": 0
                },
                {
                    "sent": "Is a location modified?",
                    "label": 0
                },
                {
                    "sent": "This is the way you write a location modify and similarly in the logical form representation.",
                    "label": 0
                },
                {
                    "sent": "Also you can add these annotations so you can save work.",
                    "label": 0
                },
                {
                    "sent": "The predicate worked is past tense word.",
                    "label": 0
                },
                {
                    "sent": "The predicate build is a person and corresponding to the annotations on the edges you get annotations on the arguments here, but this argument sees the location modifier.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to describe some of the annotations we used because we found that these linguistic features actually turn out to be more important than.",
                    "label": 0
                },
                {
                    "sent": "Many of the other things we do.",
                    "label": 0
                },
                {
                    "sent": "So I already showed that we compute parts of speech and named entities.",
                    "label": 0
                },
                {
                    "sent": "We also compute semantic rules, so here's an example which shows that semantic roles are necessary in some way to for the RT problem.",
                    "label": 0
                },
                {
                    "sent": "So this the text says that C&D Technologies announced.",
                    "label": 1
                },
                {
                    "sent": "Disclose the acquisition of data and then there are actually two hypothesis which says CMD acquired detail or detail acquired CMD and to differentiate between these two.",
                    "label": 1
                },
                {
                    "sent": "We think that it's necessary that you consider the semantic role of this required work.",
                    "label": 0
                },
                {
                    "sent": "So we label all the arguments to avert using the semantic role labeler.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "History.",
                    "label": 0
                },
                {
                    "sent": "Another annotation we might add is coreference, so here's an example.",
                    "label": 0
                },
                {
                    "sent": "Since its permission in 1948 Israel.",
                    "label": 1
                },
                {
                    "sent": "Blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "And then you need to know that it's related to Israel.",
                    "label": 0
                },
                {
                    "sent": "To infer that the hypothesis is.",
                    "label": 0
                },
                {
                    "sent": "Some kind of correct conditional random field model again for coreference detection.",
                    "label": 1
                },
                {
                    "sent": "And each of the annotations each of the representations can be annotated with these extra information.",
                    "label": 0
                },
                {
                    "sent": "And I also want to point out that the positive references which were.",
                    "label": 0
                },
                {
                    "sent": "Which seems to be pretty useful on this task, but rejected, but by a different mechanism.",
                    "label": 0
                },
                {
                    "sent": "It because they could be detected just from the parse tree heuristically.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Influences like this can be bad.",
                    "label": 0
                },
                {
                    "sent": "And there were some other annotations which are not that interesting.",
                    "label": 0
                },
                {
                    "sent": "For example, you can add word stems, you can advert senses, but many of these were not useful for us in the end.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there was this huge Stickley used to find investments which is also.",
                    "label": 0
                },
                {
                    "sent": "Which also shows up in Nominalizations as it was discussed earlier.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have.",
                    "label": 0
                },
                {
                    "sent": "This next sentence witnessed the murder of police commander.",
                    "label": 1
                },
                {
                    "sent": "And then you have to prove that police officer killed.",
                    "label": 0
                },
                {
                    "sent": "So because of the particular way that the inference mechanisms work, they don't know that murder.",
                    "label": 0
                },
                {
                    "sent": "Represents an event of murdering in which the person that was murdered was the police commander.",
                    "label": 0
                },
                {
                    "sent": "Because if you look at the logical formula, for example.",
                    "label": 0
                },
                {
                    "sent": "Then it looks like this.",
                    "label": 0
                },
                {
                    "sent": "So there is a murderer and then police commander and there is no link between them through event work.",
                    "label": 0
                },
                {
                    "sent": "It just shows that the model and the way we augment the representation is.",
                    "label": 1
                },
                {
                    "sent": "That the.",
                    "label": 0
                },
                {
                    "sent": "For all the nouns in this text logical formula we find.",
                    "label": 0
                },
                {
                    "sent": "Verb forms using word net television links, and will augment this representation with a term like this which says that.",
                    "label": 0
                },
                {
                    "sent": "Murder isn't event.",
                    "label": 0
                },
                {
                    "sent": "the M is the murder and he's the police commander which it finds.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at this stage we have come to the.",
                    "label": 0
                },
                {
                    "sent": "Finish the representation part of this talk and I'll now describe each of the inference algorithms 1 by 1.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1st, I'll start with the graph matching approach, which works with the syntactic dependencies.",
                    "label": 1
                },
                {
                    "sent": "So let's skip some of the motivation, because we've already seen this quite a bit of detail, so here's a toy example.",
                    "label": 0
                },
                {
                    "sent": "I'll use that John Border deal BMW and it's supposed to entail.",
                    "label": 1
                },
                {
                    "sent": "John purchased the car, so here are some.",
                    "label": 0
                },
                {
                    "sent": "Dependency graphs for these.",
                    "label": 0
                },
                {
                    "sent": "And each of the vertices will be words or phrases, phrases because they thought they might be collapsed collocations or named entities.",
                    "label": 0
                },
                {
                    "sent": "And it is a label dependencies.",
                    "label": 0
                },
                {
                    "sent": "The neighbors might be semantic roles as they are shown here, or they might be subject object.",
                    "label": 0
                },
                {
                    "sent": "And our goal is that we want to produce a cost of matching edge to T. Because I said our systems work like that.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the idea that we want to align vertices of edge to vertices of T. So just to define, we call a matching a mapping of vertices from HTT.",
                    "label": 1
                },
                {
                    "sent": "So for every vertex and edge you should map to a vertex in T. And this these set of links correspond to a matching.",
                    "label": 0
                },
                {
                    "sent": "And the overall idea would be that we want to find.",
                    "label": 0
                },
                {
                    "sent": "A matching and say that the hypothesis is entailed if and only if we can find a matching that seems to preserve the relations between the vertices and edges.",
                    "label": 0
                },
                {
                    "sent": "So just to make it more formal.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will define the cost of a matching.",
                    "label": 0
                },
                {
                    "sent": "To measure the quality of a matching in terms of two components, we have a vertex cost, which measures how correspondingly similar.",
                    "label": 0
                },
                {
                    "sent": "The vertices are and we have a relation costs which will measure how correspondingly similar relations are.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "The vertex cost might look at how similar is joined to Jaunpur chased aboard car to BMW.",
                    "label": 0
                },
                {
                    "sent": "And the relation cost might look at.",
                    "label": 0
                },
                {
                    "sent": "So you map purchased a botton car to BMW.",
                    "label": 0
                },
                {
                    "sent": "So you are mapping this relation to this solution.",
                    "label": 0
                },
                {
                    "sent": "How similar are these relations?",
                    "label": 0
                },
                {
                    "sent": "And we'll Add all these contributions from the vertices and the relations in a.",
                    "label": 0
                },
                {
                    "sent": "Innovative way so the final cost will say is the vertex cost plus the relation cost.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I should also mention that the overall idea then would be that given this model and given a text in hypothesis fair, the final goal is to find a matching which minimizes this match cost.",
                    "label": 0
                },
                {
                    "sent": "And if that match cost has a low cost, then you say that the hypothesis isn't in otherwise not.",
                    "label": 0
                },
                {
                    "sent": "So I'll describe some of the details now of the vertex cost in relation cost.",
                    "label": 0
                },
                {
                    "sent": "How you could put in features there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the vertex cost, what we're looking at is that we have a vertex in edge and a vertex it is mapped to in T. Then the questions you could ask her, do these vertex vertices have the same stem or the same part of speech?",
                    "label": 0
                },
                {
                    "sent": "Are they hypernyms of each other?",
                    "label": 0
                },
                {
                    "sent": "Are they similar phrases?",
                    "label": 0
                },
                {
                    "sent": "And the way the cost is built up is that for each of these there is a corresponding cost, and these costs are added up throughout to give the overall vertex cost.",
                    "label": 0
                },
                {
                    "sent": "I want to take a digression here to discuss this thing about phrase similarity, because it seems to be very similar for.",
                    "label": 0
                },
                {
                    "sent": "Very important for this.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Several kinds of phrase similarity, the first one being just measures based on word net, which we've seen.",
                    "label": 0
                },
                {
                    "sent": "Then we use measures based on distributional similarity becausw.",
                    "label": 1
                },
                {
                    "sent": "Take this example of running marathon.",
                    "label": 0
                },
                {
                    "sent": "There they seem to be related, but they don't occur in any.",
                    "label": 0
                },
                {
                    "sent": "Very similarly inverted.",
                    "label": 0
                },
                {
                    "sent": "They're not part of the is Iraqi.",
                    "label": 0
                },
                {
                    "sent": "And so we use latent semantic analysis to discover words that are similar, and we also used some.",
                    "label": 1
                },
                {
                    "sent": "We tried some websites business measures and we tried some.",
                    "label": 0
                },
                {
                    "sent": "Some measures based on learning paraphrases, which is similar to previous work but didn't don't help, so we didn't use them in the final system.",
                    "label": 0
                },
                {
                    "sent": "And the last thing was also that we found that cases like the system was supposed to another CEO response to Chief Executive Officer.",
                    "label": 1
                },
                {
                    "sent": "It might be supposed to move in.",
                    "label": 0
                },
                {
                    "sent": "Philippines is the country in Filipino is a person.",
                    "label": 0
                },
                {
                    "sent": "So we actually found.",
                    "label": 1
                },
                {
                    "sent": "Fill in the forms.",
                    "label": 0
                },
                {
                    "sent": "And we can also add common facts that we didn't do that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So returning back I just described the vertex plus 10 for the relation cost.",
                    "label": 0
                },
                {
                    "sent": "If we do something similar, you could say that if there is an edge really prime in edge just now to some agenti.",
                    "label": 0
                },
                {
                    "sent": "Then our parents are relations in edge.",
                    "label": 0
                },
                {
                    "sent": "Indeed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Match this match is found to be the minimum cost matching this exam.",
                    "label": 0
                },
                {
                    "sent": "But this vertex match there is an exact match does not cost for this vertex match.",
                    "label": 0
                },
                {
                    "sent": "It discovers these are synonyms, so there's some conspiracy knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the vertex cost turns out to be original, please just .2.",
                    "label": 0
                },
                {
                    "sent": "Duration cost is 0 because all the edges are preserved.",
                    "label": 0
                },
                {
                    "sent": "The cost is lower.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I finished talking about the graph matching.",
                    "label": 0
                },
                {
                    "sent": "I'm joining.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea of the inductive inferences that we represent extra hypothesis as a logical family and the basic idea is.",
                    "label": 0
                },
                {
                    "sent": "That I believe this can be inferred from the text if and only the logical formula can be proved from the text.",
                    "label": 1
                },
                {
                    "sent": "So we saw something similar about this before.",
                    "label": 0
                },
                {
                    "sent": "Text, John.",
                    "label": 0
                },
                {
                    "sent": "And what we wanted to do was to prove this is given.",
                    "label": 1
                },
                {
                    "sent": "You need to know that a BMW is a car.",
                    "label": 0
                },
                {
                    "sent": "We need to know that buying something in game purchasing.",
                    "label": 0
                },
                {
                    "sent": "It does not seem to be including Axiom Sports.",
                    "label": 0
                },
                {
                    "sent": "At the same time we start.",
                    "label": 0
                },
                {
                    "sent": "Possible words to use your.",
                    "label": 0
                },
                {
                    "sent": "Possibility to also check.",
                    "label": 0
                },
                {
                    "sent": "Assumptions in the group assumptions correspond to axioms in the normal setting, but we also have a cost associated.",
                    "label": 0
                },
                {
                    "sent": "Zoom.",
                    "label": 0
                },
                {
                    "sent": "What is a purchase only?",
                    "label": 0
                },
                {
                    "sent": "The task that remains to find.",
                    "label": 0
                },
                {
                    "sent": "Search costs for assumptions like this and then given such a cost model to find the minimum cost to fund hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So we have all the assumption cost that you use and then you say that OK, the minimum cost is 3 for this because this assumption in this assumption and then based on that person.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just trying to say.",
                    "label": 0
                },
                {
                    "sent": "This is a logical term piece of predicate.",
                    "label": 0
                },
                {
                    "sent": "Who is a predicate?",
                    "label": 0
                },
                {
                    "sent": "If you want to know the arguments and we want to say.",
                    "label": 0
                },
                {
                    "sent": "What is the cost for assuming that this is like this?",
                    "label": 0
                },
                {
                    "sent": "So the way to do it is that we allow predicates to match and then allow arguments to match it.",
                    "label": 0
                },
                {
                    "sent": "The reason we relax the ordering of the argument system.",
                    "label": 0
                },
                {
                    "sent": "Some arguments might correspond multiple arguments diagnostics.",
                    "label": 0
                },
                {
                    "sent": "It might be that.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "And then once you assign such a matching between the predicate and the arguments, we add them and we had the costs associated with each.",
                    "label": 0
                },
                {
                    "sent": "So we have a match for possible matching predicates.",
                    "label": 0
                },
                {
                    "sent": "Look at other features.",
                    "label": 0
                },
                {
                    "sent": "Location, location.",
                    "label": 0
                },
                {
                    "sent": "There is something else going on Cincinnati cost which.",
                    "label": 0
                },
                {
                    "sent": "Construct logical.",
                    "label": 0
                },
                {
                    "sent": "Each other.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I'm supervising potential group Staten.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One last thing is that I just write this way of.",
                    "label": 0
                },
                {
                    "sent": "Assumption costs based on these individual assumption costs, right?",
                    "label": 0
                },
                {
                    "sent": "Of course, for free similarities in their ability.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that we can frame a learning algorithm which are automatically learns fast for these individual assumptions based on.",
                    "label": 0
                },
                {
                    "sent": "In addition, is just that.",
                    "label": 0
                },
                {
                    "sent": "Examples and lower costs.",
                    "label": 0
                },
                {
                    "sent": "Describe some other.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing to do is.",
                    "label": 0
                },
                {
                    "sent": "John Dixon charged.",
                    "label": 0
                },
                {
                    "sent": "It does not match.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cost to score.",
                    "label": 0
                },
                {
                    "sent": "Systems classified in one submission.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are other jobs.",
                    "label": 0
                },
                {
                    "sent": "So at this time.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like us it seems to be doing well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are some results based on confidence, So what I did was I looked at.",
                    "label": 0
                },
                {
                    "sent": "50%.",
                    "label": 0
                },
                {
                    "sent": "Only only on the top 50% confident examples.",
                    "label": 0
                },
                {
                    "sent": "If you only need to stop them from 50% if you go to one of the curves and you find accuracy there.",
                    "label": 0
                },
                {
                    "sent": "So this means that if I were to do abortion submission with only 50% of the bells.",
                    "label": 0
                },
                {
                    "sent": "Coverage.",
                    "label": 0
                },
                {
                    "sent": "Interesting things out there.",
                    "label": 0
                },
                {
                    "sent": "This curve is the better it is.",
                    "label": 0
                },
                {
                    "sent": "30% coverage then you can get elected.",
                    "label": 0
                },
                {
                    "sent": "And you can also draw similar coverage and confidence with school clubs.",
                    "label": 1
                },
                {
                    "sent": "So you say that you find cover only 50% of the examples.",
                    "label": 0
                },
                {
                    "sent": "What complaints?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some interesting issues.",
                    "label": 0
                },
                {
                    "sent": "Saying that something is of any of the Ryder Cup does not always mean is the host.",
                    "label": 1
                },
                {
                    "sent": "For example, you might say London is the running of the limits.",
                    "label": 0
                },
                {
                    "sent": "Responsibility for restoration.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think this one is interesting, but.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "I was.",
                    "label": 0
                },
                {
                    "sent": "Suppose I say that for the top 30% examples and classify and for the other side, just in case it turns out that system does better than ours, around 3% very close to the best.",
                    "label": 0
                },
                {
                    "sent": "So we have some logic.",
                    "label": 0
                },
                {
                    "sent": "On this stage.",
                    "label": 0
                },
                {
                    "sent": "Using an assumption based model for instance.",
                    "label": 0
                },
                {
                    "sent": "This song.",
                    "label": 0
                },
                {
                    "sent": "Automatically generated using this.",
                    "label": 0
                },
                {
                    "sent": "So it's like a search procedure that you're starting out with.",
                    "label": 0
                },
                {
                    "sent": "Using resolution definition you have you starting out with this kind of.",
                    "label": 0
                },
                {
                    "sent": "Individual.",
                    "label": 0
                },
                {
                    "sent": "And then you want to find the minimum cost path to the number.",
                    "label": 0
                },
                {
                    "sent": "So there are some cases in which you might have.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have good evidence, then you might have a link between.",
                    "label": 0
                },
                {
                    "sent": "So I didn't mention that, but finding the minimum matching cost is not known to be attractive.",
                    "label": 0
                },
                {
                    "sent": "Because the minimizing relation costs that I mentioned this in some class which is not known.",
                    "label": 0
                },
                {
                    "sent": "So what we do actually is that we minimize the vertex costs because that is tractable, and then we do a local search around that process.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention it.",
                    "label": 0
                },
                {
                    "sent": "Versus low carb.",
                    "label": 0
                },
                {
                    "sent": "So do you have any?",
                    "label": 0
                },
                {
                    "sent": "Insights about comparing the two, which ones?",
                    "label": 0
                },
                {
                    "sent": "Is it really necessary value?",
                    "label": 0
                },
                {
                    "sent": "Something to move into large representation?",
                    "label": 0
                },
                {
                    "sent": "Turns out that both of the systems perform about equally India, but we haven't done internal system.",
                    "label": 0
                },
                {
                    "sent": "Which one does better?",
                    "label": 0
                },
                {
                    "sent": "So I'm easier than the other.",
                    "label": 0
                },
                {
                    "sent": "It might be that the graph matching.",
                    "label": 0
                }
            ]
        }
    }
}