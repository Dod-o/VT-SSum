{
    "id": "d7mlliesx2z2tm7o2llla6xcb5sbu7ft",
    "title": "Sparse Multiscale Gaussian Process Regression",
    "info": {
        "author": [
            "Christian Walder, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "July 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_walder_spg/",
    "segmentation": [
        [
            "OK, I'm very excited about this next paper because it's a Gaussian process paper with Bernard Chill Cop's name on it, so that's a very good thing.",
            "So the next talk is sparse, multiscale Gaussian process regression, and it can be given by Christian Wilder.",
            "Say hello everybody, this work is done in collaboration with Connie and Kim and Bernhard Schoelkopf at the Max Planck Institute in 2 min.",
            "Before I even get to the outline slide, I'd like to just give you a very short description in words in where we're going.",
            "The idea is this.",
            "If you are familiar with Gaussian processes, you probably know that the posterior mean is usually written as a combination of covariance function centered at the data.",
            "And if you're familiar with sparse approximations, you might know that people sometimes, as an approximation, restrict this sum to some subset of these points.",
            "The main idea in this work here is to reduce.",
            "Remove the restriction that these basis functions should be coming from the covariance function.",
            "What I mean by that is typically the basis functions of the covariance function with one of its two inputs fixed."
        ],
        [
            "So here's the outline.",
            "To begin with, I'll give some kind of a brief history of sparse approximations to GPS.",
            "And as I do that, I divide it into two separate groups.",
            "So first of all they consider those cases where the basis functions are taken from the covariance function.",
            "As I said, again with one of its two inputs fixed, and this covers probably 95% of the work.",
            "In sparse GPS, the other section, and this is the category into which this work falls is the case where the function basis is in fact unrelated to the covariance function, and the idea is that if you have a more rich space of basis functions you could achieve.",
            "Better approximations for the same number of basis functions.",
            "That will then bring us to the main part of the work and the idea here is to derive the necessary equations in order to do this for the special case where the covariance function of the process is Gaussian and the basis functions are also Gaussian but with arbitrary length scales.",
            "So you can vary the width of each basis function independently.",
            "So once I've computed the necessary equations for that or give some interpretation.",
            "In particular, I'll make a statement about which Gaussian functions belong to the arc HS of a Gaussian OK chess.",
            "And following that will have some examples.",
            "So give a toy example for the GP regression case in one dimension.",
            "Some real examples with the GP regression and also just because bernhards on the paper I'll have something about support vector classification."
        ],
        [
            "OK, so as I said, this is the basic history of sparse approximations to GPS, so typically the posterior mean can be written in the end.",
            "As a combination of.",
            "A kernel function centered on the data, which is the XI.",
            "Probably the most simple approximation to this, is to restrict this posterior mean to take the second form, and the only difference here is that the sum is not from.",
            "I want AN, but over some subset of this.",
            "You can go one step further.",
            "In particular, I think the paper of Snelson and Ghahramani does this by removing the restriction that these centers come from the data.",
            "So you could instead have some of this form where these are arbitrary.",
            "In this case you would have a continuous optimization over these die."
        ],
        [
            "So for the more exotic case where these basis functions not come from the current function, we could write the posterior mean in this form.",
            "So basically the main difference is that rather than having a covariance function, here we have an arbitrary function UI, so this should have some additional parameters which allow additional flexibility in the approximation.",
            "Naturally, if you set the UI to be K of says that IX, this collapses back to the previous case in the previous slide, which is what the majority of work has focused on.",
            "As I said, there's not much work on this.",
            "We had a paper at the second last nips, where we chose for the UI compactly supported basis functions, which were given by the B3 spline, and we tried to approximate the performance of a thin plate spline regularizer.",
            "This led to some nice results.",
            "In particular, we had regression results with 50 million data points.",
            "But it was necessary to restrict to low dimensional cases.",
            "This one only one other paper which I'm aware of which does this and this was by Peter Gala, an Matthias friends.",
            "In this case they choose polynomial basis functions and they tried to approximate the performance of a GP with Gaussian covariance.",
            "So the reason they did that was because they wanted their final solution in the form of a polynomial for post processing.",
            "So as I said, the main focus of our work is the special case where the covariance function of the GP is itself Gaussian and the basis functions are also Gaussian with arbitrary length scales."
        ],
        [
            "OK, so the key ingredient we need here is to.",
            "Compute the probability under the prior of a mixture of arbitrary Gaussian basis functions.",
            "When I say arbitrary Gaussian basis functions, I mean Gaussian basis functions with arbitrary length scales.",
            "To do this, we basically start from the finite case and take an infinite limit.",
            "I would say this is perhaps not really in the spirit of Gaussian process is normally a GP, is defined on a finite set of point evaluations, but in this case we take a limit an infinite limit of that which seems to work.",
            "So that's what we do.",
            "So to this end, if we let G of K be the zero mean Gaussian process with covariance K, and we let the function you will be drawn from that process, we can define this random variable U which is just the valuations of you at some set of points and we also define cave XX in the usual way to the gram matrix defined by this covariance function and this set of data.",
            "And in this case we have immediately from the definition of the GP, the probability of an arbitrary function which follows the distribution of that random variable UX.",
            "So the form is given, then it should be familiar.",
            "The reason why this is being written as a sum.",
            "So the reason the argument here is the sum of vectors UI is because ultimately we want to compute the likely.",
            "So the probability of.",
            "A mixture of basis functions so it's natural to start this way.",
            "And as I said, to do that, we take an infinite limit an going to Infinity.",
            "So an infinite number of points XI and OK, it's necessary to assume that the distribution of the XI is nonvanishing.",
            "For example, it could be uniformly distributed.",
            "And in this case, the vectors UI becomes something like functions.",
            "And the the.",
            "Probability here is fairly straightforward, except perhaps for this term here.",
            "So this is a vector matrix vector product with an inverse, but in the infinite limit this becomes a double integral against some function K -- 1, XY against the two functions UI and UJ.",
            "So I should clarify that this function K to the minus one is not the inverse of K. This is in fact something more complicated."
        ],
        [
            "So in particular, in order to decide upon what the generalization to the infinite dimensional case of this matrix inverses.",
            "We can start by simply defining the matrix inverse in the following way.",
            "So for the finite dimensional case we have that.",
            "If U equals K Alpha then and if the inverse of K exists then Alpha is Caden minus one you.",
            "So taking this to the infinite dimensional case.",
            "We have this function K to minus one should satisfy, so if you is analogous to a matrix vector product in the infinite dimensional case, in other words, you is the integral of Alpha against K. Here.",
            "Then we have that this should hold for the infinite dimensional case.",
            "In other words, this integral should give back Alpha.",
            "This for somebody who comes from a Colonel perspective.",
            "I for someone who comes from Tubingen.",
            "It's interesting to view this as a greens function, so if we define the operator MK to be the integral operator against the integral kernel K as given here.",
            "Then what we have is in fact that Kate Kate the minus one is the Greens function of this operator MK."
        ],
        [
            "So in general it's not so easy to compute this term.",
            "So for example, if you take an arbitrary covariance function and an arbitrary set of basis functions, it's necessary to solve several integrals, and this cannot always be done.",
            "But as is often the case, if you restrict everything to be Gaussian, then things come out in close form, so in particular we define the normalized Gaussian like so.",
            "So this is basically it's like you get Gaussian covariance function between X&Y.",
            "With diagonal covariance matrix with diagonal given by Sigma.",
            "And it's normalized for convenience.",
            "If we now choose that the covariance function of the GP is Gaussian, so it is of this form.",
            "It's basically a Gaussian with length scales Sigma and some constant term C. And we also define all of the basis functions to be Gaussian, but in this case they have arbitrary length scales and arbitrary centers.",
            "Then we can do the math.",
            "Then you can check it in the paper that you get the solution in closed form as given at the bottom of the slide.",
            "So in particular this turns out to be a Gaussian again.",
            "And we get a very neat form.",
            "Which is this interesting term here?",
            "Sigma Alpha, Sigma J Minus Sigma, which will come up again in the talk."
        ],
        [
            "So.",
            "In case you haven't followed what I said, it doesn't really matter because everything is summarized in this slide, so I'll take some time to pause it.",
            "Basically what we've computed is the probability under the GP prior with the Gaussian covariance function of a mixture of Gaussians with arbitrary link skills.",
            "So to put that in equations we have, well I've written.",
            "It is as follows.",
            "This is the arbitrary mixture of Gaussians with Clinkscales, Sigmai, incenters, VI.",
            "They waited by the CIA.",
            "And we want to compute the probability of that function under the GP prior with this covariance function.",
            "Which is Gaussian with with Sigma.",
            "So don't confuse the fact that so these are different.",
            "Here we have Sigma eyes and here we have just sigmas.",
            "Um?",
            "And what we have computed is that that term is proportional to this expression.",
            "Which you may recognize as basically a multivariate Gaussian distribution in the CI.",
            "Naturally, disinfect an analogous statement that you can make in the context of arcade justice.",
            "In particular, if you let H be the arcade chest with kernel, Gaussian with Clinkscales Sigma, it turns out that you have this closed form for this inner product between two Gaussian functions, so the inner product under that arcade just norm of a Gaussian thank scale.",
            "Sigma Iron Center VI against another one with center Vijay and Link Scale Sigma J.",
            "Is again given by this funny term which is Gaussian.",
            "With Gaussian in between the two centers with this link scale."
        ],
        [
            "OK, so you can interpret this in very various ways.",
            "Naturally, if you take the case where the basis functions and the covariance function of the Gaussian process all have the same length scale.",
            "Then you recover the normal case because this link scale becomes Sigma Alpha Sigma Gamma Sigma which is Sigma Plus Sigma minus Sigma which is Sigma.",
            "Moreover, under the GP prior, the most likely Gaussian function is that which has length scale equal to that of the covariance function.",
            "Another point is that as the dimensionality increases, the probability of a Gaussian function whose length scale is not the same as the covariance function decreases quite sharply.",
            "So what that means in practice for approximations is that.",
            "In higher dimensional cases, this multiscale approximation will be less useful.",
            "Environmental dimensional cases.",
            "It will be more useful.",
            "And the final point is probably the most interesting one.",
            "This one was actually noted I discovered in the kernel.",
            "I see a paper from Bark and Jordan and the statement is it's easier to see in words.",
            "The statement is that if you have a Gaussian process with Gaussian covariance function, then the probability of a Gaussian function under that prior.",
            "Is 0 if the width of the function is less than half that of the width of the covariance function.",
            "This means that a Gaussian process cannot recover any function which is Gaussian and too narrow.",
            "And by two narrow I mean only that it's less than half the width of the covariance function.",
            "This is perhaps surprising."
        ],
        [
            "OK, so now I can go to some examples.",
            "The first one is a toy example in one dimension which illustrates what's happening basically on the right hand side we have the exact GP.",
            "On the far left we have a basically this Nelson and Ghahramani approximation.",
            "Which, like most Gaussian process sparse Gaussian process methods, has the constraint that all the basis functions of the same width as the covariance function and using this approximation with four basis function leads to the solution on the left.",
            "Um?",
            "These denote the centers of the basis functions.",
            "Now if we remove the restriction that these centers should have the same width as that of the covariance function, we can obtain a better solution.",
            "So the height now denotes the width of the basis functions and you can see that it's approximating something.",
            "It's approximating this somehow a little bit better."
        ],
        [
            "So it's probably not necessary to go into the details of these figures.",
            "I'll just summarize the results on the real world examples.",
            "Basically what happens is.",
            "If the dimension of these two higher this pose you very little in comparison to the other methods.",
            "If the dimensionality is sufficiently low, it can give you an advantage.",
            "In particular, if you're interested in cases with very few basis functions, you tend to do quite well.",
            "And Moreover, this this right hand side figure is a depiction in of how much the individual basis function, which vary from those of from that of the covariance function.",
            "So as you can see is you increase the number of basis functions.",
            "The.",
            "Their widths tend to vary less, which is not surprising.",
            "Of course, if you have as many basis functions as data points.",
            "Then there will be the optimal solution would be to set all of their words to that of the covariance function."
        ],
        [
            "I'd like to just advertise that this.",
            "So this kind of approach is useful for any kernel machine, so it's really simple to apply it.",
            "Normally so if you have an arcade chess H with Gaussian kernel and you're minimizing something of this form.",
            "The key trick is that if you want to approximate your function as a multi scale expansion as before, it's possible to compute the norm in closed form of this mixture.",
            "In particular, it becomes simply a matrix vector matrix, vector matrix vector product.",
            "And the matrix is again given by this collection of.",
            "Funny terms, Sigma Alpha Sigma Sigma."
        ],
        [
            "So I have a video which which visualizes the optimization process for the support vector machine case.",
            "So what we've done here is.",
            "We've optimized the SVM objective function with a reduced number of basis functions.",
            "Initially we set their widths to be fixed to that of the kernel of the arcade chess and optimize their positions, and then after convergence we then remove the restriction that their width should be the same as that of the covariance function and continue optimization, which gives you some idea of what this can buy you.",
            "OK, so right now the widths of fixed and their denoted by these lengths of these crosses.",
            "The data points are given here for the positive class in the negative course.",
            "Now the widths have been released and you can see.",
            "The length scale of each dimension given by the black line.",
            "And the length scale of the actual kernel of the OK chest is given by the white line in the cross.",
            "So basically by having this extra degree of freedom, it's possible, for example, to have one basis function covered.",
            "The territory here, which would otherwise require several basis functions.",
            "And you can also see that the objective function gets considerably better.",
            "Unfortunately, the frame rate of the video seems to be slower than what it should be.",
            "OK, so it's basically converged."
        ],
        [
            "This basically concludes the talk, I'll just give a quick summary.",
            "We've seen how to approximate a Gaussian kernel machine using a function basis, which is a set of Gaussians about tree length scales.",
            "This generalizes basically all the vast majority of previous methods, hence it's at least as good as them.",
            "It's particularly good if you want short test times.",
            "Basically, you're sacrificing training time for test time because you have a more complex optimization problem, But it results in a solution with less basis functions.",
            "If the input dimension of these lower you've got the chance for a better improvement, and finally it's possible to apply this idea to other combinations of covariance functions and basis functions.",
            "But to do so requires solving some integrals which may not be available in closed form.",
            "I can say that I have in fact done it for the case of the Laplace kernel.",
            "Thank you very much.",
            "Qua"
        ],
        [
            "Agents so I have a question too.",
            "Tying a little bit to the previous talk, we've seen that Oliver, compared with Zubin and EDS.",
            "Spot parametric Gaussian processes.",
            "Do you think there's also scope here that although that in the 1st place was designed as a sparse approximation, I lent itself to nonstationary kernels.",
            "You get the same effects with this type of kernel.",
            "Yeah, indeed you do.",
            "We have some experiments have shown that in fact these past method can outperform the full GP.",
            "Although initially we thought of this is in fact just an approximation to the full GP, but naturally you can think of the basis centers and widths as.",
            "Hyperparameters of your covariance function it induces basically a different covariance function of a full GP and hence if you tune these, you could possibly do better, yes?",
            "We have to stand here for 4 minutes if they will ask the question.",
            "No.",
            "The problem is we have a lot of information, so the question was what is the problem when you have a lot of input dimensions?",
            "What happens is this.",
            "So we have this term, the Gaussian between the two basis function centers with Bandwidth Sigma I plus Sigma J minus Sigma.",
            "But there's also a normalization term, and this normalization term depends on the width on the effective bandwidth, namely Sigma, Alpha, Sigma, Gamma, Sigma, and if you check, it turns out that.",
            "As you as the dimensionality increases, this quantity increases and effectively what it means is that.",
            "If you think of it from the arcade, just perspective, the norm of these functions is very large, so the norm of a Gaussian which is twice as broad as the kernel function in a high dimensional space is extremely high.",
            "In other words, it's extremely unlikely under the GP prior.",
            "It's kind of hidden because you have to think about the normalization term in the definition of this Gaussian G. That's basically the problem.",
            "Don't.",
            "Just to make this a normalization problem solving problem.",
            "I guess my what's happening.",
            "Goodnight space factor kinda spread outside the main points I'd.",
            "So what you're doing you're spending your Internet domains are outside your points at that time period.",
            "It was through the Gaussian register.",
            "Very small service in the cap lock, so that's the reason we're seeing reasonably after three problematic rules.",
            "Simply excellent, but much more ringing the model.",
            "Not necessarily, but I think.",
            "Yeah, that's quite possible.",
            "You can also think of it though in the following way.",
            "Basically, if you have a very broad Gaussian, then to compute the norm of it you could represent it as some kind of possibly infinite sum of Gaussians which are narrower and then OK if as the function gets broader you clearly need more of these small basis functions to represent it.",
            "So actually the normal is certainly increasing but.",
            "Probably probably not.",
            "Well.",
            "Just do something.",
            "Stop typing OK?",
            "Yeah, that's right.",
            "Having a more regular.",
            "Yeah.",
            "Yeah, I agree.",
            "Basically in general it's difficult to construct a good finite dimensional basis for high dimensional space.",
            "You kind of guaranteed to run into problems.",
            "Good, OK, if there's no more questions, we can just think Christian again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm very excited about this next paper because it's a Gaussian process paper with Bernard Chill Cop's name on it, so that's a very good thing.",
                    "label": 0
                },
                {
                    "sent": "So the next talk is sparse, multiscale Gaussian process regression, and it can be given by Christian Wilder.",
                    "label": 1
                },
                {
                    "sent": "Say hello everybody, this work is done in collaboration with Connie and Kim and Bernhard Schoelkopf at the Max Planck Institute in 2 min.",
                    "label": 0
                },
                {
                    "sent": "Before I even get to the outline slide, I'd like to just give you a very short description in words in where we're going.",
                    "label": 0
                },
                {
                    "sent": "The idea is this.",
                    "label": 0
                },
                {
                    "sent": "If you are familiar with Gaussian processes, you probably know that the posterior mean is usually written as a combination of covariance function centered at the data.",
                    "label": 0
                },
                {
                    "sent": "And if you're familiar with sparse approximations, you might know that people sometimes, as an approximation, restrict this sum to some subset of these points.",
                    "label": 0
                },
                {
                    "sent": "The main idea in this work here is to reduce.",
                    "label": 0
                },
                {
                    "sent": "Remove the restriction that these basis functions should be coming from the covariance function.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that is typically the basis functions of the covariance function with one of its two inputs fixed.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline.",
                    "label": 0
                },
                {
                    "sent": "To begin with, I'll give some kind of a brief history of sparse approximations to GPS.",
                    "label": 1
                },
                {
                    "sent": "And as I do that, I divide it into two separate groups.",
                    "label": 0
                },
                {
                    "sent": "So first of all they consider those cases where the basis functions are taken from the covariance function.",
                    "label": 0
                },
                {
                    "sent": "As I said, again with one of its two inputs fixed, and this covers probably 95% of the work.",
                    "label": 0
                },
                {
                    "sent": "In sparse GPS, the other section, and this is the category into which this work falls is the case where the function basis is in fact unrelated to the covariance function, and the idea is that if you have a more rich space of basis functions you could achieve.",
                    "label": 1
                },
                {
                    "sent": "Better approximations for the same number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "That will then bring us to the main part of the work and the idea here is to derive the necessary equations in order to do this for the special case where the covariance function of the process is Gaussian and the basis functions are also Gaussian but with arbitrary length scales.",
                    "label": 0
                },
                {
                    "sent": "So you can vary the width of each basis function independently.",
                    "label": 0
                },
                {
                    "sent": "So once I've computed the necessary equations for that or give some interpretation.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'll make a statement about which Gaussian functions belong to the arc HS of a Gaussian OK chess.",
                    "label": 1
                },
                {
                    "sent": "And following that will have some examples.",
                    "label": 0
                },
                {
                    "sent": "So give a toy example for the GP regression case in one dimension.",
                    "label": 0
                },
                {
                    "sent": "Some real examples with the GP regression and also just because bernhards on the paper I'll have something about support vector classification.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, this is the basic history of sparse approximations to GPS, so typically the posterior mean can be written in the end.",
                    "label": 1
                },
                {
                    "sent": "As a combination of.",
                    "label": 0
                },
                {
                    "sent": "A kernel function centered on the data, which is the XI.",
                    "label": 0
                },
                {
                    "sent": "Probably the most simple approximation to this, is to restrict this posterior mean to take the second form, and the only difference here is that the sum is not from.",
                    "label": 0
                },
                {
                    "sent": "I want AN, but over some subset of this.",
                    "label": 0
                },
                {
                    "sent": "You can go one step further.",
                    "label": 0
                },
                {
                    "sent": "In particular, I think the paper of Snelson and Ghahramani does this by removing the restriction that these centers come from the data.",
                    "label": 0
                },
                {
                    "sent": "So you could instead have some of this form where these are arbitrary.",
                    "label": 0
                },
                {
                    "sent": "In this case you would have a continuous optimization over these die.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the more exotic case where these basis functions not come from the current function, we could write the posterior mean in this form.",
                    "label": 1
                },
                {
                    "sent": "So basically the main difference is that rather than having a covariance function, here we have an arbitrary function UI, so this should have some additional parameters which allow additional flexibility in the approximation.",
                    "label": 1
                },
                {
                    "sent": "Naturally, if you set the UI to be K of says that IX, this collapses back to the previous case in the previous slide, which is what the majority of work has focused on.",
                    "label": 0
                },
                {
                    "sent": "As I said, there's not much work on this.",
                    "label": 1
                },
                {
                    "sent": "We had a paper at the second last nips, where we chose for the UI compactly supported basis functions, which were given by the B3 spline, and we tried to approximate the performance of a thin plate spline regularizer.",
                    "label": 1
                },
                {
                    "sent": "This led to some nice results.",
                    "label": 0
                },
                {
                    "sent": "In particular, we had regression results with 50 million data points.",
                    "label": 0
                },
                {
                    "sent": "But it was necessary to restrict to low dimensional cases.",
                    "label": 0
                },
                {
                    "sent": "This one only one other paper which I'm aware of which does this and this was by Peter Gala, an Matthias friends.",
                    "label": 1
                },
                {
                    "sent": "In this case they choose polynomial basis functions and they tried to approximate the performance of a GP with Gaussian covariance.",
                    "label": 0
                },
                {
                    "sent": "So the reason they did that was because they wanted their final solution in the form of a polynomial for post processing.",
                    "label": 1
                },
                {
                    "sent": "So as I said, the main focus of our work is the special case where the covariance function of the GP is itself Gaussian and the basis functions are also Gaussian with arbitrary length scales.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the key ingredient we need here is to.",
                    "label": 0
                },
                {
                    "sent": "Compute the probability under the prior of a mixture of arbitrary Gaussian basis functions.",
                    "label": 1
                },
                {
                    "sent": "When I say arbitrary Gaussian basis functions, I mean Gaussian basis functions with arbitrary length scales.",
                    "label": 0
                },
                {
                    "sent": "To do this, we basically start from the finite case and take an infinite limit.",
                    "label": 1
                },
                {
                    "sent": "I would say this is perhaps not really in the spirit of Gaussian process is normally a GP, is defined on a finite set of point evaluations, but in this case we take a limit an infinite limit of that which seems to work.",
                    "label": 0
                },
                {
                    "sent": "So that's what we do.",
                    "label": 1
                },
                {
                    "sent": "So to this end, if we let G of K be the zero mean Gaussian process with covariance K, and we let the function you will be drawn from that process, we can define this random variable U which is just the valuations of you at some set of points and we also define cave XX in the usual way to the gram matrix defined by this covariance function and this set of data.",
                    "label": 1
                },
                {
                    "sent": "And in this case we have immediately from the definition of the GP, the probability of an arbitrary function which follows the distribution of that random variable UX.",
                    "label": 0
                },
                {
                    "sent": "So the form is given, then it should be familiar.",
                    "label": 0
                },
                {
                    "sent": "The reason why this is being written as a sum.",
                    "label": 0
                },
                {
                    "sent": "So the reason the argument here is the sum of vectors UI is because ultimately we want to compute the likely.",
                    "label": 0
                },
                {
                    "sent": "So the probability of.",
                    "label": 0
                },
                {
                    "sent": "A mixture of basis functions so it's natural to start this way.",
                    "label": 0
                },
                {
                    "sent": "And as I said, to do that, we take an infinite limit an going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So an infinite number of points XI and OK, it's necessary to assume that the distribution of the XI is nonvanishing.",
                    "label": 0
                },
                {
                    "sent": "For example, it could be uniformly distributed.",
                    "label": 0
                },
                {
                    "sent": "And in this case, the vectors UI becomes something like functions.",
                    "label": 0
                },
                {
                    "sent": "And the the.",
                    "label": 0
                },
                {
                    "sent": "Probability here is fairly straightforward, except perhaps for this term here.",
                    "label": 0
                },
                {
                    "sent": "So this is a vector matrix vector product with an inverse, but in the infinite limit this becomes a double integral against some function K -- 1, XY against the two functions UI and UJ.",
                    "label": 0
                },
                {
                    "sent": "So I should clarify that this function K to the minus one is not the inverse of K. This is in fact something more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular, in order to decide upon what the generalization to the infinite dimensional case of this matrix inverses.",
                    "label": 0
                },
                {
                    "sent": "We can start by simply defining the matrix inverse in the following way.",
                    "label": 0
                },
                {
                    "sent": "So for the finite dimensional case we have that.",
                    "label": 0
                },
                {
                    "sent": "If U equals K Alpha then and if the inverse of K exists then Alpha is Caden minus one you.",
                    "label": 1
                },
                {
                    "sent": "So taking this to the infinite dimensional case.",
                    "label": 0
                },
                {
                    "sent": "We have this function K to minus one should satisfy, so if you is analogous to a matrix vector product in the infinite dimensional case, in other words, you is the integral of Alpha against K. Here.",
                    "label": 0
                },
                {
                    "sent": "Then we have that this should hold for the infinite dimensional case.",
                    "label": 0
                },
                {
                    "sent": "In other words, this integral should give back Alpha.",
                    "label": 0
                },
                {
                    "sent": "This for somebody who comes from a Colonel perspective.",
                    "label": 0
                },
                {
                    "sent": "I for someone who comes from Tubingen.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to view this as a greens function, so if we define the operator MK to be the integral operator against the integral kernel K as given here.",
                    "label": 1
                },
                {
                    "sent": "Then what we have is in fact that Kate Kate the minus one is the Greens function of this operator MK.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in general it's not so easy to compute this term.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take an arbitrary covariance function and an arbitrary set of basis functions, it's necessary to solve several integrals, and this cannot always be done.",
                    "label": 0
                },
                {
                    "sent": "But as is often the case, if you restrict everything to be Gaussian, then things come out in close form, so in particular we define the normalized Gaussian like so.",
                    "label": 0
                },
                {
                    "sent": "So this is basically it's like you get Gaussian covariance function between X&Y.",
                    "label": 0
                },
                {
                    "sent": "With diagonal covariance matrix with diagonal given by Sigma.",
                    "label": 0
                },
                {
                    "sent": "And it's normalized for convenience.",
                    "label": 0
                },
                {
                    "sent": "If we now choose that the covariance function of the GP is Gaussian, so it is of this form.",
                    "label": 0
                },
                {
                    "sent": "It's basically a Gaussian with length scales Sigma and some constant term C. And we also define all of the basis functions to be Gaussian, but in this case they have arbitrary length scales and arbitrary centers.",
                    "label": 0
                },
                {
                    "sent": "Then we can do the math.",
                    "label": 0
                },
                {
                    "sent": "Then you can check it in the paper that you get the solution in closed form as given at the bottom of the slide.",
                    "label": 0
                },
                {
                    "sent": "So in particular this turns out to be a Gaussian again.",
                    "label": 0
                },
                {
                    "sent": "And we get a very neat form.",
                    "label": 0
                },
                {
                    "sent": "Which is this interesting term here?",
                    "label": 0
                },
                {
                    "sent": "Sigma Alpha, Sigma J Minus Sigma, which will come up again in the talk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In case you haven't followed what I said, it doesn't really matter because everything is summarized in this slide, so I'll take some time to pause it.",
                    "label": 0
                },
                {
                    "sent": "Basically what we've computed is the probability under the GP prior with the Gaussian covariance function of a mixture of Gaussians with arbitrary link skills.",
                    "label": 0
                },
                {
                    "sent": "So to put that in equations we have, well I've written.",
                    "label": 0
                },
                {
                    "sent": "It is as follows.",
                    "label": 0
                },
                {
                    "sent": "This is the arbitrary mixture of Gaussians with Clinkscales, Sigmai, incenters, VI.",
                    "label": 0
                },
                {
                    "sent": "They waited by the CIA.",
                    "label": 0
                },
                {
                    "sent": "And we want to compute the probability of that function under the GP prior with this covariance function.",
                    "label": 0
                },
                {
                    "sent": "Which is Gaussian with with Sigma.",
                    "label": 0
                },
                {
                    "sent": "So don't confuse the fact that so these are different.",
                    "label": 0
                },
                {
                    "sent": "Here we have Sigma eyes and here we have just sigmas.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And what we have computed is that that term is proportional to this expression.",
                    "label": 0
                },
                {
                    "sent": "Which you may recognize as basically a multivariate Gaussian distribution in the CI.",
                    "label": 0
                },
                {
                    "sent": "Naturally, disinfect an analogous statement that you can make in the context of arcade justice.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you let H be the arcade chest with kernel, Gaussian with Clinkscales Sigma, it turns out that you have this closed form for this inner product between two Gaussian functions, so the inner product under that arcade just norm of a Gaussian thank scale.",
                    "label": 1
                },
                {
                    "sent": "Sigma Iron Center VI against another one with center Vijay and Link Scale Sigma J.",
                    "label": 0
                },
                {
                    "sent": "Is again given by this funny term which is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "With Gaussian in between the two centers with this link scale.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you can interpret this in very various ways.",
                    "label": 0
                },
                {
                    "sent": "Naturally, if you take the case where the basis functions and the covariance function of the Gaussian process all have the same length scale.",
                    "label": 0
                },
                {
                    "sent": "Then you recover the normal case because this link scale becomes Sigma Alpha Sigma Gamma Sigma which is Sigma Plus Sigma minus Sigma which is Sigma.",
                    "label": 0
                },
                {
                    "sent": "Moreover, under the GP prior, the most likely Gaussian function is that which has length scale equal to that of the covariance function.",
                    "label": 1
                },
                {
                    "sent": "Another point is that as the dimensionality increases, the probability of a Gaussian function whose length scale is not the same as the covariance function decreases quite sharply.",
                    "label": 1
                },
                {
                    "sent": "So what that means in practice for approximations is that.",
                    "label": 0
                },
                {
                    "sent": "In higher dimensional cases, this multiscale approximation will be less useful.",
                    "label": 0
                },
                {
                    "sent": "Environmental dimensional cases.",
                    "label": 0
                },
                {
                    "sent": "It will be more useful.",
                    "label": 0
                },
                {
                    "sent": "And the final point is probably the most interesting one.",
                    "label": 0
                },
                {
                    "sent": "This one was actually noted I discovered in the kernel.",
                    "label": 1
                },
                {
                    "sent": "I see a paper from Bark and Jordan and the statement is it's easier to see in words.",
                    "label": 0
                },
                {
                    "sent": "The statement is that if you have a Gaussian process with Gaussian covariance function, then the probability of a Gaussian function under that prior.",
                    "label": 0
                },
                {
                    "sent": "Is 0 if the width of the function is less than half that of the width of the covariance function.",
                    "label": 0
                },
                {
                    "sent": "This means that a Gaussian process cannot recover any function which is Gaussian and too narrow.",
                    "label": 0
                },
                {
                    "sent": "And by two narrow I mean only that it's less than half the width of the covariance function.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps surprising.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I can go to some examples.",
                    "label": 0
                },
                {
                    "sent": "The first one is a toy example in one dimension which illustrates what's happening basically on the right hand side we have the exact GP.",
                    "label": 1
                },
                {
                    "sent": "On the far left we have a basically this Nelson and Ghahramani approximation.",
                    "label": 0
                },
                {
                    "sent": "Which, like most Gaussian process sparse Gaussian process methods, has the constraint that all the basis functions of the same width as the covariance function and using this approximation with four basis function leads to the solution on the left.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "These denote the centers of the basis functions.",
                    "label": 0
                },
                {
                    "sent": "Now if we remove the restriction that these centers should have the same width as that of the covariance function, we can obtain a better solution.",
                    "label": 0
                },
                {
                    "sent": "So the height now denotes the width of the basis functions and you can see that it's approximating something.",
                    "label": 0
                },
                {
                    "sent": "It's approximating this somehow a little bit better.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's probably not necessary to go into the details of these figures.",
                    "label": 0
                },
                {
                    "sent": "I'll just summarize the results on the real world examples.",
                    "label": 1
                },
                {
                    "sent": "Basically what happens is.",
                    "label": 0
                },
                {
                    "sent": "If the dimension of these two higher this pose you very little in comparison to the other methods.",
                    "label": 0
                },
                {
                    "sent": "If the dimensionality is sufficiently low, it can give you an advantage.",
                    "label": 1
                },
                {
                    "sent": "In particular, if you're interested in cases with very few basis functions, you tend to do quite well.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, this this right hand side figure is a depiction in of how much the individual basis function, which vary from those of from that of the covariance function.",
                    "label": 1
                },
                {
                    "sent": "So as you can see is you increase the number of basis functions.",
                    "label": 1
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Their widths tend to vary less, which is not surprising.",
                    "label": 1
                },
                {
                    "sent": "Of course, if you have as many basis functions as data points.",
                    "label": 0
                },
                {
                    "sent": "Then there will be the optimal solution would be to set all of their words to that of the covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to just advertise that this.",
                    "label": 0
                },
                {
                    "sent": "So this kind of approach is useful for any kernel machine, so it's really simple to apply it.",
                    "label": 0
                },
                {
                    "sent": "Normally so if you have an arcade chess H with Gaussian kernel and you're minimizing something of this form.",
                    "label": 0
                },
                {
                    "sent": "The key trick is that if you want to approximate your function as a multi scale expansion as before, it's possible to compute the norm in closed form of this mixture.",
                    "label": 0
                },
                {
                    "sent": "In particular, it becomes simply a matrix vector matrix, vector matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "And the matrix is again given by this collection of.",
                    "label": 0
                },
                {
                    "sent": "Funny terms, Sigma Alpha Sigma Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have a video which which visualizes the optimization process for the support vector machine case.",
                    "label": 1
                },
                {
                    "sent": "So what we've done here is.",
                    "label": 0
                },
                {
                    "sent": "We've optimized the SVM objective function with a reduced number of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Initially we set their widths to be fixed to that of the kernel of the arcade chess and optimize their positions, and then after convergence we then remove the restriction that their width should be the same as that of the covariance function and continue optimization, which gives you some idea of what this can buy you.",
                    "label": 0
                },
                {
                    "sent": "OK, so right now the widths of fixed and their denoted by these lengths of these crosses.",
                    "label": 0
                },
                {
                    "sent": "The data points are given here for the positive class in the negative course.",
                    "label": 0
                },
                {
                    "sent": "Now the widths have been released and you can see.",
                    "label": 0
                },
                {
                    "sent": "The length scale of each dimension given by the black line.",
                    "label": 0
                },
                {
                    "sent": "And the length scale of the actual kernel of the OK chest is given by the white line in the cross.",
                    "label": 0
                },
                {
                    "sent": "So basically by having this extra degree of freedom, it's possible, for example, to have one basis function covered.",
                    "label": 1
                },
                {
                    "sent": "The territory here, which would otherwise require several basis functions.",
                    "label": 1
                },
                {
                    "sent": "And you can also see that the objective function gets considerably better.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the frame rate of the video seems to be slower than what it should be.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's basically converged.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This basically concludes the talk, I'll just give a quick summary.",
                    "label": 0
                },
                {
                    "sent": "We've seen how to approximate a Gaussian kernel machine using a function basis, which is a set of Gaussians about tree length scales.",
                    "label": 1
                },
                {
                    "sent": "This generalizes basically all the vast majority of previous methods, hence it's at least as good as them.",
                    "label": 0
                },
                {
                    "sent": "It's particularly good if you want short test times.",
                    "label": 1
                },
                {
                    "sent": "Basically, you're sacrificing training time for test time because you have a more complex optimization problem, But it results in a solution with less basis functions.",
                    "label": 0
                },
                {
                    "sent": "If the input dimension of these lower you've got the chance for a better improvement, and finally it's possible to apply this idea to other combinations of covariance functions and basis functions.",
                    "label": 0
                },
                {
                    "sent": "But to do so requires solving some integrals which may not be available in closed form.",
                    "label": 0
                },
                {
                    "sent": "I can say that I have in fact done it for the case of the Laplace kernel.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Qua",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agents so I have a question too.",
                    "label": 0
                },
                {
                    "sent": "Tying a little bit to the previous talk, we've seen that Oliver, compared with Zubin and EDS.",
                    "label": 0
                },
                {
                    "sent": "Spot parametric Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Do you think there's also scope here that although that in the 1st place was designed as a sparse approximation, I lent itself to nonstationary kernels.",
                    "label": 0
                },
                {
                    "sent": "You get the same effects with this type of kernel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, indeed you do.",
                    "label": 0
                },
                {
                    "sent": "We have some experiments have shown that in fact these past method can outperform the full GP.",
                    "label": 0
                },
                {
                    "sent": "Although initially we thought of this is in fact just an approximation to the full GP, but naturally you can think of the basis centers and widths as.",
                    "label": 0
                },
                {
                    "sent": "Hyperparameters of your covariance function it induces basically a different covariance function of a full GP and hence if you tune these, you could possibly do better, yes?",
                    "label": 0
                },
                {
                    "sent": "We have to stand here for 4 minutes if they will ask the question.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "The problem is we have a lot of information, so the question was what is the problem when you have a lot of input dimensions?",
                    "label": 0
                },
                {
                    "sent": "What happens is this.",
                    "label": 0
                },
                {
                    "sent": "So we have this term, the Gaussian between the two basis function centers with Bandwidth Sigma I plus Sigma J minus Sigma.",
                    "label": 0
                },
                {
                    "sent": "But there's also a normalization term, and this normalization term depends on the width on the effective bandwidth, namely Sigma, Alpha, Sigma, Gamma, Sigma, and if you check, it turns out that.",
                    "label": 0
                },
                {
                    "sent": "As you as the dimensionality increases, this quantity increases and effectively what it means is that.",
                    "label": 0
                },
                {
                    "sent": "If you think of it from the arcade, just perspective, the norm of these functions is very large, so the norm of a Gaussian which is twice as broad as the kernel function in a high dimensional space is extremely high.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's extremely unlikely under the GP prior.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hidden because you have to think about the normalization term in the definition of this Gaussian G. That's basically the problem.",
                    "label": 0
                },
                {
                    "sent": "Don't.",
                    "label": 0
                },
                {
                    "sent": "Just to make this a normalization problem solving problem.",
                    "label": 0
                },
                {
                    "sent": "I guess my what's happening.",
                    "label": 0
                },
                {
                    "sent": "Goodnight space factor kinda spread outside the main points I'd.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing you're spending your Internet domains are outside your points at that time period.",
                    "label": 0
                },
                {
                    "sent": "It was through the Gaussian register.",
                    "label": 0
                },
                {
                    "sent": "Very small service in the cap lock, so that's the reason we're seeing reasonably after three problematic rules.",
                    "label": 0
                },
                {
                    "sent": "Simply excellent, but much more ringing the model.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily, but I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's quite possible.",
                    "label": 0
                },
                {
                    "sent": "You can also think of it though in the following way.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you have a very broad Gaussian, then to compute the norm of it you could represent it as some kind of possibly infinite sum of Gaussians which are narrower and then OK if as the function gets broader you clearly need more of these small basis functions to represent it.",
                    "label": 0
                },
                {
                    "sent": "So actually the normal is certainly increasing but.",
                    "label": 0
                },
                {
                    "sent": "Probably probably not.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Just do something.",
                    "label": 0
                },
                {
                    "sent": "Stop typing OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Having a more regular.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "Basically in general it's difficult to construct a good finite dimensional basis for high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You kind of guaranteed to run into problems.",
                    "label": 0
                },
                {
                    "sent": "Good, OK, if there's no more questions, we can just think Christian again.",
                    "label": 0
                }
            ]
        }
    }
}