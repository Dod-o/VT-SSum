{
    "id": "hhdx6sc6dxg4gs5tmhlemedsanfj4qfm",
    "title": "Statistical Models for Partial Membership",
    "info": {
        "author": [
            "Katherine A. Heller, Department of Statistical Science, Duke University"
        ],
        "published": "Aug. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/icml08_heller_smpm/",
    "segmentation": [
        [
            "So as you said, I'll be talking bout statistical models for partial membership.",
            "Make a song.",
            "Is that OK?",
            "OK, so I'll be talking about statistical models for partial membership and this is joint work with Sinead Williamson and Zubin Ghahremani."
        ],
        [
            "So the goal of this work is really to generalize clustering and allow data points to have partial membership in multiple clusters, and so the running example of partial membership that I'm going to be using throughout this talk is a person with a mixed ethnic background, so someone who's 50% Asian and 50% European partly belongs to two different groups or ethnicities.",
            "An this partial membership can be relevant for predicting things about this person.",
            "Like their phenotype or their food preferences.",
            "Conceptually, this isn't the same as uncertain membership.",
            "Being certain that someone is half Asian and half European is very different than being unsure about their ethnicity.",
            "So things like DNA tests can be used to help resolve issues with uncertainty, but it's not going to change their ethnic ethnic memberships.",
            "There's been a lot of work on modeling partial membership by the physiological."
        ],
        [
            "City.",
            "But the goal of this talk is to describe a fully probabilistic Bayesian approach to data modeling with partial memberships, and so first I'm going to start by describing our Bayesian partial membership model.",
            "Talk about learning a little bit in that model, and then show you some experimental results."
        ],
        [
            "So the place we start on our quest for a pressure membership model is with a standard finite mixture model.",
            "So consider modeling a data set of N data points X one through XN, using a standard finite mixture model with K components.",
            "So for example, here we have a data set which is being modeled using a mixture of six Gaussians.",
            "And so the generative process for for the standard finite mixture model is.",
            "First we choose one of these clusters, or one of these Gaussians and then we generated data data points from it conditioned on the on the particular parameters of the cluster of the cluster that we've chosen.",
            "And so we can compute the probability of a data point XN conditioned on the model parameters by summing over each one of these of these clusters in our mixture model of the prior probability of the data point coming from that particular cluster times the probability of the data point under that particular cluster conditioned on that clusters particular model parameters, and so we can write this expression out here like this using some latent.",
            "Indicator variables \u03c0 N and supply an is K length vector equal to the number of mixture components and it's a vector of all zeros in one one and so high.",
            "NK is equal to 1.",
            "When data Point N belongs to cluster K. So what we see is that \u03c0 and Cade denotes memberships of data points to clusters.",
            "And so in the next slide I'm just going to take all these equations."
        ],
        [
            "Here and rewrite them.",
            "OK, so in order to get a partial membership model, what we can do is try to relax this constraint that pion care has to take on a value of either zero or one and let it take on any real value in the range between zero and one and so now pine K denotes partial memberships of data points to clusters, but we have a couple of issues that we need to clean up in terms of the equation for our model.",
            "So we can't any longer some over the values of \u03c0, and we need to integrate it.",
            "And also this this product here no longer normalizes, so we need to add in a normalizing constant C which.",
            "Depends on Theta and \u03c0. OK, so this basically gives us a partial membership model of that that we're going to use."
        ],
        [
            "For the rest of the talk, but.",
            "Why?",
            "What does this make sense as as as a model for partial membership?",
            "OK, so if we look 1st at A at a normal mixture model.",
            "So here we have a mixture model with two clusters, blue Gaussian and a green Gaussian.",
            "If we're going to generate data from this model right?",
            "All of our data is going to either be generated from this blue Gaussian or be generated from the screen Gaussian, whereas with the partial membership model that I just described to you with the same two Gaussian clusters.",
            "Data can be can be generated from the blue Gaussian.",
            "It can be generated from the green Gaussian.",
            "It can be generated from some gas in between, which represents having half membership in the blue cluster and half membership in the green cluster.",
            "Or can be generated from one of an infinite number of Gaussians which lie in between our blue and green cluster.",
            "And so if we have like an Asian cluster and a European cluster, the partial membership model is going to better capture people with mixed ethnicity who have features which lies somewhere in between those of the original two ethnicity clusters."
        ],
        [
            "OK, so on this side I'm going to talk about the distribution of PK, which is the distribution of our original clusters.",
            "So for the rest of the talk we're going to be considering the case where our clusters are having exponential family family distribution.",
            "So basically this can be written with this expression here, where SFX are sufficient.",
            "Statistics and data are the natural parameters.",
            "And so if we plug this into our partial membership model equation, it follows that a data point XN conditioned on its partial membership vector Pi and the model parameters Theta has the same exponential family distribution as the original clusters but with a new set of natural parameters, which is just a convex combination of the natural parameters of our original clusters.",
            "Weighted by \u03c0 and K, the partial memberships of that data point to those clusters.",
            "And this makes makes it tractable to compute that normalizing constant.",
            "OK, and since we want to want a Bayesian model, we're also going to do things like like put a prior over over our model parameters."
        ],
        [
            "Alright, so.",
            "So this slide gives the generative process for our Bayesian partial membership model and the outline of the generative process is sort of on the left side.",
            "And what I'm going to do is go through each step and go through each variable and then try to draw an analogy to the to the ethnicity example that we've been talking about in order to try to make these variables slightly slightly more intuitive.",
            "OK, so for each cluster K. So so for each ethnic group, we're going to draw Theta K, which are the parameters for that cluster.",
            "And so this is going to define in our ethnicity example, this is going to define a distribution over features for each of the K ethnic groups.",
            "So if we're looking at things like food preferences for ethnic groups, right?",
            "This is going to say how much each ethnic group likes pizza or Marmite.",
            "Or bindi bhaji.",
            "OK, and then the next parameter is.",
            "Next time we're going to Israel, which comes from Additionally distribution an, this is going to define the ethnic composition of our population.",
            "So if we're looking at a population that has ethnic groups, white, British and Pakistani, maybe this could tell us that our population is 75% white British and 25% Pakistani.",
            "Let me draw variable A from, which is exponentially distributed.",
            "And this is going to control how similar to the population we expect an individual to be.",
            "So do we think that our entire population is themselves 75% white, British and 25% Pakistani?",
            "Or do we think that 75% of our population is entirely white British and 25% of our population is entirely Pakistani?",
            "Or do we think that the distribution lies somewhere in between?",
            "And then for each individual N. We draw pie in our partial membership vector from Additionally distribution parameters by 8 times row, and this gives the ethnic composition of a particular individual.",
            "And then we draw.",
            "XN from Jackson from, uh, an exponential family distribution with parameters which are that convex combination of natural parameters that which I discussed on on the previous slide, and basically this gives the feature values for particular individuals.",
            "So how much that individual themselves like likes pizza or Marmot urban ibaji?"
        ],
        [
            "And this is the third graphical model which wants to to the model that we just described."
        ],
        [
            "OK, so we can.",
            "Generate data from our Bayesian partial membership model as both the four plots shown here are 3000 data points which are drawn from the BPM with the same 3 full covariance Gaussian clusters which are given in red and we've shown them as we valued the variable A and what we can notice is that when a is very small, the data points tend to have mostly membership in one one of the clusters, and as a gets quite large all the data points.",
            "Seem to kind of come from the same Gaussian which lies somewhere in between our three clusters."
        ],
        [
            "And in fact, we can prove that in the limit as X goes to zero, the exponential family BPM is a mixture of components with mixing proportions Ro Ann in the limit is a goes to Infinity.",
            "The exponential family BPM has only a single component with natural parameters which are convex combination of the cluster cluster parameters of our clusters weighted by row."
        ],
        [
            "And then to do learning.",
            "In this model, we'd like to infer all our own unknowns given by Omega.",
            "Conditioned on our data and we try to fix that hyperparameters, and so we'd like to infer the probability of unknowns in mega given our data X and our hyperparameters.",
            "But we can't compute this exactly, it's intractable, so we use MCMC.",
            "But since all of the parameters and in the BPM or continuous, we can use hybrid Monte Carlo to do to do inference and basically hybrid Monte Carlo's and efficient MCMC method which uses gradient information to find high probability regions."
        ],
        [
            "OK, so the first the first data set that we're in our model on is is a synthetic data set.",
            "So we generated synthetic binary data.",
            "50 points, 32 dimensions, and three clusters.",
            "We ran our hybrid Monte Carlo sample sampler for 4000 iterations and we computed are these matrices, and these matrices are of the form pipi transpose, where Pi is our partial membership matrix that partial membership vector for each data point in our data set an intuitively what pipe I transposed?",
            "Tell us it tells us is the amount of shared partial membership between.",
            "Each pair pair of data points in our data set.",
            "So we compute you, which is \u03c0 transpose.",
            "Four are in Ferd by matrices and you star which is \u03c0 Pi transpose for the true generated by matrix.",
            "And so hopefully you can see that the algorithm is pretty good at recovering the structure."
        ],
        [
            "The next week we ran the business partial membership model on is is central called data set and so central called data set has 633 votes from the years 2001 from the year 2000.",
            "One 2002 is the 633 votes of 99 senator and there's also an outcome variable which says which way the vote actually went.",
            "We modeled this using two multivariate Bernoulli clusters and we adopted the model to handle missing handle missing data, 'cause often senators don't actually vote.",
            "All the time.",
            "Anne Anne so.",
            "I don't know how well you guys can see this, but sort of the interesting thing to note is the most Democrat like Republican Senator, Senator Lincoln Chafee, who was known for being extremely extremely liberal Republican.",
            "The outcome variable is also like quite close to the middle, so so I mean, mostly, we're finding like most Republican cluster at one end and a Democrat cluster at the other end.",
            "And most of the Senators fall into one of those two clusters.",
            "But of the ones in between, the outcome variable lies in the middle, which sort of makes a lot of sense for the year 2000, one 2002 because.",
            "In that year, the sun.",
            "It started off being split exactly 5050 Republicans and Democrats with with Republican Dick Cheney breaking ties and then Senator Senator Jim Jeffords, who was a Republican defected from the Republican Party, became an independent, became independent and started caucusing with Democrats.",
            "And so Jim Jeffords is shown here.",
            "And so the Democrats, and had a majority of 1."
        ],
        [
            "So we compared.",
            "We compared our algorithm to fuzzy K means, but the communes basically tries to minimize that cost function there, where D squared gives gives the squared distance between a data point XN and a cluster center.",
            "CK and gamma is.",
            "A.",
            "Is a fuzzy exponent, which basically is set by the user and kind of tries to control how much partial membership we expect to see in our data set.",
            "And so the actual ranking of senators is not usually different, But the interesting thing to notice is if we look at the value of the partial memberships as we vary this fuzzy exponent gamma, we can see that.",
            "If we plot in Blue the most Democrat like Senator in Red, the most Republican senator and black the outcome variable which we remember was somewhere in between.",
            "As we've very value of gamma as the outcome variable starts to reach a reasonable value of 1/2, right are most most severe.",
            "Republican is already 20% of Democrat in our most severe Democrat is already 15% of Republican and so not only are the partial membership values very sensitive to this, to this fuzzy exponent gamma.",
            "But also for no value of gamma, do the membership values actually make a lot of sense?"
        ],
        [
            "We also compared to.",
            "Dear Ashley, process mixture models because you guys think that Oh well, you know maybe I'll just model this within a normal mixture model and if I have partial membership then there will be uncertainty and I can just use that uncertainty in place of actually modeling the partial membership and so the Dursley process mixture model confidently infers that there are four clusters in the data set.",
            "Basically, this just shows that we can't really use uncertainty as a good substitute for partial membership in order to further demonstrate this we.",
            "Computed the negative log predictive probability and bits across senators, and we compared the BPM in the DPM by several different by several different measures.",
            "Mean number of bits, median, min, Max and the number of bits for the outcome variable.",
            "And basically what this shows is that the VPM outperforms the judicial process mixture model in by almost."
        ],
        [
            "Measures and the last data set that we ran our model on was an image data set with 329 tower and sunset images and 240 simple binary texture and color features, and we had two clusters and the top program which is basically shows the most sunset like like images.",
            "The bottom row shows and most tower like images and then the ones in between show the ones with the most sort of partial membership in both in both clusters."
        ],
        [
            "There's a bunch of related work late and early allocation membership models, fuzzy clustering, and exponential family PCA.",
            "I don't really have time to go into any of those during the talk, but if you have any questions about them you can ask me later at my poster."
        ],
        [
            "And so in the future it would really be nice to have a nonparametric version of this of this model an the obvious thing to try would be a hierarchical during the process, But this would require summing over infinitely many elements of \u03c0, and so this isn't computationally feasible.",
            "It's also not semantically very nice, because we don't really believe that each data point has an infinitesimal amount of membership in infinitely many clusters, so an Indian buffet process using it in doing buffet process might work better.",
            "We could sample in ICP matrix with the interpretation that A1 means having some non zero amount of membership in that cluster and then draw the continuous exact amount separately."
        ],
        [
            "OK, so in conclusion, we've developed a fully probabilistic approach to data modeling with partial membership.",
            "It uses continuously in variables and can be seen as a relaxation of clustering with standard mixture models, and we use hybrid Monte Carlo for inference, which was extremely fast.",
            "Thanks.",
            "Any questions?",
            "What happens if the number of clusters gets larger?",
            "Is the biggest one.",
            "Yeah, so I mean, I think I think I had an example, maybe with."
        ],
        [
            "With three at some point.",
            "But yeah, I mean the principles is the same, right?",
            "Computationally, it'll it'll get more expensive, but yeah.",
            "I think I'd have to.",
            "I'd have to actually think about it.",
            "Anymore questions.",
            "So it means related.",
            "It's a related computational question.",
            "I'm assuming fuzzy K means is fast.",
            "I don't know how it works, but could you initialize your method from its?",
            "Prediction sure.",
            "Sure you could do that.",
            "I mean, we haven't tried that, but this is actually I mean.",
            "It was."
        ],
        [
            "And it wasn't that this method wasn't wasn't actually that slow when you use hybrid Monte Carlo, it's like you do actually get get quite reasonable results very quickly.",
            "But yeah, you could.",
            "Certainly you could certainly do that.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as you said, I'll be talking bout statistical models for partial membership.",
                    "label": 0
                },
                {
                    "sent": "Make a song.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll be talking about statistical models for partial membership and this is joint work with Sinead Williamson and Zubin Ghahremani.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal of this work is really to generalize clustering and allow data points to have partial membership in multiple clusters, and so the running example of partial membership that I'm going to be using throughout this talk is a person with a mixed ethnic background, so someone who's 50% Asian and 50% European partly belongs to two different groups or ethnicities.",
                    "label": 0
                },
                {
                    "sent": "An this partial membership can be relevant for predicting things about this person.",
                    "label": 0
                },
                {
                    "sent": "Like their phenotype or their food preferences.",
                    "label": 0
                },
                {
                    "sent": "Conceptually, this isn't the same as uncertain membership.",
                    "label": 0
                },
                {
                    "sent": "Being certain that someone is half Asian and half European is very different than being unsure about their ethnicity.",
                    "label": 1
                },
                {
                    "sent": "So things like DNA tests can be used to help resolve issues with uncertainty, but it's not going to change their ethnic ethnic memberships.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work on modeling partial membership by the physiological.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City.",
                    "label": 0
                },
                {
                    "sent": "But the goal of this talk is to describe a fully probabilistic Bayesian approach to data modeling with partial memberships, and so first I'm going to start by describing our Bayesian partial membership model.",
                    "label": 1
                },
                {
                    "sent": "Talk about learning a little bit in that model, and then show you some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the place we start on our quest for a pressure membership model is with a standard finite mixture model.",
                    "label": 0
                },
                {
                    "sent": "So consider modeling a data set of N data points X one through XN, using a standard finite mixture model with K components.",
                    "label": 1
                },
                {
                    "sent": "So for example, here we have a data set which is being modeled using a mixture of six Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And so the generative process for for the standard finite mixture model is.",
                    "label": 0
                },
                {
                    "sent": "First we choose one of these clusters, or one of these Gaussians and then we generated data data points from it conditioned on the on the particular parameters of the cluster of the cluster that we've chosen.",
                    "label": 0
                },
                {
                    "sent": "And so we can compute the probability of a data point XN conditioned on the model parameters by summing over each one of these of these clusters in our mixture model of the prior probability of the data point coming from that particular cluster times the probability of the data point under that particular cluster conditioned on that clusters particular model parameters, and so we can write this expression out here like this using some latent.",
                    "label": 0
                },
                {
                    "sent": "Indicator variables \u03c0 N and supply an is K length vector equal to the number of mixture components and it's a vector of all zeros in one one and so high.",
                    "label": 0
                },
                {
                    "sent": "NK is equal to 1.",
                    "label": 1
                },
                {
                    "sent": "When data Point N belongs to cluster K. So what we see is that \u03c0 and Cade denotes memberships of data points to clusters.",
                    "label": 0
                },
                {
                    "sent": "And so in the next slide I'm just going to take all these equations.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here and rewrite them.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to get a partial membership model, what we can do is try to relax this constraint that pion care has to take on a value of either zero or one and let it take on any real value in the range between zero and one and so now pine K denotes partial memberships of data points to clusters, but we have a couple of issues that we need to clean up in terms of the equation for our model.",
                    "label": 1
                },
                {
                    "sent": "So we can't any longer some over the values of \u03c0, and we need to integrate it.",
                    "label": 0
                },
                {
                    "sent": "And also this this product here no longer normalizes, so we need to add in a normalizing constant C which.",
                    "label": 0
                },
                {
                    "sent": "Depends on Theta and \u03c0. OK, so this basically gives us a partial membership model of that that we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the rest of the talk, but.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "What does this make sense as as as a model for partial membership?",
                    "label": 0
                },
                {
                    "sent": "OK, so if we look 1st at A at a normal mixture model.",
                    "label": 0
                },
                {
                    "sent": "So here we have a mixture model with two clusters, blue Gaussian and a green Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If we're going to generate data from this model right?",
                    "label": 0
                },
                {
                    "sent": "All of our data is going to either be generated from this blue Gaussian or be generated from the screen Gaussian, whereas with the partial membership model that I just described to you with the same two Gaussian clusters.",
                    "label": 0
                },
                {
                    "sent": "Data can be can be generated from the blue Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It can be generated from the green Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It can be generated from some gas in between, which represents having half membership in the blue cluster and half membership in the green cluster.",
                    "label": 0
                },
                {
                    "sent": "Or can be generated from one of an infinite number of Gaussians which lie in between our blue and green cluster.",
                    "label": 0
                },
                {
                    "sent": "And so if we have like an Asian cluster and a European cluster, the partial membership model is going to better capture people with mixed ethnicity who have features which lies somewhere in between those of the original two ethnicity clusters.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so on this side I'm going to talk about the distribution of PK, which is the distribution of our original clusters.",
                    "label": 0
                },
                {
                    "sent": "So for the rest of the talk we're going to be considering the case where our clusters are having exponential family family distribution.",
                    "label": 1
                },
                {
                    "sent": "So basically this can be written with this expression here, where SFX are sufficient.",
                    "label": 0
                },
                {
                    "sent": "Statistics and data are the natural parameters.",
                    "label": 0
                },
                {
                    "sent": "And so if we plug this into our partial membership model equation, it follows that a data point XN conditioned on its partial membership vector Pi and the model parameters Theta has the same exponential family distribution as the original clusters but with a new set of natural parameters, which is just a convex combination of the natural parameters of our original clusters.",
                    "label": 0
                },
                {
                    "sent": "Weighted by \u03c0 and K, the partial memberships of that data point to those clusters.",
                    "label": 0
                },
                {
                    "sent": "And this makes makes it tractable to compute that normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "OK, and since we want to want a Bayesian model, we're also going to do things like like put a prior over over our model parameters.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So this slide gives the generative process for our Bayesian partial membership model and the outline of the generative process is sort of on the left side.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to do is go through each step and go through each variable and then try to draw an analogy to the to the ethnicity example that we've been talking about in order to try to make these variables slightly slightly more intuitive.",
                    "label": 0
                },
                {
                    "sent": "OK, so for each cluster K. So so for each ethnic group, we're going to draw Theta K, which are the parameters for that cluster.",
                    "label": 0
                },
                {
                    "sent": "And so this is going to define in our ethnicity example, this is going to define a distribution over features for each of the K ethnic groups.",
                    "label": 1
                },
                {
                    "sent": "So if we're looking at things like food preferences for ethnic groups, right?",
                    "label": 0
                },
                {
                    "sent": "This is going to say how much each ethnic group likes pizza or Marmite.",
                    "label": 0
                },
                {
                    "sent": "Or bindi bhaji.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the next parameter is.",
                    "label": 0
                },
                {
                    "sent": "Next time we're going to Israel, which comes from Additionally distribution an, this is going to define the ethnic composition of our population.",
                    "label": 0
                },
                {
                    "sent": "So if we're looking at a population that has ethnic groups, white, British and Pakistani, maybe this could tell us that our population is 75% white British and 25% Pakistani.",
                    "label": 0
                },
                {
                    "sent": "Let me draw variable A from, which is exponentially distributed.",
                    "label": 1
                },
                {
                    "sent": "And this is going to control how similar to the population we expect an individual to be.",
                    "label": 0
                },
                {
                    "sent": "So do we think that our entire population is themselves 75% white, British and 25% Pakistani?",
                    "label": 0
                },
                {
                    "sent": "Or do we think that 75% of our population is entirely white British and 25% of our population is entirely Pakistani?",
                    "label": 0
                },
                {
                    "sent": "Or do we think that the distribution lies somewhere in between?",
                    "label": 0
                },
                {
                    "sent": "And then for each individual N. We draw pie in our partial membership vector from Additionally distribution parameters by 8 times row, and this gives the ethnic composition of a particular individual.",
                    "label": 0
                },
                {
                    "sent": "And then we draw.",
                    "label": 0
                },
                {
                    "sent": "XN from Jackson from, uh, an exponential family distribution with parameters which are that convex combination of natural parameters that which I discussed on on the previous slide, and basically this gives the feature values for particular individuals.",
                    "label": 0
                },
                {
                    "sent": "So how much that individual themselves like likes pizza or Marmot urban ibaji?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the third graphical model which wants to to the model that we just described.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we can.",
                    "label": 0
                },
                {
                    "sent": "Generate data from our Bayesian partial membership model as both the four plots shown here are 3000 data points which are drawn from the BPM with the same 3 full covariance Gaussian clusters which are given in red and we've shown them as we valued the variable A and what we can notice is that when a is very small, the data points tend to have mostly membership in one one of the clusters, and as a gets quite large all the data points.",
                    "label": 1
                },
                {
                    "sent": "Seem to kind of come from the same Gaussian which lies somewhere in between our three clusters.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, we can prove that in the limit as X goes to zero, the exponential family BPM is a mixture of components with mixing proportions Ro Ann in the limit is a goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The exponential family BPM has only a single component with natural parameters which are convex combination of the cluster cluster parameters of our clusters weighted by row.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then to do learning.",
                    "label": 0
                },
                {
                    "sent": "In this model, we'd like to infer all our own unknowns given by Omega.",
                    "label": 0
                },
                {
                    "sent": "Conditioned on our data and we try to fix that hyperparameters, and so we'd like to infer the probability of unknowns in mega given our data X and our hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "But we can't compute this exactly, it's intractable, so we use MCMC.",
                    "label": 0
                },
                {
                    "sent": "But since all of the parameters and in the BPM or continuous, we can use hybrid Monte Carlo to do to do inference and basically hybrid Monte Carlo's and efficient MCMC method which uses gradient information to find high probability regions.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the first the first data set that we're in our model on is is a synthetic data set.",
                    "label": 0
                },
                {
                    "sent": "So we generated synthetic binary data.",
                    "label": 1
                },
                {
                    "sent": "50 points, 32 dimensions, and three clusters.",
                    "label": 0
                },
                {
                    "sent": "We ran our hybrid Monte Carlo sample sampler for 4000 iterations and we computed are these matrices, and these matrices are of the form pipi transpose, where Pi is our partial membership matrix that partial membership vector for each data point in our data set an intuitively what pipe I transposed?",
                    "label": 1
                },
                {
                    "sent": "Tell us it tells us is the amount of shared partial membership between.",
                    "label": 0
                },
                {
                    "sent": "Each pair pair of data points in our data set.",
                    "label": 0
                },
                {
                    "sent": "So we compute you, which is \u03c0 transpose.",
                    "label": 0
                },
                {
                    "sent": "Four are in Ferd by matrices and you star which is \u03c0 Pi transpose for the true generated by matrix.",
                    "label": 0
                },
                {
                    "sent": "And so hopefully you can see that the algorithm is pretty good at recovering the structure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next week we ran the business partial membership model on is is central called data set and so central called data set has 633 votes from the years 2001 from the year 2000.",
                    "label": 0
                },
                {
                    "sent": "One 2002 is the 633 votes of 99 senator and there's also an outcome variable which says which way the vote actually went.",
                    "label": 0
                },
                {
                    "sent": "We modeled this using two multivariate Bernoulli clusters and we adopted the model to handle missing handle missing data, 'cause often senators don't actually vote.",
                    "label": 1
                },
                {
                    "sent": "All the time.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne so.",
                    "label": 0
                },
                {
                    "sent": "I don't know how well you guys can see this, but sort of the interesting thing to note is the most Democrat like Republican Senator, Senator Lincoln Chafee, who was known for being extremely extremely liberal Republican.",
                    "label": 0
                },
                {
                    "sent": "The outcome variable is also like quite close to the middle, so so I mean, mostly, we're finding like most Republican cluster at one end and a Democrat cluster at the other end.",
                    "label": 0
                },
                {
                    "sent": "And most of the Senators fall into one of those two clusters.",
                    "label": 0
                },
                {
                    "sent": "But of the ones in between, the outcome variable lies in the middle, which sort of makes a lot of sense for the year 2000, one 2002 because.",
                    "label": 0
                },
                {
                    "sent": "In that year, the sun.",
                    "label": 0
                },
                {
                    "sent": "It started off being split exactly 5050 Republicans and Democrats with with Republican Dick Cheney breaking ties and then Senator Senator Jim Jeffords, who was a Republican defected from the Republican Party, became an independent, became independent and started caucusing with Democrats.",
                    "label": 0
                },
                {
                    "sent": "And so Jim Jeffords is shown here.",
                    "label": 0
                },
                {
                    "sent": "And so the Democrats, and had a majority of 1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we compared.",
                    "label": 0
                },
                {
                    "sent": "We compared our algorithm to fuzzy K means, but the communes basically tries to minimize that cost function there, where D squared gives gives the squared distance between a data point XN and a cluster center.",
                    "label": 0
                },
                {
                    "sent": "CK and gamma is.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Is a fuzzy exponent, which basically is set by the user and kind of tries to control how much partial membership we expect to see in our data set.",
                    "label": 0
                },
                {
                    "sent": "And so the actual ranking of senators is not usually different, But the interesting thing to notice is if we look at the value of the partial memberships as we vary this fuzzy exponent gamma, we can see that.",
                    "label": 0
                },
                {
                    "sent": "If we plot in Blue the most Democrat like Senator in Red, the most Republican senator and black the outcome variable which we remember was somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "As we've very value of gamma as the outcome variable starts to reach a reasonable value of 1/2, right are most most severe.",
                    "label": 0
                },
                {
                    "sent": "Republican is already 20% of Democrat in our most severe Democrat is already 15% of Republican and so not only are the partial membership values very sensitive to this, to this fuzzy exponent gamma.",
                    "label": 0
                },
                {
                    "sent": "But also for no value of gamma, do the membership values actually make a lot of sense?",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also compared to.",
                    "label": 0
                },
                {
                    "sent": "Dear Ashley, process mixture models because you guys think that Oh well, you know maybe I'll just model this within a normal mixture model and if I have partial membership then there will be uncertainty and I can just use that uncertainty in place of actually modeling the partial membership and so the Dursley process mixture model confidently infers that there are four clusters in the data set.",
                    "label": 0
                },
                {
                    "sent": "Basically, this just shows that we can't really use uncertainty as a good substitute for partial membership in order to further demonstrate this we.",
                    "label": 1
                },
                {
                    "sent": "Computed the negative log predictive probability and bits across senators, and we compared the BPM in the DPM by several different by several different measures.",
                    "label": 0
                },
                {
                    "sent": "Mean number of bits, median, min, Max and the number of bits for the outcome variable.",
                    "label": 0
                },
                {
                    "sent": "And basically what this shows is that the VPM outperforms the judicial process mixture model in by almost.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Measures and the last data set that we ran our model on was an image data set with 329 tower and sunset images and 240 simple binary texture and color features, and we had two clusters and the top program which is basically shows the most sunset like like images.",
                    "label": 0
                },
                {
                    "sent": "The bottom row shows and most tower like images and then the ones in between show the ones with the most sort of partial membership in both in both clusters.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a bunch of related work late and early allocation membership models, fuzzy clustering, and exponential family PCA.",
                    "label": 0
                },
                {
                    "sent": "I don't really have time to go into any of those during the talk, but if you have any questions about them you can ask me later at my poster.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so in the future it would really be nice to have a nonparametric version of this of this model an the obvious thing to try would be a hierarchical during the process, But this would require summing over infinitely many elements of \u03c0, and so this isn't computationally feasible.",
                    "label": 1
                },
                {
                    "sent": "It's also not semantically very nice, because we don't really believe that each data point has an infinitesimal amount of membership in infinitely many clusters, so an Indian buffet process using it in doing buffet process might work better.",
                    "label": 1
                },
                {
                    "sent": "We could sample in ICP matrix with the interpretation that A1 means having some non zero amount of membership in that cluster and then draw the continuous exact amount separately.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion, we've developed a fully probabilistic approach to data modeling with partial membership.",
                    "label": 1
                },
                {
                    "sent": "It uses continuously in variables and can be seen as a relaxation of clustering with standard mixture models, and we use hybrid Monte Carlo for inference, which was extremely fast.",
                    "label": 1
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "What happens if the number of clusters gets larger?",
                    "label": 0
                },
                {
                    "sent": "Is the biggest one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean, I think I think I had an example, maybe with.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With three at some point.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I mean the principles is the same, right?",
                    "label": 0
                },
                {
                    "sent": "Computationally, it'll it'll get more expensive, but yeah.",
                    "label": 0
                },
                {
                    "sent": "I think I'd have to.",
                    "label": 0
                },
                {
                    "sent": "I'd have to actually think about it.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "So it means related.",
                    "label": 0
                },
                {
                    "sent": "It's a related computational question.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming fuzzy K means is fast.",
                    "label": 0
                },
                {
                    "sent": "I don't know how it works, but could you initialize your method from its?",
                    "label": 0
                },
                {
                    "sent": "Prediction sure.",
                    "label": 0
                },
                {
                    "sent": "Sure you could do that.",
                    "label": 0
                },
                {
                    "sent": "I mean, we haven't tried that, but this is actually I mean.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it wasn't that this method wasn't wasn't actually that slow when you use hybrid Monte Carlo, it's like you do actually get get quite reasonable results very quickly.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you could.",
                    "label": 0
                },
                {
                    "sent": "Certainly you could certainly do that.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}