{
    "id": "pg4v4f4h3y6hirbip3ryru5tqn35drwv",
    "title": "MoT - Mixture of Trees Probabilistic Graphical Model for Video Segmentation",
    "info": {
        "author": [
            "Ignas Budvytis, University of Cambridge"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision->Video Analysis"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_budvytis_video_segmentation/",
    "segmentation": [
        [
            "This is joint work with Doctor Vijay, but in the Ryan Budget and Narayanan professor, better Chipola.",
            "Out of research in computer vision, it relies on availability on vast amounts of ground truth label data.",
            "However, obtaining such data can be a very tedious."
        ],
        [
            "Task for example, labeling such a simple video might result in more than 30 minutes of labeling and even worse."
        ],
        [
            "Labeling of more complex video like this might result into 30 minutes of labeling for one frame, which means you would spend hours of labeling for just one minute of ground truth data.",
            "Of course, videos are not sets of random images and this inherent similarity between subsequent frames as well as objects in distance frames should be used, exploited.",
            "So for that we create a probabilistic graph."
        ],
        [
            "The model for the temporal structure of the video and it works in this particular way.",
            "So we have a video as our input."
        ],
        [
            "We then have very sparse labels, so we typically we have either starting frame labeled or start and end frame labeled or only some user private mouse brushstrokes.",
            "Then we iterate between temporal label propagation and semi supervised learning of both units of labels as well as the temporal structure, and here are in each of those boxes you see actually there in Ferd posterior marginals for labor for labels.",
            "So the red color.",
            "Sponse to foreground class and the blue color corresponds to a background class.",
            "Here's a learned classifier from from our temporal propagation, and that classifier represents then non temple structure.",
            "So all the correlations which are not happening from frame to frame.",
            "So there are free keep key things I have to mention about our approach.",
            "So first we efficiently efficiently use the labels as well as computation power.",
            "We use the efficiency of using labels from from the fact that we actually learn the sentiment segmentation in SM supervised fashion and we are able to learn by semi supervised parameter updates because we're able to infer that uncertainty in both.",
            "Actually the labels as well as the temple structure of the video.",
            "Inverse of a talk.",
            "I'm going to present briefly talk about that."
        ],
        [
            "Network introduce our model and talk about inference and finally show some quality results as well as quantum."
        ],
        [
            "So recent."
        ],
        [
            "Currently it is very popular to formulate best video segmentation problem as an inference on a spatial temporal MRF and then use a graph cut or a similar approach to get the MLP labeling of a video.",
            "However, if you do that MLP estimate of the video segmentation, then you do not get the uncertainty of your labeling, and that's what we're after.",
            "So one of the exceptions is."
        ],
        [
            "Their work by community set tile on geodesics, video segment image and video segmentation.",
            "There are actually mostly."
        ],
        [
            "Focusing on their videos and the data, and they use the standard formulation of their of their spatial Templar.",
            "MRF for video segmentation.",
            "However, the main difference is that they restrict the possibility the possible space of solutions from from getting a true mmaps metal segmentation to some local MPs myth of segmentation and that allows them to infer the uncertainty of the labeled.",
            "And however, they do not demonstrate that update of their temple structure, so of the mappings between frames."
        ],
        [
            "Now, another interesting work is work by its high a tile and it's work on motion motion an on tracking and motion and motion segmentation and segmentation."
        ],
        [
            "And the key idea in that work is that they represent they have variables which actually jointly represent the labeling as well as motion vector between the frames.",
            "So when we do the MLP estimate, they actually infer not only the labels, but how frames are mapped from one frame to another.",
            "However, since they do use the MLP approach where you are not able to infer billable labor uncertainty and also we have a drawback of having to use the sliding window approach.",
            "For their inference, which which leads into actually a shrinking bias.",
            "Finally, there is work by 5 year tile on."
        ],
        [
            "Self training and segmentation and that is."
        ],
        [
            "Very interesting work.",
            "'cause actually we formed the segmentation problem as an inference on a Gaussian random field where they are able to find the harmonic solutions which represents pseudo probabilities for the unlabeled data and therefore they had the uncertainty of.",
            "Are there labels and also they are able to update their parameters, however we have to resort to a self training self training approach which is very similar to a sliding window approach where you can introduce drift or mislabeling and also they're not modeling the temporal structure explicitly.",
            "So if you wanted to expand their model to let's say semi supervised Optic flow determination, you would have to do some additional work on it whereas in our case we are modeling the temple structure between frames.",
            "Explicitly where?"
        ],
        [
            "Also, doing the inference on the full full video."
        ],
        [
            "I suppose on some sliding sliding windows and finally were able to infer the uncertainty of our legs."
        ],
        [
            "So our model."
        ],
        [
            "Here are two things you have to remember about our model.",
            "So first we have three distinct layers at the first layer on top, it's their parents layer and it corresponds to their super pixels super pixelized images.",
            "The bottom layer corresponds to labels of those super pixels, and finally the middle layer corresponds to mappings between the Super pixels in one frame and the previous frame there.",
            "Important important thing about that, our temple mapping is that if you take one particular estimate of our mapping then you have built a tree in a video and that is important for the inference and I'll talk about it a bit later.",
            "Ah, so just maybe."
        ],
        [
            "Driving model, I'd like to remind that our input is super pixelized image and also some labels in all the experiments which I'm talking.",
            "I'm going to talk about here.",
            "We use only the 1st frame labels and then we want to infer the labels of the rest of the frames.",
            "And while inferring the labels of the rest of the frame.",
            "So we want to update our temple structure for quicker segmentation of quicker convergence of the segmentation of video.",
            "So."
        ],
        [
            "Let's look at our model for just two frames.",
            "So here on the top you see variables corresponding to the Super pixels on the bottom.",
            "Again, you see variables corresponding to their labels and then T is there napping variable which connects the superpixel.",
            "Now T is is special because it takes for one super pixel.",
            "In this frame it Maps to limited window of super pixels in the previous frame and the similarity between the Super pixels.",
            "In an appearance is measured by the factor of Sighet and similar similarity in labels is measured by the factor of sight L, and I'm going."
        ],
        [
            "Tell how do we calculate the similarity between super pixels?",
            "So let's assume we have this superpixel Sigi here and then we want to find how similar it is to the superpixel S I -- 1 TJ.",
            "We have a very simple purchase procedure.",
            "We simply find all the patches in the superpixel SIJ and find their best matches in a previous window.",
            "I once we're done, we simply count how many best matches each particular superpixel received in a previous frame and take it as our similarity measure.",
            "So, for example, this superpixel has five best matches, and this one has two, so this superpixel is more similar than another one 4."
        ],
        [
            "For similarity in label domain, we have even more more simple function.",
            "We basically award super pixels of same label to value mu answer because of a different label to value alignment.",
            "1 minus mu and in all the experiments we kept this value fixed 2.9.",
            "Now, if you infer."
        ],
        [
            "The posterior distribution of a mapping variables.",
            "Then you'll see that actually it's at corresponds to a multiplication of similarity distributions in both appearance and in labels.",
            "So therefore when you update your mapping distribution, you are actually both using the label information as well as the appearance information, and that's what we were after finally."
        ],
        [
            "Those couple slides are for showing that if we fix a fix our mapping variable then we end up with a tree structure, so we end up with a mixture of trees raffle model.",
            "So here each superpixel has exactly 1 neighbor in the previous frame and so they can end up being connected."
        ],
        [
            "One super pixel, however, they can never."
        ],
        [
            "Add form loops.",
            "Now when we concatenate our two frame model into into a sequence, we get."
        ],
        [
            "This particular full model of that video, and that's it.",
            "So it's quite a simple temporal structure model."
        ],
        [
            "Now before."
        ],
        [
            "Priming of inference.",
            "Let's remind ourselves what are our variables.",
            "So our observed variables are the Super pixels.",
            "So and the label frame.",
            "Also, labels can come as user, mouse, brushstrokes and our latent variables.",
            "So wearables which we want to infer are the labels as well as the mapping variables.",
            "This is our joint distribution over the variables and it has four for May 4 main terms.",
            "So first term is bad parents, so this is their similarity in appearance, we vent their label term, which is similarity in the labels.",
            "We then have unary terms for for for labels or the other variables.",
            "So here we can plug either classifier learned in some supervised fashion from your video or you can even plug an external classifier, learn from other data.",
            "And finally we have a mapping prior which just limits the mappings in a previous frame to some particular window.",
            "And if you want to find out more detail about more details about the inference, you can look at our BMC paper or a couple of papers here.",
            "So for inference we use a structural variational inference."
        ],
        [
            "We shape our structured variational posterior in this particular way, so we have two terms.",
            "One term we completely factorized the mapping variables.",
            "And Secondly, we keep the structure in their variational posterior of the label variables and then before starting inference, we initialize the operational posterior of label variables into uniform distribution, and we do the same visionaries.",
            "We started out inference by first inferring the mappings.",
            "And then we have a choice.",
            "Since our model is a mixture of trees, we can ever take estimate the best tree structure and then use the exact inference over that tree structure, which would be very fast and also very predictable.",
            "Or we can use a loopy BP over our mixture of trees, graphical model and so we have a paper in CPR on the on.",
            "The inference on that tree structure.",
            "And here for this paper we have a picture of trees model.",
            "And once we infer the labels, then we can either obtain more labels from the user, or we can.",
            "We can learn the universe in SM supervised way, inject them back into our model and do the inference again.",
            "How?"
        ],
        [
            "We learn our users.",
            "We actually simply use the random forest classifier because it's very fast and we train it with the inferred posterior of our labels and we inject back results simply as our universe.",
            "We modify their normal classifier, taking hard labels into classifier, taking soft labels.",
            "However, in random forest case it's a very simple modification.",
            "So let's see an example of our inference.",
            "So here on the top you have a video sequence below you have."
        ],
        [
            "Corresponding ground truth labels and here we have a marginal posteriors of our inference on the temple structure, so again red corresponds to to foreground class.",
            "An blue corresponds to bag on the bus, and here is just the Max of those marginals, and as you see, it's not ideal results, so we have some problems with the background we."
        ],
        [
            "And have a user introducing more label."
        ],
        [
            "Here and here.",
            "And then we suddenly see that we cleaned up the background."
        ],
        [
            "So the good thing is, yes, we cleaned up the background.",
            "However, since we used very simple features, we actually caused some problems with some parts of an object, let's say like hands, and so we actually hear loss hands power bats in the inevitable 'cause it it's.",
            "So features are not very strong finally."
        ],
        [
            "Let's see so far to get very results we use."
        ],
        [
            "We used asset track segmentation and tracking data set introduced by title and here on the top.",
            "So we see that sequence called Girl an in here we labeled only the starting frame on the left side you see the video corresponding with labels and hear our inferred segmentation similar with the sequence called Cheetah.",
            "We have the video ground truth and the inferred segmentation.",
            "You see, actually.",
            "So we're we're segmenting the deer and you see part of the cheater coming out here and that's because we're using classifier so classifier can introduce an segmentation far away from the object of interest."
        ],
        [
            "In terms of quantitative evaluation, at this table shows numbers in.",
            "This table corresponds to average amount of mislabels points.",
            "Mislabeled points in a video frame.",
            "We compare it with a level set of low level sets approached at side invitee approach which I mentioned and also our previous work on Patch based model and so our current work achieves the best score for those two and also we have pretty decent score for others except that.",
            "Grateful.",
            "I'll talk about baseball in a minute.",
            "I'll just mention as quickly about our computational speed, so we need only 40 megabytes of RAM to process the largest video sequence in this data set, which is called Penguin, and we only need 5 to 7 seconds to per frame to do the inference on an unoptimized code so it can easily be made to run at interactive speed.",
            "Outlets, sorry it's actually not per frame.",
            "It's actually per video.",
            "That's my mistake.",
            "So let's let's come back to the bird."
        ],
        [
            "So we get a bad score in a bird because actually we lose part of Bird there early on.",
            "And that's happening because our temple structure learn template structure over super pixels is wrong because the bird is very small object and so once we superpixel eyes, small mistake on a small object might lead in complete loss of a track of the object and since we lose the object very early on, Bender classifiers not able to pick up pick that object up for later propagation of labels.",
            "Here are a few."
        ],
        [
            "Results so this is a parachute sequence.",
            "And also bear monkey drop sequence.",
            "Ah, so today."
        ],
        [
            "I presented a probabilistic framework based on mixture of trees graphical model.",
            "We are using the label sufficiently and we are also very efficient in computation power and memory.",
            "We also able to estimate the uncertainties of classification labels as well as the temple mappings.",
            "And finally we are able to use those uncertainty estimates to perform some surprise updates of the temple mappings and the parameters corresponding to labeling.",
            "Finally, in terms of our future work, we are thinking of applying our model and the inferred Temple estimates to perform some supervised optic flow to find the summer to infer sensor eyes, optic flow, or actually using them to train a random classifier and instead of needing to do NCC between subsequent frames to actually apply the classifier for a very quick, I defined identification for which links should we use for the label propagation, which links we should drop.",
            "Finally, if you have any questions, feel free to email me.",
            "And I have to acknowledge our sponsors, Toyota Research Europe and Toyota Motor Europe.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Doctor Vijay, but in the Ryan Budget and Narayanan professor, better Chipola.",
                    "label": 0
                },
                {
                    "sent": "Out of research in computer vision, it relies on availability on vast amounts of ground truth label data.",
                    "label": 0
                },
                {
                    "sent": "However, obtaining such data can be a very tedious.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task for example, labeling such a simple video might result in more than 30 minutes of labeling and even worse.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labeling of more complex video like this might result into 30 minutes of labeling for one frame, which means you would spend hours of labeling for just one minute of ground truth data.",
                    "label": 0
                },
                {
                    "sent": "Of course, videos are not sets of random images and this inherent similarity between subsequent frames as well as objects in distance frames should be used, exploited.",
                    "label": 0
                },
                {
                    "sent": "So for that we create a probabilistic graph.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model for the temporal structure of the video and it works in this particular way.",
                    "label": 0
                },
                {
                    "sent": "So we have a video as our input.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We then have very sparse labels, so we typically we have either starting frame labeled or start and end frame labeled or only some user private mouse brushstrokes.",
                    "label": 0
                },
                {
                    "sent": "Then we iterate between temporal label propagation and semi supervised learning of both units of labels as well as the temporal structure, and here are in each of those boxes you see actually there in Ferd posterior marginals for labor for labels.",
                    "label": 1
                },
                {
                    "sent": "So the red color.",
                    "label": 0
                },
                {
                    "sent": "Sponse to foreground class and the blue color corresponds to a background class.",
                    "label": 0
                },
                {
                    "sent": "Here's a learned classifier from from our temporal propagation, and that classifier represents then non temple structure.",
                    "label": 0
                },
                {
                    "sent": "So all the correlations which are not happening from frame to frame.",
                    "label": 0
                },
                {
                    "sent": "So there are free keep key things I have to mention about our approach.",
                    "label": 1
                },
                {
                    "sent": "So first we efficiently efficiently use the labels as well as computation power.",
                    "label": 0
                },
                {
                    "sent": "We use the efficiency of using labels from from the fact that we actually learn the sentiment segmentation in SM supervised fashion and we are able to learn by semi supervised parameter updates because we're able to infer that uncertainty in both.",
                    "label": 0
                },
                {
                    "sent": "Actually the labels as well as the temple structure of the video.",
                    "label": 0
                },
                {
                    "sent": "Inverse of a talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present briefly talk about that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network introduce our model and talk about inference and finally show some quality results as well as quantum.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So recent.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Currently it is very popular to formulate best video segmentation problem as an inference on a spatial temporal MRF and then use a graph cut or a similar approach to get the MLP labeling of a video.",
                    "label": 0
                },
                {
                    "sent": "However, if you do that MLP estimate of the video segmentation, then you do not get the uncertainty of your labeling, and that's what we're after.",
                    "label": 0
                },
                {
                    "sent": "So one of the exceptions is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their work by community set tile on geodesics, video segment image and video segmentation.",
                    "label": 0
                },
                {
                    "sent": "There are actually mostly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focusing on their videos and the data, and they use the standard formulation of their of their spatial Templar.",
                    "label": 0
                },
                {
                    "sent": "MRF for video segmentation.",
                    "label": 0
                },
                {
                    "sent": "However, the main difference is that they restrict the possibility the possible space of solutions from from getting a true mmaps metal segmentation to some local MPs myth of segmentation and that allows them to infer the uncertainty of the labeled.",
                    "label": 0
                },
                {
                    "sent": "And however, they do not demonstrate that update of their temple structure, so of the mappings between frames.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, another interesting work is work by its high a tile and it's work on motion motion an on tracking and motion and motion segmentation and segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the key idea in that work is that they represent they have variables which actually jointly represent the labeling as well as motion vector between the frames.",
                    "label": 0
                },
                {
                    "sent": "So when we do the MLP estimate, they actually infer not only the labels, but how frames are mapped from one frame to another.",
                    "label": 0
                },
                {
                    "sent": "However, since they do use the MLP approach where you are not able to infer billable labor uncertainty and also we have a drawback of having to use the sliding window approach.",
                    "label": 1
                },
                {
                    "sent": "For their inference, which which leads into actually a shrinking bias.",
                    "label": 0
                },
                {
                    "sent": "Finally, there is work by 5 year tile on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Self training and segmentation and that is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very interesting work.",
                    "label": 0
                },
                {
                    "sent": "'cause actually we formed the segmentation problem as an inference on a Gaussian random field where they are able to find the harmonic solutions which represents pseudo probabilities for the unlabeled data and therefore they had the uncertainty of.",
                    "label": 1
                },
                {
                    "sent": "Are there labels and also they are able to update their parameters, however we have to resort to a self training self training approach which is very similar to a sliding window approach where you can introduce drift or mislabeling and also they're not modeling the temporal structure explicitly.",
                    "label": 1
                },
                {
                    "sent": "So if you wanted to expand their model to let's say semi supervised Optic flow determination, you would have to do some additional work on it whereas in our case we are modeling the temple structure between frames.",
                    "label": 0
                },
                {
                    "sent": "Explicitly where?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, doing the inference on the full full video.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I suppose on some sliding sliding windows and finally were able to infer the uncertainty of our legs.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are two things you have to remember about our model.",
                    "label": 0
                },
                {
                    "sent": "So first we have three distinct layers at the first layer on top, it's their parents layer and it corresponds to their super pixels super pixelized images.",
                    "label": 0
                },
                {
                    "sent": "The bottom layer corresponds to labels of those super pixels, and finally the middle layer corresponds to mappings between the Super pixels in one frame and the previous frame there.",
                    "label": 0
                },
                {
                    "sent": "Important important thing about that, our temple mapping is that if you take one particular estimate of our mapping then you have built a tree in a video and that is important for the inference and I'll talk about it a bit later.",
                    "label": 0
                },
                {
                    "sent": "Ah, so just maybe.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Driving model, I'd like to remind that our input is super pixelized image and also some labels in all the experiments which I'm talking.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about here.",
                    "label": 0
                },
                {
                    "sent": "We use only the 1st frame labels and then we want to infer the labels of the rest of the frames.",
                    "label": 0
                },
                {
                    "sent": "And while inferring the labels of the rest of the frame.",
                    "label": 0
                },
                {
                    "sent": "So we want to update our temple structure for quicker segmentation of quicker convergence of the segmentation of video.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at our model for just two frames.",
                    "label": 0
                },
                {
                    "sent": "So here on the top you see variables corresponding to the Super pixels on the bottom.",
                    "label": 0
                },
                {
                    "sent": "Again, you see variables corresponding to their labels and then T is there napping variable which connects the superpixel.",
                    "label": 0
                },
                {
                    "sent": "Now T is is special because it takes for one super pixel.",
                    "label": 0
                },
                {
                    "sent": "In this frame it Maps to limited window of super pixels in the previous frame and the similarity between the Super pixels.",
                    "label": 0
                },
                {
                    "sent": "In an appearance is measured by the factor of Sighet and similar similarity in labels is measured by the factor of sight L, and I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tell how do we calculate the similarity between super pixels?",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have this superpixel Sigi here and then we want to find how similar it is to the superpixel S I -- 1 TJ.",
                    "label": 0
                },
                {
                    "sent": "We have a very simple purchase procedure.",
                    "label": 0
                },
                {
                    "sent": "We simply find all the patches in the superpixel SIJ and find their best matches in a previous window.",
                    "label": 0
                },
                {
                    "sent": "I once we're done, we simply count how many best matches each particular superpixel received in a previous frame and take it as our similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this superpixel has five best matches, and this one has two, so this superpixel is more similar than another one 4.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For similarity in label domain, we have even more more simple function.",
                    "label": 0
                },
                {
                    "sent": "We basically award super pixels of same label to value mu answer because of a different label to value alignment.",
                    "label": 0
                },
                {
                    "sent": "1 minus mu and in all the experiments we kept this value fixed 2.9.",
                    "label": 0
                },
                {
                    "sent": "Now, if you infer.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The posterior distribution of a mapping variables.",
                    "label": 0
                },
                {
                    "sent": "Then you'll see that actually it's at corresponds to a multiplication of similarity distributions in both appearance and in labels.",
                    "label": 0
                },
                {
                    "sent": "So therefore when you update your mapping distribution, you are actually both using the label information as well as the appearance information, and that's what we were after finally.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those couple slides are for showing that if we fix a fix our mapping variable then we end up with a tree structure, so we end up with a mixture of trees raffle model.",
                    "label": 0
                },
                {
                    "sent": "So here each superpixel has exactly 1 neighbor in the previous frame and so they can end up being connected.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One super pixel, however, they can never.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add form loops.",
                    "label": 0
                },
                {
                    "sent": "Now when we concatenate our two frame model into into a sequence, we get.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This particular full model of that video, and that's it.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a simple temporal structure model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now before.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Priming of inference.",
                    "label": 0
                },
                {
                    "sent": "Let's remind ourselves what are our variables.",
                    "label": 0
                },
                {
                    "sent": "So our observed variables are the Super pixels.",
                    "label": 0
                },
                {
                    "sent": "So and the label frame.",
                    "label": 0
                },
                {
                    "sent": "Also, labels can come as user, mouse, brushstrokes and our latent variables.",
                    "label": 0
                },
                {
                    "sent": "So wearables which we want to infer are the labels as well as the mapping variables.",
                    "label": 0
                },
                {
                    "sent": "This is our joint distribution over the variables and it has four for May 4 main terms.",
                    "label": 0
                },
                {
                    "sent": "So first term is bad parents, so this is their similarity in appearance, we vent their label term, which is similarity in the labels.",
                    "label": 0
                },
                {
                    "sent": "We then have unary terms for for for labels or the other variables.",
                    "label": 1
                },
                {
                    "sent": "So here we can plug either classifier learned in some supervised fashion from your video or you can even plug an external classifier, learn from other data.",
                    "label": 1
                },
                {
                    "sent": "And finally we have a mapping prior which just limits the mappings in a previous frame to some particular window.",
                    "label": 0
                },
                {
                    "sent": "And if you want to find out more detail about more details about the inference, you can look at our BMC paper or a couple of papers here.",
                    "label": 0
                },
                {
                    "sent": "So for inference we use a structural variational inference.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We shape our structured variational posterior in this particular way, so we have two terms.",
                    "label": 0
                },
                {
                    "sent": "One term we completely factorized the mapping variables.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we keep the structure in their variational posterior of the label variables and then before starting inference, we initialize the operational posterior of label variables into uniform distribution, and we do the same visionaries.",
                    "label": 0
                },
                {
                    "sent": "We started out inference by first inferring the mappings.",
                    "label": 0
                },
                {
                    "sent": "And then we have a choice.",
                    "label": 0
                },
                {
                    "sent": "Since our model is a mixture of trees, we can ever take estimate the best tree structure and then use the exact inference over that tree structure, which would be very fast and also very predictable.",
                    "label": 1
                },
                {
                    "sent": "Or we can use a loopy BP over our mixture of trees, graphical model and so we have a paper in CPR on the on.",
                    "label": 1
                },
                {
                    "sent": "The inference on that tree structure.",
                    "label": 0
                },
                {
                    "sent": "And here for this paper we have a picture of trees model.",
                    "label": 0
                },
                {
                    "sent": "And once we infer the labels, then we can either obtain more labels from the user, or we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn the universe in SM supervised way, inject them back into our model and do the inference again.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We learn our users.",
                    "label": 0
                },
                {
                    "sent": "We actually simply use the random forest classifier because it's very fast and we train it with the inferred posterior of our labels and we inject back results simply as our universe.",
                    "label": 1
                },
                {
                    "sent": "We modify their normal classifier, taking hard labels into classifier, taking soft labels.",
                    "label": 1
                },
                {
                    "sent": "However, in random forest case it's a very simple modification.",
                    "label": 0
                },
                {
                    "sent": "So let's see an example of our inference.",
                    "label": 0
                },
                {
                    "sent": "So here on the top you have a video sequence below you have.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Corresponding ground truth labels and here we have a marginal posteriors of our inference on the temple structure, so again red corresponds to to foreground class.",
                    "label": 0
                },
                {
                    "sent": "An blue corresponds to bag on the bus, and here is just the Max of those marginals, and as you see, it's not ideal results, so we have some problems with the background we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And have a user introducing more label.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here and here.",
                    "label": 0
                },
                {
                    "sent": "And then we suddenly see that we cleaned up the background.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the good thing is, yes, we cleaned up the background.",
                    "label": 0
                },
                {
                    "sent": "However, since we used very simple features, we actually caused some problems with some parts of an object, let's say like hands, and so we actually hear loss hands power bats in the inevitable 'cause it it's.",
                    "label": 0
                },
                {
                    "sent": "So features are not very strong finally.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see so far to get very results we use.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We used asset track segmentation and tracking data set introduced by title and here on the top.",
                    "label": 0
                },
                {
                    "sent": "So we see that sequence called Girl an in here we labeled only the starting frame on the left side you see the video corresponding with labels and hear our inferred segmentation similar with the sequence called Cheetah.",
                    "label": 0
                },
                {
                    "sent": "We have the video ground truth and the inferred segmentation.",
                    "label": 0
                },
                {
                    "sent": "You see, actually.",
                    "label": 0
                },
                {
                    "sent": "So we're we're segmenting the deer and you see part of the cheater coming out here and that's because we're using classifier so classifier can introduce an segmentation far away from the object of interest.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of quantitative evaluation, at this table shows numbers in.",
                    "label": 1
                },
                {
                    "sent": "This table corresponds to average amount of mislabels points.",
                    "label": 1
                },
                {
                    "sent": "Mislabeled points in a video frame.",
                    "label": 0
                },
                {
                    "sent": "We compare it with a level set of low level sets approached at side invitee approach which I mentioned and also our previous work on Patch based model and so our current work achieves the best score for those two and also we have pretty decent score for others except that.",
                    "label": 0
                },
                {
                    "sent": "Grateful.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about baseball in a minute.",
                    "label": 1
                },
                {
                    "sent": "I'll just mention as quickly about our computational speed, so we need only 40 megabytes of RAM to process the largest video sequence in this data set, which is called Penguin, and we only need 5 to 7 seconds to per frame to do the inference on an unoptimized code so it can easily be made to run at interactive speed.",
                    "label": 0
                },
                {
                    "sent": "Outlets, sorry it's actually not per frame.",
                    "label": 0
                },
                {
                    "sent": "It's actually per video.",
                    "label": 0
                },
                {
                    "sent": "That's my mistake.",
                    "label": 0
                },
                {
                    "sent": "So let's let's come back to the bird.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we get a bad score in a bird because actually we lose part of Bird there early on.",
                    "label": 0
                },
                {
                    "sent": "And that's happening because our temple structure learn template structure over super pixels is wrong because the bird is very small object and so once we superpixel eyes, small mistake on a small object might lead in complete loss of a track of the object and since we lose the object very early on, Bender classifiers not able to pick up pick that object up for later propagation of labels.",
                    "label": 0
                },
                {
                    "sent": "Here are a few.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results so this is a parachute sequence.",
                    "label": 0
                },
                {
                    "sent": "And also bear monkey drop sequence.",
                    "label": 0
                },
                {
                    "sent": "Ah, so today.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I presented a probabilistic framework based on mixture of trees graphical model.",
                    "label": 1
                },
                {
                    "sent": "We are using the label sufficiently and we are also very efficient in computation power and memory.",
                    "label": 0
                },
                {
                    "sent": "We also able to estimate the uncertainties of classification labels as well as the temple mappings.",
                    "label": 0
                },
                {
                    "sent": "And finally we are able to use those uncertainty estimates to perform some surprise updates of the temple mappings and the parameters corresponding to labeling.",
                    "label": 0
                },
                {
                    "sent": "Finally, in terms of our future work, we are thinking of applying our model and the inferred Temple estimates to perform some supervised optic flow to find the summer to infer sensor eyes, optic flow, or actually using them to train a random classifier and instead of needing to do NCC between subsequent frames to actually apply the classifier for a very quick, I defined identification for which links should we use for the label propagation, which links we should drop.",
                    "label": 0
                },
                {
                    "sent": "Finally, if you have any questions, feel free to email me.",
                    "label": 1
                },
                {
                    "sent": "And I have to acknowledge our sponsors, Toyota Research Europe and Toyota Motor Europe.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}