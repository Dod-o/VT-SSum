{
    "id": "cin2dd57avcuhq7anwoisagit2framte",
    "title": "TopicFlow Model: Unsupervised Learning of Topic-specific Influences of Hyperlinked Documents",
    "info": {
        "author": [
            "Ramesh Nallapati, Carnegie Mellon University"
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/aistats2011_nallapati_model/",
    "segmentation": [
        [
            "Thank you very much.",
            "Yeah, this is joint work with my mentors Dan Mcfarlan and Chris Money at Stanford.",
            "So let's start with the introduction and motivation.",
            "So, given a set of documents and the hyperlink structure information within the between the documents, there are lots of algorithms like page rank and hits that can give you the most influential or authoritative documents in that set.",
            "But the problem one problem with such approaches is that they completely ignore the contextual information, such as the text."
        ],
        [
            "Topics of the documents, so such algorithms may give like one of these documents, like CNN politics has the most authoritative document in this set.",
            "But since they ignore content, we completely miss the topical relevance or the influence of those documents.",
            "So what we really want is to actually model context sensitive or topical global influence.",
            "So we want to know that of these four documents, each of them are actually quite relevant and influential, but within their own topic.",
            "So CNN politics is highly influential and topic on."
        ],
        [
            "Topic of politics.",
            "ESPN is highly influential on sports and so on, and this is a problem that I want to address in this work.",
            "So it's basically we want to model topic specific global influence of documents.",
            "So this is how the model works.",
            "So these are three document."
        ],
        [
            "Here, each of them has a generative process as defined by the latent initially allocation model.",
            "So I'll just assume that everybody is familiar with the LDA model in the interest of time.",
            "So these two links are not part of the graphical model, they're actually the hyperlinks or the citations between these two documents.",
            "So there are three documents here, and this document cites these two documents and the edges here actually are part of the LDS.",
            "Graphical structure, but these edges are actually citations and will use network flow paradigm to model the spread of influence across the network.",
            "Will also assume two hypothetical nodes.",
            "Apart from these documents, called the source in the sink, like any network flow problem and we also assume some hypothetical hyperlinks that go from the source to all the documents and also."
        ],
        [
            "Each document for the single.",
            "I'll shortly talk about what the flows actually mean, but I'll first describe the model.",
            "So there are two kind of paradigms we considered.",
            "So there are multiple topics that we want to model, and independent sources.",
            "Topic sources paradigm will assume that each topic has its own.",
            "Flow and the network structure is replicated across topics.",
            "As shown in this animation, the second paradigm we considered is a single source formalism where all the topics have their own link structure, but they compete for flow from the same source, so they have the same replicated network structure.",
            "Each topic represented by each color, but they share the same source, which means topics compete for the flows which allows for."
        ],
        [
            "So modeling the domination of 1 topic with respect to the other because not all topics are equally important in a given corpus.",
            "So we want to model the."
        ],
        [
            "Importance of topics.",
            "So there are two paradigms we considered in this model.",
            "Now we'll talk about some definitions of the model so.",
            "We assume that each document, whatever flow is coming in on a given topic, is exactly the flow going out of the document, so the flow is basically balance with each document.",
            "And also we assume that this is the key thing that kid definition that links the flows to the actual content generation in each document.",
            "So in LDA model we have data which is the distribution over topics for each document.",
            "And we say that this distribution is basically given by the normalization of the incoming flows to the document on that topic.",
            "So the proportion of topic relevant document is proportional to the flow on the topic.",
            "That's what it means.",
            "So the higher the amount of flow, the more is the probability that document generates words on the topic and vice versa if there's a high topic loading for a given document on a given topic, that means you should also attract a lot of flow on the topic.",
            "So that's how these two are kind of related.",
            "And we also define the notion of influence, topic specific influence of a document in terms of the net incoming flow of the document, excluding the flow from the source.",
            "Also multiplied by the topic loading of the document on the topic.",
            "So what?"
        ],
        [
            "The flows actually mean so flow document A has certain amount of flow to Document B on a given topic.",
            "It is basically the amount of Ace topical influence that is attributed to document be.",
            "That's what the interpretation is for.",
            "The topical flows.",
            "This follows from 2 simple observations.",
            "So the flow balance constraint ensures that all the citations of a share as incoming flow on the topic and also document a assigns tropical flows document B.",
            "Based on how relevant Document B is to given topic.",
            "So these two conditions these two observations mean that the topical flow from A to B is basically the amount of influence of a that's attributed document B and we also use the source and sink formalism to account for the missing hyperlinks.",
            "So the document has no hyperlinks.",
            "It can still get some flow from the source to explain it's a topical relevance."
        ],
        [
            "Um?",
            "So.",
            "As we have more documents citing a given document on a given topic, which means there's more flow coming into the document given topic, the higher is the probability as we discussed and also higher is its influence.",
            "So this is a kind of wisdom of the crowds model.",
            "So the we actually model documents, topical relevance and influence based on how many citations topically relevant citations it gets.",
            "So in reality actually.",
            "Text is first generated and then the hyperlinks come in the future, but were actually modeling them jointly, so we actually have the luxury of hindsight, so we're able to look at the future citations so they're coming into the document and then model it's textual content based on the citations also."
        ],
        [
            "So also we claim that the model actually captures global influence of a document in a given topic, and I'll just explain it in a visual way without actually any formal proof.",
            "So let's assume this is a hyperlink network.",
            "And each of those boxes a document, let's say the document in the middle discusses the topic.",
            "The red colored topic with high probability, and so since it has a high Theta on a given topic, it means that it should attract a lot of incoming flow from its own citations, incoming citations.",
            "So that means the flows.",
            "From those from this document.",
            "Should be high on the topic and since the flows are balanced.",
            "Influence the effect actually spreads upstream into the network, and same is the case downstream as well.",
            "So since this document has high amount of flow on the topic, the flow has to spread through its roots children.",
            "So the.",
            "The flow keeps spreading downstream as well, and since there is some flow here.",
            "Also explain the topical content so there's some topic loading in those documents as well so that it keeps spreading across the network.",
            "So that's how the influence spreads across the whole network and the influence with capture is actually global, not just local.",
            "And so I'll just compare."
        ],
        [
            "The topic flow model with another popular algorithm called the topic sensitive Pagerank.",
            "So both these models capture the notion of global topic specific influence and both these models also are able to capture missing links using different formalisms.",
            "For example, topics to patient uses teleportation where the random surfer is allowed to jump to a random document without any hyperlink, and we also use a similar formalism called sourcing as described, so we also allow like modeling of missing hyperlinks.",
            "The main difference is here.",
            "So topic sense to page then requires a seed set of labeled documents for each topic to start the page rank algorithm biased towards documents on that topic.",
            "But in this case we do not require any label documents.",
            "Totally unsupervised.",
            "Also, the Pagerank algorithm assumes that the page rank for a given document is equally distributed among its children.",
            "But topic flow model is able to weigh the citations based on the topical relevance of the documents cited.",
            "So and also the topic says to page rank is not a joint model of topics and of words and hyperlinks.",
            "It actually assumes topic labels to be given and then runs runs of Pagerank algorithm top of that.",
            "But here we are doing a joint modeling of hyperlinks and text.",
            "So also the topic since 2 page rank at the end of the process gives you a list of.",
            "Page rank so given topic is this ranked list but topic flow model is actually able to tell you the amount of flow going through the network on each citation edge.",
            "So we can actually track how the influence of a topic spreads across the network.",
            "So I'll just talk briefly about.",
            "The."
        ],
        [
            "Being an inference, so we have the following objective function.",
            "So the first one is just the observed data log likelihood, the first term in the objective function in the second term is a regularization term of the flows.",
            "We ensure that flows are as small as possible as defined by the L2 norm unless required by the observed data to explain the observed phenomenon, and we also have.",
            "Unlike LDA we have.",
            "This is a more constrained optimization problem since we need to balance the flows at this document.",
            "So we have.",
            "Equality constraints, so we solve this problem in."
        ],
        [
            "Steps first, since the problem is intractable to do exact inference, we use variational lower bound just like in LDA.",
            "That's the first step."
        ],
        [
            "We also eliminate the equality constraints in two steps.",
            "First, we convert the flow balance constraints into multinomial constraints.",
            "Since the total incoming flow into a given document is spread equally among its children, we can use a multi normal distribution there to capture the distribution of the flows across the children.",
            "And then we eliminate the multinomial constraints using logistic transformation of the multinomial variables, and that's how we get to.",
            "Kind of unconstrained optimization problem, using these transform."
        ],
        [
            "Once we do that, we can just apply standard gradient descent or LB FGS to learn the pyramid flow parameters as well as the topics in the corpus.",
            "So here is a graphic of how the topic how the output of the model looks like.",
            "So standard LDA model gives you all these for each topic it listed the top ranking words so you can get an estimate of what the topic is about in addition to the standard.",
            "This output this model is also able to tell you what are the most influential documents on each topic, so this graphic is actually run on the ACL corpus consisting of the computational linguistics papers spanning over 30 years time.",
            "So it finds that on the topic of machine translation.",
            "Della Pietra's paper on mathematical mathematical translation is probably the most influential document, and not only that, it also tells you how the flows actually spread across the network, so this is a slice of the topic flows output in the neighborhood of the most influential document on the topic.",
            "So it tells you you know how the flow spreads among its parents as well as children, and it quantifies the flow of influence as well.",
            "We also did some empirical analysis on the ACL data and we compared the output with ACL's most cited papers and we found that six out of the 10 most cited papers also figure in the topic flows top 10 list at least one topic, so it's kind of a sanity check and we also found that all the 10 papers actually earlier in the topic flows top 50 two documents on at least one topic.",
            "So all those things are captured at the top and also more important."
        ],
        [
            "See what we found is that these top rated papers are also ranked pretty low on topics that are not relevant to those documents, so it's able to detect the topic specific influence of those documents.",
            "So you didn't.",
            "Topics are ranked pretty low, but on relevant topics are ranked very high, so that's the main distinction from page rank versus topic sensitive Pagerank or the topic flow model.",
            "And we also found the papers are missing from the top 10 list are actually broad data set papers or evaluation metrics based papers like the Penn Treebank Paper or the Blue paper that talks about the Blau evaluation metric for machine translation.",
            "So these are highly cited, but there's cited probably more often than not.",
            "For the you know, because they use the data set or because they use the evaluation metric, but there's no direct influence in terms of the propagation of ideas, so this kind of.",
            "To filter out documents that are typically inferential versus broad evaluation metrics, data set papers."
        ],
        [
            "And also we have some objective evaluations, so we did the task of citation recommendation.",
            "So this is a task of predicting the citations for new documents given the model.",
            "And so we chose this task cause in academic literature typically citations mean the document is both topically relevant as well as probably influential.",
            "So performance on this task means the model is able to gauge topical influence.",
            "So this is mean average precision is standard information retrieval metric and with the higher number of topics, the topic flow model as well as the.",
            "As well as the topic since 2 page rank based on topic flows output both performed quite well compared to TF IDF baseline.",
            "It also beats other state of the art relational topic models such as the RTM RTM is the."
        ],
        [
            "Solid line here and we also did comparison on document completion, log likelihood and the model is quite competitive with RPM and also outperforms LDA."
        ],
        [
            "This is a core data set consisting of scientific articles from computer science.",
            "We also did the same analysis on the ACL data and the traffic flow model is quite good in terms of log."
        ],
        [
            "Likelihood as well, so to conclude.",
            "There's a new model.",
            "It's able to capture topical influence of documents as well as the topic sensitive Pagerank, but it requires no label documents completely unsupervised.",
            "And it's also a good model for text, and it's comparable to the state of the art topic models in the in computing the log likelihood and in the future we want to apply this model to a range of other textual network data like blogs, web, social media, etc.",
            "And we're also in the process of building a graphical browser that allows the users to track how the topics actually spread across the network, as I've shown in the graphic earlier, will actually building a browser and I'll be happy to share the hyperlink with people who are interested.",
            "Um?",
            "Yeah, I guess that concludes my talk and I'll be happy to take any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is joint work with my mentors Dan Mcfarlan and Chris Money at Stanford.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the introduction and motivation.",
                    "label": 0
                },
                {
                    "sent": "So, given a set of documents and the hyperlink structure information within the between the documents, there are lots of algorithms like page rank and hits that can give you the most influential or authoritative documents in that set.",
                    "label": 0
                },
                {
                    "sent": "But the problem one problem with such approaches is that they completely ignore the contextual information, such as the text.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topics of the documents, so such algorithms may give like one of these documents, like CNN politics has the most authoritative document in this set.",
                    "label": 0
                },
                {
                    "sent": "But since they ignore content, we completely miss the topical relevance or the influence of those documents.",
                    "label": 1
                },
                {
                    "sent": "So what we really want is to actually model context sensitive or topical global influence.",
                    "label": 1
                },
                {
                    "sent": "So we want to know that of these four documents, each of them are actually quite relevant and influential, but within their own topic.",
                    "label": 0
                },
                {
                    "sent": "So CNN politics is highly influential and topic on.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic of politics.",
                    "label": 0
                },
                {
                    "sent": "ESPN is highly influential on sports and so on, and this is a problem that I want to address in this work.",
                    "label": 0
                },
                {
                    "sent": "So it's basically we want to model topic specific global influence of documents.",
                    "label": 1
                },
                {
                    "sent": "So this is how the model works.",
                    "label": 0
                },
                {
                    "sent": "So these are three document.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, each of them has a generative process as defined by the latent initially allocation model.",
                    "label": 0
                },
                {
                    "sent": "So I'll just assume that everybody is familiar with the LDA model in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "So these two links are not part of the graphical model, they're actually the hyperlinks or the citations between these two documents.",
                    "label": 0
                },
                {
                    "sent": "So there are three documents here, and this document cites these two documents and the edges here actually are part of the LDS.",
                    "label": 0
                },
                {
                    "sent": "Graphical structure, but these edges are actually citations and will use network flow paradigm to model the spread of influence across the network.",
                    "label": 0
                },
                {
                    "sent": "Will also assume two hypothetical nodes.",
                    "label": 0
                },
                {
                    "sent": "Apart from these documents, called the source in the sink, like any network flow problem and we also assume some hypothetical hyperlinks that go from the source to all the documents and also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each document for the single.",
                    "label": 0
                },
                {
                    "sent": "I'll shortly talk about what the flows actually mean, but I'll first describe the model.",
                    "label": 0
                },
                {
                    "sent": "So there are two kind of paradigms we considered.",
                    "label": 0
                },
                {
                    "sent": "So there are multiple topics that we want to model, and independent sources.",
                    "label": 0
                },
                {
                    "sent": "Topic sources paradigm will assume that each topic has its own.",
                    "label": 0
                },
                {
                    "sent": "Flow and the network structure is replicated across topics.",
                    "label": 0
                },
                {
                    "sent": "As shown in this animation, the second paradigm we considered is a single source formalism where all the topics have their own link structure, but they compete for flow from the same source, so they have the same replicated network structure.",
                    "label": 0
                },
                {
                    "sent": "Each topic represented by each color, but they share the same source, which means topics compete for the flows which allows for.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So modeling the domination of 1 topic with respect to the other because not all topics are equally important in a given corpus.",
                    "label": 0
                },
                {
                    "sent": "So we want to model the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Importance of topics.",
                    "label": 0
                },
                {
                    "sent": "So there are two paradigms we considered in this model.",
                    "label": 0
                },
                {
                    "sent": "Now we'll talk about some definitions of the model so.",
                    "label": 0
                },
                {
                    "sent": "We assume that each document, whatever flow is coming in on a given topic, is exactly the flow going out of the document, so the flow is basically balance with each document.",
                    "label": 0
                },
                {
                    "sent": "And also we assume that this is the key thing that kid definition that links the flows to the actual content generation in each document.",
                    "label": 0
                },
                {
                    "sent": "So in LDA model we have data which is the distribution over topics for each document.",
                    "label": 0
                },
                {
                    "sent": "And we say that this distribution is basically given by the normalization of the incoming flows to the document on that topic.",
                    "label": 0
                },
                {
                    "sent": "So the proportion of topic relevant document is proportional to the flow on the topic.",
                    "label": 1
                },
                {
                    "sent": "That's what it means.",
                    "label": 0
                },
                {
                    "sent": "So the higher the amount of flow, the more is the probability that document generates words on the topic and vice versa if there's a high topic loading for a given document on a given topic, that means you should also attract a lot of flow on the topic.",
                    "label": 0
                },
                {
                    "sent": "So that's how these two are kind of related.",
                    "label": 0
                },
                {
                    "sent": "And we also define the notion of influence, topic specific influence of a document in terms of the net incoming flow of the document, excluding the flow from the source.",
                    "label": 1
                },
                {
                    "sent": "Also multiplied by the topic loading of the document on the topic.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The flows actually mean so flow document A has certain amount of flow to Document B on a given topic.",
                    "label": 0
                },
                {
                    "sent": "It is basically the amount of Ace topical influence that is attributed to document be.",
                    "label": 1
                },
                {
                    "sent": "That's what the interpretation is for.",
                    "label": 0
                },
                {
                    "sent": "The topical flows.",
                    "label": 0
                },
                {
                    "sent": "This follows from 2 simple observations.",
                    "label": 1
                },
                {
                    "sent": "So the flow balance constraint ensures that all the citations of a share as incoming flow on the topic and also document a assigns tropical flows document B.",
                    "label": 0
                },
                {
                    "sent": "Based on how relevant Document B is to given topic.",
                    "label": 0
                },
                {
                    "sent": "So these two conditions these two observations mean that the topical flow from A to B is basically the amount of influence of a that's attributed document B and we also use the source and sink formalism to account for the missing hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "So the document has no hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "It can still get some flow from the source to explain it's a topical relevance.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As we have more documents citing a given document on a given topic, which means there's more flow coming into the document given topic, the higher is the probability as we discussed and also higher is its influence.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of wisdom of the crowds model.",
                    "label": 1
                },
                {
                    "sent": "So the we actually model documents, topical relevance and influence based on how many citations topically relevant citations it gets.",
                    "label": 0
                },
                {
                    "sent": "So in reality actually.",
                    "label": 1
                },
                {
                    "sent": "Text is first generated and then the hyperlinks come in the future, but were actually modeling them jointly, so we actually have the luxury of hindsight, so we're able to look at the future citations so they're coming into the document and then model it's textual content based on the citations also.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So also we claim that the model actually captures global influence of a document in a given topic, and I'll just explain it in a visual way without actually any formal proof.",
                    "label": 0
                },
                {
                    "sent": "So let's assume this is a hyperlink network.",
                    "label": 0
                },
                {
                    "sent": "And each of those boxes a document, let's say the document in the middle discusses the topic.",
                    "label": 0
                },
                {
                    "sent": "The red colored topic with high probability, and so since it has a high Theta on a given topic, it means that it should attract a lot of incoming flow from its own citations, incoming citations.",
                    "label": 0
                },
                {
                    "sent": "So that means the flows.",
                    "label": 0
                },
                {
                    "sent": "From those from this document.",
                    "label": 0
                },
                {
                    "sent": "Should be high on the topic and since the flows are balanced.",
                    "label": 0
                },
                {
                    "sent": "Influence the effect actually spreads upstream into the network, and same is the case downstream as well.",
                    "label": 0
                },
                {
                    "sent": "So since this document has high amount of flow on the topic, the flow has to spread through its roots children.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The flow keeps spreading downstream as well, and since there is some flow here.",
                    "label": 0
                },
                {
                    "sent": "Also explain the topical content so there's some topic loading in those documents as well so that it keeps spreading across the network.",
                    "label": 0
                },
                {
                    "sent": "So that's how the influence spreads across the whole network and the influence with capture is actually global, not just local.",
                    "label": 0
                },
                {
                    "sent": "And so I'll just compare.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The topic flow model with another popular algorithm called the topic sensitive Pagerank.",
                    "label": 1
                },
                {
                    "sent": "So both these models capture the notion of global topic specific influence and both these models also are able to capture missing links using different formalisms.",
                    "label": 0
                },
                {
                    "sent": "For example, topics to patient uses teleportation where the random surfer is allowed to jump to a random document without any hyperlink, and we also use a similar formalism called sourcing as described, so we also allow like modeling of missing hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "The main difference is here.",
                    "label": 0
                },
                {
                    "sent": "So topic sense to page then requires a seed set of labeled documents for each topic to start the page rank algorithm biased towards documents on that topic.",
                    "label": 0
                },
                {
                    "sent": "But in this case we do not require any label documents.",
                    "label": 0
                },
                {
                    "sent": "Totally unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Also, the Pagerank algorithm assumes that the page rank for a given document is equally distributed among its children.",
                    "label": 0
                },
                {
                    "sent": "But topic flow model is able to weigh the citations based on the topical relevance of the documents cited.",
                    "label": 1
                },
                {
                    "sent": "So and also the topic says to page rank is not a joint model of topics and of words and hyperlinks.",
                    "label": 0
                },
                {
                    "sent": "It actually assumes topic labels to be given and then runs runs of Pagerank algorithm top of that.",
                    "label": 1
                },
                {
                    "sent": "But here we are doing a joint modeling of hyperlinks and text.",
                    "label": 0
                },
                {
                    "sent": "So also the topic since 2 page rank at the end of the process gives you a list of.",
                    "label": 1
                },
                {
                    "sent": "Page rank so given topic is this ranked list but topic flow model is actually able to tell you the amount of flow going through the network on each citation edge.",
                    "label": 0
                },
                {
                    "sent": "So we can actually track how the influence of a topic spreads across the network.",
                    "label": 0
                },
                {
                    "sent": "So I'll just talk briefly about.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Being an inference, so we have the following objective function.",
                    "label": 0
                },
                {
                    "sent": "So the first one is just the observed data log likelihood, the first term in the objective function in the second term is a regularization term of the flows.",
                    "label": 1
                },
                {
                    "sent": "We ensure that flows are as small as possible as defined by the L2 norm unless required by the observed data to explain the observed phenomenon, and we also have.",
                    "label": 0
                },
                {
                    "sent": "Unlike LDA we have.",
                    "label": 0
                },
                {
                    "sent": "This is a more constrained optimization problem since we need to balance the flows at this document.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Equality constraints, so we solve this problem in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steps first, since the problem is intractable to do exact inference, we use variational lower bound just like in LDA.",
                    "label": 0
                },
                {
                    "sent": "That's the first step.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also eliminate the equality constraints in two steps.",
                    "label": 0
                },
                {
                    "sent": "First, we convert the flow balance constraints into multinomial constraints.",
                    "label": 1
                },
                {
                    "sent": "Since the total incoming flow into a given document is spread equally among its children, we can use a multi normal distribution there to capture the distribution of the flows across the children.",
                    "label": 1
                },
                {
                    "sent": "And then we eliminate the multinomial constraints using logistic transformation of the multinomial variables, and that's how we get to.",
                    "label": 0
                },
                {
                    "sent": "Kind of unconstrained optimization problem, using these transform.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we do that, we can just apply standard gradient descent or LB FGS to learn the pyramid flow parameters as well as the topics in the corpus.",
                    "label": 0
                },
                {
                    "sent": "So here is a graphic of how the topic how the output of the model looks like.",
                    "label": 0
                },
                {
                    "sent": "So standard LDA model gives you all these for each topic it listed the top ranking words so you can get an estimate of what the topic is about in addition to the standard.",
                    "label": 0
                },
                {
                    "sent": "This output this model is also able to tell you what are the most influential documents on each topic, so this graphic is actually run on the ACL corpus consisting of the computational linguistics papers spanning over 30 years time.",
                    "label": 0
                },
                {
                    "sent": "So it finds that on the topic of machine translation.",
                    "label": 0
                },
                {
                    "sent": "Della Pietra's paper on mathematical mathematical translation is probably the most influential document, and not only that, it also tells you how the flows actually spread across the network, so this is a slice of the topic flows output in the neighborhood of the most influential document on the topic.",
                    "label": 0
                },
                {
                    "sent": "So it tells you you know how the flow spreads among its parents as well as children, and it quantifies the flow of influence as well.",
                    "label": 0
                },
                {
                    "sent": "We also did some empirical analysis on the ACL data and we compared the output with ACL's most cited papers and we found that six out of the 10 most cited papers also figure in the topic flows top 10 list at least one topic, so it's kind of a sanity check and we also found that all the 10 papers actually earlier in the topic flows top 50 two documents on at least one topic.",
                    "label": 0
                },
                {
                    "sent": "So all those things are captured at the top and also more important.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See what we found is that these top rated papers are also ranked pretty low on topics that are not relevant to those documents, so it's able to detect the topic specific influence of those documents.",
                    "label": 0
                },
                {
                    "sent": "So you didn't.",
                    "label": 0
                },
                {
                    "sent": "Topics are ranked pretty low, but on relevant topics are ranked very high, so that's the main distinction from page rank versus topic sensitive Pagerank or the topic flow model.",
                    "label": 0
                },
                {
                    "sent": "And we also found the papers are missing from the top 10 list are actually broad data set papers or evaluation metrics based papers like the Penn Treebank Paper or the Blue paper that talks about the Blau evaluation metric for machine translation.",
                    "label": 1
                },
                {
                    "sent": "So these are highly cited, but there's cited probably more often than not.",
                    "label": 0
                },
                {
                    "sent": "For the you know, because they use the data set or because they use the evaluation metric, but there's no direct influence in terms of the propagation of ideas, so this kind of.",
                    "label": 0
                },
                {
                    "sent": "To filter out documents that are typically inferential versus broad evaluation metrics, data set papers.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also we have some objective evaluations, so we did the task of citation recommendation.",
                    "label": 1
                },
                {
                    "sent": "So this is a task of predicting the citations for new documents given the model.",
                    "label": 0
                },
                {
                    "sent": "And so we chose this task cause in academic literature typically citations mean the document is both topically relevant as well as probably influential.",
                    "label": 0
                },
                {
                    "sent": "So performance on this task means the model is able to gauge topical influence.",
                    "label": 0
                },
                {
                    "sent": "So this is mean average precision is standard information retrieval metric and with the higher number of topics, the topic flow model as well as the.",
                    "label": 0
                },
                {
                    "sent": "As well as the topic since 2 page rank based on topic flows output both performed quite well compared to TF IDF baseline.",
                    "label": 0
                },
                {
                    "sent": "It also beats other state of the art relational topic models such as the RTM RTM is the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solid line here and we also did comparison on document completion, log likelihood and the model is quite competitive with RPM and also outperforms LDA.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a core data set consisting of scientific articles from computer science.",
                    "label": 0
                },
                {
                    "sent": "We also did the same analysis on the ACL data and the traffic flow model is quite good in terms of log.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likelihood as well, so to conclude.",
                    "label": 0
                },
                {
                    "sent": "There's a new model.",
                    "label": 0
                },
                {
                    "sent": "It's able to capture topical influence of documents as well as the topic sensitive Pagerank, but it requires no label documents completely unsupervised.",
                    "label": 1
                },
                {
                    "sent": "And it's also a good model for text, and it's comparable to the state of the art topic models in the in computing the log likelihood and in the future we want to apply this model to a range of other textual network data like blogs, web, social media, etc.",
                    "label": 0
                },
                {
                    "sent": "And we're also in the process of building a graphical browser that allows the users to track how the topics actually spread across the network, as I've shown in the graphic earlier, will actually building a browser and I'll be happy to share the hyperlink with people who are interested.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess that concludes my talk and I'll be happy to take any questions.",
                    "label": 0
                }
            ]
        }
    }
}