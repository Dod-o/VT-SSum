{
    "id": "ulofzsltngjt4mcr4ah6v7yo35hh6eig",
    "title": "Probabilistic Interpretation of Quasi-Newton Methods",
    "info": {
        "author": [
            "Philipp Hennig, Max Planck Institute for Intelligent Systems, Max Planck Institute"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_hennig_quasi_newton_methods/",
    "segmentation": [
        [
            "So I'll I'll talk about optimization to round up today.",
            "And as John said, I'll the point of the talk, at least for me, is to try to sort of take some general lessons that form the study of 1 particular kind of numerical method to the more general question of how does the probabilistic study of an American method.",
            "What kind of information does that?",
            "Just does this provide and how many made help us and other communities so advanced, warning you won't see too many numerical results in this talk.",
            "This is not because they aren't there, you can see them all in the paper."
        ],
        [
            "If you want it.",
            "OK, so as I said, I'm going to be talking about optimization started with very simple basic thing.",
            "What is an optimization algorithm?",
            "There are many different communities here at NIPS and elsewhere who do optimization, so I think I should be somewhat precise about what I actually mean when I talk about optimization.",
            "I'll save you have some objective function F which Maps from a high dimensional vector space or to the North to the real line and in numerical problem typically will assume that N is something large, so it might be high dimensional, maybe 10, maybe 100, maybe 10,000, maybe millions of parameters.",
            "So if you're training and you'll network and end might be in the millions maybe.",
            "And we're looking for some local minimum, because in that kind of dimensionality we can't hope at all to look for.",
            "Global Optima will call that local minimum X star, and we're guessing that over assuming that we have actually access to that function, we can evaluate it.",
            "Let's say without noise for the moment, and maybe we can evaluate some local features of it as well, like the gradient and maybe higher order derivatives as well at some arbitrary point X, for example by automatic differentiation.",
            "So this can just be done by a machine.",
            "So there's nothing random here.",
            "Somebody just gives us a function and we're trying to optimize it now the way that essentially all numerical optimization algorithms workers they start out with some initial guess as we've several.",
            "As we've already heard several times today.",
            "Let's call that X zero, and then they perform iterations that hopefully move towards a minimum so they move from location X item location XI plus one at that new point, they will evaluate the function and its gradient and something else maybe about the function build some local model for the function.",
            "So that's some kind of estimation or approximation.",
            "Use that model to decide on a search direction.",
            "Which is usually 1 dimensional.",
            "That's called align search.",
            "Then they do a search and that one dimensional space.",
            "This is very efficiently converges very fast, and then they move to a new estimate in that one dimensional search space, which is now a new estimate for X star and I call that XI plus one.",
            "Then they repeat.",
            "Anna simplest example that we all know is Newtons method which approximates the function by a quadratic using its Hessian and its gradient, and for some reason the literature the Hessian tends to be called B of the objective function.",
            "So whenever you see a be in the next few slides then you know what I'm talking about.",
            "the Haitian B for hessian.",
            "So in which direction does Newtons method search?",
            "I don't have to derive this because everyone has seen it before.",
            "The search direction is the inverse Hessian times the gradient of the function at the current point and then a minus in front because we're looking for a minimum.",
            "So being a socialist action, and we usually assume that the action is positive definite and therefore the function is convex and that makes things a lot easier.",
            "Maybe it isn't and then we'll have to deal with all sorts of problems and but we'll assume that we can somehow deal with that.",
            "OK, so that's not really an inference problem.",
            "It's kind of trivial.",
            "You just observe the things you want to see.",
            "You observe the Hessian, you observe the gradient, then you know everything.",
            "But in high dimensional optimization problems, we cannot usually actually invest the time to build this Hessian Andrew inverted.",
            "It's not that we can't have, we don't have access to it, it just takes too long to build.",
            "So if N is a million, then this object has while 10 to the 12 elements.",
            "Then you have to invert it, which is 10 to 15 or 10 to 18.",
            "So no chance, so yes.",
            "So there are two levels of assumptions here.",
            "One is that we can.",
            "We're doing a smart thing by doing a local.",
            "Approximation and the other assumption is sort of.",
            "We just observe something we're interested in, which is the Hessian.",
            "I'll today talk about that second step, which will now suddenly turn into an actual thing.",
            "We're trying to infer, and I won't question this first assumption of Newton, but it's a good.",
            "It's a good point so that there are papers written on various ways in which one could question that and say what are other options for other models that might work better.",
            "So Tom Baker has a nice one, for example called Beyond Newtons method.",
            "So this is quite interesting actually."
        ],
        [
            "Yes, so there's a family of methods called Quasi Newton methods.",
            "Becausw quasi Newton because they are estimating what Newtons method would do.",
            "So they're trying to be close to Newton's method.",
            "Which are actually, that's migrating argument learning algorithms, algorithms that are estimating the Hessian or the inverse Hessian of the objective function repeatedly.",
            "And they're doing that by starting out with a simple estimate, so it initially there is a B0, which might be the unit matrix, or some precondition or something simple.",
            "And then they update these estimates in some low rank fashion.",
            "So this is some outer product of maybe two matrices you envy which are somehow low rank, because if this is low rank then we can use the matrix inversion lemma and efficiently evaluate inverse of the Hessian and use it to estimate the direction of Newtons method.",
            "Now how do they do this?",
            "So this is the derivation for maybe the simplest possible cause, Newton method.",
            "But all the other quasi Newton methods have similar ways of going up sort of reaching their goal, and I'll catch up on some of them later on and some I'll just brush over.",
            "By the way, if you've never heard the term quasi Newton method before, if you were nips you, I'm pretty certain you heard the abbreviation BFG S before or DFP.",
            "These are all quasi Newton methods, elbil sort of little variants of this family of methods.",
            "There are even more.",
            "There's something called Brians method, as I say here.",
            "There's another one called Powell symmetric broyden and as an SR1 method and they all fall into this framework.",
            "So how do they work?",
            "They say I'm going to construct an estimator.",
            "And I want my estimated to fulfill a an approximate property of the Hessian.",
            "I'm going to want it to fulfill a difference relation, so I want the difference between the Hessian at the previous.",
            "Sorry sorry the gradient at the previous location and the gradient where I'm currently at is equal to the estimate of the Hessian times the distance between those two points, so that's a difference relation.",
            "If the function were actually a quadratic, then this would be an exact statement.",
            "If the function is quadratic but sufficiently regular, it's approximately true.",
            "And then usually the literature, because this is sort of long notation.",
            "These objects are short and this is called Y and this is called S and the rest of the slides will be full of why is an SS, but you don't actually have to fully understand all this algebra now.",
            "Clearly this is a vector.",
            "This is a matrix, so there are numbers in here N squared numbers in here.",
            "That's not enough to estimate N squared numbers, so we need to regularize and the way that the classic derivation of these methods works is we're going to regularize by some Frobenius norm.",
            "So we're going to say we want that new estimator which fulfills this equation, but it's.",
            "Also, as close as possible under the Frobenius norm to our previous estimator, so we don't want to move too much that's sufficiently regularize.",
            "One way to write the Frobenius norm maybe somewhat convoluted, but helpful for what's going to follow is as a quadratic form.",
            "So here's the difference between two matrices.",
            "This notation up here means that I take these matrices and stack them into a long vector.",
            "I think a lot of you have seen this before, and then I can fight.",
            "The phobia is known as a quadratic form involving some weight matrix in the middle, which in general.",
            "I'll call V and this is the Kronecker product.",
            "If you've never seen this before, it doesn't matter.",
            "It's going to vanish in a second if you know what it is then it's obvious.",
            "And then it can be shown that the unique estimator that minimizes this regularizer subject to this restriction is this update here.",
            "So the new Hessian is the old fashion plus some outer product between vector and another vector.",
            "And there are some scalar down here.",
            "Now this workshop is about probabilistic numerics.",
            "So what is one way of interpreting this for probabilistic perspective is actually relatively straightforward.",
            "This is sort of this classic insight that a regularised loss can usually be interpreted as a log likelihood plus some log prior, and then what we're looking for here is the map, sort of."
        ],
        [
            "Similar some posterior in.",
            "In this case, what does that mean and precious?",
            "Just pointed out that this does not necessarily have to be true, but it's sort of one way of.",
            "Sort of in a forward way, generating these models.",
            "So I'm saying if we make the following assumptions then we end up with the same algorithm.",
            "I'm not saying this is the only way to get to these algorithms.",
            "Anne.",
            "So this sort of strict requirement advise to be equal to BS.",
            "This this this is going to be our likelihood.",
            "This likelihood is a sort of Dirac Delta point mass which says Y is equal to BS, and that's the limit of a Gaussian distribution.",
            "And a quadratic form in a regularizer is of course the logarithm of ocean.",
            "So what we have here is a Gaussian prior times the Gaussian likelihoods, which of course returns a Gaussian posterior and that new Gaussian posterior has a mean which is equal to its mode, and that is exactly.",
            "Our updates are new best guess on your estimate.",
            "It also has a posterior uncertainty covariance which does not feature in these classic algorithms because they are not necessary to just find the maximum.",
            "I'll just sort of keep them around, keep it around anyway.",
            "If you look at this expression is sort of.",
            "This looks complicated, but you might notice that it's sort of asymmetric.",
            "There's something happening on the right hand side of the Chronicle product, nothing happening on the left hand side.",
            "That's maybe a bit weird.",
            "Nevertheless, so this is."
        ],
        [
            "Sort of my first.",
            "There will be several of these slides.",
            "Benefit number one of.",
            "Treating an American algorithm for probabilistic perspective using a probabilistic interpretation can help uncover some kind of implicit assumptions and give you a better feeling for what is going on in these algorithms.",
            "So at least this particular cross Newton method can be interpreted as following from a Gaussian prior over the passion of protective function relative to the previous estimate.",
            "And the Gaussian likelihood.",
            "Now if you have."
        ],
        [
            "Been sort of looking at this update closely, then something you may have noticed or may not is that this matrix here is not symmetric.",
            "And that's annoying because sessions are symmetric matrices, so we really want our estimate to be symmetric, so this is not symmetric, because this is a vector and this is a vector and the two are not the same.",
            "So if I transpose this is not just not the same matrix anymore, even if this is a symmetric matrix now."
        ],
        [
            "We're not the first people to notice this.",
            "Numerical maths is of course full of very smart people and they know."
        ],
        [
            "Just this.",
            "At least in the 50s, probably even earlier, as I learned today.",
            "Ann, but let's just forget that there is no metrics for a second.",
            "Let's think about what we would do if we were faced with this problem.",
            "So as a proper probabilist, I would say, well of course I have to encode this knowledge.",
            "I should put it in my prior.",
            "How do I do this?",
            "I encode this with a another likelihood term or another factor in my graphical model.",
            "Depending on how you want to phrase this using another linear operator called operator Delta, that's the operator that takes in a matrix stepped into a vector and returns the difference between the matrix and its transpose.",
            "So that's the anti symmetry operator that exists in physics as well.",
            "Anne.",
            "And this is of course a linear operator, because it just consists of rearranging the elements of a matrix and subtracting them from each other.",
            "So that's a linear operation, so we could have another Dirac Delta term which says the matrix is equal to its transpose.",
            "So Delta B is equal to 0.",
            "Now what you can do is you can look up in some of these smart algebra books from the 50s what the rank of this operator is, because that's a well known operator and it turns out it's rank it's over North.",
            "Squared is the same rank as that as the number of elements in a lower triangular matrix.",
            "Or a lower triangular matrix.",
            "So that's unfortunate, because so let's let's imagine even if we knew what the inverse of that of this operator is, which we're going to need for our inference.",
            "I don't actually know that, but maybe there's a way of writing it down.",
            "Then to do this operation, this projection we would have to take all the elements of the matrix of our estimator and sort of jumble them up and project them in the right way using this inverse of this operator.",
            "And that's going to take at least quarterly many operations.",
            "So this is going to be too expensive to use in a proper and actual numerical algorithm.",
            "So we're sort of stuck with our smart probabilistic logic here, and what I did then.",
            "Or what we did then.",
            "Martin and I have both paper together earlier this year is we looked at the classic methods again and said, well, they seem to have solved this.",
            "How did they do this?"
        ],
        [
            "And so I'm going to spare you all the gory details.",
            "The result is they essentially introduce a second observation.",
            "You can interpret what's going on as a second independent.",
            "Term a second factor that says not only have I observed Y is equal to BS, so this difference equation this way round.",
            "I've also observed that Y transpose is equal to X transpose times B.",
            "So that's not the same as saying that the matrix is symmetric, because it's only saying it acts like a symmetric matrix in the space spanned by S, so it's a much weaker statement.",
            "Nevertheless, if you multiply in this factor into your current posterior after the first step, then you'll get out a new posterior mean, which is more complicated, but it turns out this is a rank to update, so it's still low rank, and it's symmetric.",
            "So if you stare at this for awhile, you'll notice that it's a symmetric object.",
            "And lo and behold, the posterior covariance is also now a very sort of symmetric, neat little little object.",
            "Not saying that this means much, it's just sort of pleasing to the eye.",
            "Anne.",
            "So now the question is.",
            "Again, from our probabilistic perspective, are we allowed to do this because it sounds really weird, right?",
            "We make one observation and then we treat it like 2 independent observations and stick it into the same algorithm.",
            "That sounds like double counting.",
            "So really, we probably shouldn't be doing that.",
            "To understand.",
            "That you are actually allowed to do that.",
            "You have to look again at the prior that these algorithms have so this.",
            "Chronicle product in here in our in our prior covariance and again."
        ],
        [
            "Leaving out a few details, it turns out that what this amounts to is the assumption that the that the function we're trying to optimize.",
            "Actually depends on twice the number of input parameters that it really has, it treats.",
            "Vectors and covectors as two separate things.",
            "It says there's a thing called X, and there's a thing called X transpose, and I can take the derivative of a function with respect to X transpose and with respect to X, and then I can take the derivative respect to X transpose or with respect to X.",
            "And outcome two different separate objects.",
            "One is this sort of the left right hash, and one is the right left session, and we know that they are the same.",
            "But this algorithm doesn't know it.",
            "It treats them as separate independent objects, and it just observes that they happen to be equal.",
            "And that ends up with a symmetric mean.",
            "So the posterior actually does not put.",
            "Mass only on symmetric matrices.",
            "In fact, if you sample from this posterior with probability one, you'll get a non symmetric matrix.",
            "But the mean happens to be symmetric.",
            "So one way of thinking about this is as some kind of relaxation.",
            "So there's this is our input domain now, so there's all the vectors X.",
            "Or you can think of.",
            "Think of this as a N dimensional vector space and there all the X transpose and that real function depends entirely on this linear subspace here, and we're only ever going to evaluate it at this point.",
            "But we are introducing observations, which essentially means that we first observe the integral of the function from here to here from here to here, then from here to here.",
            "And we don't know anything about how the function behaves along here.",
            "So if this weren't the parametric model but have full Gaussian process, I could sample from it, and if it would move freely along here, but have a perfectly defined integral from here to here.",
            "And then I make a second observation, which is exactly opposite opposite way around and the two together give me more information than just one of them would, even on this diagonal subspace."
        ],
        [
            "So here's a takeaway number 2.",
            "Actually this works the other way around as well.",
            "So looking at America algorithms may give us hints for cool little tricks that can help us with machine learning algorithms, so this is a way of learning as well.",
            "Finding an estimator for symmetric matrix of very low computational cost.",
            "I'll point out data that's actually of linear costs.",
            "It's possible to do this in linear cost.",
            "Now we have many areas in NIPS.",
            "You have seen several posters this year where people are learning some kind of metric or some kind of matrix that happens to be symmetric.",
            "So here's a trick that can do this.",
            "She, by the way, if you want to hear more about the details of this is a nice email paper.",
            "This just been accepted, John.",
            "If you go back."
        ],
        [
            "That one so.",
            "What you hear sometimes?",
            "And I'm trying to help understand the relationship here, which you hear sometimes is that people say if you're working with structured matrices, symmetric, or any symmetric matrices, the provenience norm, which is just implied Euclidean metric, is the wrong.",
            "Normally using because essentially you're double counting all of your off diagonal elements, right?",
            "So we want him to be symmetric, and so I'm going to count all the diagonal elements once and all the off diagonal elements twice.",
            "So is that the same thing that's going on?",
            "So this is weaker.",
            "The resulting as I said, so the this is what you would like to have is a belief that directly encodes that there are only half N * N + 1 three parameters in this object rather than N ^2.",
            "And I think what you are going through might be a way of doing that if I understand it right now.",
            "Sing to go back one step further back to when you said use the Frobenius norm.",
            "You could use, you could use a normal, it's more back and then the update is going to be more expensive I think.",
            "Yeah, I guess similar thought would be instead of a Gaussian prior on the elements of the matrix.",
            "Then yes, something like we should.",
            "Distribution would naturally exactly yeah, and then it's going to be very tricky to do the inference in linear algebra, fashion, innovation.",
            "And yet another thought is you could start with a Gaussian process prior on the function.",
            "Say I'm going to learn the function and now I can sort of indie.",
            "So if you do the opposite thing of what Ben's been talking about, I can just differentiate that the Gaussian process twice, and then I know what the Haitian is everywhere.",
            "Well, I can estimate.",
            "Have a belief.",
            "Ovation is everywhere that's going to be a symmetric matrix, But again, doing that is going to be at least cubically expensive to invert it.",
            "Anne."
        ],
        [
            "OK, so now we have this benefit now is sort of reverse the arrow again.",
            "And let's see whether we can do something for numerical algorithm with with our probabilistic knowledge.",
            "So what we did is we sort of the obvious thing for.",
            "A Bayesian machine learner.",
            "Sort of a Gaussian process everywhere, so we're moving from a parametric model for the Hessian, which says the Hessian is sort of locally constant and sort of Gaussian constant relative to the previous session to a nonparametric model which says the Hessian is actually in a function which returns an N square matrix at every point in the input domain.",
            "So there are two an input dimensions at every point in that in that space we can return a matrix and we do that with.",
            "Two kernels and these two kernels have played the same role that there's two movies played in the previous slide, so they also encode that this fact that we're treating XNXX trans separate things to keep our computational costs low.",
            "And now once we do this, we can do something like interesting, which is that we can make a more precise statement about about the likelihood.",
            "In fact, you can make the exact statement.",
            "So before our likelihood was this finite difference equation, which says the difference in the gradients is equal to the Haitian times the difference in the steps.",
            "And now, because we have no parametric formulation, we can actually encode the exact statement, which is that the Hessian.",
            "Is sorry, the gradients are the integral over the Hessian, so the difference between two gradients is the line integral over the Hessian along the line connecting those two, and this is what these complicated long equations you are saying.",
            "We can do that and then what does leads us through what that forces us to do?",
            "An inference is to integrate our kernels.",
            "You have to choose a kernel such that we can do this integral in some sufficiently fast way.",
            "And in our case we are currently using the very popular square exponential, Gaussian radial basis function kernel for that.",
            "These integrals, the resulting integrals you have to do an inference boiled down through bivariate Gaussian integrals.",
            "Those are not analytic, but they can be done very efficiently numerically with numerical integration.",
            "We've got very fast code for that by now.",
            "The maybe surprising thing is.",
            "So.",
            "Usually when you hear the word Gaussian process, everyone thinks so that's going to be expensive because Gaussian processes are expensive.",
            "They're keeping the number of data points.",
            "And that's true.",
            "So at least naive Gaussian process inference involves inverting a matrix and matrix of the size, number of function evaluations, times, number of function, elevations, not of size, number of input domains, input dimensions of interventions.",
            "So this algorithm is still linearly expensive in the number of input dimensions, and we so it scales to very large scale problems if now actually running on computer vision problems with more than a million parameters, and so for about a million parameters currently are still not so nice.",
            "Code takes about half a second.",
            "Update so this is something that just works.",
            "Of course we have this issue that yes we cannot store every data point because that's going to make us eventually make a slow, but here sort of a feature of optimization comes to our rescue, which is that an optimizer is not a regression algorithm that has a bounded domain and then keeps evaluating at random points and then somehow builds a model and optimizing walks along a trajectory.",
            "So if you made 50 observations, chances are the observation you made 50 steps ago are not so helpful anymore.",
            "That's the bfsi dear.",
            "Again, an old idea.",
            "We are using that to make our algorithm fast.",
            "OK, so yes you go back.",
            "Intuition about what this T parameter that you're integrating with, respectively.",
            "So geometrically what.",
            "So this is, so I'm parameterising the.",
            "So we've taken, evaluated, here, and here.",
            "There's a line in between.",
            "This is the vector S and we are amortizing this by T from T equal to 0 equal to 1.",
            "Um?",
            "OK, so here's some.",
            "Just some things to point out that this kind of treatment gives."
        ],
        [
            "Right away, the first one already pointed out.",
            "We now have an exact treatment of gradient observations.",
            "We can actually say what we mean by when we observe a gradient.",
            "It's the integral over the issue.",
            "We can do that so that gives us an actual Gaussian process posterior over the elements of the Hessian.",
            "So here's the everyone.",
            "Well, not everyone's, but sort of a very, very famous optimization problem is actually non convex, but it's sort of reasonably well behaved.",
            "This is rosenbrook polynomial, also known as the banana function.",
            "You start out somewhere up here optimize along this Valley and this is where the minimum is.",
            "Once our algorithms run along this trajectory, we can ask it what do you believe that the Hessian is actually and so the escalation of this two by two.",
            "Or dimension dimension 2 input function is 2 by two, so there are three elements to the Hessian, but only one of them is actually really interesting.",
            "All the other ones have a very, very boring structure.",
            "This is the truth.",
            "This is what the Haitian actually looks like.",
            "This is this is the one one element, so the second derivative of the of this of this dimension.",
            "Here is the posterior mean estimate of our algorithm after it has run.",
            "So in the regions under two plots of the same color scale.",
            "So in the regions where we've been, we make a very good estimate which is reasonably correct when I don't plot here, because this sort of don't didn't quite get the PDF to work, is uncertainty around this.",
            "Of course, it's also true that there is some kind of sausage of uncertainty around here, which means up here and down here.",
            "We don't actually know what the issue is, and we are aware of the fact that we're not.",
            "We don't know what the issue is."
        ],
        [
            "Some other neat thing we can do is because we have another metric formulation.",
            "This is maybe a somewhat complicated plot.",
            "We can.",
            "We can now use every observation made anywhere.",
            "So for example, any observation made.",
            "At some other locations, some other gradient observation can be directly included in the model, so that means we can parallelize this algorithm.",
            "We can also use all the observations made during one line search, so a traditional algorithm makes a line search that involves maybe 1, two, three or four function evaluations, and then it has to throw away all the evaluations made during the line search because there's no algebraic room in the framework to encode it in the update.",
            "So that's the picture here.",
            "Let's say we start with this gradient.",
            "Then we observe this gradient equation.",
            "We overstepped of moves that move back, said.",
            "Now we're done with our line search.",
            "You have to throw out everything we made.",
            "We observe in between and that amounts to observing just the integral from here to here.",
            "If you keep all those steps around, we can tell our Gaussian process regressor and that works because it's nonparametric, so there are sufficiently many eigenvalues available to encode everything that we've made all these observations and that gives us a much more detailed picture of what the function looks like.",
            "That's a helpful thing to have, because keep in mind that each of these Gray lines represents maybe a million numbers, because it's a.",
            "It's a gradient of the objective function."
        ],
        [
            "Some other trick.",
            "So now I'm just sort of starting to ramble on all the things that immediately come to your mind.",
            "We know how to do, how to treat noise in regression.",
            "Sort of standard thing, so I'm very happy to report that we just got a paper into HTML weirdly doing NIPS.",
            "So really weird feeling.",
            "On doing regression with noise, so sort of the the.",
            "DD appetizing slogan might might be stochastic Newton descendants that also has the gradient descent, even though that's a little bit little bit over doing it.",
            "Maybe so here are a bunch of algorithms trying to minimize a very simple 2 dimensional function without noise.",
            "The blue one is our numerical algorithm, the red one is gradient descent, which is known to be only linearly efficient and lines up here.",
            "The pink and the green line with our various ways of exact Newtons method.",
            "So this is gradient, Hessian free and actually just numerical invert inversion of a automatically calculated.",
            "And obviously this Newton is just the best algorithm of those because it has more information available.",
            "It's just inverted matrix.",
            "Now once you add noise in the just on the elements of the of the gradient, all these classic algorithms just sort of become much less convergence.",
            "Not very surprisingly, this is well known, but if you sort of exact actually model the fact that there is noise on your observations, then you can still get a meaningful estimate for the Hessian and still converge.",
            "And by the way, this so this is the range of basically all double precision floating point numbers, so there's.",
            "Really a lot of difference in between those two lines.",
            "So there's no no batch, here is just actual evaluations of a function.",
            "I'm not training manual, network or anything, it's just just a function with noise.",
            "That function with noise could be from some small batch of the larger batch, yes, so in the paper there is an experiment with a neural network where we basically just we just copy paste it.",
            "Jeff, Jeff Stephens code and replaced our algorithm instead of stochastic gradient descent.",
            "And then you have the situation that if you have if you have one mega Patch and you have no noise essentially.",
            "And as you make mini batches smaller and smaller, you have more and more noise on your observations relative to some latent function you can't observe, and this is essentially what this is doing.",
            "By your grade central bank.",
            "PDF version.",
            "Yes, so this is like this.",
            "So this is a toy problem.",
            "It's just sort of pointing out that you can learn with noise.",
            "And various other ideas that I'm not going to talk about, mainly because they don't actually have results on them yet.",
            "There's just sort of, but I guess everyone can have their favorite idea of what you can do with regression in optimization, because we know so much about regression."
        ],
        [
            "For example, you could imagine that your function is dynamically changing, so if you're a roboticist and you're expecting your sort of every 40 milliseconds, you're solving optimization problem.",
            "Maybe you can't run your beef Stew convergence, but you can do it for one step or two steps, and then say something has changed a little bit, but not very much.",
            "And I just keep on running by using a comma."
        ],
        [
            "Filter in that framework.",
            "OK, so benefit #3 is.",
            "Probably interpretation American method may lead to sort of connections to other areas of inference and machine learning and statistics, and that can help us to gain all sorts of generalizations, improvements, other applications I don't know.",
            "Now I don't want to just."
        ],
        [
            "The ultra positive I also want to point out a few issues that are challenges rather than benefits so.",
            "If you read a numerical analysis paper on optimization algorithm, then of course these papers are very precise about convergence is and they can't contain usually convergence algorithm convergence proofs and one very basic property of an optimization algorithm that people would like to see is called locally linear convergence or local linear convergence.",
            "This is saying so this is sort of somewhat simplified statement, but this is saying that if we start out at an initial point which is epsilon close to the true minimum.",
            "And we start out with an estimate for the Hessian that is Delta close to the true Hessian.",
            "Then our algorithm will converge and it will converge linearly fast.",
            "Oh sorry, yeah yeah, that's true.",
            "Now, unfortunately, it turns out this nonparametric algorithm I just presented so this."
        ],
        [
            "Here.",
            "Does not, at least not easily have that property.",
            "So in the sense that at the."
        ],
        [
            "But I don't know how to show it, so at least the classic ways of showing it the usual theorems do not apply the user elevated.",
            "This is shown is to show that from one step to the next, the error on the on the Hessian drops at most linearly fast, or sorry, gets worse, at least at most linearly fast.",
            "Because that's something you can recover by sort of more than linear converges.",
            "Now in a non linear regression you can sometimes have terms that are non linearly large.",
            "Now, in some sense, that's something that hasn't held us back from using nonlinear regression.",
            "In other areas where the same applies.",
            "But of course it's a big question whether that causes a huge problem for optimization or not.",
            "So far I haven't seen a case where our algorithm doesn't converge.",
            "But as Perseus pointed out, experiments are nice.",
            "Theorems are nice as well.",
            "So far we don't have a theorem."
        ],
        [
            "So here's a real challenge.",
            "Anne.",
            "The two frameworks, the way that we sometimes work in probabilistic inference and the sort of thing that a numerical tool actually requires in terms of reliability and robustness is not always easy to get.",
            "And sometimes that might be just the problem of re phrasing the convergence and just finding a more parsimonious description of what we're actually looking for, but sometimes it might be a real problem, and then we should sort of accept that.",
            "So I guess the challenge for someone trying to work in both of these fields is that we have to decide where we want to sort of accept the other communities way of working and adapt to it, and where we want to challenge it, and then try to change it."
        ],
        [
            "So to summarize.",
            "I too, of course.",
            "Of my own work, but also try to sort of make some make some connections too.",
            "Of wider view.",
            "And some examples were that on the concrete side there is a connection between Quasi Newton optimization and least squares regression.",
            "And that's an example of how a probabilistic formulation can help improve understanding of some numerical method.",
            "I've and then turned out that the way that these algorithms achieve symmetry in the estimates.",
            "Is a very, very intriguing algebraic assumption in the prior.",
            "So this is an idea that we can take from.",
            "Numerics and applying machine learning.",
            "Maybe, at least in this particular case.",
            "I then showed some like a long list of extensions that are become immediately obvious.",
            "Once you have this inside, so I think this is something that happens a lot at once.",
            "Once you have a probabilistic formulation, you can just use everything you know from other areas and just plug them in, because probability formulations are so nicely modular.",
            "But there are also some challenges.",
            "In particular, if you want to use all these great extensions, then sometimes in the process will throw out some assumptions that are usually made for the for the existing numerical algorithm and we have to be careful not to throw out the baby with the bathwater by doing that.",
            "So by the way, if you want to read more on the details of this so I didn't show you many numerical results and performance results, you can look them all up in our papers and you can download code for some of the algorithms from my website.",
            "Thanks.",
            "We got time for a couple questions.",
            "So I'm curious about this cultural challenge.",
            "I mean, I can perfectly imagine that that is there."
        ],
        [
            "You have you actually tried to?",
            "I'm in the process of trying.",
            "I haven't actually submitted that paper yet.",
            "I'm saying we can't even prove so I had I had a result like a response like that from one of the ICL reviewers.",
            "Is that you can't really submit an optimization algorithm for which you don't even know whether we converges or not.",
            "I guess in this particular case that reviewer was trumped by the other reviewer.",
            "But I'm expecting more of that as I send these papers to numerical values.",
            "Well and of course I'm trying to actually address them in the paper as well.",
            "So ideally you would have a proof of something else that is also convenient.",
            "Yeah.",
            "Any other questions?",
            "I've got a handful of questions that I could ask now or or during the panel, but I'll put one in here.",
            "Something that that your talk highlighted quite a bit as well as ya sex talk."
        ],
        [
            "Is this idea that we would love to carry around 2nd order information, but can't because it's practically too expensive so you've gone ahead and reinterpreted quasi Newton methods in a probabilistic framework?",
            "And then I was talking to ask about this before you see a path to doing the same for.",
            "Exact search methods in interior point.",
            "I think I'm not qualified enough to give a sort of a meaningful answer to that, because I just don't know too little about interior point methods.",
            "I mean, I think you've asked Jessica as well, and maybe we're both sort of not not close enough to the to the divide to give it to give a meaningful answer to that.",
            "It would of course be neat, but I have absolutely no idea, but it's not so difficult because in the end.",
            "The Hessian multiplied by the gradient, and you've chosen two different ways to do that besides that.",
            "That would be really cool if there was some connection here.",
            "It's just that I haven't looked at it at all, so I can't make any qualified statements.",
            "Timo in practice, if you compare to those images, so in the, of course, because the machine learning paper in the original ICM paper we had that plot where you're better than everyone else.",
            "Not everyone always has in their paper, and so we have.",
            "What we are currently working on and this was sort of sidetrack for awhile, is a larger benchmark set on which to make a meaningful statement of this actually works this much better or worse on.",
            "A large number of problems optimization community has these benchmarks.",
            "They are maybe biased towards certain types of problems that show up in other non machine learning areas.",
            "So for example we tend to be very low dimensional.",
            "We're trying to add some of our own some large scale problems.",
            "Anne.",
            "So far I haven't seen a case where our algorithm is.",
            "Sort of noticeably worse than DFCS.",
            "I've seen several cases where it's a lot better.",
            "I haven't seen a case yet where it doesn't converge apart from super.",
            "It'll what's the word condition problems on which just know optimizer converges so that it's easy to write an optimization problem that just just nothing converges on right?",
            "Because it's some tiny little sliver.",
            "So we compared to BF, GSL, Bgcse, DFP, various other ones.",
            "Anne.",
            "So ours is similar in the spiritual beliefs in the sense that we throw out all things.",
            "Anne.",
            "So one comparison is the LDS.",
            "We're also comparing to Pfc.",
            "So far.",
            "So there is a destrier difference between objects, and that's well known, and they behave in slightly different ways.",
            "How they different behavior depends on the size of the problem, right?",
            "Because essentially with every observation you throw out a rank one.",
            "Part of your of your prior uncertainty so.",
            "If your memory limit is larger than the domains that input domain, then that's fine.",
            "Then you're sort of almost back in.",
            "What gifts would be doing so so that the results are sort of?",
            "It's very hand WAVY because because the differences between algorithms are sort of sketchy.",
            "I might have missed it, but it didn't seem like you were using the uncertainty.",
            "I don't at all, so this is one of the this is this."
        ],
        [
            "Is one of the things that are hidden behind this Graybar, yes, so it's obviously something you would like to think about some kind of active learning of how.",
            "So essentially you can treat these sort of interpreted algorithms as V enforcement learning algorithms that are that are only exploitive.",
            "They're greedy, they take a data point, make their best guess that can make, and just run it direction.",
            "And you could imagine so on the one hand, you could imagine that maybe some exploration is a good idea.",
            "You should sort of decide on some directions to look for at the same time.",
            "There's also that, so with numerical algorithms, there's this sort of.",
            "The issue with that, usually the important parts of the Hessian are sort of.",
            "Pointing out all the eigenvectors of the Hessian are pointing in a similar direction to the gradient.",
            "Right on some this is very vague, but it's not a precise statement, so maybe you don't actually want to look orthogonal to the gradient, because that might just be the really pretty boring part of the problem anyway, or irrelevant for the optimization problem, so I don't have a good answer on that, but it's an obvious obvious thing to address.",
            "OK.",
            "I think."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll I'll talk about optimization to round up today.",
                    "label": 0
                },
                {
                    "sent": "And as John said, I'll the point of the talk, at least for me, is to try to sort of take some general lessons that form the study of 1 particular kind of numerical method to the more general question of how does the probabilistic study of an American method.",
                    "label": 0
                },
                {
                    "sent": "What kind of information does that?",
                    "label": 0
                },
                {
                    "sent": "Just does this provide and how many made help us and other communities so advanced, warning you won't see too many numerical results in this talk.",
                    "label": 0
                },
                {
                    "sent": "This is not because they aren't there, you can see them all in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want it.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I said, I'm going to be talking about optimization started with very simple basic thing.",
                    "label": 0
                },
                {
                    "sent": "What is an optimization algorithm?",
                    "label": 0
                },
                {
                    "sent": "There are many different communities here at NIPS and elsewhere who do optimization, so I think I should be somewhat precise about what I actually mean when I talk about optimization.",
                    "label": 0
                },
                {
                    "sent": "I'll save you have some objective function F which Maps from a high dimensional vector space or to the North to the real line and in numerical problem typically will assume that N is something large, so it might be high dimensional, maybe 10, maybe 100, maybe 10,000, maybe millions of parameters.",
                    "label": 0
                },
                {
                    "sent": "So if you're training and you'll network and end might be in the millions maybe.",
                    "label": 0
                },
                {
                    "sent": "And we're looking for some local minimum, because in that kind of dimensionality we can't hope at all to look for.",
                    "label": 1
                },
                {
                    "sent": "Global Optima will call that local minimum X star, and we're guessing that over assuming that we have actually access to that function, we can evaluate it.",
                    "label": 0
                },
                {
                    "sent": "Let's say without noise for the moment, and maybe we can evaluate some local features of it as well, like the gradient and maybe higher order derivatives as well at some arbitrary point X, for example by automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "So this can just be done by a machine.",
                    "label": 0
                },
                {
                    "sent": "So there's nothing random here.",
                    "label": 0
                },
                {
                    "sent": "Somebody just gives us a function and we're trying to optimize it now the way that essentially all numerical optimization algorithms workers they start out with some initial guess as we've several.",
                    "label": 0
                },
                {
                    "sent": "As we've already heard several times today.",
                    "label": 0
                },
                {
                    "sent": "Let's call that X zero, and then they perform iterations that hopefully move towards a minimum so they move from location X item location XI plus one at that new point, they will evaluate the function and its gradient and something else maybe about the function build some local model for the function.",
                    "label": 0
                },
                {
                    "sent": "So that's some kind of estimation or approximation.",
                    "label": 0
                },
                {
                    "sent": "Use that model to decide on a search direction.",
                    "label": 1
                },
                {
                    "sent": "Which is usually 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "That's called align search.",
                    "label": 0
                },
                {
                    "sent": "Then they do a search and that one dimensional space.",
                    "label": 1
                },
                {
                    "sent": "This is very efficiently converges very fast, and then they move to a new estimate in that one dimensional search space, which is now a new estimate for X star and I call that XI plus one.",
                    "label": 0
                },
                {
                    "sent": "Then they repeat.",
                    "label": 0
                },
                {
                    "sent": "Anna simplest example that we all know is Newtons method which approximates the function by a quadratic using its Hessian and its gradient, and for some reason the literature the Hessian tends to be called B of the objective function.",
                    "label": 0
                },
                {
                    "sent": "So whenever you see a be in the next few slides then you know what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "the Haitian B for hessian.",
                    "label": 1
                },
                {
                    "sent": "So in which direction does Newtons method search?",
                    "label": 0
                },
                {
                    "sent": "I don't have to derive this because everyone has seen it before.",
                    "label": 0
                },
                {
                    "sent": "The search direction is the inverse Hessian times the gradient of the function at the current point and then a minus in front because we're looking for a minimum.",
                    "label": 0
                },
                {
                    "sent": "So being a socialist action, and we usually assume that the action is positive definite and therefore the function is convex and that makes things a lot easier.",
                    "label": 0
                },
                {
                    "sent": "Maybe it isn't and then we'll have to deal with all sorts of problems and but we'll assume that we can somehow deal with that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's not really an inference problem.",
                    "label": 0
                },
                {
                    "sent": "It's kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "You just observe the things you want to see.",
                    "label": 0
                },
                {
                    "sent": "You observe the Hessian, you observe the gradient, then you know everything.",
                    "label": 0
                },
                {
                    "sent": "But in high dimensional optimization problems, we cannot usually actually invest the time to build this Hessian Andrew inverted.",
                    "label": 0
                },
                {
                    "sent": "It's not that we can't have, we don't have access to it, it just takes too long to build.",
                    "label": 0
                },
                {
                    "sent": "So if N is a million, then this object has while 10 to the 12 elements.",
                    "label": 0
                },
                {
                    "sent": "Then you have to invert it, which is 10 to 15 or 10 to 18.",
                    "label": 1
                },
                {
                    "sent": "So no chance, so yes.",
                    "label": 0
                },
                {
                    "sent": "So there are two levels of assumptions here.",
                    "label": 0
                },
                {
                    "sent": "One is that we can.",
                    "label": 0
                },
                {
                    "sent": "We're doing a smart thing by doing a local.",
                    "label": 0
                },
                {
                    "sent": "Approximation and the other assumption is sort of.",
                    "label": 1
                },
                {
                    "sent": "We just observe something we're interested in, which is the Hessian.",
                    "label": 0
                },
                {
                    "sent": "I'll today talk about that second step, which will now suddenly turn into an actual thing.",
                    "label": 0
                },
                {
                    "sent": "We're trying to infer, and I won't question this first assumption of Newton, but it's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good point so that there are papers written on various ways in which one could question that and say what are other options for other models that might work better.",
                    "label": 0
                },
                {
                    "sent": "So Tom Baker has a nice one, for example called Beyond Newtons method.",
                    "label": 0
                },
                {
                    "sent": "So this is quite interesting actually.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, so there's a family of methods called Quasi Newton methods.",
                    "label": 0
                },
                {
                    "sent": "Becausw quasi Newton because they are estimating what Newtons method would do.",
                    "label": 0
                },
                {
                    "sent": "So they're trying to be close to Newton's method.",
                    "label": 0
                },
                {
                    "sent": "Which are actually, that's migrating argument learning algorithms, algorithms that are estimating the Hessian or the inverse Hessian of the objective function repeatedly.",
                    "label": 0
                },
                {
                    "sent": "And they're doing that by starting out with a simple estimate, so it initially there is a B0, which might be the unit matrix, or some precondition or something simple.",
                    "label": 0
                },
                {
                    "sent": "And then they update these estimates in some low rank fashion.",
                    "label": 0
                },
                {
                    "sent": "So this is some outer product of maybe two matrices you envy which are somehow low rank, because if this is low rank then we can use the matrix inversion lemma and efficiently evaluate inverse of the Hessian and use it to estimate the direction of Newtons method.",
                    "label": 0
                },
                {
                    "sent": "Now how do they do this?",
                    "label": 0
                },
                {
                    "sent": "So this is the derivation for maybe the simplest possible cause, Newton method.",
                    "label": 0
                },
                {
                    "sent": "But all the other quasi Newton methods have similar ways of going up sort of reaching their goal, and I'll catch up on some of them later on and some I'll just brush over.",
                    "label": 0
                },
                {
                    "sent": "By the way, if you've never heard the term quasi Newton method before, if you were nips you, I'm pretty certain you heard the abbreviation BFG S before or DFP.",
                    "label": 0
                },
                {
                    "sent": "These are all quasi Newton methods, elbil sort of little variants of this family of methods.",
                    "label": 0
                },
                {
                    "sent": "There are even more.",
                    "label": 0
                },
                {
                    "sent": "There's something called Brians method, as I say here.",
                    "label": 0
                },
                {
                    "sent": "There's another one called Powell symmetric broyden and as an SR1 method and they all fall into this framework.",
                    "label": 0
                },
                {
                    "sent": "So how do they work?",
                    "label": 0
                },
                {
                    "sent": "They say I'm going to construct an estimator.",
                    "label": 0
                },
                {
                    "sent": "And I want my estimated to fulfill a an approximate property of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "I'm going to want it to fulfill a difference relation, so I want the difference between the Hessian at the previous.",
                    "label": 0
                },
                {
                    "sent": "Sorry sorry the gradient at the previous location and the gradient where I'm currently at is equal to the estimate of the Hessian times the distance between those two points, so that's a difference relation.",
                    "label": 0
                },
                {
                    "sent": "If the function were actually a quadratic, then this would be an exact statement.",
                    "label": 0
                },
                {
                    "sent": "If the function is quadratic but sufficiently regular, it's approximately true.",
                    "label": 0
                },
                {
                    "sent": "And then usually the literature, because this is sort of long notation.",
                    "label": 0
                },
                {
                    "sent": "These objects are short and this is called Y and this is called S and the rest of the slides will be full of why is an SS, but you don't actually have to fully understand all this algebra now.",
                    "label": 0
                },
                {
                    "sent": "Clearly this is a vector.",
                    "label": 0
                },
                {
                    "sent": "This is a matrix, so there are numbers in here N squared numbers in here.",
                    "label": 0
                },
                {
                    "sent": "That's not enough to estimate N squared numbers, so we need to regularize and the way that the classic derivation of these methods works is we're going to regularize by some Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "So we're going to say we want that new estimator which fulfills this equation, but it's.",
                    "label": 0
                },
                {
                    "sent": "Also, as close as possible under the Frobenius norm to our previous estimator, so we don't want to move too much that's sufficiently regularize.",
                    "label": 0
                },
                {
                    "sent": "One way to write the Frobenius norm maybe somewhat convoluted, but helpful for what's going to follow is as a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So here's the difference between two matrices.",
                    "label": 0
                },
                {
                    "sent": "This notation up here means that I take these matrices and stack them into a long vector.",
                    "label": 0
                },
                {
                    "sent": "I think a lot of you have seen this before, and then I can fight.",
                    "label": 0
                },
                {
                    "sent": "The phobia is known as a quadratic form involving some weight matrix in the middle, which in general.",
                    "label": 0
                },
                {
                    "sent": "I'll call V and this is the Kronecker product.",
                    "label": 0
                },
                {
                    "sent": "If you've never seen this before, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "It's going to vanish in a second if you know what it is then it's obvious.",
                    "label": 0
                },
                {
                    "sent": "And then it can be shown that the unique estimator that minimizes this regularizer subject to this restriction is this update here.",
                    "label": 0
                },
                {
                    "sent": "So the new Hessian is the old fashion plus some outer product between vector and another vector.",
                    "label": 0
                },
                {
                    "sent": "And there are some scalar down here.",
                    "label": 0
                },
                {
                    "sent": "Now this workshop is about probabilistic numerics.",
                    "label": 0
                },
                {
                    "sent": "So what is one way of interpreting this for probabilistic perspective is actually relatively straightforward.",
                    "label": 0
                },
                {
                    "sent": "This is sort of this classic insight that a regularised loss can usually be interpreted as a log likelihood plus some log prior, and then what we're looking for here is the map, sort of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar some posterior in.",
                    "label": 0
                },
                {
                    "sent": "In this case, what does that mean and precious?",
                    "label": 0
                },
                {
                    "sent": "Just pointed out that this does not necessarily have to be true, but it's sort of one way of.",
                    "label": 0
                },
                {
                    "sent": "Sort of in a forward way, generating these models.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying if we make the following assumptions then we end up with the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying this is the only way to get to these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this sort of strict requirement advise to be equal to BS.",
                    "label": 0
                },
                {
                    "sent": "This this this is going to be our likelihood.",
                    "label": 0
                },
                {
                    "sent": "This likelihood is a sort of Dirac Delta point mass which says Y is equal to BS, and that's the limit of a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And a quadratic form in a regularizer is of course the logarithm of ocean.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is a Gaussian prior times the Gaussian likelihoods, which of course returns a Gaussian posterior and that new Gaussian posterior has a mean which is equal to its mode, and that is exactly.",
                    "label": 0
                },
                {
                    "sent": "Our updates are new best guess on your estimate.",
                    "label": 0
                },
                {
                    "sent": "It also has a posterior uncertainty covariance which does not feature in these classic algorithms because they are not necessary to just find the maximum.",
                    "label": 0
                },
                {
                    "sent": "I'll just sort of keep them around, keep it around anyway.",
                    "label": 0
                },
                {
                    "sent": "If you look at this expression is sort of.",
                    "label": 0
                },
                {
                    "sent": "This looks complicated, but you might notice that it's sort of asymmetric.",
                    "label": 0
                },
                {
                    "sent": "There's something happening on the right hand side of the Chronicle product, nothing happening on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "That's maybe a bit weird.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of my first.",
                    "label": 0
                },
                {
                    "sent": "There will be several of these slides.",
                    "label": 0
                },
                {
                    "sent": "Benefit number one of.",
                    "label": 0
                },
                {
                    "sent": "Treating an American algorithm for probabilistic perspective using a probabilistic interpretation can help uncover some kind of implicit assumptions and give you a better feeling for what is going on in these algorithms.",
                    "label": 1
                },
                {
                    "sent": "So at least this particular cross Newton method can be interpreted as following from a Gaussian prior over the passion of protective function relative to the previous estimate.",
                    "label": 1
                },
                {
                    "sent": "And the Gaussian likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now if you have.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Been sort of looking at this update closely, then something you may have noticed or may not is that this matrix here is not symmetric.",
                    "label": 0
                },
                {
                    "sent": "And that's annoying because sessions are symmetric matrices, so we really want our estimate to be symmetric, so this is not symmetric, because this is a vector and this is a vector and the two are not the same.",
                    "label": 0
                },
                {
                    "sent": "So if I transpose this is not just not the same matrix anymore, even if this is a symmetric matrix now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're not the first people to notice this.",
                    "label": 0
                },
                {
                    "sent": "Numerical maths is of course full of very smart people and they know.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just this.",
                    "label": 0
                },
                {
                    "sent": "At least in the 50s, probably even earlier, as I learned today.",
                    "label": 0
                },
                {
                    "sent": "Ann, but let's just forget that there is no metrics for a second.",
                    "label": 0
                },
                {
                    "sent": "Let's think about what we would do if we were faced with this problem.",
                    "label": 0
                },
                {
                    "sent": "So as a proper probabilist, I would say, well of course I have to encode this knowledge.",
                    "label": 0
                },
                {
                    "sent": "I should put it in my prior.",
                    "label": 0
                },
                {
                    "sent": "How do I do this?",
                    "label": 0
                },
                {
                    "sent": "I encode this with a another likelihood term or another factor in my graphical model.",
                    "label": 0
                },
                {
                    "sent": "Depending on how you want to phrase this using another linear operator called operator Delta, that's the operator that takes in a matrix stepped into a vector and returns the difference between the matrix and its transpose.",
                    "label": 0
                },
                {
                    "sent": "So that's the anti symmetry operator that exists in physics as well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And this is of course a linear operator, because it just consists of rearranging the elements of a matrix and subtracting them from each other.",
                    "label": 0
                },
                {
                    "sent": "So that's a linear operation, so we could have another Dirac Delta term which says the matrix is equal to its transpose.",
                    "label": 0
                },
                {
                    "sent": "So Delta B is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Now what you can do is you can look up in some of these smart algebra books from the 50s what the rank of this operator is, because that's a well known operator and it turns out it's rank it's over North.",
                    "label": 0
                },
                {
                    "sent": "Squared is the same rank as that as the number of elements in a lower triangular matrix.",
                    "label": 0
                },
                {
                    "sent": "Or a lower triangular matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's unfortunate, because so let's let's imagine even if we knew what the inverse of that of this operator is, which we're going to need for our inference.",
                    "label": 0
                },
                {
                    "sent": "I don't actually know that, but maybe there's a way of writing it down.",
                    "label": 0
                },
                {
                    "sent": "Then to do this operation, this projection we would have to take all the elements of the matrix of our estimator and sort of jumble them up and project them in the right way using this inverse of this operator.",
                    "label": 0
                },
                {
                    "sent": "And that's going to take at least quarterly many operations.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be too expensive to use in a proper and actual numerical algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we're sort of stuck with our smart probabilistic logic here, and what I did then.",
                    "label": 0
                },
                {
                    "sent": "Or what we did then.",
                    "label": 0
                },
                {
                    "sent": "Martin and I have both paper together earlier this year is we looked at the classic methods again and said, well, they seem to have solved this.",
                    "label": 0
                },
                {
                    "sent": "How did they do this?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I'm going to spare you all the gory details.",
                    "label": 0
                },
                {
                    "sent": "The result is they essentially introduce a second observation.",
                    "label": 0
                },
                {
                    "sent": "You can interpret what's going on as a second independent.",
                    "label": 0
                },
                {
                    "sent": "Term a second factor that says not only have I observed Y is equal to BS, so this difference equation this way round.",
                    "label": 0
                },
                {
                    "sent": "I've also observed that Y transpose is equal to X transpose times B.",
                    "label": 0
                },
                {
                    "sent": "So that's not the same as saying that the matrix is symmetric, because it's only saying it acts like a symmetric matrix in the space spanned by S, so it's a much weaker statement.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, if you multiply in this factor into your current posterior after the first step, then you'll get out a new posterior mean, which is more complicated, but it turns out this is a rank to update, so it's still low rank, and it's symmetric.",
                    "label": 0
                },
                {
                    "sent": "So if you stare at this for awhile, you'll notice that it's a symmetric object.",
                    "label": 0
                },
                {
                    "sent": "And lo and behold, the posterior covariance is also now a very sort of symmetric, neat little little object.",
                    "label": 0
                },
                {
                    "sent": "Not saying that this means much, it's just sort of pleasing to the eye.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So now the question is.",
                    "label": 0
                },
                {
                    "sent": "Again, from our probabilistic perspective, are we allowed to do this because it sounds really weird, right?",
                    "label": 0
                },
                {
                    "sent": "We make one observation and then we treat it like 2 independent observations and stick it into the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "That sounds like double counting.",
                    "label": 0
                },
                {
                    "sent": "So really, we probably shouldn't be doing that.",
                    "label": 0
                },
                {
                    "sent": "To understand.",
                    "label": 0
                },
                {
                    "sent": "That you are actually allowed to do that.",
                    "label": 0
                },
                {
                    "sent": "You have to look again at the prior that these algorithms have so this.",
                    "label": 0
                },
                {
                    "sent": "Chronicle product in here in our in our prior covariance and again.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leaving out a few details, it turns out that what this amounts to is the assumption that the that the function we're trying to optimize.",
                    "label": 0
                },
                {
                    "sent": "Actually depends on twice the number of input parameters that it really has, it treats.",
                    "label": 0
                },
                {
                    "sent": "Vectors and covectors as two separate things.",
                    "label": 1
                },
                {
                    "sent": "It says there's a thing called X, and there's a thing called X transpose, and I can take the derivative of a function with respect to X transpose and with respect to X, and then I can take the derivative respect to X transpose or with respect to X.",
                    "label": 0
                },
                {
                    "sent": "And outcome two different separate objects.",
                    "label": 0
                },
                {
                    "sent": "One is this sort of the left right hash, and one is the right left session, and we know that they are the same.",
                    "label": 0
                },
                {
                    "sent": "But this algorithm doesn't know it.",
                    "label": 1
                },
                {
                    "sent": "It treats them as separate independent objects, and it just observes that they happen to be equal.",
                    "label": 0
                },
                {
                    "sent": "And that ends up with a symmetric mean.",
                    "label": 0
                },
                {
                    "sent": "So the posterior actually does not put.",
                    "label": 0
                },
                {
                    "sent": "Mass only on symmetric matrices.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you sample from this posterior with probability one, you'll get a non symmetric matrix.",
                    "label": 0
                },
                {
                    "sent": "But the mean happens to be symmetric.",
                    "label": 0
                },
                {
                    "sent": "So one way of thinking about this is as some kind of relaxation.",
                    "label": 0
                },
                {
                    "sent": "So there's this is our input domain now, so there's all the vectors X.",
                    "label": 0
                },
                {
                    "sent": "Or you can think of.",
                    "label": 0
                },
                {
                    "sent": "Think of this as a N dimensional vector space and there all the X transpose and that real function depends entirely on this linear subspace here, and we're only ever going to evaluate it at this point.",
                    "label": 0
                },
                {
                    "sent": "But we are introducing observations, which essentially means that we first observe the integral of the function from here to here from here to here, then from here to here.",
                    "label": 0
                },
                {
                    "sent": "And we don't know anything about how the function behaves along here.",
                    "label": 0
                },
                {
                    "sent": "So if this weren't the parametric model but have full Gaussian process, I could sample from it, and if it would move freely along here, but have a perfectly defined integral from here to here.",
                    "label": 0
                },
                {
                    "sent": "And then I make a second observation, which is exactly opposite opposite way around and the two together give me more information than just one of them would, even on this diagonal subspace.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a takeaway number 2.",
                    "label": 0
                },
                {
                    "sent": "Actually this works the other way around as well.",
                    "label": 0
                },
                {
                    "sent": "So looking at America algorithms may give us hints for cool little tricks that can help us with machine learning algorithms, so this is a way of learning as well.",
                    "label": 0
                },
                {
                    "sent": "Finding an estimator for symmetric matrix of very low computational cost.",
                    "label": 0
                },
                {
                    "sent": "I'll point out data that's actually of linear costs.",
                    "label": 0
                },
                {
                    "sent": "It's possible to do this in linear cost.",
                    "label": 0
                },
                {
                    "sent": "Now we have many areas in NIPS.",
                    "label": 0
                },
                {
                    "sent": "You have seen several posters this year where people are learning some kind of metric or some kind of matrix that happens to be symmetric.",
                    "label": 0
                },
                {
                    "sent": "So here's a trick that can do this.",
                    "label": 0
                },
                {
                    "sent": "She, by the way, if you want to hear more about the details of this is a nice email paper.",
                    "label": 0
                },
                {
                    "sent": "This just been accepted, John.",
                    "label": 0
                },
                {
                    "sent": "If you go back.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one so.",
                    "label": 0
                },
                {
                    "sent": "What you hear sometimes?",
                    "label": 0
                },
                {
                    "sent": "And I'm trying to help understand the relationship here, which you hear sometimes is that people say if you're working with structured matrices, symmetric, or any symmetric matrices, the provenience norm, which is just implied Euclidean metric, is the wrong.",
                    "label": 0
                },
                {
                    "sent": "Normally using because essentially you're double counting all of your off diagonal elements, right?",
                    "label": 0
                },
                {
                    "sent": "So we want him to be symmetric, and so I'm going to count all the diagonal elements once and all the off diagonal elements twice.",
                    "label": 0
                },
                {
                    "sent": "So is that the same thing that's going on?",
                    "label": 0
                },
                {
                    "sent": "So this is weaker.",
                    "label": 0
                },
                {
                    "sent": "The resulting as I said, so the this is what you would like to have is a belief that directly encodes that there are only half N * N + 1 three parameters in this object rather than N ^2.",
                    "label": 0
                },
                {
                    "sent": "And I think what you are going through might be a way of doing that if I understand it right now.",
                    "label": 0
                },
                {
                    "sent": "Sing to go back one step further back to when you said use the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "You could use, you could use a normal, it's more back and then the update is going to be more expensive I think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess similar thought would be instead of a Gaussian prior on the elements of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Then yes, something like we should.",
                    "label": 0
                },
                {
                    "sent": "Distribution would naturally exactly yeah, and then it's going to be very tricky to do the inference in linear algebra, fashion, innovation.",
                    "label": 0
                },
                {
                    "sent": "And yet another thought is you could start with a Gaussian process prior on the function.",
                    "label": 0
                },
                {
                    "sent": "Say I'm going to learn the function and now I can sort of indie.",
                    "label": 0
                },
                {
                    "sent": "So if you do the opposite thing of what Ben's been talking about, I can just differentiate that the Gaussian process twice, and then I know what the Haitian is everywhere.",
                    "label": 0
                },
                {
                    "sent": "Well, I can estimate.",
                    "label": 0
                },
                {
                    "sent": "Have a belief.",
                    "label": 0
                },
                {
                    "sent": "Ovation is everywhere that's going to be a symmetric matrix, But again, doing that is going to be at least cubically expensive to invert it.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have this benefit now is sort of reverse the arrow again.",
                    "label": 0
                },
                {
                    "sent": "And let's see whether we can do something for numerical algorithm with with our probabilistic knowledge.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we sort of the obvious thing for.",
                    "label": 0
                },
                {
                    "sent": "A Bayesian machine learner.",
                    "label": 0
                },
                {
                    "sent": "Sort of a Gaussian process everywhere, so we're moving from a parametric model for the Hessian, which says the Hessian is sort of locally constant and sort of Gaussian constant relative to the previous session to a nonparametric model which says the Hessian is actually in a function which returns an N square matrix at every point in the input domain.",
                    "label": 0
                },
                {
                    "sent": "So there are two an input dimensions at every point in that in that space we can return a matrix and we do that with.",
                    "label": 0
                },
                {
                    "sent": "Two kernels and these two kernels have played the same role that there's two movies played in the previous slide, so they also encode that this fact that we're treating XNXX trans separate things to keep our computational costs low.",
                    "label": 0
                },
                {
                    "sent": "And now once we do this, we can do something like interesting, which is that we can make a more precise statement about about the likelihood.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can make the exact statement.",
                    "label": 0
                },
                {
                    "sent": "So before our likelihood was this finite difference equation, which says the difference in the gradients is equal to the Haitian times the difference in the steps.",
                    "label": 0
                },
                {
                    "sent": "And now, because we have no parametric formulation, we can actually encode the exact statement, which is that the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Is sorry, the gradients are the integral over the Hessian, so the difference between two gradients is the line integral over the Hessian along the line connecting those two, and this is what these complicated long equations you are saying.",
                    "label": 0
                },
                {
                    "sent": "We can do that and then what does leads us through what that forces us to do?",
                    "label": 0
                },
                {
                    "sent": "An inference is to integrate our kernels.",
                    "label": 0
                },
                {
                    "sent": "You have to choose a kernel such that we can do this integral in some sufficiently fast way.",
                    "label": 0
                },
                {
                    "sent": "And in our case we are currently using the very popular square exponential, Gaussian radial basis function kernel for that.",
                    "label": 0
                },
                {
                    "sent": "These integrals, the resulting integrals you have to do an inference boiled down through bivariate Gaussian integrals.",
                    "label": 0
                },
                {
                    "sent": "Those are not analytic, but they can be done very efficiently numerically with numerical integration.",
                    "label": 0
                },
                {
                    "sent": "We've got very fast code for that by now.",
                    "label": 0
                },
                {
                    "sent": "The maybe surprising thing is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Usually when you hear the word Gaussian process, everyone thinks so that's going to be expensive because Gaussian processes are expensive.",
                    "label": 0
                },
                {
                    "sent": "They're keeping the number of data points.",
                    "label": 0
                },
                {
                    "sent": "And that's true.",
                    "label": 0
                },
                {
                    "sent": "So at least naive Gaussian process inference involves inverting a matrix and matrix of the size, number of function evaluations, times, number of function, elevations, not of size, number of input domains, input dimensions of interventions.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is still linearly expensive in the number of input dimensions, and we so it scales to very large scale problems if now actually running on computer vision problems with more than a million parameters, and so for about a million parameters currently are still not so nice.",
                    "label": 0
                },
                {
                    "sent": "Code takes about half a second.",
                    "label": 0
                },
                {
                    "sent": "Update so this is something that just works.",
                    "label": 0
                },
                {
                    "sent": "Of course we have this issue that yes we cannot store every data point because that's going to make us eventually make a slow, but here sort of a feature of optimization comes to our rescue, which is that an optimizer is not a regression algorithm that has a bounded domain and then keeps evaluating at random points and then somehow builds a model and optimizing walks along a trajectory.",
                    "label": 0
                },
                {
                    "sent": "So if you made 50 observations, chances are the observation you made 50 steps ago are not so helpful anymore.",
                    "label": 0
                },
                {
                    "sent": "That's the bfsi dear.",
                    "label": 0
                },
                {
                    "sent": "Again, an old idea.",
                    "label": 0
                },
                {
                    "sent": "We are using that to make our algorithm fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes you go back.",
                    "label": 0
                },
                {
                    "sent": "Intuition about what this T parameter that you're integrating with, respectively.",
                    "label": 0
                },
                {
                    "sent": "So geometrically what.",
                    "label": 0
                },
                {
                    "sent": "So this is, so I'm parameterising the.",
                    "label": 0
                },
                {
                    "sent": "So we've taken, evaluated, here, and here.",
                    "label": 0
                },
                {
                    "sent": "There's a line in between.",
                    "label": 0
                },
                {
                    "sent": "This is the vector S and we are amortizing this by T from T equal to 0 equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's some.",
                    "label": 0
                },
                {
                    "sent": "Just some things to point out that this kind of treatment gives.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right away, the first one already pointed out.",
                    "label": 0
                },
                {
                    "sent": "We now have an exact treatment of gradient observations.",
                    "label": 1
                },
                {
                    "sent": "We can actually say what we mean by when we observe a gradient.",
                    "label": 0
                },
                {
                    "sent": "It's the integral over the issue.",
                    "label": 0
                },
                {
                    "sent": "We can do that so that gives us an actual Gaussian process posterior over the elements of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "So here's the everyone.",
                    "label": 0
                },
                {
                    "sent": "Well, not everyone's, but sort of a very, very famous optimization problem is actually non convex, but it's sort of reasonably well behaved.",
                    "label": 0
                },
                {
                    "sent": "This is rosenbrook polynomial, also known as the banana function.",
                    "label": 0
                },
                {
                    "sent": "You start out somewhere up here optimize along this Valley and this is where the minimum is.",
                    "label": 0
                },
                {
                    "sent": "Once our algorithms run along this trajectory, we can ask it what do you believe that the Hessian is actually and so the escalation of this two by two.",
                    "label": 0
                },
                {
                    "sent": "Or dimension dimension 2 input function is 2 by two, so there are three elements to the Hessian, but only one of them is actually really interesting.",
                    "label": 0
                },
                {
                    "sent": "All the other ones have a very, very boring structure.",
                    "label": 0
                },
                {
                    "sent": "This is the truth.",
                    "label": 0
                },
                {
                    "sent": "This is what the Haitian actually looks like.",
                    "label": 0
                },
                {
                    "sent": "This is this is the one one element, so the second derivative of the of this of this dimension.",
                    "label": 0
                },
                {
                    "sent": "Here is the posterior mean estimate of our algorithm after it has run.",
                    "label": 0
                },
                {
                    "sent": "So in the regions under two plots of the same color scale.",
                    "label": 0
                },
                {
                    "sent": "So in the regions where we've been, we make a very good estimate which is reasonably correct when I don't plot here, because this sort of don't didn't quite get the PDF to work, is uncertainty around this.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's also true that there is some kind of sausage of uncertainty around here, which means up here and down here.",
                    "label": 0
                },
                {
                    "sent": "We don't actually know what the issue is, and we are aware of the fact that we're not.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the issue is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some other neat thing we can do is because we have another metric formulation.",
                    "label": 0
                },
                {
                    "sent": "This is maybe a somewhat complicated plot.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can now use every observation made anywhere.",
                    "label": 0
                },
                {
                    "sent": "So for example, any observation made.",
                    "label": 0
                },
                {
                    "sent": "At some other locations, some other gradient observation can be directly included in the model, so that means we can parallelize this algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can also use all the observations made during one line search, so a traditional algorithm makes a line search that involves maybe 1, two, three or four function evaluations, and then it has to throw away all the evaluations made during the line search because there's no algebraic room in the framework to encode it in the update.",
                    "label": 0
                },
                {
                    "sent": "So that's the picture here.",
                    "label": 0
                },
                {
                    "sent": "Let's say we start with this gradient.",
                    "label": 0
                },
                {
                    "sent": "Then we observe this gradient equation.",
                    "label": 0
                },
                {
                    "sent": "We overstepped of moves that move back, said.",
                    "label": 0
                },
                {
                    "sent": "Now we're done with our line search.",
                    "label": 0
                },
                {
                    "sent": "You have to throw out everything we made.",
                    "label": 0
                },
                {
                    "sent": "We observe in between and that amounts to observing just the integral from here to here.",
                    "label": 0
                },
                {
                    "sent": "If you keep all those steps around, we can tell our Gaussian process regressor and that works because it's nonparametric, so there are sufficiently many eigenvalues available to encode everything that we've made all these observations and that gives us a much more detailed picture of what the function looks like.",
                    "label": 0
                },
                {
                    "sent": "That's a helpful thing to have, because keep in mind that each of these Gray lines represents maybe a million numbers, because it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a gradient of the objective function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some other trick.",
                    "label": 0
                },
                {
                    "sent": "So now I'm just sort of starting to ramble on all the things that immediately come to your mind.",
                    "label": 0
                },
                {
                    "sent": "We know how to do, how to treat noise in regression.",
                    "label": 0
                },
                {
                    "sent": "Sort of standard thing, so I'm very happy to report that we just got a paper into HTML weirdly doing NIPS.",
                    "label": 0
                },
                {
                    "sent": "So really weird feeling.",
                    "label": 0
                },
                {
                    "sent": "On doing regression with noise, so sort of the the.",
                    "label": 0
                },
                {
                    "sent": "DD appetizing slogan might might be stochastic Newton descendants that also has the gradient descent, even though that's a little bit little bit over doing it.",
                    "label": 0
                },
                {
                    "sent": "Maybe so here are a bunch of algorithms trying to minimize a very simple 2 dimensional function without noise.",
                    "label": 0
                },
                {
                    "sent": "The blue one is our numerical algorithm, the red one is gradient descent, which is known to be only linearly efficient and lines up here.",
                    "label": 0
                },
                {
                    "sent": "The pink and the green line with our various ways of exact Newtons method.",
                    "label": 0
                },
                {
                    "sent": "So this is gradient, Hessian free and actually just numerical invert inversion of a automatically calculated.",
                    "label": 0
                },
                {
                    "sent": "And obviously this Newton is just the best algorithm of those because it has more information available.",
                    "label": 0
                },
                {
                    "sent": "It's just inverted matrix.",
                    "label": 0
                },
                {
                    "sent": "Now once you add noise in the just on the elements of the of the gradient, all these classic algorithms just sort of become much less convergence.",
                    "label": 0
                },
                {
                    "sent": "Not very surprisingly, this is well known, but if you sort of exact actually model the fact that there is noise on your observations, then you can still get a meaningful estimate for the Hessian and still converge.",
                    "label": 0
                },
                {
                    "sent": "And by the way, this so this is the range of basically all double precision floating point numbers, so there's.",
                    "label": 0
                },
                {
                    "sent": "Really a lot of difference in between those two lines.",
                    "label": 0
                },
                {
                    "sent": "So there's no no batch, here is just actual evaluations of a function.",
                    "label": 0
                },
                {
                    "sent": "I'm not training manual, network or anything, it's just just a function with noise.",
                    "label": 0
                },
                {
                    "sent": "That function with noise could be from some small batch of the larger batch, yes, so in the paper there is an experiment with a neural network where we basically just we just copy paste it.",
                    "label": 0
                },
                {
                    "sent": "Jeff, Jeff Stephens code and replaced our algorithm instead of stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And then you have the situation that if you have if you have one mega Patch and you have no noise essentially.",
                    "label": 0
                },
                {
                    "sent": "And as you make mini batches smaller and smaller, you have more and more noise on your observations relative to some latent function you can't observe, and this is essentially what this is doing.",
                    "label": 0
                },
                {
                    "sent": "By your grade central bank.",
                    "label": 0
                },
                {
                    "sent": "PDF version.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a toy problem.",
                    "label": 0
                },
                {
                    "sent": "It's just sort of pointing out that you can learn with noise.",
                    "label": 0
                },
                {
                    "sent": "And various other ideas that I'm not going to talk about, mainly because they don't actually have results on them yet.",
                    "label": 0
                },
                {
                    "sent": "There's just sort of, but I guess everyone can have their favorite idea of what you can do with regression in optimization, because we know so much about regression.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, you could imagine that your function is dynamically changing, so if you're a roboticist and you're expecting your sort of every 40 milliseconds, you're solving optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can't run your beef Stew convergence, but you can do it for one step or two steps, and then say something has changed a little bit, but not very much.",
                    "label": 0
                },
                {
                    "sent": "And I just keep on running by using a comma.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Filter in that framework.",
                    "label": 0
                },
                {
                    "sent": "OK, so benefit #3 is.",
                    "label": 1
                },
                {
                    "sent": "Probably interpretation American method may lead to sort of connections to other areas of inference and machine learning and statistics, and that can help us to gain all sorts of generalizations, improvements, other applications I don't know.",
                    "label": 1
                },
                {
                    "sent": "Now I don't want to just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The ultra positive I also want to point out a few issues that are challenges rather than benefits so.",
                    "label": 0
                },
                {
                    "sent": "If you read a numerical analysis paper on optimization algorithm, then of course these papers are very precise about convergence is and they can't contain usually convergence algorithm convergence proofs and one very basic property of an optimization algorithm that people would like to see is called locally linear convergence or local linear convergence.",
                    "label": 1
                },
                {
                    "sent": "This is saying so this is sort of somewhat simplified statement, but this is saying that if we start out at an initial point which is epsilon close to the true minimum.",
                    "label": 0
                },
                {
                    "sent": "And we start out with an estimate for the Hessian that is Delta close to the true Hessian.",
                    "label": 1
                },
                {
                    "sent": "Then our algorithm will converge and it will converge linearly fast.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, yeah yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately, it turns out this nonparametric algorithm I just presented so this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Does not, at least not easily have that property.",
                    "label": 0
                },
                {
                    "sent": "So in the sense that at the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I don't know how to show it, so at least the classic ways of showing it the usual theorems do not apply the user elevated.",
                    "label": 0
                },
                {
                    "sent": "This is shown is to show that from one step to the next, the error on the on the Hessian drops at most linearly fast, or sorry, gets worse, at least at most linearly fast.",
                    "label": 0
                },
                {
                    "sent": "Because that's something you can recover by sort of more than linear converges.",
                    "label": 0
                },
                {
                    "sent": "Now in a non linear regression you can sometimes have terms that are non linearly large.",
                    "label": 0
                },
                {
                    "sent": "Now, in some sense, that's something that hasn't held us back from using nonlinear regression.",
                    "label": 0
                },
                {
                    "sent": "In other areas where the same applies.",
                    "label": 0
                },
                {
                    "sent": "But of course it's a big question whether that causes a huge problem for optimization or not.",
                    "label": 0
                },
                {
                    "sent": "So far I haven't seen a case where our algorithm doesn't converge.",
                    "label": 0
                },
                {
                    "sent": "But as Perseus pointed out, experiments are nice.",
                    "label": 0
                },
                {
                    "sent": "Theorems are nice as well.",
                    "label": 0
                },
                {
                    "sent": "So far we don't have a theorem.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a real challenge.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The two frameworks, the way that we sometimes work in probabilistic inference and the sort of thing that a numerical tool actually requires in terms of reliability and robustness is not always easy to get.",
                    "label": 1
                },
                {
                    "sent": "And sometimes that might be just the problem of re phrasing the convergence and just finding a more parsimonious description of what we're actually looking for, but sometimes it might be a real problem, and then we should sort of accept that.",
                    "label": 0
                },
                {
                    "sent": "So I guess the challenge for someone trying to work in both of these fields is that we have to decide where we want to sort of accept the other communities way of working and adapt to it, and where we want to challenge it, and then try to change it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize.",
                    "label": 0
                },
                {
                    "sent": "I too, of course.",
                    "label": 0
                },
                {
                    "sent": "Of my own work, but also try to sort of make some make some connections too.",
                    "label": 0
                },
                {
                    "sent": "Of wider view.",
                    "label": 0
                },
                {
                    "sent": "And some examples were that on the concrete side there is a connection between Quasi Newton optimization and least squares regression.",
                    "label": 1
                },
                {
                    "sent": "And that's an example of how a probabilistic formulation can help improve understanding of some numerical method.",
                    "label": 1
                },
                {
                    "sent": "I've and then turned out that the way that these algorithms achieve symmetry in the estimates.",
                    "label": 0
                },
                {
                    "sent": "Is a very, very intriguing algebraic assumption in the prior.",
                    "label": 0
                },
                {
                    "sent": "So this is an idea that we can take from.",
                    "label": 1
                },
                {
                    "sent": "Numerics and applying machine learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe, at least in this particular case.",
                    "label": 0
                },
                {
                    "sent": "I then showed some like a long list of extensions that are become immediately obvious.",
                    "label": 0
                },
                {
                    "sent": "Once you have this inside, so I think this is something that happens a lot at once.",
                    "label": 0
                },
                {
                    "sent": "Once you have a probabilistic formulation, you can just use everything you know from other areas and just plug them in, because probability formulations are so nicely modular.",
                    "label": 0
                },
                {
                    "sent": "But there are also some challenges.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you want to use all these great extensions, then sometimes in the process will throw out some assumptions that are usually made for the for the existing numerical algorithm and we have to be careful not to throw out the baby with the bathwater by doing that.",
                    "label": 0
                },
                {
                    "sent": "So by the way, if you want to read more on the details of this so I didn't show you many numerical results and performance results, you can look them all up in our papers and you can download code for some of the algorithms from my website.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "We got time for a couple questions.",
                    "label": 0
                },
                {
                    "sent": "So I'm curious about this cultural challenge.",
                    "label": 0
                },
                {
                    "sent": "I mean, I can perfectly imagine that that is there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have you actually tried to?",
                    "label": 0
                },
                {
                    "sent": "I'm in the process of trying.",
                    "label": 0
                },
                {
                    "sent": "I haven't actually submitted that paper yet.",
                    "label": 0
                },
                {
                    "sent": "I'm saying we can't even prove so I had I had a result like a response like that from one of the ICL reviewers.",
                    "label": 0
                },
                {
                    "sent": "Is that you can't really submit an optimization algorithm for which you don't even know whether we converges or not.",
                    "label": 0
                },
                {
                    "sent": "I guess in this particular case that reviewer was trumped by the other reviewer.",
                    "label": 0
                },
                {
                    "sent": "But I'm expecting more of that as I send these papers to numerical values.",
                    "label": 0
                },
                {
                    "sent": "Well and of course I'm trying to actually address them in the paper as well.",
                    "label": 0
                },
                {
                    "sent": "So ideally you would have a proof of something else that is also convenient.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "I've got a handful of questions that I could ask now or or during the panel, but I'll put one in here.",
                    "label": 0
                },
                {
                    "sent": "Something that that your talk highlighted quite a bit as well as ya sex talk.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this idea that we would love to carry around 2nd order information, but can't because it's practically too expensive so you've gone ahead and reinterpreted quasi Newton methods in a probabilistic framework?",
                    "label": 0
                },
                {
                    "sent": "And then I was talking to ask about this before you see a path to doing the same for.",
                    "label": 0
                },
                {
                    "sent": "Exact search methods in interior point.",
                    "label": 0
                },
                {
                    "sent": "I think I'm not qualified enough to give a sort of a meaningful answer to that, because I just don't know too little about interior point methods.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think you've asked Jessica as well, and maybe we're both sort of not not close enough to the to the divide to give it to give a meaningful answer to that.",
                    "label": 0
                },
                {
                    "sent": "It would of course be neat, but I have absolutely no idea, but it's not so difficult because in the end.",
                    "label": 0
                },
                {
                    "sent": "The Hessian multiplied by the gradient, and you've chosen two different ways to do that besides that.",
                    "label": 0
                },
                {
                    "sent": "That would be really cool if there was some connection here.",
                    "label": 0
                },
                {
                    "sent": "It's just that I haven't looked at it at all, so I can't make any qualified statements.",
                    "label": 0
                },
                {
                    "sent": "Timo in practice, if you compare to those images, so in the, of course, because the machine learning paper in the original ICM paper we had that plot where you're better than everyone else.",
                    "label": 0
                },
                {
                    "sent": "Not everyone always has in their paper, and so we have.",
                    "label": 0
                },
                {
                    "sent": "What we are currently working on and this was sort of sidetrack for awhile, is a larger benchmark set on which to make a meaningful statement of this actually works this much better or worse on.",
                    "label": 0
                },
                {
                    "sent": "A large number of problems optimization community has these benchmarks.",
                    "label": 0
                },
                {
                    "sent": "They are maybe biased towards certain types of problems that show up in other non machine learning areas.",
                    "label": 0
                },
                {
                    "sent": "So for example we tend to be very low dimensional.",
                    "label": 0
                },
                {
                    "sent": "We're trying to add some of our own some large scale problems.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So far I haven't seen a case where our algorithm is.",
                    "label": 0
                },
                {
                    "sent": "Sort of noticeably worse than DFCS.",
                    "label": 0
                },
                {
                    "sent": "I've seen several cases where it's a lot better.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen a case yet where it doesn't converge apart from super.",
                    "label": 0
                },
                {
                    "sent": "It'll what's the word condition problems on which just know optimizer converges so that it's easy to write an optimization problem that just just nothing converges on right?",
                    "label": 0
                },
                {
                    "sent": "Because it's some tiny little sliver.",
                    "label": 0
                },
                {
                    "sent": "So we compared to BF, GSL, Bgcse, DFP, various other ones.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So ours is similar in the spiritual beliefs in the sense that we throw out all things.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So one comparison is the LDS.",
                    "label": 0
                },
                {
                    "sent": "We're also comparing to Pfc.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "So there is a destrier difference between objects, and that's well known, and they behave in slightly different ways.",
                    "label": 0
                },
                {
                    "sent": "How they different behavior depends on the size of the problem, right?",
                    "label": 0
                },
                {
                    "sent": "Because essentially with every observation you throw out a rank one.",
                    "label": 0
                },
                {
                    "sent": "Part of your of your prior uncertainty so.",
                    "label": 0
                },
                {
                    "sent": "If your memory limit is larger than the domains that input domain, then that's fine.",
                    "label": 0
                },
                {
                    "sent": "Then you're sort of almost back in.",
                    "label": 0
                },
                {
                    "sent": "What gifts would be doing so so that the results are sort of?",
                    "label": 0
                },
                {
                    "sent": "It's very hand WAVY because because the differences between algorithms are sort of sketchy.",
                    "label": 0
                },
                {
                    "sent": "I might have missed it, but it didn't seem like you were using the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "I don't at all, so this is one of the this is this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is one of the things that are hidden behind this Graybar, yes, so it's obviously something you would like to think about some kind of active learning of how.",
                    "label": 0
                },
                {
                    "sent": "So essentially you can treat these sort of interpreted algorithms as V enforcement learning algorithms that are that are only exploitive.",
                    "label": 0
                },
                {
                    "sent": "They're greedy, they take a data point, make their best guess that can make, and just run it direction.",
                    "label": 0
                },
                {
                    "sent": "And you could imagine so on the one hand, you could imagine that maybe some exploration is a good idea.",
                    "label": 0
                },
                {
                    "sent": "You should sort of decide on some directions to look for at the same time.",
                    "label": 0
                },
                {
                    "sent": "There's also that, so with numerical algorithms, there's this sort of.",
                    "label": 0
                },
                {
                    "sent": "The issue with that, usually the important parts of the Hessian are sort of.",
                    "label": 0
                },
                {
                    "sent": "Pointing out all the eigenvectors of the Hessian are pointing in a similar direction to the gradient.",
                    "label": 0
                },
                {
                    "sent": "Right on some this is very vague, but it's not a precise statement, so maybe you don't actually want to look orthogonal to the gradient, because that might just be the really pretty boring part of the problem anyway, or irrelevant for the optimization problem, so I don't have a good answer on that, but it's an obvious obvious thing to address.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                }
            ]
        }
    }
}