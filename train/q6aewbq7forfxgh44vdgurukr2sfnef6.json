{
    "id": "q6aewbq7forfxgh44vdgurukr2sfnef6",
    "title": "A Second-order Bound with Excess Losses",
    "info": {
        "author": [
            "Pierre Gaillard, French Alternative Energies and Atomic Energy Commission (CEA)"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_gaillard_losses/",
    "segmentation": [
        [
            "So I'm going to present some don't work with the team by Nathan.",
            "We just did the amazing talk just before me and she starts my PhD advisor who can come here at cards."
        ],
        [
            "So as we are in the online learning session, I'm going to do some online learning, so at each one T some learner has to make some.",
            "Prediction by choosing your weight vector PT piraty of non negative weights that sum to one.",
            "Then every expert ski of a finite pool of experts in Q as some finite class that you're higher assumed to be bonded in 01.",
            "And the learner incurs loss, which is a weighted average of the expected losses according to the weight vector he he had.",
            "He has chosen before.",
            "And the good officer learner is to minimize his communitive loss and to do So what you do, man, you saw it in the previous talks.",
            "Minimize his communitive regrets against any experts.",
            "And the cumulative regret is a difference between the community of loss of the learner and the cumulative loss of the of any expert key."
        ],
        [
            "So in the worst case, the best that we can hope to guarantee is regret against any expert of order square root of T lo K, where K is the capital case in number of experts.",
            "And when I write a small square like this, it does not mean that this is that is in season on ASCII character or something that was not working.",
            "It's that there is a constant that I don't remember that I don't want to show.",
            "OK, so."
        ],
        [
            "This is the worst case bounds, but.",
            "In some cases it can be improved.",
            "For example, there is the improvement for small losses.",
            "That bones the regrets against any experts by a square root of the log K times the cumulative loss of the expert.",
            "And that was what team was.",
            "Zero is the rate bond that had seen before.",
            "And the issue is that there is some issue in the scaling if you divide all the losses by two, for example the regret bound here is not divided by two as well.",
            "Of course there is some constants in the square but.",
            "I'm keeping this."
        ],
        [
            "OK, so to deal with that issue, she's a blanky and all in a 2007 suggested to prove second outer bounds to get to improve further.",
            "This type of bonds that they exhibited 2 forms of 2nd order of signal are not bound.",
            "The first one was to bond to bond the regrets against any expert K biolog K over the learning rate plus the learning rate times.",
            "Communitive sudden some of Iran's of the expelled square, the loss of the expert square.",
            "And the issue with that bond is that there exist no methods.",
            "To optimize the learning rates so it's not possible to get."
        ],
        [
            "These aren't optimized bounds."
        ],
        [
            "The second form of 2nd outer bound exhibited by.",
            "She's a blocky Anon was of this form, and it bounds the regret against any expert K by square root of log K times the sum of Iran's of some variance term.",
            "And this Vt is actually some dispersion of the losses of the experts.",
            "According to the, the weight distribution chosen by their owner.",
            "And the issue with that balance is that it does not reflect the fact that it might be easier to to compete with some experts than with other experts.",
            "So what?"
        ],
        [
            "Our contribution in our paper is to prove a new form of signal out of bounds, and this is in terms of excess losses.",
            "So this.",
            "The second outer bound is the following.",
            "It bounds or regrets against any expertcare by square root of log K times the sum of all runs.",
            "Of the excess loss of the learner against the expelled K square."
        ],
        [
            "So this is our bounds and the goal of the stork will be to explain you what why this is interesting.",
            "Because of course if you have a new form of bonds, it's not.",
            "It's not enough.",
            "So in interest I will show you some nice feature of these bounds.",
            "So like from like in the setting of experts, reporting confidence is like we will show that it enters an improvement or small excess losses will ensure that there is a.",
            "It entails constant rates in the in the special case of IID losses and the last points rise prove very recently by only giving 10 version.",
            "We proved that.",
            "That this kind of loss entails a probabilistic bounds on the communitive risk.",
            "In some stochastic scenario, with very few assumption.",
            "And we will show that the key element in our analysis is to consider not one learning rates for their algorithm, but when learning wiper experts will consider an algorithm with multiple learning rates."
        ],
        [
            "So to get our regret bounds, the.",
            "The algorithm that we considers is largely inspired by the pod forecast of Chester Brown Keenan that they introduced to prove second abounds.",
            "And so our first record, what was this algorithm?",
            "So this algorithm depends on the learning rates.",
            "ITA, which is positive, and at each runs T it's a.",
            "It assigns to the expert K wait PPP Katie, which is normalization of some sort of weights so that the weights vector sum to one and then it updates the weight vector of the weight of the experts as as it as it is shown here.",
            "OK, it's not working.",
            "Maybe this one as it is shown a it's a take the previous set of weights.",
            "And multiply it by 1 plus.",
            "The learning rates times the excess loss of experts of the learner against expertcare.",
            "We can show that this algorithm if we don't optimize the learning rates, get this kind of regret bounds.",
            "So it's almost our second order bounds.",
            "The issue is that we cannot optimize alert.",
            "The learning rates for all experts simultaneously.",
            "We don't know in advance which one will be the best expert, so.",
            "We have a kind of impossible tuning issue."
        ],
        [
            "The solutions that we propose ways to consider not almost the same algorithm, but with multiple learning rates, so we have no K learning rates.",
            "Each expert has its own learning rates and we have almost the same algorithm accepts that no, we have an learning rate that depends on the expert.",
            "And we can show that we retrieve exactly the same bounds, except that now we have a learning rates depending on the experts and so now we don't have to know in advance which one will be the best expert we can optimize each learning rates independently."
        ],
        [
            "So if we optimize the bonds with the running rates, we get our second out of bounds.",
            "In excess loss and we can, we can show that this can be calibrated online at a small multiplicative cost of a log log T, so it's almost constant."
        ],
        [
            "OK, no I would try to explain you why this is interesting.",
            "So first I tell you what touch controls and improvement for small excess losses.",
            "And what's nice is that all the features that I will show you are not captured by the algorithm, but by the bounds only.",
            "So the first feature is that any strategy that satisfies this kind of regret bounds.",
            "Also satisfies this.",
            "From regret bounds and disregard bond is almost the improvement for small losses.",
            "Here we retrieve almost the cumulative loss of Expertcare, except two things.",
            "The first is that we don't consider or runs, but only the runs where the expert K incurred a loss bigger than the learner and the second one is that we.",
            "Remove the loss of the learner to the loss of the gift cards, so this is always OK up to the constants always at least as good as the.",
            "Improvement for small losses, except that now we have some additional nice features.",
            "For example, this bond is invariant by translation.",
            "If we remove the constraints to the loss of our experts, then the the bounds remains unchanged, which was not the case with these bonds.",
            "As."
        ],
        [
            "A second feature of our Bones is in the setting of experts that reports their confidences.",
            "This setting was introduced by Bluman Monsour in 2007 and it is almost the same setting at the previous one, except that no at the beginning of each round, each expert K expresses his confidence in his prediction by giving a number in 01 ikt.",
            "And then the rest exactly the same as the previous setting, each learner choose a weight vector.",
            "Each expert incurs some loss, and the learner accused the loss, which is weighted average of the expert's losses.",
            "No, the major difference with the previous setting is in the definition of their regrets.",
            "Which is not the difference between the cumulative loss of the learner and the cumulative loss of the expert, but the runs rated by the confidence of the expert.",
            "To understand this, you can show the special case where the confidence are zero or one which expresses the fact that expert can be active or inactive."
        ],
        [
            "In this setting, the best bonds available bonds before for confidence.",
            "Free weights was in the paper of Bloom and more so in 2007 and was the following, so it was it bounced.",
            "The regret, the confidence we get against any expert.",
            "K bye you square root of log K times.",
            "Times this.",
            "And this is almost the improvement for small losses, except that we have now the confidence of the experts just before their losses.",
            "The issue with that bond is that he does not care linearly with the confidence.",
            "For instance, here the confidence regrets if we divide or the confidence is by two.",
            "The red should be divided by two, which is not the case in this bound.",
            "So this is."
        ],
        [
            "Is the nice feature of our bounds that we can show that any strategy?",
            "That's the only the algorithm that I showed to you, but any strategies that satisfies this regret bounds.",
            "If we apply it to some modify losses which are as follow but.",
            "Don't try and try to understand it.",
            "It's not very complicated, but.",
            "It's fine, so any strategy that I applied to this modified losses will.",
            "Will get a confidence regulates with this regret bounds.",
            "And we show that we have almost are second order bound exactly expects that we have the confidence is of the expert squared just before the excess losses.",
            "And if we bounce this by one, we get this kind of regret bounds.",
            "And now we don't have the scaling issue in the confidences, and as the confidences are between zero and one, this is always better than the previous bounds."
        ],
        [
            "OK, III's nice feature of these bounds is in the same setting presented by Tim before.",
            "In the case of IID losses, we assume that know the losses are not in the worst case they are not generated by anniversary, but by some idea setting and that some expert is better than all the others in expectation is loss is better than or other experts by some constant.",
            "Alpha so if we are in this setting we can show that any strategy that satisfies our right bounds.",
            "Has a constant regulating expectation against the best expert.",
            "And we have the similar results in probability with high probability.",
            "They regret against the best expert K star will be constant as well.",
            "So it is very nice is that.",
            "In the previous analysis, this kind of features was only in the algorithm, it was.",
            "Not in the record bonds.",
            "Now we have one regret bounds that captures either.",
            "The worst case is the easy data with constant regret."
        ],
        [
            "OK, last last consequences of our record bounds is.",
            "Is, uh was proved very recently by only giving it energy in the very recent paper, which is an archive.",
            "I won't present.",
            "This wasn't very impressive very precisely, but if you want you can check his paper.",
            "And what he does in his paper, he extends our analysis to exponential updates instead of product updates that we used.",
            "And he proved that.",
            "Any algorithm, any strategy that satisfies our second order bounding excess loss gets also nice bounds.",
            "I think optimal bounds in on cumulative risk in a very in question are stochastic stochastic setting."
        ],
        [
            "So to conclude, we had we proved this new form of 2nd order bounds with lots of with some very nice features and.",
            "I think that's all, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to present some don't work with the team by Nathan.",
                    "label": 0
                },
                {
                    "sent": "We just did the amazing talk just before me and she starts my PhD advisor who can come here at cards.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we are in the online learning session, I'm going to do some online learning, so at each one T some learner has to make some.",
                    "label": 0
                },
                {
                    "sent": "Prediction by choosing your weight vector PT piraty of non negative weights that sum to one.",
                    "label": 1
                },
                {
                    "sent": "Then every expert ski of a finite pool of experts in Q as some finite class that you're higher assumed to be bonded in 01.",
                    "label": 1
                },
                {
                    "sent": "And the learner incurs loss, which is a weighted average of the expected losses according to the weight vector he he had.",
                    "label": 1
                },
                {
                    "sent": "He has chosen before.",
                    "label": 0
                },
                {
                    "sent": "And the good officer learner is to minimize his communitive loss and to do So what you do, man, you saw it in the previous talks.",
                    "label": 0
                },
                {
                    "sent": "Minimize his communitive regrets against any experts.",
                    "label": 1
                },
                {
                    "sent": "And the cumulative regret is a difference between the community of loss of the learner and the cumulative loss of the of any expert key.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the worst case, the best that we can hope to guarantee is regret against any expert of order square root of T lo K, where K is the capital case in number of experts.",
                    "label": 0
                },
                {
                    "sent": "And when I write a small square like this, it does not mean that this is that is in season on ASCII character or something that was not working.",
                    "label": 0
                },
                {
                    "sent": "It's that there is a constant that I don't remember that I don't want to show.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the worst case bounds, but.",
                    "label": 0
                },
                {
                    "sent": "In some cases it can be improved.",
                    "label": 0
                },
                {
                    "sent": "For example, there is the improvement for small losses.",
                    "label": 1
                },
                {
                    "sent": "That bones the regrets against any experts by a square root of the log K times the cumulative loss of the expert.",
                    "label": 0
                },
                {
                    "sent": "And that was what team was.",
                    "label": 0
                },
                {
                    "sent": "Zero is the rate bond that had seen before.",
                    "label": 1
                },
                {
                    "sent": "And the issue is that there is some issue in the scaling if you divide all the losses by two, for example the regret bound here is not divided by two as well.",
                    "label": 0
                },
                {
                    "sent": "Of course there is some constants in the square but.",
                    "label": 0
                },
                {
                    "sent": "I'm keeping this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to deal with that issue, she's a blanky and all in a 2007 suggested to prove second outer bounds to get to improve further.",
                    "label": 0
                },
                {
                    "sent": "This type of bonds that they exhibited 2 forms of 2nd order of signal are not bound.",
                    "label": 0
                },
                {
                    "sent": "The first one was to bond to bond the regrets against any expert K biolog K over the learning rate plus the learning rate times.",
                    "label": 0
                },
                {
                    "sent": "Communitive sudden some of Iran's of the expelled square, the loss of the expert square.",
                    "label": 0
                },
                {
                    "sent": "And the issue with that bond is that there exist no methods.",
                    "label": 0
                },
                {
                    "sent": "To optimize the learning rates so it's not possible to get.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These aren't optimized bounds.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second form of 2nd outer bound exhibited by.",
                    "label": 0
                },
                {
                    "sent": "She's a blocky Anon was of this form, and it bounds the regret against any expert K by square root of log K times the sum of Iran's of some variance term.",
                    "label": 0
                },
                {
                    "sent": "And this Vt is actually some dispersion of the losses of the experts.",
                    "label": 0
                },
                {
                    "sent": "According to the, the weight distribution chosen by their owner.",
                    "label": 0
                },
                {
                    "sent": "And the issue with that balance is that it does not reflect the fact that it might be easier to to compete with some experts than with other experts.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our contribution in our paper is to prove a new form of signal out of bounds, and this is in terms of excess losses.",
                    "label": 1
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "The second outer bound is the following.",
                    "label": 0
                },
                {
                    "sent": "It bounds or regrets against any expertcare by square root of log K times the sum of all runs.",
                    "label": 0
                },
                {
                    "sent": "Of the excess loss of the learner against the expelled K square.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our bounds and the goal of the stork will be to explain you what why this is interesting.",
                    "label": 0
                },
                {
                    "sent": "Because of course if you have a new form of bonds, it's not.",
                    "label": 1
                },
                {
                    "sent": "It's not enough.",
                    "label": 0
                },
                {
                    "sent": "So in interest I will show you some nice feature of these bounds.",
                    "label": 0
                },
                {
                    "sent": "So like from like in the setting of experts, reporting confidence is like we will show that it enters an improvement or small excess losses will ensure that there is a.",
                    "label": 1
                },
                {
                    "sent": "It entails constant rates in the in the special case of IID losses and the last points rise prove very recently by only giving 10 version.",
                    "label": 1
                },
                {
                    "sent": "We proved that.",
                    "label": 1
                },
                {
                    "sent": "That this kind of loss entails a probabilistic bounds on the communitive risk.",
                    "label": 0
                },
                {
                    "sent": "In some stochastic scenario, with very few assumption.",
                    "label": 0
                },
                {
                    "sent": "And we will show that the key element in our analysis is to consider not one learning rates for their algorithm, but when learning wiper experts will consider an algorithm with multiple learning rates.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to get our regret bounds, the.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that we considers is largely inspired by the pod forecast of Chester Brown Keenan that they introduced to prove second abounds.",
                    "label": 0
                },
                {
                    "sent": "And so our first record, what was this algorithm?",
                    "label": 0
                },
                {
                    "sent": "So this algorithm depends on the learning rates.",
                    "label": 0
                },
                {
                    "sent": "ITA, which is positive, and at each runs T it's a.",
                    "label": 0
                },
                {
                    "sent": "It assigns to the expert K wait PPP Katie, which is normalization of some sort of weights so that the weights vector sum to one and then it updates the weight vector of the weight of the experts as as it as it is shown here.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not working.",
                    "label": 0
                },
                {
                    "sent": "Maybe this one as it is shown a it's a take the previous set of weights.",
                    "label": 0
                },
                {
                    "sent": "And multiply it by 1 plus.",
                    "label": 0
                },
                {
                    "sent": "The learning rates times the excess loss of experts of the learner against expertcare.",
                    "label": 0
                },
                {
                    "sent": "We can show that this algorithm if we don't optimize the learning rates, get this kind of regret bounds.",
                    "label": 0
                },
                {
                    "sent": "So it's almost our second order bounds.",
                    "label": 0
                },
                {
                    "sent": "The issue is that we cannot optimize alert.",
                    "label": 0
                },
                {
                    "sent": "The learning rates for all experts simultaneously.",
                    "label": 0
                },
                {
                    "sent": "We don't know in advance which one will be the best expert, so.",
                    "label": 0
                },
                {
                    "sent": "We have a kind of impossible tuning issue.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The solutions that we propose ways to consider not almost the same algorithm, but with multiple learning rates, so we have no K learning rates.",
                    "label": 1
                },
                {
                    "sent": "Each expert has its own learning rates and we have almost the same algorithm accepts that no, we have an learning rate that depends on the expert.",
                    "label": 0
                },
                {
                    "sent": "And we can show that we retrieve exactly the same bounds, except that now we have a learning rates depending on the experts and so now we don't have to know in advance which one will be the best expert we can optimize each learning rates independently.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we optimize the bonds with the running rates, we get our second out of bounds.",
                    "label": 0
                },
                {
                    "sent": "In excess loss and we can, we can show that this can be calibrated online at a small multiplicative cost of a log log T, so it's almost constant.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, no I would try to explain you why this is interesting.",
                    "label": 0
                },
                {
                    "sent": "So first I tell you what touch controls and improvement for small excess losses.",
                    "label": 1
                },
                {
                    "sent": "And what's nice is that all the features that I will show you are not captured by the algorithm, but by the bounds only.",
                    "label": 0
                },
                {
                    "sent": "So the first feature is that any strategy that satisfies this kind of regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Also satisfies this.",
                    "label": 0
                },
                {
                    "sent": "From regret bounds and disregard bond is almost the improvement for small losses.",
                    "label": 1
                },
                {
                    "sent": "Here we retrieve almost the cumulative loss of Expertcare, except two things.",
                    "label": 0
                },
                {
                    "sent": "The first is that we don't consider or runs, but only the runs where the expert K incurred a loss bigger than the learner and the second one is that we.",
                    "label": 0
                },
                {
                    "sent": "Remove the loss of the learner to the loss of the gift cards, so this is always OK up to the constants always at least as good as the.",
                    "label": 1
                },
                {
                    "sent": "Improvement for small losses, except that now we have some additional nice features.",
                    "label": 0
                },
                {
                    "sent": "For example, this bond is invariant by translation.",
                    "label": 0
                },
                {
                    "sent": "If we remove the constraints to the loss of our experts, then the the bounds remains unchanged, which was not the case with these bonds.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A second feature of our Bones is in the setting of experts that reports their confidences.",
                    "label": 0
                },
                {
                    "sent": "This setting was introduced by Bluman Monsour in 2007 and it is almost the same setting at the previous one, except that no at the beginning of each round, each expert K expresses his confidence in his prediction by giving a number in 01 ikt.",
                    "label": 1
                },
                {
                    "sent": "And then the rest exactly the same as the previous setting, each learner choose a weight vector.",
                    "label": 0
                },
                {
                    "sent": "Each expert incurs some loss, and the learner accused the loss, which is weighted average of the expert's losses.",
                    "label": 0
                },
                {
                    "sent": "No, the major difference with the previous setting is in the definition of their regrets.",
                    "label": 0
                },
                {
                    "sent": "Which is not the difference between the cumulative loss of the learner and the cumulative loss of the expert, but the runs rated by the confidence of the expert.",
                    "label": 0
                },
                {
                    "sent": "To understand this, you can show the special case where the confidence are zero or one which expresses the fact that expert can be active or inactive.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this setting, the best bonds available bonds before for confidence.",
                    "label": 0
                },
                {
                    "sent": "Free weights was in the paper of Bloom and more so in 2007 and was the following, so it was it bounced.",
                    "label": 0
                },
                {
                    "sent": "The regret, the confidence we get against any expert.",
                    "label": 0
                },
                {
                    "sent": "K bye you square root of log K times.",
                    "label": 0
                },
                {
                    "sent": "Times this.",
                    "label": 0
                },
                {
                    "sent": "And this is almost the improvement for small losses, except that we have now the confidence of the experts just before their losses.",
                    "label": 0
                },
                {
                    "sent": "The issue with that bond is that he does not care linearly with the confidence.",
                    "label": 0
                },
                {
                    "sent": "For instance, here the confidence regrets if we divide or the confidence is by two.",
                    "label": 0
                },
                {
                    "sent": "The red should be divided by two, which is not the case in this bound.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the nice feature of our bounds that we can show that any strategy?",
                    "label": 0
                },
                {
                    "sent": "That's the only the algorithm that I showed to you, but any strategies that satisfies this regret bounds.",
                    "label": 0
                },
                {
                    "sent": "If we apply it to some modify losses which are as follow but.",
                    "label": 0
                },
                {
                    "sent": "Don't try and try to understand it.",
                    "label": 0
                },
                {
                    "sent": "It's not very complicated, but.",
                    "label": 0
                },
                {
                    "sent": "It's fine, so any strategy that I applied to this modified losses will.",
                    "label": 0
                },
                {
                    "sent": "Will get a confidence regulates with this regret bounds.",
                    "label": 0
                },
                {
                    "sent": "And we show that we have almost are second order bound exactly expects that we have the confidence is of the expert squared just before the excess losses.",
                    "label": 0
                },
                {
                    "sent": "And if we bounce this by one, we get this kind of regret bounds.",
                    "label": 0
                },
                {
                    "sent": "And now we don't have the scaling issue in the confidences, and as the confidences are between zero and one, this is always better than the previous bounds.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, III's nice feature of these bounds is in the same setting presented by Tim before.",
                    "label": 0
                },
                {
                    "sent": "In the case of IID losses, we assume that know the losses are not in the worst case they are not generated by anniversary, but by some idea setting and that some expert is better than all the others in expectation is loss is better than or other experts by some constant.",
                    "label": 0
                },
                {
                    "sent": "Alpha so if we are in this setting we can show that any strategy that satisfies our right bounds.",
                    "label": 0
                },
                {
                    "sent": "Has a constant regulating expectation against the best expert.",
                    "label": 0
                },
                {
                    "sent": "And we have the similar results in probability with high probability.",
                    "label": 0
                },
                {
                    "sent": "They regret against the best expert K star will be constant as well.",
                    "label": 0
                },
                {
                    "sent": "So it is very nice is that.",
                    "label": 0
                },
                {
                    "sent": "In the previous analysis, this kind of features was only in the algorithm, it was.",
                    "label": 0
                },
                {
                    "sent": "Not in the record bonds.",
                    "label": 0
                },
                {
                    "sent": "Now we have one regret bounds that captures either.",
                    "label": 0
                },
                {
                    "sent": "The worst case is the easy data with constant regret.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, last last consequences of our record bounds is.",
                    "label": 0
                },
                {
                    "sent": "Is, uh was proved very recently by only giving it energy in the very recent paper, which is an archive.",
                    "label": 1
                },
                {
                    "sent": "I won't present.",
                    "label": 0
                },
                {
                    "sent": "This wasn't very impressive very precisely, but if you want you can check his paper.",
                    "label": 0
                },
                {
                    "sent": "And what he does in his paper, he extends our analysis to exponential updates instead of product updates that we used.",
                    "label": 1
                },
                {
                    "sent": "And he proved that.",
                    "label": 0
                },
                {
                    "sent": "Any algorithm, any strategy that satisfies our second order bounding excess loss gets also nice bounds.",
                    "label": 0
                },
                {
                    "sent": "I think optimal bounds in on cumulative risk in a very in question are stochastic stochastic setting.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, we had we proved this new form of 2nd order bounds with lots of with some very nice features and.",
                    "label": 0
                },
                {
                    "sent": "I think that's all, thank you.",
                    "label": 0
                }
            ]
        }
    }
}