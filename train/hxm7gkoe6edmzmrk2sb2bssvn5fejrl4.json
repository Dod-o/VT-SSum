{
    "id": "hxm7gkoe6edmzmrk2sb2bssvn5fejrl4",
    "title": "Tutorial on Neural Network Optimization Problems",
    "info": {
        "author": [
            "Ian Goodfellow, Google, Inc."
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_goodfellow_network_optimization/",
    "segmentation": [
        [
            "So today I'm here to give you a tutorial on what I'm going to call neural network optimization problems.",
            "And the talk is mostly going to focus on the structure of the objective functions that we optimize when we train neural Nets, but.",
            "It's also going to look a little bit at the interactions between the learning algorithms we use.",
            "And the structure of those loss functions.",
            "Some aspects of the loss functions don't matter if the optimization algorithms never interact with them.",
            "I'm not going to go into a lot of detail about the algorithms themselves.",
            "I'm going to give you just a few small cartoons of a few algorithms that you'll probably already know, and the main reason we bring in those cartoons is so we can have a simple tool that we can use to understand the way that the optimization algorithm interacts with the structure of the neural network loss function.",
            "This is not really an area where we have definite knowledge of what everything looks like.",
            "This is an area where we're just starting to make really serious inquiries that actually make progress, so understanding the structure of neural network optimization problems is itself a very open problem.",
            "Then something that maybe some of you can go home and study, but for those of you who aren't going to study this, hopefully you can gain some insight into why your optimization algorithms might not be.",
            "Performing as well as you want them to.",
            "And unfortunately, it turns out that the reasons they might not perform well are complicated, and there are many different ways that they can fail.",
            "So the main purpose of the talk is for you to realize that there are all these different ways that they can fail and be able to tell to some extent which one is happening in your particular case."
        ],
        [
            "A lot of this tutorial is just information that is so basic and so old that it doesn't necessarily need a specific citation, but about half the tutorial is newer research work, and in that part of the tutorial I'll be presenting essentially ideas from 4 different recent research papers, starting with Andrew's ex and his collaborators paper at Highclere 2014.",
            "An exact solutions to the nonlinear dynamics.",
            "Of learning and deep linear networks.",
            "And then there's been some other follow up work on that here in Montreal, Yondo fan and some collaborators, including Joshua, wrote a paper about.",
            "Extending some of the theoretical ideas from entry sexist paper to actual empirical experiments with neural Nets.",
            "And Anna Torv Manske and her collaborators at NYU.",
            "You developed the theory a lot further and made the theory more specific to neural Nets.",
            "And then finally, I'll be showing you a lot of visualizations that I made.",
            "For a paper with some of my Google collaborators for I clear this year.",
            "Because of the format of the tutorial, it's a talk with slides.",
            "I'm going to show you a lot more of the visualizations from the last paper than the other three.",
            "The other three papers are important, and if you're interested in this, you should definitely read them.",
            "Don't take my emphasis on the visualizations as saying that the other three are less important.",
            "It's just that visualizations are the easiest things to engage with in a talk."
        ],
        [
            "The basic idea of optimization for training and neural network is that we have some cost function J and some set of parameters Theta, and we represent the parameters as a vector.",
            "We want to find the value of this vector that minimizes J.",
            "In practice, we almost never find the actual minimal value of J.",
            "We just want to make J get significantly smaller by the end of training than it was at the start of training, and there are many different ways you can do this.",
            "In principle for a lot of people in other fields, optimization means exhaustive search where you just scroll through a space looking for low values of the cost function and a lot of the time discrete optimization relies on techniques like that, and they come up with.",
            "Tricks to prune the search space.",
            "We're lucky that neural Nets are usually parameter isable in terms of real numbers where we can compute gradients.",
            "And that allows us to use more sophisticated search procedures than just random search.",
            "And random search appears for neural Nets occasionally as genetic algorithms, but it's usually not nearly as efficient as when you can take advantage of.",
            "So the extra knowledge you have about the cost function.",
            "In some cases we can actually solve analytically for optimal variables, either all of them at once or for a subset of them given another subset.",
            "It's also possible to solve these optimization problems using model based search, where for example you have a Gaussian process that predicts the value of the cost function for different parameter points, and you solve for the minimum set.",
            "But most of these methods usually are not feasible or don't work as well when you get to the scale of problem that we use for neural Nets.",
            "So neural Nets almost always use gradient based search.",
            "Which I'll describe more in a few slides."
        ],
        [
            "In order to do gradient based search you need to have derivatives and 2nd derivatives to actually implement the algorithm you need the derivatives and to understand whether the algorithm is going to perform well at all, you need to have some idea of what the second derivatives look like.",
            "The basic idea is that the gradient, the vector of all the derivatives of the cost function with respect to the parameters, tells you the direction where the cost function decreases the fastest in an infinitesimal window.",
            "So if you want to make a very small step, the gradient tells you the fastest direction that you can move downhill.",
            "The second derivatives tell you how quickly those first derivatives are going to change.",
            "And that is important because it tells you the size of this step that you can make.",
            "If you have a derivative that's negative and you have a second derivative, that's positive, it means that your derivative is going to start increasing as you move along that axis.",
            "And after it is increased too much, eventually it will be 0 and you're no longer moving downhill in that direction.",
            "I've plotted 3 little plots here where you can see what happens when you have negative curvature, no curvature and positive curvature.",
            "Negative curvature just means that the second derivative is negative.",
            "And it means that your derivative is going to decrease as you move to the right.",
            "So here if we were doing gradient descent in a single dimension.",
            "We would take the derivative and see that the derivative is negative, so we're going to move to the right.",
            "And then when we want to figure out how well this derivative predicts what's going to happen in the future, we can look to the second derivative and see that the cost function is going to accelerate.",
            "It's decent as we move to the right.",
            "So in this case, where there's negative curvature, gradient descent actually is pretty nice, as long as this first and second derivative remain a good approximation for the function, we can take really big steps, and we're actually going to get more improvement than you would expect just from the derivative alone.",
            "Another nice case for gradient descent is the situation where there's no curvature where the second derivative is zero.",
            "In that case, the gradient predicts exactly what the value of the cost function is going to be, regardless of this step size.",
            "And so to the extent that the 1st and 2nd derivative continued to give you a good approximation of the cost function, any stepsize is safe.",
            "The third case is when you have positive curvature.",
            "In this case, a second derivative is positive and it means that the derivative will increase as you move to the right.",
            "Unfortunately, that means that eventually the derivative will be 0 and you won't be able to make anymore progress.",
            "You can see that here where the cost function levels out.",
            "So in this case, the relationship between the derivative and the second derivative determines the largest stepsize that's safe.",
            "I'll go into more detail about what step sizes are safe in a second."
        ],
        [
            "Those pictures I just showed you were all in one dimension.",
            "When we're training neural networks, we usually have millions or billions of dimensions and because of that we need to work with.",
            "An entire matrix of 2nd derivatives, called the Hessian.",
            "So the Hessian is a matrix where element I, J gives you the derivative of gradient I with respect to parameter J.",
            "It gives us all of the different second derivatives of all of the first derivatives that we computed for the gradient descent algorithm.",
            "A lot of the time when we talk about the Hessian, it's not very informative to just look at a huge end by end grid of numbers.",
            "It's much more intuitive to look at its eigenvectors and eigenvalues.",
            "Is there anybody here who doesn't know what an eigenvector and eigenvalue is?",
            "If anybody doesn't know they're too shy to raise their hand so.",
            "Yeah, who knows what?",
            "Who knows what?",
            "Eigenvalues and eigenvectors are.",
            "It looks like pretty much everybody.",
            "If there was anybody who didn't raise their hand an eigenvector when multiplied by a matrix.",
            "Only gets scaled, so if you have a vector V and you multiply it by H. If V is an eigenvector, you'll get Lambda times V back out of it.",
            "The Hessian matrix of neural net optimization functions is almost always symmetric.",
            "Has had matrices in general are symmetric if both the function and its derivatives, and I forget maybe its second derivatives as well or are continuous.",
            "I don't remember the exact condition is, but for neural networks, the condition that guarantees that the Hessian will be symmetric is almost always satisfied.",
            "For things like rectifier networks, the Hessian might not be symmetric when you're right on the boundary where a rectifier goes from zero to linear, but most places on most neural networks, the Hessian will always be symmetric.",
            "Because the Hessian is symmetric, that means that it has a very nice decomposition into eigenvectors and eigenvalues.",
            "Every real symmetric matrix can be turned into a matrix containing orthogonal eigenvectors and a diagonal matrix containing their eigenvalues.",
            "So we can decompose the Hessian matrix into a basis of eigenvectors Q.",
            "A diagonal matrix of eigenvalues associated with them, called Lambda.",
            "And then if we multiply that by Q transpose, we get the Hessian back.",
            "In order to get a directional second derivative to actually take the second derivative along one particular line in space, we can represent that line with a vector D. This is a unit vector telling us the direction in which we want to take the second derivative.",
            "And if we multiply D transpose HD.",
            "That gives us the second derivative in the direction that we just chose.",
            "If we understand the Hessian matrix in terms of its eigenvalues and eigenvectors, that gives us an intuitive way to understand what the directional second derivatives can look like.",
            "Specifically, you add up the cosine squared of the angle between D and each eigenvector multiplied by the corresponding eigenvalue.",
            "This might be a lot to digest right now.",
            "We keep in mind that cosine squared is always going to be between zero and one cosine is between negative one and positive one.",
            "Cosine squared is going to make that all the positives, it always between zero and one.",
            "So you can think of each directional second derivative as ticking a combination using coefficients between zero and one of the eigenvalues of the Hessian matrix.",
            "That means the eigen values tell us what the.",
            "The total possible range of the directional second derivatives will be the yeah.",
            "Yeah, so I'm saying we're defining D to be a unit vector indicating a direction, yeah?",
            "And the the directional second derivative is based on.",
            "Looking in a specific direction and saying what's the second derivative.",
            "That way it's it's not really defined with respect to the magnitude of any vector.",
            "So the largest second derivative that you can find in any direction is given by the largest eigenvalue of the Hessian matrix and the smallest the most negative.",
            "Eigenvalue that you can find gives you the smallest directional second derivative that could happen by choosing any vector.",
            "In this plot over here, I'm plotting the directional second derivative as the radius, and I'm plotting the angle between two different eigenvectors as Theta.",
            "So over here we've got an eigenvector with a negative eigenvalue, so Lambda times the eigenvector puts you over here.",
            "Over here we've got an eigenvector with a positive eigenvalue, so Lambda times the eigenvector puts you here and as we swing around the circle.",
            "If we're over here, then the direction of 2nd derivatives here.",
            "If we're over here, then the directional second derivatives.",
            "Here.",
            "You can kind of think of the different directions as tracing out this multi dimensional ellipse where the different eigenvalues determine the scale of the ellipse and also whether its axes are flipped.",
            "So for a lot of the rest of the talk I'm going to be telling you about the eigenvalues of the Hessian matrix.",
            "Just remember that these eigenvalues are the different possible directional second derivatives that can be blended together.",
            "When you look for directional second derivatives in specific directions that are not eigenvectors."
        ],
        [
            "One very important tool that we use a lot of the time that makes use of the directional second derivative is the Taylor series approximation to a function.",
            "I'm assuming a lot of you have probably seen a Taylor series in one dimension before.",
            "The idea is you can use the derivatives and the second derivatives of a function to approximate its behavior near some reference point.",
            "So if you begin by evaluating the function at some reference point X0.",
            "Then when you want to look at a new value of X, you can take the difference between the new X and X0, and I have a mistake here.",
            "This should say X0 inside both of these derivatives.",
            "Sorry bout that.",
            "The idea is you can multiply the derivative at the initial point by the difference between the reference point in your current location and that will give you a linear prediction of what the cost function will be at the new point.",
            "But that linear prediction is going to become less and less accurate the farther the text gets from X0.",
            "So to correct for it, we add another term.",
            "Based on the second derivative, where we add 1/2 times the difference squared times the second derivative.",
            "You can go on adding more and more terms like this using third derivatives and so on, but typically in the context of neural net optimization, we're only going to look at the first 2 derivatives.",
            "If we extend this to multiple dimensions, we're still doing more or less the same thing every time we want to evaluate the derivative at a point, or evaluate the approximate cost function at a point Theta.",
            "We essentially construct a 1 dimensional Taylor series approximation in the Ray looking from Theta zero to Theta.",
            "So we evaluate J of Theta zero.",
            "That's our baseline cost.",
            "Then we look at the vector Theta minus Theta Zero, which gives us the offset from the baseline point to the new point.",
            "And we take the dot product of that and the gradient that gives us a linear prediction of the change in the cost function value due to the gradient itself.",
            "But then we can correct that further by using the directional second derivative scaled by the magnitude of the change.",
            "So we add 1/2 Theta minus Theta, zero transpose H times Theta minus Theta 0.",
            "This Theta minus Theta zero is standing in for the D on the previous slide and it's no longer a unit vector, so the magnitude of the of the offset actually affects the magnitude of the correction that we get from looking at the 2nd order terms."
        ],
        [
            "One thing we can use with the Taylor series approximation is to figure out how much we can expect a gradient descent step to improve.",
            "So we can say right now we're at Theta.",
            "We want to go to Theta minus epsilon times G, and we want to predict how much better the cost function is going to be.",
            "To evaluate that point using a second order Taylor series prediction, we start off by saying, well, what is J of Theta minus epsilon times G. We can approximate it as being J of Theta.",
            "Minus epsilon times G that's are offset.",
            "Multiplied by the gradient that's from the Taylor series formula.",
            "Plus 1/2 epsilon G transpose H epsilon G and we simplify that the epsilon squared send up on the left.",
            "So this gives us an idea of what the cost function looks like for different step sizes epsilon that we use to descend the cost function.",
            "In the worst case.",
            "G will be perfectly aligned with the eigenvector of H that has the largest eigenvalue.",
            "That's the worst case because it makes this second order correction term rise up the highest and counteract the improvement from the gradient the most.",
            "You'll notice that this middle term here the negative epsilon G transpose G. That is always going to be non negative in the worst case the gradient has zero norm, so you just can't improve by stepping downhill.",
            "Or rather, there's not a downhill direction to step.",
            "This term here can be either negative or positive depending on the directional second derivative.",
            "So the worst case is when that directional second derivative is either positive or when it's not as negative as it could be.",
            "It's not accelerating you as much as you want.",
            "When this directional second derivative is negative, the Taylor series actually predicts that you can keep on stepping farther out forever, and you'll always get an improvement.",
            "In fact, you'll improve faster the further out you step.",
            "Usually you can't trust that kind of prediction.",
            "Usually the Taylor series will become inaccurate after you've moved very far, and terms involving the third derivatives will start to dominate.",
            "Or in the case of things like rectifier Nets, there's very sharp discontinuity's that make the Taylor series approximation become inaccurate much earlier.",
            "Where this view is useful is when the second derivative is positive, and in that case we can use it to compute the optimal stepsize optimal, at least from the point of view of getting the most improvement on this particular step.",
            "So just by looking at the quadratic function that we've got here, we can solve that.",
            "The optimal step size is the norm of the gradient squared divided by the 2nd derivative times the norm of the gradient, or G, transpose, G. /, G, transpose HG.",
            "Keep in mind that G transpose HG can at most be the norm of the gradient squared times the largest eigenvalue.",
            "So the eigenvalues actually tell you what the largest safe step size is.",
            "Overtime the eigenvalues tend to become more positive after you've trained longer and a lot of the time the reason that you are no longer able to train is not so much that the gradient has become small on an objective scale, but because the gradient has remained about the same size and the eigenvalues have become very much larger and more positive.",
            "That means that the largest safe step size you can make is very small."
        ],
        [
            "So there are several different kinds of critical points that occur in neural network objective functions.",
            "A critical point is any point where the gradient is 0, so that means that the cost function is locally flat at that point.",
            "The Hessian are useful again for telling what kind of critical point we've reached.",
            "If all of the eigenvalues are positive, then the critical point at the bottom of this graph.",
            "Is a local minimum and we like those.",
            "We want to find a minimal point and stop there so it's very nice if you can find a place that has zero gradient and all of the eigenvalues of the Hessian being positive.",
            "There is also a critical points that are local Maxima, so at a local maximum all the eigenvalues of the Hessian are negative and that means that any direction you step the second derivative is going to cause the cost function to go down even though right where you are in the very top, it's flat.",
            "These points don't really come up in practice very much when you're using a cost function.",
            "When you're minimizing a cost function.",
            "The reason is that they're very unstable if you initialize right next to one, gradient descent will easily move away from it and go downhill.",
            "You have to initialize right on it in order to have a problem, and the way that we optimize neural Nets is usually so noisy that even if you are at a local maximum of the cost function, you might not be at the local maximum of the mini batch that you use for the first step of gradient descent.",
            "And even if you're at the local maximum of that first mini batch, the second mini batch is likely to have a different one, so you'll get shaken off of these very easily.",
            "A third kind of point is a saddle point.",
            "This is an interesting critical point that is a local maximum.",
            "If you look at some axes at a local minimum.",
            "If you look at some other axes.",
            "So here you can see across this axis, it looks like you're at a local minimum.",
            "If you're at this axis, it looks like you're at a local maximum.",
            "These critical points have some positive and some negative eigenvalues of the Hessian and over the past few years several people have become much more interested in their role in optimizing neural networks.",
            "Previously they were more or less ignored.",
            "They are usually considered to be fairly unstable points because if you initialize near one, you get shaken off of it.",
            "But not every optimization algorithm has that property.",
            "Also, sometimes they are surrounded by wide flat regions where even though the critical point itself is unstable, the wide flat region around it might be difficult to escape."
        ],
        [
            "One method that has a strong interaction with subtle points is Newton's method.",
            "The idea behind Newton's method.",
            "Is to form a local quadratic approximation and then minimize that quadratic approximation.",
            "That's the way that it's usually presented.",
            "An easier way to see the problem that Newton's method heads with saddle points is to use this alternative derivation.",
            "I'm going to provide here.",
            "So first we begin by assuming that the minimum eigenvalue is greater than zero.",
            "That means that every critical point we could find is a maximum.",
            "It's not a saddle point, it's not a minimum.",
            "Next we want to solve for where the gradient at point Theta is equal to 0.",
            "We're going to solve that equation for Theta and that will give us a critical point.",
            "And by our assumption that critical point will be a minimum.",
            "So we just succeeded in finding a minimum, yeah.",
            "Yeah, sorry, that's a mistake.",
            "Yeah, so all the critical points are minimum.",
            "If it's too hard to solve this equation here.",
            "Then we need to make a simplification when when G of Theta is actually back, prop innarelli net then we can't really solve this equation analytically and you can solve it numerically, but that's just as hard as training the neural net in the first place.",
            "What we do to make a an analytical solvable version of this equation is we approximate the gradient with a first order Taylor series approximation.",
            "So before we were talking about Taylor series approximations of a single scalar.",
            "Now we're going to make a Taylor series approximation of a vector.",
            "The idea is we use the gradient plus the Hessian times the difference between the reference point and Theta.",
            "And that will predict the gradient at the new Theta point.",
            "Now we want to solve for where that predicted gradient is equal to 0.",
            "The solution tells us that the new Theta that we want to jump to is equal to the baseline minus the Hessian inverse times the gradient.",
            "And this works very well if you have convex problems where all the where all the eigenvalues of the Hessian are positive.",
            "You can see that if we get rid of this first assumption here, if we allow the eigenvalues to be both positive and negative, then this could go to any kind of critical point it could go to a local maximum.",
            "It could go to a saddle point."
        ],
        [
            "In practice, people try to mitigate this by using what's called damping they add.",
            "Essentially they make a fake shift of the eigen spectrum to make it more positive and then that shrinks the step size.",
            "It can be difficult to know what the smallest eigenvalue is there, so it can be difficult to be sure that you're dumping enough to avoid this problem without damping so much that you caused this step size to shrink beyond what is necessary."
        ],
        [
            "OK, so a lot of people have a very.",
            "Wrong Cartoon picture of how optimization ends in their head.",
            "People think that SGD usually moves downhill.",
            "And then eventually SGD will encounter a critical point.",
            "That critical point is usually a local minimum, and there are other minima that have much lower value and we failed because we got stuck on this bad local minimum and what you really wanted to do is go to some other global minimum.",
            "But the thing that's wrong about this cartoon is that you think that you've definitely reached a critical point and you've definitely converged in practice.",
            "That is usually not how a neural net training proceeds and one of the biggest reasons is overfitting.",
            "Usually, we don't actually determine convergence.",
            "By testing the gradient to see if we've reached a critical point, usually we determine the end of the training algorithm by monitoring the validation set error and when the validation set error starts to go up, we terminate the training.",
            "There's no reason to think that you're actually on a critical point when that happens, and usually you can see that the validation set error is going up very fast, so you are actually definitely not on a critical point.",
            "You're somewhere moving rapidly through parameter space."
        ],
        [
            "So that's that's one of the convergence myths that have been widespread that we want to bust.",
            "But in a few slides and make sure that there are even more reasons besides overfitting.",
            "While you can't usually expect that you're converging to a critical point, there's another new myth about SGD convergence.",
            "That's kind of become popular recently.",
            "The new myth is a SGD usually moves downhill until it encounters a saddle point, and then it becomes stuck on that saddle point and cannot escape.",
            "Newtons method definitely does this.",
            "There's not much reason to think that SGD does this."
        ],
        [
            "One reason that both of these ideas are myths is that some functions lack critical points of any kind.",
            "And.",
            "In many cases the function can have critical points, but you avoid them altogether.",
            "This is actually extremely common with unregularized classifiers, So what I'm plotting here is a 1 dimensional cross section of the negative log likelihood for a softmax classifier.",
            "The idea is that once you've found a configuration of the weights where you can classify the training set correctly, you can always become more and more confident about your classifications and marginally reduce the training set.",
            "Likely the you can marginally increase your confidence and.",
            "Is the negative log likelihood on the training set?",
            "So stochastic gradient descent training something like logistic regression with a separable class distribution will never converge.",
            "And that's not anything exotic and related to neural Nets, it's just a relatively universal property.",
            "You do see this happen in neural Nets if they have enough capacity to fit the training set, but we often just use such large training sets that they don't manage to fit them."
        ],
        [
            "We also see in practice, if we actually go ahead and instrument the norm of the gradient, that it's quite common for SGD to not encounter any kind of critical point or flat region.",
            "I am not saying that this happens in every case, but the cartoon view that you can generally expect SGD to go to a flat place and then stay there does not really explain a lot of very common cases.",
            "So this neural network here was state of the art on cifar 10 a few years ago, and if you look at its gradient, you can see if we take a running average of the gradient, it increases smoothly overtime and actually never gets smaller than where it started.",
            "It's because as you train the neural net, its weights get bigger and so it's possible for it to have a larger gradient.",
            "And in the case where it makes a mistake.",
            "In addition to this running average, we also took a snapshot once after every pass through the training set.",
            "That's the green curve that zigzags a lot, so you can see that there is some variance in the norm of the gradient, but overall it's on a very smooth increasing trend.",
            "You can also see that the optimization is proceeding correctly.",
            "The validation set objective is going down after a while it starts to go up due to overfitting the classification rate.",
            "The misclassification rate continued to go down for a long time, while the objective on the validation set was going up."
        ],
        [
            "So one way that you can see how gradient descent behaves near a saddle point is to actually solve the differential equations that predict what it will do.",
            "This is assuming that the 2nd order Taylor series approximation is a valid way of modeling the cost function.",
            "Near where you initialize the process, actually it's a little bit simpler than that.",
            "It's assuming that the 1st order Taylor series approximation of the gradient is reliable.",
            "So the idea is, we say that we're going to make infinitesimal step sizes, so we're going to look at how Theta evolves overtime.",
            "So we're going to write down an equation for D /, D T of Theta at time T. At Times Zero, the data is going to just travel directly downhill.",
            "This is running just normal gradient descent, no momentum or anything.",
            "So to show that initially we're going directly downhill, we say that we're going to move in direction negative G. And then after we've moved.",
            "Our gradient would actually be G anymore.",
            "Instead, it's going to be predicted by a first order Taylor series prediction.",
            "Of the Hessian, multiplied by the difference from where we began the process.",
            "So this is going to when you add these up.",
            "This gives you the predicted gradient at time T, and we're saying we're just always going to send that.",
            "If you apply this to a quadratic function, then these equations are exact.",
            "If you apply these to a neural net, then they are only approximate, but they do tell us what happens on quadratic functions that have saddle points.",
            "If we solve the differential equation, we end up with Theta, T is equal to the original Theta minus Q, Lambda prime of T * Q transpose G. So what's going on here?",
            "Q is the eigenvectors of the Hessian.",
            "Lambda Prime is a modification of the original eigenvalues of the Hessian, and it's a function of time.",
            "The way that this works is each eigenvalue gets rescaled independently.",
            "According to this squashing function here.",
            "Lambda is the original eigenvalue of the Hessian and Lambda prime of T is the new value that we use to predict where.",
            "Where the parameters will be at time T. And the function that ends up determining what the new eigenvalues look like is 1 -- Y to the negative Lambda T divided by Lambda.",
            "That's pretty hard to evaluate in your head, so here's a plot of it on the X axis.",
            "I'm showing you the original Lambda on the Y axis.",
            "I'm showing you the new Lambda.",
            "When the original Lambda is very large.",
            "The squashing function goes down close to 0.",
            "This is plotted for T = 1 as indicated at the top of the plot.",
            "But it has the same basic shape, just less extreme for other values and more extreme for larger values.",
            "But the basic the basic property I'm describing holds for any value of T except 0.",
            "And negative T is not allowed.",
            "If you're right at T = 0, then there's no squashing.",
            "And if you go to your bed and equals 0, there's there's no squashing.",
            "If you go to negative Lambda, then the contribution of the gradient in that direction actually gets exponentially amplified.",
            "So when you, yeah.",
            "How much for moving?",
            "What I'm doing is I'm analyzing what happens if you initialize SGD nearest saddle point so some people would say SGD doesn't look at the 2nd order information and therefore it's not able to figure out how to step away from the saddle point you have.",
            "Negative curvature in One Direction, positive curvature in the orthogonal direction.",
            "Some people would say you are not able to see that there is negative curvature this way, so you don't go there.",
            "This differential equation is analyzing what actually happens, and it's saying if you make the step size small enough that the differential equation is valid.",
            "Then we know for sure that unless your initial gradient is completely orthogonal to any direction of negative curvature, you will exponentially amplify the component of the gradient that aligns with the negative eigenvalues, and you will run away from the saddle point, so this isn't this isn't making a recommendation about what optimization algorithm we should use, it's it's saying what actually should happen.",
            "What is the theoretical prediction for what will happen in general for stochastic gradient descent?",
            "The reason I'm using a differential equation rather than like running an experiment is you can always look at it.",
            "Experiment and say well, you just did some trick.",
            "You had fancy preprocessing or or you picked the parameters in the step size to make it work.",
            "But what I'm saying here is there's an actual principle that the differential equation says in less you really go out of your way to make sure that it's impossible to see that any negative curvature is there and you and you put the initial point exactly on the saddle point.",
            "You will run away from it and.",
            "A more realistic way of thinking about it is.",
            "When you're starting farther away from a saddle point, you're going to get repelled from it.",
            "If there's any negative curvature at all, they're not attractive points there.",
            "As long as there's negative curvature, they're repulsive."
        ],
        [
            "Another important problem more generally, that does actually affect gradient descent, is just the problem of poor conditioning.",
            "This refers to the Hessian having.",
            "Different ranges of eigenvalues, either very small ones or very large ones.",
            "What happens is you end up with a long quadratic truf.",
            "Where it's much steeper when you move back and forth this way.",
            "Then when you move this way in the long run you want to move down the truck to the very bottom of the truck.",
            "But stochastic gradient descent initialized on the walls of the Canyon is mostly going to see the the Canyon walls as being the main source of gradient, so it's going to move more or less orthogonally across the Canyon, and make very little progress along this slow axis.",
            "I it essentially bounces back and forth several times.",
            "You can fix this to some extent by making the step size small, but then you require more steps to reach convergence, so either way this makes you progress very slowly, either due to oscillations or due to small step size."
        ],
        [
            "Recall from earlier I showed you that the optimal step size if our goal is just to find the smallest cost function from a single step.",
            "Is G transpose G / G, transpose HG.",
            "And recall that this G transpose HG term here is sort of blending the eigenvalues together based on which eigenvector it aligns with most closely.",
            "Which eigenvector of HG aligns with most closely.",
            "When you have very different eigenvalues in your Hessian matrix, G can select very different values depending on which direction it's facing in.",
            "If your Hessian matrix has one eigenvalue, that's 10 to the minus three, and another one that ended the positive 3, then you can end up with your step size getting divided by 10 to the three and then multiplied by 2:50 on the next step.",
            "If G changes direction to visit both of those eigenvalues.",
            "A lot of the time we choose our step sizes ahead of time and just run them with a linearly or exponentially decreasing schedule.",
            "But usually any two consecutive steps have around the same step size.",
            "If this directional second derivative factor is rapidly changing, then these constant size steps are not going to be able to keep up with the optimal step size.",
            "Interesting, but.",
            "Expanding into the case where.",
            "Change direction.",
            "Yeah, so when the step direction is not G, you're going to perform worse on a single step basis, but the really difficult thing about analyzing stochastic gradient descent.",
            "Where your accounting for the mini batches is that?",
            "What I'm trying to say here is that you can get stuck and fail to make any progress in in a moment of time.",
            "I'm not trying to say that this is the optimal step size for all of learning.",
            "A lot of the time there's a delayed reward problem where very early in learning you want to actually use much bigger step sizes and this would predict is optimal and at the end of learning you'll do better because you use these large step sizes early on.",
            "We don't entirely understand what the mechanism is there, so we don't have a good way to theoretically analyze it, but we do know that if you use this to choose your step size all along, it's not going to yield that grade of final results at the end of learning.",
            "What we can understand from this kind of analysis is that the eigen values tell us something about when we might accidentally go uphill.",
            "To specialize this on the case you were asking about a bit where you're not actually following G, But you're following a wrong direction that isn't quite G you would have.",
            "You'd end up with, I believe D transpose G / D. Transpose HD.",
            "So the numerator would be smaller and the denominator would be using the blend of eigenvalues selected by D rather than by G. So you're.",
            "Overall, you're expected to make less improvement with such a step, but we know that.",
            "Because of things not captured in this small analysis, you can do better overall.",
            "In multiple dimensions, I'll drive over here so the camera can see it.",
            "You can end up with these kind of egg like shapes.",
            "Where this is meant to be a 3D drawing.",
            "So here's the axis drawn inside.",
            "Our little egg.",
            "And.",
            "I'm pretty bad at drawing 3D on a whiteboard, but.",
            "The idea is we've got an egg that descends for a long time with relatively little curvature.",
            "And reaches a relatively low value over here.",
            "And it descends very rapidly.",
            "With very strong negative curvature and reaches a low value are relatively high value over here, so we'd like to go here.",
            "If we follow the negative curvature, we end up here quickly and in a lot of cases I've seen neural Nets.",
            "Train like this.",
            "Where first they follow the negative curvature and run off the side.",
            "Of the egg and then they spend most of their training time.",
            "Winding around the egg.",
            "I think one reason that large learning rates help initially is that they cause you to pay attention to the gradient more than to the negative curvature.",
            "When you make the stepsize very, very small, you end up with the equation that go millane was asking the question about a minute ago where you have the exponential amplification of the negative curvature directions, and I think that that sucks you right off the steep side of the egg.",
            "The larger learning rate is going to be dominated more by the gradient than by the Hessian at the top, and so if you see if you make like a big step like this and end up over here.",
            "You've made a lot of progress toward escaping the area where you're tempted to go off the.",
            "Negative curvature side of the egg.",
            "You usually do still end up going off the side, but you at least do it later and you spend less time on this.",
            "This part of the arc over here.",
            "Like I said, this is a speculative idea.",
            "I'm not prepared to give you like a proof or or.",
            "Detailed visualizations that prove that this is happening.",
            "It's something that I suspect, based on some of the visualizations that I've made.",
            "Of learning trajectories and based on that equation.",
            "OK, so resuming where we were before.",
            "We just finished seeing how the eigenvalues of the Hessian can cause you to accidentally go uphill.",
            "If your step size is chosen without knowledge of the way that the directional second derivative is going to fluctuate as you change your step direction."
        ],
        [
            "Overall, we've reviewed a lot of reasons why convergence to a critical point may not happen.",
            "You might just never stop if the function doesn't have a local minimum for you to run into, or even if there are local minima, you might find a subspace where you can continue outward to Infinity.",
            "That happens a lot with classifiers.",
            "You might also get stuck in the sense that you can't compute elocal direction where you can make an update that actually causes the objective function to go down.",
            "That can happen from the conditioning being too bad, where you can't really actually go up Hill by using a fixed learning rate schedule.",
            "It can also happen by by the gradient being too noisy.",
            "If your different mini batches are very different from each other and your mini batch size is too small to estimate the true gradient very accurately.",
            "Gradient noise can also happen when you're using approximate gradients.",
            "I strongly suspect that a lot of the reason Boltzmann machines are hard to train is that there's too much not just noise, but also bias in the estimate of the gradient.",
            "And then finally, the classical reason that people usually have known anyway that we wouldn't converge to yeah.",
            "I eat that is true.",
            "But you don't necessarily know what the smallest eigenvalue is.",
            "Yeah.",
            "You're talking about damping, or you're talking about subtle Frieden.",
            "So saddle Free Newton is Joshua and Jan's project with.",
            "But you're talking about modifying it by adding.",
            "Yeah, I'm not talking about modified versions of it.",
            "The important point here is the point I'm trying to argue is not so much that Newton's method is federally flood.",
            "What I'm saying is that.",
            "The danger of getting stuck on a critical point is primarily a danger of UN modified Newton's method.",
            "It's it's not a generic danger that effects all optimization algorithms OK. Anne.",
            "There are also many modifications of.",
            "It are still affected by it, but yeah.",
            "For example, the thing most people recommend doing is called damping, where you just add a value to all of the eigenvalues, and in that case it can be extremely expensive to estimate the actual minimum eigenvalue and know that you're using the right amount of damping.",
            "A.",
            "Finding that minimum eigenvalue is so expensive that.",
            "You may as well just revert to using gradient descent.",
            "It's not really the feasibility of the method is."
        ],
        [
            "Is related anyway.",
            "OK, so an interesting question about the structure of neural net optimization problems is whether saddle points or local minima are more common.",
            "And this has become a very interesting area of research over the past few years.",
            "There are a lot of results in random matrix theory that I'm going to give you like a very cartoon view of here.",
            "This isn't the way that the proofs actually work, but it helps you to develop some intuition.",
            "Imagine that we've got a critical point.",
            "In a cost function and we want to generate the eigenvalues for its Hessian.",
            "So imagine that we're going to generate the eigenvalues by flipping a coin every time we flip a coin, and it comes up heads, we make the corresponding eigenvalue be positive every time we flip a coin, and it comes up tails.",
            "We make the corresponding eigenvalue be negative.",
            "So how do we get a local minimum in this setting?",
            "The eigenvalues of a local minimum are all positive, so that means we have to flip all heads.",
            "If you only have one variable, then you only have one eigenvalue.",
            "It's easy to flip heads once.",
            "If you have a million variables.",
            "Then you have a million eigenvalues and you've got to flip heads a million times in order to get a local minimum.",
            "That's not exactly how random matrix theory works, but it gives you the basic idea that.",
            "As the number of variables increases, the number of local minima decreases exponentially relative to the number of saddle points.",
            "Keep in mind, anytime you get both positive and negative eigenvalues in the same critical point, you've got a saddle point, so it's very easy to flip a coin twice and get one heads and one tails.",
            "And if you flip a coin a million times, it's very easy to get at least one heads and at least one tails out of all those million coin flips.",
            "The really cool part that actually makes us much more optimistic about our ability to optimize high dimensional nonconvex functions is that the coin seems to actually be weighted.",
            "It's not 5050, and the weighting changes as you descend.",
            "The cost function specifically toward the bottom of the cost function where the values of the cost function are lower.",
            "You're more likely to flip heads and make positive eigenvalues, so when the cost function is very low, you're more likely to make local minima.",
            "And when the cost function is high, you're more likely to make saddle points.",
            "If you've got a method that doesn't get stuck on saddle points, that means you can go downhill and you're not going to be worried about getting stuck in a bad local minimum.",
            "Most of the local minima are good.",
            "To be honest, I don't really know exactly in in real life a lot of the theoretical frameworks assume that they are independent, I think."
        ],
        [
            "When you're optimizing your higher probability to end up in a high dimensional Canyon with a.",
            "As far as I know, this analysis doesn't really tell us a whole lot about what Canyon structures look like.",
            "You mean, having like a mixture of small and large positive eigenvalues?",
            "It does, yeah, it does tell you that toward the end of learning there are going to be very many large positive eigenvalues and.",
            "If you instrument them, if you instrument the like the directional second, derivative derivative in the direction of your gradient.",
            "You definitely do see this happen that at the start of training the directional second derivative is actually negative.",
            "A lot of the time, and at the end of training it will be positive and several 100 times larger than the norm of the gradient itself.",
            "And that happens even when the gradient norm doesn't shrink.",
            "So traditionally people have thought of optimization as you move downhill until you approach a critical point and your gradient shrinks and shrinks and shrinks and shrinks, and then you can't move anymore.",
            "But lately it seems much.",
            "This is still only a cartoon that doesn't capture all of the difficulties, but it seems much more accurate to say your gradient stays about the same size or maybe even grows, but the positive eigenvalues grow much much faster until your gradient norm is very small in comparison to them.",
            "So in 2013 Andrew Sachs and some of his collaborators at Stanford wrote a paper analyzing what happens in very deep linear Nets.",
            "So in this framework, you just take several matrices and multiply them together, and that's your neural net.",
            "There's no non linearity at each layer.",
            "This sounds like it's not a very good model at 1st and the reviewers for Nips thought so, but the the reviewers for I clear, are much more prescient and accepted the paper.",
            "The reason that it's interesting is that even though the mapping from inputs to outputs is linear, the mapping from parameters to the cost function is nonlinear, and so this model captures a lot of the properties of what's difficult about optimizing neural Nets.",
            "This paper kicked off the recent interest in saddle Points because they observed that these these models have an extremely large number of saddle points.",
            "And.",
            "They also have a set of global minima that are all connected to each other in this hyperbola shape and no local minima.",
            "19 yeah.",
            "Separated by.",
            "Here.",
            "If we want to go into the history in 1987, Pierre Baldi showed.",
            "That if you, if you make a neural net with one hidden layer and no non linearity then it has only subtle points and no non global minima.",
            "Yeah so so this is this is like a very long.",
            "History of research, but the current wave of enthusiasm began with Andrew Saxs analysis and his collaborators.",
            "Analysis of deep Nets.",
            "This plot that I'm showing you right here.",
            "Is what happens when you train a linear net with one unit at each layer.",
            "And two layers total, just a hidden layer and then an output layer.",
            "And we all you want to train it to do is to map an input value of 1 to an output value of 1.",
            "Using mean squared error, so this is plotting the function.",
            "The square of 1 -- W one times W2 with W one on the X axis and W2 on the Y axis.",
            "The global minimum is a place where W one times W 2 = 1.",
            "And you can see that that's equal to W 2 = 1 / W one, so you get this hyperbola shape of optimality.",
            "In the middle you get negative curvature and very small gradients along the hyperbola you get very strong positive curvature and very small gradients in the middle you get larger gradients and a blend between negative or positive curvature.",
            "If you run stochastic gradient descent, it's curvature.",
            "The trajectory is initially affected by the negative curvature, and then after a while it goes more or less straight to the hyperbola.",
            "The curvature isn't plotted here.",
            "This is plotting the actual cost function value.",
            "Yeah, so so.",
            "One way to see it is that the center is a local maximum.",
            "And so at a local maximum, all the eigenvalues are negative."
        ],
        [
            "After Andrew sexes paper, yahshua and surface students, including young Defendin resident pass canoe.",
            "I had an idea that maybe this same result also applied to neural Nets that actually have nonlinearities, and they also brought in a lot of analysis of.",
            "Theorems from the study of random matrices and functions defined by.",
            "Random random error functions from simple families to analyze, like Gaussian processes.",
            "So their basic idea was that maybe saddle points are a big problem for neural net training as well, and maybe neural net cost functions have these saddle points everywhere.",
            "They then ran several experiments for me.",
            "The most interesting experiment is 1 where they use stochastic gradient descent to move through the parameter space and periodically they use Newton's method to solve for a nearby critical point.",
            "Then they test its eigenvalues.",
            "Using that kind of experiment, they were able to see that there are lots of saddle points all throughout the space that neural net parameters occupy and that their eigenvalue distribution is more or less what the random matrix theory predicts that their eigenvalues become more and more positive as the cost function descents.",
            "And then more recently, Anatra Manske and her collaborators at NYU.",
            "You wrote a paper where they develop the theory, arguing that neural net loss functions should have this characteristic.",
            "So they're coming up with theoretical arguments that justify the conjecture from the Montreal lab.",
            "Well, yeah.",
            "General then.",
            "Their paper looks at specific architecture approximations with the class.",
            "So I guess if I can defend what are Moskovitz today, she would say you studied a very broad class of functions, but he didn't have an argument that neural net functions live within that class.",
            "So she's providing the argument that they're all that functions.",
            "Are lie within that class modulo some simplifying assumptions that she had to use.",
            "Yeah.",
            "Many more selling points then local, right, yeah?",
            "So you can still have an exponential number of local.",
            "What is saying about quality?",
            "I.",
            "So, especially on a tour, masker paper goes into a lot of detail about that.",
            "Her paper says specifically that.",
            "Up high on the cost function, there are many saddle points down low.",
            "There's a thin band of cost function values in which there are many critical points that are mostly local minima and outside of that thin bound it's exponentially unlikely to find a local minimum.",
            "There, there isn't necessarily a clear argument for what the average gap between a local minimum and the global minimum would be.",
            "You also need to remember that we don't actually converge to a critical point of any kind.",
            "In most cases we just we stop where we start overfitting or where the conditioning or the OR the noise gets too bad for us to continue.",
            "So.",
            "The thing that's interesting where these results to me is.",
            "That means there are not local minima that will block us from continuing up high on the cost function.",
            "We will probably get blocked by something other than a critical point when we get download near this.",
            "This thin band where the local minima mostly lie.",
            "The other very important result from.",
            "All of this theoretical work is that.",
            "The relative quality of the minimum.",
            "Let's see so most minima are good.",
            "And that claim is more true if the model is very big.",
            "When I keep saying that local minima are exponentially unlikely to lie outside of the band of low cost function values.",
            "When I say exponential, I'm referring to the size of the network that as the number of parameters increases, the probability of lying outside that bound.",
            "That bound decreases exponentially.",
            "So that's the major implication that you should take home, yeah?",
            "Change that.",
            "Four different mini batches.",
            "Yeah sure.",
            "1.",
            "For one training, sometimes that might change.",
            "So how can these things?",
            "Open, you extend these things to the action general setting where we do the really trying to.",
            "Well so mini batch mini batch SGD is trying to estimate the expected gradient over all the examples.",
            "And if it's if it's estimate is too far off, then you are going to do a bad job.",
            "Of minimizing the overall cost function.",
            "Hold for the agenda setting up.",
            "Well, it's something that's interesting to study, but these problems are usually so difficult that it's.",
            "It's hard to get solid results even for the simplified versions.",
            "Trying to analyze the entire entire complete framework all in one shot.",
            "Maybe we'll get there someday, but it doesn't seem very likely.",
            "Is it natural that those all happen?",
            "No, so that's The thing is.",
            "You may not even really want to get to the global minimum, because that point might correspond to very severe overfitting.",
            "So a lot of the time, if you did manage to get stuck on a local minimum of low value, but that does have a high chance of being the best you could do anyway.",
            "As far as test set error goes.",
            "So there is another implication of all this work, which is that unmodified Newtons method or Newton's method without the correct kind of modification will get stuck on saddle points and that those saddle points are all over the place, so it will get stuck very early on.",
            "That's that's that's interesting.",
            "If you want to improve neural Nets by moving to 2nd order methods.",
            "If you're using something like SGD in momentum, then you probably don't need to worry about the presence of those subtle points, because they probably propel your SSD anyway.",
            "Hi.",
            "Newton's method is designed under an assumption that all the eigenvalues are positive.",
            "It's solving for where there's a critical point in jumping to it.",
            "Yeah, yeah.",
            "Well more than so.",
            "You"
        ],
        [
            "Could imagine designing an algorithm based on.",
            "This principle here where I've said like what infinitesimal stochastic gradient descent should do?",
            "You could imagine making an algorithm where you actually take the matrix exponential of the negative Hessian and try to simulate where SGD would go for a large time difference and that would be a second order method.",
            "That does not solve for a saddle point but uses a different principle to figure out how to go far downhill from 2nd order information.",
            "I've tried that and it gets pretty unstable and it's also it's computationally expensive to do the matrix exponential.",
            "Very idea is though that this is not solving for a saddle point or something for a critical point in jumping to it.",
            "It's it's using the 2nd order information in a qualitatively different way.",
            "The problem with Newton's method isn't so much that it's quadratic, it's not.",
            "It's intentionally going to intentionally going to a critical point without caring what kind of critical point it is.",
            "You could end up with the same problem if you made like third order Newtons method where you have a third order approximation of the cost and you solve for a critical point in that approximation.",
            "Does that?",
            "Does that make sense?",
            "It's OK."
        ],
        [
            "OK, so modern neural net optimization like what what state are we actually in?",
            "Why are we all upset about it?",
            "Is it totally broken?",
            "Does it kind of work?",
            "This is more of a matter of opinion, but I guess what I would say is that right now we're able to optimize most classifiers, autoencoders and recurrent Nets.",
            "Provided that they are mostly based on linear layers, so where you take a matrix, multiply and then you apply a sigmoid or a relative or something like that.",
            "To some extent, it's harder to optimize sigmoids than it is to optimize reluz or Max out or LS teams.",
            "But even sigmoids we can still optimize relatively well most of the time, especially if you use the latest tricks like batch normalization.",
            "For these kinds of less interesting architectures that are mostly based on matrix multiply non linearity matrix multiply non linearity.",
            "We can generally find the right cost function value that we want, yeah?",
            "It's easy to back, propagate through them and there they are relatively sensitive to perturbations of their input.",
            "I'm.",
            "So basically what I'm saying describes everything that people actually use in the deep learning literature, but things things like RBF networks that are based on like take a difference, square it, and then take E to the negative square.",
            "Those suffer from the vanishing gradient problem a lot more than matrix multiplied.",
            "Non linearity matrix multiply non linearity.",
            "So like for example you have networks, squaring operation.",
            "If the value you're squaring is very close to 0.",
            "Then the gradient is going to be extremely close to zero through the squaring operation there.",
            "I mean, it's probably more complicated than that.",
            "Probably there are wide flat regions surrounding.",
            "Points where there's wide regions where all these points in very small gradients and.",
            "It's more than just a numerical issue because.",
            "There is probably extreme flatness in some directions and extreme positive curvature in other directions.",
            "It's it's not something you can just fix by rescaling the gradient.",
            "So even very deep networks of this format can be trained fairly successfully.",
            "David Cestello on at Google has trained networks up to 1000 layers in his submission to I cleared this year.",
            "The main problem is that a lot of the time training can be much slower than we want.",
            "The other big problem is that there are many models we might like to use that we can't optimize, like the model I was just talking about with the Deep RBF network.",
            "One problem I'd like to highlight during this tutorial is that optimization algorithms are usually benchmarked on models that we can already train, and the argument of the paper is just now we can train this model slightly faster or we can reach a slightly lower cost function value.",
            "I think it would be great if people continue to do that to show that you actually are performing comparably to the previous optimization algorithm, but it would be much better is to Additionally show that now you can train a model that.",
            "Previously we were not able to optimize and that's what I think the real selling point of research and optimization should be is that it expands the frontier of the models that we can use."
        ],
        [
            "As for the models that we're already able to train, why is training them so slow?",
            "I guess I've already touched on a lot of the things that can prevent you from converging, and those things can all make you slow to learn as well.",
            "If the gradients are noisy or if the conditioning is poor, or you need to turn the learning rate down a lot in order to not go uphill.",
            "Another reason that optimization might be slow is that even if you're able to compute these local updates very well and make reasonably large steps that go downhill, the local updates might not really correspond to where the global solution is.",
            "People have thought this thought before, but they've usually thought it in the sense of there are local minima and I might go to the wrong one, but it's a little bit more nuanced than that.",
            "You might spend a very long time following a suboptimal path to a region of low cost value, and so to sort of help refocus the thinking, I want to provide this example where there are actually no minima anywhere on the cost function.",
            "But you can spend a very long time getting to a low value.",
            "So over here, the cost function descends to an asymptotes and never quite reaches it, so there's not actually a minimum.",
            "On the left side, the function also descends to an asymptotes, but it's a higher asymptotes.",
            "This is also a 3D function.",
            "There's an access coming out of the board that you can't really see.",
            "The idea is that you have a mountain, and if your initialized on the wrong side of the mountain, you initially descend the wrong side until you come to a relatively flat part of the plane.",
            "At that point, you figure out you need to come around the mountain.",
            "And wind your way coming out of the board.",
            "Back around it.",
            "And over to this direction.",
            "I see this kind of thing happen in visualization sometimes, and I think it contributes a lot to their training time being very long.",
            "So among models that we can train this sort of discrepancy between correctly computed local updates and the global structure can make their training time very slow compared to what we'd like it to be.",
            "I guess that.",
            "Yeah and OK.",
            "So I guess one thing I've sort of avoided discussing here.",
            "In the subspace that I drew, this area is very flat, but recall the random matrix theory results.",
            "In most directions, the eigenvalues are going to be very positive, so I'm already sort of struggling to convey the 3D structure, but there's actually ND structure and the dimension I've drawn is the nicest one.",
            "The one coming out of the board is the 2nd nicest one, but there might be another N -- 2 directions where it looks like you're in an extremely steep Canyon and you're very constrained as to how you can move without going up the sides of the Canyon."
        ],
        [
            "A lot of people have analyzed neural Nets using the linear approximation that Andrew Sachs used.",
            "Like Yahshua said, young Lacon use this approximation.",
            "Did describe neural Nets in his book chapter in the 1990s.",
            "This view of the difficulty of training deep networks helps to build some intuition for what happens when I use different kinds of optimization algorithms and Andrew Sachs regardes some of these problems as being some of the larger problems in what makes it hard to train a neural net.",
            "So I'm just sort of repeating things that he's told me.",
            "New person.",
            "I not using his exact words, so I may be summarizing it imperfectly.",
            "The basic idea is suppose that you have some output Y and you find Y by multiplying together many different weights.",
            "And at the input there's some X variable.",
            "The derivative of the output Y with respect to one of the weights is X times all of the other weights.",
            "When you initialize all the weights are very small and that means that when you make an update, the update is going to have a very small change because it gets multiplied by so many small weights.",
            "So initially you want to make relatively big updates to outweigh the effect of all the way it's being tiny.",
            "But then later on, the weights begin to get big after you've made those big updates.",
            "And in that case, rather than getting exponentially shrunk, your updates get exponentially magnified.",
            "So there's this difficulty of trying to make updates that are large to compensate for the multiplication by many small values early on, and then they need to make updates that are smaller to compensate by the multiplication by many large values later on.",
            "Now, if you have a network with more than one unit per layer, this gets even worse because you'll have some directions that have very small eigenvalues in other directions, have very big eigenvalues, and the kind of update that you need to make is going to depend on exactly which of these eigenvalues you're aligning with.",
            "The reason that this happens is that when you use the gradient, it's telling you how each unit should change independently in order to make a rapid rapid improvement.",
            "But this expected change doesn't take the change in the other units into account.",
            "If you have a second order method then you can say I'm going to update each unit taking the change of the other units into account, but it's only considering pairwise effects.",
            "It's not considering the change of all of them simultaneously.",
            "To entirely resolve this problem, we would need an NTH order method, but for NTH order methods you can't really solve the resulting equations very easily and.",
            "For neural Nets, and can become really quite large.",
            "Well, in the most general sense, and would be the number of parameters for this problem of iterated multiplication.",
            "Then it's the number of layers that determine how many things get multiplied together before they hit you."
        ],
        [
            "So this is sort of the world view that Andrew Sachs who is pitching to me the last time I saw him and.",
            "The idea is you have some loss function that's based on mean squared error, where you've got a target value of, for example one and then you might subtract.",
            "You know, a few multiples of parameter one times, parameter zero.",
            "I'm multiplying by parameter one a few times to emulate parameter sharing here, but the idea is that on the top it's very flat.",
            "These areas right here ignore them, just cropping areas that would stick out of the graph and include your view of the rest of it.",
            "So on the top it's very flat, in the middle it's very steep at the bottom it's very flat again and you've got to handle that transition.",
            "Without accidentaly.",
            "Doing something that causes your loss function to explode, but also while making big enough steps that you can escape the flat region at the top."
        ],
        [
            "Let's see, so I'm.",
            "I'm supposed to finish at 10:30 or 11.",
            "OK, I'd better hurry up then."
        ],
        [
            "So we have some visualizations for."
        ],
        [
            "You usually if you do visualization in two dimensions.",
            "If you just pick two random dimensions and you plot the cost and you plot the trajectory, you see that the trajectory is very yeah.",
            "The colors are the value of the cost function and because.",
            "The band is very narrow.",
            "The normalization of the colors went a little bit crazy, yeah?",
            "The SG trajectory and I'm plotting the value of the cost function in this 2D subspace and you can see that you don't see very much variation at all if you just choose a random subspace and you see that the SGD trajectory is very complicated if you yeah.",
            "I don't know of any results about depth and number of saddle points.",
            "So this this 2D subspace visualization shows you the trajectory is very wild, but the cost function is very flat.",
            "If you would choose random projections to see something more interest."
        ],
        [
            "We need to use a Special 1 dimensional subspace where.",
            "In three dimensions, we've got the cost on the Y axis and then two parameters on the.",
            "At the X and the Z axis I guess.",
            "This is the trajectory that SGD followed, and we're going to look at the 1 dimensional subspace spanning the initial point and the final point.",
            "If we interpolate linearly between this parameter and this parameter, we see that the cost function decreases smoothly along this curve and that allows us to see some things that we couldn't see just from the learning curve.",
            "Overtime, the learning curve overtime includes some bumps just because it accidentally went uphill in some places due to too large of learning rate and from the learning curve we can't tell if those bumps are from accidentally making uphill steps, or from there being bumps on the cost function."
        ],
        [
            "So if we run the linear interpolation experiment on Max out on amnist, we actually see that we monotonically decrease from the initial point.",
            "To the final point, which is right here and then, if you interpolate farther in the same direction, it starts to go up again.",
            "The green curve shows that you can overfit if you do the same thing.",
            "On the right, I'm showing the learning curve overtime and you can see that in terms of time we descend.",
            "This negative curvature region very quickly we get down to the same cost function value right here.",
            "After just the first few epox.",
            "And then we spend several 100 parks traversing from here.",
            "Two here or so.",
            "So most of the time is spent after you escape the big local maximum or saddle point at the origin.",
            "It's very hard to move around in the bottom where the condition number is high."
        ],
        [
            "The same results hold if you use other activation functions.",
            "This isn't just Max out, we get basically the same curve for sigmoid, zanfer, reluz."
        ],
        [
            "We run the same experiment on convolutional networks and convolutional networks.",
            "It's hard to see, but if you zoom in near the top or very start the interpolation, we can actually see that you're slightly on the wrong side of the mountain when you start.",
            "And it's too expensive to run a more detailed visualization and determine exactly how the convolutional Nets got around the mountain.",
            "But we can see that there is a small barrier between where they were initialized and the point that they went to.",
            "In the end.",
            "The cost function actually goes higher.",
            "This barrier is not very tall, so it's possible that noise just shook them through it.",
            "It's also possible that they had to wind around it somehow."
        ],
        [
            "We also looked at a model of sequences.",
            "This is an LS TM train on Penn Treebank and has more or less the same shape as the Max out function where you start at a high cost function value and monotonically decrease to a low one with no real obstacles in the way."
        ],
        [
            "The first thing we actually found, any interesting obstacles was a generative model.",
            "The multi prediction Deep Boltzmann machine and here we found this big plateau on a straight line from the initial point to the solution.",
            "So to see if SGD actually had to interact with this plateau.",
            "We made a new visualization where."
        ],
        [
            "We're going to move from this parameter point to this parameter point and linearly interpolate along them in that direction.",
            "But then on a second axis, at every point we're going to interpolate out.",
            "From this main axis to the point that SGD actually went to, so we're going to have one axis that's just scrolling down the path from the solution, and then we have another access that scrolls side to side at each point to see what lies between the SGD trajectory and the the like kind of imaginary point that was visiting."
        ],
        [
            "When we do this for the MPD BM, we can see that from the initial point we swing out in a wide circle and we actually entirely avoid the plateau with the saddle point in the middle of it.",
            "I think this might be because of the effect of saddle points repelling SGD, but we don't know that for sure.",
            "We also know that we shouldn't take the shape of the trajectory in this plot too seriously.",
            "As you saw in the 2D sub plot, the trajectory wobbles around quite."
        ],
        [
            "A lot.",
            "It turns out that the semicircle shape in the plot where you intentionally start at one point and end at the other end point naturally becomes a semicircle, as the dimensionality of the space increases.",
            "So this is with two dimensions.",
            "This is with 10.",
            "This is with 1000.",
            "This is with 100,000 by the time you get 100,000 dimensions, you essentially deterministically get this semicircle shape."
        ],
        [
            "So it's interesting in this plot is not the path moves in a semicircle, but the path lies outside the plateau."
        ],
        [
            "There are many other 3D plots that we made that have no obstacles and a lot of them have kind of this like.",
            "It looks like the inside of a shell like you find on the beach.",
            "Anne.",
            "It's a gently curving Canyon with a lot of negative curvature near the initial point and then positive curvature after you get down to the bottom of it.",
            "This looks the same for deep factored linear networks.",
            "Rehler Nets trained with adversarial training.",
            "Ananel STM."
        ],
        [
            "If you look at Max out trained with adversarial training, we actually do find little bumps at the bottom and if you zoom in you see that there's a narrow tight Canyon that the cost function is constraining the parameters into.",
            "During gradient descent training.",
            "So it looks like a lot of neural net optimization problems do not really have any interesting obstacles, But some do, and we don't really know which ones do and which ones don't yet.",
            "It's an interesting direction to study."
        ],
        [
            "Overall, we've learned that there exists a linear subspace where the value decreases monotonically for essentially 90% of the models we've studied.",
            "For some problems, there are actually obstacles preventing SGD from traveling down, that is that.",
            "That nice subspace?",
            "And factored linear models seem to capture most of the qualitative aspects of neural net.",
            "Last functions that we capture these visualization."
        ],
        [
            "In conclusion, I would say.",
            "When you are training a neural net and it doesn't work as well as you want, don't automatically blame one specific boogeyman like local minima.",
            "Be aware that there are many boogiemen out there and look for specific evidence for one of them.",
            "In particular, if you think you're stuck in a local minimum, look at your gradient norm.",
            "It should be small if you think that you have bad conditioning, look at the cost function value on successive steps and see if you're going uphill.",
            "And also look at the eigenvalues and see if there's a wide variety of them.",
            "If you think that the problem is noise in your gradient, then actually look at your gradient and see how noisy it is and also look for uphill steps coming from taking bad directions.",
            "And finally, if you think the problem is subtle points, you know, look at the gradient norm and also look for negative eigenvalues.",
            "You should find a small gradient norm and a reasonable negative eigenvalue.",
            "Overall, make visualizations, and I think it would be good if we all feel a challenge to show that the problem we believe in exists.",
            "If you think there's a saddle point or a local minimum, then make a visualization of it and show it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So today I'm here to give you a tutorial on what I'm going to call neural network optimization problems.",
                    "label": 1
                },
                {
                    "sent": "And the talk is mostly going to focus on the structure of the objective functions that we optimize when we train neural Nets, but.",
                    "label": 0
                },
                {
                    "sent": "It's also going to look a little bit at the interactions between the learning algorithms we use.",
                    "label": 0
                },
                {
                    "sent": "And the structure of those loss functions.",
                    "label": 0
                },
                {
                    "sent": "Some aspects of the loss functions don't matter if the optimization algorithms never interact with them.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into a lot of detail about the algorithms themselves.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you just a few small cartoons of a few algorithms that you'll probably already know, and the main reason we bring in those cartoons is so we can have a simple tool that we can use to understand the way that the optimization algorithm interacts with the structure of the neural network loss function.",
                    "label": 0
                },
                {
                    "sent": "This is not really an area where we have definite knowledge of what everything looks like.",
                    "label": 0
                },
                {
                    "sent": "This is an area where we're just starting to make really serious inquiries that actually make progress, so understanding the structure of neural network optimization problems is itself a very open problem.",
                    "label": 0
                },
                {
                    "sent": "Then something that maybe some of you can go home and study, but for those of you who aren't going to study this, hopefully you can gain some insight into why your optimization algorithms might not be.",
                    "label": 0
                },
                {
                    "sent": "Performing as well as you want them to.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately, it turns out that the reasons they might not perform well are complicated, and there are many different ways that they can fail.",
                    "label": 0
                },
                {
                    "sent": "So the main purpose of the talk is for you to realize that there are all these different ways that they can fail and be able to tell to some extent which one is happening in your particular case.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of this tutorial is just information that is so basic and so old that it doesn't necessarily need a specific citation, but about half the tutorial is newer research work, and in that part of the tutorial I'll be presenting essentially ideas from 4 different recent research papers, starting with Andrew's ex and his collaborators paper at Highclere 2014.",
                    "label": 0
                },
                {
                    "sent": "An exact solutions to the nonlinear dynamics.",
                    "label": 1
                },
                {
                    "sent": "Of learning and deep linear networks.",
                    "label": 0
                },
                {
                    "sent": "And then there's been some other follow up work on that here in Montreal, Yondo fan and some collaborators, including Joshua, wrote a paper about.",
                    "label": 0
                },
                {
                    "sent": "Extending some of the theoretical ideas from entry sexist paper to actual empirical experiments with neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And Anna Torv Manske and her collaborators at NYU.",
                    "label": 0
                },
                {
                    "sent": "You developed the theory a lot further and made the theory more specific to neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And then finally, I'll be showing you a lot of visualizations that I made.",
                    "label": 0
                },
                {
                    "sent": "For a paper with some of my Google collaborators for I clear this year.",
                    "label": 0
                },
                {
                    "sent": "Because of the format of the tutorial, it's a talk with slides.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you a lot more of the visualizations from the last paper than the other three.",
                    "label": 0
                },
                {
                    "sent": "The other three papers are important, and if you're interested in this, you should definitely read them.",
                    "label": 0
                },
                {
                    "sent": "Don't take my emphasis on the visualizations as saying that the other three are less important.",
                    "label": 0
                },
                {
                    "sent": "It's just that visualizations are the easiest things to engage with in a talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea of optimization for training and neural network is that we have some cost function J and some set of parameters Theta, and we represent the parameters as a vector.",
                    "label": 0
                },
                {
                    "sent": "We want to find the value of this vector that minimizes J.",
                    "label": 0
                },
                {
                    "sent": "In practice, we almost never find the actual minimal value of J.",
                    "label": 0
                },
                {
                    "sent": "We just want to make J get significantly smaller by the end of training than it was at the start of training, and there are many different ways you can do this.",
                    "label": 0
                },
                {
                    "sent": "In principle for a lot of people in other fields, optimization means exhaustive search where you just scroll through a space looking for low values of the cost function and a lot of the time discrete optimization relies on techniques like that, and they come up with.",
                    "label": 0
                },
                {
                    "sent": "Tricks to prune the search space.",
                    "label": 0
                },
                {
                    "sent": "We're lucky that neural Nets are usually parameter isable in terms of real numbers where we can compute gradients.",
                    "label": 0
                },
                {
                    "sent": "And that allows us to use more sophisticated search procedures than just random search.",
                    "label": 0
                },
                {
                    "sent": "And random search appears for neural Nets occasionally as genetic algorithms, but it's usually not nearly as efficient as when you can take advantage of.",
                    "label": 1
                },
                {
                    "sent": "So the extra knowledge you have about the cost function.",
                    "label": 0
                },
                {
                    "sent": "In some cases we can actually solve analytically for optimal variables, either all of them at once or for a subset of them given another subset.",
                    "label": 0
                },
                {
                    "sent": "It's also possible to solve these optimization problems using model based search, where for example you have a Gaussian process that predicts the value of the cost function for different parameter points, and you solve for the minimum set.",
                    "label": 0
                },
                {
                    "sent": "But most of these methods usually are not feasible or don't work as well when you get to the scale of problem that we use for neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So neural Nets almost always use gradient based search.",
                    "label": 0
                },
                {
                    "sent": "Which I'll describe more in a few slides.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to do gradient based search you need to have derivatives and 2nd derivatives to actually implement the algorithm you need the derivatives and to understand whether the algorithm is going to perform well at all, you need to have some idea of what the second derivatives look like.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that the gradient, the vector of all the derivatives of the cost function with respect to the parameters, tells you the direction where the cost function decreases the fastest in an infinitesimal window.",
                    "label": 0
                },
                {
                    "sent": "So if you want to make a very small step, the gradient tells you the fastest direction that you can move downhill.",
                    "label": 0
                },
                {
                    "sent": "The second derivatives tell you how quickly those first derivatives are going to change.",
                    "label": 1
                },
                {
                    "sent": "And that is important because it tells you the size of this step that you can make.",
                    "label": 0
                },
                {
                    "sent": "If you have a derivative that's negative and you have a second derivative, that's positive, it means that your derivative is going to start increasing as you move along that axis.",
                    "label": 0
                },
                {
                    "sent": "And after it is increased too much, eventually it will be 0 and you're no longer moving downhill in that direction.",
                    "label": 0
                },
                {
                    "sent": "I've plotted 3 little plots here where you can see what happens when you have negative curvature, no curvature and positive curvature.",
                    "label": 0
                },
                {
                    "sent": "Negative curvature just means that the second derivative is negative.",
                    "label": 0
                },
                {
                    "sent": "And it means that your derivative is going to decrease as you move to the right.",
                    "label": 0
                },
                {
                    "sent": "So here if we were doing gradient descent in a single dimension.",
                    "label": 0
                },
                {
                    "sent": "We would take the derivative and see that the derivative is negative, so we're going to move to the right.",
                    "label": 0
                },
                {
                    "sent": "And then when we want to figure out how well this derivative predicts what's going to happen in the future, we can look to the second derivative and see that the cost function is going to accelerate.",
                    "label": 0
                },
                {
                    "sent": "It's decent as we move to the right.",
                    "label": 0
                },
                {
                    "sent": "So in this case, where there's negative curvature, gradient descent actually is pretty nice, as long as this first and second derivative remain a good approximation for the function, we can take really big steps, and we're actually going to get more improvement than you would expect just from the derivative alone.",
                    "label": 0
                },
                {
                    "sent": "Another nice case for gradient descent is the situation where there's no curvature where the second derivative is zero.",
                    "label": 0
                },
                {
                    "sent": "In that case, the gradient predicts exactly what the value of the cost function is going to be, regardless of this step size.",
                    "label": 0
                },
                {
                    "sent": "And so to the extent that the 1st and 2nd derivative continued to give you a good approximation of the cost function, any stepsize is safe.",
                    "label": 0
                },
                {
                    "sent": "The third case is when you have positive curvature.",
                    "label": 0
                },
                {
                    "sent": "In this case, a second derivative is positive and it means that the derivative will increase as you move to the right.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that means that eventually the derivative will be 0 and you won't be able to make anymore progress.",
                    "label": 0
                },
                {
                    "sent": "You can see that here where the cost function levels out.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the relationship between the derivative and the second derivative determines the largest stepsize that's safe.",
                    "label": 0
                },
                {
                    "sent": "I'll go into more detail about what step sizes are safe in a second.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those pictures I just showed you were all in one dimension.",
                    "label": 0
                },
                {
                    "sent": "When we're training neural networks, we usually have millions or billions of dimensions and because of that we need to work with.",
                    "label": 0
                },
                {
                    "sent": "An entire matrix of 2nd derivatives, called the Hessian.",
                    "label": 0
                },
                {
                    "sent": "So the Hessian is a matrix where element I, J gives you the derivative of gradient I with respect to parameter J.",
                    "label": 0
                },
                {
                    "sent": "It gives us all of the different second derivatives of all of the first derivatives that we computed for the gradient descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time when we talk about the Hessian, it's not very informative to just look at a huge end by end grid of numbers.",
                    "label": 0
                },
                {
                    "sent": "It's much more intuitive to look at its eigenvectors and eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Is there anybody here who doesn't know what an eigenvector and eigenvalue is?",
                    "label": 0
                },
                {
                    "sent": "If anybody doesn't know they're too shy to raise their hand so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, who knows what?",
                    "label": 0
                },
                {
                    "sent": "Who knows what?",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues and eigenvectors are.",
                    "label": 0
                },
                {
                    "sent": "It looks like pretty much everybody.",
                    "label": 0
                },
                {
                    "sent": "If there was anybody who didn't raise their hand an eigenvector when multiplied by a matrix.",
                    "label": 0
                },
                {
                    "sent": "Only gets scaled, so if you have a vector V and you multiply it by H. If V is an eigenvector, you'll get Lambda times V back out of it.",
                    "label": 0
                },
                {
                    "sent": "The Hessian matrix of neural net optimization functions is almost always symmetric.",
                    "label": 0
                },
                {
                    "sent": "Has had matrices in general are symmetric if both the function and its derivatives, and I forget maybe its second derivatives as well or are continuous.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the exact condition is, but for neural networks, the condition that guarantees that the Hessian will be symmetric is almost always satisfied.",
                    "label": 0
                },
                {
                    "sent": "For things like rectifier networks, the Hessian might not be symmetric when you're right on the boundary where a rectifier goes from zero to linear, but most places on most neural networks, the Hessian will always be symmetric.",
                    "label": 0
                },
                {
                    "sent": "Because the Hessian is symmetric, that means that it has a very nice decomposition into eigenvectors and eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Every real symmetric matrix can be turned into a matrix containing orthogonal eigenvectors and a diagonal matrix containing their eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So we can decompose the Hessian matrix into a basis of eigenvectors Q.",
                    "label": 0
                },
                {
                    "sent": "A diagonal matrix of eigenvalues associated with them, called Lambda.",
                    "label": 0
                },
                {
                    "sent": "And then if we multiply that by Q transpose, we get the Hessian back.",
                    "label": 0
                },
                {
                    "sent": "In order to get a directional second derivative to actually take the second derivative along one particular line in space, we can represent that line with a vector D. This is a unit vector telling us the direction in which we want to take the second derivative.",
                    "label": 0
                },
                {
                    "sent": "And if we multiply D transpose HD.",
                    "label": 0
                },
                {
                    "sent": "That gives us the second derivative in the direction that we just chose.",
                    "label": 0
                },
                {
                    "sent": "If we understand the Hessian matrix in terms of its eigenvalues and eigenvectors, that gives us an intuitive way to understand what the directional second derivatives can look like.",
                    "label": 0
                },
                {
                    "sent": "Specifically, you add up the cosine squared of the angle between D and each eigenvector multiplied by the corresponding eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "This might be a lot to digest right now.",
                    "label": 0
                },
                {
                    "sent": "We keep in mind that cosine squared is always going to be between zero and one cosine is between negative one and positive one.",
                    "label": 0
                },
                {
                    "sent": "Cosine squared is going to make that all the positives, it always between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So you can think of each directional second derivative as ticking a combination using coefficients between zero and one of the eigenvalues of the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "That means the eigen values tell us what the.",
                    "label": 0
                },
                {
                    "sent": "The total possible range of the directional second derivatives will be the yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm saying we're defining D to be a unit vector indicating a direction, yeah?",
                    "label": 0
                },
                {
                    "sent": "And the the directional second derivative is based on.",
                    "label": 0
                },
                {
                    "sent": "Looking in a specific direction and saying what's the second derivative.",
                    "label": 0
                },
                {
                    "sent": "That way it's it's not really defined with respect to the magnitude of any vector.",
                    "label": 0
                },
                {
                    "sent": "So the largest second derivative that you can find in any direction is given by the largest eigenvalue of the Hessian matrix and the smallest the most negative.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalue that you can find gives you the smallest directional second derivative that could happen by choosing any vector.",
                    "label": 0
                },
                {
                    "sent": "In this plot over here, I'm plotting the directional second derivative as the radius, and I'm plotting the angle between two different eigenvectors as Theta.",
                    "label": 0
                },
                {
                    "sent": "So over here we've got an eigenvector with a negative eigenvalue, so Lambda times the eigenvector puts you over here.",
                    "label": 0
                },
                {
                    "sent": "Over here we've got an eigenvector with a positive eigenvalue, so Lambda times the eigenvector puts you here and as we swing around the circle.",
                    "label": 0
                },
                {
                    "sent": "If we're over here, then the direction of 2nd derivatives here.",
                    "label": 0
                },
                {
                    "sent": "If we're over here, then the directional second derivatives.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You can kind of think of the different directions as tracing out this multi dimensional ellipse where the different eigenvalues determine the scale of the ellipse and also whether its axes are flipped.",
                    "label": 0
                },
                {
                    "sent": "So for a lot of the rest of the talk I'm going to be telling you about the eigenvalues of the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "Just remember that these eigenvalues are the different possible directional second derivatives that can be blended together.",
                    "label": 0
                },
                {
                    "sent": "When you look for directional second derivatives in specific directions that are not eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One very important tool that we use a lot of the time that makes use of the directional second derivative is the Taylor series approximation to a function.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming a lot of you have probably seen a Taylor series in one dimension before.",
                    "label": 0
                },
                {
                    "sent": "The idea is you can use the derivatives and the second derivatives of a function to approximate its behavior near some reference point.",
                    "label": 0
                },
                {
                    "sent": "So if you begin by evaluating the function at some reference point X0.",
                    "label": 0
                },
                {
                    "sent": "Then when you want to look at a new value of X, you can take the difference between the new X and X0, and I have a mistake here.",
                    "label": 0
                },
                {
                    "sent": "This should say X0 inside both of these derivatives.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "The idea is you can multiply the derivative at the initial point by the difference between the reference point in your current location and that will give you a linear prediction of what the cost function will be at the new point.",
                    "label": 0
                },
                {
                    "sent": "But that linear prediction is going to become less and less accurate the farther the text gets from X0.",
                    "label": 0
                },
                {
                    "sent": "So to correct for it, we add another term.",
                    "label": 0
                },
                {
                    "sent": "Based on the second derivative, where we add 1/2 times the difference squared times the second derivative.",
                    "label": 0
                },
                {
                    "sent": "You can go on adding more and more terms like this using third derivatives and so on, but typically in the context of neural net optimization, we're only going to look at the first 2 derivatives.",
                    "label": 0
                },
                {
                    "sent": "If we extend this to multiple dimensions, we're still doing more or less the same thing every time we want to evaluate the derivative at a point, or evaluate the approximate cost function at a point Theta.",
                    "label": 0
                },
                {
                    "sent": "We essentially construct a 1 dimensional Taylor series approximation in the Ray looking from Theta zero to Theta.",
                    "label": 1
                },
                {
                    "sent": "So we evaluate J of Theta zero.",
                    "label": 0
                },
                {
                    "sent": "That's our baseline cost.",
                    "label": 0
                },
                {
                    "sent": "Then we look at the vector Theta minus Theta Zero, which gives us the offset from the baseline point to the new point.",
                    "label": 0
                },
                {
                    "sent": "And we take the dot product of that and the gradient that gives us a linear prediction of the change in the cost function value due to the gradient itself.",
                    "label": 0
                },
                {
                    "sent": "But then we can correct that further by using the directional second derivative scaled by the magnitude of the change.",
                    "label": 0
                },
                {
                    "sent": "So we add 1/2 Theta minus Theta, zero transpose H times Theta minus Theta 0.",
                    "label": 0
                },
                {
                    "sent": "This Theta minus Theta zero is standing in for the D on the previous slide and it's no longer a unit vector, so the magnitude of the of the offset actually affects the magnitude of the correction that we get from looking at the 2nd order terms.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing we can use with the Taylor series approximation is to figure out how much we can expect a gradient descent step to improve.",
                    "label": 1
                },
                {
                    "sent": "So we can say right now we're at Theta.",
                    "label": 0
                },
                {
                    "sent": "We want to go to Theta minus epsilon times G, and we want to predict how much better the cost function is going to be.",
                    "label": 0
                },
                {
                    "sent": "To evaluate that point using a second order Taylor series prediction, we start off by saying, well, what is J of Theta minus epsilon times G. We can approximate it as being J of Theta.",
                    "label": 0
                },
                {
                    "sent": "Minus epsilon times G that's are offset.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by the gradient that's from the Taylor series formula.",
                    "label": 0
                },
                {
                    "sent": "Plus 1/2 epsilon G transpose H epsilon G and we simplify that the epsilon squared send up on the left.",
                    "label": 0
                },
                {
                    "sent": "So this gives us an idea of what the cost function looks like for different step sizes epsilon that we use to descend the cost function.",
                    "label": 0
                },
                {
                    "sent": "In the worst case.",
                    "label": 0
                },
                {
                    "sent": "G will be perfectly aligned with the eigenvector of H that has the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "That's the worst case because it makes this second order correction term rise up the highest and counteract the improvement from the gradient the most.",
                    "label": 0
                },
                {
                    "sent": "You'll notice that this middle term here the negative epsilon G transpose G. That is always going to be non negative in the worst case the gradient has zero norm, so you just can't improve by stepping downhill.",
                    "label": 0
                },
                {
                    "sent": "Or rather, there's not a downhill direction to step.",
                    "label": 0
                },
                {
                    "sent": "This term here can be either negative or positive depending on the directional second derivative.",
                    "label": 0
                },
                {
                    "sent": "So the worst case is when that directional second derivative is either positive or when it's not as negative as it could be.",
                    "label": 0
                },
                {
                    "sent": "It's not accelerating you as much as you want.",
                    "label": 0
                },
                {
                    "sent": "When this directional second derivative is negative, the Taylor series actually predicts that you can keep on stepping farther out forever, and you'll always get an improvement.",
                    "label": 0
                },
                {
                    "sent": "In fact, you'll improve faster the further out you step.",
                    "label": 0
                },
                {
                    "sent": "Usually you can't trust that kind of prediction.",
                    "label": 0
                },
                {
                    "sent": "Usually the Taylor series will become inaccurate after you've moved very far, and terms involving the third derivatives will start to dominate.",
                    "label": 0
                },
                {
                    "sent": "Or in the case of things like rectifier Nets, there's very sharp discontinuity's that make the Taylor series approximation become inaccurate much earlier.",
                    "label": 0
                },
                {
                    "sent": "Where this view is useful is when the second derivative is positive, and in that case we can use it to compute the optimal stepsize optimal, at least from the point of view of getting the most improvement on this particular step.",
                    "label": 0
                },
                {
                    "sent": "So just by looking at the quadratic function that we've got here, we can solve that.",
                    "label": 0
                },
                {
                    "sent": "The optimal step size is the norm of the gradient squared divided by the 2nd derivative times the norm of the gradient, or G, transpose, G. /, G, transpose HG.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind that G transpose HG can at most be the norm of the gradient squared times the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvalues actually tell you what the largest safe step size is.",
                    "label": 0
                },
                {
                    "sent": "Overtime the eigenvalues tend to become more positive after you've trained longer and a lot of the time the reason that you are no longer able to train is not so much that the gradient has become small on an objective scale, but because the gradient has remained about the same size and the eigenvalues have become very much larger and more positive.",
                    "label": 0
                },
                {
                    "sent": "That means that the largest safe step size you can make is very small.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several different kinds of critical points that occur in neural network objective functions.",
                    "label": 0
                },
                {
                    "sent": "A critical point is any point where the gradient is 0, so that means that the cost function is locally flat at that point.",
                    "label": 0
                },
                {
                    "sent": "The Hessian are useful again for telling what kind of critical point we've reached.",
                    "label": 0
                },
                {
                    "sent": "If all of the eigenvalues are positive, then the critical point at the bottom of this graph.",
                    "label": 0
                },
                {
                    "sent": "Is a local minimum and we like those.",
                    "label": 0
                },
                {
                    "sent": "We want to find a minimal point and stop there so it's very nice if you can find a place that has zero gradient and all of the eigenvalues of the Hessian being positive.",
                    "label": 0
                },
                {
                    "sent": "There is also a critical points that are local Maxima, so at a local maximum all the eigenvalues of the Hessian are negative and that means that any direction you step the second derivative is going to cause the cost function to go down even though right where you are in the very top, it's flat.",
                    "label": 0
                },
                {
                    "sent": "These points don't really come up in practice very much when you're using a cost function.",
                    "label": 0
                },
                {
                    "sent": "When you're minimizing a cost function.",
                    "label": 0
                },
                {
                    "sent": "The reason is that they're very unstable if you initialize right next to one, gradient descent will easily move away from it and go downhill.",
                    "label": 0
                },
                {
                    "sent": "You have to initialize right on it in order to have a problem, and the way that we optimize neural Nets is usually so noisy that even if you are at a local maximum of the cost function, you might not be at the local maximum of the mini batch that you use for the first step of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And even if you're at the local maximum of that first mini batch, the second mini batch is likely to have a different one, so you'll get shaken off of these very easily.",
                    "label": 0
                },
                {
                    "sent": "A third kind of point is a saddle point.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting critical point that is a local maximum.",
                    "label": 0
                },
                {
                    "sent": "If you look at some axes at a local minimum.",
                    "label": 0
                },
                {
                    "sent": "If you look at some other axes.",
                    "label": 0
                },
                {
                    "sent": "So here you can see across this axis, it looks like you're at a local minimum.",
                    "label": 0
                },
                {
                    "sent": "If you're at this axis, it looks like you're at a local maximum.",
                    "label": 0
                },
                {
                    "sent": "These critical points have some positive and some negative eigenvalues of the Hessian and over the past few years several people have become much more interested in their role in optimizing neural networks.",
                    "label": 1
                },
                {
                    "sent": "Previously they were more or less ignored.",
                    "label": 0
                },
                {
                    "sent": "They are usually considered to be fairly unstable points because if you initialize near one, you get shaken off of it.",
                    "label": 0
                },
                {
                    "sent": "But not every optimization algorithm has that property.",
                    "label": 0
                },
                {
                    "sent": "Also, sometimes they are surrounded by wide flat regions where even though the critical point itself is unstable, the wide flat region around it might be difficult to escape.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One method that has a strong interaction with subtle points is Newton's method.",
                    "label": 0
                },
                {
                    "sent": "The idea behind Newton's method.",
                    "label": 0
                },
                {
                    "sent": "Is to form a local quadratic approximation and then minimize that quadratic approximation.",
                    "label": 0
                },
                {
                    "sent": "That's the way that it's usually presented.",
                    "label": 0
                },
                {
                    "sent": "An easier way to see the problem that Newton's method heads with saddle points is to use this alternative derivation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to provide here.",
                    "label": 0
                },
                {
                    "sent": "So first we begin by assuming that the minimum eigenvalue is greater than zero.",
                    "label": 0
                },
                {
                    "sent": "That means that every critical point we could find is a maximum.",
                    "label": 0
                },
                {
                    "sent": "It's not a saddle point, it's not a minimum.",
                    "label": 0
                },
                {
                    "sent": "Next we want to solve for where the gradient at point Theta is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "We're going to solve that equation for Theta and that will give us a critical point.",
                    "label": 0
                },
                {
                    "sent": "And by our assumption that critical point will be a minimum.",
                    "label": 0
                },
                {
                    "sent": "So we just succeeded in finding a minimum, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, that's a mistake.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so all the critical points are minimum.",
                    "label": 0
                },
                {
                    "sent": "If it's too hard to solve this equation here.",
                    "label": 0
                },
                {
                    "sent": "Then we need to make a simplification when when G of Theta is actually back, prop innarelli net then we can't really solve this equation analytically and you can solve it numerically, but that's just as hard as training the neural net in the first place.",
                    "label": 0
                },
                {
                    "sent": "What we do to make a an analytical solvable version of this equation is we approximate the gradient with a first order Taylor series approximation.",
                    "label": 0
                },
                {
                    "sent": "So before we were talking about Taylor series approximations of a single scalar.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to make a Taylor series approximation of a vector.",
                    "label": 0
                },
                {
                    "sent": "The idea is we use the gradient plus the Hessian times the difference between the reference point and Theta.",
                    "label": 0
                },
                {
                    "sent": "And that will predict the gradient at the new Theta point.",
                    "label": 0
                },
                {
                    "sent": "Now we want to solve for where that predicted gradient is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "The solution tells us that the new Theta that we want to jump to is equal to the baseline minus the Hessian inverse times the gradient.",
                    "label": 0
                },
                {
                    "sent": "And this works very well if you have convex problems where all the where all the eigenvalues of the Hessian are positive.",
                    "label": 0
                },
                {
                    "sent": "You can see that if we get rid of this first assumption here, if we allow the eigenvalues to be both positive and negative, then this could go to any kind of critical point it could go to a local maximum.",
                    "label": 0
                },
                {
                    "sent": "It could go to a saddle point.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, people try to mitigate this by using what's called damping they add.",
                    "label": 0
                },
                {
                    "sent": "Essentially they make a fake shift of the eigen spectrum to make it more positive and then that shrinks the step size.",
                    "label": 0
                },
                {
                    "sent": "It can be difficult to know what the smallest eigenvalue is there, so it can be difficult to be sure that you're dumping enough to avoid this problem without damping so much that you caused this step size to shrink beyond what is necessary.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so a lot of people have a very.",
                    "label": 0
                },
                {
                    "sent": "Wrong Cartoon picture of how optimization ends in their head.",
                    "label": 0
                },
                {
                    "sent": "People think that SGD usually moves downhill.",
                    "label": 1
                },
                {
                    "sent": "And then eventually SGD will encounter a critical point.",
                    "label": 0
                },
                {
                    "sent": "That critical point is usually a local minimum, and there are other minima that have much lower value and we failed because we got stuck on this bad local minimum and what you really wanted to do is go to some other global minimum.",
                    "label": 1
                },
                {
                    "sent": "But the thing that's wrong about this cartoon is that you think that you've definitely reached a critical point and you've definitely converged in practice.",
                    "label": 0
                },
                {
                    "sent": "That is usually not how a neural net training proceeds and one of the biggest reasons is overfitting.",
                    "label": 0
                },
                {
                    "sent": "Usually, we don't actually determine convergence.",
                    "label": 0
                },
                {
                    "sent": "By testing the gradient to see if we've reached a critical point, usually we determine the end of the training algorithm by monitoring the validation set error and when the validation set error starts to go up, we terminate the training.",
                    "label": 0
                },
                {
                    "sent": "There's no reason to think that you're actually on a critical point when that happens, and usually you can see that the validation set error is going up very fast, so you are actually definitely not on a critical point.",
                    "label": 0
                },
                {
                    "sent": "You're somewhere moving rapidly through parameter space.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's one of the convergence myths that have been widespread that we want to bust.",
                    "label": 0
                },
                {
                    "sent": "But in a few slides and make sure that there are even more reasons besides overfitting.",
                    "label": 0
                },
                {
                    "sent": "While you can't usually expect that you're converging to a critical point, there's another new myth about SGD convergence.",
                    "label": 0
                },
                {
                    "sent": "That's kind of become popular recently.",
                    "label": 0
                },
                {
                    "sent": "The new myth is a SGD usually moves downhill until it encounters a saddle point, and then it becomes stuck on that saddle point and cannot escape.",
                    "label": 1
                },
                {
                    "sent": "Newtons method definitely does this.",
                    "label": 0
                },
                {
                    "sent": "There's not much reason to think that SGD does this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One reason that both of these ideas are myths is that some functions lack critical points of any kind.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In many cases the function can have critical points, but you avoid them altogether.",
                    "label": 0
                },
                {
                    "sent": "This is actually extremely common with unregularized classifiers, So what I'm plotting here is a 1 dimensional cross section of the negative log likelihood for a softmax classifier.",
                    "label": 0
                },
                {
                    "sent": "The idea is that once you've found a configuration of the weights where you can classify the training set correctly, you can always become more and more confident about your classifications and marginally reduce the training set.",
                    "label": 0
                },
                {
                    "sent": "Likely the you can marginally increase your confidence and.",
                    "label": 0
                },
                {
                    "sent": "Is the negative log likelihood on the training set?",
                    "label": 0
                },
                {
                    "sent": "So stochastic gradient descent training something like logistic regression with a separable class distribution will never converge.",
                    "label": 0
                },
                {
                    "sent": "And that's not anything exotic and related to neural Nets, it's just a relatively universal property.",
                    "label": 0
                },
                {
                    "sent": "You do see this happen in neural Nets if they have enough capacity to fit the training set, but we often just use such large training sets that they don't manage to fit them.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also see in practice, if we actually go ahead and instrument the norm of the gradient, that it's quite common for SGD to not encounter any kind of critical point or flat region.",
                    "label": 0
                },
                {
                    "sent": "I am not saying that this happens in every case, but the cartoon view that you can generally expect SGD to go to a flat place and then stay there does not really explain a lot of very common cases.",
                    "label": 0
                },
                {
                    "sent": "So this neural network here was state of the art on cifar 10 a few years ago, and if you look at its gradient, you can see if we take a running average of the gradient, it increases smoothly overtime and actually never gets smaller than where it started.",
                    "label": 0
                },
                {
                    "sent": "It's because as you train the neural net, its weights get bigger and so it's possible for it to have a larger gradient.",
                    "label": 0
                },
                {
                    "sent": "And in the case where it makes a mistake.",
                    "label": 0
                },
                {
                    "sent": "In addition to this running average, we also took a snapshot once after every pass through the training set.",
                    "label": 0
                },
                {
                    "sent": "That's the green curve that zigzags a lot, so you can see that there is some variance in the norm of the gradient, but overall it's on a very smooth increasing trend.",
                    "label": 0
                },
                {
                    "sent": "You can also see that the optimization is proceeding correctly.",
                    "label": 0
                },
                {
                    "sent": "The validation set objective is going down after a while it starts to go up due to overfitting the classification rate.",
                    "label": 0
                },
                {
                    "sent": "The misclassification rate continued to go down for a long time, while the objective on the validation set was going up.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way that you can see how gradient descent behaves near a saddle point is to actually solve the differential equations that predict what it will do.",
                    "label": 0
                },
                {
                    "sent": "This is assuming that the 2nd order Taylor series approximation is a valid way of modeling the cost function.",
                    "label": 0
                },
                {
                    "sent": "Near where you initialize the process, actually it's a little bit simpler than that.",
                    "label": 0
                },
                {
                    "sent": "It's assuming that the 1st order Taylor series approximation of the gradient is reliable.",
                    "label": 0
                },
                {
                    "sent": "So the idea is, we say that we're going to make infinitesimal step sizes, so we're going to look at how Theta evolves overtime.",
                    "label": 0
                },
                {
                    "sent": "So we're going to write down an equation for D /, D T of Theta at time T. At Times Zero, the data is going to just travel directly downhill.",
                    "label": 0
                },
                {
                    "sent": "This is running just normal gradient descent, no momentum or anything.",
                    "label": 1
                },
                {
                    "sent": "So to show that initially we're going directly downhill, we say that we're going to move in direction negative G. And then after we've moved.",
                    "label": 0
                },
                {
                    "sent": "Our gradient would actually be G anymore.",
                    "label": 0
                },
                {
                    "sent": "Instead, it's going to be predicted by a first order Taylor series prediction.",
                    "label": 0
                },
                {
                    "sent": "Of the Hessian, multiplied by the difference from where we began the process.",
                    "label": 0
                },
                {
                    "sent": "So this is going to when you add these up.",
                    "label": 0
                },
                {
                    "sent": "This gives you the predicted gradient at time T, and we're saying we're just always going to send that.",
                    "label": 0
                },
                {
                    "sent": "If you apply this to a quadratic function, then these equations are exact.",
                    "label": 0
                },
                {
                    "sent": "If you apply these to a neural net, then they are only approximate, but they do tell us what happens on quadratic functions that have saddle points.",
                    "label": 0
                },
                {
                    "sent": "If we solve the differential equation, we end up with Theta, T is equal to the original Theta minus Q, Lambda prime of T * Q transpose G. So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "Q is the eigenvectors of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Lambda Prime is a modification of the original eigenvalues of the Hessian, and it's a function of time.",
                    "label": 0
                },
                {
                    "sent": "The way that this works is each eigenvalue gets rescaled independently.",
                    "label": 0
                },
                {
                    "sent": "According to this squashing function here.",
                    "label": 0
                },
                {
                    "sent": "Lambda is the original eigenvalue of the Hessian and Lambda prime of T is the new value that we use to predict where.",
                    "label": 0
                },
                {
                    "sent": "Where the parameters will be at time T. And the function that ends up determining what the new eigenvalues look like is 1 -- Y to the negative Lambda T divided by Lambda.",
                    "label": 0
                },
                {
                    "sent": "That's pretty hard to evaluate in your head, so here's a plot of it on the X axis.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you the original Lambda on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you the new Lambda.",
                    "label": 0
                },
                {
                    "sent": "When the original Lambda is very large.",
                    "label": 0
                },
                {
                    "sent": "The squashing function goes down close to 0.",
                    "label": 0
                },
                {
                    "sent": "This is plotted for T = 1 as indicated at the top of the plot.",
                    "label": 0
                },
                {
                    "sent": "But it has the same basic shape, just less extreme for other values and more extreme for larger values.",
                    "label": 0
                },
                {
                    "sent": "But the basic the basic property I'm describing holds for any value of T except 0.",
                    "label": 0
                },
                {
                    "sent": "And negative T is not allowed.",
                    "label": 0
                },
                {
                    "sent": "If you're right at T = 0, then there's no squashing.",
                    "label": 0
                },
                {
                    "sent": "And if you go to your bed and equals 0, there's there's no squashing.",
                    "label": 0
                },
                {
                    "sent": "If you go to negative Lambda, then the contribution of the gradient in that direction actually gets exponentially amplified.",
                    "label": 0
                },
                {
                    "sent": "So when you, yeah.",
                    "label": 0
                },
                {
                    "sent": "How much for moving?",
                    "label": 0
                },
                {
                    "sent": "What I'm doing is I'm analyzing what happens if you initialize SGD nearest saddle point so some people would say SGD doesn't look at the 2nd order information and therefore it's not able to figure out how to step away from the saddle point you have.",
                    "label": 0
                },
                {
                    "sent": "Negative curvature in One Direction, positive curvature in the orthogonal direction.",
                    "label": 0
                },
                {
                    "sent": "Some people would say you are not able to see that there is negative curvature this way, so you don't go there.",
                    "label": 0
                },
                {
                    "sent": "This differential equation is analyzing what actually happens, and it's saying if you make the step size small enough that the differential equation is valid.",
                    "label": 0
                },
                {
                    "sent": "Then we know for sure that unless your initial gradient is completely orthogonal to any direction of negative curvature, you will exponentially amplify the component of the gradient that aligns with the negative eigenvalues, and you will run away from the saddle point, so this isn't this isn't making a recommendation about what optimization algorithm we should use, it's it's saying what actually should happen.",
                    "label": 0
                },
                {
                    "sent": "What is the theoretical prediction for what will happen in general for stochastic gradient descent?",
                    "label": 0
                },
                {
                    "sent": "The reason I'm using a differential equation rather than like running an experiment is you can always look at it.",
                    "label": 0
                },
                {
                    "sent": "Experiment and say well, you just did some trick.",
                    "label": 0
                },
                {
                    "sent": "You had fancy preprocessing or or you picked the parameters in the step size to make it work.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying here is there's an actual principle that the differential equation says in less you really go out of your way to make sure that it's impossible to see that any negative curvature is there and you and you put the initial point exactly on the saddle point.",
                    "label": 0
                },
                {
                    "sent": "You will run away from it and.",
                    "label": 0
                },
                {
                    "sent": "A more realistic way of thinking about it is.",
                    "label": 0
                },
                {
                    "sent": "When you're starting farther away from a saddle point, you're going to get repelled from it.",
                    "label": 0
                },
                {
                    "sent": "If there's any negative curvature at all, they're not attractive points there.",
                    "label": 0
                },
                {
                    "sent": "As long as there's negative curvature, they're repulsive.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important problem more generally, that does actually affect gradient descent, is just the problem of poor conditioning.",
                    "label": 0
                },
                {
                    "sent": "This refers to the Hessian having.",
                    "label": 0
                },
                {
                    "sent": "Different ranges of eigenvalues, either very small ones or very large ones.",
                    "label": 0
                },
                {
                    "sent": "What happens is you end up with a long quadratic truf.",
                    "label": 0
                },
                {
                    "sent": "Where it's much steeper when you move back and forth this way.",
                    "label": 0
                },
                {
                    "sent": "Then when you move this way in the long run you want to move down the truck to the very bottom of the truck.",
                    "label": 0
                },
                {
                    "sent": "But stochastic gradient descent initialized on the walls of the Canyon is mostly going to see the the Canyon walls as being the main source of gradient, so it's going to move more or less orthogonally across the Canyon, and make very little progress along this slow axis.",
                    "label": 0
                },
                {
                    "sent": "I it essentially bounces back and forth several times.",
                    "label": 0
                },
                {
                    "sent": "You can fix this to some extent by making the step size small, but then you require more steps to reach convergence, so either way this makes you progress very slowly, either due to oscillations or due to small step size.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recall from earlier I showed you that the optimal step size if our goal is just to find the smallest cost function from a single step.",
                    "label": 0
                },
                {
                    "sent": "Is G transpose G / G, transpose HG.",
                    "label": 0
                },
                {
                    "sent": "And recall that this G transpose HG term here is sort of blending the eigenvalues together based on which eigenvector it aligns with most closely.",
                    "label": 0
                },
                {
                    "sent": "Which eigenvector of HG aligns with most closely.",
                    "label": 0
                },
                {
                    "sent": "When you have very different eigenvalues in your Hessian matrix, G can select very different values depending on which direction it's facing in.",
                    "label": 0
                },
                {
                    "sent": "If your Hessian matrix has one eigenvalue, that's 10 to the minus three, and another one that ended the positive 3, then you can end up with your step size getting divided by 10 to the three and then multiplied by 2:50 on the next step.",
                    "label": 0
                },
                {
                    "sent": "If G changes direction to visit both of those eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time we choose our step sizes ahead of time and just run them with a linearly or exponentially decreasing schedule.",
                    "label": 0
                },
                {
                    "sent": "But usually any two consecutive steps have around the same step size.",
                    "label": 0
                },
                {
                    "sent": "If this directional second derivative factor is rapidly changing, then these constant size steps are not going to be able to keep up with the optimal step size.",
                    "label": 0
                },
                {
                    "sent": "Interesting, but.",
                    "label": 0
                },
                {
                    "sent": "Expanding into the case where.",
                    "label": 0
                },
                {
                    "sent": "Change direction.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so when the step direction is not G, you're going to perform worse on a single step basis, but the really difficult thing about analyzing stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Where your accounting for the mini batches is that?",
                    "label": 0
                },
                {
                    "sent": "What I'm trying to say here is that you can get stuck and fail to make any progress in in a moment of time.",
                    "label": 0
                },
                {
                    "sent": "I'm not trying to say that this is the optimal step size for all of learning.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time there's a delayed reward problem where very early in learning you want to actually use much bigger step sizes and this would predict is optimal and at the end of learning you'll do better because you use these large step sizes early on.",
                    "label": 0
                },
                {
                    "sent": "We don't entirely understand what the mechanism is there, so we don't have a good way to theoretically analyze it, but we do know that if you use this to choose your step size all along, it's not going to yield that grade of final results at the end of learning.",
                    "label": 0
                },
                {
                    "sent": "What we can understand from this kind of analysis is that the eigen values tell us something about when we might accidentally go uphill.",
                    "label": 0
                },
                {
                    "sent": "To specialize this on the case you were asking about a bit where you're not actually following G, But you're following a wrong direction that isn't quite G you would have.",
                    "label": 0
                },
                {
                    "sent": "You'd end up with, I believe D transpose G / D. Transpose HD.",
                    "label": 0
                },
                {
                    "sent": "So the numerator would be smaller and the denominator would be using the blend of eigenvalues selected by D rather than by G. So you're.",
                    "label": 0
                },
                {
                    "sent": "Overall, you're expected to make less improvement with such a step, but we know that.",
                    "label": 0
                },
                {
                    "sent": "Because of things not captured in this small analysis, you can do better overall.",
                    "label": 0
                },
                {
                    "sent": "In multiple dimensions, I'll drive over here so the camera can see it.",
                    "label": 0
                },
                {
                    "sent": "You can end up with these kind of egg like shapes.",
                    "label": 0
                },
                {
                    "sent": "Where this is meant to be a 3D drawing.",
                    "label": 0
                },
                {
                    "sent": "So here's the axis drawn inside.",
                    "label": 0
                },
                {
                    "sent": "Our little egg.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty bad at drawing 3D on a whiteboard, but.",
                    "label": 0
                },
                {
                    "sent": "The idea is we've got an egg that descends for a long time with relatively little curvature.",
                    "label": 0
                },
                {
                    "sent": "And reaches a relatively low value over here.",
                    "label": 0
                },
                {
                    "sent": "And it descends very rapidly.",
                    "label": 0
                },
                {
                    "sent": "With very strong negative curvature and reaches a low value are relatively high value over here, so we'd like to go here.",
                    "label": 0
                },
                {
                    "sent": "If we follow the negative curvature, we end up here quickly and in a lot of cases I've seen neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Train like this.",
                    "label": 0
                },
                {
                    "sent": "Where first they follow the negative curvature and run off the side.",
                    "label": 0
                },
                {
                    "sent": "Of the egg and then they spend most of their training time.",
                    "label": 0
                },
                {
                    "sent": "Winding around the egg.",
                    "label": 0
                },
                {
                    "sent": "I think one reason that large learning rates help initially is that they cause you to pay attention to the gradient more than to the negative curvature.",
                    "label": 0
                },
                {
                    "sent": "When you make the stepsize very, very small, you end up with the equation that go millane was asking the question about a minute ago where you have the exponential amplification of the negative curvature directions, and I think that that sucks you right off the steep side of the egg.",
                    "label": 0
                },
                {
                    "sent": "The larger learning rate is going to be dominated more by the gradient than by the Hessian at the top, and so if you see if you make like a big step like this and end up over here.",
                    "label": 0
                },
                {
                    "sent": "You've made a lot of progress toward escaping the area where you're tempted to go off the.",
                    "label": 0
                },
                {
                    "sent": "Negative curvature side of the egg.",
                    "label": 0
                },
                {
                    "sent": "You usually do still end up going off the side, but you at least do it later and you spend less time on this.",
                    "label": 0
                },
                {
                    "sent": "This part of the arc over here.",
                    "label": 0
                },
                {
                    "sent": "Like I said, this is a speculative idea.",
                    "label": 0
                },
                {
                    "sent": "I'm not prepared to give you like a proof or or.",
                    "label": 0
                },
                {
                    "sent": "Detailed visualizations that prove that this is happening.",
                    "label": 0
                },
                {
                    "sent": "It's something that I suspect, based on some of the visualizations that I've made.",
                    "label": 0
                },
                {
                    "sent": "Of learning trajectories and based on that equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so resuming where we were before.",
                    "label": 0
                },
                {
                    "sent": "We just finished seeing how the eigenvalues of the Hessian can cause you to accidentally go uphill.",
                    "label": 0
                },
                {
                    "sent": "If your step size is chosen without knowledge of the way that the directional second derivative is going to fluctuate as you change your step direction.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall, we've reviewed a lot of reasons why convergence to a critical point may not happen.",
                    "label": 1
                },
                {
                    "sent": "You might just never stop if the function doesn't have a local minimum for you to run into, or even if there are local minima, you might find a subspace where you can continue outward to Infinity.",
                    "label": 1
                },
                {
                    "sent": "That happens a lot with classifiers.",
                    "label": 0
                },
                {
                    "sent": "You might also get stuck in the sense that you can't compute elocal direction where you can make an update that actually causes the objective function to go down.",
                    "label": 0
                },
                {
                    "sent": "That can happen from the conditioning being too bad, where you can't really actually go up Hill by using a fixed learning rate schedule.",
                    "label": 0
                },
                {
                    "sent": "It can also happen by by the gradient being too noisy.",
                    "label": 0
                },
                {
                    "sent": "If your different mini batches are very different from each other and your mini batch size is too small to estimate the true gradient very accurately.",
                    "label": 0
                },
                {
                    "sent": "Gradient noise can also happen when you're using approximate gradients.",
                    "label": 0
                },
                {
                    "sent": "I strongly suspect that a lot of the reason Boltzmann machines are hard to train is that there's too much not just noise, but also bias in the estimate of the gradient.",
                    "label": 0
                },
                {
                    "sent": "And then finally, the classical reason that people usually have known anyway that we wouldn't converge to yeah.",
                    "label": 0
                },
                {
                    "sent": "I eat that is true.",
                    "label": 0
                },
                {
                    "sent": "But you don't necessarily know what the smallest eigenvalue is.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You're talking about damping, or you're talking about subtle Frieden.",
                    "label": 0
                },
                {
                    "sent": "So saddle Free Newton is Joshua and Jan's project with.",
                    "label": 0
                },
                {
                    "sent": "But you're talking about modifying it by adding.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not talking about modified versions of it.",
                    "label": 0
                },
                {
                    "sent": "The important point here is the point I'm trying to argue is not so much that Newton's method is federally flood.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying is that.",
                    "label": 0
                },
                {
                    "sent": "The danger of getting stuck on a critical point is primarily a danger of UN modified Newton's method.",
                    "label": 0
                },
                {
                    "sent": "It's it's not a generic danger that effects all optimization algorithms OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "There are also many modifications of.",
                    "label": 0
                },
                {
                    "sent": "It are still affected by it, but yeah.",
                    "label": 0
                },
                {
                    "sent": "For example, the thing most people recommend doing is called damping, where you just add a value to all of the eigenvalues, and in that case it can be extremely expensive to estimate the actual minimum eigenvalue and know that you're using the right amount of damping.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Finding that minimum eigenvalue is so expensive that.",
                    "label": 0
                },
                {
                    "sent": "You may as well just revert to using gradient descent.",
                    "label": 0
                },
                {
                    "sent": "It's not really the feasibility of the method is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is related anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, so an interesting question about the structure of neural net optimization problems is whether saddle points or local minima are more common.",
                    "label": 1
                },
                {
                    "sent": "And this has become a very interesting area of research over the past few years.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of results in random matrix theory that I'm going to give you like a very cartoon view of here.",
                    "label": 0
                },
                {
                    "sent": "This isn't the way that the proofs actually work, but it helps you to develop some intuition.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we've got a critical point.",
                    "label": 0
                },
                {
                    "sent": "In a cost function and we want to generate the eigenvalues for its Hessian.",
                    "label": 0
                },
                {
                    "sent": "So imagine that we're going to generate the eigenvalues by flipping a coin every time we flip a coin, and it comes up heads, we make the corresponding eigenvalue be positive every time we flip a coin, and it comes up tails.",
                    "label": 0
                },
                {
                    "sent": "We make the corresponding eigenvalue be negative.",
                    "label": 0
                },
                {
                    "sent": "So how do we get a local minimum in this setting?",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues of a local minimum are all positive, so that means we have to flip all heads.",
                    "label": 0
                },
                {
                    "sent": "If you only have one variable, then you only have one eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "It's easy to flip heads once.",
                    "label": 1
                },
                {
                    "sent": "If you have a million variables.",
                    "label": 0
                },
                {
                    "sent": "Then you have a million eigenvalues and you've got to flip heads a million times in order to get a local minimum.",
                    "label": 1
                },
                {
                    "sent": "That's not exactly how random matrix theory works, but it gives you the basic idea that.",
                    "label": 0
                },
                {
                    "sent": "As the number of variables increases, the number of local minima decreases exponentially relative to the number of saddle points.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind, anytime you get both positive and negative eigenvalues in the same critical point, you've got a saddle point, so it's very easy to flip a coin twice and get one heads and one tails.",
                    "label": 1
                },
                {
                    "sent": "And if you flip a coin a million times, it's very easy to get at least one heads and at least one tails out of all those million coin flips.",
                    "label": 0
                },
                {
                    "sent": "The really cool part that actually makes us much more optimistic about our ability to optimize high dimensional nonconvex functions is that the coin seems to actually be weighted.",
                    "label": 0
                },
                {
                    "sent": "It's not 5050, and the weighting changes as you descend.",
                    "label": 0
                },
                {
                    "sent": "The cost function specifically toward the bottom of the cost function where the values of the cost function are lower.",
                    "label": 1
                },
                {
                    "sent": "You're more likely to flip heads and make positive eigenvalues, so when the cost function is very low, you're more likely to make local minima.",
                    "label": 0
                },
                {
                    "sent": "And when the cost function is high, you're more likely to make saddle points.",
                    "label": 0
                },
                {
                    "sent": "If you've got a method that doesn't get stuck on saddle points, that means you can go downhill and you're not going to be worried about getting stuck in a bad local minimum.",
                    "label": 0
                },
                {
                    "sent": "Most of the local minima are good.",
                    "label": 0
                },
                {
                    "sent": "To be honest, I don't really know exactly in in real life a lot of the theoretical frameworks assume that they are independent, I think.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you're optimizing your higher probability to end up in a high dimensional Canyon with a.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, this analysis doesn't really tell us a whole lot about what Canyon structures look like.",
                    "label": 0
                },
                {
                    "sent": "You mean, having like a mixture of small and large positive eigenvalues?",
                    "label": 0
                },
                {
                    "sent": "It does, yeah, it does tell you that toward the end of learning there are going to be very many large positive eigenvalues and.",
                    "label": 0
                },
                {
                    "sent": "If you instrument them, if you instrument the like the directional second, derivative derivative in the direction of your gradient.",
                    "label": 0
                },
                {
                    "sent": "You definitely do see this happen that at the start of training the directional second derivative is actually negative.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time, and at the end of training it will be positive and several 100 times larger than the norm of the gradient itself.",
                    "label": 0
                },
                {
                    "sent": "And that happens even when the gradient norm doesn't shrink.",
                    "label": 0
                },
                {
                    "sent": "So traditionally people have thought of optimization as you move downhill until you approach a critical point and your gradient shrinks and shrinks and shrinks and shrinks, and then you can't move anymore.",
                    "label": 0
                },
                {
                    "sent": "But lately it seems much.",
                    "label": 0
                },
                {
                    "sent": "This is still only a cartoon that doesn't capture all of the difficulties, but it seems much more accurate to say your gradient stays about the same size or maybe even grows, but the positive eigenvalues grow much much faster until your gradient norm is very small in comparison to them.",
                    "label": 0
                },
                {
                    "sent": "So in 2013 Andrew Sachs and some of his collaborators at Stanford wrote a paper analyzing what happens in very deep linear Nets.",
                    "label": 0
                },
                {
                    "sent": "So in this framework, you just take several matrices and multiply them together, and that's your neural net.",
                    "label": 0
                },
                {
                    "sent": "There's no non linearity at each layer.",
                    "label": 0
                },
                {
                    "sent": "This sounds like it's not a very good model at 1st and the reviewers for Nips thought so, but the the reviewers for I clear, are much more prescient and accepted the paper.",
                    "label": 0
                },
                {
                    "sent": "The reason that it's interesting is that even though the mapping from inputs to outputs is linear, the mapping from parameters to the cost function is nonlinear, and so this model captures a lot of the properties of what's difficult about optimizing neural Nets.",
                    "label": 0
                },
                {
                    "sent": "This paper kicked off the recent interest in saddle Points because they observed that these these models have an extremely large number of saddle points.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "They also have a set of global minima that are all connected to each other in this hyperbola shape and no local minima.",
                    "label": 0
                },
                {
                    "sent": "19 yeah.",
                    "label": 0
                },
                {
                    "sent": "Separated by.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "If we want to go into the history in 1987, Pierre Baldi showed.",
                    "label": 0
                },
                {
                    "sent": "That if you, if you make a neural net with one hidden layer and no non linearity then it has only subtle points and no non global minima.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so this is this is like a very long.",
                    "label": 0
                },
                {
                    "sent": "History of research, but the current wave of enthusiasm began with Andrew Saxs analysis and his collaborators.",
                    "label": 0
                },
                {
                    "sent": "Analysis of deep Nets.",
                    "label": 0
                },
                {
                    "sent": "This plot that I'm showing you right here.",
                    "label": 0
                },
                {
                    "sent": "Is what happens when you train a linear net with one unit at each layer.",
                    "label": 0
                },
                {
                    "sent": "And two layers total, just a hidden layer and then an output layer.",
                    "label": 0
                },
                {
                    "sent": "And we all you want to train it to do is to map an input value of 1 to an output value of 1.",
                    "label": 0
                },
                {
                    "sent": "Using mean squared error, so this is plotting the function.",
                    "label": 0
                },
                {
                    "sent": "The square of 1 -- W one times W2 with W one on the X axis and W2 on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "The global minimum is a place where W one times W 2 = 1.",
                    "label": 0
                },
                {
                    "sent": "And you can see that that's equal to W 2 = 1 / W one, so you get this hyperbola shape of optimality.",
                    "label": 0
                },
                {
                    "sent": "In the middle you get negative curvature and very small gradients along the hyperbola you get very strong positive curvature and very small gradients in the middle you get larger gradients and a blend between negative or positive curvature.",
                    "label": 0
                },
                {
                    "sent": "If you run stochastic gradient descent, it's curvature.",
                    "label": 0
                },
                {
                    "sent": "The trajectory is initially affected by the negative curvature, and then after a while it goes more or less straight to the hyperbola.",
                    "label": 0
                },
                {
                    "sent": "The curvature isn't plotted here.",
                    "label": 0
                },
                {
                    "sent": "This is plotting the actual cost function value.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so.",
                    "label": 0
                },
                {
                    "sent": "One way to see it is that the center is a local maximum.",
                    "label": 0
                },
                {
                    "sent": "And so at a local maximum, all the eigenvalues are negative.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After Andrew sexes paper, yahshua and surface students, including young Defendin resident pass canoe.",
                    "label": 0
                },
                {
                    "sent": "I had an idea that maybe this same result also applied to neural Nets that actually have nonlinearities, and they also brought in a lot of analysis of.",
                    "label": 0
                },
                {
                    "sent": "Theorems from the study of random matrices and functions defined by.",
                    "label": 0
                },
                {
                    "sent": "Random random error functions from simple families to analyze, like Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "So their basic idea was that maybe saddle points are a big problem for neural net training as well, and maybe neural net cost functions have these saddle points everywhere.",
                    "label": 0
                },
                {
                    "sent": "They then ran several experiments for me.",
                    "label": 0
                },
                {
                    "sent": "The most interesting experiment is 1 where they use stochastic gradient descent to move through the parameter space and periodically they use Newton's method to solve for a nearby critical point.",
                    "label": 0
                },
                {
                    "sent": "Then they test its eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Using that kind of experiment, they were able to see that there are lots of saddle points all throughout the space that neural net parameters occupy and that their eigenvalue distribution is more or less what the random matrix theory predicts that their eigenvalues become more and more positive as the cost function descents.",
                    "label": 1
                },
                {
                    "sent": "And then more recently, Anatra Manske and her collaborators at NYU.",
                    "label": 0
                },
                {
                    "sent": "You wrote a paper where they develop the theory, arguing that neural net loss functions should have this characteristic.",
                    "label": 0
                },
                {
                    "sent": "So they're coming up with theoretical arguments that justify the conjecture from the Montreal lab.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "General then.",
                    "label": 0
                },
                {
                    "sent": "Their paper looks at specific architecture approximations with the class.",
                    "label": 0
                },
                {
                    "sent": "So I guess if I can defend what are Moskovitz today, she would say you studied a very broad class of functions, but he didn't have an argument that neural net functions live within that class.",
                    "label": 0
                },
                {
                    "sent": "So she's providing the argument that they're all that functions.",
                    "label": 0
                },
                {
                    "sent": "Are lie within that class modulo some simplifying assumptions that she had to use.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Many more selling points then local, right, yeah?",
                    "label": 0
                },
                {
                    "sent": "So you can still have an exponential number of local.",
                    "label": 0
                },
                {
                    "sent": "What is saying about quality?",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So, especially on a tour, masker paper goes into a lot of detail about that.",
                    "label": 0
                },
                {
                    "sent": "Her paper says specifically that.",
                    "label": 1
                },
                {
                    "sent": "Up high on the cost function, there are many saddle points down low.",
                    "label": 0
                },
                {
                    "sent": "There's a thin band of cost function values in which there are many critical points that are mostly local minima and outside of that thin bound it's exponentially unlikely to find a local minimum.",
                    "label": 0
                },
                {
                    "sent": "There, there isn't necessarily a clear argument for what the average gap between a local minimum and the global minimum would be.",
                    "label": 0
                },
                {
                    "sent": "You also need to remember that we don't actually converge to a critical point of any kind.",
                    "label": 0
                },
                {
                    "sent": "In most cases we just we stop where we start overfitting or where the conditioning or the OR the noise gets too bad for us to continue.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The thing that's interesting where these results to me is.",
                    "label": 0
                },
                {
                    "sent": "That means there are not local minima that will block us from continuing up high on the cost function.",
                    "label": 0
                },
                {
                    "sent": "We will probably get blocked by something other than a critical point when we get download near this.",
                    "label": 0
                },
                {
                    "sent": "This thin band where the local minima mostly lie.",
                    "label": 0
                },
                {
                    "sent": "The other very important result from.",
                    "label": 0
                },
                {
                    "sent": "All of this theoretical work is that.",
                    "label": 0
                },
                {
                    "sent": "The relative quality of the minimum.",
                    "label": 1
                },
                {
                    "sent": "Let's see so most minima are good.",
                    "label": 0
                },
                {
                    "sent": "And that claim is more true if the model is very big.",
                    "label": 1
                },
                {
                    "sent": "When I keep saying that local minima are exponentially unlikely to lie outside of the band of low cost function values.",
                    "label": 0
                },
                {
                    "sent": "When I say exponential, I'm referring to the size of the network that as the number of parameters increases, the probability of lying outside that bound.",
                    "label": 0
                },
                {
                    "sent": "That bound decreases exponentially.",
                    "label": 0
                },
                {
                    "sent": "So that's the major implication that you should take home, yeah?",
                    "label": 0
                },
                {
                    "sent": "Change that.",
                    "label": 0
                },
                {
                    "sent": "Four different mini batches.",
                    "label": 0
                },
                {
                    "sent": "Yeah sure.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "For one training, sometimes that might change.",
                    "label": 0
                },
                {
                    "sent": "So how can these things?",
                    "label": 0
                },
                {
                    "sent": "Open, you extend these things to the action general setting where we do the really trying to.",
                    "label": 0
                },
                {
                    "sent": "Well so mini batch mini batch SGD is trying to estimate the expected gradient over all the examples.",
                    "label": 0
                },
                {
                    "sent": "And if it's if it's estimate is too far off, then you are going to do a bad job.",
                    "label": 0
                },
                {
                    "sent": "Of minimizing the overall cost function.",
                    "label": 0
                },
                {
                    "sent": "Hold for the agenda setting up.",
                    "label": 0
                },
                {
                    "sent": "Well, it's something that's interesting to study, but these problems are usually so difficult that it's.",
                    "label": 0
                },
                {
                    "sent": "It's hard to get solid results even for the simplified versions.",
                    "label": 0
                },
                {
                    "sent": "Trying to analyze the entire entire complete framework all in one shot.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll get there someday, but it doesn't seem very likely.",
                    "label": 0
                },
                {
                    "sent": "Is it natural that those all happen?",
                    "label": 0
                },
                {
                    "sent": "No, so that's The thing is.",
                    "label": 0
                },
                {
                    "sent": "You may not even really want to get to the global minimum, because that point might correspond to very severe overfitting.",
                    "label": 1
                },
                {
                    "sent": "So a lot of the time, if you did manage to get stuck on a local minimum of low value, but that does have a high chance of being the best you could do anyway.",
                    "label": 0
                },
                {
                    "sent": "As far as test set error goes.",
                    "label": 0
                },
                {
                    "sent": "So there is another implication of all this work, which is that unmodified Newtons method or Newton's method without the correct kind of modification will get stuck on saddle points and that those saddle points are all over the place, so it will get stuck very early on.",
                    "label": 0
                },
                {
                    "sent": "That's that's that's interesting.",
                    "label": 0
                },
                {
                    "sent": "If you want to improve neural Nets by moving to 2nd order methods.",
                    "label": 0
                },
                {
                    "sent": "If you're using something like SGD in momentum, then you probably don't need to worry about the presence of those subtle points, because they probably propel your SSD anyway.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "Newton's method is designed under an assumption that all the eigenvalues are positive.",
                    "label": 0
                },
                {
                    "sent": "It's solving for where there's a critical point in jumping to it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well more than so.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could imagine designing an algorithm based on.",
                    "label": 0
                },
                {
                    "sent": "This principle here where I've said like what infinitesimal stochastic gradient descent should do?",
                    "label": 1
                },
                {
                    "sent": "You could imagine making an algorithm where you actually take the matrix exponential of the negative Hessian and try to simulate where SGD would go for a large time difference and that would be a second order method.",
                    "label": 0
                },
                {
                    "sent": "That does not solve for a saddle point but uses a different principle to figure out how to go far downhill from 2nd order information.",
                    "label": 0
                },
                {
                    "sent": "I've tried that and it gets pretty unstable and it's also it's computationally expensive to do the matrix exponential.",
                    "label": 0
                },
                {
                    "sent": "Very idea is though that this is not solving for a saddle point or something for a critical point in jumping to it.",
                    "label": 0
                },
                {
                    "sent": "It's it's using the 2nd order information in a qualitatively different way.",
                    "label": 0
                },
                {
                    "sent": "The problem with Newton's method isn't so much that it's quadratic, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's intentionally going to intentionally going to a critical point without caring what kind of critical point it is.",
                    "label": 0
                },
                {
                    "sent": "You could end up with the same problem if you made like third order Newtons method where you have a third order approximation of the cost and you solve for a critical point in that approximation.",
                    "label": 0
                },
                {
                    "sent": "Does that?",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "It's OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so modern neural net optimization like what what state are we actually in?",
                    "label": 0
                },
                {
                    "sent": "Why are we all upset about it?",
                    "label": 0
                },
                {
                    "sent": "Is it totally broken?",
                    "label": 0
                },
                {
                    "sent": "Does it kind of work?",
                    "label": 0
                },
                {
                    "sent": "This is more of a matter of opinion, but I guess what I would say is that right now we're able to optimize most classifiers, autoencoders and recurrent Nets.",
                    "label": 1
                },
                {
                    "sent": "Provided that they are mostly based on linear layers, so where you take a matrix, multiply and then you apply a sigmoid or a relative or something like that.",
                    "label": 0
                },
                {
                    "sent": "To some extent, it's harder to optimize sigmoids than it is to optimize reluz or Max out or LS teams.",
                    "label": 0
                },
                {
                    "sent": "But even sigmoids we can still optimize relatively well most of the time, especially if you use the latest tricks like batch normalization.",
                    "label": 0
                },
                {
                    "sent": "For these kinds of less interesting architectures that are mostly based on matrix multiply non linearity matrix multiply non linearity.",
                    "label": 0
                },
                {
                    "sent": "We can generally find the right cost function value that we want, yeah?",
                    "label": 0
                },
                {
                    "sent": "It's easy to back, propagate through them and there they are relatively sensitive to perturbations of their input.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So basically what I'm saying describes everything that people actually use in the deep learning literature, but things things like RBF networks that are based on like take a difference, square it, and then take E to the negative square.",
                    "label": 0
                },
                {
                    "sent": "Those suffer from the vanishing gradient problem a lot more than matrix multiplied.",
                    "label": 0
                },
                {
                    "sent": "Non linearity matrix multiply non linearity.",
                    "label": 0
                },
                {
                    "sent": "So like for example you have networks, squaring operation.",
                    "label": 0
                },
                {
                    "sent": "If the value you're squaring is very close to 0.",
                    "label": 0
                },
                {
                    "sent": "Then the gradient is going to be extremely close to zero through the squaring operation there.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's probably more complicated than that.",
                    "label": 0
                },
                {
                    "sent": "Probably there are wide flat regions surrounding.",
                    "label": 0
                },
                {
                    "sent": "Points where there's wide regions where all these points in very small gradients and.",
                    "label": 0
                },
                {
                    "sent": "It's more than just a numerical issue because.",
                    "label": 0
                },
                {
                    "sent": "There is probably extreme flatness in some directions and extreme positive curvature in other directions.",
                    "label": 0
                },
                {
                    "sent": "It's it's not something you can just fix by rescaling the gradient.",
                    "label": 0
                },
                {
                    "sent": "So even very deep networks of this format can be trained fairly successfully.",
                    "label": 0
                },
                {
                    "sent": "David Cestello on at Google has trained networks up to 1000 layers in his submission to I cleared this year.",
                    "label": 1
                },
                {
                    "sent": "The main problem is that a lot of the time training can be much slower than we want.",
                    "label": 0
                },
                {
                    "sent": "The other big problem is that there are many models we might like to use that we can't optimize, like the model I was just talking about with the Deep RBF network.",
                    "label": 0
                },
                {
                    "sent": "One problem I'd like to highlight during this tutorial is that optimization algorithms are usually benchmarked on models that we can already train, and the argument of the paper is just now we can train this model slightly faster or we can reach a slightly lower cost function value.",
                    "label": 0
                },
                {
                    "sent": "I think it would be great if people continue to do that to show that you actually are performing comparably to the previous optimization algorithm, but it would be much better is to Additionally show that now you can train a model that.",
                    "label": 0
                },
                {
                    "sent": "Previously we were not able to optimize and that's what I think the real selling point of research and optimization should be is that it expands the frontier of the models that we can use.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As for the models that we're already able to train, why is training them so slow?",
                    "label": 1
                },
                {
                    "sent": "I guess I've already touched on a lot of the things that can prevent you from converging, and those things can all make you slow to learn as well.",
                    "label": 0
                },
                {
                    "sent": "If the gradients are noisy or if the conditioning is poor, or you need to turn the learning rate down a lot in order to not go uphill.",
                    "label": 0
                },
                {
                    "sent": "Another reason that optimization might be slow is that even if you're able to compute these local updates very well and make reasonably large steps that go downhill, the local updates might not really correspond to where the global solution is.",
                    "label": 0
                },
                {
                    "sent": "People have thought this thought before, but they've usually thought it in the sense of there are local minima and I might go to the wrong one, but it's a little bit more nuanced than that.",
                    "label": 0
                },
                {
                    "sent": "You might spend a very long time following a suboptimal path to a region of low cost value, and so to sort of help refocus the thinking, I want to provide this example where there are actually no minima anywhere on the cost function.",
                    "label": 0
                },
                {
                    "sent": "But you can spend a very long time getting to a low value.",
                    "label": 0
                },
                {
                    "sent": "So over here, the cost function descends to an asymptotes and never quite reaches it, so there's not actually a minimum.",
                    "label": 0
                },
                {
                    "sent": "On the left side, the function also descends to an asymptotes, but it's a higher asymptotes.",
                    "label": 0
                },
                {
                    "sent": "This is also a 3D function.",
                    "label": 0
                },
                {
                    "sent": "There's an access coming out of the board that you can't really see.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you have a mountain, and if your initialized on the wrong side of the mountain, you initially descend the wrong side until you come to a relatively flat part of the plane.",
                    "label": 0
                },
                {
                    "sent": "At that point, you figure out you need to come around the mountain.",
                    "label": 0
                },
                {
                    "sent": "And wind your way coming out of the board.",
                    "label": 0
                },
                {
                    "sent": "Back around it.",
                    "label": 0
                },
                {
                    "sent": "And over to this direction.",
                    "label": 0
                },
                {
                    "sent": "I see this kind of thing happen in visualization sometimes, and I think it contributes a lot to their training time being very long.",
                    "label": 1
                },
                {
                    "sent": "So among models that we can train this sort of discrepancy between correctly computed local updates and the global structure can make their training time very slow compared to what we'd like it to be.",
                    "label": 0
                },
                {
                    "sent": "I guess that.",
                    "label": 0
                },
                {
                    "sent": "Yeah and OK.",
                    "label": 0
                },
                {
                    "sent": "So I guess one thing I've sort of avoided discussing here.",
                    "label": 0
                },
                {
                    "sent": "In the subspace that I drew, this area is very flat, but recall the random matrix theory results.",
                    "label": 0
                },
                {
                    "sent": "In most directions, the eigenvalues are going to be very positive, so I'm already sort of struggling to convey the 3D structure, but there's actually ND structure and the dimension I've drawn is the nicest one.",
                    "label": 0
                },
                {
                    "sent": "The one coming out of the board is the 2nd nicest one, but there might be another N -- 2 directions where it looks like you're in an extremely steep Canyon and you're very constrained as to how you can move without going up the sides of the Canyon.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of people have analyzed neural Nets using the linear approximation that Andrew Sachs used.",
                    "label": 0
                },
                {
                    "sent": "Like Yahshua said, young Lacon use this approximation.",
                    "label": 0
                },
                {
                    "sent": "Did describe neural Nets in his book chapter in the 1990s.",
                    "label": 0
                },
                {
                    "sent": "This view of the difficulty of training deep networks helps to build some intuition for what happens when I use different kinds of optimization algorithms and Andrew Sachs regardes some of these problems as being some of the larger problems in what makes it hard to train a neural net.",
                    "label": 1
                },
                {
                    "sent": "So I'm just sort of repeating things that he's told me.",
                    "label": 0
                },
                {
                    "sent": "New person.",
                    "label": 0
                },
                {
                    "sent": "I not using his exact words, so I may be summarizing it imperfectly.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is suppose that you have some output Y and you find Y by multiplying together many different weights.",
                    "label": 0
                },
                {
                    "sent": "And at the input there's some X variable.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the output Y with respect to one of the weights is X times all of the other weights.",
                    "label": 0
                },
                {
                    "sent": "When you initialize all the weights are very small and that means that when you make an update, the update is going to have a very small change because it gets multiplied by so many small weights.",
                    "label": 0
                },
                {
                    "sent": "So initially you want to make relatively big updates to outweigh the effect of all the way it's being tiny.",
                    "label": 0
                },
                {
                    "sent": "But then later on, the weights begin to get big after you've made those big updates.",
                    "label": 0
                },
                {
                    "sent": "And in that case, rather than getting exponentially shrunk, your updates get exponentially magnified.",
                    "label": 0
                },
                {
                    "sent": "So there's this difficulty of trying to make updates that are large to compensate for the multiplication by many small values early on, and then they need to make updates that are smaller to compensate by the multiplication by many large values later on.",
                    "label": 0
                },
                {
                    "sent": "Now, if you have a network with more than one unit per layer, this gets even worse because you'll have some directions that have very small eigenvalues in other directions, have very big eigenvalues, and the kind of update that you need to make is going to depend on exactly which of these eigenvalues you're aligning with.",
                    "label": 0
                },
                {
                    "sent": "The reason that this happens is that when you use the gradient, it's telling you how each unit should change independently in order to make a rapid rapid improvement.",
                    "label": 0
                },
                {
                    "sent": "But this expected change doesn't take the change in the other units into account.",
                    "label": 0
                },
                {
                    "sent": "If you have a second order method then you can say I'm going to update each unit taking the change of the other units into account, but it's only considering pairwise effects.",
                    "label": 0
                },
                {
                    "sent": "It's not considering the change of all of them simultaneously.",
                    "label": 0
                },
                {
                    "sent": "To entirely resolve this problem, we would need an NTH order method, but for NTH order methods you can't really solve the resulting equations very easily and.",
                    "label": 0
                },
                {
                    "sent": "For neural Nets, and can become really quite large.",
                    "label": 0
                },
                {
                    "sent": "Well, in the most general sense, and would be the number of parameters for this problem of iterated multiplication.",
                    "label": 0
                },
                {
                    "sent": "Then it's the number of layers that determine how many things get multiplied together before they hit you.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is sort of the world view that Andrew Sachs who is pitching to me the last time I saw him and.",
                    "label": 0
                },
                {
                    "sent": "The idea is you have some loss function that's based on mean squared error, where you've got a target value of, for example one and then you might subtract.",
                    "label": 0
                },
                {
                    "sent": "You know, a few multiples of parameter one times, parameter zero.",
                    "label": 0
                },
                {
                    "sent": "I'm multiplying by parameter one a few times to emulate parameter sharing here, but the idea is that on the top it's very flat.",
                    "label": 0
                },
                {
                    "sent": "These areas right here ignore them, just cropping areas that would stick out of the graph and include your view of the rest of it.",
                    "label": 0
                },
                {
                    "sent": "So on the top it's very flat, in the middle it's very steep at the bottom it's very flat again and you've got to handle that transition.",
                    "label": 0
                },
                {
                    "sent": "Without accidentaly.",
                    "label": 0
                },
                {
                    "sent": "Doing something that causes your loss function to explode, but also while making big enough steps that you can escape the flat region at the top.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see, so I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm supposed to finish at 10:30 or 11.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd better hurry up then.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have some visualizations for.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You usually if you do visualization in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "If you just pick two random dimensions and you plot the cost and you plot the trajectory, you see that the trajectory is very yeah.",
                    "label": 0
                },
                {
                    "sent": "The colors are the value of the cost function and because.",
                    "label": 0
                },
                {
                    "sent": "The band is very narrow.",
                    "label": 0
                },
                {
                    "sent": "The normalization of the colors went a little bit crazy, yeah?",
                    "label": 0
                },
                {
                    "sent": "The SG trajectory and I'm plotting the value of the cost function in this 2D subspace and you can see that you don't see very much variation at all if you just choose a random subspace and you see that the SGD trajectory is very complicated if you yeah.",
                    "label": 0
                },
                {
                    "sent": "I don't know of any results about depth and number of saddle points.",
                    "label": 0
                },
                {
                    "sent": "So this this 2D subspace visualization shows you the trajectory is very wild, but the cost function is very flat.",
                    "label": 1
                },
                {
                    "sent": "If you would choose random projections to see something more interest.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We need to use a Special 1 dimensional subspace where.",
                    "label": 1
                },
                {
                    "sent": "In three dimensions, we've got the cost on the Y axis and then two parameters on the.",
                    "label": 0
                },
                {
                    "sent": "At the X and the Z axis I guess.",
                    "label": 0
                },
                {
                    "sent": "This is the trajectory that SGD followed, and we're going to look at the 1 dimensional subspace spanning the initial point and the final point.",
                    "label": 0
                },
                {
                    "sent": "If we interpolate linearly between this parameter and this parameter, we see that the cost function decreases smoothly along this curve and that allows us to see some things that we couldn't see just from the learning curve.",
                    "label": 0
                },
                {
                    "sent": "Overtime, the learning curve overtime includes some bumps just because it accidentally went uphill in some places due to too large of learning rate and from the learning curve we can't tell if those bumps are from accidentally making uphill steps, or from there being bumps on the cost function.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we run the linear interpolation experiment on Max out on amnist, we actually see that we monotonically decrease from the initial point.",
                    "label": 0
                },
                {
                    "sent": "To the final point, which is right here and then, if you interpolate farther in the same direction, it starts to go up again.",
                    "label": 0
                },
                {
                    "sent": "The green curve shows that you can overfit if you do the same thing.",
                    "label": 0
                },
                {
                    "sent": "On the right, I'm showing the learning curve overtime and you can see that in terms of time we descend.",
                    "label": 0
                },
                {
                    "sent": "This negative curvature region very quickly we get down to the same cost function value right here.",
                    "label": 0
                },
                {
                    "sent": "After just the first few epox.",
                    "label": 0
                },
                {
                    "sent": "And then we spend several 100 parks traversing from here.",
                    "label": 0
                },
                {
                    "sent": "Two here or so.",
                    "label": 0
                },
                {
                    "sent": "So most of the time is spent after you escape the big local maximum or saddle point at the origin.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to move around in the bottom where the condition number is high.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same results hold if you use other activation functions.",
                    "label": 0
                },
                {
                    "sent": "This isn't just Max out, we get basically the same curve for sigmoid, zanfer, reluz.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We run the same experiment on convolutional networks and convolutional networks.",
                    "label": 0
                },
                {
                    "sent": "It's hard to see, but if you zoom in near the top or very start the interpolation, we can actually see that you're slightly on the wrong side of the mountain when you start.",
                    "label": 1
                },
                {
                    "sent": "And it's too expensive to run a more detailed visualization and determine exactly how the convolutional Nets got around the mountain.",
                    "label": 0
                },
                {
                    "sent": "But we can see that there is a small barrier between where they were initialized and the point that they went to.",
                    "label": 0
                },
                {
                    "sent": "In the end.",
                    "label": 0
                },
                {
                    "sent": "The cost function actually goes higher.",
                    "label": 0
                },
                {
                    "sent": "This barrier is not very tall, so it's possible that noise just shook them through it.",
                    "label": 0
                },
                {
                    "sent": "It's also possible that they had to wind around it somehow.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also looked at a model of sequences.",
                    "label": 0
                },
                {
                    "sent": "This is an LS TM train on Penn Treebank and has more or less the same shape as the Max out function where you start at a high cost function value and monotonically decrease to a low one with no real obstacles in the way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first thing we actually found, any interesting obstacles was a generative model.",
                    "label": 1
                },
                {
                    "sent": "The multi prediction Deep Boltzmann machine and here we found this big plateau on a straight line from the initial point to the solution.",
                    "label": 0
                },
                {
                    "sent": "So to see if SGD actually had to interact with this plateau.",
                    "label": 0
                },
                {
                    "sent": "We made a new visualization where.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to move from this parameter point to this parameter point and linearly interpolate along them in that direction.",
                    "label": 0
                },
                {
                    "sent": "But then on a second axis, at every point we're going to interpolate out.",
                    "label": 0
                },
                {
                    "sent": "From this main axis to the point that SGD actually went to, so we're going to have one axis that's just scrolling down the path from the solution, and then we have another access that scrolls side to side at each point to see what lies between the SGD trajectory and the the like kind of imaginary point that was visiting.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we do this for the MPD BM, we can see that from the initial point we swing out in a wide circle and we actually entirely avoid the plateau with the saddle point in the middle of it.",
                    "label": 0
                },
                {
                    "sent": "I think this might be because of the effect of saddle points repelling SGD, but we don't know that for sure.",
                    "label": 0
                },
                {
                    "sent": "We also know that we shouldn't take the shape of the trajectory in this plot too seriously.",
                    "label": 0
                },
                {
                    "sent": "As you saw in the 2D sub plot, the trajectory wobbles around quite.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the semicircle shape in the plot where you intentionally start at one point and end at the other end point naturally becomes a semicircle, as the dimensionality of the space increases.",
                    "label": 0
                },
                {
                    "sent": "So this is with two dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is with 10.",
                    "label": 0
                },
                {
                    "sent": "This is with 1000.",
                    "label": 0
                },
                {
                    "sent": "This is with 100,000 by the time you get 100,000 dimensions, you essentially deterministically get this semicircle shape.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's interesting in this plot is not the path moves in a semicircle, but the path lies outside the plateau.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many other 3D plots that we made that have no obstacles and a lot of them have kind of this like.",
                    "label": 1
                },
                {
                    "sent": "It looks like the inside of a shell like you find on the beach.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It's a gently curving Canyon with a lot of negative curvature near the initial point and then positive curvature after you get down to the bottom of it.",
                    "label": 0
                },
                {
                    "sent": "This looks the same for deep factored linear networks.",
                    "label": 0
                },
                {
                    "sent": "Rehler Nets trained with adversarial training.",
                    "label": 0
                },
                {
                    "sent": "Ananel STM.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at Max out trained with adversarial training, we actually do find little bumps at the bottom and if you zoom in you see that there's a narrow tight Canyon that the cost function is constraining the parameters into.",
                    "label": 0
                },
                {
                    "sent": "During gradient descent training.",
                    "label": 0
                },
                {
                    "sent": "So it looks like a lot of neural net optimization problems do not really have any interesting obstacles, But some do, and we don't really know which ones do and which ones don't yet.",
                    "label": 0
                },
                {
                    "sent": "It's an interesting direction to study.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall, we've learned that there exists a linear subspace where the value decreases monotonically for essentially 90% of the models we've studied.",
                    "label": 1
                },
                {
                    "sent": "For some problems, there are actually obstacles preventing SGD from traveling down, that is that.",
                    "label": 1
                },
                {
                    "sent": "That nice subspace?",
                    "label": 1
                },
                {
                    "sent": "And factored linear models seem to capture most of the qualitative aspects of neural net.",
                    "label": 0
                },
                {
                    "sent": "Last functions that we capture these visualization.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, I would say.",
                    "label": 0
                },
                {
                    "sent": "When you are training a neural net and it doesn't work as well as you want, don't automatically blame one specific boogeyman like local minima.",
                    "label": 1
                },
                {
                    "sent": "Be aware that there are many boogiemen out there and look for specific evidence for one of them.",
                    "label": 1
                },
                {
                    "sent": "In particular, if you think you're stuck in a local minimum, look at your gradient norm.",
                    "label": 0
                },
                {
                    "sent": "It should be small if you think that you have bad conditioning, look at the cost function value on successive steps and see if you're going uphill.",
                    "label": 1
                },
                {
                    "sent": "And also look at the eigenvalues and see if there's a wide variety of them.",
                    "label": 0
                },
                {
                    "sent": "If you think that the problem is noise in your gradient, then actually look at your gradient and see how noisy it is and also look for uphill steps coming from taking bad directions.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you think the problem is subtle points, you know, look at the gradient norm and also look for negative eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "You should find a small gradient norm and a reasonable negative eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Overall, make visualizations, and I think it would be good if we all feel a challenge to show that the problem we believe in exists.",
                    "label": 0
                },
                {
                    "sent": "If you think there's a saddle point or a local minimum, then make a visualization of it and show it.",
                    "label": 0
                }
            ]
        }
    }
}