{
    "id": "nhykzysplrfhkdfnaoaynw2prcki7lep",
    "title": "Surrogate-based Constrained Multi-Objective Optimization",
    "info": {
        "author": [
            "Alexander Forrester, School of Engineering Sciences, University of Southampton"
        ],
        "published": "July 20, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/mla09_forrester_sbcmoo/",
    "segmentation": [
        [
            "OK, so I'm going to talk about surrogate based optimization, in particular global optimization and when it's applied to real world problems and getting around the problems that we're faced with when we actually do optimization for real.",
            "OK."
        ],
        [
            "So.",
            "First of all, I'm going to start off with some of the basics.",
            "Probably I'll just be teaching you all stuff you already know, but maybe a bit of a refresher or some of you will suddenly think.",
            "Yeah, that's.",
            "That's what it's all about anyway, so I'm going to talk about.",
            "The basic idea, then Gaussian process based modeling or rigging whatever you want to call it and then go on to talk about some infill criteria used to enhance surrogate models during optimization, probability improvement and expected improvement.",
            "And then I'm going to go on to the real stuff, which is how to cope with when you have design evaluations fail, leading to missing data in your optimization, noisy data, which I'm sure a lot of you are faced with.",
            "And then go into constraints and multi objectives and then if."
        ],
        [
            "Is time an example at the end?",
            "OK then, so lots of people see surrogate based optimization in different ways and this is the way we do it at Southampton.",
            "We well naturally need some idea of the problem to begin with, based on some preliminary experiments to look at how different design variables affect the problem.",
            "Maybe, and just get an idea of what's going on.",
            "After that, we'll need some sample data.",
            "So we'll perform some kind of formal sampling plan.",
            "Roll.",
            "Collect our observations, IE run computational fluid dynamics or whatever it is we're doing.",
            "Then construct the surrogates.",
            "I won't go into these details today, but at this point we want to think about whether we had design sensitivity.",
            "Sensitivity is available whether we're going to have gradient enhanced model.",
            "Have we got multi Fidelity data?",
            "So is it going to be a multi level model?",
            "Once the surrogate's constructed, we then go on to search the model and then update it using additional simulations and at this point.",
            "We want to think about whether we've got constraints, whether we've got noise, whether it's multiple objectives that we're dealing with, and so on.",
            "And we go around this loop.",
            "Until we get bored, run out of money, reach a convergence criteria, whatever.",
            "OK, and.",
            "Contrary to what I think some researchers think we're not too interested in the global accuracy of the surrogate, as long as it's accurate where it needs to be.",
            "If we make a really accurate surrogate in the area of poor designs, that's just time wasted.",
            "Really, as far as we are concerned."
        ],
        [
            "OK."
        ],
        [
            "So.",
            "Gaussian process modeling.",
            "I'll just patiently go through at the beginning of that flow chart.",
            "We start off with a sample plan.",
            "Here.",
            "We're going to try and predict this function, which the branding function and we're going to predict based on 20 pieces of information shown by the black dots there.",
            "That's an optimized Latin hypercube sampling plan.",
            "OK."
        ],
        [
            "Then we're going to correlate all the points with each other using this correlation here.",
            "And this is what it's the whole models based around.",
            "Really, this is a popular correlation used in kriging.",
            "We tend to set this thing here to two, which makes it a Gaussian and.",
            "It's this theater here, which the whole thing hangs off, and getting that activity parameter correct.",
            "Which really leads to the kriging model predicting the data well, and this parameter here essentially affects the width of our correlation functions and you see on this plot on the right there how we go from a very low theater, not .1.",
            "A correlation hardly drops off as we move points apart.",
            "I as X -- X moves away from zero.",
            "If we've got a higher theater, we have a sharply drop dropping off correlation, so our data doesn't have much to do with other data, so our cricket model is going to be a bit more lumpy OK?",
            "So."
        ],
        [
            "We get how each of our 20 sample points are correlated with all the other sample points and that leads essentially to these 20 Gaussian bumps with the widths determined by these theater parameters, they've got two Theta parameters, so their non axisymmetric bumps OK so.",
            "And we choose the widths based on maximizing the likelihood of the data.",
            "So we've got this likelihood maximization embedded."
        ],
        [
            "With all of this.",
            "Then we multiply these bumps by weightings so they no longer scale between zero and one.",
            "Some go up, some go down.",
            "We had them all together."
        ],
        [
            "And we got our cricket prediction.",
            "Which is very similar to the true function here, so it's this kind of modeling that everything that I'm going to go on to show you in the talk is based around this way of predicting engineering functions.",
            "It's quite a flexible method, which is essentially only based upon the function being smooth and continuous, so it's really quite useful for engineering problems which tend to be so."
        ],
        [
            "OK.",
            "Right then, so we've got a model we want to do some optimization with it."
        ],
        [
            "Now I'll show you why we like using tricking.",
            "By just taking you through a couple of examples, where are we to use polynomial regression and where are we being particularly horrible to people who use polynomial regression, which are an example like this where we're trying to optimize this function here?",
            "OK, and we sample at three points.",
            "And we fit our polynomial through that second order and then we try to research this and we find the optimum and we update the model there and we keep on doing that and we don't.",
            "We never get anywhere because clearly this shape isn't a second order polynomial and I matter how much we throw data at the optimum.",
            "We're never going to distort it and actually lead lead towards the global optimum so.",
            "Maybe want to use some kind of interpolating model?"
        ],
        [
            "EG creeking radial basis function.",
            "Who knows what?",
            "So we do the same thing.",
            "We start off with a very similar prediction, but when we add data to the minimum here.",
            "At this point there.",
            "The prediction distorts and goes through the data, and as we add more and more data, we're guaranteed to converge to a local optimum.",
            "Here using this method."
        ],
        [
            "OK.",
            "But what if we decide we're going to be horrible to people who use radial basis function and kriging prediction as an infill strategy?",
            "Well safe.",
            "This function was particularly misleading like this.",
            "That previous strategy.",
            "Would just end up at this local optimum here, but we want to explore the model more and I eventually find this one here.",
            "OK, but Gaussian process models are quite use."
        ],
        [
            "Because we can extract some kind of information as to what the error in the model might be.",
            "So if we look at this same prediction.",
            "And then calculate the error that are Gaussian process model predicts.",
            "We get this so just it's completely intuitively.",
            "Basically it just goes up in the gaps between the data and the rate that it goes up at is dependent upon that feature parameter which we've estimated using maximum likelihood estimation.",
            "OK, so clearly if we were going to add some points at the maximum error in the prediction we get over to this point very soon, but that would be a just complete exploration.",
            "We want some kind of exploration exploitation tradeoff.",
            "So we're going to use this error to produce some exploitation."
        ],
        [
            "Operation tradeoff metrics.",
            "So this is a key graph and understanding probability of improvement and expected improvement.",
            "So what I've done is I've taken the same prediction as before.",
            "We talk the same function we're trying to predict.",
            "To optimize this is the prediction here and then what we've done is plotted.",
            "The distribution that the prediction might take at this point here.",
            "So the mean of this distribution is the prediction.",
            "OK, that's basically what the gathering process model is.",
            "It's it's the mean of sort of window that the prediction might fall into.",
            "So, but there's also a finite probability that the prediction could take values up and down.",
            "Here, that probability given by this distribution that's plotted here.",
            "Now if we draw a line from the minimum value found so far.",
            "And then integrate below that line and below the probability distribution we get the probability of improvement.",
            "OK, now the expected improvement is the first moment of this area.",
            "About the best point so far, so that's a sort of intuitive visual way of understanding probability of improvement and expected improve."
        ],
        [
            "So.",
            "I've included equations down here just for those people who are that way inclined, but.",
            "Hopefully the talk should should follow a sort of intuitive progression.",
            "Anyhow, so this is what we get if we plotted the probability of improvement for that last prediction.",
            "So we had a real cluster of points around this area, but so it's clear that we're not going to get very much more improvement in that area.",
            "But if I was going to get my house on it, I'd probably say, yeah, maybe we'll get a tiny bit of improvement there.",
            "So the probability of getting any improvement at all is very high.",
            "But there's also some probability that there will be improvement over in the other area where we had sparse sampling, so it's not a measure of how much improvement will get, it's just a measure of the probability that there will be some improvement.",
            "Expected improvement, on the other hand.",
            "Actually tells you how much improvement you might get, so these values here correspond to how much improvement the statistics thinks is possible in the model tends to be a real under estimator.",
            "I won't go into that too much in this talk, but it's something to bear in mind, and I suppose an open air of research to try and get the expected improvement measures to be a bit more accurate.",
            "Anyway, so this is the rest of the talk is essentially based around using permutation of the expected improvement criterion, because this really nice tradeoff between local exploitation and global exploration.",
            "And as I said here can be extended to constrained multi objective."
        ],
        [
            "Blums OK before I go on there just a little demo of how it actually pans out, applying expected improvements that first problem.",
            "We start off with a pretty rubbish prediction.",
            "We've got reasonably even expected improvement on each side.",
            "The search tends over to this side and we start to follow a similar route to the exploitation only update strategy.",
            "What happens here is we really do over exploit this area and that's something to do with the theater parameters not being estimated particularly well with such a sparsity of data.",
            "But in any case, once we've.",
            "Hammered away and found this local optimum.",
            "Here we start to see these expectation of improvement popping up on this side, and we reached the global optimum in the end and under certain mild assumptions you can prove that this method will always converge towards the global optimum.",
            "Although to be honest, with the engineering design, we're not too worried about asymptotically convergence proofs, although it's nice to have them if they're not there.",
            "Maybe you start to worry."
        ],
        [
            "OK, So what if we were going through an expected improvement update strategy, or indeed any surrogate model based update strategy and we came across missing data.",
            "So say your matching fails.",
            "Your flow simulation diverges.",
            "Some of these problems.",
            "You might want to go back and try and really work the problem hard and get an answer there, but in other cases you may not have access to the code.",
            "You might not be the expert, so you just need to basically consider the missing data as regions where of unknown constraint violations.",
            "So you've got boundaries that you have no way of calculating.",
            "You've just got places where."
        ],
        [
            "You know you can't.",
            "You can't solve.",
            "OK, so.",
            "If you you're running through your update strategy, you get your surrogate model.",
            "You put a new piece of data in there.",
            "That modifies the surrogate and then you go around the process again.",
            "If no piece of data is forthcoming, you're stuck.",
            "You have to do something, otherwise the optimization will stall.",
            "So maybe you could just think, well, let's add a random point that will just perturb the model and then we can go on as normal, hopefully will be OK, we won't hit that problem again, or maybe something that might be a little bit more intelligent would be too imputed value there that was equal to the prediction.",
            "That you've made, so you would just say well, for the time being, let's just assume we're correct in our prediction there.",
            "And that would mean that you would never have another expected improvement there, because you can't have an improvement where you already know the value, OK?",
            "So that might be a good idea.",
            "Better idea we think would be to penalize.",
            "The value at that point somehow.",
            "So you just sort of distort your surrogate model upwards there, assuming you're minimizing so that you tend to not visit that region again.",
            "So ascentia, what we've done is we've taken the prediction plus the mean squared error in the prediction and imputed those values where we have."
        ],
        [
            "Failures and I'll show you some results of a problem that we've applied that too, so it's a 2D airfoil optimization problem.",
            "This is a basic naecker aerofoil here.",
            "And then we've got one as orthogonal basis functions, which are these various functions that are detailed on the side here, which can be added with various weightings to this airfoil to change its shape.",
            "And it's a very powerful way of optimizing this transonic airfoil.",
            "It turns out the parameterization and so on is really not the point here.",
            "We've got an error for problem, and that's what it is.",
            "Goal two of these shape functions, which essentially alter the camber and thickness towards the rear of the airfoil.",
            "We're using a potential flow solver, which is the S do VGC solver and we get approximately 35% failure rate.",
            "UTEP apps very occasionally the upper and lower surface will of the error will actually cross something that we could figure out.",
            "You know if we went into parameterization a little bit more deeply, but most of the problems where you get separation and because the error can't actually make the lift produced, the lift that we want it to.",
            "OK, so we start off this problem with 20 point optimal Latin hypercube design exactly the same way as I did with rounding function.",
            "At the beginning we do maximum expects improvement updates until within one drag count of the optimum because it's a cheap problem, we know what the optimum is so."
        ],
        [
            "This is all being done with hindsight.",
            "It's a nice test.",
            "So we try these different strategies.",
            "Here we try the random update strategy, which is when we hit a failure.",
            "We just take a random point.",
            "We try and impute value.",
            "The predicted value at the point of failure and we also try to impute the predicted value plus the mean squared error at a failure, and then we've also tried a genetic algorithm random search just to check that we're not way out.",
            "And so I've got to actually go through the results.",
            "Here we are so.",
            "All have basically the same degree of failure to begin with before we start doing our infill strategy.",
            "Then, as you'd expect, random update.",
            "Actually you don't get any improvement.",
            "You're still tending to get as many failures afterwards.",
            "Just computing the prediction you do slightly better.",
            "The penalized imputations, though, do incredibly well.",
            "And then you can see the total number of simulations, including failures, until we reach the optimum.",
            "This penalized imputation method really wins out."
        ],
        [
            "And just in case you haven't really quite followed it, I've got this in pictures here.",
            "This is our design space essentially.",
            "That's the region where we can simulate.",
            "That's our coefficient of drag.",
            "That's a first design variable.",
            "Our second design variable, so we're trying to find the optimum of this surface, which we've been able to compute.",
            "At Priory, because it's such a cheap problem, so we take a 20 point sampling plan and the black ones are the ones that have succeeded.",
            "The red ones are the ones that have failed.",
            "OK, so that's our starting point and the green points are the updates, so all our green points.",
            "The three that it's taken to find the optimum are down in the bottom of this basin.",
            "This course mesh here is our original kriging prediction and the fine mesh is the distorted kriging prediction because we've penalized these points around the edges so we just make this basin.",
            "In which our optimal feasible designs are and underneath are also contours of the expected improvement, and this shows how this method is is a global method as well 'cause although we've got high expected improvement right here in the obvious basin there we also maintain.",
            "The vague possibility that we could go and explore outside.",
            "So if you just left this running, it would keep firing points off into the corners where you weren't sure exactly what the optimum values were."
        ],
        [
            "OK, so we've been extended this to higher dimensions, not too high, only to four so that we can still go on visualizing it here.",
            "We've got 82% failure rate, so you know really quite rubbish problem set up here, but sometimes you have to deal with these things.",
            "And again, I won't go through the results too much, but we've got.",
            "308 there compared to much higher values here.",
            "This predictor imputation is not AM.",
            "A great one because it we didn't actually managed to reach the optimum for some of the runs, so that's just the best.",
            "It did not on average, so penalized imputation wins out."
        ],
        [
            "And this is a picture of that process as well, so I don't know if any of you well at least one of you here seen these kind of pictures before.",
            "This is a hierarchical axis technique plot where each one of these tiles is a 2D design space.",
            "So if I flip back up.",
            "If you remember what that looks like.",
            "Looking down on it.",
            "That's kind of like that.",
            "OK, so we've got our first and second aerofoil variables bearing inside each tile.",
            "And then third and 4th variables vary from tile to tile, so if you stand back and squint a little bit, you can see the drag varying across this 4 dimensional design space.",
            "The blue dot regions or where there's a geometry violation, upper and lower surfaces of crossed over so.",
            "In effect, we could discount those regions anyway.",
            "The white areas where we don't really know what's happening with the simulation, it just failed, so we've thrown.",
            "200 points at this black for the initial sampling plan, the black ones are initial sampling points that succeeded.",
            "The red ones, one that failed.",
            "Now, even in such a way, there's 82% of failures possible in our subsequent infills strategy.",
            "Only these crosses here.",
            "Well, the ones that failed so we got one up there and a few down here and the.",
            "Green dots here are the ones at the infill points that have actually succeeded and have finally ended up finding this optimum here.",
            "So quite simple method, but quite powerful when you're faced with these problems where you don't really know why it's failing, you don't have access to the source code.",
            "It comes in useful."
        ],
        [
            "OK then, so as well as missing data will almost certainly have noise in your data."
        ],
        [
            "Most datasets are corrupted by noise.",
            "Here I'm interested in deterministic noise and I think most of you here will be interesting that as well, so noise would normally mean noise you get from an experiment from human error, and so on and so forth.",
            "That's it, repeatable, deterministic, deterministic noise means essentially error that looks kind of random, but isn't because if you went back and did the simulation again, you get the same answer, but nevertheless.",
            "It can be treated a bit like noise when you're fitting models, so.",
            "Here we've got the same problem, but I'm just varying one.",
            "Airfoil design variable, and this is the effect that it's having here.",
            "You start with a low camber there and go to very high camber.",
            "I don't need to point out that these are skewed axis here.",
            "It's not actually that fat.",
            "This is a thickness to chord of 12% I think, so I'm just screwed them up.",
            "Put them on the plot anyway, so this is the drag.",
            "Very the courage and drag varying and we have this noisy response here which are gradient based.",
            "Optimiser would really struggle with using surrogate models.",
            "We can smooth this out."
        ],
        [
            "OK, so I'll go on and show a few problems with doing that.",
            "First of all, let's just look at.",
            "If you were going to interpolate the data, so we start off with four sample points and we do a maximum expected improvement based infill strategy.",
            "And it all goes kind of nicely to begin with until point start bunching up here.",
            "And then finally we have this really snaking plot there.",
            "And because the plots becoming very snaky or well because the data is becoming difficult fit, this feature parameter goes up because the correlation in the data isn't so strong.",
            "So we get a very snaky model when we've got a strong correlation, we have high errors in R model, so the expected improvement goes up in other areas, and we start to do global exploration when really it's a very simple problem.",
            "We want exploitation, so it's kind of easy in this one dimensional problem.",
            "You can say, well, yes, obviously optimums there.",
            "If we're going to higher dimensions, this thing is completely exacerbated, and we can.",
            "Sometimes we would never get to the optimum.",
            "OK, again I've got a equations down here for the for what the error is and.",
            "These variances, which will come up again later.",
            "OK, so."
        ],
        [
            "We can add a regularization constant to our correlation matrix.",
            "And instead of having this ridiculously snakey plot which was the final plot of the last slide, we have this lovely smooth progression through there.",
            "OK, so looks as though everything is."
        ],
        [
            "Fine, we'll just.",
            "We'll just use this regressing model.",
            "OK, so.",
            "We start off again.",
            "And we start adding points.",
            "But then they all start to get really close together.",
            "And actually after this point here, they're sitting on top of each other and the optimization stalled.",
            "That's because the error in the regression is including the error due to noise, so there's actually an error at a sample location, so.",
            "When there's an error to sample location, and that's got a very low value, there's a chance that these could combine together to give a high expected improvement, and at this point the maximum expected improvement is actually where we've sampled, and in a discern ministik experiment, clearly there's no possible improvement there, because if we sample again will get the same answer so.",
            "And what's happening with the error equations here is, we've included this regression parameter.",
            "OK, so that's popping up in the air for the equation and the."
        ],
        [
            "The variance.",
            "So what we want to do is strip the error due to the noise in the data out of our prediction and just concentrate on the modeling error, so to speak.",
            "So we essentially re interpolate the data.",
            "It's as if we pick off predictions from the regression model and then fitting into a smooth interpolation through those.",
            "The maths all works out and you don't need to actually physically go in there and build 2 models and so on.",
            "But if you do that, it's equivalent to this method, which is essentially done.",
            "By putting this substitution into the variance, which will reduce down to standard interpolation if your regularization constants removed.",
            "So this is just a fix that you can add to creating models and what happens here is.",
            "We start off and we very similar to the interpolation to begin with, but then.",
            "We start to put a smoother model through the points.",
            "Even though there's noise there and we continue to have high expectations of improvement in this area, and we've essentially found the optimum at this point when we at this final point where it wants to go and explore a bit more, we're actually going to expect an improvement of 10 to the minus 81 there, so we've essentially the Prob."
        ],
        [
            "Be nailed by that point.",
            "OK, so just have a quick look at it in two dimensions as well.",
            "OK, so this is the same problem.",
            "But as essentially for the missing data example.",
            "But here we've played with the matter of it and made it really nice and noisy so that it's a difficult problem.",
            "Then you can see we've got all these kind of ridges here.",
            "This is a classic problem that gradient based optimizers would struggle with getting stuck in these ridges.",
            "Anyway, so."
        ],
        [
            "Will try and solve it with these three methods interpolation.",
            "You may as well be doing random updates here.",
            "The expected improvement goes absolutely everywhere.",
            "Here we got the contour of drag coefficient.",
            "So here we got a surface.",
            "Here we got contours.",
            "This is our error estimate which is just going up absolutely everywhere.",
            "And here we got the expected improvement which has high points in virtually every gap between every point.",
            "So although we've got some density of points in the lower region here, essentially we're just we've gone completely globe."
        ],
        [
            "At this stage.",
            "The regression model has stalled by this point.",
            "We've got high expected improvement right there where we've got a cluster of points, so it's not going any further, and the optimum is actually over here, so that's."
        ],
        [
            "Child the re interpolation is a little bit of a mix of the two we've got.",
            "Two clusters of points in local optimum and then this is just on the point where it actually hits the optimum there.",
            "So it's solved the problem.",
            "So both of these methods, the missing date around the noise and dealing with noise, just quite simple plug-in embellishments to the standard cruising methodology.",
            "I say creating it could be applied to any other.",
            "Surrogate model that produces some kind of error."
        ],
        [
            "Essentially.",
            "OK then, so we'll move on from single from constraint a constraint."
        ],
        [
            "Blums still looking at expected improvement, although like I said the last.",
            "Methods this can be applied to any global local info criteria that uses error estimation.",
            "OK, so.",
            "What we're going to do too?",
            "Deal with constraints in a surrogate based optimization is we're not going to use any kind of fixed penalization penalty function or anything like that.",
            "We're instead going to look at the probability that a constraint satisfied.",
            "So we're going to cast it in the same context as probability improvement, an expected improvement.",
            "So let's say that G of X is the constraint function, and then we've got F as a measure of feasibility.",
            "We calculate the probability of feasibility as that OK. And."
        ],
        [
            "It's essentially.",
            "Exactly the same.",
            "As expected improvement, except just imagine that this is a constraint limit rather than a minimum.",
            "We're just integrating the area of feasibility rather than the area of improvement."
        ],
        [
            "So exactly the same kind of maths.",
            "And then to find the constrained probability of improvement, we assume that the two the constraint an, the objective, uncorrelated and we simply just multiply the two together and then you've got a probability of improvement conditional upon constraint satisfaction.",
            "It's as simple as that, and you can multiply by three by as many constraints as you like.",
            "You can do some clever stuff with logarithms to make sure that you are.",
            "You don't start suffering from floating point."
        ],
        [
            "Under flag expected improvement exactly the same, we've got an expectation instead of a probability, we multiply through by probability of constraint satisfaction.",
            "So we've got a constraint expecting."
        ],
        [
            "Provement now again over a simple, so let's have a look at how it works.",
            "So the same 1 dimensional test function.",
            "With the current prediction, I'm not quite sure why the screen prediction is so poor here, but it will serve the purpose.",
            "I didn't look at that before I put the put the plot together OK, and let's assume that this is our constraint function and we want an area feasibilities below this constraint limit.",
            "So essentially saying that we can't achieve this optimum.",
            "Here we can go up to there or we can go up to there.",
            "OK, so this is the standard expected improvement which.",
            "Has a peak just either side of that sample point there in the region of the global optimum.",
            "Now the probability that constraint is met?",
            "Is obviously one at each of these points, because we've got a sample point there and it's net.",
            "We've got a slightly lower probability in between, even though it's pretty likely that it will be met, and it's definitely not met at that point where we've already sampled.",
            "OK, so we multiply the two together and essentially we just get a slight flattening here.",
            "In this region, saying that we and a widening of these points just deviating us away."
        ],
        [
            "From this region.",
            "So we put a point in there and it turns out that we've essentially solved the problem.",
            "And but more importantly, just to show how this works intuitively, we've now got this large flat area there where we know the constraint isn't satisfied.",
            "And as this goes on, we essentially build up Cliff edges either side of the region of infeasibility that we multiply the expected improvement by, which essentially just puts the expected improvement to 0 where you're not going to satisfy a constraint.",
            "But because we've got this probabilistic method, there's always the chance that.",
            "If your model is not quite right, you're a little bit incorrect.",
            "You can sample again to improve your model of the constraint.",
            "OK."
        ],
        [
            "So here it is in 2D, we're going to go back to the Browning function, but we've put a product constraint through here.",
            "So this is our area of infeasibility, and that's the optimum.",
            "We sample it rather fewer times here to make it into a slightly more difficult problem is round dots.",
            "Here are sample points, so just as in the 1D example.",
            "Really, definitely not going to satisfy the constraint in these regions, but it's.",
            "Here we're looking at the constraint.",
            "Expect an improvement, but away from those regions we get slightly more improvement.",
            "We get the highest improvement is where the constraint thinks that it's going to be met.",
            "The constraint model thinks that the constraint model is going to be met, and we've also got a low value down here, so we end up putting an infill point there."
        ],
        [
            "OK, and we go, and once we've done that we starting to get closer towards modeling this constraint boundary.",
            "While still we have the possibility that maybe we're wrong over in this corner, it turns out that we never go there because the probability never gets quite high enough.",
            "And after a few more updates, we've got this wonderful Cliff here.",
            "An unexpected improvement has led us down to this corner."
        ],
        [
            "So it really works quite nicely.",
            "OK, so going on tomorrow."
        ],
        [
            "Objectives so we've had a few talks on multi objective optimization and everybody here is quite clear about what a peretto front is that we've essentially got a set of nondominated solutions, and these define Apparate frontier and we want to go.",
            "Down in that direction.",
            "So we want to look.",
            "At the possibility of improved instead of expected improvement, we want to look at the expectation of improving.",
            "On all of these designs at once OK.",
            "So."
        ],
        [
            "Rather than this probability density function that we had the expected improvement, this one dimensional probability density function.",
            "We've now got a 2 dimensional probability density function because we're looking at two objectives so.",
            "A probability looks like that this being our first objective, this being our second objective, so I wasn't able to come up with a plot that was really clear enough to demonstrate this.",
            "But imagine the probability of improvement plots and cost those in two dimensions in your mind and start integrating that sort of three dimensional picture below the Pareto front and that's it."
        ],
        [
            "What we're doing.",
            "OK so I won't take it up.",
            "These integrals get incredibly long and you could just refer to an Deakins paper on this or our book, which I'll give you a reference to at the end so I won't put all the maths up here, but essentially if we just consider one point on the pressure front here we want to integrate that section, that section and this section.",
            "And that's that integral there, and we then need to extend that."
        ],
        [
            "To the whole porretta front.",
            "And that's the integral there.",
            "We've only done this for two objectives.",
            "Clearly, you could extend this to more objectives, but I don't fancy doing all the integration.",
            "Maybe we'll throw it at Maple or do it numerically.",
            "I don't know, but it's there's no reason why it can't be extended other than not finding somebody who's willing to go to do it.",
            "That's just the probability of improvement.",
            "As you remember from the single objective case, the expected improvement is the first moment of the probability of improvement about the best design so far.",
            "So expected improvement is the first moment again here and the math to get there is a little bit too big to fit on this slide, but there's nothing.",
            "There's nothing complicated about it, it's just integration by parts, and it fills up a lot of space."
        ],
        [
            "But do please refer to the paper.",
            "So just a little toy example to show this and how just as expected improvement has this global local exploitation exploration trade off, so does this so we can find disjointed regions of parameter optimal solutions, and we can do it very fast here.",
            "We've got the same old function as our first objective and I've just distorted that slightly to give us our second objective.",
            "So we've got this clear region of.",
            "Prayer to optimal solutions that the expected improvement update strategies found initially, and they're showing there."
        ],
        [
            "Run this on a little bit further.",
            "And we escape this local optimum over here and we get another solution there.",
            "So our initial solutions are down there now and we've got this other area approach to optimal solutions up here.",
            "Well, one solution so far we could continue going and fill in these areas are a little bit more."
        ],
        [
            "OK, so that's pretty much the end of the talk, but I'm just going to show you this actually working and hopefully."
        ],
        [
            "Run a little bit of the Matlab code.",
            "While we maybe have some questions, all of the code that implements these.",
            "Methods is available on the Wiley website and goes alongside out book.",
            "But you know, you don't need to buy the book to download the code, and I don't get much money if you do download.",
            "If you do buy the book, so don't feel obliged to, but it may help.",
            "OK, so you won't go through this, but essentially you've got a short piece of Matlab code which is mainly plotting out the results.",
            "It.",
            "Hang on a minute."
        ],
        [
            "Let me just go back to the problem definition.",
            "I won't go back into the slide slideshow, so the problem that we're going to look at is the nowacki being problem.",
            "Hopefully you know some of you have seen this before.",
            "It's a classic problem.",
            "They think the papers in this was written in the 70s or 80s where we're going to optimize a fixed length steel cantilever beam under 5 kilonewtons of no load and the variables are the height and width of the beam.",
            "We're going to try and minimize cross sectional area.",
            "Either wait and also the bending moment and we've got an area ratio.",
            "Bending moment, buckling deflection, shear constraint.",
            "So we got five constraints, two objectives.",
            "OK."
        ],
        [
            "We're going to start with 10 point optimal Latin hypercube, so that's really not that many, and we've got a cricket model of each objective constraint.",
            "We're going to tune the parameters of the rigging models with a genetic algorithm, followed by an SQP which are going to be very fast.",
            "'cause we've got the advent of the likelihood in just two dimensions.",
            "You don't get much there, but if you start to get ten 2050 dimensions, the joint really comes in.",
            "And then we're going to add 20 points at the maximum of the constrained multi objective expected improvement.",
            "So I just go back to the code.",
            "Uh."
        ],
        [
            "We've got don't expect any amazing visual stuff here.",
            "It's just going to be a lot, OK?",
            "It's quite a simple setup.",
            "We've got our calculate Arab.",
            "A sampling plan here.",
            "One line of code there.",
            "Anne.",
            "We then put in our constraint limits.",
            "We then cycle through and Calculator to objectives and all of our constraints.",
            "This section here tunes creaking models.",
            "Then going to vast amount of plotting.",
            "Which you can just skip out if you're just running it and then we.",
            "We're going to search essentially, so I'll just get that running.",
            "And we can have that going in the background."
        ],
        [
            "Because this is optimizing the sampling plan so reasonably clunky process, but works very well.",
            "It's the same Methodism.",
            "Dawkins uses in the blind watchmaker to make his little insects."
        ],
        [
            "Tuning the models is done in 6.5 seconds there.",
            "This is the initial front red solutions of violated the constraints.",
            "Green ones haven't, and the blue ones that are highlighted porretto optimal."
        ],
        [
            "Are the ones now that violated the constraints?",
            "'cause otherwise the preference just hidden in the corner and as it cycles through it's just going to go through an add up add 20 updates?",
            "So that could potentially be going in the background.",
            "While there are any quest."
        ],
        [
            "The first one would be.",
            "I mean, do you guys have any important performance on more rigorous mathematical benchmarks like the entire GCS?",
            "Or if your document multi objective with CTS problems in DTS DTS not only test problems ascentia Leon this test problem and on aerodynamic design problem.",
            "I suppose we never really.",
            "Well, the methods being developed and we're using it for our own purposes in engineering design rather than pitching it.",
            "I suppose against other multi objective methods, but maybe we should try try some test problems, yeah?",
            "The comment right under question when your simulation input fails.",
            "Yeah, I think that's quite serious.",
            "Opinion I occasionally saying you were gonna penalize your pale face simulations.",
            "It may.",
            "It may not work all the time.",
            "In your example of notice that most of your failures until he was away from your optimal point where minimum drag was, so we were lucky that you have a lot of failures outside video where the optimum is.",
            "If you had a mixture where the failures are crazy surrounding your ostropol optimal point, you would have distorted your designer space quite badly.",
            "And depending how fitting procedure, you're never gonna get done so well, that's what's good about the method, because the penalization changes from update to update.",
            "So if you're very near the optimal region where you've got more sampling, there's not much error in the model, so you only add a very small penalization, and it will allow you to actually drive up to the very border of the region of infeasibility.",
            "So it's kind of it's adaptive in the way that it penalizes.",
            "So in.",
            "One of the problems that we had, we were quite quite close to the region of infeasibility and.",
            "You're kind of on a downhill trend.",
            "As you went into in infeasibility, so it performed quite well there, and it's performed up to sort of 90% regions of infeasibility.",
            "But yeah, it will always be a problem that could could catch any of these systems out, but so far it's been pretty good.",
            "Can I have one more question?",
            "I mean looking at this plot coming, yeah."
        ],
        [
            "Exactly how many function evaluations has it done?",
            "At this point, it's.",
            "It's going to."
        ],
        [
            "2.",
            "40 in total.",
            "40 in total when it's finished.",
            "So I'll show you a plot of them.",
            "It's only going to maybe three of which will violate the constraint and not that many of which are sub optimal.",
            "I mean it's not a, it's not an incredibly hard problem."
        ],
        [
            "Comments I have about this is the you can see very clearly the diversity of the front that you're getting.",
            "Injuring application is fine.",
            "Yeah, you can say you have covered it, but the diversity definitely is not.",
            "Anywhere close to what you would expect.",
            "Up and this is not a difficult problem such I mean.",
            "That's the thing we've only we've only given it 40 something.",
            "Definitely look into, even if you allow it for long.",
            "Is there any reason to believe it will have a proper diversity?",
            "Because the because?",
            "The way they expect improvements calculated.",
            "It's taking a distance from the Proto front such that it will tend to look to fill in gaps.",
            "It will be much more rigorous if you really go into the performance metrics of testing a multiobjective algorithm.",
            "I mean there are metrics like IGB, which is basically generational distance.",
            "Of course, for this problem you won't have a true understanding of the parental frontier, but if it's a mathematical benchmark, you exactly know better than point is only parity or not.",
            "What is the hypervolume?",
            "What so those metrics?",
            "If you can quantify?",
            "It would be a much stronger case.",
            "Maybe not in 40, maybe 120.",
            "I've achieved excellent diversity and it is on the front, yeah?",
            "Which is quite interesting stuff.",
            "Look at, yeah, yeah, yeah, it would.",
            "No doubt it would be good to push it a bit further and validated against other methods as well.",
            "But anyway, so just before we have any more questions, if there were any, this is the final front here and those are the.",
            "Constraint violation points, so we've not run for very long, and there's hardly any.",
            "That's hardly any data there, but the data that is there.",
            "There are quite a few points on the Proto Front.",
            "We've hardly had any subsequent constraint violations, and it's really performed very well.",
            "But yeah, more more validation needed probably.",
            "So just to.",
            "Conclude."
        ],
        [
            "Anne.",
            "Not really far wide, reaching conclusions or necessarily particularly applicable to this talk, but just some thoughts perhaps.",
            "Surrogate modeling doesn't necessarily offer all the answers, but it can definitely offer ways around a lot of the problems that you'll face in optimization.",
            "When we talk about just inputing values and smoothing and so on, it sounds like we're just hammering away with a blunt tool, but there are a lot of traps that you can fall into, particularly with the tuning of the models and so on and so forth, assuming and assuring that you get a global.",
            "You move towards the global optimum and have a good exploration exploitation tradeoff.",
            "In multi objective context, I think it's particularly promising given the huge number of evaluations that you often see in multiobjective design.",
            "And also just to note that I haven't covered newer surrogate methods such as blind cragging, which is really promising where instead of just saying well I'm going to add bumps to and from a mean fit.",
            "I'm going to try to represent the function using some sort of polynomial terms 1st, and then add bumps to that, so you've kind of got the best of both worlds.",
            "It's sort of a thinking man's ensemble maybe.",
            "Multi Fidelity modeling.",
            "I haven't covered that combining different levels of Euler rans codes maybe, or with experimental data, that kind of thing and also yeah further exploit enhancements to expect improvement in terms of improving exploitation exploration tradeoffs and combating this problem with poor parameter estimation.",
            "When you've got a sparsity of data.",
            "Anne.",
            "There's a few references here."
        ],
        [
            "Yeah.",
            "I mean, yeah, somebody else put up a whole list of books that you might want to refer to.",
            "I'm not going to go into asking people to look at other people's work, it's all mine.",
            "Now we've got a book out on it.",
            "Try the code out.",
            "It's on the Wiley website.",
            "If you don't want to buy the book, there's a review paper in progress in aerospace Sciences which covers quite a lot of."
        ],
        [
            "Stuff, and just in case you still don't know which book to buy, it's that one or that one.",
            "OK, that's me done.",
            "Improvement is more concerning this location than this location.",
            "I was wondering how you make a tradeoff between the two components of working for them at the impression that it's more related to installation.",
            "No, I wouldn't say so.",
            "If you look at the 1st.",
            "The first plot I had for that.",
            "I could almost say quite the contrary.",
            "OK, so that's the probability of improvement.",
            "And that's the expected improvement.",
            "If we go back to the prediction.",
            "The probability of improvements very high where?",
            "We're really, really close to where we previously sampled, but if the surface just moved ever so slightly, you would have some improvement.",
            "So high probability, expected improvements would maybe.",
            "Improvement is not related to the uncertainty of Liberty.",
            "It is related to the uncertainty of projection.",
            "Let me just try and think of how to.",
            "If you've got a, you've got a very narrow.",
            "With your distribution there, but most of it is below the minimum, so you've got a high probability distribution.",
            "You've got a wide, so you've got a high probability of improvement here.",
            "It's very wide.",
            "But only a small amount of it is below the best value so far.",
            "So you're talking about proportions of this that are below the best value so far, and so probability improvement will tend to favor going close to previous points maybe, but it's very difficult, so once you start blowing up the dimensions of the problem, you're just wandering around in the dark, really.",
            "Any questions, so your first assumption of advocating circuit model is that the surface is smooth, yes?",
            "There he is well.",
            "You can if you go back to the correlation.",
            "In this work I've set this to two here, but we can vary that normally between one and two in engineering and geostatistics, they'll take it down to zero, and that means that you can have near cliffs in your correlation.",
            "But yeah, people use everybody uses their own correlation.",
            "I can't remember who it was, but somebody yesterday set that as constant for all variables, so essentially kind of radial basis function, which I find doesn't work so well.",
            "I stick with P2.",
            "Some people say.",
            "How could you do that?",
            "Also, if you've got serious discontinuities, you can Patch together a number of surrogates.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about surrogate based optimization, in particular global optimization and when it's applied to real world problems and getting around the problems that we're faced with when we actually do optimization for real.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm going to start off with some of the basics.",
                    "label": 0
                },
                {
                    "sent": "Probably I'll just be teaching you all stuff you already know, but maybe a bit of a refresher or some of you will suddenly think.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's.",
                    "label": 0
                },
                {
                    "sent": "That's what it's all about anyway, so I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "The basic idea, then Gaussian process based modeling or rigging whatever you want to call it and then go on to talk about some infill criteria used to enhance surrogate models during optimization, probability improvement and expected improvement.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to go on to the real stuff, which is how to cope with when you have design evaluations fail, leading to missing data in your optimization, noisy data, which I'm sure a lot of you are faced with.",
                    "label": 0
                },
                {
                    "sent": "And then go into constraints and multi objectives and then if.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is time an example at the end?",
                    "label": 0
                },
                {
                    "sent": "OK then, so lots of people see surrogate based optimization in different ways and this is the way we do it at Southampton.",
                    "label": 1
                },
                {
                    "sent": "We well naturally need some idea of the problem to begin with, based on some preliminary experiments to look at how different design variables affect the problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe, and just get an idea of what's going on.",
                    "label": 0
                },
                {
                    "sent": "After that, we'll need some sample data.",
                    "label": 0
                },
                {
                    "sent": "So we'll perform some kind of formal sampling plan.",
                    "label": 1
                },
                {
                    "sent": "Roll.",
                    "label": 0
                },
                {
                    "sent": "Collect our observations, IE run computational fluid dynamics or whatever it is we're doing.",
                    "label": 0
                },
                {
                    "sent": "Then construct the surrogates.",
                    "label": 0
                },
                {
                    "sent": "I won't go into these details today, but at this point we want to think about whether we had design sensitivity.",
                    "label": 0
                },
                {
                    "sent": "Sensitivity is available whether we're going to have gradient enhanced model.",
                    "label": 0
                },
                {
                    "sent": "Have we got multi Fidelity data?",
                    "label": 1
                },
                {
                    "sent": "So is it going to be a multi level model?",
                    "label": 0
                },
                {
                    "sent": "Once the surrogate's constructed, we then go on to search the model and then update it using additional simulations and at this point.",
                    "label": 0
                },
                {
                    "sent": "We want to think about whether we've got constraints, whether we've got noise, whether it's multiple objectives that we're dealing with, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we go around this loop.",
                    "label": 0
                },
                {
                    "sent": "Until we get bored, run out of money, reach a convergence criteria, whatever.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Contrary to what I think some researchers think we're not too interested in the global accuracy of the surrogate, as long as it's accurate where it needs to be.",
                    "label": 1
                },
                {
                    "sent": "If we make a really accurate surrogate in the area of poor designs, that's just time wasted.",
                    "label": 0
                },
                {
                    "sent": "Really, as far as we are concerned.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process modeling.",
                    "label": 0
                },
                {
                    "sent": "I'll just patiently go through at the beginning of that flow chart.",
                    "label": 0
                },
                {
                    "sent": "We start off with a sample plan.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We're going to try and predict this function, which the branding function and we're going to predict based on 20 pieces of information shown by the black dots there.",
                    "label": 0
                },
                {
                    "sent": "That's an optimized Latin hypercube sampling plan.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we're going to correlate all the points with each other using this correlation here.",
                    "label": 1
                },
                {
                    "sent": "And this is what it's the whole models based around.",
                    "label": 0
                },
                {
                    "sent": "Really, this is a popular correlation used in kriging.",
                    "label": 1
                },
                {
                    "sent": "We tend to set this thing here to two, which makes it a Gaussian and.",
                    "label": 0
                },
                {
                    "sent": "It's this theater here, which the whole thing hangs off, and getting that activity parameter correct.",
                    "label": 0
                },
                {
                    "sent": "Which really leads to the kriging model predicting the data well, and this parameter here essentially affects the width of our correlation functions and you see on this plot on the right there how we go from a very low theater, not .1.",
                    "label": 0
                },
                {
                    "sent": "A correlation hardly drops off as we move points apart.",
                    "label": 0
                },
                {
                    "sent": "I as X -- X moves away from zero.",
                    "label": 0
                },
                {
                    "sent": "If we've got a higher theater, we have a sharply drop dropping off correlation, so our data doesn't have much to do with other data, so our cricket model is going to be a bit more lumpy OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We get how each of our 20 sample points are correlated with all the other sample points and that leads essentially to these 20 Gaussian bumps with the widths determined by these theater parameters, they've got two Theta parameters, so their non axisymmetric bumps OK so.",
                    "label": 1
                },
                {
                    "sent": "And we choose the widths based on maximizing the likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "So we've got this likelihood maximization embedded.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With all of this.",
                    "label": 0
                },
                {
                    "sent": "Then we multiply these bumps by weightings so they no longer scale between zero and one.",
                    "label": 1
                },
                {
                    "sent": "Some go up, some go down.",
                    "label": 0
                },
                {
                    "sent": "We had them all together.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we got our cricket prediction.",
                    "label": 0
                },
                {
                    "sent": "Which is very similar to the true function here, so it's this kind of modeling that everything that I'm going to go on to show you in the talk is based around this way of predicting engineering functions.",
                    "label": 0
                },
                {
                    "sent": "It's quite a flexible method, which is essentially only based upon the function being smooth and continuous, so it's really quite useful for engineering problems which tend to be so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right then, so we've got a model we want to do some optimization with it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'll show you why we like using tricking.",
                    "label": 0
                },
                {
                    "sent": "By just taking you through a couple of examples, where are we to use polynomial regression and where are we being particularly horrible to people who use polynomial regression, which are an example like this where we're trying to optimize this function here?",
                    "label": 0
                },
                {
                    "sent": "OK, and we sample at three points.",
                    "label": 0
                },
                {
                    "sent": "And we fit our polynomial through that second order and then we try to research this and we find the optimum and we update the model there and we keep on doing that and we don't.",
                    "label": 0
                },
                {
                    "sent": "We never get anywhere because clearly this shape isn't a second order polynomial and I matter how much we throw data at the optimum.",
                    "label": 0
                },
                {
                    "sent": "We're never going to distort it and actually lead lead towards the global optimum so.",
                    "label": 0
                },
                {
                    "sent": "Maybe want to use some kind of interpolating model?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "EG creeking radial basis function.",
                    "label": 0
                },
                {
                    "sent": "Who knows what?",
                    "label": 0
                },
                {
                    "sent": "So we do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We start off with a very similar prediction, but when we add data to the minimum here.",
                    "label": 0
                },
                {
                    "sent": "At this point there.",
                    "label": 0
                },
                {
                    "sent": "The prediction distorts and goes through the data, and as we add more and more data, we're guaranteed to converge to a local optimum.",
                    "label": 0
                },
                {
                    "sent": "Here using this method.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But what if we decide we're going to be horrible to people who use radial basis function and kriging prediction as an infill strategy?",
                    "label": 0
                },
                {
                    "sent": "Well safe.",
                    "label": 0
                },
                {
                    "sent": "This function was particularly misleading like this.",
                    "label": 0
                },
                {
                    "sent": "That previous strategy.",
                    "label": 0
                },
                {
                    "sent": "Would just end up at this local optimum here, but we want to explore the model more and I eventually find this one here.",
                    "label": 0
                },
                {
                    "sent": "OK, but Gaussian process models are quite use.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we can extract some kind of information as to what the error in the model might be.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this same prediction.",
                    "label": 0
                },
                {
                    "sent": "And then calculate the error that are Gaussian process model predicts.",
                    "label": 0
                },
                {
                    "sent": "We get this so just it's completely intuitively.",
                    "label": 0
                },
                {
                    "sent": "Basically it just goes up in the gaps between the data and the rate that it goes up at is dependent upon that feature parameter which we've estimated using maximum likelihood estimation.",
                    "label": 0
                },
                {
                    "sent": "OK, so clearly if we were going to add some points at the maximum error in the prediction we get over to this point very soon, but that would be a just complete exploration.",
                    "label": 0
                },
                {
                    "sent": "We want some kind of exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use this error to produce some exploitation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operation tradeoff metrics.",
                    "label": 0
                },
                {
                    "sent": "So this is a key graph and understanding probability of improvement and expected improvement.",
                    "label": 1
                },
                {
                    "sent": "So what I've done is I've taken the same prediction as before.",
                    "label": 0
                },
                {
                    "sent": "We talk the same function we're trying to predict.",
                    "label": 0
                },
                {
                    "sent": "To optimize this is the prediction here and then what we've done is plotted.",
                    "label": 0
                },
                {
                    "sent": "The distribution that the prediction might take at this point here.",
                    "label": 0
                },
                {
                    "sent": "So the mean of this distribution is the prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, that's basically what the gathering process model is.",
                    "label": 0
                },
                {
                    "sent": "It's it's the mean of sort of window that the prediction might fall into.",
                    "label": 0
                },
                {
                    "sent": "So, but there's also a finite probability that the prediction could take values up and down.",
                    "label": 0
                },
                {
                    "sent": "Here, that probability given by this distribution that's plotted here.",
                    "label": 0
                },
                {
                    "sent": "Now if we draw a line from the minimum value found so far.",
                    "label": 0
                },
                {
                    "sent": "And then integrate below that line and below the probability distribution we get the probability of improvement.",
                    "label": 0
                },
                {
                    "sent": "OK, now the expected improvement is the first moment of this area.",
                    "label": 0
                },
                {
                    "sent": "About the best point so far, so that's a sort of intuitive visual way of understanding probability of improvement and expected improve.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I've included equations down here just for those people who are that way inclined, but.",
                    "label": 0
                },
                {
                    "sent": "Hopefully the talk should should follow a sort of intuitive progression.",
                    "label": 0
                },
                {
                    "sent": "Anyhow, so this is what we get if we plotted the probability of improvement for that last prediction.",
                    "label": 1
                },
                {
                    "sent": "So we had a real cluster of points around this area, but so it's clear that we're not going to get very much more improvement in that area.",
                    "label": 0
                },
                {
                    "sent": "But if I was going to get my house on it, I'd probably say, yeah, maybe we'll get a tiny bit of improvement there.",
                    "label": 0
                },
                {
                    "sent": "So the probability of getting any improvement at all is very high.",
                    "label": 0
                },
                {
                    "sent": "But there's also some probability that there will be improvement over in the other area where we had sparse sampling, so it's not a measure of how much improvement will get, it's just a measure of the probability that there will be some improvement.",
                    "label": 1
                },
                {
                    "sent": "Expected improvement, on the other hand.",
                    "label": 0
                },
                {
                    "sent": "Actually tells you how much improvement you might get, so these values here correspond to how much improvement the statistics thinks is possible in the model tends to be a real under estimator.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that too much in this talk, but it's something to bear in mind, and I suppose an open air of research to try and get the expected improvement measures to be a bit more accurate.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so this is the rest of the talk is essentially based around using permutation of the expected improvement criterion, because this really nice tradeoff between local exploitation and global exploration.",
                    "label": 0
                },
                {
                    "sent": "And as I said here can be extended to constrained multi objective.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blums OK before I go on there just a little demo of how it actually pans out, applying expected improvements that first problem.",
                    "label": 0
                },
                {
                    "sent": "We start off with a pretty rubbish prediction.",
                    "label": 0
                },
                {
                    "sent": "We've got reasonably even expected improvement on each side.",
                    "label": 1
                },
                {
                    "sent": "The search tends over to this side and we start to follow a similar route to the exploitation only update strategy.",
                    "label": 0
                },
                {
                    "sent": "What happens here is we really do over exploit this area and that's something to do with the theater parameters not being estimated particularly well with such a sparsity of data.",
                    "label": 0
                },
                {
                    "sent": "But in any case, once we've.",
                    "label": 0
                },
                {
                    "sent": "Hammered away and found this local optimum.",
                    "label": 0
                },
                {
                    "sent": "Here we start to see these expectation of improvement popping up on this side, and we reached the global optimum in the end and under certain mild assumptions you can prove that this method will always converge towards the global optimum.",
                    "label": 1
                },
                {
                    "sent": "Although to be honest, with the engineering design, we're not too worried about asymptotically convergence proofs, although it's nice to have them if they're not there.",
                    "label": 0
                },
                {
                    "sent": "Maybe you start to worry.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what if we were going through an expected improvement update strategy, or indeed any surrogate model based update strategy and we came across missing data.",
                    "label": 0
                },
                {
                    "sent": "So say your matching fails.",
                    "label": 0
                },
                {
                    "sent": "Your flow simulation diverges.",
                    "label": 0
                },
                {
                    "sent": "Some of these problems.",
                    "label": 0
                },
                {
                    "sent": "You might want to go back and try and really work the problem hard and get an answer there, but in other cases you may not have access to the code.",
                    "label": 0
                },
                {
                    "sent": "You might not be the expert, so you just need to basically consider the missing data as regions where of unknown constraint violations.",
                    "label": 0
                },
                {
                    "sent": "So you've got boundaries that you have no way of calculating.",
                    "label": 0
                },
                {
                    "sent": "You've just got places where.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you you're running through your update strategy, you get your surrogate model.",
                    "label": 0
                },
                {
                    "sent": "You put a new piece of data in there.",
                    "label": 0
                },
                {
                    "sent": "That modifies the surrogate and then you go around the process again.",
                    "label": 0
                },
                {
                    "sent": "If no piece of data is forthcoming, you're stuck.",
                    "label": 0
                },
                {
                    "sent": "You have to do something, otherwise the optimization will stall.",
                    "label": 0
                },
                {
                    "sent": "So maybe you could just think, well, let's add a random point that will just perturb the model and then we can go on as normal, hopefully will be OK, we won't hit that problem again, or maybe something that might be a little bit more intelligent would be too imputed value there that was equal to the prediction.",
                    "label": 0
                },
                {
                    "sent": "That you've made, so you would just say well, for the time being, let's just assume we're correct in our prediction there.",
                    "label": 0
                },
                {
                    "sent": "And that would mean that you would never have another expected improvement there, because you can't have an improvement where you already know the value, OK?",
                    "label": 0
                },
                {
                    "sent": "So that might be a good idea.",
                    "label": 0
                },
                {
                    "sent": "Better idea we think would be to penalize.",
                    "label": 0
                },
                {
                    "sent": "The value at that point somehow.",
                    "label": 0
                },
                {
                    "sent": "So you just sort of distort your surrogate model upwards there, assuming you're minimizing so that you tend to not visit that region again.",
                    "label": 0
                },
                {
                    "sent": "So ascentia, what we've done is we've taken the prediction plus the mean squared error in the prediction and imputed those values where we have.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Failures and I'll show you some results of a problem that we've applied that too, so it's a 2D airfoil optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This is a basic naecker aerofoil here.",
                    "label": 0
                },
                {
                    "sent": "And then we've got one as orthogonal basis functions, which are these various functions that are detailed on the side here, which can be added with various weightings to this airfoil to change its shape.",
                    "label": 0
                },
                {
                    "sent": "And it's a very powerful way of optimizing this transonic airfoil.",
                    "label": 0
                },
                {
                    "sent": "It turns out the parameterization and so on is really not the point here.",
                    "label": 0
                },
                {
                    "sent": "We've got an error for problem, and that's what it is.",
                    "label": 0
                },
                {
                    "sent": "Goal two of these shape functions, which essentially alter the camber and thickness towards the rear of the airfoil.",
                    "label": 0
                },
                {
                    "sent": "We're using a potential flow solver, which is the S do VGC solver and we get approximately 35% failure rate.",
                    "label": 0
                },
                {
                    "sent": "UTEP apps very occasionally the upper and lower surface will of the error will actually cross something that we could figure out.",
                    "label": 0
                },
                {
                    "sent": "You know if we went into parameterization a little bit more deeply, but most of the problems where you get separation and because the error can't actually make the lift produced, the lift that we want it to.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start off this problem with 20 point optimal Latin hypercube design exactly the same way as I did with rounding function.",
                    "label": 0
                },
                {
                    "sent": "At the beginning we do maximum expects improvement updates until within one drag count of the optimum because it's a cheap problem, we know what the optimum is so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is all being done with hindsight.",
                    "label": 0
                },
                {
                    "sent": "It's a nice test.",
                    "label": 0
                },
                {
                    "sent": "So we try these different strategies.",
                    "label": 0
                },
                {
                    "sent": "Here we try the random update strategy, which is when we hit a failure.",
                    "label": 0
                },
                {
                    "sent": "We just take a random point.",
                    "label": 0
                },
                {
                    "sent": "We try and impute value.",
                    "label": 0
                },
                {
                    "sent": "The predicted value at the point of failure and we also try to impute the predicted value plus the mean squared error at a failure, and then we've also tried a genetic algorithm random search just to check that we're not way out.",
                    "label": 0
                },
                {
                    "sent": "And so I've got to actually go through the results.",
                    "label": 0
                },
                {
                    "sent": "Here we are so.",
                    "label": 0
                },
                {
                    "sent": "All have basically the same degree of failure to begin with before we start doing our infill strategy.",
                    "label": 0
                },
                {
                    "sent": "Then, as you'd expect, random update.",
                    "label": 0
                },
                {
                    "sent": "Actually you don't get any improvement.",
                    "label": 0
                },
                {
                    "sent": "You're still tending to get as many failures afterwards.",
                    "label": 0
                },
                {
                    "sent": "Just computing the prediction you do slightly better.",
                    "label": 0
                },
                {
                    "sent": "The penalized imputations, though, do incredibly well.",
                    "label": 0
                },
                {
                    "sent": "And then you can see the total number of simulations, including failures, until we reach the optimum.",
                    "label": 0
                },
                {
                    "sent": "This penalized imputation method really wins out.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just in case you haven't really quite followed it, I've got this in pictures here.",
                    "label": 0
                },
                {
                    "sent": "This is our design space essentially.",
                    "label": 0
                },
                {
                    "sent": "That's the region where we can simulate.",
                    "label": 0
                },
                {
                    "sent": "That's our coefficient of drag.",
                    "label": 0
                },
                {
                    "sent": "That's a first design variable.",
                    "label": 0
                },
                {
                    "sent": "Our second design variable, so we're trying to find the optimum of this surface, which we've been able to compute.",
                    "label": 0
                },
                {
                    "sent": "At Priory, because it's such a cheap problem, so we take a 20 point sampling plan and the black ones are the ones that have succeeded.",
                    "label": 0
                },
                {
                    "sent": "The red ones are the ones that have failed.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's our starting point and the green points are the updates, so all our green points.",
                    "label": 0
                },
                {
                    "sent": "The three that it's taken to find the optimum are down in the bottom of this basin.",
                    "label": 0
                },
                {
                    "sent": "This course mesh here is our original kriging prediction and the fine mesh is the distorted kriging prediction because we've penalized these points around the edges so we just make this basin.",
                    "label": 0
                },
                {
                    "sent": "In which our optimal feasible designs are and underneath are also contours of the expected improvement, and this shows how this method is is a global method as well 'cause although we've got high expected improvement right here in the obvious basin there we also maintain.",
                    "label": 0
                },
                {
                    "sent": "The vague possibility that we could go and explore outside.",
                    "label": 0
                },
                {
                    "sent": "So if you just left this running, it would keep firing points off into the corners where you weren't sure exactly what the optimum values were.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've been extended this to higher dimensions, not too high, only to four so that we can still go on visualizing it here.",
                    "label": 0
                },
                {
                    "sent": "We've got 82% failure rate, so you know really quite rubbish problem set up here, but sometimes you have to deal with these things.",
                    "label": 0
                },
                {
                    "sent": "And again, I won't go through the results too much, but we've got.",
                    "label": 0
                },
                {
                    "sent": "308 there compared to much higher values here.",
                    "label": 0
                },
                {
                    "sent": "This predictor imputation is not AM.",
                    "label": 0
                },
                {
                    "sent": "A great one because it we didn't actually managed to reach the optimum for some of the runs, so that's just the best.",
                    "label": 0
                },
                {
                    "sent": "It did not on average, so penalized imputation wins out.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a picture of that process as well, so I don't know if any of you well at least one of you here seen these kind of pictures before.",
                    "label": 0
                },
                {
                    "sent": "This is a hierarchical axis technique plot where each one of these tiles is a 2D design space.",
                    "label": 0
                },
                {
                    "sent": "So if I flip back up.",
                    "label": 0
                },
                {
                    "sent": "If you remember what that looks like.",
                    "label": 0
                },
                {
                    "sent": "Looking down on it.",
                    "label": 0
                },
                {
                    "sent": "That's kind of like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've got our first and second aerofoil variables bearing inside each tile.",
                    "label": 0
                },
                {
                    "sent": "And then third and 4th variables vary from tile to tile, so if you stand back and squint a little bit, you can see the drag varying across this 4 dimensional design space.",
                    "label": 0
                },
                {
                    "sent": "The blue dot regions or where there's a geometry violation, upper and lower surfaces of crossed over so.",
                    "label": 0
                },
                {
                    "sent": "In effect, we could discount those regions anyway.",
                    "label": 0
                },
                {
                    "sent": "The white areas where we don't really know what's happening with the simulation, it just failed, so we've thrown.",
                    "label": 0
                },
                {
                    "sent": "200 points at this black for the initial sampling plan, the black ones are initial sampling points that succeeded.",
                    "label": 0
                },
                {
                    "sent": "The red ones, one that failed.",
                    "label": 0
                },
                {
                    "sent": "Now, even in such a way, there's 82% of failures possible in our subsequent infills strategy.",
                    "label": 0
                },
                {
                    "sent": "Only these crosses here.",
                    "label": 0
                },
                {
                    "sent": "Well, the ones that failed so we got one up there and a few down here and the.",
                    "label": 0
                },
                {
                    "sent": "Green dots here are the ones at the infill points that have actually succeeded and have finally ended up finding this optimum here.",
                    "label": 0
                },
                {
                    "sent": "So quite simple method, but quite powerful when you're faced with these problems where you don't really know why it's failing, you don't have access to the source code.",
                    "label": 0
                },
                {
                    "sent": "It comes in useful.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK then, so as well as missing data will almost certainly have noise in your data.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most datasets are corrupted by noise.",
                    "label": 0
                },
                {
                    "sent": "Here I'm interested in deterministic noise and I think most of you here will be interesting that as well, so noise would normally mean noise you get from an experiment from human error, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "That's it, repeatable, deterministic, deterministic noise means essentially error that looks kind of random, but isn't because if you went back and did the simulation again, you get the same answer, but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "It can be treated a bit like noise when you're fitting models, so.",
                    "label": 0
                },
                {
                    "sent": "Here we've got the same problem, but I'm just varying one.",
                    "label": 0
                },
                {
                    "sent": "Airfoil design variable, and this is the effect that it's having here.",
                    "label": 0
                },
                {
                    "sent": "You start with a low camber there and go to very high camber.",
                    "label": 0
                },
                {
                    "sent": "I don't need to point out that these are skewed axis here.",
                    "label": 0
                },
                {
                    "sent": "It's not actually that fat.",
                    "label": 0
                },
                {
                    "sent": "This is a thickness to chord of 12% I think, so I'm just screwed them up.",
                    "label": 0
                },
                {
                    "sent": "Put them on the plot anyway, so this is the drag.",
                    "label": 0
                },
                {
                    "sent": "Very the courage and drag varying and we have this noisy response here which are gradient based.",
                    "label": 0
                },
                {
                    "sent": "Optimiser would really struggle with using surrogate models.",
                    "label": 0
                },
                {
                    "sent": "We can smooth this out.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll go on and show a few problems with doing that.",
                    "label": 0
                },
                {
                    "sent": "First of all, let's just look at.",
                    "label": 0
                },
                {
                    "sent": "If you were going to interpolate the data, so we start off with four sample points and we do a maximum expected improvement based infill strategy.",
                    "label": 0
                },
                {
                    "sent": "And it all goes kind of nicely to begin with until point start bunching up here.",
                    "label": 0
                },
                {
                    "sent": "And then finally we have this really snaking plot there.",
                    "label": 0
                },
                {
                    "sent": "And because the plots becoming very snaky or well because the data is becoming difficult fit, this feature parameter goes up because the correlation in the data isn't so strong.",
                    "label": 0
                },
                {
                    "sent": "So we get a very snaky model when we've got a strong correlation, we have high errors in R model, so the expected improvement goes up in other areas, and we start to do global exploration when really it's a very simple problem.",
                    "label": 0
                },
                {
                    "sent": "We want exploitation, so it's kind of easy in this one dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, yes, obviously optimums there.",
                    "label": 0
                },
                {
                    "sent": "If we're going to higher dimensions, this thing is completely exacerbated, and we can.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we would never get to the optimum.",
                    "label": 0
                },
                {
                    "sent": "OK, again I've got a equations down here for the for what the error is and.",
                    "label": 0
                },
                {
                    "sent": "These variances, which will come up again later.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can add a regularization constant to our correlation matrix.",
                    "label": 0
                },
                {
                    "sent": "And instead of having this ridiculously snakey plot which was the final plot of the last slide, we have this lovely smooth progression through there.",
                    "label": 0
                },
                {
                    "sent": "OK, so looks as though everything is.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fine, we'll just.",
                    "label": 0
                },
                {
                    "sent": "We'll just use this regressing model.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We start off again.",
                    "label": 0
                },
                {
                    "sent": "And we start adding points.",
                    "label": 0
                },
                {
                    "sent": "But then they all start to get really close together.",
                    "label": 0
                },
                {
                    "sent": "And actually after this point here, they're sitting on top of each other and the optimization stalled.",
                    "label": 0
                },
                {
                    "sent": "That's because the error in the regression is including the error due to noise, so there's actually an error at a sample location, so.",
                    "label": 0
                },
                {
                    "sent": "When there's an error to sample location, and that's got a very low value, there's a chance that these could combine together to give a high expected improvement, and at this point the maximum expected improvement is actually where we've sampled, and in a discern ministik experiment, clearly there's no possible improvement there, because if we sample again will get the same answer so.",
                    "label": 0
                },
                {
                    "sent": "And what's happening with the error equations here is, we've included this regression parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's popping up in the air for the equation and the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variance.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is strip the error due to the noise in the data out of our prediction and just concentrate on the modeling error, so to speak.",
                    "label": 0
                },
                {
                    "sent": "So we essentially re interpolate the data.",
                    "label": 0
                },
                {
                    "sent": "It's as if we pick off predictions from the regression model and then fitting into a smooth interpolation through those.",
                    "label": 0
                },
                {
                    "sent": "The maths all works out and you don't need to actually physically go in there and build 2 models and so on.",
                    "label": 0
                },
                {
                    "sent": "But if you do that, it's equivalent to this method, which is essentially done.",
                    "label": 0
                },
                {
                    "sent": "By putting this substitution into the variance, which will reduce down to standard interpolation if your regularization constants removed.",
                    "label": 0
                },
                {
                    "sent": "So this is just a fix that you can add to creating models and what happens here is.",
                    "label": 0
                },
                {
                    "sent": "We start off and we very similar to the interpolation to begin with, but then.",
                    "label": 0
                },
                {
                    "sent": "We start to put a smoother model through the points.",
                    "label": 0
                },
                {
                    "sent": "Even though there's noise there and we continue to have high expectations of improvement in this area, and we've essentially found the optimum at this point when we at this final point where it wants to go and explore a bit more, we're actually going to expect an improvement of 10 to the minus 81 there, so we've essentially the Prob.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be nailed by that point.",
                    "label": 0
                },
                {
                    "sent": "OK, so just have a quick look at it in two dimensions as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the same problem.",
                    "label": 0
                },
                {
                    "sent": "But as essentially for the missing data example.",
                    "label": 0
                },
                {
                    "sent": "But here we've played with the matter of it and made it really nice and noisy so that it's a difficult problem.",
                    "label": 0
                },
                {
                    "sent": "Then you can see we've got all these kind of ridges here.",
                    "label": 0
                },
                {
                    "sent": "This is a classic problem that gradient based optimizers would struggle with getting stuck in these ridges.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will try and solve it with these three methods interpolation.",
                    "label": 0
                },
                {
                    "sent": "You may as well be doing random updates here.",
                    "label": 0
                },
                {
                    "sent": "The expected improvement goes absolutely everywhere.",
                    "label": 0
                },
                {
                    "sent": "Here we got the contour of drag coefficient.",
                    "label": 0
                },
                {
                    "sent": "So here we got a surface.",
                    "label": 0
                },
                {
                    "sent": "Here we got contours.",
                    "label": 0
                },
                {
                    "sent": "This is our error estimate which is just going up absolutely everywhere.",
                    "label": 0
                },
                {
                    "sent": "And here we got the expected improvement which has high points in virtually every gap between every point.",
                    "label": 0
                },
                {
                    "sent": "So although we've got some density of points in the lower region here, essentially we're just we've gone completely globe.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this stage.",
                    "label": 0
                },
                {
                    "sent": "The regression model has stalled by this point.",
                    "label": 0
                },
                {
                    "sent": "We've got high expected improvement right there where we've got a cluster of points, so it's not going any further, and the optimum is actually over here, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Child the re interpolation is a little bit of a mix of the two we've got.",
                    "label": 0
                },
                {
                    "sent": "Two clusters of points in local optimum and then this is just on the point where it actually hits the optimum there.",
                    "label": 0
                },
                {
                    "sent": "So it's solved the problem.",
                    "label": 0
                },
                {
                    "sent": "So both of these methods, the missing date around the noise and dealing with noise, just quite simple plug-in embellishments to the standard cruising methodology.",
                    "label": 0
                },
                {
                    "sent": "I say creating it could be applied to any other.",
                    "label": 0
                },
                {
                    "sent": "Surrogate model that produces some kind of error.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially.",
                    "label": 0
                },
                {
                    "sent": "OK then, so we'll move on from single from constraint a constraint.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blums still looking at expected improvement, although like I said the last.",
                    "label": 0
                },
                {
                    "sent": "Methods this can be applied to any global local info criteria that uses error estimation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do too?",
                    "label": 0
                },
                {
                    "sent": "Deal with constraints in a surrogate based optimization is we're not going to use any kind of fixed penalization penalty function or anything like that.",
                    "label": 0
                },
                {
                    "sent": "We're instead going to look at the probability that a constraint satisfied.",
                    "label": 0
                },
                {
                    "sent": "So we're going to cast it in the same context as probability improvement, an expected improvement.",
                    "label": 0
                },
                {
                    "sent": "So let's say that G of X is the constraint function, and then we've got F as a measure of feasibility.",
                    "label": 0
                },
                {
                    "sent": "We calculate the probability of feasibility as that OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's essentially.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same.",
                    "label": 0
                },
                {
                    "sent": "As expected improvement, except just imagine that this is a constraint limit rather than a minimum.",
                    "label": 1
                },
                {
                    "sent": "We're just integrating the area of feasibility rather than the area of improvement.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So exactly the same kind of maths.",
                    "label": 0
                },
                {
                    "sent": "And then to find the constrained probability of improvement, we assume that the two the constraint an, the objective, uncorrelated and we simply just multiply the two together and then you've got a probability of improvement conditional upon constraint satisfaction.",
                    "label": 1
                },
                {
                    "sent": "It's as simple as that, and you can multiply by three by as many constraints as you like.",
                    "label": 0
                },
                {
                    "sent": "You can do some clever stuff with logarithms to make sure that you are.",
                    "label": 0
                },
                {
                    "sent": "You don't start suffering from floating point.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under flag expected improvement exactly the same, we've got an expectation instead of a probability, we multiply through by probability of constraint satisfaction.",
                    "label": 0
                },
                {
                    "sent": "So we've got a constraint expecting.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Provement now again over a simple, so let's have a look at how it works.",
                    "label": 1
                },
                {
                    "sent": "So the same 1 dimensional test function.",
                    "label": 0
                },
                {
                    "sent": "With the current prediction, I'm not quite sure why the screen prediction is so poor here, but it will serve the purpose.",
                    "label": 0
                },
                {
                    "sent": "I didn't look at that before I put the put the plot together OK, and let's assume that this is our constraint function and we want an area feasibilities below this constraint limit.",
                    "label": 0
                },
                {
                    "sent": "So essentially saying that we can't achieve this optimum.",
                    "label": 0
                },
                {
                    "sent": "Here we can go up to there or we can go up to there.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the standard expected improvement which.",
                    "label": 1
                },
                {
                    "sent": "Has a peak just either side of that sample point there in the region of the global optimum.",
                    "label": 0
                },
                {
                    "sent": "Now the probability that constraint is met?",
                    "label": 0
                },
                {
                    "sent": "Is obviously one at each of these points, because we've got a sample point there and it's net.",
                    "label": 0
                },
                {
                    "sent": "We've got a slightly lower probability in between, even though it's pretty likely that it will be met, and it's definitely not met at that point where we've already sampled.",
                    "label": 0
                },
                {
                    "sent": "OK, so we multiply the two together and essentially we just get a slight flattening here.",
                    "label": 0
                },
                {
                    "sent": "In this region, saying that we and a widening of these points just deviating us away.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From this region.",
                    "label": 0
                },
                {
                    "sent": "So we put a point in there and it turns out that we've essentially solved the problem.",
                    "label": 0
                },
                {
                    "sent": "And but more importantly, just to show how this works intuitively, we've now got this large flat area there where we know the constraint isn't satisfied.",
                    "label": 0
                },
                {
                    "sent": "And as this goes on, we essentially build up Cliff edges either side of the region of infeasibility that we multiply the expected improvement by, which essentially just puts the expected improvement to 0 where you're not going to satisfy a constraint.",
                    "label": 0
                },
                {
                    "sent": "But because we've got this probabilistic method, there's always the chance that.",
                    "label": 0
                },
                {
                    "sent": "If your model is not quite right, you're a little bit incorrect.",
                    "label": 0
                },
                {
                    "sent": "You can sample again to improve your model of the constraint.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here it is in 2D, we're going to go back to the Browning function, but we've put a product constraint through here.",
                    "label": 0
                },
                {
                    "sent": "So this is our area of infeasibility, and that's the optimum.",
                    "label": 0
                },
                {
                    "sent": "We sample it rather fewer times here to make it into a slightly more difficult problem is round dots.",
                    "label": 0
                },
                {
                    "sent": "Here are sample points, so just as in the 1D example.",
                    "label": 0
                },
                {
                    "sent": "Really, definitely not going to satisfy the constraint in these regions, but it's.",
                    "label": 0
                },
                {
                    "sent": "Here we're looking at the constraint.",
                    "label": 0
                },
                {
                    "sent": "Expect an improvement, but away from those regions we get slightly more improvement.",
                    "label": 0
                },
                {
                    "sent": "We get the highest improvement is where the constraint thinks that it's going to be met.",
                    "label": 0
                },
                {
                    "sent": "The constraint model thinks that the constraint model is going to be met, and we've also got a low value down here, so we end up putting an infill point there.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and we go, and once we've done that we starting to get closer towards modeling this constraint boundary.",
                    "label": 0
                },
                {
                    "sent": "While still we have the possibility that maybe we're wrong over in this corner, it turns out that we never go there because the probability never gets quite high enough.",
                    "label": 0
                },
                {
                    "sent": "And after a few more updates, we've got this wonderful Cliff here.",
                    "label": 0
                },
                {
                    "sent": "An unexpected improvement has led us down to this corner.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it really works quite nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, so going on tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Objectives so we've had a few talks on multi objective optimization and everybody here is quite clear about what a peretto front is that we've essentially got a set of nondominated solutions, and these define Apparate frontier and we want to go.",
                    "label": 0
                },
                {
                    "sent": "Down in that direction.",
                    "label": 0
                },
                {
                    "sent": "So we want to look.",
                    "label": 0
                },
                {
                    "sent": "At the possibility of improved instead of expected improvement, we want to look at the expectation of improving.",
                    "label": 0
                },
                {
                    "sent": "On all of these designs at once OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rather than this probability density function that we had the expected improvement, this one dimensional probability density function.",
                    "label": 0
                },
                {
                    "sent": "We've now got a 2 dimensional probability density function because we're looking at two objectives so.",
                    "label": 0
                },
                {
                    "sent": "A probability looks like that this being our first objective, this being our second objective, so I wasn't able to come up with a plot that was really clear enough to demonstrate this.",
                    "label": 0
                },
                {
                    "sent": "But imagine the probability of improvement plots and cost those in two dimensions in your mind and start integrating that sort of three dimensional picture below the Pareto front and that's it.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're doing.",
                    "label": 0
                },
                {
                    "sent": "OK so I won't take it up.",
                    "label": 0
                },
                {
                    "sent": "These integrals get incredibly long and you could just refer to an Deakins paper on this or our book, which I'll give you a reference to at the end so I won't put all the maths up here, but essentially if we just consider one point on the pressure front here we want to integrate that section, that section and this section.",
                    "label": 0
                },
                {
                    "sent": "And that's that integral there, and we then need to extend that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the whole porretta front.",
                    "label": 0
                },
                {
                    "sent": "And that's the integral there.",
                    "label": 0
                },
                {
                    "sent": "We've only done this for two objectives.",
                    "label": 0
                },
                {
                    "sent": "Clearly, you could extend this to more objectives, but I don't fancy doing all the integration.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll throw it at Maple or do it numerically.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but it's there's no reason why it can't be extended other than not finding somebody who's willing to go to do it.",
                    "label": 0
                },
                {
                    "sent": "That's just the probability of improvement.",
                    "label": 1
                },
                {
                    "sent": "As you remember from the single objective case, the expected improvement is the first moment of the probability of improvement about the best design so far.",
                    "label": 0
                },
                {
                    "sent": "So expected improvement is the first moment again here and the math to get there is a little bit too big to fit on this slide, but there's nothing.",
                    "label": 0
                },
                {
                    "sent": "There's nothing complicated about it, it's just integration by parts, and it fills up a lot of space.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But do please refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "So just a little toy example to show this and how just as expected improvement has this global local exploitation exploration trade off, so does this so we can find disjointed regions of parameter optimal solutions, and we can do it very fast here.",
                    "label": 0
                },
                {
                    "sent": "We've got the same old function as our first objective and I've just distorted that slightly to give us our second objective.",
                    "label": 0
                },
                {
                    "sent": "So we've got this clear region of.",
                    "label": 0
                },
                {
                    "sent": "Prayer to optimal solutions that the expected improvement update strategies found initially, and they're showing there.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run this on a little bit further.",
                    "label": 0
                },
                {
                    "sent": "And we escape this local optimum over here and we get another solution there.",
                    "label": 0
                },
                {
                    "sent": "So our initial solutions are down there now and we've got this other area approach to optimal solutions up here.",
                    "label": 0
                },
                {
                    "sent": "Well, one solution so far we could continue going and fill in these areas are a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's pretty much the end of the talk, but I'm just going to show you this actually working and hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run a little bit of the Matlab code.",
                    "label": 0
                },
                {
                    "sent": "While we maybe have some questions, all of the code that implements these.",
                    "label": 0
                },
                {
                    "sent": "Methods is available on the Wiley website and goes alongside out book.",
                    "label": 0
                },
                {
                    "sent": "But you know, you don't need to buy the book to download the code, and I don't get much money if you do download.",
                    "label": 0
                },
                {
                    "sent": "If you do buy the book, so don't feel obliged to, but it may help.",
                    "label": 0
                },
                {
                    "sent": "OK, so you won't go through this, but essentially you've got a short piece of Matlab code which is mainly plotting out the results.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Hang on a minute.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just go back to the problem definition.",
                    "label": 0
                },
                {
                    "sent": "I won't go back into the slide slideshow, so the problem that we're going to look at is the nowacki being problem.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you know some of you have seen this before.",
                    "label": 0
                },
                {
                    "sent": "It's a classic problem.",
                    "label": 0
                },
                {
                    "sent": "They think the papers in this was written in the 70s or 80s where we're going to optimize a fixed length steel cantilever beam under 5 kilonewtons of no load and the variables are the height and width of the beam.",
                    "label": 0
                },
                {
                    "sent": "We're going to try and minimize cross sectional area.",
                    "label": 0
                },
                {
                    "sent": "Either wait and also the bending moment and we've got an area ratio.",
                    "label": 0
                },
                {
                    "sent": "Bending moment, buckling deflection, shear constraint.",
                    "label": 0
                },
                {
                    "sent": "So we got five constraints, two objectives.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to start with 10 point optimal Latin hypercube, so that's really not that many, and we've got a cricket model of each objective constraint.",
                    "label": 0
                },
                {
                    "sent": "We're going to tune the parameters of the rigging models with a genetic algorithm, followed by an SQP which are going to be very fast.",
                    "label": 0
                },
                {
                    "sent": "'cause we've got the advent of the likelihood in just two dimensions.",
                    "label": 0
                },
                {
                    "sent": "You don't get much there, but if you start to get ten 2050 dimensions, the joint really comes in.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to add 20 points at the maximum of the constrained multi objective expected improvement.",
                    "label": 0
                },
                {
                    "sent": "So I just go back to the code.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've got don't expect any amazing visual stuff here.",
                    "label": 0
                },
                {
                    "sent": "It's just going to be a lot, OK?",
                    "label": 0
                },
                {
                    "sent": "It's quite a simple setup.",
                    "label": 0
                },
                {
                    "sent": "We've got our calculate Arab.",
                    "label": 0
                },
                {
                    "sent": "A sampling plan here.",
                    "label": 0
                },
                {
                    "sent": "One line of code there.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We then put in our constraint limits.",
                    "label": 0
                },
                {
                    "sent": "We then cycle through and Calculator to objectives and all of our constraints.",
                    "label": 0
                },
                {
                    "sent": "This section here tunes creaking models.",
                    "label": 0
                },
                {
                    "sent": "Then going to vast amount of plotting.",
                    "label": 0
                },
                {
                    "sent": "Which you can just skip out if you're just running it and then we.",
                    "label": 0
                },
                {
                    "sent": "We're going to search essentially, so I'll just get that running.",
                    "label": 0
                },
                {
                    "sent": "And we can have that going in the background.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because this is optimizing the sampling plan so reasonably clunky process, but works very well.",
                    "label": 0
                },
                {
                    "sent": "It's the same Methodism.",
                    "label": 0
                },
                {
                    "sent": "Dawkins uses in the blind watchmaker to make his little insects.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tuning the models is done in 6.5 seconds there.",
                    "label": 0
                },
                {
                    "sent": "This is the initial front red solutions of violated the constraints.",
                    "label": 0
                },
                {
                    "sent": "Green ones haven't, and the blue ones that are highlighted porretto optimal.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are the ones now that violated the constraints?",
                    "label": 0
                },
                {
                    "sent": "'cause otherwise the preference just hidden in the corner and as it cycles through it's just going to go through an add up add 20 updates?",
                    "label": 0
                },
                {
                    "sent": "So that could potentially be going in the background.",
                    "label": 0
                },
                {
                    "sent": "While there are any quest.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one would be.",
                    "label": 0
                },
                {
                    "sent": "I mean, do you guys have any important performance on more rigorous mathematical benchmarks like the entire GCS?",
                    "label": 0
                },
                {
                    "sent": "Or if your document multi objective with CTS problems in DTS DTS not only test problems ascentia Leon this test problem and on aerodynamic design problem.",
                    "label": 0
                },
                {
                    "sent": "I suppose we never really.",
                    "label": 0
                },
                {
                    "sent": "Well, the methods being developed and we're using it for our own purposes in engineering design rather than pitching it.",
                    "label": 0
                },
                {
                    "sent": "I suppose against other multi objective methods, but maybe we should try try some test problems, yeah?",
                    "label": 0
                },
                {
                    "sent": "The comment right under question when your simulation input fails.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's quite serious.",
                    "label": 0
                },
                {
                    "sent": "Opinion I occasionally saying you were gonna penalize your pale face simulations.",
                    "label": 0
                },
                {
                    "sent": "It may.",
                    "label": 0
                },
                {
                    "sent": "It may not work all the time.",
                    "label": 0
                },
                {
                    "sent": "In your example of notice that most of your failures until he was away from your optimal point where minimum drag was, so we were lucky that you have a lot of failures outside video where the optimum is.",
                    "label": 0
                },
                {
                    "sent": "If you had a mixture where the failures are crazy surrounding your ostropol optimal point, you would have distorted your designer space quite badly.",
                    "label": 0
                },
                {
                    "sent": "And depending how fitting procedure, you're never gonna get done so well, that's what's good about the method, because the penalization changes from update to update.",
                    "label": 0
                },
                {
                    "sent": "So if you're very near the optimal region where you've got more sampling, there's not much error in the model, so you only add a very small penalization, and it will allow you to actually drive up to the very border of the region of infeasibility.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of it's adaptive in the way that it penalizes.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                },
                {
                    "sent": "One of the problems that we had, we were quite quite close to the region of infeasibility and.",
                    "label": 0
                },
                {
                    "sent": "You're kind of on a downhill trend.",
                    "label": 0
                },
                {
                    "sent": "As you went into in infeasibility, so it performed quite well there, and it's performed up to sort of 90% regions of infeasibility.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it will always be a problem that could could catch any of these systems out, but so far it's been pretty good.",
                    "label": 0
                },
                {
                    "sent": "Can I have one more question?",
                    "label": 0
                },
                {
                    "sent": "I mean looking at this plot coming, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly how many function evaluations has it done?",
                    "label": 0
                },
                {
                    "sent": "At this point, it's.",
                    "label": 0
                },
                {
                    "sent": "It's going to.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "40 in total.",
                    "label": 0
                },
                {
                    "sent": "40 in total when it's finished.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you a plot of them.",
                    "label": 0
                },
                {
                    "sent": "It's only going to maybe three of which will violate the constraint and not that many of which are sub optimal.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not a, it's not an incredibly hard problem.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comments I have about this is the you can see very clearly the diversity of the front that you're getting.",
                    "label": 0
                },
                {
                    "sent": "Injuring application is fine.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can say you have covered it, but the diversity definitely is not.",
                    "label": 0
                },
                {
                    "sent": "Anywhere close to what you would expect.",
                    "label": 0
                },
                {
                    "sent": "Up and this is not a difficult problem such I mean.",
                    "label": 0
                },
                {
                    "sent": "That's the thing we've only we've only given it 40 something.",
                    "label": 0
                },
                {
                    "sent": "Definitely look into, even if you allow it for long.",
                    "label": 0
                },
                {
                    "sent": "Is there any reason to believe it will have a proper diversity?",
                    "label": 0
                },
                {
                    "sent": "Because the because?",
                    "label": 0
                },
                {
                    "sent": "The way they expect improvements calculated.",
                    "label": 0
                },
                {
                    "sent": "It's taking a distance from the Proto front such that it will tend to look to fill in gaps.",
                    "label": 0
                },
                {
                    "sent": "It will be much more rigorous if you really go into the performance metrics of testing a multiobjective algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean there are metrics like IGB, which is basically generational distance.",
                    "label": 0
                },
                {
                    "sent": "Of course, for this problem you won't have a true understanding of the parental frontier, but if it's a mathematical benchmark, you exactly know better than point is only parity or not.",
                    "label": 0
                },
                {
                    "sent": "What is the hypervolume?",
                    "label": 0
                },
                {
                    "sent": "What so those metrics?",
                    "label": 0
                },
                {
                    "sent": "If you can quantify?",
                    "label": 0
                },
                {
                    "sent": "It would be a much stronger case.",
                    "label": 0
                },
                {
                    "sent": "Maybe not in 40, maybe 120.",
                    "label": 0
                },
                {
                    "sent": "I've achieved excellent diversity and it is on the front, yeah?",
                    "label": 0
                },
                {
                    "sent": "Which is quite interesting stuff.",
                    "label": 0
                },
                {
                    "sent": "Look at, yeah, yeah, yeah, it would.",
                    "label": 0
                },
                {
                    "sent": "No doubt it would be good to push it a bit further and validated against other methods as well.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so just before we have any more questions, if there were any, this is the final front here and those are the.",
                    "label": 0
                },
                {
                    "sent": "Constraint violation points, so we've not run for very long, and there's hardly any.",
                    "label": 0
                },
                {
                    "sent": "That's hardly any data there, but the data that is there.",
                    "label": 0
                },
                {
                    "sent": "There are quite a few points on the Proto Front.",
                    "label": 0
                },
                {
                    "sent": "We've hardly had any subsequent constraint violations, and it's really performed very well.",
                    "label": 0
                },
                {
                    "sent": "But yeah, more more validation needed probably.",
                    "label": 0
                },
                {
                    "sent": "So just to.",
                    "label": 0
                },
                {
                    "sent": "Conclude.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Not really far wide, reaching conclusions or necessarily particularly applicable to this talk, but just some thoughts perhaps.",
                    "label": 0
                },
                {
                    "sent": "Surrogate modeling doesn't necessarily offer all the answers, but it can definitely offer ways around a lot of the problems that you'll face in optimization.",
                    "label": 0
                },
                {
                    "sent": "When we talk about just inputing values and smoothing and so on, it sounds like we're just hammering away with a blunt tool, but there are a lot of traps that you can fall into, particularly with the tuning of the models and so on and so forth, assuming and assuring that you get a global.",
                    "label": 1
                },
                {
                    "sent": "You move towards the global optimum and have a good exploration exploitation tradeoff.",
                    "label": 1
                },
                {
                    "sent": "In multi objective context, I think it's particularly promising given the huge number of evaluations that you often see in multiobjective design.",
                    "label": 0
                },
                {
                    "sent": "And also just to note that I haven't covered newer surrogate methods such as blind cragging, which is really promising where instead of just saying well I'm going to add bumps to and from a mean fit.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to represent the function using some sort of polynomial terms 1st, and then add bumps to that, so you've kind of got the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a thinking man's ensemble maybe.",
                    "label": 0
                },
                {
                    "sent": "Multi Fidelity modeling.",
                    "label": 0
                },
                {
                    "sent": "I haven't covered that combining different levels of Euler rans codes maybe, or with experimental data, that kind of thing and also yeah further exploit enhancements to expect improvement in terms of improving exploitation exploration tradeoffs and combating this problem with poor parameter estimation.",
                    "label": 1
                },
                {
                    "sent": "When you've got a sparsity of data.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There's a few references here.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, yeah, somebody else put up a whole list of books that you might want to refer to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into asking people to look at other people's work, it's all mine.",
                    "label": 0
                },
                {
                    "sent": "Now we've got a book out on it.",
                    "label": 0
                },
                {
                    "sent": "Try the code out.",
                    "label": 0
                },
                {
                    "sent": "It's on the Wiley website.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to buy the book, there's a review paper in progress in aerospace Sciences which covers quite a lot of.",
                    "label": 1
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stuff, and just in case you still don't know which book to buy, it's that one or that one.",
                    "label": 0
                },
                {
                    "sent": "OK, that's me done.",
                    "label": 0
                },
                {
                    "sent": "Improvement is more concerning this location than this location.",
                    "label": 0
                },
                {
                    "sent": "I was wondering how you make a tradeoff between the two components of working for them at the impression that it's more related to installation.",
                    "label": 0
                },
                {
                    "sent": "No, I wouldn't say so.",
                    "label": 0
                },
                {
                    "sent": "If you look at the 1st.",
                    "label": 0
                },
                {
                    "sent": "The first plot I had for that.",
                    "label": 0
                },
                {
                    "sent": "I could almost say quite the contrary.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the probability of improvement.",
                    "label": 0
                },
                {
                    "sent": "And that's the expected improvement.",
                    "label": 0
                },
                {
                    "sent": "If we go back to the prediction.",
                    "label": 0
                },
                {
                    "sent": "The probability of improvements very high where?",
                    "label": 0
                },
                {
                    "sent": "We're really, really close to where we previously sampled, but if the surface just moved ever so slightly, you would have some improvement.",
                    "label": 0
                },
                {
                    "sent": "So high probability, expected improvements would maybe.",
                    "label": 0
                },
                {
                    "sent": "Improvement is not related to the uncertainty of Liberty.",
                    "label": 0
                },
                {
                    "sent": "It is related to the uncertainty of projection.",
                    "label": 0
                },
                {
                    "sent": "Let me just try and think of how to.",
                    "label": 0
                },
                {
                    "sent": "If you've got a, you've got a very narrow.",
                    "label": 0
                },
                {
                    "sent": "With your distribution there, but most of it is below the minimum, so you've got a high probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You've got a wide, so you've got a high probability of improvement here.",
                    "label": 0
                },
                {
                    "sent": "It's very wide.",
                    "label": 0
                },
                {
                    "sent": "But only a small amount of it is below the best value so far.",
                    "label": 0
                },
                {
                    "sent": "So you're talking about proportions of this that are below the best value so far, and so probability improvement will tend to favor going close to previous points maybe, but it's very difficult, so once you start blowing up the dimensions of the problem, you're just wandering around in the dark, really.",
                    "label": 0
                },
                {
                    "sent": "Any questions, so your first assumption of advocating circuit model is that the surface is smooth, yes?",
                    "label": 0
                },
                {
                    "sent": "There he is well.",
                    "label": 0
                },
                {
                    "sent": "You can if you go back to the correlation.",
                    "label": 0
                },
                {
                    "sent": "In this work I've set this to two here, but we can vary that normally between one and two in engineering and geostatistics, they'll take it down to zero, and that means that you can have near cliffs in your correlation.",
                    "label": 0
                },
                {
                    "sent": "But yeah, people use everybody uses their own correlation.",
                    "label": 0
                },
                {
                    "sent": "I can't remember who it was, but somebody yesterday set that as constant for all variables, so essentially kind of radial basis function, which I find doesn't work so well.",
                    "label": 0
                },
                {
                    "sent": "I stick with P2.",
                    "label": 0
                },
                {
                    "sent": "Some people say.",
                    "label": 0
                },
                {
                    "sent": "How could you do that?",
                    "label": 0
                },
                {
                    "sent": "Also, if you've got serious discontinuities, you can Patch together a number of surrogates.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}