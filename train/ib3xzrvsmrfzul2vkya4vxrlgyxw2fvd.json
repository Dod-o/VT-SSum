{
    "id": "ib3xzrvsmrfzul2vkya4vxrlgyxw2fvd",
    "title": "Feature Selection - From Correlation to Causality",
    "info": {
        "author": [
            "Isabelle Guyon, Clopinet"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/lkasok08_guyon_fsfc/",
    "segmentation": [
        [
            "Thank you for the introduction and thank you for giving this opportunity to talk to you today about the feature selection.",
            "I would like for us to start the."
        ],
        [
            "Your credit to my collaborators, most of the work I'm talking about today comes from a collection of papers that we have edited in the book that gathers the contribution from a challenge we organized for needs 2003 on feature selection, and the book also contains a CD with the data and some code to reproduce the best results of the challenge.",
            "And we went on and started being interested in causality.",
            "And so some of the work I'm talking about today is drawn from a paper I wrote with constantly face and already set with whom we organized the workshop two years ago at NIPS on causality.",
            "And this year we also had a workshop."
        ],
        [
            "Little.",
            "And we went on and continued working on the how we can incorporate notions of causality in feature selection and I'm here giving you credit to my collaborators of the Causality Workbench, which is a platform that we have set up to evaluate causality algorithms, and we've already had the two challenges, one of which was discussed this here in a workshop yesterday.",
            "And so hopefully, you know, as I saw from this morning session with people are most interested in is what works and why it works, so I'll be able to tell you from the results of these challenges.",
            "What seems to be working.",
            "And I'll give you my take it.",
            "You know why I think these algorithms are."
        ],
        [
            "Best.",
            "Before we get there, most of the talk is going to be about giving you some notations and ideas about you know what with the different algorithms."
        ],
        [
            "Or the point of feature selection consists in given a data matrix with N features and M training examples, select the best columns in this data matrix in order to build better, faster, easier to understand learning machines.",
            "And this problem."
        ],
        [
            "Is not likely to go away because we are getting ever more data right?",
            "So this I keep showing this picture with this picture with these different applications and I keep pushing these bubbles, stretching them up and down here because we're getting one more variables and we're getting more and more examples in our datasets."
        ],
        [
            "Here's the new Michael Shelby using I'll be talking about univariate methods.",
            "So we are going to consider methods that utilize one variable at a time to make predictions and multivariate methods that consider subsets of variables together.",
            "We'll be talking about filter methods that Frank features or feature subsets independently of the predictor or the classifier.",
            "And wrapper methods that, On the contrary, use the classifier or the predictor more generally to assess features or feature subsets."
        ],
        [
            "So first of all, I'll be talking about univariate filter methods.",
            "The reason for starting with those is not only that they are simplest.",
            "But they are also very powerful and in a wide variety of application.",
            "All you need is the univariate filter methods.",
            "That makes partitions very happy and machine learning people very unhappy 'cause they can't write more papers.",
            "It's already all done."
        ],
        [
            "So you write feature ranking consistent, finding a ranking criterion for the features and on this picture I'm illustrating a simple classification problem with two classes, the blue class and the right class, and XI is 1 variable.",
            "And these histograms represent.",
            "The density of the two classes.",
            "So if the two classes completely overlap, so the two densities are on top of one another.",
            "As you can guess, the feature is completely useless with respect to separating the two classes, and you might think you can safely discard it.",
            "And their tests to figure out whether the distributions are significantly displaced with respect to one another.",
            "The distribution, say of class blue and class read one of the simple statistics to do that is the T statistics.",
            "That is, the ratio of the difference between the means to the pooled within class standard deviation appropriately normalized.",
            "And that obeys a student distribution that makes, of course, some assumptions like the.",
            "The classes are normally distributed and they have equal variances.",
            "So basically you are testing the hype than our iPod.",
            "This is that the mean of the two classes are identical.",
            "Why am I talking about that?",
            "Well, because this is one of the most powerful methods.",
            "Really works well."
        ],
        [
            "In practice, but there are many other statistical tests that have been covered in the review chapter #2 of the book.",
            "And they will basically all do similar things.",
            "They have no hypothesis that is.",
            "But the variable of interest and the target variable are independent.",
            "And you build a relevant index and you use it as a test statistic.",
            "And then.",
            "From the test statistic, you opened up a new P value.",
            "The P value is.",
            "The area under the curve in the tail of the distribution, the tail of the null distribution.",
            "That is, the distribution of the irrelevant variables.",
            "So we have to think that you know all these methods are based on the idea that you can define a distribution of your relevant variables, which is not that obvious.",
            "Um and then?",
            "OK, well this is kind of details on this multiple testing problem.",
            "That is if you are trying to rank many many features then you don't doing a single test to know whether the particular feature is relevant or relevant.",
            "But you testing many many features so you need to have to correct your P value.",
            "Is this a little bit accessory?",
            "Because really what people do in feature selection with these statistical tests that they're going to set a threshold on the P value.",
            "You can think of this P value as a kind of a normalized.",
            "Relevance index and the threshold you place on it is a little bit arbitrary.",
            "People use, you know, maybe 5% or 1%, but nobody tells you what is the correct threshold.",
            "So whether you need to correct for for multiple testing or artists just a little bit accessory.",
            "More importantly, I think people have device to another quantity that's called the false discovery rate, and that instead of being like the P value, the false positive rate that is the fraction of false positive over the number of irrelevant features that oppose discovery rate is the fraction of false positive over the number of features you've selected.",
            "So you take your ranking, we take a slice of your ranking corresponding to your most promising features, and you're looking among your most promising features.",
            "What fraction of those are the bad guys?",
            "And this quantity can be bounded by the false positive rate, which if you have a tabulated statistic, is estimated by the P value and you have just 2 multiplied by the total number of features and divide by the number of selected features.",
            "So this this has been quite useful in many applications, particularly in bioinformatics where people like to select a given set of features.",
            "And no among these set of features, what fraction of those are going to be garbage features?",
            "So think about, you know in genomics you want to have jeans that are going to be important too.",
            "To diagnose disease and would like to know what fraction of those jeans are actually completely garbage, so that's what gives it to you.",
            "Note that I haven't talked at all yet about predictive power.",
            "How you know what error rate your going to get with these features on the classifier?",
            "In fact, I think it's quite disconnected and you all experiments indicate that if you just set a threshold on the full discovery rate on the P value, you get an arbitrary number of features, and this number of features, not necessarily the number of feature that's optimal to predict to get better.",
            "Addiction performance that day is actually virtually no correlation.",
            "Um?"
        ],
        [
            "Sorry.",
            "So now which relevance index should be you'll be using for doing this ranking of features?",
            "As it turns out, independence is easier to define than dependence.",
            "Mathematically, independence between 2 random variables say one random variable representing a feature, and another representing the target, is defined by P of X and Y = P of XP of Y.",
            "So natural natural kind of measure of dependence could be mutual information basically measures the distance between the district, the joint distribution of X&Y and the product of P of XY.",
            "Also note in order to go back library Divergance.",
            "No.",
            "Why shouldn't we just always use mutual informations in this, since like you know a natural choice?",
            "The reason is that it's done hard to estimate mutual information in a lot of cases, particularly when you have continuous variables, you need basically to estimate the densities and so how do you estimate densities?",
            "Well, you know there are many ways of estimating densities, but it's not a trivial problem and it might be prone to overfitting.",
            "So you approximate it when we are or another right?",
            "So the question is whether that approximation is good, right?",
            "It's so it's exactly the same problem we have of overfitting as you know everywhere, right?",
            "You have to trade these green, having maybe a better model and better in principle, right?",
            "But we don't have enough data to get a good approximation of it, so maybe you're better off with a weaker model.",
            "That's easier to estimate, so."
        ],
        [
            "In that case weaker.",
            "What could be weaker?",
            "This is allowed choice of you know of selection criteria.",
            "Depending on the nature of the target variables, the variables and the target, whether they're biting, binary, categorical or continuous, the type of problem dependencies between variables, linear nonlinear relationship between variables, etc.",
            "The available data.",
            "And whether they are tabulated.",
            "The statistics.",
            "But bottom line, if you don't know which criterion to choose, which you start with, well, I'd say you know, start with person correlation coefficient, right?",
            "So the T statistic is actually very analogous to the to the person correlation coefficient.",
            "In the case where you have balanced classes.",
            "So you know one of those right is, of course, if you have very unbalanced classes, you only need to re balance things etc etc, but.",
            "Bottom line, this the very simple criteria that make linearly linear assumptions assumptions of linear dependency, like the person correlation coefficient, work pretty well in practice.",
            "Alright."
        ],
        [
            "So I told you about univariate methods.",
            "So how about multivariate misses?",
            "Why, you know, would you want to do anything else than univariate methods?"
        ],
        [
            "Well, Universe selection may fail.",
            "So consider now two variables, so variable X, one and variable X2.",
            "And this is a scatter plot in the two dimensional space of these two variables that could be, for example, the age of a patient and say the height of the patient or whatever.",
            "And you you have two populations of patients, the right ones and the green ones, right?",
            "So as you can see from this plot feature X1, when you marginalized that is when you project all the features on X on the Axis X1.",
            "The two classes are reasonably well separated, but if you project on X2, this is what you get here.",
            "You see that the two classes overlap almost perfectly.",
            "So one feature is useful by itself.",
            "The other feature is completely useless by itself.",
            "Yes, in two dimensions.",
            "You get almost perfect separation.",
            "That is, you get better separation in two dimension that than you would get with only one feature.",
            "This is a case in which one feature which is completely irrelevant by itself, can help another feature.",
            "Improve its its predictive power.",
            "This case is sometimes overlooked.",
            "Sometimes people think that the reason for going from a univariate methods to multivariate method is the exact problem or the chess board problem.",
            "Most people think that the reason is that when you have two variables that are completely irrelevant by themselves, but in two dimensions separate perfectly the data, you wouldn't see it if you didn't have you considered the variable separately.",
            "You would need to look at them joint link, right?",
            "But those problems are actually important because in variety of real datasets you don't get this kind of relationships, but you get a lot.",
            "We get a lot of the first kind.",
            "Why don't you get a lot of those?",
            "Well, it's because that if really you have disjoint clusters.",
            "Usually people have figured that out ahead of time and they've turned it into different classes.",
            "They would label that as a different class as that right, because people tend when they when they work along time in a domain to identify that there are clusters and they will call it differently."
        ],
        [
            "Now how do we do this?",
            "Multi variate feature selection?",
            "Three ways.",
            "Filter methods you put all features into a filter.",
            "You get a feature subset and then using this feature subset ability predictor.",
            "Rappers you generate multiple feature subsets.",
            "You train multiple predictors.",
            "And the wrapper selects, you know whatever predictor is best, and eventually you iterate.",
            "Then you make a step in the space of all possible features, generate more feature subsets, etc, until you find the feature subset that gives you best predictions.",
            "So the predictor is in the loop of the feature selection process.",
            "An embedded methods basically produce simultaneously a feature subset Anna Predictor."
        ],
        [
            "People often think that multivariate methods are only wrapper methods, so I'm illustrating here a case of a filter methods for multivariate feature selection.",
            "It's also a very powerful one that I think people should know about.",
            "It's called the relief filter.",
            "And here is my chess board problem again.",
            "No.",
            "How can I build a univariate criterion?",
            "Such that I'm going to favor features that by themselves in projection.",
            "Are shown, you know, separation between the two the two classes, but yet there are relevant, so it's kind of a relevance in context.",
            "So the way to do it is let's consider a point in that scatter plot and let's look for the nearest kit that is the nearest example of the same class and their nearest miss, which is the nearest example of the opposite class.",
            "And then you look in projection on the two axis.",
            "At the ratio of the difference here, I'm using the ratio because it's self normalizing.",
            "The ratio of the distance to the nearest nearest to the distance to the nearest case.",
            "So this would be for each dimension I'm computing the average overall training example of this ratio.",
            "And using this criterion then I'm ranking all the features and as you can understand, if you know I have a distance to the nearest miss which is larger than the distance to the nearest hit on average, then this is a feature that might be irrelevant."
        ],
        [
            "But as I said, most people work now since the paper of Caribbean Join 97.",
            "The work with rappers for feature selection because Kevin John did a good job at explaining that the relief filter was not good.",
            "Now with rappers, what you do is that you start this.",
            "This would be representing the space of all possible feature subset.",
            "In the case of four features.",
            "So if you have a feature, there is a one in this vector and if you don't have the feature, that's a 0.",
            "So these are all the possible states in your life.",
            "You have any features you have two to the end possible feature subsets.",
            "And copy and John Advocate working in this space of all possible feature subsets and assessing every feature subset or not necessarily every feature so said that every feature subset you go through with your predictor."
        ],
        [
            "So walking in this feature subset space can be done with various strategies, including you know, exhaustive search for fastek serve beam searching.",
            "All of them have been reviewed in this."
        ],
        [
            "After and the ones that seem to be working best for most application of forward selection or backward elimination that are greedy search methods.",
            "So in forward selection you start with an empty set and then you try all possible features and then you select one and then you try to add another feature and then you select the best and then you try all possible next features etc."
        ],
        [
            "There is a an embedded method version of that, in which instead of trying all possible next feature, you try only one possible next feature, which is the one that you're training algorithm advises you to choose so often in the process of learning, because you're doing gradient descent or whatever, you have some idea of what is going to be the next feature that is going to be interesting, so you try only that one in this case.",
            "You have only N feature subsets that that you're trying."
        ],
        [
            "OK, same thing for backward elimination.",
            "You can start with a full feature set and then you eliminate progressively features."
        ],
        [
            "And you can also do it in an embedded method way where you guide your search instead of trying all possible next features."
        ],
        [
            "This setting can be variant of this setting is that instead of having zeros and one for every feature, you can have a continuous variable.",
            "So instead of selecting really feature, you can penalize a feature.",
            "So if you replace the zeros and ones binary values by scaling factors that are coefficients, but when you multiply the the features.",
            "Then you can start doing gradient descent and other optimization methods so."
        ],
        [
            "In your paper, bilau Sheffield western MSFT.",
            "They put a framework in which the which encompasses many many algorithms of feature selection.",
            "Based on this principle.",
            "You, like, you know, in many learning problems, including kernel methods, you formalize the problem.",
            "I've as minimizing risk and the loss function this time is a function of, you know your your model and inside your model you have the parameters, your usual parameters and you have extra parameters.",
            "Now we show the scaling factors, so this operation is actually a componentwise multiplication of the vector of scaling factors times your input vector.",
            "So each input is multiplied by a little.",
            "Scaling factor OK.",
            "So.",
            "This lends itself of course to training kernels, because this is a training kernel.",
            "Workshop can imagine that you can take any kernel you want, and now you add this Sigma comet or extra parameters that are just killing the inputs and you do an optimization."
        ],
        [
            "By all turning the optimization, the alphas, and the sigmas.",
            "I can have also other parameters of course.",
            "So this framework lends itself to creating many new types of embedded feature selection."
        ],
        [
            "Message.",
            "So, so I'm speaking a little bit because I have only a few minutes to explain you now what where causality comes into the picture so far.",
            "I've given you ideas of how to do feature selection and the idea of whether a feature is a cause or an effect of the target variable has never come into play and after all, why should we care about whether to know whether the feature is a cause or an effect of the target?",
            "After all, if you're interested in making predictions like medical diagnosis both causes an effects can be equivalently predictive of a target variable, for example.",
            "Virgin defect could be a cause of a disease and an immune reaction could be a consequence of a disease and screening for gene defects is one way of preventing disease and diagnosis.",
            "The diagnosing.",
            "Potentially this is.",
            "And also having an ELISA test that measures a antibody reactions is also a way of diagnosing disease.",
            "So both causes and effect are used in practice.",
            "But in some other settings, for example, if you want to perform actions on a system, like if you want to administer a drug to cure disease.",
            "If you administer a drug that eliminates the symptom, it's not going to cure your disease.",
            "Well, you know if you and Mr drug that eliminates the cause, then you might actually have a desirable effect.",
            "So in some instances you're interested in distinguishing between causes and effects.",
            "Additionally, sometimes you have other variations that are not due to the intervention of an external agent, but interventions that due to.",
            "Some changes that are beyond your control.",
            "So for example, you used to use always the same laboratory equipment, but then one of your machines work down and you need to buy another machine and this new machine is calibrated differently.",
            "Or has you know slightly different properties and maybe some features that you thought were very relevant with the previous machine are no longer relevant, so this is what I'm."
        ],
        [
            "Three list right now.",
            "What can go wrong?",
            "So I justified for you that we use multivariate methods with these two graphs saying, well, you know some features can be irrelevant taken by themselves, but they become relevant in the context of others.",
            "What can go wrong?",
            "So this is an example from real data that they had to analyze in mass spectrometry where we had features in this case were just the height of the peak in a mass spectrometer effects a very very large mass spectrometer.",
            "Here it's small part that you see the algorithm we use identified as the most important feature this feature and here all you know the examples of the right class and of the green class are overlaid.",
            "So you see all the spectrum on top of one another.",
            "So this is a good feature, it separates well the red Spectra from the from the green spectrum.",
            "Amazingly enough, the second best feature or the you know most complimentary feature selected by the algorithm was this Valley here.",
            "So a feature that separates not at all by itself the two categories, so we're really in that case here.",
            "Featuring X1 is the one that separates the two category and feature X2 doesn't separate at all.",
            "Yet in two dimensions, the."
        ],
        [
            "Got separated, this actually are the real graph of the real data to show you that."
        ],
        [
            "Like this?",
            "So.",
            "How do we interpret that well?",
            "We can interpret that in terms of a causal graph, so why in that case was a disease and the X one was a protein expression?",
            "So the disease causes the protein expression and X one is what I would call a truly relevant feature.",
            "Well, it's true.",
            "In fact, in my interpretation is an artifact.",
            "Is a measurement artifact.",
            "It's the in the spectrum.",
            "It is the feature that is closest to my peak of interest and that measures the local baseline.",
            "So basically what happened is that we performed any perfect preprocessing.",
            "We didn't remove quite well the baseline of our spectrum, so it pays off finding a point which is close to our peak that estimates that baseline and subtracting.",
            "The value of that baseline gives us better discrimination.",
            "So what we really have is that you know a noise variable here that feeds into into our into a result and we also have the true.",
            "Through a variable of interest, which is the the disease that feeds into.",
            "In two RX one.",
            "So these two variables X to the noise variable and the disease are independent.",
            "But they become dependent given this.",
            "This variable X2, which is our peak measurement.",
            "And we actually have, you know, similar situation here in the in this case here so.",
            "It takes awhile, you know, today this and meditate or this diagram.",
            "But what this?",
            "Got me to think is that well?",
            "We used to think OK, it's great to have.",
            "Multivariate method is great to have the possibility of including features that by them self are irrelevant and that become relevant in the context of other.",
            "But by now I think it's a red flag when I have a feature like that that's complete garbage by itself and that becomes relevant in the context of others.",
            "I have to start thinking is it truly relevant variable or is it an artifact?",
            "Is this something that you know it's something my my feature, but maybe maybe it's just annoys variable that's plugging too.",
            "Into my result.",
            "So I'm getting now some."
        ],
        [
            "Right?",
            "In a in causality where people are trying to do is given a target variable, for example lung cancer that trying to find the most direct causes in the most direct effects and so called spouses spouses are variables that are parents of direct causes.",
            "And those also play a role.",
            "These are, you know, if we go back."
        ],
        [
            "My previous graph.",
            "This is, you know what people call a spouse.",
            "That is, this is apparent of the children of my target variable.",
            "So spouses are like this guy here, right?",
            "Poses they can help you can help you getting a better separation, but you need to be cautious because they are not relevant by themselves.",
            "Solo."
        ],
        [
            "Here, here this would be a good spouse.",
            "For example, if you try to use coughing as a predictor of lung cancer, you want to know whether the person is allergy, because if the person is allergic and is constantly coughing because of allergy, this is not a good indicator of whether or not the person has lung cancer.",
            "Consequently, what you in that case want to do is that you want to include allergy in your set of important features.",
            "But if you are in the previous case that I mentioned where you know this variable with the noise variable, you might not want to include it.",
            "You might want to do a better preprocessing in order to eliminate the artifact that because if you keep it.",
            "In this case it was, you know, correcting for the baseline, it's great and it's going to give you better results on your test set.",
            "But tomorrow you're going to change your instrument and tomorrow you know the calibration is going to be different and that particular variable is maybe not going to be helpful.",
            "It's going to be just contributing errors."
        ],
        [
            "Alright, So what works and why?",
            "So I'm going to summarize in, with or without the algorithms that work best in the challenges that we organized.",
            "First of all, in the feature selection or challenge, we were surprised that some people got extremely good results without feature selection at all.",
            "And that is in spite of the fact that we had injected a large number of complete garbage features.",
            "So we wanted to test the power of feature selection methods and in one way of doing that is to put purely garbage feature and see how many of these bad guys people select accidentally.",
            "It turns out that you know people who took all the features, including all the bad guys, got very good results and the initially we interpreted that as well.",
            "This is great with this knew, you know, regularised kernel methods or with these new regularised neural networks or whatever we can work in these very high dimensional spaces and were insensitive to a garbage features.",
            "Then this of course feature selection helps.",
            "If you fix the number of features.",
            "If you say, well now I want to find the best predictor that doesn't have more than 10 features, then then you really need to do it and those are real constraints.",
            "Sometimes that you have in practice.",
            "Now next we wanted to assess the power of causal feature selection, 'cause I, as I told you, there is some potential benefit in knowing the local causal graph because then we can interpret the features we have.",
            "We can know you know if it's a spouse, well, maybe it can be useful, but we maybe have to look really at what this particular feature means and if you want later down the road to designer drug for example to cure disease, then it's going to be different if the feature that you selected is a cause or.",
            "Or or in effect so.",
            "Might have an interest in understanding better who your features are.",
            "So we designed the."
        ],
        [
            "A challenge in which we had the training data in Thursday to that are differently distributed and we purposely made the test sets such that some of the features that you select on training data become harmful on test data, for example because they are consequences that you've manipulated, that you've changed somehow and they are not predictive anymore.",
            "You realize that causes are always going to be predictive of your target.",
            "If you don't, you know tweak your targets, people manipulate your target.",
            "But consequences won't.",
            "So, for example it.",
            "If you are studying a population of smoking people, if you prevent people from smoking.",
            "They will eventually operation.",
            "Eventually their health will improve, but if you administer massively cough medicine, maybe it's not going to do anything.",
            "So which we tested those concepts with the challenge and figured that well.",
            "It doesn't seem to be that causal discovery methods have that much of an advantage over just a plain feature selection method, or even worse, over not doing feature selection at all.",
            "So why is that?",
            "So first let me go back to the traditional way of doing a selection of the best feature set, so most people what most people do is that they.",
            "Split the data into training, validation and test set and they train on training data.",
            "They select the best feature test the best feature subset on validation data and then they.",
            "Finally, test the final model on a separate phase data.",
            "So of course you can do cross validation by having multiple plates like that, but let's for simplicity assume that you just have one training set when validation set and one tester.",
            "So."
        ],
        [
            "People have noticed that the best working feature selection methods.",
            "Often universe methods.",
            "And or very simple nested subset methods.",
            "This does the next next best thing after you need various method is those methods where you do forward selection or backward elimination.",
            "Just the.",
            "Handwaving theoretical justification is by looking at bounds at the second level of inference, so using the validation set this time.",
            "Using your validation set, you're going to learn which feature subset is best, and this is a learning problem in a finite number of predictors.",
            "So if you're learning with a finite number of predictors, then what's governing your complexity is the log of the number of things you've been trying.",
            "And in that case, if you're trying an exhaustive search, you have to do the impossible feature subsets, so your complexity sort of right is N, or as if you're trying only and things with like with feature ranking or N N + 1 / 2 like in Fort selection of backward elimination, you have only lug event in your complexity.",
            "Which is, you know why this is the classical you know picture you 100 times, which is a justification why you know a few rappers that do a extensive search tend to overfit more than.",
            "Then for selection of feature ranking methods that have had more success.",
            "And."
        ],
        [
            "Um?",
            "No, not my second story.",
            "This the story about the sensitivity of your relevant features.",
            "OK, I'm"
        ],
        [
            "So asking that you you believe me that have a justification for the insensitivity of.",
            "Of the learning algorithms to having a large number of garbage features.",
            "So.",
            "With what we got out of these benchmarks is that multi right feature selection is in principle more powerful than univariate feature selection, but not always.",
            "In practice that you need to take a closer look at the type of dependencies that you have in in terms of causal relationship and that feature selection causal discovery.",
            "Mabel may be more harmful than useful so often if you only care about predictive performance, it may be better just to take all your features and just build a predictor.",
            "If you only if you care about understanding your data, generative process or limiting the number of features for some efficiency reason.",
            "You might want to consider you know, going into these these methods.",
            "Thank you.",
            "OK, great.",
            "It does so.",
            "This is the slide in right?",
            "I skipped here I.",
            "Very roughly estimated, the penalty that you have for omitting a good feature versus including a bad feature, and it's completely symmetric.",
            "It's much more penalizing to admit a good feature that we included that feature.",
            "And the scaling roughly is like if you have Ng good features you can afford having the square root of bad features over M, the number of training examples.",
            "So eventually, if you have an infinite number of chronic samples, you can have an infinite number of bad features, right?",
            "You're incurring some noise, and you can bound.",
            "You know you're the error that you're making.",
            "So basically I'm looking at the standard deviation of the error that you're that you're making in this argument here.",
            "And it's it's good.",
            "It's a little bit worse if you do this causal discovery thing because you're manipulating variables in the test set.",
            "So you're training on something and you're testing on something different, so some of the features now.",
            "You can't know that they're they're bad by looking at training data, but it still kind of cancel out each other because some some will be on.",
            "Some will be a few in test data, so they contribute some noise, but the but the informative features they always kind of aligned with the right answer, so informative features they give you more.",
            "Mileage then.",
            "Then the noise contributed by the back issues there.",
            "In essence, the good features contribute each positively and then the bad features contribute with the.",
            "The dancing traditionally.",
            "Collection depends what you want to do right.",
            "Depends what you want to do again to know we're not throwing away feature selection.",
            "Just saying that if you don't need it, you don't necessarily want it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the introduction and thank you for giving this opportunity to talk to you today about the feature selection.",
                    "label": 0
                },
                {
                    "sent": "I would like for us to start the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your credit to my collaborators, most of the work I'm talking about today comes from a collection of papers that we have edited in the book that gathers the contribution from a challenge we organized for needs 2003 on feature selection, and the book also contains a CD with the data and some code to reproduce the best results of the challenge.",
                    "label": 0
                },
                {
                    "sent": "And we went on and started being interested in causality.",
                    "label": 0
                },
                {
                    "sent": "And so some of the work I'm talking about today is drawn from a paper I wrote with constantly face and already set with whom we organized the workshop two years ago at NIPS on causality.",
                    "label": 0
                },
                {
                    "sent": "And this year we also had a workshop.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little.",
                    "label": 0
                },
                {
                    "sent": "And we went on and continued working on the how we can incorporate notions of causality in feature selection and I'm here giving you credit to my collaborators of the Causality Workbench, which is a platform that we have set up to evaluate causality algorithms, and we've already had the two challenges, one of which was discussed this here in a workshop yesterday.",
                    "label": 0
                },
                {
                    "sent": "And so hopefully, you know, as I saw from this morning session with people are most interested in is what works and why it works, so I'll be able to tell you from the results of these challenges.",
                    "label": 0
                },
                {
                    "sent": "What seems to be working.",
                    "label": 0
                },
                {
                    "sent": "And I'll give you my take it.",
                    "label": 0
                },
                {
                    "sent": "You know why I think these algorithms are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best.",
                    "label": 0
                },
                {
                    "sent": "Before we get there, most of the talk is going to be about giving you some notations and ideas about you know what with the different algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the point of feature selection consists in given a data matrix with N features and M training examples, select the best columns in this data matrix in order to build better, faster, easier to understand learning machines.",
                    "label": 0
                },
                {
                    "sent": "And this problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is not likely to go away because we are getting ever more data right?",
                    "label": 0
                },
                {
                    "sent": "So this I keep showing this picture with this picture with these different applications and I keep pushing these bubbles, stretching them up and down here because we're getting one more variables and we're getting more and more examples in our datasets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the new Michael Shelby using I'll be talking about univariate methods.",
                    "label": 0
                },
                {
                    "sent": "So we are going to consider methods that utilize one variable at a time to make predictions and multivariate methods that consider subsets of variables together.",
                    "label": 1
                },
                {
                    "sent": "We'll be talking about filter methods that Frank features or feature subsets independently of the predictor or the classifier.",
                    "label": 1
                },
                {
                    "sent": "And wrapper methods that, On the contrary, use the classifier or the predictor more generally to assess features or feature subsets.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, I'll be talking about univariate filter methods.",
                    "label": 0
                },
                {
                    "sent": "The reason for starting with those is not only that they are simplest.",
                    "label": 0
                },
                {
                    "sent": "But they are also very powerful and in a wide variety of application.",
                    "label": 0
                },
                {
                    "sent": "All you need is the univariate filter methods.",
                    "label": 0
                },
                {
                    "sent": "That makes partitions very happy and machine learning people very unhappy 'cause they can't write more papers.",
                    "label": 0
                },
                {
                    "sent": "It's already all done.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you write feature ranking consistent, finding a ranking criterion for the features and on this picture I'm illustrating a simple classification problem with two classes, the blue class and the right class, and XI is 1 variable.",
                    "label": 0
                },
                {
                    "sent": "And these histograms represent.",
                    "label": 0
                },
                {
                    "sent": "The density of the two classes.",
                    "label": 0
                },
                {
                    "sent": "So if the two classes completely overlap, so the two densities are on top of one another.",
                    "label": 0
                },
                {
                    "sent": "As you can guess, the feature is completely useless with respect to separating the two classes, and you might think you can safely discard it.",
                    "label": 0
                },
                {
                    "sent": "And their tests to figure out whether the distributions are significantly displaced with respect to one another.",
                    "label": 0
                },
                {
                    "sent": "The distribution, say of class blue and class read one of the simple statistics to do that is the T statistics.",
                    "label": 0
                },
                {
                    "sent": "That is, the ratio of the difference between the means to the pooled within class standard deviation appropriately normalized.",
                    "label": 0
                },
                {
                    "sent": "And that obeys a student distribution that makes, of course, some assumptions like the.",
                    "label": 0
                },
                {
                    "sent": "The classes are normally distributed and they have equal variances.",
                    "label": 0
                },
                {
                    "sent": "So basically you are testing the hype than our iPod.",
                    "label": 0
                },
                {
                    "sent": "This is that the mean of the two classes are identical.",
                    "label": 0
                },
                {
                    "sent": "Why am I talking about that?",
                    "label": 0
                },
                {
                    "sent": "Well, because this is one of the most powerful methods.",
                    "label": 0
                },
                {
                    "sent": "Really works well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In practice, but there are many other statistical tests that have been covered in the review chapter #2 of the book.",
                    "label": 0
                },
                {
                    "sent": "And they will basically all do similar things.",
                    "label": 0
                },
                {
                    "sent": "They have no hypothesis that is.",
                    "label": 0
                },
                {
                    "sent": "But the variable of interest and the target variable are independent.",
                    "label": 1
                },
                {
                    "sent": "And you build a relevant index and you use it as a test statistic.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "From the test statistic, you opened up a new P value.",
                    "label": 0
                },
                {
                    "sent": "The P value is.",
                    "label": 0
                },
                {
                    "sent": "The area under the curve in the tail of the distribution, the tail of the null distribution.",
                    "label": 0
                },
                {
                    "sent": "That is, the distribution of the irrelevant variables.",
                    "label": 0
                },
                {
                    "sent": "So we have to think that you know all these methods are based on the idea that you can define a distribution of your relevant variables, which is not that obvious.",
                    "label": 0
                },
                {
                    "sent": "Um and then?",
                    "label": 0
                },
                {
                    "sent": "OK, well this is kind of details on this multiple testing problem.",
                    "label": 1
                },
                {
                    "sent": "That is if you are trying to rank many many features then you don't doing a single test to know whether the particular feature is relevant or relevant.",
                    "label": 0
                },
                {
                    "sent": "But you testing many many features so you need to have to correct your P value.",
                    "label": 0
                },
                {
                    "sent": "Is this a little bit accessory?",
                    "label": 0
                },
                {
                    "sent": "Because really what people do in feature selection with these statistical tests that they're going to set a threshold on the P value.",
                    "label": 1
                },
                {
                    "sent": "You can think of this P value as a kind of a normalized.",
                    "label": 0
                },
                {
                    "sent": "Relevance index and the threshold you place on it is a little bit arbitrary.",
                    "label": 0
                },
                {
                    "sent": "People use, you know, maybe 5% or 1%, but nobody tells you what is the correct threshold.",
                    "label": 0
                },
                {
                    "sent": "So whether you need to correct for for multiple testing or artists just a little bit accessory.",
                    "label": 0
                },
                {
                    "sent": "More importantly, I think people have device to another quantity that's called the false discovery rate, and that instead of being like the P value, the false positive rate that is the fraction of false positive over the number of irrelevant features that oppose discovery rate is the fraction of false positive over the number of features you've selected.",
                    "label": 1
                },
                {
                    "sent": "So you take your ranking, we take a slice of your ranking corresponding to your most promising features, and you're looking among your most promising features.",
                    "label": 0
                },
                {
                    "sent": "What fraction of those are the bad guys?",
                    "label": 0
                },
                {
                    "sent": "And this quantity can be bounded by the false positive rate, which if you have a tabulated statistic, is estimated by the P value and you have just 2 multiplied by the total number of features and divide by the number of selected features.",
                    "label": 0
                },
                {
                    "sent": "So this this has been quite useful in many applications, particularly in bioinformatics where people like to select a given set of features.",
                    "label": 0
                },
                {
                    "sent": "And no among these set of features, what fraction of those are going to be garbage features?",
                    "label": 0
                },
                {
                    "sent": "So think about, you know in genomics you want to have jeans that are going to be important too.",
                    "label": 0
                },
                {
                    "sent": "To diagnose disease and would like to know what fraction of those jeans are actually completely garbage, so that's what gives it to you.",
                    "label": 0
                },
                {
                    "sent": "Note that I haven't talked at all yet about predictive power.",
                    "label": 0
                },
                {
                    "sent": "How you know what error rate your going to get with these features on the classifier?",
                    "label": 0
                },
                {
                    "sent": "In fact, I think it's quite disconnected and you all experiments indicate that if you just set a threshold on the full discovery rate on the P value, you get an arbitrary number of features, and this number of features, not necessarily the number of feature that's optimal to predict to get better.",
                    "label": 0
                },
                {
                    "sent": "Addiction performance that day is actually virtually no correlation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So now which relevance index should be you'll be using for doing this ranking of features?",
                    "label": 0
                },
                {
                    "sent": "As it turns out, independence is easier to define than dependence.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, independence between 2 random variables say one random variable representing a feature, and another representing the target, is defined by P of X and Y = P of XP of Y.",
                    "label": 0
                },
                {
                    "sent": "So natural natural kind of measure of dependence could be mutual information basically measures the distance between the district, the joint distribution of X&Y and the product of P of XY.",
                    "label": 1
                },
                {
                    "sent": "Also note in order to go back library Divergance.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Why shouldn't we just always use mutual informations in this, since like you know a natural choice?",
                    "label": 0
                },
                {
                    "sent": "The reason is that it's done hard to estimate mutual information in a lot of cases, particularly when you have continuous variables, you need basically to estimate the densities and so how do you estimate densities?",
                    "label": 0
                },
                {
                    "sent": "Well, you know there are many ways of estimating densities, but it's not a trivial problem and it might be prone to overfitting.",
                    "label": 0
                },
                {
                    "sent": "So you approximate it when we are or another right?",
                    "label": 0
                },
                {
                    "sent": "So the question is whether that approximation is good, right?",
                    "label": 0
                },
                {
                    "sent": "It's so it's exactly the same problem we have of overfitting as you know everywhere, right?",
                    "label": 0
                },
                {
                    "sent": "You have to trade these green, having maybe a better model and better in principle, right?",
                    "label": 0
                },
                {
                    "sent": "But we don't have enough data to get a good approximation of it, so maybe you're better off with a weaker model.",
                    "label": 0
                },
                {
                    "sent": "That's easier to estimate, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that case weaker.",
                    "label": 0
                },
                {
                    "sent": "What could be weaker?",
                    "label": 0
                },
                {
                    "sent": "This is allowed choice of you know of selection criteria.",
                    "label": 0
                },
                {
                    "sent": "Depending on the nature of the target variables, the variables and the target, whether they're biting, binary, categorical or continuous, the type of problem dependencies between variables, linear nonlinear relationship between variables, etc.",
                    "label": 1
                },
                {
                    "sent": "The available data.",
                    "label": 0
                },
                {
                    "sent": "And whether they are tabulated.",
                    "label": 0
                },
                {
                    "sent": "The statistics.",
                    "label": 0
                },
                {
                    "sent": "But bottom line, if you don't know which criterion to choose, which you start with, well, I'd say you know, start with person correlation coefficient, right?",
                    "label": 0
                },
                {
                    "sent": "So the T statistic is actually very analogous to the to the person correlation coefficient.",
                    "label": 0
                },
                {
                    "sent": "In the case where you have balanced classes.",
                    "label": 0
                },
                {
                    "sent": "So you know one of those right is, of course, if you have very unbalanced classes, you only need to re balance things etc etc, but.",
                    "label": 0
                },
                {
                    "sent": "Bottom line, this the very simple criteria that make linearly linear assumptions assumptions of linear dependency, like the person correlation coefficient, work pretty well in practice.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I told you about univariate methods.",
                    "label": 0
                },
                {
                    "sent": "So how about multivariate misses?",
                    "label": 0
                },
                {
                    "sent": "Why, you know, would you want to do anything else than univariate methods?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, Universe selection may fail.",
                    "label": 1
                },
                {
                    "sent": "So consider now two variables, so variable X, one and variable X2.",
                    "label": 0
                },
                {
                    "sent": "And this is a scatter plot in the two dimensional space of these two variables that could be, for example, the age of a patient and say the height of the patient or whatever.",
                    "label": 0
                },
                {
                    "sent": "And you you have two populations of patients, the right ones and the green ones, right?",
                    "label": 0
                },
                {
                    "sent": "So as you can see from this plot feature X1, when you marginalized that is when you project all the features on X on the Axis X1.",
                    "label": 0
                },
                {
                    "sent": "The two classes are reasonably well separated, but if you project on X2, this is what you get here.",
                    "label": 0
                },
                {
                    "sent": "You see that the two classes overlap almost perfectly.",
                    "label": 0
                },
                {
                    "sent": "So one feature is useful by itself.",
                    "label": 0
                },
                {
                    "sent": "The other feature is completely useless by itself.",
                    "label": 0
                },
                {
                    "sent": "Yes, in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "You get almost perfect separation.",
                    "label": 0
                },
                {
                    "sent": "That is, you get better separation in two dimension that than you would get with only one feature.",
                    "label": 0
                },
                {
                    "sent": "This is a case in which one feature which is completely irrelevant by itself, can help another feature.",
                    "label": 0
                },
                {
                    "sent": "Improve its its predictive power.",
                    "label": 0
                },
                {
                    "sent": "This case is sometimes overlooked.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people think that the reason for going from a univariate methods to multivariate method is the exact problem or the chess board problem.",
                    "label": 0
                },
                {
                    "sent": "Most people think that the reason is that when you have two variables that are completely irrelevant by themselves, but in two dimensions separate perfectly the data, you wouldn't see it if you didn't have you considered the variable separately.",
                    "label": 0
                },
                {
                    "sent": "You would need to look at them joint link, right?",
                    "label": 0
                },
                {
                    "sent": "But those problems are actually important because in variety of real datasets you don't get this kind of relationships, but you get a lot.",
                    "label": 0
                },
                {
                    "sent": "We get a lot of the first kind.",
                    "label": 0
                },
                {
                    "sent": "Why don't you get a lot of those?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because that if really you have disjoint clusters.",
                    "label": 0
                },
                {
                    "sent": "Usually people have figured that out ahead of time and they've turned it into different classes.",
                    "label": 0
                },
                {
                    "sent": "They would label that as a different class as that right, because people tend when they when they work along time in a domain to identify that there are clusters and they will call it differently.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Multi variate feature selection?",
                    "label": 0
                },
                {
                    "sent": "Three ways.",
                    "label": 0
                },
                {
                    "sent": "Filter methods you put all features into a filter.",
                    "label": 1
                },
                {
                    "sent": "You get a feature subset and then using this feature subset ability predictor.",
                    "label": 0
                },
                {
                    "sent": "Rappers you generate multiple feature subsets.",
                    "label": 1
                },
                {
                    "sent": "You train multiple predictors.",
                    "label": 0
                },
                {
                    "sent": "And the wrapper selects, you know whatever predictor is best, and eventually you iterate.",
                    "label": 0
                },
                {
                    "sent": "Then you make a step in the space of all possible features, generate more feature subsets, etc, until you find the feature subset that gives you best predictions.",
                    "label": 0
                },
                {
                    "sent": "So the predictor is in the loop of the feature selection process.",
                    "label": 1
                },
                {
                    "sent": "An embedded methods basically produce simultaneously a feature subset Anna Predictor.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People often think that multivariate methods are only wrapper methods, so I'm illustrating here a case of a filter methods for multivariate feature selection.",
                    "label": 0
                },
                {
                    "sent": "It's also a very powerful one that I think people should know about.",
                    "label": 0
                },
                {
                    "sent": "It's called the relief filter.",
                    "label": 0
                },
                {
                    "sent": "And here is my chess board problem again.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "How can I build a univariate criterion?",
                    "label": 0
                },
                {
                    "sent": "Such that I'm going to favor features that by themselves in projection.",
                    "label": 0
                },
                {
                    "sent": "Are shown, you know, separation between the two the two classes, but yet there are relevant, so it's kind of a relevance in context.",
                    "label": 0
                },
                {
                    "sent": "So the way to do it is let's consider a point in that scatter plot and let's look for the nearest kit that is the nearest example of the same class and their nearest miss, which is the nearest example of the opposite class.",
                    "label": 0
                },
                {
                    "sent": "And then you look in projection on the two axis.",
                    "label": 0
                },
                {
                    "sent": "At the ratio of the difference here, I'm using the ratio because it's self normalizing.",
                    "label": 0
                },
                {
                    "sent": "The ratio of the distance to the nearest nearest to the distance to the nearest case.",
                    "label": 0
                },
                {
                    "sent": "So this would be for each dimension I'm computing the average overall training example of this ratio.",
                    "label": 0
                },
                {
                    "sent": "And using this criterion then I'm ranking all the features and as you can understand, if you know I have a distance to the nearest miss which is larger than the distance to the nearest hit on average, then this is a feature that might be irrelevant.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But as I said, most people work now since the paper of Caribbean Join 97.",
                    "label": 0
                },
                {
                    "sent": "The work with rappers for feature selection because Kevin John did a good job at explaining that the relief filter was not good.",
                    "label": 1
                },
                {
                    "sent": "Now with rappers, what you do is that you start this.",
                    "label": 0
                },
                {
                    "sent": "This would be representing the space of all possible feature subset.",
                    "label": 0
                },
                {
                    "sent": "In the case of four features.",
                    "label": 0
                },
                {
                    "sent": "So if you have a feature, there is a one in this vector and if you don't have the feature, that's a 0.",
                    "label": 0
                },
                {
                    "sent": "So these are all the possible states in your life.",
                    "label": 0
                },
                {
                    "sent": "You have any features you have two to the end possible feature subsets.",
                    "label": 1
                },
                {
                    "sent": "And copy and John Advocate working in this space of all possible feature subsets and assessing every feature subset or not necessarily every feature so said that every feature subset you go through with your predictor.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So walking in this feature subset space can be done with various strategies, including you know, exhaustive search for fastek serve beam searching.",
                    "label": 0
                },
                {
                    "sent": "All of them have been reviewed in this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After and the ones that seem to be working best for most application of forward selection or backward elimination that are greedy search methods.",
                    "label": 0
                },
                {
                    "sent": "So in forward selection you start with an empty set and then you try all possible features and then you select one and then you try to add another feature and then you select the best and then you try all possible next features etc.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a an embedded method version of that, in which instead of trying all possible next feature, you try only one possible next feature, which is the one that you're training algorithm advises you to choose so often in the process of learning, because you're doing gradient descent or whatever, you have some idea of what is going to be the next feature that is going to be interesting, so you try only that one in this case.",
                    "label": 0
                },
                {
                    "sent": "You have only N feature subsets that that you're trying.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, same thing for backward elimination.",
                    "label": 0
                },
                {
                    "sent": "You can start with a full feature set and then you eliminate progressively features.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can also do it in an embedded method way where you guide your search instead of trying all possible next features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This setting can be variant of this setting is that instead of having zeros and one for every feature, you can have a continuous variable.",
                    "label": 1
                },
                {
                    "sent": "So instead of selecting really feature, you can penalize a feature.",
                    "label": 0
                },
                {
                    "sent": "So if you replace the zeros and ones binary values by scaling factors that are coefficients, but when you multiply the the features.",
                    "label": 1
                },
                {
                    "sent": "Then you can start doing gradient descent and other optimization methods so.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In your paper, bilau Sheffield western MSFT.",
                    "label": 0
                },
                {
                    "sent": "They put a framework in which the which encompasses many many algorithms of feature selection.",
                    "label": 0
                },
                {
                    "sent": "Based on this principle.",
                    "label": 0
                },
                {
                    "sent": "You, like, you know, in many learning problems, including kernel methods, you formalize the problem.",
                    "label": 0
                },
                {
                    "sent": "I've as minimizing risk and the loss function this time is a function of, you know your your model and inside your model you have the parameters, your usual parameters and you have extra parameters.",
                    "label": 0
                },
                {
                    "sent": "Now we show the scaling factors, so this operation is actually a componentwise multiplication of the vector of scaling factors times your input vector.",
                    "label": 0
                },
                {
                    "sent": "So each input is multiplied by a little.",
                    "label": 0
                },
                {
                    "sent": "Scaling factor OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This lends itself of course to training kernels, because this is a training kernel.",
                    "label": 0
                },
                {
                    "sent": "Workshop can imagine that you can take any kernel you want, and now you add this Sigma comet or extra parameters that are just killing the inputs and you do an optimization.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By all turning the optimization, the alphas, and the sigmas.",
                    "label": 0
                },
                {
                    "sent": "I can have also other parameters of course.",
                    "label": 0
                },
                {
                    "sent": "So this framework lends itself to creating many new types of embedded feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Message.",
                    "label": 0
                },
                {
                    "sent": "So, so I'm speaking a little bit because I have only a few minutes to explain you now what where causality comes into the picture so far.",
                    "label": 0
                },
                {
                    "sent": "I've given you ideas of how to do feature selection and the idea of whether a feature is a cause or an effect of the target variable has never come into play and after all, why should we care about whether to know whether the feature is a cause or an effect of the target?",
                    "label": 0
                },
                {
                    "sent": "After all, if you're interested in making predictions like medical diagnosis both causes an effects can be equivalently predictive of a target variable, for example.",
                    "label": 0
                },
                {
                    "sent": "Virgin defect could be a cause of a disease and an immune reaction could be a consequence of a disease and screening for gene defects is one way of preventing disease and diagnosis.",
                    "label": 0
                },
                {
                    "sent": "The diagnosing.",
                    "label": 0
                },
                {
                    "sent": "Potentially this is.",
                    "label": 0
                },
                {
                    "sent": "And also having an ELISA test that measures a antibody reactions is also a way of diagnosing disease.",
                    "label": 0
                },
                {
                    "sent": "So both causes and effect are used in practice.",
                    "label": 0
                },
                {
                    "sent": "But in some other settings, for example, if you want to perform actions on a system, like if you want to administer a drug to cure disease.",
                    "label": 0
                },
                {
                    "sent": "If you administer a drug that eliminates the symptom, it's not going to cure your disease.",
                    "label": 0
                },
                {
                    "sent": "Well, you know if you and Mr drug that eliminates the cause, then you might actually have a desirable effect.",
                    "label": 0
                },
                {
                    "sent": "So in some instances you're interested in distinguishing between causes and effects.",
                    "label": 0
                },
                {
                    "sent": "Additionally, sometimes you have other variations that are not due to the intervention of an external agent, but interventions that due to.",
                    "label": 0
                },
                {
                    "sent": "Some changes that are beyond your control.",
                    "label": 0
                },
                {
                    "sent": "So for example, you used to use always the same laboratory equipment, but then one of your machines work down and you need to buy another machine and this new machine is calibrated differently.",
                    "label": 0
                },
                {
                    "sent": "Or has you know slightly different properties and maybe some features that you thought were very relevant with the previous machine are no longer relevant, so this is what I'm.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three list right now.",
                    "label": 0
                },
                {
                    "sent": "What can go wrong?",
                    "label": 0
                },
                {
                    "sent": "So I justified for you that we use multivariate methods with these two graphs saying, well, you know some features can be irrelevant taken by themselves, but they become relevant in the context of others.",
                    "label": 0
                },
                {
                    "sent": "What can go wrong?",
                    "label": 0
                },
                {
                    "sent": "So this is an example from real data that they had to analyze in mass spectrometry where we had features in this case were just the height of the peak in a mass spectrometer effects a very very large mass spectrometer.",
                    "label": 0
                },
                {
                    "sent": "Here it's small part that you see the algorithm we use identified as the most important feature this feature and here all you know the examples of the right class and of the green class are overlaid.",
                    "label": 0
                },
                {
                    "sent": "So you see all the spectrum on top of one another.",
                    "label": 0
                },
                {
                    "sent": "So this is a good feature, it separates well the red Spectra from the from the green spectrum.",
                    "label": 0
                },
                {
                    "sent": "Amazingly enough, the second best feature or the you know most complimentary feature selected by the algorithm was this Valley here.",
                    "label": 0
                },
                {
                    "sent": "So a feature that separates not at all by itself the two categories, so we're really in that case here.",
                    "label": 0
                },
                {
                    "sent": "Featuring X1 is the one that separates the two category and feature X2 doesn't separate at all.",
                    "label": 0
                },
                {
                    "sent": "Yet in two dimensions, the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Got separated, this actually are the real graph of the real data to show you that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do we interpret that well?",
                    "label": 0
                },
                {
                    "sent": "We can interpret that in terms of a causal graph, so why in that case was a disease and the X one was a protein expression?",
                    "label": 0
                },
                {
                    "sent": "So the disease causes the protein expression and X one is what I would call a truly relevant feature.",
                    "label": 0
                },
                {
                    "sent": "Well, it's true.",
                    "label": 0
                },
                {
                    "sent": "In fact, in my interpretation is an artifact.",
                    "label": 0
                },
                {
                    "sent": "Is a measurement artifact.",
                    "label": 0
                },
                {
                    "sent": "It's the in the spectrum.",
                    "label": 0
                },
                {
                    "sent": "It is the feature that is closest to my peak of interest and that measures the local baseline.",
                    "label": 0
                },
                {
                    "sent": "So basically what happened is that we performed any perfect preprocessing.",
                    "label": 0
                },
                {
                    "sent": "We didn't remove quite well the baseline of our spectrum, so it pays off finding a point which is close to our peak that estimates that baseline and subtracting.",
                    "label": 0
                },
                {
                    "sent": "The value of that baseline gives us better discrimination.",
                    "label": 0
                },
                {
                    "sent": "So what we really have is that you know a noise variable here that feeds into into our into a result and we also have the true.",
                    "label": 0
                },
                {
                    "sent": "Through a variable of interest, which is the the disease that feeds into.",
                    "label": 0
                },
                {
                    "sent": "In two RX one.",
                    "label": 0
                },
                {
                    "sent": "So these two variables X to the noise variable and the disease are independent.",
                    "label": 0
                },
                {
                    "sent": "But they become dependent given this.",
                    "label": 0
                },
                {
                    "sent": "This variable X2, which is our peak measurement.",
                    "label": 0
                },
                {
                    "sent": "And we actually have, you know, similar situation here in the in this case here so.",
                    "label": 0
                },
                {
                    "sent": "It takes awhile, you know, today this and meditate or this diagram.",
                    "label": 0
                },
                {
                    "sent": "But what this?",
                    "label": 0
                },
                {
                    "sent": "Got me to think is that well?",
                    "label": 0
                },
                {
                    "sent": "We used to think OK, it's great to have.",
                    "label": 0
                },
                {
                    "sent": "Multivariate method is great to have the possibility of including features that by them self are irrelevant and that become relevant in the context of other.",
                    "label": 0
                },
                {
                    "sent": "But by now I think it's a red flag when I have a feature like that that's complete garbage by itself and that becomes relevant in the context of others.",
                    "label": 0
                },
                {
                    "sent": "I have to start thinking is it truly relevant variable or is it an artifact?",
                    "label": 0
                },
                {
                    "sent": "Is this something that you know it's something my my feature, but maybe maybe it's just annoys variable that's plugging too.",
                    "label": 0
                },
                {
                    "sent": "Into my result.",
                    "label": 0
                },
                {
                    "sent": "So I'm getting now some.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "In a in causality where people are trying to do is given a target variable, for example lung cancer that trying to find the most direct causes in the most direct effects and so called spouses spouses are variables that are parents of direct causes.",
                    "label": 0
                },
                {
                    "sent": "And those also play a role.",
                    "label": 0
                },
                {
                    "sent": "These are, you know, if we go back.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My previous graph.",
                    "label": 0
                },
                {
                    "sent": "This is, you know what people call a spouse.",
                    "label": 0
                },
                {
                    "sent": "That is, this is apparent of the children of my target variable.",
                    "label": 0
                },
                {
                    "sent": "So spouses are like this guy here, right?",
                    "label": 0
                },
                {
                    "sent": "Poses they can help you can help you getting a better separation, but you need to be cautious because they are not relevant by themselves.",
                    "label": 0
                },
                {
                    "sent": "Solo.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, here this would be a good spouse.",
                    "label": 0
                },
                {
                    "sent": "For example, if you try to use coughing as a predictor of lung cancer, you want to know whether the person is allergy, because if the person is allergic and is constantly coughing because of allergy, this is not a good indicator of whether or not the person has lung cancer.",
                    "label": 0
                },
                {
                    "sent": "Consequently, what you in that case want to do is that you want to include allergy in your set of important features.",
                    "label": 0
                },
                {
                    "sent": "But if you are in the previous case that I mentioned where you know this variable with the noise variable, you might not want to include it.",
                    "label": 0
                },
                {
                    "sent": "You might want to do a better preprocessing in order to eliminate the artifact that because if you keep it.",
                    "label": 0
                },
                {
                    "sent": "In this case it was, you know, correcting for the baseline, it's great and it's going to give you better results on your test set.",
                    "label": 0
                },
                {
                    "sent": "But tomorrow you're going to change your instrument and tomorrow you know the calibration is going to be different and that particular variable is maybe not going to be helpful.",
                    "label": 0
                },
                {
                    "sent": "It's going to be just contributing errors.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, So what works and why?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to summarize in, with or without the algorithms that work best in the challenges that we organized.",
                    "label": 0
                },
                {
                    "sent": "First of all, in the feature selection or challenge, we were surprised that some people got extremely good results without feature selection at all.",
                    "label": 0
                },
                {
                    "sent": "And that is in spite of the fact that we had injected a large number of complete garbage features.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to test the power of feature selection methods and in one way of doing that is to put purely garbage feature and see how many of these bad guys people select accidentally.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you know people who took all the features, including all the bad guys, got very good results and the initially we interpreted that as well.",
                    "label": 0
                },
                {
                    "sent": "This is great with this knew, you know, regularised kernel methods or with these new regularised neural networks or whatever we can work in these very high dimensional spaces and were insensitive to a garbage features.",
                    "label": 0
                },
                {
                    "sent": "Then this of course feature selection helps.",
                    "label": 0
                },
                {
                    "sent": "If you fix the number of features.",
                    "label": 0
                },
                {
                    "sent": "If you say, well now I want to find the best predictor that doesn't have more than 10 features, then then you really need to do it and those are real constraints.",
                    "label": 0
                },
                {
                    "sent": "Sometimes that you have in practice.",
                    "label": 0
                },
                {
                    "sent": "Now next we wanted to assess the power of causal feature selection, 'cause I, as I told you, there is some potential benefit in knowing the local causal graph because then we can interpret the features we have.",
                    "label": 0
                },
                {
                    "sent": "We can know you know if it's a spouse, well, maybe it can be useful, but we maybe have to look really at what this particular feature means and if you want later down the road to designer drug for example to cure disease, then it's going to be different if the feature that you selected is a cause or.",
                    "label": 0
                },
                {
                    "sent": "Or or in effect so.",
                    "label": 0
                },
                {
                    "sent": "Might have an interest in understanding better who your features are.",
                    "label": 0
                },
                {
                    "sent": "So we designed the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A challenge in which we had the training data in Thursday to that are differently distributed and we purposely made the test sets such that some of the features that you select on training data become harmful on test data, for example because they are consequences that you've manipulated, that you've changed somehow and they are not predictive anymore.",
                    "label": 0
                },
                {
                    "sent": "You realize that causes are always going to be predictive of your target.",
                    "label": 0
                },
                {
                    "sent": "If you don't, you know tweak your targets, people manipulate your target.",
                    "label": 0
                },
                {
                    "sent": "But consequences won't.",
                    "label": 0
                },
                {
                    "sent": "So, for example it.",
                    "label": 0
                },
                {
                    "sent": "If you are studying a population of smoking people, if you prevent people from smoking.",
                    "label": 0
                },
                {
                    "sent": "They will eventually operation.",
                    "label": 0
                },
                {
                    "sent": "Eventually their health will improve, but if you administer massively cough medicine, maybe it's not going to do anything.",
                    "label": 0
                },
                {
                    "sent": "So which we tested those concepts with the challenge and figured that well.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to be that causal discovery methods have that much of an advantage over just a plain feature selection method, or even worse, over not doing feature selection at all.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "So first let me go back to the traditional way of doing a selection of the best feature set, so most people what most people do is that they.",
                    "label": 0
                },
                {
                    "sent": "Split the data into training, validation and test set and they train on training data.",
                    "label": 1
                },
                {
                    "sent": "They select the best feature test the best feature subset on validation data and then they.",
                    "label": 0
                },
                {
                    "sent": "Finally, test the final model on a separate phase data.",
                    "label": 0
                },
                {
                    "sent": "So of course you can do cross validation by having multiple plates like that, but let's for simplicity assume that you just have one training set when validation set and one tester.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People have noticed that the best working feature selection methods.",
                    "label": 1
                },
                {
                    "sent": "Often universe methods.",
                    "label": 0
                },
                {
                    "sent": "And or very simple nested subset methods.",
                    "label": 0
                },
                {
                    "sent": "This does the next next best thing after you need various method is those methods where you do forward selection or backward elimination.",
                    "label": 0
                },
                {
                    "sent": "Just the.",
                    "label": 0
                },
                {
                    "sent": "Handwaving theoretical justification is by looking at bounds at the second level of inference, so using the validation set this time.",
                    "label": 0
                },
                {
                    "sent": "Using your validation set, you're going to learn which feature subset is best, and this is a learning problem in a finite number of predictors.",
                    "label": 1
                },
                {
                    "sent": "So if you're learning with a finite number of predictors, then what's governing your complexity is the log of the number of things you've been trying.",
                    "label": 1
                },
                {
                    "sent": "And in that case, if you're trying an exhaustive search, you have to do the impossible feature subsets, so your complexity sort of right is N, or as if you're trying only and things with like with feature ranking or N N + 1 / 2 like in Fort selection of backward elimination, you have only lug event in your complexity.",
                    "label": 0
                },
                {
                    "sent": "Which is, you know why this is the classical you know picture you 100 times, which is a justification why you know a few rappers that do a extensive search tend to overfit more than.",
                    "label": 1
                },
                {
                    "sent": "Then for selection of feature ranking methods that have had more success.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "No, not my second story.",
                    "label": 0
                },
                {
                    "sent": "This the story about the sensitivity of your relevant features.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So asking that you you believe me that have a justification for the insensitivity of.",
                    "label": 0
                },
                {
                    "sent": "Of the learning algorithms to having a large number of garbage features.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With what we got out of these benchmarks is that multi right feature selection is in principle more powerful than univariate feature selection, but not always.",
                    "label": 1
                },
                {
                    "sent": "In practice that you need to take a closer look at the type of dependencies that you have in in terms of causal relationship and that feature selection causal discovery.",
                    "label": 0
                },
                {
                    "sent": "Mabel may be more harmful than useful so often if you only care about predictive performance, it may be better just to take all your features and just build a predictor.",
                    "label": 0
                },
                {
                    "sent": "If you only if you care about understanding your data, generative process or limiting the number of features for some efficiency reason.",
                    "label": 0
                },
                {
                    "sent": "You might want to consider you know, going into these these methods.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, great.",
                    "label": 0
                },
                {
                    "sent": "It does so.",
                    "label": 0
                },
                {
                    "sent": "This is the slide in right?",
                    "label": 0
                },
                {
                    "sent": "I skipped here I.",
                    "label": 0
                },
                {
                    "sent": "Very roughly estimated, the penalty that you have for omitting a good feature versus including a bad feature, and it's completely symmetric.",
                    "label": 0
                },
                {
                    "sent": "It's much more penalizing to admit a good feature that we included that feature.",
                    "label": 0
                },
                {
                    "sent": "And the scaling roughly is like if you have Ng good features you can afford having the square root of bad features over M, the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "So eventually, if you have an infinite number of chronic samples, you can have an infinite number of bad features, right?",
                    "label": 0
                },
                {
                    "sent": "You're incurring some noise, and you can bound.",
                    "label": 0
                },
                {
                    "sent": "You know you're the error that you're making.",
                    "label": 0
                },
                {
                    "sent": "So basically I'm looking at the standard deviation of the error that you're that you're making in this argument here.",
                    "label": 0
                },
                {
                    "sent": "And it's it's good.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit worse if you do this causal discovery thing because you're manipulating variables in the test set.",
                    "label": 0
                },
                {
                    "sent": "So you're training on something and you're testing on something different, so some of the features now.",
                    "label": 0
                },
                {
                    "sent": "You can't know that they're they're bad by looking at training data, but it still kind of cancel out each other because some some will be on.",
                    "label": 0
                },
                {
                    "sent": "Some will be a few in test data, so they contribute some noise, but the but the informative features they always kind of aligned with the right answer, so informative features they give you more.",
                    "label": 0
                },
                {
                    "sent": "Mileage then.",
                    "label": 0
                },
                {
                    "sent": "Then the noise contributed by the back issues there.",
                    "label": 0
                },
                {
                    "sent": "In essence, the good features contribute each positively and then the bad features contribute with the.",
                    "label": 0
                },
                {
                    "sent": "The dancing traditionally.",
                    "label": 0
                },
                {
                    "sent": "Collection depends what you want to do right.",
                    "label": 0
                },
                {
                    "sent": "Depends what you want to do again to know we're not throwing away feature selection.",
                    "label": 0
                },
                {
                    "sent": "Just saying that if you don't need it, you don't necessarily want it.",
                    "label": 0
                }
            ]
        }
    }
}