{
    "id": "ewnngszzbzavcqx7gzqaq324hz2sz6b3",
    "title": "Scalable Knowledge Harvesting with High Precision and High Recall",
    "info": {
        "author": [
            "Ndapandula Nakashole, Max Planck Institute for Informatics, Max Planck Institute"
        ],
        "published": "Aug. 9, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm2011_nakashole_skh/",
    "segmentation": [
        [
            "So Miami Snap-on Acachalla, and I'll be talking about scalable knowledge harvesting with high precision and high recall, and this is joint work with modern Theobald Hanguard vicam at the Max Planck Institute for Informatics in."
        ],
        [
            "Germany.",
            "So one of the main tasks of knowledge harvesting is that of organizing text into precise facts, and this entails scanning through natural language text, an identifying facts or assertions which are deemed to be interesting and such interesting facts can be found in various places on the web.",
            "For example on home pages or news websites or Wikipedia articles.",
            "So for example, the Wikipedia page of an athlete might mention the sports leagues that he plays in, or the sports teams that he has played for.",
            "And other relevant details.",
            "And So what not harvesting methods aim to do is to essentially have this information represented in a more structured way, and having this information explicitly stated out has the advantage that it can potentially support are more advanced applications than our current state of the art can handle such applications as a semantic search for example, or question answering and various other applications.",
            "And there are lots of ongoing efforts towards automatic knowledge harvesting.",
            "Several methods have emerged, each taking a slightly different approach."
        ],
        [
            "So if we were to classify the prevalent approaches are to knowledge harvesting will end up with roughly two categories.",
            "The first one is made up of methods that are primarily based on matching text patterns and the way these work at a high level.",
            "Essentially they take a few seed examples as a form of supervision and they see examples are used to identify patterns in text and in turn the patterns are used to find new facts in text and essentially the.",
            "The entire process can be repeated in multiple iterations, whereby the new facts are used as the seeds of the next iteration.",
            "Now, on the other hand, there are also methods that, in addition to using pattern based techniques, they also use a constraint aware reasoning and that means they make use of rules to reason about the possibility of the extractions as well as to infer knew facts that have not been explicitly stated in text.",
            "So for example, there might be a constraint rule which says in athlete can only belong to a single national team or an inference rule which says teammates usually belong to the same team and so such an inference rule can be used to infer.",
            "Teams of those athletes, athletes whose teammates are known but their teams have not necessarily been stated in text."
        ],
        [
            "Now, in terms of performance, the two approaches differ quite significantly.",
            "Our pattern based approaches tend to have a high recall and cannot scale seamlessly to large collections, but their position often suffers.",
            "On the other hand, are constrained away.",
            "Approaches tend to have high precision, but because they can be very conservative they can end up with very low recall and also they can very computationally intensive and that results in a poor scaleability.",
            "So really, there's a 3 way tradeoff between.",
            "Precision, recalling scaleability in current state of the art.",
            "Approaches to a.",
            "Knowledge harvesting another issue which is often not addressed in either approach, is that of a Canonical entities.",
            "So the idea here is that the extracted fact should refer to Chronicle disambiguate IDT entities so that multiple representations of the same entity should be resolved.",
            "Today, a unique representation of the entity.",
            "So, for example, a Lakers, Los Angeles Lakers, and Lakers should all be disambiguated to a unique representation of the entity, and this is really important to ensure that.",
            "The fact that I extracted I have no ambiguities whatsoever."
        ],
        [
            "Sure.",
            "So the main question really is then how do we reconcile a precision recall and scalability while delivering high quality input output that is disambiguated and does not have any ambiguities?",
            "In our work we have tried or we have works to what reconciling these are three metrics in a system called Prosper.",
            "We contribute first of all towards recall by making use of generalized patterns which essentially aimed to capture approximate matches within patent, while at the same time are able to withstand noisy patterns that might be encountered and in terms of precision we make use of streamlines are constrained our reasoning.",
            "The idea here is that we only pursue those candidates that are promising enough so that we can cut out on our runtimes and in terms of scaleability.",
            "We distribute our algorithms as map reduce algorithms in order to do to essentially ensure better scalability."
        ],
        [
            "So in the rest of the talk, I will give a bit more details on each of these PROSPER components, and then I'll elaborate on the experimental evaluation that we carried out, and I conclude."
        ],
        [
            "To talk.",
            "So at a high level, prosper is the three phase architecture which takes as input a few seed examples in some kind of examples, Anna text corpus and the first phase, which is that of pattern gathering, essentially identifies patterns in the corpus.",
            "That is, it identifies phrases occurring between entity pairs and the second phase, which is that of button analysis, essentially takes the output of the pattern gathering phase and generalizes the patterns to ensure that.",
            "They allow for approximate matches while at the same time handling any noisy patterns that might be encountered later on.",
            "And the last face is that of reasoning.",
            "Essentially, this phase generates, generates facts are based on the occurrences of Paris of part of speech tagged enhanced patterns, which are generalized to allow for large scale extraction in terms of brickel, while at the same time ensuring that these are highly precise facts by making use of this logical reasoning component, which essentially gets rid of the force of positives.",
            "And so then, here again, the entire process can be repeated in multiple iterations, whereby the output of the reasoner is used as the input of the next iteration.",
            "And here all the three faces are distributed as map reduce algorithms.",
            "I'll talk a bit more about that later on, but first of all, I'll go into the details of each of the three phases."
        ],
        [
            "So now if you take a closer look at pattern gathering, suppose we encounter sentence which talks about Alex Rodriguez hitting a home run for the New York Yankees.",
            "Now, if we already have a seat example which says Alex Rodriguez plays for the New York Yankees, then essentially we can consider this phrase occurring between the two entities as an indicator that it's a good pattern for expressing their relation athlete plays for team.",
            "But the issue here though is that the pattern is overly specific that it will not match a similar, but not exactly the same sentences.",
            "That will encounter in text.",
            "So what we really want to be able to do here is to allow for approximate matches in order to help boost recall.",
            "And there are several possible ways one could do this.",
            "One could make use of... whereby certain words are replaced by the para speech tags, or one could use regular expressions or a dependency parsing.",
            "But the issue here is that these methods can be computationally intensive and in terms of... they are prone to the.",
            "Pitfall of potentially semantically diluted results."
        ],
        [
            "So what we do is we take a slightly different approach, which is how we feed all the parents into a frequent engram itemset mining algorithm, which aims to identify frequently Co occurring sub patterns and so for example for the previous pattern algorithm might capture the essence of the sentence with a 2 sub patterns such as heat.",
            "He's or home run helping B and so having these kind of subpatterns then they can be used as a basis of approximate matches thereby helping too.",
            "Boost or recall.",
            "Now we can still encounter variations with regard to adjectives and other non contact bearing words such as pronouns, and for this we make use of part of speech tags whereby at certain words are replaced by their power switch tags."
        ],
        [
            "Now we can also still encounter misleading patterns that can lead to force.",
            "Extractions are such pattern sees as X is a fun of Y or X was drafted by Y and to control the effect of such patterns, make use of counterexamples.",
            "And the idea here is that if you actually see a pattern in multiple contexts with different cities that stay in, that's 10 in widely different relations, then that's an indicator of a poor quality.",
            "Button."
        ],
        [
            "So we take these counterexamples, and the seeds are examples themselves and take note of the pattern occurrence information with these seeds and the counter seeds and compute button quality measures.",
            "The first one is the support, which essentially reflects the frequency of the pattern in the corpus, and the second one is the confidence which reflects the likelihood that the pattern indeed expresses the relation.",
            "It said to express, given the current information with the seeds and the counter seeds.",
            "And the last confidence measure or quality measure is the weight, which essentially is the product of the support and confident and so each pattern essentially has this confidence weight assigned to it."
        ],
        [
            "So now by matching these weighted stick buttons two sentences I encountered in text, we generated a set of seed or sort of.",
            "A candidate facts and these are the sessions are who's truth values we are trying to determine.",
            "But before we do that, we aggregate them in two ways.",
            "Are first of all by fact candidates because each factor needed can be seen in multiple or Contacts with different patterns and the 2nd way of releasing these is by patterns because essentially these aggregated weight will reflect the quality of the pattern based on the instructions it has led to."
        ],
        [
            "So once we have, these are aggregated wait for the candidate fact.",
            "We can then use them as input to a logical reasoning component which essentially serves to prune force our candidates and the way we do this.",
            "We build on rule based reasoning model which then assigns through values to these five candidates by solving a maximum satisfiability problem over the instantiated rules.",
            "And for this we make improvements of these.",
            "First of all, we judiciously prune the inputs of original so that we only pursue the more sort of promising candidates in order to ensure that we cut down on the runtimes, and also by making use of this informed wait, we essentially guide the reason I into the more likely salute."
        ],
        [
            "It's.",
            "So in terms of scaleability, each of the PROSPER components are distributed as map reduce algorithms.",
            "The first phase, which is that of pattern gathering, is essentially trivially paralyzed by a single map reduce job, since each document can be processed independently, but the pattern analysis component is accomplished by a sequence of map reduce jobs.",
            "So for example this one for engram items that generation, or this one for.",
            "Putting weight computation.",
            "This one for factory data generation and so these together help to accomplish that particular."
        ],
        [
            "Race.",
            "Distributed reasoning, on the other hand, is not as straightforward because the input there is linked in a nontrivial manner, and so there are no clear lines along which to partition it, and what you do here we represent the input of the reason as a graph.",
            "Essentially, statements of grounded rules are verticies, and there's an edge between vertices that have literals in common.",
            "So ideally a good partitioning strategy should leave us such a connected component.",
            "In the same competition, and for this we make use of our minimum cut edge of partitioning algorithms whereby we try to essentially cut as few edges as possible to ensure that as few information as possible is lost, and for this we make use of our well established approximation algorithms for our graph partitioning."
        ],
        [
            "So now getting to the experiment, we carried out experiments on the blue web corpus and did comparisons to our current state of the art results that were reported in the triple AI conference last year, and this was also distributed implementation and for the domains we primarily focused on two domains for two reasons, are first of all, the sports domain was chosen are primarily because the reported results where for the large part on the spots domain and the academic domain was chosen in order to stress test the.",
            "Yeah, the reason are in terms of our domain specific constraint."
        ],
        [
            "So here's a small part of the opposition.",
            "Recall our experiments on the on the esports relations, and here, across the board for all relations are prosperity is much higher.",
            "Recall than Nell.",
            "And in terms of precision, prosperous precision is in the 80s with us or as nails.",
            "Precision is reported to be 100%, but in fact if you look at the top 1000 extractions are prosperous.",
            "Actually has the same precision as that of Nell while still maintaining much higher.",
            "Recall."
        ],
        [
            "In terms of runtimes.",
            "The experiments on this plus domain for Prospero were completed in about 2 1/2 days.",
            "Are worse nails results were obtained over 66 days.",
            "Overall.",
            "This graph here shows that the.",
            "For the for the large pot, the runtime essentially here made up of the penalizes component for the sports domain as opposed to reasoning, because in the sports domain there are not that many domain specific constraints to deal with."
        ],
        [
            "In terms of accumulated facts here, each iteration progressively picks up more facts suggesting that further iterations would lead to more facts."
        ],
        [
            "In terms of quality of output here the prospera facts verse to a disambiguate IDT entities whereby the team Chicago Bulls is always referred to as Chicago Bulls, as opposed to sometimes using Bulls or Chicago Bulls, not necessarily knowing whether they are the same team or not.",
            "It's kind of high quality output is really important not to support compelling applications with this kind of data."
        ],
        [
            "In terms of our speedup obtained in paralyzing the reasoner here, by increasing the number of processing units involved, indeed, there is a significant speedup obtained in the runtimes."
        ],
        [
            "So to wrap up, essentially we have made contributions towards reconciling precision, recalling scalability in knowledge harvesting.",
            "We made contributions to what each of the metrics and in terms of experiments are.",
            "Experiments showed that we make significant gains over our current state of the art and our experimental data is available at the project website, and you're more than welcome to take a look."
        ],
        [
            "Thanks.",
            "Any questions?",
            "OK, so in the last step you use a Mac set to solve the constraints.",
            "Do you run into scalability problem?",
            "Sorry, can you say that again use Mac set to solve the constraints in the last step?",
            "Do you run into scalability problem now?",
            "Actually no, in comparison to the previous work on actually using the Mac set to solve the.",
            "Scientist values The fact candidate here.",
            "What you do is that we approve the inputs of.",
            "The reason are so that we only pursue those candidates that are promising.",
            "So with this data actually on this particular domains, we didn't actually run into a scalability problems due to this sort of well informed pruning of the of the inputs of the reason.",
            "Can you tell like how many rules in the reasoning system for the sports domain?",
            "For example, the sports domain had more or less only the general purpose type of rules, where, for example, the pattern factor factor duality rules but not any specific set of rules specific to the domain.",
            "And that's the reason why we essentially worked with the academic domain as well to ensure that.",
            "I will also look at the domain that has more sort of domain specific constraints, But here there are there is you input a few rules really.",
            "Not that you spend days trying to specify these rules.",
            "The idea is to bootstrap the process with some sort of human input, but essentially then you gather enough data at some point that where you can actually expand the process by learning more patterns from this data in terms of rules.",
            "Or extractions more than you can actually specify with any sort of human effort.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Miami Snap-on Acachalla, and I'll be talking about scalable knowledge harvesting with high precision and high recall, and this is joint work with modern Theobald Hanguard vicam at the Max Planck Institute for Informatics in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Germany.",
                    "label": 0
                },
                {
                    "sent": "So one of the main tasks of knowledge harvesting is that of organizing text into precise facts, and this entails scanning through natural language text, an identifying facts or assertions which are deemed to be interesting and such interesting facts can be found in various places on the web.",
                    "label": 0
                },
                {
                    "sent": "For example on home pages or news websites or Wikipedia articles.",
                    "label": 0
                },
                {
                    "sent": "So for example, the Wikipedia page of an athlete might mention the sports leagues that he plays in, or the sports teams that he has played for.",
                    "label": 0
                },
                {
                    "sent": "And other relevant details.",
                    "label": 0
                },
                {
                    "sent": "And So what not harvesting methods aim to do is to essentially have this information represented in a more structured way, and having this information explicitly stated out has the advantage that it can potentially support are more advanced applications than our current state of the art can handle such applications as a semantic search for example, or question answering and various other applications.",
                    "label": 0
                },
                {
                    "sent": "And there are lots of ongoing efforts towards automatic knowledge harvesting.",
                    "label": 1
                },
                {
                    "sent": "Several methods have emerged, each taking a slightly different approach.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we were to classify the prevalent approaches are to knowledge harvesting will end up with roughly two categories.",
                    "label": 1
                },
                {
                    "sent": "The first one is made up of methods that are primarily based on matching text patterns and the way these work at a high level.",
                    "label": 1
                },
                {
                    "sent": "Essentially they take a few seed examples as a form of supervision and they see examples are used to identify patterns in text and in turn the patterns are used to find new facts in text and essentially the.",
                    "label": 0
                },
                {
                    "sent": "The entire process can be repeated in multiple iterations, whereby the new facts are used as the seeds of the next iteration.",
                    "label": 1
                },
                {
                    "sent": "Now, on the other hand, there are also methods that, in addition to using pattern based techniques, they also use a constraint aware reasoning and that means they make use of rules to reason about the possibility of the extractions as well as to infer knew facts that have not been explicitly stated in text.",
                    "label": 0
                },
                {
                    "sent": "So for example, there might be a constraint rule which says in athlete can only belong to a single national team or an inference rule which says teammates usually belong to the same team and so such an inference rule can be used to infer.",
                    "label": 0
                },
                {
                    "sent": "Teams of those athletes, athletes whose teammates are known but their teams have not necessarily been stated in text.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in terms of performance, the two approaches differ quite significantly.",
                    "label": 0
                },
                {
                    "sent": "Our pattern based approaches tend to have a high recall and cannot scale seamlessly to large collections, but their position often suffers.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, are constrained away.",
                    "label": 0
                },
                {
                    "sent": "Approaches tend to have high precision, but because they can be very conservative they can end up with very low recall and also they can very computationally intensive and that results in a poor scaleability.",
                    "label": 0
                },
                {
                    "sent": "So really, there's a 3 way tradeoff between.",
                    "label": 0
                },
                {
                    "sent": "Precision, recalling scaleability in current state of the art.",
                    "label": 0
                },
                {
                    "sent": "Approaches to a.",
                    "label": 0
                },
                {
                    "sent": "Knowledge harvesting another issue which is often not addressed in either approach, is that of a Canonical entities.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that the extracted fact should refer to Chronicle disambiguate IDT entities so that multiple representations of the same entity should be resolved.",
                    "label": 1
                },
                {
                    "sent": "Today, a unique representation of the entity.",
                    "label": 1
                },
                {
                    "sent": "So, for example, a Lakers, Los Angeles Lakers, and Lakers should all be disambiguated to a unique representation of the entity, and this is really important to ensure that.",
                    "label": 0
                },
                {
                    "sent": "The fact that I extracted I have no ambiguities whatsoever.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So the main question really is then how do we reconcile a precision recall and scalability while delivering high quality input output that is disambiguated and does not have any ambiguities?",
                    "label": 1
                },
                {
                    "sent": "In our work we have tried or we have works to what reconciling these are three metrics in a system called Prosper.",
                    "label": 0
                },
                {
                    "sent": "We contribute first of all towards recall by making use of generalized patterns which essentially aimed to capture approximate matches within patent, while at the same time are able to withstand noisy patterns that might be encountered and in terms of precision we make use of streamlines are constrained our reasoning.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that we only pursue those candidates that are promising enough so that we can cut out on our runtimes and in terms of scaleability.",
                    "label": 0
                },
                {
                    "sent": "We distribute our algorithms as map reduce algorithms in order to do to essentially ensure better scalability.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the rest of the talk, I will give a bit more details on each of these PROSPER components, and then I'll elaborate on the experimental evaluation that we carried out, and I conclude.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To talk.",
                    "label": 0
                },
                {
                    "sent": "So at a high level, prosper is the three phase architecture which takes as input a few seed examples in some kind of examples, Anna text corpus and the first phase, which is that of pattern gathering, essentially identifies patterns in the corpus.",
                    "label": 1
                },
                {
                    "sent": "That is, it identifies phrases occurring between entity pairs and the second phase, which is that of button analysis, essentially takes the output of the pattern gathering phase and generalizes the patterns to ensure that.",
                    "label": 0
                },
                {
                    "sent": "They allow for approximate matches while at the same time handling any noisy patterns that might be encountered later on.",
                    "label": 0
                },
                {
                    "sent": "And the last face is that of reasoning.",
                    "label": 0
                },
                {
                    "sent": "Essentially, this phase generates, generates facts are based on the occurrences of Paris of part of speech tagged enhanced patterns, which are generalized to allow for large scale extraction in terms of brickel, while at the same time ensuring that these are highly precise facts by making use of this logical reasoning component, which essentially gets rid of the force of positives.",
                    "label": 0
                },
                {
                    "sent": "And so then, here again, the entire process can be repeated in multiple iterations, whereby the output of the reasoner is used as the input of the next iteration.",
                    "label": 0
                },
                {
                    "sent": "And here all the three faces are distributed as map reduce algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a bit more about that later on, but first of all, I'll go into the details of each of the three phases.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if you take a closer look at pattern gathering, suppose we encounter sentence which talks about Alex Rodriguez hitting a home run for the New York Yankees.",
                    "label": 1
                },
                {
                    "sent": "Now, if we already have a seat example which says Alex Rodriguez plays for the New York Yankees, then essentially we can consider this phrase occurring between the two entities as an indicator that it's a good pattern for expressing their relation athlete plays for team.",
                    "label": 0
                },
                {
                    "sent": "But the issue here though is that the pattern is overly specific that it will not match a similar, but not exactly the same sentences.",
                    "label": 0
                },
                {
                    "sent": "That will encounter in text.",
                    "label": 0
                },
                {
                    "sent": "So what we really want to be able to do here is to allow for approximate matches in order to help boost recall.",
                    "label": 0
                },
                {
                    "sent": "And there are several possible ways one could do this.",
                    "label": 0
                },
                {
                    "sent": "One could make use of... whereby certain words are replaced by the para speech tags, or one could use regular expressions or a dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "But the issue here is that these methods can be computationally intensive and in terms of... they are prone to the.",
                    "label": 0
                },
                {
                    "sent": "Pitfall of potentially semantically diluted results.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is we take a slightly different approach, which is how we feed all the parents into a frequent engram itemset mining algorithm, which aims to identify frequently Co occurring sub patterns and so for example for the previous pattern algorithm might capture the essence of the sentence with a 2 sub patterns such as heat.",
                    "label": 1
                },
                {
                    "sent": "He's or home run helping B and so having these kind of subpatterns then they can be used as a basis of approximate matches thereby helping too.",
                    "label": 0
                },
                {
                    "sent": "Boost or recall.",
                    "label": 0
                },
                {
                    "sent": "Now we can still encounter variations with regard to adjectives and other non contact bearing words such as pronouns, and for this we make use of part of speech tags whereby at certain words are replaced by their power switch tags.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can also still encounter misleading patterns that can lead to force.",
                    "label": 0
                },
                {
                    "sent": "Extractions are such pattern sees as X is a fun of Y or X was drafted by Y and to control the effect of such patterns, make use of counterexamples.",
                    "label": 1
                },
                {
                    "sent": "And the idea here is that if you actually see a pattern in multiple contexts with different cities that stay in, that's 10 in widely different relations, then that's an indicator of a poor quality.",
                    "label": 0
                },
                {
                    "sent": "Button.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we take these counterexamples, and the seeds are examples themselves and take note of the pattern occurrence information with these seeds and the counter seeds and compute button quality measures.",
                    "label": 0
                },
                {
                    "sent": "The first one is the support, which essentially reflects the frequency of the pattern in the corpus, and the second one is the confidence which reflects the likelihood that the pattern indeed expresses the relation.",
                    "label": 0
                },
                {
                    "sent": "It said to express, given the current information with the seeds and the counter seeds.",
                    "label": 0
                },
                {
                    "sent": "And the last confidence measure or quality measure is the weight, which essentially is the product of the support and confident and so each pattern essentially has this confidence weight assigned to it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now by matching these weighted stick buttons two sentences I encountered in text, we generated a set of seed or sort of.",
                    "label": 1
                },
                {
                    "sent": "A candidate facts and these are the sessions are who's truth values we are trying to determine.",
                    "label": 0
                },
                {
                    "sent": "But before we do that, we aggregate them in two ways.",
                    "label": 0
                },
                {
                    "sent": "Are first of all by fact candidates because each factor needed can be seen in multiple or Contacts with different patterns and the 2nd way of releasing these is by patterns because essentially these aggregated weight will reflect the quality of the pattern based on the instructions it has led to.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have, these are aggregated wait for the candidate fact.",
                    "label": 0
                },
                {
                    "sent": "We can then use them as input to a logical reasoning component which essentially serves to prune force our candidates and the way we do this.",
                    "label": 1
                },
                {
                    "sent": "We build on rule based reasoning model which then assigns through values to these five candidates by solving a maximum satisfiability problem over the instantiated rules.",
                    "label": 0
                },
                {
                    "sent": "And for this we make improvements of these.",
                    "label": 0
                },
                {
                    "sent": "First of all, we judiciously prune the inputs of original so that we only pursue the more sort of promising candidates in order to ensure that we cut down on the runtimes, and also by making use of this informed wait, we essentially guide the reason I into the more likely salute.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So in terms of scaleability, each of the PROSPER components are distributed as map reduce algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first phase, which is that of pattern gathering, is essentially trivially paralyzed by a single map reduce job, since each document can be processed independently, but the pattern analysis component is accomplished by a sequence of map reduce jobs.",
                    "label": 1
                },
                {
                    "sent": "So for example this one for engram items that generation, or this one for.",
                    "label": 0
                },
                {
                    "sent": "Putting weight computation.",
                    "label": 0
                },
                {
                    "sent": "This one for factory data generation and so these together help to accomplish that particular.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Race.",
                    "label": 0
                },
                {
                    "sent": "Distributed reasoning, on the other hand, is not as straightforward because the input there is linked in a nontrivial manner, and so there are no clear lines along which to partition it, and what you do here we represent the input of the reason as a graph.",
                    "label": 0
                },
                {
                    "sent": "Essentially, statements of grounded rules are verticies, and there's an edge between vertices that have literals in common.",
                    "label": 1
                },
                {
                    "sent": "So ideally a good partitioning strategy should leave us such a connected component.",
                    "label": 0
                },
                {
                    "sent": "In the same competition, and for this we make use of our minimum cut edge of partitioning algorithms whereby we try to essentially cut as few edges as possible to ensure that as few information as possible is lost, and for this we make use of our well established approximation algorithms for our graph partitioning.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now getting to the experiment, we carried out experiments on the blue web corpus and did comparisons to our current state of the art results that were reported in the triple AI conference last year, and this was also distributed implementation and for the domains we primarily focused on two domains for two reasons, are first of all, the sports domain was chosen are primarily because the reported results where for the large part on the spots domain and the academic domain was chosen in order to stress test the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the reason are in terms of our domain specific constraint.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a small part of the opposition.",
                    "label": 0
                },
                {
                    "sent": "Recall our experiments on the on the esports relations, and here, across the board for all relations are prosperity is much higher.",
                    "label": 0
                },
                {
                    "sent": "Recall than Nell.",
                    "label": 0
                },
                {
                    "sent": "And in terms of precision, prosperous precision is in the 80s with us or as nails.",
                    "label": 0
                },
                {
                    "sent": "Precision is reported to be 100%, but in fact if you look at the top 1000 extractions are prosperous.",
                    "label": 0
                },
                {
                    "sent": "Actually has the same precision as that of Nell while still maintaining much higher.",
                    "label": 0
                },
                {
                    "sent": "Recall.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of runtimes.",
                    "label": 0
                },
                {
                    "sent": "The experiments on this plus domain for Prospero were completed in about 2 1/2 days.",
                    "label": 0
                },
                {
                    "sent": "Are worse nails results were obtained over 66 days.",
                    "label": 1
                },
                {
                    "sent": "Overall.",
                    "label": 0
                },
                {
                    "sent": "This graph here shows that the.",
                    "label": 0
                },
                {
                    "sent": "For the for the large pot, the runtime essentially here made up of the penalizes component for the sports domain as opposed to reasoning, because in the sports domain there are not that many domain specific constraints to deal with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of accumulated facts here, each iteration progressively picks up more facts suggesting that further iterations would lead to more facts.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of quality of output here the prospera facts verse to a disambiguate IDT entities whereby the team Chicago Bulls is always referred to as Chicago Bulls, as opposed to sometimes using Bulls or Chicago Bulls, not necessarily knowing whether they are the same team or not.",
                    "label": 0
                },
                {
                    "sent": "It's kind of high quality output is really important not to support compelling applications with this kind of data.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of our speedup obtained in paralyzing the reasoner here, by increasing the number of processing units involved, indeed, there is a significant speedup obtained in the runtimes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to wrap up, essentially we have made contributions towards reconciling precision, recalling scalability in knowledge harvesting.",
                    "label": 1
                },
                {
                    "sent": "We made contributions to what each of the metrics and in terms of experiments are.",
                    "label": 0
                },
                {
                    "sent": "Experiments showed that we make significant gains over our current state of the art and our experimental data is available at the project website, and you're more than welcome to take a look.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so in the last step you use a Mac set to solve the constraints.",
                    "label": 0
                },
                {
                    "sent": "Do you run into scalability problem?",
                    "label": 0
                },
                {
                    "sent": "Sorry, can you say that again use Mac set to solve the constraints in the last step?",
                    "label": 0
                },
                {
                    "sent": "Do you run into scalability problem now?",
                    "label": 0
                },
                {
                    "sent": "Actually no, in comparison to the previous work on actually using the Mac set to solve the.",
                    "label": 0
                },
                {
                    "sent": "Scientist values The fact candidate here.",
                    "label": 0
                },
                {
                    "sent": "What you do is that we approve the inputs of.",
                    "label": 0
                },
                {
                    "sent": "The reason are so that we only pursue those candidates that are promising.",
                    "label": 0
                },
                {
                    "sent": "So with this data actually on this particular domains, we didn't actually run into a scalability problems due to this sort of well informed pruning of the of the inputs of the reason.",
                    "label": 0
                },
                {
                    "sent": "Can you tell like how many rules in the reasoning system for the sports domain?",
                    "label": 0
                },
                {
                    "sent": "For example, the sports domain had more or less only the general purpose type of rules, where, for example, the pattern factor factor duality rules but not any specific set of rules specific to the domain.",
                    "label": 0
                },
                {
                    "sent": "And that's the reason why we essentially worked with the academic domain as well to ensure that.",
                    "label": 0
                },
                {
                    "sent": "I will also look at the domain that has more sort of domain specific constraints, But here there are there is you input a few rules really.",
                    "label": 0
                },
                {
                    "sent": "Not that you spend days trying to specify these rules.",
                    "label": 0
                },
                {
                    "sent": "The idea is to bootstrap the process with some sort of human input, but essentially then you gather enough data at some point that where you can actually expand the process by learning more patterns from this data in terms of rules.",
                    "label": 0
                },
                {
                    "sent": "Or extractions more than you can actually specify with any sort of human effort.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}