{
    "id": "tlopz6uh3usl42cbdvxy2ihcyee24bst",
    "title": "Aspects of Semi-Supervised and Active Learning in Conditional Random Fields",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Nataliya Sokolovska, Macquarie University"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_sokolovska_semisupervised/",
    "segmentation": [
        [
            "I'm actually.",
            "And I'm going to tell you about our contributions to this.",
            "Active learning in conditional random fields."
        ],
        [
            "So I will stand by our motivation.",
            "Creation is the natural language processing actually, and we consider the model of conditional random fields.",
            "Since this model is, it's an old model, it's in 10 year old model, but it's still the state of the art model to make the natural language.",
            "So if you are interested in two aspects we were interested in same supervised learning and active loading, so our contribution is based on the following idea that if we have a lot of unlabeled observation, we can estimate the marginal probability of observations.",
            "So we challenged.",
            "Two questions how we can introduce the marginal probability warm celebrations into the semi supervised probabilistic parent and two.",
            "Some framework will dictate.",
            "So we performed several experiments.",
            "I'm going to share our results with you.",
            "We have performed to our experiments on two realistic datasets, namely on monetization problem.",
            "It said network task and an order named Entity Recognition program."
        ],
        [
            "So some words about our motivation, so we generally we are interested in the problem of sequence learning, and in particular in natural language processing.",
            "Linguistic data are data with very rich complex underlying structure, so we decided to consider to work with the model of conditional random fields.",
            "So.",
            "We know that label data extremely expensive and unlabeled data extremely cheap, so we try to answer 2 questions.",
            "One of the questions in the question of a citizen of Estonia.",
            "It's counter exploit unlabeled data, and the second question is how to perform active learning.",
            "How to choose instances of five training videos?"
        ],
        [
            "So let's formalize a little bit the problem of sequence labeling.",
            "We have an independent label sequences.",
            "We have observations which are fixed, we have labels, which are why we suppose that our observations and labels are aligned and they have the same."
        ],
        [
            "So we are interested in the discriminative probabilistic parametric models.",
            "So our aim is to minimize the following function, which is the negated conditional maximum likelihood.",
            "We've been on my cell will function via standard L2 norm in order to avoid overfitting, and we have a conditional probability.",
            "Can you?"
        ],
        [
            "In our case, is the model of permission already fields which is a discriminative probabilistic parametric model which models directly the probability often output, which is why given sequence of observation pics.",
            "So in this model we trust introduced 10 years ago by Lafferty and Tool F is a arbitrary set of functions in our work we consider more or less linear chain conditional random fields.",
            "Feature is a real valued parameters associated with F associated with feature functions, and since this is the probability we need some normalization."
        ],
        [
            "So a couple of words about feature functions.",
            "It's easy to explain it using the graphical model framework and condition.",
            "Other fields is a graphical model, so in our work in our contribution we have used two types of features, be great feature and unigram feature be my feature models.",
            "Dependency between current labeled previous label and current observation and its director Lambda.",
            "And the unigram features a feature of the test only the currency of current state and current, current observation, and current label.",
            "And then this is the new vector.",
            "The green one, so it's necessary to to say that even using only these two feature functions, the complexity of the model is quite high, so the complexity is as follows where.",
            "Cardiology or the alphabet of observations?",
            "And why is the cardinality of set of weights?"
        ],
        [
            "So couple of words of the real data set we worked on.",
            "We walked on a finalization task is a very intuitive task.",
            "We have 20,000 English words and their fanatical transcriptions.",
            "Each observation selectors and wife for names.",
            "So we want to predict a phonetic phonetic transcription given and Eric report."
        ],
        [
            "The second application is the named Entity Recognition task known as well as Cornell 2003 Challenge.",
            "Um?",
            "They did.",
            "We say we want to predict a sequence of labels which are named entities, but we want to do it based on three sequences or observationes which are worse part of speech and selected text."
        ],
        [
            "So let's.",
            "Let's pass to.",
            "This image provides probabilistic return and what we should do."
        ],
        [
            "Actually, you may have some observations and their labels.",
            "The standard conditional maximum likelihood criterion takes the following form.",
            "It's a standard one.",
            "Our idea is to wait with standard maximum likelihood criterion by probability, by marginal probability of observations.",
            "Actually, is it criterion keeping this reduced to couple of years ago?",
            "Quitus I see Mail 2008 and.",
            "We tested it already on the logistic regression model and in the current world we extrapolated it.",
            "We generalized it to the conditional random fields.",
            "So sorry to say.",
            "Which was introduced?",
            "There's some very nice statistical properties, and we have shown that this criterion is asymptotically optimal."
        ],
        [
            "So this is the extrapolation of the probabilistic stress criterion to the conditional random fields.",
            "Pull it, waited conditional random fields.",
            "Since we weight each sequence of observations so it can terabytes marginal probability.",
            "So it's the criterion be used in our work and know that be the conditional probability is the model of the conditional random fields."
        ],
        [
            "So.",
            "We have started to with a simulated data.",
            "We have simulated our data using entity Markov oil.",
            "So in this case if we simulate our data we can compute the marginal probability of observations directly.",
            "Seems to be no.",
            "The eight of these state transition probabilities and we know the observation probabilities as well.",
            "So.",
            "I show you our results on the simulated data is the box plot a boxplot of the difference between the error rates of a standard and whether conditional random fields?",
            "It's quite easy to see yet.",
            "Weighted conditional random fields outperformed the standard one when and the number of observations is quite small.",
            "However, if an is quite big.",
            "This statistical difference is not very important.",
            "It's not significant."
        ],
        [
            "So if you want to apply the criteria to be realistic data set, we have a problem.",
            "We don't have the true marginal probability observations is not possible, so we should approximate it.",
            "We working really interested in the natural language data set, so we applied the engrams linguistic models to approximate probability of observations.",
            "The probability of words, sequences, sentences.",
            "And we have chosen the ngram models with N = 3 for the network and equals 220 corner with 2003.",
            "And note that for the corner to currently three we have three types of observations, so we should take it into consideration."
        ],
        [
            "What should we set?",
            "We have been formed our experiments of the service with probabilistic with secretary and or the realistic datasets.",
            "What I should tell you that it was not statistically significant.",
            "We cannot notice statistical significance between the standard conditional order filled and waited.",
            "But these experiments give us some intuition."
        ],
        [
            "So.",
            "The censorship of respiratory and retrace produce is associated with a stratified sample.",
            "What does it mean?",
            "It means that if marginal probabilities high, we can see that this observation to be very important.",
            "If it's below.",
            "If the QR fixes low, then we can see that the observation to be not important.",
            "It's not a very good idea too.",
            "Play in the domain of the natural language.",
            "Learning why?",
            "Because rare events is much important, is frequent ones, so we decided to apply Porta sampling to select training instances efficiently.",
            "So what you do?",
            "We sort our candidates.",
            "We thought our observations according to their marginal probabilities, and we get an frequency groups.",
            "And we choose randomly.",
            "One training instance purple.",
            "That's all."
        ],
        [
            "So I present you our results on the point of the 1003.",
            "Here I will exploit.",
            "Our results.",
            "On random sampling or salt water sampling, so on the left just book club books, blood whenever the number of observation equals 10, when the right it's 50, so on the left of each box plot its results for random sampling on the right.",
            "Explore what a sampling we see that in all cases whether something outperforms random sampling.",
            "So A&B is just different.",
            "Test test."
        ],
        [
            "So on the other hands to be better than trend, and maybe it's not very exciting.",
            "So we compared our results with diffuse of algorithm which we just called foolish provides.",
            "Active learning was introduced recently by terminating on 2009.",
            "And sensor to say that what is sampling outperforms the state of the art approach.",
            "And I give you some intuition why this is the framework introduced by Tony Kanal.",
            "So what they do is they train some model or label data set.",
            "Bing yeah.",
            "Guys see this somehow reach observations should be added to the label data set to be able to be edited.",
            "Data set.",
            "They use some utility function to do it.",
            "Then they get individually labeled data, ignoring data set and we restarted the system so.",
            "You know experiments would be full utility function provided by Tony Kanal.",
            "It's a butylated function of each chooses instances where the modern is the least confident.",
            "It's a very good idea.",
            "It actually functions.",
            "But this problem to choose the utility function.",
            "How we choose voting sensors is there is still an open problem.",
            "And another problem in this framework is the stability since.",
            "We have to train model M using some initial label data.",
            "So if this model is quite bad from the beginning.",
            "The launch is not stable at all."
        ],
        [
            "So and I share with you some conclusions.",
            "Actually, it's a rather well known fact that if they're not real, hope celebration with small, then a state of the art methods most cost.",
            "State of the art methods are not stable.",
            "We have shown that Angel introduced what a mistyped, ignoring outperforms state of that methods.",
            "It's good news.",
            "Some.",
            "Not so good news is the application of this initial progress criterion is problematic since the approximation of marginal probability of sequences is A is an open question.",
            "So our perspectives as follows.",
            "We are very interested in methods to approximate marginal probability of structured data.",
            "It's not necessarily sequences, but data that can be represented as graphical models.",
            "In general, it's an open problem.",
            "So we are.",
            "We continue to work on our method and we we are interested in the theoretical analysis of the full basic deep learning method we have introduced.",
            "And the last but not least, the theoretical analysis of.",
            "This German supervisory theorem.",
            "Actually, this analysis has been already done for the asymptotic case, it's.",
            "Optimal, but what what is especially interested is the theoretical analysis of the enormous important.",
            "Thank you for your attention.",
            "Very interesting talk, any questions?",
            "For the approximation of these marginal probabilities in speech recognition, usually people use some smoothing approach, so if the.",
            "Number of observed trigrams or bigrams gets low.",
            "One gets back to these lower dimensional things and does some smoothing.",
            "Please Andre and so did you use that also?",
            "Actually, the problem is that.",
            "You can, you can can easily get zero probability in the natural language.",
            "Application is the problem that you are quite close to 0 only 10.",
            "There is one sequence with a very high probability.",
            "Any other questions?",
            "Solutions.",
            "Comments.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm actually.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to tell you about our contributions to this.",
                    "label": 0
                },
                {
                    "sent": "Active learning in conditional random fields.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will stand by our motivation.",
                    "label": 0
                },
                {
                    "sent": "Creation is the natural language processing actually, and we consider the model of conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "Since this model is, it's an old model, it's in 10 year old model, but it's still the state of the art model to make the natural language.",
                    "label": 0
                },
                {
                    "sent": "So if you are interested in two aspects we were interested in same supervised learning and active loading, so our contribution is based on the following idea that if we have a lot of unlabeled observation, we can estimate the marginal probability of observations.",
                    "label": 0
                },
                {
                    "sent": "So we challenged.",
                    "label": 0
                },
                {
                    "sent": "Two questions how we can introduce the marginal probability warm celebrations into the semi supervised probabilistic parent and two.",
                    "label": 0
                },
                {
                    "sent": "Some framework will dictate.",
                    "label": 0
                },
                {
                    "sent": "So we performed several experiments.",
                    "label": 0
                },
                {
                    "sent": "I'm going to share our results with you.",
                    "label": 0
                },
                {
                    "sent": "We have performed to our experiments on two realistic datasets, namely on monetization problem.",
                    "label": 0
                },
                {
                    "sent": "It said network task and an order named Entity Recognition program.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some words about our motivation, so we generally we are interested in the problem of sequence learning, and in particular in natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Linguistic data are data with very rich complex underlying structure, so we decided to consider to work with the model of conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know that label data extremely expensive and unlabeled data extremely cheap, so we try to answer 2 questions.",
                    "label": 0
                },
                {
                    "sent": "One of the questions in the question of a citizen of Estonia.",
                    "label": 1
                },
                {
                    "sent": "It's counter exploit unlabeled data, and the second question is how to perform active learning.",
                    "label": 0
                },
                {
                    "sent": "How to choose instances of five training videos?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's formalize a little bit the problem of sequence labeling.",
                    "label": 1
                },
                {
                    "sent": "We have an independent label sequences.",
                    "label": 0
                },
                {
                    "sent": "We have observations which are fixed, we have labels, which are why we suppose that our observations and labels are aligned and they have the same.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are interested in the discriminative probabilistic parametric models.",
                    "label": 0
                },
                {
                    "sent": "So our aim is to minimize the following function, which is the negated conditional maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "We've been on my cell will function via standard L2 norm in order to avoid overfitting, and we have a conditional probability.",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our case, is the model of permission already fields which is a discriminative probabilistic parametric model which models directly the probability often output, which is why given sequence of observation pics.",
                    "label": 1
                },
                {
                    "sent": "So in this model we trust introduced 10 years ago by Lafferty and Tool F is a arbitrary set of functions in our work we consider more or less linear chain conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "Feature is a real valued parameters associated with F associated with feature functions, and since this is the probability we need some normalization.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a couple of words about feature functions.",
                    "label": 1
                },
                {
                    "sent": "It's easy to explain it using the graphical model framework and condition.",
                    "label": 0
                },
                {
                    "sent": "Other fields is a graphical model, so in our work in our contribution we have used two types of features, be great feature and unigram feature be my feature models.",
                    "label": 0
                },
                {
                    "sent": "Dependency between current labeled previous label and current observation and its director Lambda.",
                    "label": 1
                },
                {
                    "sent": "And the unigram features a feature of the test only the currency of current state and current, current observation, and current label.",
                    "label": 0
                },
                {
                    "sent": "And then this is the new vector.",
                    "label": 0
                },
                {
                    "sent": "The green one, so it's necessary to to say that even using only these two feature functions, the complexity of the model is quite high, so the complexity is as follows where.",
                    "label": 0
                },
                {
                    "sent": "Cardiology or the alphabet of observations?",
                    "label": 0
                },
                {
                    "sent": "And why is the cardinality of set of weights?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So couple of words of the real data set we worked on.",
                    "label": 0
                },
                {
                    "sent": "We walked on a finalization task is a very intuitive task.",
                    "label": 0
                },
                {
                    "sent": "We have 20,000 English words and their fanatical transcriptions.",
                    "label": 1
                },
                {
                    "sent": "Each observation selectors and wife for names.",
                    "label": 0
                },
                {
                    "sent": "So we want to predict a phonetic phonetic transcription given and Eric report.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second application is the named Entity Recognition task known as well as Cornell 2003 Challenge.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "They did.",
                    "label": 0
                },
                {
                    "sent": "We say we want to predict a sequence of labels which are named entities, but we want to do it based on three sequences or observationes which are worse part of speech and selected text.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "Let's pass to.",
                    "label": 0
                },
                {
                    "sent": "This image provides probabilistic return and what we should do.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, you may have some observations and their labels.",
                    "label": 1
                },
                {
                    "sent": "The standard conditional maximum likelihood criterion takes the following form.",
                    "label": 1
                },
                {
                    "sent": "It's a standard one.",
                    "label": 0
                },
                {
                    "sent": "Our idea is to wait with standard maximum likelihood criterion by probability, by marginal probability of observations.",
                    "label": 0
                },
                {
                    "sent": "Actually, is it criterion keeping this reduced to couple of years ago?",
                    "label": 1
                },
                {
                    "sent": "Quitus I see Mail 2008 and.",
                    "label": 0
                },
                {
                    "sent": "We tested it already on the logistic regression model and in the current world we extrapolated it.",
                    "label": 0
                },
                {
                    "sent": "We generalized it to the conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "So sorry to say.",
                    "label": 0
                },
                {
                    "sent": "Which was introduced?",
                    "label": 0
                },
                {
                    "sent": "There's some very nice statistical properties, and we have shown that this criterion is asymptotically optimal.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the extrapolation of the probabilistic stress criterion to the conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "Pull it, waited conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "Since we weight each sequence of observations so it can terabytes marginal probability.",
                    "label": 0
                },
                {
                    "sent": "So it's the criterion be used in our work and know that be the conditional probability is the model of the conditional random fields.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have started to with a simulated data.",
                    "label": 1
                },
                {
                    "sent": "We have simulated our data using entity Markov oil.",
                    "label": 0
                },
                {
                    "sent": "So in this case if we simulate our data we can compute the marginal probability of observations directly.",
                    "label": 0
                },
                {
                    "sent": "Seems to be no.",
                    "label": 0
                },
                {
                    "sent": "The eight of these state transition probabilities and we know the observation probabilities as well.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "I show you our results on the simulated data is the box plot a boxplot of the difference between the error rates of a standard and whether conditional random fields?",
                    "label": 0
                },
                {
                    "sent": "It's quite easy to see yet.",
                    "label": 0
                },
                {
                    "sent": "Weighted conditional random fields outperformed the standard one when and the number of observations is quite small.",
                    "label": 0
                },
                {
                    "sent": "However, if an is quite big.",
                    "label": 0
                },
                {
                    "sent": "This statistical difference is not very important.",
                    "label": 0
                },
                {
                    "sent": "It's not significant.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you want to apply the criteria to be realistic data set, we have a problem.",
                    "label": 0
                },
                {
                    "sent": "We don't have the true marginal probability observations is not possible, so we should approximate it.",
                    "label": 1
                },
                {
                    "sent": "We working really interested in the natural language data set, so we applied the engrams linguistic models to approximate probability of observations.",
                    "label": 1
                },
                {
                    "sent": "The probability of words, sequences, sentences.",
                    "label": 0
                },
                {
                    "sent": "And we have chosen the ngram models with N = 3 for the network and equals 220 corner with 2003.",
                    "label": 0
                },
                {
                    "sent": "And note that for the corner to currently three we have three types of observations, so we should take it into consideration.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What should we set?",
                    "label": 0
                },
                {
                    "sent": "We have been formed our experiments of the service with probabilistic with secretary and or the realistic datasets.",
                    "label": 0
                },
                {
                    "sent": "What I should tell you that it was not statistically significant.",
                    "label": 0
                },
                {
                    "sent": "We cannot notice statistical significance between the standard conditional order filled and waited.",
                    "label": 0
                },
                {
                    "sent": "But these experiments give us some intuition.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The censorship of respiratory and retrace produce is associated with a stratified sample.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means that if marginal probabilities high, we can see that this observation to be very important.",
                    "label": 0
                },
                {
                    "sent": "If it's below.",
                    "label": 0
                },
                {
                    "sent": "If the QR fixes low, then we can see that the observation to be not important.",
                    "label": 0
                },
                {
                    "sent": "It's not a very good idea too.",
                    "label": 0
                },
                {
                    "sent": "Play in the domain of the natural language.",
                    "label": 0
                },
                {
                    "sent": "Learning why?",
                    "label": 0
                },
                {
                    "sent": "Because rare events is much important, is frequent ones, so we decided to apply Porta sampling to select training instances efficiently.",
                    "label": 1
                },
                {
                    "sent": "So what you do?",
                    "label": 0
                },
                {
                    "sent": "We sort our candidates.",
                    "label": 1
                },
                {
                    "sent": "We thought our observations according to their marginal probabilities, and we get an frequency groups.",
                    "label": 0
                },
                {
                    "sent": "And we choose randomly.",
                    "label": 0
                },
                {
                    "sent": "One training instance purple.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I present you our results on the point of the 1003.",
                    "label": 0
                },
                {
                    "sent": "Here I will exploit.",
                    "label": 0
                },
                {
                    "sent": "Our results.",
                    "label": 0
                },
                {
                    "sent": "On random sampling or salt water sampling, so on the left just book club books, blood whenever the number of observation equals 10, when the right it's 50, so on the left of each box plot its results for random sampling on the right.",
                    "label": 0
                },
                {
                    "sent": "Explore what a sampling we see that in all cases whether something outperforms random sampling.",
                    "label": 0
                },
                {
                    "sent": "So A&B is just different.",
                    "label": 0
                },
                {
                    "sent": "Test test.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on the other hands to be better than trend, and maybe it's not very exciting.",
                    "label": 0
                },
                {
                    "sent": "So we compared our results with diffuse of algorithm which we just called foolish provides.",
                    "label": 0
                },
                {
                    "sent": "Active learning was introduced recently by terminating on 2009.",
                    "label": 0
                },
                {
                    "sent": "And sensor to say that what is sampling outperforms the state of the art approach.",
                    "label": 0
                },
                {
                    "sent": "And I give you some intuition why this is the framework introduced by Tony Kanal.",
                    "label": 0
                },
                {
                    "sent": "So what they do is they train some model or label data set.",
                    "label": 0
                },
                {
                    "sent": "Bing yeah.",
                    "label": 0
                },
                {
                    "sent": "Guys see this somehow reach observations should be added to the label data set to be able to be edited.",
                    "label": 0
                },
                {
                    "sent": "Data set.",
                    "label": 0
                },
                {
                    "sent": "They use some utility function to do it.",
                    "label": 0
                },
                {
                    "sent": "Then they get individually labeled data, ignoring data set and we restarted the system so.",
                    "label": 0
                },
                {
                    "sent": "You know experiments would be full utility function provided by Tony Kanal.",
                    "label": 0
                },
                {
                    "sent": "It's a butylated function of each chooses instances where the modern is the least confident.",
                    "label": 0
                },
                {
                    "sent": "It's a very good idea.",
                    "label": 0
                },
                {
                    "sent": "It actually functions.",
                    "label": 0
                },
                {
                    "sent": "But this problem to choose the utility function.",
                    "label": 0
                },
                {
                    "sent": "How we choose voting sensors is there is still an open problem.",
                    "label": 0
                },
                {
                    "sent": "And another problem in this framework is the stability since.",
                    "label": 0
                },
                {
                    "sent": "We have to train model M using some initial label data.",
                    "label": 1
                },
                {
                    "sent": "So if this model is quite bad from the beginning.",
                    "label": 0
                },
                {
                    "sent": "The launch is not stable at all.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and I share with you some conclusions.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a rather well known fact that if they're not real, hope celebration with small, then a state of the art methods most cost.",
                    "label": 0
                },
                {
                    "sent": "State of the art methods are not stable.",
                    "label": 1
                },
                {
                    "sent": "We have shown that Angel introduced what a mistyped, ignoring outperforms state of that methods.",
                    "label": 0
                },
                {
                    "sent": "It's good news.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Not so good news is the application of this initial progress criterion is problematic since the approximation of marginal probability of sequences is A is an open question.",
                    "label": 1
                },
                {
                    "sent": "So our perspectives as follows.",
                    "label": 0
                },
                {
                    "sent": "We are very interested in methods to approximate marginal probability of structured data.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily sequences, but data that can be represented as graphical models.",
                    "label": 0
                },
                {
                    "sent": "In general, it's an open problem.",
                    "label": 1
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "We continue to work on our method and we we are interested in the theoretical analysis of the full basic deep learning method we have introduced.",
                    "label": 0
                },
                {
                    "sent": "And the last but not least, the theoretical analysis of.",
                    "label": 1
                },
                {
                    "sent": "This German supervisory theorem.",
                    "label": 0
                },
                {
                    "sent": "Actually, this analysis has been already done for the asymptotic case, it's.",
                    "label": 0
                },
                {
                    "sent": "Optimal, but what what is especially interested is the theoretical analysis of the enormous important.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Very interesting talk, any questions?",
                    "label": 0
                },
                {
                    "sent": "For the approximation of these marginal probabilities in speech recognition, usually people use some smoothing approach, so if the.",
                    "label": 0
                },
                {
                    "sent": "Number of observed trigrams or bigrams gets low.",
                    "label": 0
                },
                {
                    "sent": "One gets back to these lower dimensional things and does some smoothing.",
                    "label": 0
                },
                {
                    "sent": "Please Andre and so did you use that also?",
                    "label": 0
                },
                {
                    "sent": "Actually, the problem is that.",
                    "label": 0
                },
                {
                    "sent": "You can, you can can easily get zero probability in the natural language.",
                    "label": 0
                },
                {
                    "sent": "Application is the problem that you are quite close to 0 only 10.",
                    "label": 0
                },
                {
                    "sent": "There is one sequence with a very high probability.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Solutions.",
                    "label": 0
                },
                {
                    "sent": "Comments.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}