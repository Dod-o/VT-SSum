{
    "id": "5hlbxhxxnb52otsep4xlwpfymsaghxxw",
    "title": "Active Learning",
    "info": {
        "author": [
            "Sanjoy Dasgupta, Department of Computer Science and Engineering, UC San Diego",
            "John Langford, Microsoft Research"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_dasgupta_langford_actl/",
    "segmentation": [
        [
            "It's an area of machine learning that is really a very crucial practical importance.",
            "And it's something that seems to require genuinely novel algorithms and novel styles of analysis.",
            "OK, so just to.",
            "Motivate it a little bit and just to start specifying the model."
        ],
        [
            "A lot of the a lot of the work, a lot of what is known about classification is in a setting where every data point has an associated label, OK?",
            "But in reality, it's frequently or very, very often the case that the role form of data is unlabeled.",
            "Anna Label is something that you actually have to buy, or you know actually obtain at some sort of expense.",
            "OK, so for instance, if you're building a document classification system.",
            "You can download millions of documents off the web while you're away for lunch.",
            "But labeling one of these documents takes a long time.",
            "OK, you have some human being has to sit and look at the document and decide whether it's about sports or business or health or whatever, and it's something that takes effort.",
            "If you're building a speech recognizer, you can just set a microphone down someplace and obtain hours and hours of speech.",
            "But if you want it labeled, then some human being has got to look at the speech waveform and decide when each phoneme begins and the next one ends, and so on, which is very painful.",
            "OK, so these are all situations where you can easily obtain masses of unlabeled data, but labels come at a cost.",
            "So it motivates a model in which some distinction is made between unlabeled data and labels that are subsequently."
        ],
        [
            "10th.",
            "So here's the general picture.",
            "Then you're in a setting where there's some vast number of unlabeled points."
        ],
        [
            "In regular supervised learning, what effectively happens is that.",
            "You get if you get labels for some of them and you ignore the rest.",
            "As a result, the size of the data set might not be all that large, but you ignore the rest of the points and now you try to separate these, and in fact in this case there many ways to do that."
        ],
        [
            "More recently, it's become.",
            "Sort of, you know there's a lot of people who started looking at semi supervised and active learning, where just as in the supervised case you have labels for some of the points.",
            "But now you don't ignore the unlabeled data.",
            "In a semi supervised model, what happens is that you do choose just some of the points at random to get labels full and you take account of the locations of the unlabeled points as well.",
            "So for instance, if you want to build a classifier just on the basis of these labeled points, there are many ways there many places where you could put the boundary.",
            "In this case, if you which is also take stock of where the unlabeled points are, it constrains the choices somewhat, because they do seem to kind of be 2 clusters, and you really want to put the boundary somewhere between those two clusters.",
            "So this is a sense in which in a semi supervised model you get this additional information about the underlying distribution from the unlabeled points.",
            "In active learning you go one step further.",
            "Once you have a few labels, what you do is you try and find you adaptively choose which point to query next.",
            "So you look at you look at it and you say, oh, you know, I think the boundary is going to be here.",
            "Maybe a very informative point to query would be this instead of say, this one, which we're pretty sure is going to be a red OK, and so the hope is that by choosing these these points very intelligently, you quickly converge on a good classifier.",
            "OK, so that's the active learning setup."
        ],
        [
            "So let me just give you a couple of examples of sort of recent case studies that have involved active learning.",
            "The first one is 1.",
            "Which was authored by the previous speaker, Manfred Barmat, along with Guna Raj and a bunch of chemists and the setting over.",
            "Here is some sort of computational drug design.",
            "OK, so the idea is that you want to find a compound that binds to a particular target.",
            "And you have a vast library of compounds from which you can choose.",
            "OK, so.",
            "In this case, the pharmaceutical company has a lot of compounds that can choose from.",
            "Some of these are virtual, you know there are things that have never before been synthesized, but can be synthesized on demand.",
            "OK, you have some examples of compounds that bind to the target and you want to find more.",
            "Now, one way in which this would be one way in which this would happen is that a bunch of scientists it done and they look at they look at which compounds seem to buy into the target and they come up with some hypothesis about you know what are the chemical properties of that compound that make it bind.",
            "And then OK.",
            "So based on the hypothesis, they say that, well, maybe this other compound will also bind.",
            "Let's try that and then they do a chemistry experiment and they revise their hypothesis and so on.",
            "But this also seems like something on which you could use active learning, and that's what that's what these people did.",
            "So in this setting, each unlabeled point corresponds to a potential compound, and it turns out that the chemist came up with a very high dimensional description space.",
            "For these compounds it was literally something like 140,000 dimensional, some incredibly high dimensional way that captures a lot of the chemically interesting features.",
            "Each point is got a label.",
            "Either it's active, it binds to the target or it's inactive.",
            "And now the querying process actually corresponds to a chemistry experiment, and so they were able to.",
            "They were able to obtain some sort of traction on this fairly difficult seeming problem using their active learning method.",
            "Here's another example, this was."
        ],
        [
            "From pedestrian detection experiment that was conducted on the streets of Paris I believe and this is by Yotam Abramson and your friend.",
            "And here the setting was that inside inside a car a camera was placed taking, you know, images of the world in front and the idea was to find the pedestrians.",
            "The hope was that you know this would eventually become part of some cars software and make.",
            "Cars, more safer pedestrians, that kind of thing.",
            "OK, so each data point in this case is not an entire image, but simply a rectangle within an image, and these rectangles could be of different sizes, and the label is is that a pedestrian or not?",
            "And using this using active learning, they were they were able to avoid having to label too much of the data so you can collect masses of data just by having the camera sitting there in your car.",
            "But but it's really laborious to have to label it, and So what they did is they developed a system in which some human being sat and looked at a whole bunch of these images and drew boxes around the human beings and then they came up with a classifier that basically you know was able to correctly classify those examples.",
            "Anything that the humans had encircled was basically a negative example, so they were able to come up with a classifier that dealt with those examples and then then they went through some sort of refinement process so you know the classifier was then applied to a whole bunch of other images, and if there was something that was really unsure about a human being was asked to label that.",
            "OK, so for instance, if there was something on the sidewalk, there was like a phone booth, maybe that's something the classifier would be unsure about, or a fire hydrant.",
            "Anyway, so these are just two examples of.",
            "Of active learning and it turns out that this."
        ],
        [
            "Actually, a large number of.",
            "Of active learning algorithms out there.",
            "Which we have already enjoyed a certain amount of success and in some sense you know.",
            "In many applications it's really needed.",
            "You know you have to use something and so so the whole sort of collection of active learning strategies has emerged.",
            "So I'll start by talking about a certain class of these strategies and.",
            "Then and then talk about what happens when we when we try and get some sort of basic statistical guarantees for these kinds of algorithms, what turns out to be the hurdles?",
            "OK, so there's a lot of strategies that look like this.",
            "You have a bunch of it's very natural sort of thing.",
            "You have a bunch of unlabeled points and you start by just choosing a bunch of them at random to get labels for just to sort of get the lay of the land.",
            "And once you have that then you do have some sort of candidate, or you have some idea.",
            "Of where the decision boundary is going to be, so you have some sort of candidate boundary, and once you look at that boundary you can then you know, decide well, you know the point I should query is the one I'm most uncertain about.",
            "There's no point in querying this one, since I know exactly what it's going to be, so why not choose something near the boundary OK, and so there are strategies that, for instance.",
            "Look for the point closest to the boundary, which in this case would be, say this one.",
            "Or there's a variant that might look for the most uncertain point where, for instance, the distance from the boundary might transfer transformed into some sort of posterior probability an.",
            "In that case this might also be the most uncertain point.",
            "Then there are some variations in which the consideration is that, well, you know, although this might be closest to the boundary, it's kind of far away from the other day to an, so maybe a better point to pick is one of these.",
            "Becausw if you have their label it would most influence the IT would lead to the largest update.",
            "It would most influence your your classifications of the other examples.",
            "OK, so rather than simply being the most uncertain point, it's the most likely to change to or to shrink your overall amount of uncertainty.",
            "And there are various ways to formalize this.",
            "OK, so there's a large class of heuristics of this form.",
            "And.",
            "And.",
            "And actually, it seems this seems like a very sort of sensible strategy, but there's one sort of basic difficulty that emerges, and so let me let me just outline what that is.",
            "Basically, in this sort of a setting we want to come up with a classifier that does well on the natural distribution out there.",
            "You know the underlying distribution from which we have a few samples.",
            "So in the first step of this process, we've chosen a few random points.",
            "We've got their labels and the labeled points we have becausw.",
            "They are chosen randomly, truly represent the underlying distribution.",
            "There a random sample from the underlying distribution.",
            "Once you start the active learning process, however, as time goes on, the training sample looks less and less like the underlying distribution.",
            "In this case you started off good, but it's time goes on.",
            "You might get examples that are essentially concentrated near the boundary.",
            "And so you know it.",
            "1000 labels later, what you're left with is a training set that looks very unlike the underlying distribution.",
            "And so now the question is, do we really want to optimize with respect to this training set?",
            "OK, it's very different from the distribution we actually care about.",
            "Who knows, maybe by being close, maybe the points right near the boundary are actually noisy points and we don't want to pay all that much attention to their labels, OK?",
            "So I mean, so there are many ways in which to sort of summarize what the issue is.",
            "But the simplest way to say it is that it's a problem of bias sampling.",
            "OK, that you.",
            "As time goes on, your training set looks very different from the distribution you actually care about, and so there is some danger in optimizing with respect to this training set.",
            "OK.",
            "So by the way, feel free to stop me at anytime, any questions?",
            "But I thought this is the nature of the active link that you want to purposely choose some examples rather than randomly sampling.",
            "Yeah, yeah, it's, but it seems like the one does want to choose informative points, and yet at the same time one wants the resulting classified to do well with respect to random looking points.",
            "So it looks like there's going to be some sort of trade off, or there is some sort of conflict over here, and so the question is, how does one resolve this conflict?",
            "And so that's the sort of thing that we'll be talking about."
        ],
        [
            "So.",
            "Certain in terms of ice of sampling, are you concerned about the bias on the distribution of the points themselves or device in the distribution of the conditional distribution of the labels given the points could be concerned about both right now, so they both be right, they're both the source of concern.",
            "Second, would be a source of concern, right?",
            "So there are two that there are two problems.",
            "The first is that the distribution of the X values is not nature's distribution of the X values, and the 2nd is the distribution of Y.",
            "Given X might be different.",
            "You know you might be by focusing on the boundary, you might essentially be in a regime where the distribution of Y given X is something like .5 four ways, and so those labels might be less reliable.",
            "So this this is sort of at a high level and little bit.",
            "You know, touchy feely.",
            "This whole issue of sampling bias.",
            "So just to make it more concrete, let's look at a specific example.",
            "OK, so here's the sort of generic strategy that I talked about and I said there are many variants of it, but we'll just look at it, a kind of a high level OK. And now suppose that you know just to just to keep things simple.",
            "Suppose we have a really easy hypothesis clause.",
            "OK, so the data all lie on the line, so it's a 1 dimensional data set and each hypothesis is just a boundary on the line and to one side it's green and to the other side it's red.",
            "OK, so a really simple sort of a hypothesis class.",
            "Now here's an example of a data distribution.",
            "45% of the points lie here and there greens.",
            "45% lie here and these are Reds.",
            "And so on.",
            "In the middle you have two and a half percent Reds and 2 1/2% greens.",
            "So let's see what would happen in one of these.",
            "In one of these active learning heuristics.",
            "So you start by sampling some random points, but since the data predominantly lies in these two clusters at the far ends, let's just say the initial batch of points pretty much like here and here.",
            "OK, so you look at those two points.",
            "Maybe you have four points here and four points in the other one and you do something like you know a maximum margin classifier or something.",
            "And based on that you put the boundary somewhere in here.",
            "OK, now you're going to query points close to the boundaries.",
            "You'll end up querying points in here as in time, and as time goes on you will query points closer and closer to this middle point and you'll basically end up converging to this hypothesis.",
            "Now this hypothesis is got 5% error because it gets these wrong and the optimal hypothesis is this one because it has only 2 1/2% error.",
            "OK, so this is an example of the problem of sampling bias.",
            "Basically what happens is that.",
            "You know you start with this hypothesis.",
            "That sort of in the middle of all of your greens and Reds.",
            "It looks like a good hypothesis, and then you become really confident that all of these have to be Reds.",
            "But you really have no justification for being so confident.",
            "It's misplaced confidence, and so you end up with.",
            "You end up with an algorithm that's."
        ],
        [
            "Not even consistent.",
            "You know which is, which is a fairly basic statistical guarantee.",
            "You know, forget about optimizing the number of labels.",
            "One would hope that as the number of labels goes to Infinity, you at least converge to the right thing.",
            "And in this case that wouldn't happen.",
            "OK, it's something that's not consistent.",
            "So so an example of.",
            "An example of how this might play out in practice if you, if you think back to that pedestrian identification pedestrian detection thing, you know it might be that after seeing the first few labeled images, the learning system density is an image of a fire hydrant and it's completely certain it's a human being.",
            "You know a child or something like that, and so it will never ask for its label.",
            "There's no, there's no need to, and you know.",
            "The cluster of fire hydrants in the space of data.",
            "There's a cluster of fly hydrants in a cluster of phone boots that look like this.",
            "The cluster of fire hydrants."
        ],
        [
            "So everything is.",
            "There's been a bunch of experimental work here.",
            "This is an example of this is an example of.",
            "Sort of a study and.",
            "So this is a text classification setting in which this sort of hidden cluster effect emerged.",
            "Here you have one of these active learning heuristics where the initial sampling process misses some cluster and as a result the learner is completely sure about the label of the cluster and never bothers to ask, but later on it turns out to be wrong.",
            "OK, so this is an example of the sampling bias problem OK, and in my view, and I think also in John's view, this really is the central.",
            "Difficulty or sort of the central hurt, or in active learning.",
            "OK."
        ],
        [
            "Yeah.",
            "This is similar to just confusing a local minimum with a global minimum.",
            "Um?",
            "So this is, you know, this is, I think this is actually.",
            "A more severe problem, you know it's.",
            "'cause because that's a computational problem.",
            "And this is this is a basic statistical issue.",
            "You know that's the local versus global optimum.",
            "That's something you know.",
            "You invest a little bit more time.",
            "You get into solution here.",
            "You end up with the data set that's not representative and you know you can get the best solution with respect to that training set.",
            "But it might be very bad on the underlying distribution.",
            "Yeah.",
            "Problem with both here you're not really estimating probabilities here mean distance from the margins.",
            "Yeah, so I mean.",
            "Because they are, there are resources about experimental design that they convert against science.",
            "Yeah, so over here.",
            "What I meant by consistency is purely in a classification setting in the sense that you want.",
            "Making the material probabilities of the class instead of putting some.",
            "Decision boundary and finding some distance from the margin I mean that can do this differently.",
            "Well, yeah, so it's it's possible that you know.",
            "Probabilistic framework one changes things somewhat, but but the bottom line."
        ],
        [
            "And is that you know these are examples.",
            "This is an example in which this classifier OK, the one at this boundary, has got the smallest error rate.",
            "It has an error rate of 2.5%.",
            "And in this setting you converge to something whose error rate is 5%.",
            "You know so.",
            "So I'm talking bout 01 loss over here.",
            "And.",
            "And so there's a clear problem of not converging to the best possible hypothesis, even as the amount of labels you have becomes arbitrarily large.",
            "No.",
            "Local minima if you think about minimizing generalization.",
            "This whole thing is an optimization strategy.",
            "To minimize that there is.",
            "Really most importantly I see right?",
            "Right, so sort of meta meta review, right?",
            "Absolutely.",
            "The reason why it doesn't work well in this example is cause it's not consistent.",
            "Consistently scenario sample bias is still a problem.",
            "Can we say there or?",
            "So actually the I didn't mean to suggest that active learning doesn't work well here.",
            "I'm just saying that a specific class of heuristics would fail on this BIH cause although they are intuitive, they fail to take into account they failed to explicitly think about sampling bias.",
            "Specifically offer, is it still have some problems?",
            "Like consistency.",
            "Concentra bias is always there when this condition or not.",
            "OK, right?",
            "So the sampling bias is always an issue, but this is the case in which.",
            "In which a bias is being created becausw of because of the querying strategy.",
            "It's true that you know what you care about.",
            "Is an infinite distribution and you're getting just a finite number of samples and those finite samples are not going to be a perfect replica of the underlying distribution, but it's very well understood how to quantify the difference between those two things in a classification setting.",
            "This is something a little bit more sinister.",
            "OK, so it seems like one of the problems is that your algorithm is kind of fixating onto one hypothesis test.",
            "I'm not even thinking about the other possible things that could have been consistent in the data and was wondering if one could make like a general statement that any algorithm which would have this property can be broken in some in some sense that would be very interesting.",
            "I mean, it's plausable and.",
            "It would be interesting to do that and the fix you suggested, which is that one should take into account not just one candidate hypothesis, one candidate hypothesis, but you know all possible candidate hypothesis that indeed is one way to fix the problem, and it's something that we'll be looking at.",
            "The label is missing completely at random or having any kind of bad assumption.",
            "That is violated the first time we have a query, but are you making any assumptions, and if not, can we fix some of these things by making that assumption so we will end up fixing this problem in setting without assumptions?",
            "But as an intermediate setting, we will have some assumptions just to make things easier to explain, but we'll eventually get to.",
            "OK, so.",
            "It turns out that.",
            "OK, So what are we doing here?"
        ],
        [
            "So.",
            "So basically the goal in this tutorial is to talk about some schemes for active learning.",
            "OK now if we were to go through everything that's known, it would really take a very long time.",
            "And So what we're going to focus on is actually actually schemes that have some sort of consistency.",
            "Some of this.",
            "Some of this had this property, at least to some extent.",
            "OK, and a lot of this would involve just just looking at some of the heuristics out there.",
            "And seeing what can be done with them to make them consistent.",
            "For example, OK now.",
            "So the first question is this active learning actually help at all?",
            "And in answering this there are at least two possible kind of toy scenarios that people have in mind."
        ],
        [
            "One of them is exploiting cluster structuring data, and the 2nd is efficient search through hypothesis space and so let me just kind of briefly explain what these caricatures are.",
            "These toy scenarios, the cluster structure thing, the cluster structure story goes like this.",
            "It says you look at your unlabeled data.",
            "And you notice that there are five clusters, and so you say, hey, I just need five labels and I'm done.",
            "OK, give me one label in each cluster.",
            "That's the label of the cluster.",
            "End of story.",
            "OK, so that's that's one sort of caricature model of active learning, OK?",
            "Of why active learning is useful."
        ],
        [
            "Of course, there are some challenges.",
            "OK, so first of all.",
            "The cluster structure might not be so clearly defined.",
            "It might not be that you look at the data and you know there are five clusters far away from each other.",
            "Second, even if there is cluster structure, it could be at many levels of granularity.",
            "You could look at it and say you know what.",
            "There are two clusters far away from each other, but you know there's also a nice way to do 3 clusters, and a nice way to do 5 clusters in a nice way to do 10 clusters.",
            "And finally, there might be clusters, but then the clusters might not be pure OK, so.",
            "So what can be done?",
            "Is there some sort of nice robust scheme that is able to exploit whatever cluster happens to exist?",
            "Whatever cluster structure happens to exist, and you know otherwise, you know behaves like supervised learning, so this is 1 sort of toy scenario that people use to.",
            "To suggest that active learn."
        ],
        [
            "Might be useful.",
            "The other scenario.",
            "Is efficient search through hypothesis space OK?",
            "So let me explain this, because this is actually what we're going to be focusing on in this tutorial.",
            "Home.",
            "So here's the here's the thing over here.",
            "This box is supposed to represent the space of candidate hypothesis.",
            "So for instance, the space of all linear separators, if that's what you're interested in, any data point can be thought of as a cut in this space.",
            "OK, so a data point divides the space into two groups.",
            "The hypothesis that label that point as a plus, and the hypothesis that label that point is a minus.",
            "OK, so suppose you take one such data point.",
            "And let's say this is the cut.",
            "That it corresponds to and now let's say you ask for its label and the label turns out to be plus.",
            "Then you can throw away all of these hypothesis over here if the problem is separable.",
            "OK, in other words, if you feel that for some reason there is some perfect hypothesis out there, some hypothesis that will perfectly classify all points.",
            "This is not a realistic assumption.",
            "But it's an intermediate thing that we'll use just to explain some of the concepts before we get to the realistic case, which is the non separable case, OK?",
            "Now, if you can do this.",
            "This is a very nice situation because here you get one label and you at least in this picture you seem to eliminate half of the hypothesis OK, and so you know the way this sort of story continues is that this happens with every single label you ask for an.",
            "So by the time you've asked for log H labels, you're down to one hypothesis and you're done.",
            "OK, so that's that's the search through hypothesis space story."
        ],
        [
            "Needless to say, there are lots of problems with this one as well, OK?",
            "First of all.",
            "This is a happy situation in which the data cut seems to divide the hypothesis space roughly in half.",
            "But what if the cut just looks like this?",
            "It cuts off a little bit.",
            "The minus is here in the pluses there and the label you get is a plus, so all you can cut off is this tiny little corner, OK?",
            "In that case, getting the label doesn't tell you very much.",
            "OK, so do they always exist queries that are going to cut off a lot of the space or not?",
            "That's problem number.",
            "One second, even if there do exist such queries, even if there is some magical point that's going to cut off half the space, how do you find what that point is?",
            "Is there some way to do that?",
            "And finally, this all seems to fall apart in a big way when the data is non separable.",
            "Because then you get a label of plus, but the best hypothesis might be over here and you can't eliminate this off.",
            "So what can you do in that setting?",
            "OK, so these are the hurdles that have to be overcome in.",
            "In this sort of toy scenario.",
            "An so."
        ],
        [
            "This is going to be the outline of our tutorial we're going to.",
            "Basically going to look at algorithms designed for each of these two scenarios, and as I said for us, we're really going to want algorithms that have statistical consistency OK, and the focus is very much going to be on the 2nd on the second of these categories, because that's where a lot of work has been done.",
            "Very little has been done on the 1st, and so I'll just quickly say a few things about it.",
            "And then move on to the second one, OK?"
        ],
        [
            "So let me give you an example of a cluster based active learning scheme.",
            "All of this scheme might not originally have been presented as such, so this is a very nice algorithm.",
            "Do too.",
            "Shall Jinju Zubin Caramani an John Lafferty?",
            "And and here's, here's how the scheme works.",
            "Very roughly, I'm going to just look at it in a very high level, so you start with a bunch of data points, and then you build a neighborhood graph on them so you have a node for each data point.",
            "And then you put an edge between points that are close together.",
            "OK, so for instance, within some radius or points that are K nearest neighbors of each other.",
            "Or sometimes you just put an edge between every two points and you wait the edges according to distance there are.",
            "There are several ways to do this, but you do this.",
            "You build a neighborhood edge.",
            "You build a neighborhood graph and then you query some points.",
            "So let's in this case, let's just say we have."
        ],
        [
            "Binary problem, query some points.",
            "Let's say we query these two points that turns out to be a zero.",
            "This one over here turns out to be a one.",
            "Now you propagate the labels."
        ],
        [
            "OK. You look at the data you have so far and you propagate it to the rest of the graph, and it turns out that there are some really nice ways to do this.",
            "Basically by solving an eigenvalue problem and one way to think about it is to think about the semantics of this number is suppose you started at this node and you did a random walk.",
            "What fraction of time would you first hit a one?",
            "What fraction of time would you first hit a zero?",
            "OK, so you can think of the semantics of these numbers in that form roughly OK. Now, once you have these."
        ],
        [
            "You can use this as a basis for deciding which point to query and so, for instance, one thing you might want to do is to query the most uncertain point which.",
            "Which would be this a .5?",
            "But then this seems like a point that's a little bit of an outlier.",
            "It's kind of sticking out from the graph, so it might be better to.",
            "To query this because it's likely to lead to the biggest reduction in overall uncertainty and one can put a nice probabilistic semantics on top of these kinds of graphs in which all these all these analysis are very meaningful and you can actually quantify the amount of residual uncertainty after finding a label.",
            "So you query a point and then you go back.",
            "So why is this?"
        ],
        [
            "In a sense, cluster based active learning.",
            "Because.",
            "If you have clearly defined clusters in the data.",
            "You might have the clusters might be something like these three and then the remaining points.",
            "Basically, when they're clearly defined clusters, the corresponding nodes will have a cut between them through which which don't have too many edges and through which influence can propagate only weakly.",
            "OK, so this is an example of.",
            "Beautiful scheme for doing active learning that that seems to be motivated in part by a cluster learning by sort of a cluster model.",
            "And so.",
            "I'm not, I'm not clear on what the what the consistency status of this is.",
            "You know what one can say about statistical consistency, but I can tell you that, but I'll now tell you briefly about some work that basically uses this as motivation to come up with a different scheme in which you can actually prove things about consistency, and that will give some idea of the kind of analysis that needs to be done.",
            "OK, so here's.",
            "He"
        ],
        [
            "Is another cluster based scheme.",
            "Which attempts to use some of the same intuitions from the previous one, while while achieving consistency and so the idea is that you look at the data, you find a bunch of clusters.",
            "You sample a few points in each cluster and then you assign each cluster its majority label."
        ],
        [
            "And you use that label set to build a classifier.",
            "OK, so that's kind of a high level of what's going to be done, but not."
        ],
        [
            "Look at the reality.",
            "OK, so you start with some unlabeled data."
        ],
        [
            "You find clusters."
        ],
        [
            "You also some labels.",
            "Point this cluster is looking pretty good because it's pure.",
            "It's relatively pure.",
            "You have six points.",
            "You've chosen 6 random points in that in this cluster.",
            "That gives you a certain confidence interval on how pure this cluster is, you know.",
            "And because it's a random sample you can use, you can come up with these sort of confidence intervals, so you're feeling good about this one.",
            "This one is looking like trouble OK?",
            "And So what you do then?"
        ],
        [
            "Is that you?"
        ],
        [
            "Then refine the clustering.",
            "OK.",
            "So.",
            "So the overall overall a nice way to do."
        ],
        [
            "This is to just use a hierarchical clustering.",
            "OK, so.",
            "And So what the figure on the right denotes is at the top level that node corresponds to a single cluster that contains all the data points.",
            "Then it gets split into two.",
            "It gets split into the one on the left and the one on the right, and then the one on the right is split into two, the one on top and the one below.",
            "And of course the tree keeps going down OK to however many levels you like.",
            "So how does this this sampling scheme work?",
            "Well, you start by being really optimistic.",
            "You say, well, let's just assume there's this one cluster all the way on top, and all the labels are the same.",
            "That's the best we can hope for, but you take just a few samples and you very quickly realize that that is wrong, OK?",
            "So then you move down to the next level.",
            "OK, and now.",
            "Now the samples tell you that on the left you're doing well and you need to recurse further on the right and so on.",
            "The key thing in this scenario is that at any given time you maintain a certain pruning of the tree, a certain clustering that you're currently working with, and as time goes on, you might have to move further down the tree and refine that clustering somewhat.",
            "However, when you're working with a certain clustering, the primitive is that you choose a cluster.",
            "And you pick it random within it.",
            "And it's very important to do that.",
            "OK, so for instance, when you're in the two cluster setting, you pick one of the clusters, but once you've picked which cluster you're going to query, you have to query a random point within that cluster, and I won't really."
        ],
        [
            "Go into the details over here because I want to focus on the main topic of the tutorial, which is really the second the second form of active learning, which is efficient search through hypothesis space.",
            "And as I said, we'll start by talking about the separable case, which you should think of as being a very unrealistic case, and then we'll show that actually you can do everything for the non separable case.",
            "For the general case as well, OK?",
            "OK, so efficient.",
            "Search through hypothesis space.",
            "Here is the Canonical example."
        ],
        [
            "The people use to motivate this type of active learning OK.",
            "Suppose.",
            "That once again, a hypothesis class.",
            "Is this really simple class where all data lies on the line and each hypothesis is just a boundary on the line and everything to the right is a plus everything to the left is a minus.",
            "OK, so the hypothesis class can be written in that way.",
            "Each hypothesis is just indexed by a single real number, which is the location of that boundary.",
            "This is the class of VC dimension one.",
            "It's a very primitive class and it's a close.",
            "This very well understood in terms of its in terms of supervised learning.",
            "What's known is that.",
            "Suppose.",
            "You're going to do supervised learning, and you want to end up with.",
            "Hypothesis, whose error is less than epsilon will then the number of labeled points you need is about one over epsilon OK, and that's basically because its VC dimension is 1.",
            "It has one parameter, so this is something that's very well understood in a supervised setting.",
            "We know exactly what we need over here.",
            "So let's see what happens in."
        ],
        [
            "Active learning.",
            "In active learning, what happens is that you start with unlabeled points.",
            "So let's start with one over epsilon unlabeled points.",
            "OK, so we have no idea what their labels are, and now we start querying.",
            "Here's how you can query.",
            "You can start by asking for the label of the middle point, the point at the 50 percentile point you ask for the one in the middle.",
            "Suppose the label turns out to be a plus.",
            "We are still in the separable setting, supposed to label turns out to be a plus.",
            "Well then you know that all of these points are pluses and you never need to query them.",
            "Suppose the label turns out to be a minus.",
            "They know that all of these points are minus is and you don't need to query them.",
            "OK, so let's say there is a plus.",
            "We know all of these are pluses and then we can move to the 25% point and in this way you can do a binary search.",
            "So what ends up happening is that you ask for just log one over epsilon labels instead of one over EPS."
        ],
        [
            "Which is a huge improvement.",
            "OK, so you do this binary search.",
            "And it's literally log one over epsilon.",
            "So for instance, if you want your error to be .1%, then in the supervised setting you need about 1000 points, whereas over here you would need just log of 1000 labels, which is something like 10 labels.",
            "OK so.",
            "So it's a setting in which active learning really does give a big improvement.",
            "OK, any questions about this particular example?",
            "OK."
        ],
        [
            "So here the challenge is the first is this again seems to assume separable data, you know, because if you think this is a plus, then you're going to assume everything here is the plus.",
            "Obviously that doesn't happen in, you know in a non separable data set, but it turns out that there are many sort of noisy binary search procedures out there which you can use for for for this problem you know procedures where.",
            "The answers are assumed to be given by a malicious adversary who makes a certain number of mistakes and so on.",
            "These things have been analyzed thoroughly, and so there are ways to their ways to adapt.",
            "This specific example for the non separable case that's not a problem with the same sort of exponential improvement.",
            "The problem really is how to use this for other hypothesis classes.",
            "This is sort of a toy hypothesis class.",
            "How do you do this more generally, and that's something on which there's been a lot of work and so let me just summarize some of what's known and.",
            "I'll use a particular sort of format where I'll divide active learning work into four categories, OK."
        ],
        [
            "The first 2 columns are separable versus non separable and a lot of the earlier work was very much in the separable case because it's much easier to deal with that case OK.",
            "The rows are two sort of different types of active learning strategies, and one of which I'm calling aggressive and the other is mellow.",
            "So what this means is so an aggressive active learner is one that is looking for the most informative point and actually the heuristics we've seen so far are pretty much aggressive heuristics.",
            "You know, find the point closest to the boundary or find the point that will most reduce uncertainty that's aggressive.",
            "There's another kind of active learning that you can call a mellow active learner where it's not trying to find the best or most informative point is just trying to find something that's not too redundant.",
            "You know it's not, it's it's a little bit more.",
            "It's slightly lower standards for what for what kind of point is going to pick OK, and it turns out that in the separable in the separable case both both have been analyzed, aggressive and mellow, and in the non separable case there's been a lot of work on this sort of mellow active learning and.",
            "And actually would surprisingly good bounds considering how mellow these algorithm."
        ],
        [
            "Seem to be OK, so that's something that I'll talk about in a second, but sort of the underlying issues over here will turn out to be computational tractability.",
            "Finding the best point, this sort of thing turns out a lot of the early schemes turned out to be computationally intractable.",
            "And a big problem is actually quantifying label complexity.",
            "OK, so you have some scheme how many labels does it actually use?",
            "Can that be analyzed?",
            "And how and what is the best possible number of labels that can be used?",
            "Can that be analyzed?",
            "Can you give Upper and lower bounds?",
            "There's actually been a lot of progress on this front, which we'll talk about.",
            "OK, so you've seen what an aggressive learner is.",
            "Let me give you an example."
        ],
        [
            "Absolutely wonderful, mellow active learner and this is work due to Cone app Listen Ladner from 91.",
            "It's a very nice algorithm and we're just going to be calling it Cal.",
            "So this is in the separable setting, although we'll see that you can do this sort of thing in non separable as well and the data is streaming in OK, so the data points come one at a time.",
            "You get one unlabeled point, then another one, then another one, and each time you get an unlabeled point you have to make a spot decision about whether or not to ask for its label.",
            "So if you ask for its label, you get it.",
            "If you don't, you don't get it either way, the point goes away after that.",
            "OK, and you get the next one and these points are generated IID from some underlying distribution.",
            "OK, so here's how Cal works.",
            "You always maintain the set of candidate hypothesis, which is the hypothesis that are consistent with everything so far.",
            "Initially, it's the whole hypothesis class, because you haven't seen any data yet.",
            "Now the learning process begins, so at each time step you receive an unlabeled point and now you get this unlabeled point and you look at all of your candidate hypothesis and you say, well, you know what they all think.",
            "This point is a one.",
            "So let me not ask for its label.",
            "There's no point.",
            "I know it's label is a one or they all think it's a zero.",
            "I know it's a 0, but if there's any disagreement.",
            "If a million minus one of them think it's a one and the last one thinks it's a zero, you ask for its label.",
            "OK, so that's it's a mellow learner.",
            "OK, it's it's just trying to avoid redundancy.",
            "That's all it's doing.",
            "OK, so any questions about this this algorithm?",
            "So let's just introduce some concepts based on the algorithm.",
            "So here's the setting.",
            "You've already seen six points or 7 points, and you ask for the labels of all of them, and now the eight points shows up.",
            "OK, so the eight point in the stream arise arrives.",
            "And you have to figure out whether or not to ask for its label.",
            "And let's say your hypothesis class is linear separators.",
            "In this case, there's no need to ask for its label, because there's no way the label could be a green.",
            "There's no linear separated that would make that agree, and so you don't ask for its label.",
            "Now here are."
        ],
        [
            "Some concepts.",
            "Age 50 is the current set of candidate hypothesis.",
            "These are simply the hypothesis that are consistent with the data so far, and there's a lot of them.",
            "There's kind of a, you know, there's a continuum of them somewhere in the middle over here."
        ],
        [
            "The other concept is the region of uncertainty.",
            "These are the points.",
            "These are the data points on which there is any uncertainty at all, and so these are the points.",
            "These are exactly the points that would be queried.",
            "You're not going to query anything outside this region.",
            "Now the hope is that this region of uncertainty will shrink quickly and so just by being mellow and avoiding redundant queries, you actually save a lot.",
            "OK, so that's the that's the hope with this scheme.",
            "OK."
        ],
        [
            "So.",
            "And just in terms of scaling this up, there are two obvious sort of issues with it.",
            "The first is is that it seems to make you keep a list of all the current hypothesis.",
            "It seems like you need to keep.",
            "What are the hypothesis that are consistent with the data so far and each time you get a new point you have to work your way down that list.",
            "That seems intractable, and Secondly it's not at all obvious what to do for non separable data, but both of them are can be tackled."
        ],
        [
            "And the first is really easy, so let's see how to do that.",
            "It turns out that you can instead of actually keeping the list of all of all hypothesis that are consistent so far, you can do this implicitly.",
            "So here's the explicit version that we saw in the previous slide where you have this explicit HD that you keep you keep track of, and each time you get a new point to go through that list, the implicit version works like this.",
            "It works by reduction to supervised learning.",
            "You assume you have a black box learn.",
            "It's just a supervised learner.",
            "You give it a label set an it gives something that's consistent with all the points.",
            "If something like that exists.",
            "Otherwise it says nothing available.",
            "OK, so you just keep track of the points you've seen each time you get a new point, you feed the supervised learner.",
            "The entire label set so far, and you give the new point with a one label and you see what it does.",
            "If it can output a hypothesis, it means that there is something in age subte that thinks he is a one.",
            "And now you do the same thing with the zero.",
            "You feed it the same data set with this new point, you give it a zero label and you see the supervised learning can do anything with that if it can will, then there's something in H subte that makes exceptia zero, and so if both of these return an answer then you have to query the label.",
            "Otherwise you know what the label is and you just stick to that.",
            "OK, so that problem of maintaining HFT just goes away by this sort of read."
        ],
        [
            "OK.",
            "The other thing that's easy to see is that this certainly not worse than supervised learning because it's doing exactly the same thing.",
            "You're just ignoring the points whose labels you're completely sure of.",
            "But at the end of the day, you end up with the same set.",
            "The real challenge is to bound the number of labels required and it turns out that this."
        ],
        [
            "Has been done and this is a beautiful result due to Steve Hanneke.",
            "He's been doing just a lot of fantastic work in active learning.",
            "OK, So what he did?",
            "Is to analyze the labeled complexity of this mellow, separable active learner, and it turns out that you need a new concept in order to do that.",
            "So for supervised learning, you can give label complexity bounds or sample complexity bounds in terms of the VC dimension.",
            "That's enough to characterize how many samples are needed, the VC dimension.",
            "And then there are also various alternatives to it.",
            "You know, various, there's other ways to do it, but.",
            "But but but it turns out that in the active setting, the VC dimension alone does not characterize the label complexity at all.",
            "And what Steve did was to develop.",
            "His main insight was to find this parameter that he calls the disagreement coefficient, and this parameter captures the label complexity of mellow active learning.",
            "OK, which is the type of active learning that's really been studied."
        ],
        [
            "A lot.",
            "So so so in order to tell you the result, let me first start by reminding you of what this regular supervised learning setting is OK.",
            "So suppose you're in a supervised setting.",
            "You have data ID from some underlying distribution, and you want a hypothesis whose misclassification rate whose error rate on the underlying distribution on natures this distribution is at most epsilon.",
            "Well, the number of examples you then need is D over epsilon up to some constant.",
            "OK, so that's.",
            "Regular supervised learning in the separable case you need D over epsilon examples.",
            "What?"
        ],
        [
            "This guy Hanneke managed to show is that in the mellow active learning setting, the number of examples you need.",
            "Is data that Mistry parameter the disagreement coefficient times D times log one over epsilon and this is the same sort of exponential improvement that we saw earlier, OK?",
            "And which is?",
            "Which is really quite remarkable.",
            "You know, there's a leading constant, but the convergence rate is exponential.",
            "It says log one over epsilon for each additional thing you have, the half the error.",
            "OK, for each additional step.",
            "So this is quite a quite a remarkable.",
            "Sort of.",
            "You know active learning analysis.",
            "OK, so any questions about this?"
        ],
        [
            "And this is all for the separable case, but it will later emerge that you can actually come up with a version of this algorithm for the non separable case as well.",
            "And even in that case this disagreement coefficient or some generalization of it still works and get you also get a bound that better than the supervised learning bound.",
            "Big Ben class of functions only distribution are both.",
            "Theater depends on the class of functions on the distribution.",
            "An even on the target hypothesis.",
            "But we'll see some examples of it in a second.",
            "Yeah."
        ],
        [
            "Data.",
            "Order more examples.",
            "Yeah.",
            "Yeah, so right.",
            "So the order does matter.",
            "Yeah, sometimes you can get really lucky you know and you get things you get, you know you start with super informative points upfront.",
            "You know the support vectors are the first 10 points and then and then instead of having."
        ],
        [
            "Instead of this bound had been log one over Theta, it's literally there's no dependence on epsilon, 'cause you have the perfect thing right up front, but this is a bound that holds regardless.",
            "You know it's it's bound that holds with high probability over a random sample from the from the underlying distribution, but it's yeah, it's very true that the order does matter for this scheme.",
            "So learning here is done.",
            "Stop setting by using that you have a sample kerbox internal sample, no.",
            "So if a cow it's not a transductive setting in the sense that you don't have access to the unlabeled points before hand, you just get one unlabeled point.",
            "Other results, it seems like they are right, so there's a lot of results that are in a transductive setting, right?",
            "So it's so it's unclear.",
            "Pop that transfer you know what that buys you.",
            "Suppose that you know.",
            "Suppose that you wanted to run something like Al, but in advance you had enormous unlabeled set that give you a really good idea of the underlying distribution.",
            "Would you change your strategy somewhat?",
            "One would imagine you would, but it's not known.",
            "You know they're not really very many results for that kind of thing.",
            "Problem where collecting unlabeled data cost something for to be very small.",
            "Having labor customer or what's the tradeoff?",
            "Yeah, that's that's a really interesting question, and it's something that you know that.",
            "But something good, you know.",
            "You know that the state of the art will hopefully one day get to the point of being able to address.",
            "OK. OK.",
            "So let me OK.",
            "So what does this disagree?"
        ],
        [
            "Demon coefficient ping an.",
            "In order to do this, we'll have to get into a little bit of math over here.",
            "So OK.",
            "So we have some input SpaceX OK and we have some underlying probability distribution on it.",
            "Nature is distribution, the distribution from which points are sampled.",
            "This distribution lets us create a distance measure between hypothesis.",
            "OK.",
            "So suppose, for instance, that we are dealing with linear classifiers and this is the optimal classifier H star.",
            "And let's say this is some other classifier.",
            "The distance between those two one very good distance measure is simply the probability mass of their disagreement set.",
            "The probability mass of the points on which they disagree.",
            "So this is a very nice distance measure on the space of functions, and it's something that plays a big role in active learning and also in other types of learning.",
            "OK, so it's.",
            "Going to use the letter D for that.",
            "Now, once you've defined this distance measure, you can talk about hypothesis that are close to the target.",
            "OK, so let's say that this is our target.",
            "This red hypothesis over here.",
            "This red linear separator.",
            "We now have a precise distance measure and so examples of things that are within distance are of the optimal.",
            "Say these, these might be what they look like.",
            "OK, so we can talk about things that are within distance R of the optimal.",
            "In other ones, things that disagree with the optimal on hypothesis on at most R fraction of the distribution under most are fraction of the data.",
            "Now, once you when you're doing active learning, hopefully you're eventually at the point where your set of candidate hypothesis is fairly close to the optimal one.",
            "OK, so it looks something like this.",
            "This set of candidate hypothesis includes the optimal.",
            "It's also and stuff that's close to it.",
            "In that case, the disagreement region would look something like this.",
            "These are the points on which there is any disagreement at all within your within this candidate set.",
            "OK, this is the disagreement region.",
            "For this point, there's no disagreement.",
            "Everything thinks that it's things that it's on this side.",
            "OK, so to be precise.",
            "The R radius ball around the optimal hypothesis is simply all hypothesis that disagree with it on at most in our fraction of the data.",
            "Things that are close to it.",
            "And this disagreement set is simply the induced data point.",
            "The data points on which there is going to be some disagreement in this set.",
            "In other words, these are the points that Carol is going to query.",
            "OK, and it's not going to query these other ones.",
            "The disagreement coefficient measures how the probability of this region scales with R. OK, and this disagreement coefficient let's let's yeah.",
            "There is no relation with the measurements but of condition.",
            "There is.",
            "There is some sort of.",
            "There is a.",
            "There is actually very very very.",
            "This is dimension without you Miss offers no run for this dimension is authority independent variable and this depends on so.",
            "Cookies or something more codependent like computer?",
            "Yeah there is.",
            "There is definitely some very intriguing connection over here with these, so just to just to just to summarize, it's well known that in supervised learning, when you see N data points.",
            "Your error rate scales with one over square root N. There's that one over square root and effect.",
            "Recently there's been some work that goes under names like sebok of condition, and there's also been work on it by Kolchinsky which identify properties of the hypothesis class that can give you a better rate than one over square root N. And there's some intriguing connection between that particular property and the disagreement coefficient, and this is very much something that needs to be worked out.",
            "Steve"
        ],
        [
            "OK, so let's disagreement coefficient is very general thing.",
            "It becomes a little easier if we look at the separable case.",
            "OK, so as we as we go through the active learning process candidates, it eventually gets very close to the target hypothesis.",
            "Let's say that it's error is at most epsilon.",
            "OK, so we get to a point where we are within the set of hypothesis whose error is less than epsilon.",
            "We can now look at the disagreement region of those hypothesis.",
            "In other words, the set of data points on which those hypothesis disagree at all, and the disagreement coefficient is simply the.",
            "Probability mass of that disagree set.",
            "It's simply proportional to that.",
            "OK, so the disagree set.",
            "Are the data points that we would actually query at this point.",
            "And so we're interested in in how the size of that data set.",
            "We hope that it's small.",
            "We hope that once we get to a point where where.",
            "You know, once we've sampled once, we've seen enough labels.",
            "We hope that we are close enough to the target that that all our candidate hypothesis pretty much agree on most of the data.",
            "The disagreement set is very small.",
            "And this is what disagree."
        ],
        [
            "And coefficient captures, so let's look at an example.",
            "OK, just to be concrete, let's say once again that the hypothesis class consists of thresholds on the line.",
            "OK, so the data space is just the real line, and each hypothesis looks like looks like a threshold on the line.",
            "Now let's say this is the target hypothesis over here.",
            "OK. What is the disagreement coefficient of this?",
            "Well, the way you figure that out is, you say, let's look at pick any epsilon.",
            "What are the hypothesis with error less than epsilon?",
            "Well, this is the target.",
            "This has error zero.",
            "This is error something this is error something the further you go away from this, the more the error increases.",
            "If we go exactly epsilon probability mass this way.",
            "This is these all have error less than or equal to epsilon and if we go exactly probability mass epsilon this way these all have error less than or equal to epsilon and so the disagree set consists of data points that lie in this zone and the total mass of this set is epsilon plus epsilon, it's two epsilon.",
            "So the constant is to the ratio of the probability of this set.",
            "Two epsilon is 2 and that's why the disagreement coefficient here is 2, yeah.",
            "So far.",
            "Considering this super all epsilon correct so.",
            "I assume that's because you want to do a worst case, but you want to get worse this browser.",
            "As you're going through the process.",
            "I mean the set up.",
            "And so it seems like it might be possible to get it in terms of a different definition of these agreements proficient, but that goes with.",
            "Yeah, so that it turns out that.",
            "Right, it turns out that.",
            "So this is, you know, this is a result from maybe four years ago or three years ago.",
            "It turns out that you know.",
            "You know that there are now more refined results that actually look at the limsup as epsilon approaches 0.",
            "Yeah, it looks very similar to like posts on arrival rate when you have interval and you probabilities proportionally interval, so yeah.",
            "Yeah, this is, but this is something that now is supposed to hold for any hypothesis class.",
            "And in more general classes, these disagreement regions would look kind of strange, you know, because for instance, in high dimensional spaces these are.",
            "These are strange looking polytopes."
        ],
        [
            "OK, So what are examples of disagreement coefficients and what sort of label complexity results do they yield?",
            "So we've already seen this threshold on the real line no matter what the data distribution is, no matter what the target is, the disagreement coefficient is 2, which means that the label complexity is log one over epsilon.",
            "For this class, as contrasted with one over epsilon for supervised learning.",
            "Linear separators."
        ],
        [
            "Indie dimensions.",
            "For a very specific data distribution, the uniform data distribution.",
            "It turns out that the disagreement coefficient is about square root T, which means the label complexity looks something like this.",
            "D for the three halves.",
            "Because it was tater times D times log one over epsilon looks something like this, which is still got this log one over epsilon rate that is very nice, but the uniform data distribution is not a realistic assumption."
        ],
        [
            "Any stretch of the imagination, and So what happens in more more general setting?",
            "Well, if you look at.",
            "Linear separators in RT and what you have is a smooth density bounded away from zero.",
            "Then the disagreement coefficient is at most some constant that depends on the target times D and again, you get this exponential decrease OK. OK, so.",
            "Yeah, this is just on this on a compact.",
            "OK. Do should we take a break or just keep going?",
            "OK, yeah.",
            "Something about this one here.",
            "Yeah, we have, so we will be seeing lower bounds fairly soon.",
            "John John has got a bunch of lower bounds.",
            "OK, so let's just take a brief break and then we'll come back and see the non separable case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's an area of machine learning that is really a very crucial practical importance.",
                    "label": 0
                },
                {
                    "sent": "And it's something that seems to require genuinely novel algorithms and novel styles of analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to.",
                    "label": 0
                },
                {
                    "sent": "Motivate it a little bit and just to start specifying the model.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot of the a lot of the work, a lot of what is known about classification is in a setting where every data point has an associated label, OK?",
                    "label": 0
                },
                {
                    "sent": "But in reality, it's frequently or very, very often the case that the role form of data is unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Anna Label is something that you actually have to buy, or you know actually obtain at some sort of expense.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, if you're building a document classification system.",
                    "label": 0
                },
                {
                    "sent": "You can download millions of documents off the web while you're away for lunch.",
                    "label": 1
                },
                {
                    "sent": "But labeling one of these documents takes a long time.",
                    "label": 0
                },
                {
                    "sent": "OK, you have some human being has to sit and look at the document and decide whether it's about sports or business or health or whatever, and it's something that takes effort.",
                    "label": 0
                },
                {
                    "sent": "If you're building a speech recognizer, you can just set a microphone down someplace and obtain hours and hours of speech.",
                    "label": 0
                },
                {
                    "sent": "But if you want it labeled, then some human being has got to look at the speech waveform and decide when each phoneme begins and the next one ends, and so on, which is very painful.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are all situations where you can easily obtain masses of unlabeled data, but labels come at a cost.",
                    "label": 0
                },
                {
                    "sent": "So it motivates a model in which some distinction is made between unlabeled data and labels that are subsequently.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "10th.",
                    "label": 0
                },
                {
                    "sent": "So here's the general picture.",
                    "label": 0
                },
                {
                    "sent": "Then you're in a setting where there's some vast number of unlabeled points.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In regular supervised learning, what effectively happens is that.",
                    "label": 1
                },
                {
                    "sent": "You get if you get labels for some of them and you ignore the rest.",
                    "label": 0
                },
                {
                    "sent": "As a result, the size of the data set might not be all that large, but you ignore the rest of the points and now you try to separate these, and in fact in this case there many ways to do that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More recently, it's become.",
                    "label": 0
                },
                {
                    "sent": "Sort of, you know there's a lot of people who started looking at semi supervised and active learning, where just as in the supervised case you have labels for some of the points.",
                    "label": 1
                },
                {
                    "sent": "But now you don't ignore the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In a semi supervised model, what happens is that you do choose just some of the points at random to get labels full and you take account of the locations of the unlabeled points as well.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you want to build a classifier just on the basis of these labeled points, there are many ways there many places where you could put the boundary.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you which is also take stock of where the unlabeled points are, it constrains the choices somewhat, because they do seem to kind of be 2 clusters, and you really want to put the boundary somewhere between those two clusters.",
                    "label": 0
                },
                {
                    "sent": "So this is a sense in which in a semi supervised model you get this additional information about the underlying distribution from the unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "In active learning you go one step further.",
                    "label": 0
                },
                {
                    "sent": "Once you have a few labels, what you do is you try and find you adaptively choose which point to query next.",
                    "label": 0
                },
                {
                    "sent": "So you look at you look at it and you say, oh, you know, I think the boundary is going to be here.",
                    "label": 0
                },
                {
                    "sent": "Maybe a very informative point to query would be this instead of say, this one, which we're pretty sure is going to be a red OK, and so the hope is that by choosing these these points very intelligently, you quickly converge on a good classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the active learning setup.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just give you a couple of examples of sort of recent case studies that have involved active learning.",
                    "label": 0
                },
                {
                    "sent": "The first one is 1.",
                    "label": 0
                },
                {
                    "sent": "Which was authored by the previous speaker, Manfred Barmat, along with Guna Raj and a bunch of chemists and the setting over.",
                    "label": 0
                },
                {
                    "sent": "Here is some sort of computational drug design.",
                    "label": 1
                },
                {
                    "sent": "OK, so the idea is that you want to find a compound that binds to a particular target.",
                    "label": 1
                },
                {
                    "sent": "And you have a vast library of compounds from which you can choose.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In this case, the pharmaceutical company has a lot of compounds that can choose from.",
                    "label": 0
                },
                {
                    "sent": "Some of these are virtual, you know there are things that have never before been synthesized, but can be synthesized on demand.",
                    "label": 0
                },
                {
                    "sent": "OK, you have some examples of compounds that bind to the target and you want to find more.",
                    "label": 0
                },
                {
                    "sent": "Now, one way in which this would be one way in which this would happen is that a bunch of scientists it done and they look at they look at which compounds seem to buy into the target and they come up with some hypothesis about you know what are the chemical properties of that compound that make it bind.",
                    "label": 0
                },
                {
                    "sent": "And then OK.",
                    "label": 0
                },
                {
                    "sent": "So based on the hypothesis, they say that, well, maybe this other compound will also bind.",
                    "label": 0
                },
                {
                    "sent": "Let's try that and then they do a chemistry experiment and they revise their hypothesis and so on.",
                    "label": 0
                },
                {
                    "sent": "But this also seems like something on which you could use active learning, and that's what that's what these people did.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, each unlabeled point corresponds to a potential compound, and it turns out that the chemist came up with a very high dimensional description space.",
                    "label": 1
                },
                {
                    "sent": "For these compounds it was literally something like 140,000 dimensional, some incredibly high dimensional way that captures a lot of the chemically interesting features.",
                    "label": 0
                },
                {
                    "sent": "Each point is got a label.",
                    "label": 0
                },
                {
                    "sent": "Either it's active, it binds to the target or it's inactive.",
                    "label": 0
                },
                {
                    "sent": "And now the querying process actually corresponds to a chemistry experiment, and so they were able to.",
                    "label": 0
                },
                {
                    "sent": "They were able to obtain some sort of traction on this fairly difficult seeming problem using their active learning method.",
                    "label": 0
                },
                {
                    "sent": "Here's another example, this was.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From pedestrian detection experiment that was conducted on the streets of Paris I believe and this is by Yotam Abramson and your friend.",
                    "label": 0
                },
                {
                    "sent": "And here the setting was that inside inside a car a camera was placed taking, you know, images of the world in front and the idea was to find the pedestrians.",
                    "label": 0
                },
                {
                    "sent": "The hope was that you know this would eventually become part of some cars software and make.",
                    "label": 0
                },
                {
                    "sent": "Cars, more safer pedestrians, that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so each data point in this case is not an entire image, but simply a rectangle within an image, and these rectangles could be of different sizes, and the label is is that a pedestrian or not?",
                    "label": 0
                },
                {
                    "sent": "And using this using active learning, they were they were able to avoid having to label too much of the data so you can collect masses of data just by having the camera sitting there in your car.",
                    "label": 0
                },
                {
                    "sent": "But but it's really laborious to have to label it, and So what they did is they developed a system in which some human being sat and looked at a whole bunch of these images and drew boxes around the human beings and then they came up with a classifier that basically you know was able to correctly classify those examples.",
                    "label": 0
                },
                {
                    "sent": "Anything that the humans had encircled was basically a negative example, so they were able to come up with a classifier that dealt with those examples and then then they went through some sort of refinement process so you know the classifier was then applied to a whole bunch of other images, and if there was something that was really unsure about a human being was asked to label that.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, if there was something on the sidewalk, there was like a phone booth, maybe that's something the classifier would be unsure about, or a fire hydrant.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so these are just two examples of.",
                    "label": 0
                },
                {
                    "sent": "Of active learning and it turns out that this.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, a large number of.",
                    "label": 0
                },
                {
                    "sent": "Of active learning algorithms out there.",
                    "label": 1
                },
                {
                    "sent": "Which we have already enjoyed a certain amount of success and in some sense you know.",
                    "label": 0
                },
                {
                    "sent": "In many applications it's really needed.",
                    "label": 0
                },
                {
                    "sent": "You know you have to use something and so so the whole sort of collection of active learning strategies has emerged.",
                    "label": 0
                },
                {
                    "sent": "So I'll start by talking about a certain class of these strategies and.",
                    "label": 0
                },
                {
                    "sent": "Then and then talk about what happens when we when we try and get some sort of basic statistical guarantees for these kinds of algorithms, what turns out to be the hurdles?",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot of strategies that look like this.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of it's very natural sort of thing.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of unlabeled points and you start by just choosing a bunch of them at random to get labels for just to sort of get the lay of the land.",
                    "label": 0
                },
                {
                    "sent": "And once you have that then you do have some sort of candidate, or you have some idea.",
                    "label": 0
                },
                {
                    "sent": "Of where the decision boundary is going to be, so you have some sort of candidate boundary, and once you look at that boundary you can then you know, decide well, you know the point I should query is the one I'm most uncertain about.",
                    "label": 0
                },
                {
                    "sent": "There's no point in querying this one, since I know exactly what it's going to be, so why not choose something near the boundary OK, and so there are strategies that, for instance.",
                    "label": 0
                },
                {
                    "sent": "Look for the point closest to the boundary, which in this case would be, say this one.",
                    "label": 1
                },
                {
                    "sent": "Or there's a variant that might look for the most uncertain point where, for instance, the distance from the boundary might transfer transformed into some sort of posterior probability an.",
                    "label": 0
                },
                {
                    "sent": "In that case this might also be the most uncertain point.",
                    "label": 0
                },
                {
                    "sent": "Then there are some variations in which the consideration is that, well, you know, although this might be closest to the boundary, it's kind of far away from the other day to an, so maybe a better point to pick is one of these.",
                    "label": 0
                },
                {
                    "sent": "Becausw if you have their label it would most influence the IT would lead to the largest update.",
                    "label": 0
                },
                {
                    "sent": "It would most influence your your classifications of the other examples.",
                    "label": 1
                },
                {
                    "sent": "OK, so rather than simply being the most uncertain point, it's the most likely to change to or to shrink your overall amount of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And there are various ways to formalize this.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a large class of heuristics of this form.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And actually, it seems this seems like a very sort of sensible strategy, but there's one sort of basic difficulty that emerges, and so let me let me just outline what that is.",
                    "label": 1
                },
                {
                    "sent": "Basically, in this sort of a setting we want to come up with a classifier that does well on the natural distribution out there.",
                    "label": 0
                },
                {
                    "sent": "You know the underlying distribution from which we have a few samples.",
                    "label": 0
                },
                {
                    "sent": "So in the first step of this process, we've chosen a few random points.",
                    "label": 0
                },
                {
                    "sent": "We've got their labels and the labeled points we have becausw.",
                    "label": 0
                },
                {
                    "sent": "They are chosen randomly, truly represent the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "There a random sample from the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "Once you start the active learning process, however, as time goes on, the training sample looks less and less like the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case you started off good, but it's time goes on.",
                    "label": 0
                },
                {
                    "sent": "You might get examples that are essentially concentrated near the boundary.",
                    "label": 0
                },
                {
                    "sent": "And so you know it.",
                    "label": 0
                },
                {
                    "sent": "1000 labels later, what you're left with is a training set that looks very unlike the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "And so now the question is, do we really want to optimize with respect to this training set?",
                    "label": 0
                },
                {
                    "sent": "OK, it's very different from the distribution we actually care about.",
                    "label": 0
                },
                {
                    "sent": "Who knows, maybe by being close, maybe the points right near the boundary are actually noisy points and we don't want to pay all that much attention to their labels, OK?",
                    "label": 0
                },
                {
                    "sent": "So I mean, so there are many ways in which to sort of summarize what the issue is.",
                    "label": 0
                },
                {
                    "sent": "But the simplest way to say it is that it's a problem of bias sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, that you.",
                    "label": 0
                },
                {
                    "sent": "As time goes on, your training set looks very different from the distribution you actually care about, and so there is some danger in optimizing with respect to this training set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So by the way, feel free to stop me at anytime, any questions?",
                    "label": 0
                },
                {
                    "sent": "But I thought this is the nature of the active link that you want to purposely choose some examples rather than randomly sampling.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, it's, but it seems like the one does want to choose informative points, and yet at the same time one wants the resulting classified to do well with respect to random looking points.",
                    "label": 0
                },
                {
                    "sent": "So it looks like there's going to be some sort of trade off, or there is some sort of conflict over here, and so the question is, how does one resolve this conflict?",
                    "label": 0
                },
                {
                    "sent": "And so that's the sort of thing that we'll be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Certain in terms of ice of sampling, are you concerned about the bias on the distribution of the points themselves or device in the distribution of the conditional distribution of the labels given the points could be concerned about both right now, so they both be right, they're both the source of concern.",
                    "label": 0
                },
                {
                    "sent": "Second, would be a source of concern, right?",
                    "label": 0
                },
                {
                    "sent": "So there are two that there are two problems.",
                    "label": 0
                },
                {
                    "sent": "The first is that the distribution of the X values is not nature's distribution of the X values, and the 2nd is the distribution of Y.",
                    "label": 0
                },
                {
                    "sent": "Given X might be different.",
                    "label": 0
                },
                {
                    "sent": "You know you might be by focusing on the boundary, you might essentially be in a regime where the distribution of Y given X is something like .5 four ways, and so those labels might be less reliable.",
                    "label": 0
                },
                {
                    "sent": "So this this is sort of at a high level and little bit.",
                    "label": 0
                },
                {
                    "sent": "You know, touchy feely.",
                    "label": 0
                },
                {
                    "sent": "This whole issue of sampling bias.",
                    "label": 1
                },
                {
                    "sent": "So just to make it more concrete, let's look at a specific example.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the sort of generic strategy that I talked about and I said there are many variants of it, but we'll just look at it, a kind of a high level OK. And now suppose that you know just to just to keep things simple.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a really easy hypothesis clause.",
                    "label": 0
                },
                {
                    "sent": "OK, so the data all lie on the line, so it's a 1 dimensional data set and each hypothesis is just a boundary on the line and to one side it's green and to the other side it's red.",
                    "label": 0
                },
                {
                    "sent": "OK, so a really simple sort of a hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Now here's an example of a data distribution.",
                    "label": 0
                },
                {
                    "sent": "45% of the points lie here and there greens.",
                    "label": 0
                },
                {
                    "sent": "45% lie here and these are Reds.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "In the middle you have two and a half percent Reds and 2 1/2% greens.",
                    "label": 0
                },
                {
                    "sent": "So let's see what would happen in one of these.",
                    "label": 0
                },
                {
                    "sent": "In one of these active learning heuristics.",
                    "label": 0
                },
                {
                    "sent": "So you start by sampling some random points, but since the data predominantly lies in these two clusters at the far ends, let's just say the initial batch of points pretty much like here and here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look at those two points.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have four points here and four points in the other one and you do something like you know a maximum margin classifier or something.",
                    "label": 1
                },
                {
                    "sent": "And based on that you put the boundary somewhere in here.",
                    "label": 0
                },
                {
                    "sent": "OK, now you're going to query points close to the boundaries.",
                    "label": 0
                },
                {
                    "sent": "You'll end up querying points in here as in time, and as time goes on you will query points closer and closer to this middle point and you'll basically end up converging to this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Now this hypothesis is got 5% error because it gets these wrong and the optimal hypothesis is this one because it has only 2 1/2% error.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example of the problem of sampling bias.",
                    "label": 0
                },
                {
                    "sent": "Basically what happens is that.",
                    "label": 1
                },
                {
                    "sent": "You know you start with this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That sort of in the middle of all of your greens and Reds.",
                    "label": 0
                },
                {
                    "sent": "It looks like a good hypothesis, and then you become really confident that all of these have to be Reds.",
                    "label": 0
                },
                {
                    "sent": "But you really have no justification for being so confident.",
                    "label": 0
                },
                {
                    "sent": "It's misplaced confidence, and so you end up with.",
                    "label": 0
                },
                {
                    "sent": "You end up with an algorithm that's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not even consistent.",
                    "label": 0
                },
                {
                    "sent": "You know which is, which is a fairly basic statistical guarantee.",
                    "label": 0
                },
                {
                    "sent": "You know, forget about optimizing the number of labels.",
                    "label": 0
                },
                {
                    "sent": "One would hope that as the number of labels goes to Infinity, you at least converge to the right thing.",
                    "label": 0
                },
                {
                    "sent": "And in this case that wouldn't happen.",
                    "label": 0
                },
                {
                    "sent": "OK, it's something that's not consistent.",
                    "label": 0
                },
                {
                    "sent": "So so an example of.",
                    "label": 0
                },
                {
                    "sent": "An example of how this might play out in practice if you, if you think back to that pedestrian identification pedestrian detection thing, you know it might be that after seeing the first few labeled images, the learning system density is an image of a fire hydrant and it's completely certain it's a human being.",
                    "label": 0
                },
                {
                    "sent": "You know a child or something like that, and so it will never ask for its label.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no need to, and you know.",
                    "label": 0
                },
                {
                    "sent": "The cluster of fire hydrants in the space of data.",
                    "label": 0
                },
                {
                    "sent": "There's a cluster of fly hydrants in a cluster of phone boots that look like this.",
                    "label": 0
                },
                {
                    "sent": "The cluster of fire hydrants.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So everything is.",
                    "label": 0
                },
                {
                    "sent": "There's been a bunch of experimental work here.",
                    "label": 0
                },
                {
                    "sent": "This is an example of this is an example of.",
                    "label": 0
                },
                {
                    "sent": "Sort of a study and.",
                    "label": 0
                },
                {
                    "sent": "So this is a text classification setting in which this sort of hidden cluster effect emerged.",
                    "label": 0
                },
                {
                    "sent": "Here you have one of these active learning heuristics where the initial sampling process misses some cluster and as a result the learner is completely sure about the label of the cluster and never bothers to ask, but later on it turns out to be wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example of the sampling bias problem OK, and in my view, and I think also in John's view, this really is the central.",
                    "label": 0
                },
                {
                    "sent": "Difficulty or sort of the central hurt, or in active learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This is similar to just confusing a local minimum with a global minimum.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, this is, I think this is actually.",
                    "label": 0
                },
                {
                    "sent": "A more severe problem, you know it's.",
                    "label": 0
                },
                {
                    "sent": "'cause because that's a computational problem.",
                    "label": 0
                },
                {
                    "sent": "And this is this is a basic statistical issue.",
                    "label": 0
                },
                {
                    "sent": "You know that's the local versus global optimum.",
                    "label": 0
                },
                {
                    "sent": "That's something you know.",
                    "label": 0
                },
                {
                    "sent": "You invest a little bit more time.",
                    "label": 0
                },
                {
                    "sent": "You get into solution here.",
                    "label": 0
                },
                {
                    "sent": "You end up with the data set that's not representative and you know you can get the best solution with respect to that training set.",
                    "label": 0
                },
                {
                    "sent": "But it might be very bad on the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Problem with both here you're not really estimating probabilities here mean distance from the margins.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean.",
                    "label": 0
                },
                {
                    "sent": "Because they are, there are resources about experimental design that they convert against science.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so over here.",
                    "label": 0
                },
                {
                    "sent": "What I meant by consistency is purely in a classification setting in the sense that you want.",
                    "label": 0
                },
                {
                    "sent": "Making the material probabilities of the class instead of putting some.",
                    "label": 0
                },
                {
                    "sent": "Decision boundary and finding some distance from the margin I mean that can do this differently.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, so it's it's possible that you know.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic framework one changes things somewhat, but but the bottom line.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is that you know these are examples.",
                    "label": 0
                },
                {
                    "sent": "This is an example in which this classifier OK, the one at this boundary, has got the smallest error rate.",
                    "label": 0
                },
                {
                    "sent": "It has an error rate of 2.5%.",
                    "label": 0
                },
                {
                    "sent": "And in this setting you converge to something whose error rate is 5%.",
                    "label": 0
                },
                {
                    "sent": "You know so.",
                    "label": 0
                },
                {
                    "sent": "So I'm talking bout 01 loss over here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so there's a clear problem of not converging to the best possible hypothesis, even as the amount of labels you have becomes arbitrarily large.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Local minima if you think about minimizing generalization.",
                    "label": 0
                },
                {
                    "sent": "This whole thing is an optimization strategy.",
                    "label": 0
                },
                {
                    "sent": "To minimize that there is.",
                    "label": 0
                },
                {
                    "sent": "Really most importantly I see right?",
                    "label": 0
                },
                {
                    "sent": "Right, so sort of meta meta review, right?",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 1
                },
                {
                    "sent": "The reason why it doesn't work well in this example is cause it's not consistent.",
                    "label": 0
                },
                {
                    "sent": "Consistently scenario sample bias is still a problem.",
                    "label": 0
                },
                {
                    "sent": "Can we say there or?",
                    "label": 0
                },
                {
                    "sent": "So actually the I didn't mean to suggest that active learning doesn't work well here.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that a specific class of heuristics would fail on this BIH cause although they are intuitive, they fail to take into account they failed to explicitly think about sampling bias.",
                    "label": 0
                },
                {
                    "sent": "Specifically offer, is it still have some problems?",
                    "label": 0
                },
                {
                    "sent": "Like consistency.",
                    "label": 0
                },
                {
                    "sent": "Concentra bias is always there when this condition or not.",
                    "label": 1
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "So the sampling bias is always an issue, but this is the case in which.",
                    "label": 1
                },
                {
                    "sent": "In which a bias is being created becausw of because of the querying strategy.",
                    "label": 0
                },
                {
                    "sent": "It's true that you know what you care about.",
                    "label": 0
                },
                {
                    "sent": "Is an infinite distribution and you're getting just a finite number of samples and those finite samples are not going to be a perfect replica of the underlying distribution, but it's very well understood how to quantify the difference between those two things in a classification setting.",
                    "label": 0
                },
                {
                    "sent": "This is something a little bit more sinister.",
                    "label": 0
                },
                {
                    "sent": "OK, so it seems like one of the problems is that your algorithm is kind of fixating onto one hypothesis test.",
                    "label": 0
                },
                {
                    "sent": "I'm not even thinking about the other possible things that could have been consistent in the data and was wondering if one could make like a general statement that any algorithm which would have this property can be broken in some in some sense that would be very interesting.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's plausable and.",
                    "label": 1
                },
                {
                    "sent": "It would be interesting to do that and the fix you suggested, which is that one should take into account not just one candidate hypothesis, one candidate hypothesis, but you know all possible candidate hypothesis that indeed is one way to fix the problem, and it's something that we'll be looking at.",
                    "label": 0
                },
                {
                    "sent": "The label is missing completely at random or having any kind of bad assumption.",
                    "label": 0
                },
                {
                    "sent": "That is violated the first time we have a query, but are you making any assumptions, and if not, can we fix some of these things by making that assumption so we will end up fixing this problem in setting without assumptions?",
                    "label": 0
                },
                {
                    "sent": "But as an intermediate setting, we will have some assumptions just to make things easier to explain, but we'll eventually get to.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are we doing here?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically the goal in this tutorial is to talk about some schemes for active learning.",
                    "label": 0
                },
                {
                    "sent": "OK now if we were to go through everything that's known, it would really take a very long time.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to focus on is actually actually schemes that have some sort of consistency.",
                    "label": 0
                },
                {
                    "sent": "Some of this.",
                    "label": 0
                },
                {
                    "sent": "Some of this had this property, at least to some extent.",
                    "label": 0
                },
                {
                    "sent": "OK, and a lot of this would involve just just looking at some of the heuristics out there.",
                    "label": 0
                },
                {
                    "sent": "And seeing what can be done with them to make them consistent.",
                    "label": 0
                },
                {
                    "sent": "For example, OK now.",
                    "label": 0
                },
                {
                    "sent": "So the first question is this active learning actually help at all?",
                    "label": 0
                },
                {
                    "sent": "And in answering this there are at least two possible kind of toy scenarios that people have in mind.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of them is exploiting cluster structuring data, and the 2nd is efficient search through hypothesis space and so let me just kind of briefly explain what these caricatures are.",
                    "label": 0
                },
                {
                    "sent": "These toy scenarios, the cluster structure thing, the cluster structure story goes like this.",
                    "label": 1
                },
                {
                    "sent": "It says you look at your unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And you notice that there are five clusters, and so you say, hey, I just need five labels and I'm done.",
                    "label": 1
                },
                {
                    "sent": "OK, give me one label in each cluster.",
                    "label": 0
                },
                {
                    "sent": "That's the label of the cluster.",
                    "label": 0
                },
                {
                    "sent": "End of story.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's one sort of caricature model of active learning, OK?",
                    "label": 0
                },
                {
                    "sent": "Of why active learning is useful.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, there are some challenges.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all.",
                    "label": 0
                },
                {
                    "sent": "The cluster structure might not be so clearly defined.",
                    "label": 1
                },
                {
                    "sent": "It might not be that you look at the data and you know there are five clusters far away from each other.",
                    "label": 1
                },
                {
                    "sent": "Second, even if there is cluster structure, it could be at many levels of granularity.",
                    "label": 0
                },
                {
                    "sent": "You could look at it and say you know what.",
                    "label": 0
                },
                {
                    "sent": "There are two clusters far away from each other, but you know there's also a nice way to do 3 clusters, and a nice way to do 5 clusters in a nice way to do 10 clusters.",
                    "label": 0
                },
                {
                    "sent": "And finally, there might be clusters, but then the clusters might not be pure OK, so.",
                    "label": 1
                },
                {
                    "sent": "So what can be done?",
                    "label": 0
                },
                {
                    "sent": "Is there some sort of nice robust scheme that is able to exploit whatever cluster happens to exist?",
                    "label": 0
                },
                {
                    "sent": "Whatever cluster structure happens to exist, and you know otherwise, you know behaves like supervised learning, so this is 1 sort of toy scenario that people use to.",
                    "label": 0
                },
                {
                    "sent": "To suggest that active learn.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Might be useful.",
                    "label": 0
                },
                {
                    "sent": "The other scenario.",
                    "label": 0
                },
                {
                    "sent": "Is efficient search through hypothesis space OK?",
                    "label": 1
                },
                {
                    "sent": "So let me explain this, because this is actually what we're going to be focusing on in this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "So here's the here's the thing over here.",
                    "label": 0
                },
                {
                    "sent": "This box is supposed to represent the space of candidate hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the space of all linear separators, if that's what you're interested in, any data point can be thought of as a cut in this space.",
                    "label": 0
                },
                {
                    "sent": "OK, so a data point divides the space into two groups.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis that label that point as a plus, and the hypothesis that label that point is a minus.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose you take one such data point.",
                    "label": 0
                },
                {
                    "sent": "And let's say this is the cut.",
                    "label": 0
                },
                {
                    "sent": "That it corresponds to and now let's say you ask for its label and the label turns out to be plus.",
                    "label": 0
                },
                {
                    "sent": "Then you can throw away all of these hypothesis over here if the problem is separable.",
                    "label": 0
                },
                {
                    "sent": "OK, in other words, if you feel that for some reason there is some perfect hypothesis out there, some hypothesis that will perfectly classify all points.",
                    "label": 0
                },
                {
                    "sent": "This is not a realistic assumption.",
                    "label": 0
                },
                {
                    "sent": "But it's an intermediate thing that we'll use just to explain some of the concepts before we get to the realistic case, which is the non separable case, OK?",
                    "label": 0
                },
                {
                    "sent": "Now, if you can do this.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice situation because here you get one label and you at least in this picture you seem to eliminate half of the hypothesis OK, and so you know the way this sort of story continues is that this happens with every single label you ask for an.",
                    "label": 0
                },
                {
                    "sent": "So by the time you've asked for log H labels, you're down to one hypothesis and you're done.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's the search through hypothesis space story.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Needless to say, there are lots of problems with this one as well, OK?",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "This is a happy situation in which the data cut seems to divide the hypothesis space roughly in half.",
                    "label": 0
                },
                {
                    "sent": "But what if the cut just looks like this?",
                    "label": 0
                },
                {
                    "sent": "It cuts off a little bit.",
                    "label": 0
                },
                {
                    "sent": "The minus is here in the pluses there and the label you get is a plus, so all you can cut off is this tiny little corner, OK?",
                    "label": 0
                },
                {
                    "sent": "In that case, getting the label doesn't tell you very much.",
                    "label": 0
                },
                {
                    "sent": "OK, so do they always exist queries that are going to cut off a lot of the space or not?",
                    "label": 1
                },
                {
                    "sent": "That's problem number.",
                    "label": 0
                },
                {
                    "sent": "One second, even if there do exist such queries, even if there is some magical point that's going to cut off half the space, how do you find what that point is?",
                    "label": 0
                },
                {
                    "sent": "Is there some way to do that?",
                    "label": 0
                },
                {
                    "sent": "And finally, this all seems to fall apart in a big way when the data is non separable.",
                    "label": 0
                },
                {
                    "sent": "Because then you get a label of plus, but the best hypothesis might be over here and you can't eliminate this off.",
                    "label": 0
                },
                {
                    "sent": "So what can you do in that setting?",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the hurdles that have to be overcome in.",
                    "label": 0
                },
                {
                    "sent": "In this sort of toy scenario.",
                    "label": 0
                },
                {
                    "sent": "An so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is going to be the outline of our tutorial we're going to.",
                    "label": 1
                },
                {
                    "sent": "Basically going to look at algorithms designed for each of these two scenarios, and as I said for us, we're really going to want algorithms that have statistical consistency OK, and the focus is very much going to be on the 2nd on the second of these categories, because that's where a lot of work has been done.",
                    "label": 0
                },
                {
                    "sent": "Very little has been done on the 1st, and so I'll just quickly say a few things about it.",
                    "label": 0
                },
                {
                    "sent": "And then move on to the second one, OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give you an example of a cluster based active learning scheme.",
                    "label": 1
                },
                {
                    "sent": "All of this scheme might not originally have been presented as such, so this is a very nice algorithm.",
                    "label": 0
                },
                {
                    "sent": "Do too.",
                    "label": 0
                },
                {
                    "sent": "Shall Jinju Zubin Caramani an John Lafferty?",
                    "label": 0
                },
                {
                    "sent": "And and here's, here's how the scheme works.",
                    "label": 0
                },
                {
                    "sent": "Very roughly, I'm going to just look at it in a very high level, so you start with a bunch of data points, and then you build a neighborhood graph on them so you have a node for each data point.",
                    "label": 0
                },
                {
                    "sent": "And then you put an edge between points that are close together.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, within some radius or points that are K nearest neighbors of each other.",
                    "label": 0
                },
                {
                    "sent": "Or sometimes you just put an edge between every two points and you wait the edges according to distance there are.",
                    "label": 0
                },
                {
                    "sent": "There are several ways to do this, but you do this.",
                    "label": 0
                },
                {
                    "sent": "You build a neighborhood edge.",
                    "label": 1
                },
                {
                    "sent": "You build a neighborhood graph and then you query some points.",
                    "label": 0
                },
                {
                    "sent": "So let's in this case, let's just say we have.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Binary problem, query some points.",
                    "label": 1
                },
                {
                    "sent": "Let's say we query these two points that turns out to be a zero.",
                    "label": 0
                },
                {
                    "sent": "This one over here turns out to be a one.",
                    "label": 0
                },
                {
                    "sent": "Now you propagate the labels.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. You look at the data you have so far and you propagate it to the rest of the graph, and it turns out that there are some really nice ways to do this.",
                    "label": 0
                },
                {
                    "sent": "Basically by solving an eigenvalue problem and one way to think about it is to think about the semantics of this number is suppose you started at this node and you did a random walk.",
                    "label": 0
                },
                {
                    "sent": "What fraction of time would you first hit a one?",
                    "label": 0
                },
                {
                    "sent": "What fraction of time would you first hit a zero?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think of the semantics of these numbers in that form roughly OK. Now, once you have these.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can use this as a basis for deciding which point to query and so, for instance, one thing you might want to do is to query the most uncertain point which.",
                    "label": 0
                },
                {
                    "sent": "Which would be this a .5?",
                    "label": 0
                },
                {
                    "sent": "But then this seems like a point that's a little bit of an outlier.",
                    "label": 0
                },
                {
                    "sent": "It's kind of sticking out from the graph, so it might be better to.",
                    "label": 0
                },
                {
                    "sent": "To query this because it's likely to lead to the biggest reduction in overall uncertainty and one can put a nice probabilistic semantics on top of these kinds of graphs in which all these all these analysis are very meaningful and you can actually quantify the amount of residual uncertainty after finding a label.",
                    "label": 0
                },
                {
                    "sent": "So you query a point and then you go back.",
                    "label": 0
                },
                {
                    "sent": "So why is this?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a sense, cluster based active learning.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "If you have clearly defined clusters in the data.",
                    "label": 0
                },
                {
                    "sent": "You might have the clusters might be something like these three and then the remaining points.",
                    "label": 0
                },
                {
                    "sent": "Basically, when they're clearly defined clusters, the corresponding nodes will have a cut between them through which which don't have too many edges and through which influence can propagate only weakly.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example of.",
                    "label": 0
                },
                {
                    "sent": "Beautiful scheme for doing active learning that that seems to be motivated in part by a cluster learning by sort of a cluster model.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I'm not, I'm not clear on what the what the consistency status of this is.",
                    "label": 0
                },
                {
                    "sent": "You know what one can say about statistical consistency, but I can tell you that, but I'll now tell you briefly about some work that basically uses this as motivation to come up with a different scheme in which you can actually prove things about consistency, and that will give some idea of the kind of analysis that needs to be done.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is another cluster based scheme.",
                    "label": 0
                },
                {
                    "sent": "Which attempts to use some of the same intuitions from the previous one, while while achieving consistency and so the idea is that you look at the data, you find a bunch of clusters.",
                    "label": 0
                },
                {
                    "sent": "You sample a few points in each cluster and then you assign each cluster its majority label.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you use that label set to build a classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of a high level of what's going to be done, but not.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the reality.",
                    "label": 0
                },
                {
                    "sent": "OK, so you start with some unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You find clusters.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also some labels.",
                    "label": 0
                },
                {
                    "sent": "Point this cluster is looking pretty good because it's pure.",
                    "label": 0
                },
                {
                    "sent": "It's relatively pure.",
                    "label": 0
                },
                {
                    "sent": "You have six points.",
                    "label": 0
                },
                {
                    "sent": "You've chosen 6 random points in that in this cluster.",
                    "label": 0
                },
                {
                    "sent": "That gives you a certain confidence interval on how pure this cluster is, you know.",
                    "label": 0
                },
                {
                    "sent": "And because it's a random sample you can use, you can come up with these sort of confidence intervals, so you're feeling good about this one.",
                    "label": 0
                },
                {
                    "sent": "This one is looking like trouble OK?",
                    "label": 0
                },
                {
                    "sent": "And So what you do then?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that you?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then refine the clustering.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the overall overall a nice way to do.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is to just use a hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And So what the figure on the right denotes is at the top level that node corresponds to a single cluster that contains all the data points.",
                    "label": 0
                },
                {
                    "sent": "Then it gets split into two.",
                    "label": 0
                },
                {
                    "sent": "It gets split into the one on the left and the one on the right, and then the one on the right is split into two, the one on top and the one below.",
                    "label": 1
                },
                {
                    "sent": "And of course the tree keeps going down OK to however many levels you like.",
                    "label": 0
                },
                {
                    "sent": "So how does this this sampling scheme work?",
                    "label": 0
                },
                {
                    "sent": "Well, you start by being really optimistic.",
                    "label": 0
                },
                {
                    "sent": "You say, well, let's just assume there's this one cluster all the way on top, and all the labels are the same.",
                    "label": 0
                },
                {
                    "sent": "That's the best we can hope for, but you take just a few samples and you very quickly realize that that is wrong, OK?",
                    "label": 0
                },
                {
                    "sent": "So then you move down to the next level.",
                    "label": 0
                },
                {
                    "sent": "OK, and now.",
                    "label": 0
                },
                {
                    "sent": "Now the samples tell you that on the left you're doing well and you need to recurse further on the right and so on.",
                    "label": 1
                },
                {
                    "sent": "The key thing in this scenario is that at any given time you maintain a certain pruning of the tree, a certain clustering that you're currently working with, and as time goes on, you might have to move further down the tree and refine that clustering somewhat.",
                    "label": 0
                },
                {
                    "sent": "However, when you're working with a certain clustering, the primitive is that you choose a cluster.",
                    "label": 0
                },
                {
                    "sent": "And you pick it random within it.",
                    "label": 0
                },
                {
                    "sent": "And it's very important to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, when you're in the two cluster setting, you pick one of the clusters, but once you've picked which cluster you're going to query, you have to query a random point within that cluster, and I won't really.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go into the details over here because I want to focus on the main topic of the tutorial, which is really the second the second form of active learning, which is efficient search through hypothesis space.",
                    "label": 1
                },
                {
                    "sent": "And as I said, we'll start by talking about the separable case, which you should think of as being a very unrealistic case, and then we'll show that actually you can do everything for the non separable case.",
                    "label": 0
                },
                {
                    "sent": "For the general case as well, OK?",
                    "label": 1
                },
                {
                    "sent": "OK, so efficient.",
                    "label": 0
                },
                {
                    "sent": "Search through hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "Here is the Canonical example.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The people use to motivate this type of active learning OK.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "That once again, a hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "Is this really simple class where all data lies on the line and each hypothesis is just a boundary on the line and everything to the right is a plus everything to the left is a minus.",
                    "label": 0
                },
                {
                    "sent": "OK, so the hypothesis class can be written in that way.",
                    "label": 0
                },
                {
                    "sent": "Each hypothesis is just indexed by a single real number, which is the location of that boundary.",
                    "label": 0
                },
                {
                    "sent": "This is the class of VC dimension one.",
                    "label": 0
                },
                {
                    "sent": "It's a very primitive class and it's a close.",
                    "label": 0
                },
                {
                    "sent": "This very well understood in terms of its in terms of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "What's known is that.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "You're going to do supervised learning, and you want to end up with.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis, whose error is less than epsilon will then the number of labeled points you need is about one over epsilon OK, and that's basically because its VC dimension is 1.",
                    "label": 0
                },
                {
                    "sent": "It has one parameter, so this is something that's very well understood in a supervised setting.",
                    "label": 0
                },
                {
                    "sent": "We know exactly what we need over here.",
                    "label": 0
                },
                {
                    "sent": "So let's see what happens in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active learning.",
                    "label": 0
                },
                {
                    "sent": "In active learning, what happens is that you start with unlabeled points.",
                    "label": 1
                },
                {
                    "sent": "So let's start with one over epsilon unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have no idea what their labels are, and now we start querying.",
                    "label": 0
                },
                {
                    "sent": "Here's how you can query.",
                    "label": 0
                },
                {
                    "sent": "You can start by asking for the label of the middle point, the point at the 50 percentile point you ask for the one in the middle.",
                    "label": 0
                },
                {
                    "sent": "Suppose the label turns out to be a plus.",
                    "label": 0
                },
                {
                    "sent": "We are still in the separable setting, supposed to label turns out to be a plus.",
                    "label": 0
                },
                {
                    "sent": "Well then you know that all of these points are pluses and you never need to query them.",
                    "label": 0
                },
                {
                    "sent": "Suppose the label turns out to be a minus.",
                    "label": 0
                },
                {
                    "sent": "They know that all of these points are minus is and you don't need to query them.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say there is a plus.",
                    "label": 0
                },
                {
                    "sent": "We know all of these are pluses and then we can move to the 25% point and in this way you can do a binary search.",
                    "label": 0
                },
                {
                    "sent": "So what ends up happening is that you ask for just log one over epsilon labels instead of one over EPS.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is a huge improvement.",
                    "label": 0
                },
                {
                    "sent": "OK, so you do this binary search.",
                    "label": 1
                },
                {
                    "sent": "And it's literally log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you want your error to be .1%, then in the supervised setting you need about 1000 points, whereas over here you would need just log of 1000 labels, which is something like 10 labels.",
                    "label": 1
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So it's a setting in which active learning really does give a big improvement.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about this particular example?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here the challenge is the first is this again seems to assume separable data, you know, because if you think this is a plus, then you're going to assume everything here is the plus.",
                    "label": 0
                },
                {
                    "sent": "Obviously that doesn't happen in, you know in a non separable data set, but it turns out that there are many sort of noisy binary search procedures out there which you can use for for for this problem you know procedures where.",
                    "label": 0
                },
                {
                    "sent": "The answers are assumed to be given by a malicious adversary who makes a certain number of mistakes and so on.",
                    "label": 0
                },
                {
                    "sent": "These things have been analyzed thoroughly, and so there are ways to their ways to adapt.",
                    "label": 0
                },
                {
                    "sent": "This specific example for the non separable case that's not a problem with the same sort of exponential improvement.",
                    "label": 1
                },
                {
                    "sent": "The problem really is how to use this for other hypothesis classes.",
                    "label": 1
                },
                {
                    "sent": "This is sort of a toy hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "How do you do this more generally, and that's something on which there's been a lot of work and so let me just summarize some of what's known and.",
                    "label": 1
                },
                {
                    "sent": "I'll use a particular sort of format where I'll divide active learning work into four categories, OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first 2 columns are separable versus non separable and a lot of the earlier work was very much in the separable case because it's much easier to deal with that case OK.",
                    "label": 0
                },
                {
                    "sent": "The rows are two sort of different types of active learning strategies, and one of which I'm calling aggressive and the other is mellow.",
                    "label": 0
                },
                {
                    "sent": "So what this means is so an aggressive active learner is one that is looking for the most informative point and actually the heuristics we've seen so far are pretty much aggressive heuristics.",
                    "label": 0
                },
                {
                    "sent": "You know, find the point closest to the boundary or find the point that will most reduce uncertainty that's aggressive.",
                    "label": 0
                },
                {
                    "sent": "There's another kind of active learning that you can call a mellow active learner where it's not trying to find the best or most informative point is just trying to find something that's not too redundant.",
                    "label": 1
                },
                {
                    "sent": "You know it's not, it's it's a little bit more.",
                    "label": 0
                },
                {
                    "sent": "It's slightly lower standards for what for what kind of point is going to pick OK, and it turns out that in the separable in the separable case both both have been analyzed, aggressive and mellow, and in the non separable case there's been a lot of work on this sort of mellow active learning and.",
                    "label": 0
                },
                {
                    "sent": "And actually would surprisingly good bounds considering how mellow these algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seem to be OK, so that's something that I'll talk about in a second, but sort of the underlying issues over here will turn out to be computational tractability.",
                    "label": 0
                },
                {
                    "sent": "Finding the best point, this sort of thing turns out a lot of the early schemes turned out to be computationally intractable.",
                    "label": 0
                },
                {
                    "sent": "And a big problem is actually quantifying label complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have some scheme how many labels does it actually use?",
                    "label": 0
                },
                {
                    "sent": "Can that be analyzed?",
                    "label": 0
                },
                {
                    "sent": "And how and what is the best possible number of labels that can be used?",
                    "label": 0
                },
                {
                    "sent": "Can that be analyzed?",
                    "label": 0
                },
                {
                    "sent": "Can you give Upper and lower bounds?",
                    "label": 0
                },
                {
                    "sent": "There's actually been a lot of progress on this front, which we'll talk about.",
                    "label": 0
                },
                {
                    "sent": "OK, so you've seen what an aggressive learner is.",
                    "label": 0
                },
                {
                    "sent": "Let me give you an example.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Absolutely wonderful, mellow active learner and this is work due to Cone app Listen Ladner from 91.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice algorithm and we're just going to be calling it Cal.",
                    "label": 0
                },
                {
                    "sent": "So this is in the separable setting, although we'll see that you can do this sort of thing in non separable as well and the data is streaming in OK, so the data points come one at a time.",
                    "label": 1
                },
                {
                    "sent": "You get one unlabeled point, then another one, then another one, and each time you get an unlabeled point you have to make a spot decision about whether or not to ask for its label.",
                    "label": 0
                },
                {
                    "sent": "So if you ask for its label, you get it.",
                    "label": 0
                },
                {
                    "sent": "If you don't, you don't get it either way, the point goes away after that.",
                    "label": 0
                },
                {
                    "sent": "OK, and you get the next one and these points are generated IID from some underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's how Cal works.",
                    "label": 0
                },
                {
                    "sent": "You always maintain the set of candidate hypothesis, which is the hypothesis that are consistent with everything so far.",
                    "label": 0
                },
                {
                    "sent": "Initially, it's the whole hypothesis class, because you haven't seen any data yet.",
                    "label": 0
                },
                {
                    "sent": "Now the learning process begins, so at each time step you receive an unlabeled point and now you get this unlabeled point and you look at all of your candidate hypothesis and you say, well, you know what they all think.",
                    "label": 1
                },
                {
                    "sent": "This point is a one.",
                    "label": 0
                },
                {
                    "sent": "So let me not ask for its label.",
                    "label": 0
                },
                {
                    "sent": "There's no point.",
                    "label": 1
                },
                {
                    "sent": "I know it's label is a one or they all think it's a zero.",
                    "label": 0
                },
                {
                    "sent": "I know it's a 0, but if there's any disagreement.",
                    "label": 1
                },
                {
                    "sent": "If a million minus one of them think it's a one and the last one thinks it's a zero, you ask for its label.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it's a mellow learner.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's just trying to avoid redundancy.",
                    "label": 0
                },
                {
                    "sent": "That's all it's doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so any questions about this this algorithm?",
                    "label": 0
                },
                {
                    "sent": "So let's just introduce some concepts based on the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here's the setting.",
                    "label": 0
                },
                {
                    "sent": "You've already seen six points or 7 points, and you ask for the labels of all of them, and now the eight points shows up.",
                    "label": 1
                },
                {
                    "sent": "OK, so the eight point in the stream arise arrives.",
                    "label": 0
                },
                {
                    "sent": "And you have to figure out whether or not to ask for its label.",
                    "label": 0
                },
                {
                    "sent": "And let's say your hypothesis class is linear separators.",
                    "label": 0
                },
                {
                    "sent": "In this case, there's no need to ask for its label, because there's no way the label could be a green.",
                    "label": 0
                },
                {
                    "sent": "There's no linear separated that would make that agree, and so you don't ask for its label.",
                    "label": 0
                },
                {
                    "sent": "Now here are.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some concepts.",
                    "label": 0
                },
                {
                    "sent": "Age 50 is the current set of candidate hypothesis.",
                    "label": 0
                },
                {
                    "sent": "These are simply the hypothesis that are consistent with the data so far, and there's a lot of them.",
                    "label": 0
                },
                {
                    "sent": "There's kind of a, you know, there's a continuum of them somewhere in the middle over here.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other concept is the region of uncertainty.",
                    "label": 1
                },
                {
                    "sent": "These are the points.",
                    "label": 1
                },
                {
                    "sent": "These are the data points on which there is any uncertainty at all, and so these are the points.",
                    "label": 0
                },
                {
                    "sent": "These are exactly the points that would be queried.",
                    "label": 0
                },
                {
                    "sent": "You're not going to query anything outside this region.",
                    "label": 0
                },
                {
                    "sent": "Now the hope is that this region of uncertainty will shrink quickly and so just by being mellow and avoiding redundant queries, you actually save a lot.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the that's the hope with this scheme.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And just in terms of scaling this up, there are two obvious sort of issues with it.",
                    "label": 0
                },
                {
                    "sent": "The first is is that it seems to make you keep a list of all the current hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It seems like you need to keep.",
                    "label": 0
                },
                {
                    "sent": "What are the hypothesis that are consistent with the data so far and each time you get a new point you have to work your way down that list.",
                    "label": 0
                },
                {
                    "sent": "That seems intractable, and Secondly it's not at all obvious what to do for non separable data, but both of them are can be tackled.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the first is really easy, so let's see how to do that.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can instead of actually keeping the list of all of all hypothesis that are consistent so far, you can do this implicitly.",
                    "label": 0
                },
                {
                    "sent": "So here's the explicit version that we saw in the previous slide where you have this explicit HD that you keep you keep track of, and each time you get a new point to go through that list, the implicit version works like this.",
                    "label": 1
                },
                {
                    "sent": "It works by reduction to supervised learning.",
                    "label": 1
                },
                {
                    "sent": "You assume you have a black box learn.",
                    "label": 0
                },
                {
                    "sent": "It's just a supervised learner.",
                    "label": 0
                },
                {
                    "sent": "You give it a label set an it gives something that's consistent with all the points.",
                    "label": 0
                },
                {
                    "sent": "If something like that exists.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it says nothing available.",
                    "label": 1
                },
                {
                    "sent": "OK, so you just keep track of the points you've seen each time you get a new point, you feed the supervised learner.",
                    "label": 0
                },
                {
                    "sent": "The entire label set so far, and you give the new point with a one label and you see what it does.",
                    "label": 0
                },
                {
                    "sent": "If it can output a hypothesis, it means that there is something in age subte that thinks he is a one.",
                    "label": 0
                },
                {
                    "sent": "And now you do the same thing with the zero.",
                    "label": 0
                },
                {
                    "sent": "You feed it the same data set with this new point, you give it a zero label and you see the supervised learning can do anything with that if it can will, then there's something in H subte that makes exceptia zero, and so if both of these return an answer then you have to query the label.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you know what the label is and you just stick to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that problem of maintaining HFT just goes away by this sort of read.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The other thing that's easy to see is that this certainly not worse than supervised learning because it's doing exactly the same thing.",
                    "label": 1
                },
                {
                    "sent": "You're just ignoring the points whose labels you're completely sure of.",
                    "label": 0
                },
                {
                    "sent": "But at the end of the day, you end up with the same set.",
                    "label": 0
                },
                {
                    "sent": "The real challenge is to bound the number of labels required and it turns out that this.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has been done and this is a beautiful result due to Steve Hanneke.",
                    "label": 0
                },
                {
                    "sent": "He's been doing just a lot of fantastic work in active learning.",
                    "label": 0
                },
                {
                    "sent": "OK, So what he did?",
                    "label": 0
                },
                {
                    "sent": "Is to analyze the labeled complexity of this mellow, separable active learner, and it turns out that you need a new concept in order to do that.",
                    "label": 0
                },
                {
                    "sent": "So for supervised learning, you can give label complexity bounds or sample complexity bounds in terms of the VC dimension.",
                    "label": 1
                },
                {
                    "sent": "That's enough to characterize how many samples are needed, the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And then there are also various alternatives to it.",
                    "label": 0
                },
                {
                    "sent": "You know, various, there's other ways to do it, but.",
                    "label": 0
                },
                {
                    "sent": "But but but it turns out that in the active setting, the VC dimension alone does not characterize the label complexity at all.",
                    "label": 0
                },
                {
                    "sent": "And what Steve did was to develop.",
                    "label": 0
                },
                {
                    "sent": "His main insight was to find this parameter that he calls the disagreement coefficient, and this parameter captures the label complexity of mellow active learning.",
                    "label": 1
                },
                {
                    "sent": "OK, which is the type of active learning that's really been studied.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A lot.",
                    "label": 0
                },
                {
                    "sent": "So so so in order to tell you the result, let me first start by reminding you of what this regular supervised learning setting is OK.",
                    "label": 0
                },
                {
                    "sent": "So suppose you're in a supervised setting.",
                    "label": 0
                },
                {
                    "sent": "You have data ID from some underlying distribution, and you want a hypothesis whose misclassification rate whose error rate on the underlying distribution on natures this distribution is at most epsilon.",
                    "label": 1
                },
                {
                    "sent": "Well, the number of examples you then need is D over epsilon up to some constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 1
                },
                {
                    "sent": "Regular supervised learning in the separable case you need D over epsilon examples.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This guy Hanneke managed to show is that in the mellow active learning setting, the number of examples you need.",
                    "label": 0
                },
                {
                    "sent": "Is data that Mistry parameter the disagreement coefficient times D times log one over epsilon and this is the same sort of exponential improvement that we saw earlier, OK?",
                    "label": 1
                },
                {
                    "sent": "And which is?",
                    "label": 0
                },
                {
                    "sent": "Which is really quite remarkable.",
                    "label": 0
                },
                {
                    "sent": "You know, there's a leading constant, but the convergence rate is exponential.",
                    "label": 0
                },
                {
                    "sent": "It says log one over epsilon for each additional thing you have, the half the error.",
                    "label": 0
                },
                {
                    "sent": "OK, for each additional step.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a quite a remarkable.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 1
                },
                {
                    "sent": "You know active learning analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so any questions about this?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is all for the separable case, but it will later emerge that you can actually come up with a version of this algorithm for the non separable case as well.",
                    "label": 1
                },
                {
                    "sent": "And even in that case this disagreement coefficient or some generalization of it still works and get you also get a bound that better than the supervised learning bound.",
                    "label": 0
                },
                {
                    "sent": "Big Ben class of functions only distribution are both.",
                    "label": 1
                },
                {
                    "sent": "Theater depends on the class of functions on the distribution.",
                    "label": 0
                },
                {
                    "sent": "An even on the target hypothesis.",
                    "label": 0
                },
                {
                    "sent": "But we'll see some examples of it in a second.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "Order more examples.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so right.",
                    "label": 0
                },
                {
                    "sent": "So the order does matter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sometimes you can get really lucky you know and you get things you get, you know you start with super informative points upfront.",
                    "label": 0
                },
                {
                    "sent": "You know the support vectors are the first 10 points and then and then instead of having.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of this bound had been log one over Theta, it's literally there's no dependence on epsilon, 'cause you have the perfect thing right up front, but this is a bound that holds regardless.",
                    "label": 0
                },
                {
                    "sent": "You know it's it's bound that holds with high probability over a random sample from the from the underlying distribution, but it's yeah, it's very true that the order does matter for this scheme.",
                    "label": 0
                },
                {
                    "sent": "So learning here is done.",
                    "label": 0
                },
                {
                    "sent": "Stop setting by using that you have a sample kerbox internal sample, no.",
                    "label": 0
                },
                {
                    "sent": "So if a cow it's not a transductive setting in the sense that you don't have access to the unlabeled points before hand, you just get one unlabeled point.",
                    "label": 0
                },
                {
                    "sent": "Other results, it seems like they are right, so there's a lot of results that are in a transductive setting, right?",
                    "label": 0
                },
                {
                    "sent": "So it's so it's unclear.",
                    "label": 0
                },
                {
                    "sent": "Pop that transfer you know what that buys you.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you know.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you wanted to run something like Al, but in advance you had enormous unlabeled set that give you a really good idea of the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "Would you change your strategy somewhat?",
                    "label": 0
                },
                {
                    "sent": "One would imagine you would, but it's not known.",
                    "label": 0
                },
                {
                    "sent": "You know they're not really very many results for that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Problem where collecting unlabeled data cost something for to be very small.",
                    "label": 0
                },
                {
                    "sent": "Having labor customer or what's the tradeoff?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a really interesting question, and it's something that you know that.",
                    "label": 0
                },
                {
                    "sent": "But something good, you know.",
                    "label": 0
                },
                {
                    "sent": "You know that the state of the art will hopefully one day get to the point of being able to address.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So let me OK.",
                    "label": 0
                },
                {
                    "sent": "So what does this disagree?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Demon coefficient ping an.",
                    "label": 0
                },
                {
                    "sent": "In order to do this, we'll have to get into a little bit of math over here.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "So we have some input SpaceX OK and we have some underlying probability distribution on it.",
                    "label": 1
                },
                {
                    "sent": "Nature is distribution, the distribution from which points are sampled.",
                    "label": 0
                },
                {
                    "sent": "This distribution lets us create a distance measure between hypothesis.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So suppose, for instance, that we are dealing with linear classifiers and this is the optimal classifier H star.",
                    "label": 0
                },
                {
                    "sent": "And let's say this is some other classifier.",
                    "label": 0
                },
                {
                    "sent": "The distance between those two one very good distance measure is simply the probability mass of their disagreement set.",
                    "label": 0
                },
                {
                    "sent": "The probability mass of the points on which they disagree.",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice distance measure on the space of functions, and it's something that plays a big role in active learning and also in other types of learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's.",
                    "label": 0
                },
                {
                    "sent": "Going to use the letter D for that.",
                    "label": 0
                },
                {
                    "sent": "Now, once you've defined this distance measure, you can talk about hypothesis that are close to the target.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say that this is our target.",
                    "label": 0
                },
                {
                    "sent": "This red hypothesis over here.",
                    "label": 0
                },
                {
                    "sent": "This red linear separator.",
                    "label": 0
                },
                {
                    "sent": "We now have a precise distance measure and so examples of things that are within distance are of the optimal.",
                    "label": 0
                },
                {
                    "sent": "Say these, these might be what they look like.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can talk about things that are within distance R of the optimal.",
                    "label": 0
                },
                {
                    "sent": "In other ones, things that disagree with the optimal on hypothesis on at most R fraction of the distribution under most are fraction of the data.",
                    "label": 0
                },
                {
                    "sent": "Now, once you when you're doing active learning, hopefully you're eventually at the point where your set of candidate hypothesis is fairly close to the optimal one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it looks something like this.",
                    "label": 1
                },
                {
                    "sent": "This set of candidate hypothesis includes the optimal.",
                    "label": 0
                },
                {
                    "sent": "It's also and stuff that's close to it.",
                    "label": 0
                },
                {
                    "sent": "In that case, the disagreement region would look something like this.",
                    "label": 1
                },
                {
                    "sent": "These are the points on which there is any disagreement at all within your within this candidate set.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the disagreement region.",
                    "label": 0
                },
                {
                    "sent": "For this point, there's no disagreement.",
                    "label": 0
                },
                {
                    "sent": "Everything thinks that it's things that it's on this side.",
                    "label": 0
                },
                {
                    "sent": "OK, so to be precise.",
                    "label": 0
                },
                {
                    "sent": "The R radius ball around the optimal hypothesis is simply all hypothesis that disagree with it on at most in our fraction of the data.",
                    "label": 0
                },
                {
                    "sent": "Things that are close to it.",
                    "label": 0
                },
                {
                    "sent": "And this disagreement set is simply the induced data point.",
                    "label": 0
                },
                {
                    "sent": "The data points on which there is going to be some disagreement in this set.",
                    "label": 0
                },
                {
                    "sent": "In other words, these are the points that Carol is going to query.",
                    "label": 1
                },
                {
                    "sent": "OK, and it's not going to query these other ones.",
                    "label": 0
                },
                {
                    "sent": "The disagreement coefficient measures how the probability of this region scales with R. OK, and this disagreement coefficient let's let's yeah.",
                    "label": 0
                },
                {
                    "sent": "There is no relation with the measurements but of condition.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "There is some sort of.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                },
                {
                    "sent": "There is actually very very very.",
                    "label": 0
                },
                {
                    "sent": "This is dimension without you Miss offers no run for this dimension is authority independent variable and this depends on so.",
                    "label": 0
                },
                {
                    "sent": "Cookies or something more codependent like computer?",
                    "label": 0
                },
                {
                    "sent": "Yeah there is.",
                    "label": 0
                },
                {
                    "sent": "There is definitely some very intriguing connection over here with these, so just to just to just to summarize, it's well known that in supervised learning, when you see N data points.",
                    "label": 0
                },
                {
                    "sent": "Your error rate scales with one over square root N. There's that one over square root and effect.",
                    "label": 0
                },
                {
                    "sent": "Recently there's been some work that goes under names like sebok of condition, and there's also been work on it by Kolchinsky which identify properties of the hypothesis class that can give you a better rate than one over square root N. And there's some intriguing connection between that particular property and the disagreement coefficient, and this is very much something that needs to be worked out.",
                    "label": 0
                },
                {
                    "sent": "Steve",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's disagreement coefficient is very general thing.",
                    "label": 1
                },
                {
                    "sent": "It becomes a little easier if we look at the separable case.",
                    "label": 0
                },
                {
                    "sent": "OK, so as we as we go through the active learning process candidates, it eventually gets very close to the target hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Let's say that it's error is at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so we get to a point where we are within the set of hypothesis whose error is less than epsilon.",
                    "label": 1
                },
                {
                    "sent": "We can now look at the disagreement region of those hypothesis.",
                    "label": 0
                },
                {
                    "sent": "In other words, the set of data points on which those hypothesis disagree at all, and the disagreement coefficient is simply the.",
                    "label": 0
                },
                {
                    "sent": "Probability mass of that disagree set.",
                    "label": 0
                },
                {
                    "sent": "It's simply proportional to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the disagree set.",
                    "label": 0
                },
                {
                    "sent": "Are the data points that we would actually query at this point.",
                    "label": 0
                },
                {
                    "sent": "And so we're interested in in how the size of that data set.",
                    "label": 0
                },
                {
                    "sent": "We hope that it's small.",
                    "label": 0
                },
                {
                    "sent": "We hope that once we get to a point where where.",
                    "label": 0
                },
                {
                    "sent": "You know, once we've sampled once, we've seen enough labels.",
                    "label": 0
                },
                {
                    "sent": "We hope that we are close enough to the target that that all our candidate hypothesis pretty much agree on most of the data.",
                    "label": 0
                },
                {
                    "sent": "The disagreement set is very small.",
                    "label": 0
                },
                {
                    "sent": "And this is what disagree.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And coefficient captures, so let's look at an example.",
                    "label": 0
                },
                {
                    "sent": "OK, just to be concrete, let's say once again that the hypothesis class consists of thresholds on the line.",
                    "label": 0
                },
                {
                    "sent": "OK, so the data space is just the real line, and each hypothesis looks like looks like a threshold on the line.",
                    "label": 0
                },
                {
                    "sent": "Now let's say this is the target hypothesis over here.",
                    "label": 0
                },
                {
                    "sent": "OK. What is the disagreement coefficient of this?",
                    "label": 1
                },
                {
                    "sent": "Well, the way you figure that out is, you say, let's look at pick any epsilon.",
                    "label": 1
                },
                {
                    "sent": "What are the hypothesis with error less than epsilon?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the target.",
                    "label": 0
                },
                {
                    "sent": "This has error zero.",
                    "label": 0
                },
                {
                    "sent": "This is error something this is error something the further you go away from this, the more the error increases.",
                    "label": 0
                },
                {
                    "sent": "If we go exactly epsilon probability mass this way.",
                    "label": 0
                },
                {
                    "sent": "This is these all have error less than or equal to epsilon and if we go exactly probability mass epsilon this way these all have error less than or equal to epsilon and so the disagree set consists of data points that lie in this zone and the total mass of this set is epsilon plus epsilon, it's two epsilon.",
                    "label": 1
                },
                {
                    "sent": "So the constant is to the ratio of the probability of this set.",
                    "label": 0
                },
                {
                    "sent": "Two epsilon is 2 and that's why the disagreement coefficient here is 2, yeah.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "Considering this super all epsilon correct so.",
                    "label": 0
                },
                {
                    "sent": "I assume that's because you want to do a worst case, but you want to get worse this browser.",
                    "label": 0
                },
                {
                    "sent": "As you're going through the process.",
                    "label": 0
                },
                {
                    "sent": "I mean the set up.",
                    "label": 0
                },
                {
                    "sent": "And so it seems like it might be possible to get it in terms of a different definition of these agreements proficient, but that goes with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Right, it turns out that.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know, this is a result from maybe four years ago or three years ago.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you know.",
                    "label": 0
                },
                {
                    "sent": "You know that there are now more refined results that actually look at the limsup as epsilon approaches 0.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it looks very similar to like posts on arrival rate when you have interval and you probabilities proportionally interval, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is, but this is something that now is supposed to hold for any hypothesis class.",
                    "label": 0
                },
                {
                    "sent": "And in more general classes, these disagreement regions would look kind of strange, you know, because for instance, in high dimensional spaces these are.",
                    "label": 0
                },
                {
                    "sent": "These are strange looking polytopes.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what are examples of disagreement coefficients and what sort of label complexity results do they yield?",
                    "label": 0
                },
                {
                    "sent": "So we've already seen this threshold on the real line no matter what the data distribution is, no matter what the target is, the disagreement coefficient is 2, which means that the label complexity is log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "For this class, as contrasted with one over epsilon for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Linear separators.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Indie dimensions.",
                    "label": 0
                },
                {
                    "sent": "For a very specific data distribution, the uniform data distribution.",
                    "label": 1
                },
                {
                    "sent": "It turns out that the disagreement coefficient is about square root T, which means the label complexity looks something like this.",
                    "label": 0
                },
                {
                    "sent": "D for the three halves.",
                    "label": 0
                },
                {
                    "sent": "Because it was tater times D times log one over epsilon looks something like this, which is still got this log one over epsilon rate that is very nice, but the uniform data distribution is not a realistic assumption.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any stretch of the imagination, and So what happens in more more general setting?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at.",
                    "label": 0
                },
                {
                    "sent": "Linear separators in RT and what you have is a smooth density bounded away from zero.",
                    "label": 1
                },
                {
                    "sent": "Then the disagreement coefficient is at most some constant that depends on the target times D and again, you get this exponential decrease OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is just on this on a compact.",
                    "label": 0
                },
                {
                    "sent": "OK. Do should we take a break or just keep going?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Something about this one here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have, so we will be seeing lower bounds fairly soon.",
                    "label": 0
                },
                {
                    "sent": "John John has got a bunch of lower bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just take a brief break and then we'll come back and see the non separable case.",
                    "label": 0
                }
            ]
        }
    }
}