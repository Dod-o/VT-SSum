{
    "id": "pq7uq4jglb7drzovgwohls6qq6nmk2xx",
    "title": "Online Dictionary Learning for Sparse Coding",
    "info": {
        "author": [
            "Julien Mairal, INRIA"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_mairal_odlsc/",
    "segmentation": [
        [
            "Good afternoon and thank you for introducing me.",
            "So we present you a new online algorithm for learning dictionaries, which are adapted, prospering, and this is a drunk with forces back wrong person.",
            "Guillermo Sergio."
        ],
        [
            "So what is the full balance?",
            "So usually when we consider starting problem, we often have some data.",
            "We have some basis set which we call dictionary and what we want is to find the spouses notation of your data onto this dictionary.",
            "An if this kind of representation exists, then it means that your dictionary is well adapted to the data.",
            "So for instance for images people have been using for your basis then wave lips and have been struggling for 20 years to find.",
            "The best dictionary, which is adapted to natural images and then there have been some other works which instead of trying to define these dictionaries by hand, have tried to learn it.",
            "And this is the kind of works that we follow today so.",
            "It turns out that."
        ],
        [
            "This dictionary learning problem can be formulated as a large scale matrix factorization problem.",
            "An Organism is intended to address this efficiently.",
            "So it turns also that if you are able to do this efficiently, then you can make some large scale image processing problems tractable and also algorithm extends to values matrix factorization problems such as non negative matrix factorization and the formulation of stuff PCA.",
            "So we start by describing the."
        ],
        [
            "Dictionary learning problem and before that I would like to come back to my first motivation, which was not at all a learning problem but an image processing task.",
            "That is, if you give me this noisy image, why?",
            "How can I recover the original image X so I can give you a very simple algorithm which can be written in three lines in this slide which provides almost state of the art results so.",
            "Key here is how it works.",
            "You consider your image.",
            "You extract all overlapping patches of a small size 4IN for instance."
        ],
        [
            "8 by 8 pixels so that you have a large number of patches.",
            "For instance, if you have a 1 megapixel image, you will have about 1 million of small budgets, so one path centered at every pixel.",
            "Then the idea is to find a dictionary which is adapted to this set of patches.",
            "So we learn the dictionary, which is a metric D here, which is composed of about a few number of elements, for instance two hundreds.",
            "And you want to solve them.",
            "These metrics acquisition problem.",
            "Each of your panties XI to admit an approximation.",
            "The Alpha I which is false.",
            "So the main point here will not be the size of XI, so we are not in the classical stop shooting setting where you have a single but large sparse coding problem you have.",
            "We have here a huge number of small sparse coding problem, so ethically greater than 100,000 could be 1,000,000.",
            "If I take a picture with my own digital camera.",
            "You have a 12 megapixel image and have about 12 millions of patches.",
            "Then after you have solved this problem for each Patch you obtain an approximation the Alpha I so that for each pixel you obtain as many estimates.",
            "As such is, it belongs to and so that you can just avoid these estimates and obtain the final reconstructed image.",
            "So here is."
        ],
        [
            "An example of what you can get with this kind of methods which are equal today to the state of the art in image processing for image denoising, at least so on the left you have the noise email on the right, so we saw the image, so there are other kind of applications for images, for instance."
        ],
        [
            "If you have some holes in your image, you have missing pixels.",
            "You can do image completion.",
            "So here what we have done.",
            "We have this image on the left which has a lot of missing pixels.",
            "We have learned the dictionary on the damaged image, knowing where the pixels are missing and which are the pixel which are not missing and we are able to reconstruct the image on the right where we are even able to recover some texture of the house which is even not visible on the left.",
            "So if you want."
        ],
        [
            "You know how the dictionary looks like.",
            "Here is an example of a dictionary learned on the noisy image.",
            "It has here 256 elements was learning the data on patches of size 8 by 8 and I think there is not much more to say about this."
        ],
        [
            "So let us come back now to the formulation of dictionary learning that we have presented and give a little more detail.",
            "So I already presented the the time that you see on the top.",
            "We just have to add some regularization to D because if you look at the first sign, it's invariant by seemingly by the seller by saving alphabets inverse, which means that they could be arbitrarily large if we don't regularize it.",
            "That's what people usually do is just that transforms train the L2 norm of the columns of D to be less than one.",
            "So now we have here on nonconvex optimization problem, But if you fix Alpha and your team is respected D, you obtain.",
            "The convex optimization problem, and if you fix the, you obtain a large number of small convex optimization problem that you can solve.",
            "So classical optimization just alternates between the optimization on D and the optimization of Alpha, so it is already very good results.",
            "But if there is slow and this is the point that we are addressing here.",
            "Anne."
        ],
        [
            "This is why you are coming to online."
        ],
        [
            "Techniques, so first of all we will rewrite this as a learning problem, so we have some training set dispatches XI of signals XI we have some parameters D which we want to learn and we have a loss function L which is the solution of the lesson.",
            "So, uh, what we want to do if you have a final training set is to optimize this empirical costs.",
            "So now here we are living with images where the cost of having data is very cheap.",
            "Like I said before, if I take a picture with a digital camera, I can have 12 million patches, so we can consider the case where N is the reluctant even grows to Infinity.",
            "If you have like if we want to learn a dictionary and a database of natural images, you can have a huge number of patches.",
            "So what we?"
        ],
        [
            "Night won't work in eyes.",
            "Instead of this empirical passes directly to optimize the expected costs.",
            "And this is something which can be otherwise using online techniques.",
            "So indeed."
        ],
        [
            "Online learning can handle potentially infinite data sets.",
            "It can also adapt to dynamic training sets.",
            "For instance, if you have a video stream, you might not have the future of data.",
            "You might want to optimize dynamically yourdictionary.",
            "An what is not.",
            "It's also in this case dramatically faster than batch algorithm.",
            "So I cited the boat when boosted for that also many other researchers who are working on this field.",
            "So here is our."
        ],
        [
            "Watch what we do is that we iterate we have will iterate with a large number of iteration.",
            "At each iteration the basic algorithm will grow one single element of the training set, which we right here XD.",
            "Then which we propose is each time we draw a new element we compute the sparse coding program.",
            "Computing this Alpha key an using the current dictionary, which is here D, T -- 1 and then we update D. By optimizing the following position problem.",
            "So here the.",
            "Two things to make this efficient.",
            "So first of all, you have to be able to deal with the sparse coding step efficiently.",
            "So here like I said, we have small sparse coding problem with very high sparsity.",
            "Like if X has like of size 100 may be helpful.",
            "We can have 10 or 15 non zero elements.",
            "So in this kind of situation it happens that last is performing very well and this is what we have used for the dictionary learning set.",
            "It's a bit more tricky because it could look like a big optimization problem that we have to solve at each iteration.",
            "But let us suppose one month that we adopt the block coordinate approach where we fix all the columns of the but one.",
            "Then we can have a close form for updating Dee Dee Ann.",
            "It turns out that this each optimization of the column of D is very cheap.",
            "So now we know how to.",
            "Deal with this algorithm efficiently.",
            "So why does it work?",
            "So the function that you we are making populating the turns out to be a surrogate of the expected costs, and we'll see later which guarantees that we have."
        ],
        [
            "So few implementation details.",
            "Like I said, for the suffering step, you can use lost an for the dictionary update.",
            "You use the block coordinate approach for the one we start because this function that we update that we minimize to obtain DT is very close to the one of the previous iteration, so that the solution DT is very close to the T -- 1 which makes it efficient to use the T -- 1 as one we sound and then there is also something we have observed.",
            "We can use a mini batch instead of doing a single element of the training set.",
            "We can make a small variation and grow like 50 elements of the training set at each iteration.",
            "This improved performance in practice."
        ],
        [
            "So now the guarantees.",
            "Like I said, the function that we optimize when we update D, which we call here F hot key.",
            "Verifies this Turkey so F had C of the T -- F of DT, where F is the expected cost which we want to minimize.",
            "And two zero when T tends to Infinity with probability one and then the second guarantees that if we consider all the stationary point of the optimization problem that we have at the beginning DT, the distance between this set and ET will tend to zero.",
            "20 tends to Infinity and we will basically will be happy with this guarantees.",
            "So now I'm coming to the external part, so.",
            "For those who are familiar with the kind of thoughts that long, but you can give would be familiar with this curse as well.",
            "So we apply the Bash algorithm, which iterates which alternate the optimization between Alpha Andy with different sizes of training sets, and we we have provided the objective function evaluated on the test set which was not used for training, and we have put it so these function as a function of the time.",
            "Of training on the logarithmic scale.",
            "So if you use 10 thousands of questions you have the curves with the Seattle's, which are simple technically is not good enough because we don't have enough training data."
        ],
        [
            "Then it becomes better when you have data, but it becomes also quite slow and queues were method.",
            "We see that the blue curves is a lot better.",
            "So here is under sitting with bigger patches.",
            "So in the previous slide.",
            "We had eight 8 batches and there were 256 elements in the dictionary.",
            "If you change the setting a bit, let's say we have 12 by 12 color Patch."
        ],
        [
            "Cheese with five twelve element in the dictionary.",
            "Then you obtain this curve with even more difference between the batch."
        ],
        [
            "Sitting in the online setting and if you go even further with 16 by 16 batches and even more element in the dictionary then the batch method becomes almost impractical when you have a large training set."
        ],
        [
            "So now we can ask why not used LP, stochastic model design and the answer is that you can.",
            "It's also works quite well.",
            "Even so you have to tune learning rates that we don't have to do with your methods.",
            "So on the first data set we have optimized the learning rate for stochastic gradient dissent on the training set and here is what we observe on the test set.",
            "We have some similar behave."
        ],
        [
            "Yeah, so method for the other data sets we have small.",
            "Safety better I seem to achieve behavioral on the last data set, which is even more difficult."
        ],
        [
            "Even more, there is the difference is even bigger.",
            "So now I told you that we could solve some large scale Metro system problem because."
        ],
        [
            "With this kind of method we have been working very often with, you know, maybe all these small image processing images which are typically of size 512 * 512.",
            "Here we are working with a 12 megapixel image and we have around the dictionary on this image and the goal will be drawing the text."
        ],
        [
            "And this is what you can get in about 5 minutes of computation."
        ],
        [
            "OK, to close up you have some texts on."
        ],
        [
            "Image and.",
            "On the re stored image, the text is hardly visible.",
            "Maybe you can maybe find a small artifacts that hardly visit, so now."
        ],
        [
            "I told you also that there were some extensions."
        ],
        [
            "Permitted, which are here a bit.",
            "And they will be published soon so.",
            "We can add up committed to the form of non negative matrix vector is non non negative matrix factorization because it's very easy to incorporate positivity constraint on Alpha.",
            "But it's also easy to add this study.",
            "We have also let us say for instance that we want or dictionary to have some sparsity which can be useful for some application.",
            "So one idea we can use is that will replace the constraints that we had on D by the constraint which will.",
            "In use a city, so we propose here to use a mix between the L2 and one regularization using the set C prime and using some kind of elastic net constraints.",
            "So here is not."
        ],
        [
            "Just some images to show you how you can get, so we have applied this data to a database of faces an so the first image shows what you get using PCA.",
            "So the the positive values are right and the negative values are blue.",
            "So here we have a dictionary which contains maybe 49 elements.",
            "An when you use NMF non negative matrix factorization we have these classical images which show that there is more locality in the features that you can learn on their faces when you use the classical dictionary learning method you obtain the image on the right.",
            "And what is interesting is that using one new constraint which induce sparsity, indeed, we can obtain this scene of images where we can."
        ],
        [
            "Show the Society of the dictionary.",
            "So from the left we have 70% of pixels which are nonzero, and on the right we have only 10%.",
            "So what do we have when we apply all these techniques on patches from natural images?",
            "So the first image on the left is CCA.",
            "The Mail."
        ],
        [
            "On the middle is nonnegative matrix factorization an.",
            "So maybe in this sale there is.",
            "It's difficult to comment on the structure alone by NMF.",
            "Maybe on PC we can first see that we learn that natural patches have more vertical and horizontal structures, and This is why the first 2 components are basically this.",
            "The dictionary learning shows some structures which.",
            "Maybe look like like wavelets on teacher improve the subsidy to increase the safety of the dictionary using the extension.",
            "Then you can obtain."
        ],
        [
            "This image is again is 70% of pixels which are not there on the left and with only 10% on the right.",
            "So now my take."
        ],
        [
            "Message of all of this is that online techniques in general order to the dictionary learning problem that's using your method.",
            "You can make some image processing task tractable using big images.",
            "And also this method extends to values matrix factorization problems.",
            "So thank you for your attention."
        ],
        [
            "Yes, good point.",
            "Your online method explain the principle by which updating we take time to growth with T. Yes, so the question is if we come back just to the method.",
            "The question is why the date of D doesn't grow with C, so the the actually.",
            "Musician from here you can develop all of these least square, adopting looking initial point which will be written the trace of the console Z times metrics A minus the trace of the transpose B.",
            "So we just have to update the metrics A and the metrics be so that it doesn't grow with T. Just wondering, after learning the dictionary new different language, yes, so for English the noises.",
            "When you learn the diction and image, what happens when I does it work?",
            "When I use it on the new English?",
            "So for image analysing, there are two approaches.",
            "You can learn the dictionary in the database of natural images and use it as a dictionary to the noisy image and this works OK when you use the dictionary which is learned on the noisy image itself, you improve.",
            "So basically show images.",
            "Has never ability.",
            "You can learn a dictionary with, which will be generic enough to be good too for some other images.",
            "If you have an image which has maybe only a single pattern, then it might be very bad for the other images.",
            "Yeah.",
            "Yes, so a lot of people are used stochastic gradient issues.",
            "Yes.",
            "So the question is when the Kurds who showed the difference wasn't that great in the statistical mention you comment on the on the sort of practical consequences differences, so actually so here in the data we argue that we don't have any power meter soon.",
            "Actually we have the size that we touch, but it was also pretty California for this location.",
            "Georgia Open it up.",
            "So for this project.",
            "And the end.",
            "The size of the universe.",
            "So we have.",
            "Now we re say this, the pastor that aren't at the parameter in your method which is not on this site which gives better results and there is a.",
            "Thinking of you folks dependent stochastic automated one or metal.",
            "Some people use your secondary approximation.",
            "Is the option to figure out the stateside this yes, so so.",
            "Yes, maybe maybe it was the basic version of the constituent method.",
            "You could.",
            "We could also imagine to have some adaptive learning right?",
            "So we didn't go into that an but we could do the same for method 2.",
            "The first data and doing some kind of learning rates.",
            "We could also, but I don't know exactly how to do this.",
            "We can do the same in the elastic network structures.",
            "Yes, here we go.",
            "So you said that stochastic gradient has the learning rate to tune, but what about the regularization parameter in this one?",
            "Yes, so there there is two kind of parameters.",
            "Are the parameters for the optimization and the parameters of the model.",
            "So here I was talking only about the parameter of the optimization.",
            "So yes, this model has the parameter for the conferring with sparsity Ann for dealing with that.",
            "He does do cross validation.",
            "OK, we have to stop acting OK.",
            "Happy OK. OK. OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon and thank you for introducing me.",
                    "label": 0
                },
                {
                    "sent": "So we present you a new online algorithm for learning dictionaries, which are adapted, prospering, and this is a drunk with forces back wrong person.",
                    "label": 0
                },
                {
                    "sent": "Guillermo Sergio.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the full balance?",
                    "label": 0
                },
                {
                    "sent": "So usually when we consider starting problem, we often have some data.",
                    "label": 0
                },
                {
                    "sent": "We have some basis set which we call dictionary and what we want is to find the spouses notation of your data onto this dictionary.",
                    "label": 0
                },
                {
                    "sent": "An if this kind of representation exists, then it means that your dictionary is well adapted to the data.",
                    "label": 0
                },
                {
                    "sent": "So for instance for images people have been using for your basis then wave lips and have been struggling for 20 years to find.",
                    "label": 0
                },
                {
                    "sent": "The best dictionary, which is adapted to natural images and then there have been some other works which instead of trying to define these dictionaries by hand, have tried to learn it.",
                    "label": 0
                },
                {
                    "sent": "And this is the kind of works that we follow today so.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This dictionary learning problem can be formulated as a large scale matrix factorization problem.",
                    "label": 1
                },
                {
                    "sent": "An Organism is intended to address this efficiently.",
                    "label": 0
                },
                {
                    "sent": "So it turns also that if you are able to do this efficiently, then you can make some large scale image processing problems tractable and also algorithm extends to values matrix factorization problems such as non negative matrix factorization and the formulation of stuff PCA.",
                    "label": 0
                },
                {
                    "sent": "So we start by describing the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dictionary learning problem and before that I would like to come back to my first motivation, which was not at all a learning problem but an image processing task.",
                    "label": 1
                },
                {
                    "sent": "That is, if you give me this noisy image, why?",
                    "label": 0
                },
                {
                    "sent": "How can I recover the original image X so I can give you a very simple algorithm which can be written in three lines in this slide which provides almost state of the art results so.",
                    "label": 0
                },
                {
                    "sent": "Key here is how it works.",
                    "label": 0
                },
                {
                    "sent": "You consider your image.",
                    "label": 0
                },
                {
                    "sent": "You extract all overlapping patches of a small size 4IN for instance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "8 by 8 pixels so that you have a large number of patches.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have a 1 megapixel image, you will have about 1 million of small budgets, so one path centered at every pixel.",
                    "label": 0
                },
                {
                    "sent": "Then the idea is to find a dictionary which is adapted to this set of patches.",
                    "label": 0
                },
                {
                    "sent": "So we learn the dictionary, which is a metric D here, which is composed of about a few number of elements, for instance two hundreds.",
                    "label": 1
                },
                {
                    "sent": "And you want to solve them.",
                    "label": 0
                },
                {
                    "sent": "These metrics acquisition problem.",
                    "label": 0
                },
                {
                    "sent": "Each of your panties XI to admit an approximation.",
                    "label": 0
                },
                {
                    "sent": "The Alpha I which is false.",
                    "label": 0
                },
                {
                    "sent": "So the main point here will not be the size of XI, so we are not in the classical stop shooting setting where you have a single but large sparse coding problem you have.",
                    "label": 0
                },
                {
                    "sent": "We have here a huge number of small sparse coding problem, so ethically greater than 100,000 could be 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "If I take a picture with my own digital camera.",
                    "label": 0
                },
                {
                    "sent": "You have a 12 megapixel image and have about 12 millions of patches.",
                    "label": 0
                },
                {
                    "sent": "Then after you have solved this problem for each Patch you obtain an approximation the Alpha I so that for each pixel you obtain as many estimates.",
                    "label": 0
                },
                {
                    "sent": "As such is, it belongs to and so that you can just avoid these estimates and obtain the final reconstructed image.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example of what you can get with this kind of methods which are equal today to the state of the art in image processing for image denoising, at least so on the left you have the noise email on the right, so we saw the image, so there are other kind of applications for images, for instance.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have some holes in your image, you have missing pixels.",
                    "label": 0
                },
                {
                    "sent": "You can do image completion.",
                    "label": 0
                },
                {
                    "sent": "So here what we have done.",
                    "label": 0
                },
                {
                    "sent": "We have this image on the left which has a lot of missing pixels.",
                    "label": 0
                },
                {
                    "sent": "We have learned the dictionary on the damaged image, knowing where the pixels are missing and which are the pixel which are not missing and we are able to reconstruct the image on the right where we are even able to recover some texture of the house which is even not visible on the left.",
                    "label": 0
                },
                {
                    "sent": "So if you want.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know how the dictionary looks like.",
                    "label": 1
                },
                {
                    "sent": "Here is an example of a dictionary learned on the noisy image.",
                    "label": 0
                },
                {
                    "sent": "It has here 256 elements was learning the data on patches of size 8 by 8 and I think there is not much more to say about this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us come back now to the formulation of dictionary learning that we have presented and give a little more detail.",
                    "label": 1
                },
                {
                    "sent": "So I already presented the the time that you see on the top.",
                    "label": 0
                },
                {
                    "sent": "We just have to add some regularization to D because if you look at the first sign, it's invariant by seemingly by the seller by saving alphabets inverse, which means that they could be arbitrarily large if we don't regularize it.",
                    "label": 0
                },
                {
                    "sent": "That's what people usually do is just that transforms train the L2 norm of the columns of D to be less than one.",
                    "label": 0
                },
                {
                    "sent": "So now we have here on nonconvex optimization problem, But if you fix Alpha and your team is respected D, you obtain.",
                    "label": 0
                },
                {
                    "sent": "The convex optimization problem, and if you fix the, you obtain a large number of small convex optimization problem that you can solve.",
                    "label": 0
                },
                {
                    "sent": "So classical optimization just alternates between the optimization on D and the optimization of Alpha, so it is already very good results.",
                    "label": 0
                },
                {
                    "sent": "But if there is slow and this is the point that we are addressing here.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is why you are coming to online.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Techniques, so first of all we will rewrite this as a learning problem, so we have some training set dispatches XI of signals XI we have some parameters D which we want to learn and we have a loss function L which is the solution of the lesson.",
                    "label": 0
                },
                {
                    "sent": "So, uh, what we want to do if you have a final training set is to optimize this empirical costs.",
                    "label": 0
                },
                {
                    "sent": "So now here we are living with images where the cost of having data is very cheap.",
                    "label": 0
                },
                {
                    "sent": "Like I said before, if I take a picture with a digital camera, I can have 12 million patches, so we can consider the case where N is the reluctant even grows to Infinity.",
                    "label": 0
                },
                {
                    "sent": "If you have like if we want to learn a dictionary and a database of natural images, you can have a huge number of patches.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Night won't work in eyes.",
                    "label": 0
                },
                {
                    "sent": "Instead of this empirical passes directly to optimize the expected costs.",
                    "label": 0
                },
                {
                    "sent": "And this is something which can be otherwise using online techniques.",
                    "label": 0
                },
                {
                    "sent": "So indeed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online learning can handle potentially infinite data sets.",
                    "label": 0
                },
                {
                    "sent": "It can also adapt to dynamic training sets.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have a video stream, you might not have the future of data.",
                    "label": 0
                },
                {
                    "sent": "You might want to optimize dynamically yourdictionary.",
                    "label": 0
                },
                {
                    "sent": "An what is not.",
                    "label": 0
                },
                {
                    "sent": "It's also in this case dramatically faster than batch algorithm.",
                    "label": 0
                },
                {
                    "sent": "So I cited the boat when boosted for that also many other researchers who are working on this field.",
                    "label": 0
                },
                {
                    "sent": "So here is our.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Watch what we do is that we iterate we have will iterate with a large number of iteration.",
                    "label": 0
                },
                {
                    "sent": "At each iteration the basic algorithm will grow one single element of the training set, which we right here XD.",
                    "label": 0
                },
                {
                    "sent": "Then which we propose is each time we draw a new element we compute the sparse coding program.",
                    "label": 0
                },
                {
                    "sent": "Computing this Alpha key an using the current dictionary, which is here D, T -- 1 and then we update D. By optimizing the following position problem.",
                    "label": 0
                },
                {
                    "sent": "So here the.",
                    "label": 0
                },
                {
                    "sent": "Two things to make this efficient.",
                    "label": 0
                },
                {
                    "sent": "So first of all, you have to be able to deal with the sparse coding step efficiently.",
                    "label": 0
                },
                {
                    "sent": "So here like I said, we have small sparse coding problem with very high sparsity.",
                    "label": 0
                },
                {
                    "sent": "Like if X has like of size 100 may be helpful.",
                    "label": 0
                },
                {
                    "sent": "We can have 10 or 15 non zero elements.",
                    "label": 0
                },
                {
                    "sent": "So in this kind of situation it happens that last is performing very well and this is what we have used for the dictionary learning set.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more tricky because it could look like a big optimization problem that we have to solve at each iteration.",
                    "label": 0
                },
                {
                    "sent": "But let us suppose one month that we adopt the block coordinate approach where we fix all the columns of the but one.",
                    "label": 0
                },
                {
                    "sent": "Then we can have a close form for updating Dee Dee Ann.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this each optimization of the column of D is very cheap.",
                    "label": 0
                },
                {
                    "sent": "So now we know how to.",
                    "label": 0
                },
                {
                    "sent": "Deal with this algorithm efficiently.",
                    "label": 0
                },
                {
                    "sent": "So why does it work?",
                    "label": 0
                },
                {
                    "sent": "So the function that you we are making populating the turns out to be a surrogate of the expected costs, and we'll see later which guarantees that we have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So few implementation details.",
                    "label": 0
                },
                {
                    "sent": "Like I said, for the suffering step, you can use lost an for the dictionary update.",
                    "label": 0
                },
                {
                    "sent": "You use the block coordinate approach for the one we start because this function that we update that we minimize to obtain DT is very close to the one of the previous iteration, so that the solution DT is very close to the T -- 1 which makes it efficient to use the T -- 1 as one we sound and then there is also something we have observed.",
                    "label": 0
                },
                {
                    "sent": "We can use a mini batch instead of doing a single element of the training set.",
                    "label": 0
                },
                {
                    "sent": "We can make a small variation and grow like 50 elements of the training set at each iteration.",
                    "label": 0
                },
                {
                    "sent": "This improved performance in practice.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the guarantees.",
                    "label": 0
                },
                {
                    "sent": "Like I said, the function that we optimize when we update D, which we call here F hot key.",
                    "label": 0
                },
                {
                    "sent": "Verifies this Turkey so F had C of the T -- F of DT, where F is the expected cost which we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "And two zero when T tends to Infinity with probability one and then the second guarantees that if we consider all the stationary point of the optimization problem that we have at the beginning DT, the distance between this set and ET will tend to zero.",
                    "label": 0
                },
                {
                    "sent": "20 tends to Infinity and we will basically will be happy with this guarantees.",
                    "label": 0
                },
                {
                    "sent": "So now I'm coming to the external part, so.",
                    "label": 0
                },
                {
                    "sent": "For those who are familiar with the kind of thoughts that long, but you can give would be familiar with this curse as well.",
                    "label": 0
                },
                {
                    "sent": "So we apply the Bash algorithm, which iterates which alternate the optimization between Alpha Andy with different sizes of training sets, and we we have provided the objective function evaluated on the test set which was not used for training, and we have put it so these function as a function of the time.",
                    "label": 0
                },
                {
                    "sent": "Of training on the logarithmic scale.",
                    "label": 0
                },
                {
                    "sent": "So if you use 10 thousands of questions you have the curves with the Seattle's, which are simple technically is not good enough because we don't have enough training data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then it becomes better when you have data, but it becomes also quite slow and queues were method.",
                    "label": 0
                },
                {
                    "sent": "We see that the blue curves is a lot better.",
                    "label": 0
                },
                {
                    "sent": "So here is under sitting with bigger patches.",
                    "label": 0
                },
                {
                    "sent": "So in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "We had eight 8 batches and there were 256 elements in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "If you change the setting a bit, let's say we have 12 by 12 color Patch.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cheese with five twelve element in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Then you obtain this curve with even more difference between the batch.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sitting in the online setting and if you go even further with 16 by 16 batches and even more element in the dictionary then the batch method becomes almost impractical when you have a large training set.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can ask why not used LP, stochastic model design and the answer is that you can.",
                    "label": 0
                },
                {
                    "sent": "It's also works quite well.",
                    "label": 0
                },
                {
                    "sent": "Even so you have to tune learning rates that we don't have to do with your methods.",
                    "label": 0
                },
                {
                    "sent": "So on the first data set we have optimized the learning rate for stochastic gradient dissent on the training set and here is what we observe on the test set.",
                    "label": 0
                },
                {
                    "sent": "We have some similar behave.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so method for the other data sets we have small.",
                    "label": 0
                },
                {
                    "sent": "Safety better I seem to achieve behavioral on the last data set, which is even more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even more, there is the difference is even bigger.",
                    "label": 0
                },
                {
                    "sent": "So now I told you that we could solve some large scale Metro system problem because.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this kind of method we have been working very often with, you know, maybe all these small image processing images which are typically of size 512 * 512.",
                    "label": 0
                },
                {
                    "sent": "Here we are working with a 12 megapixel image and we have around the dictionary on this image and the goal will be drawing the text.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what you can get in about 5 minutes of computation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, to close up you have some texts on.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image and.",
                    "label": 0
                },
                {
                    "sent": "On the re stored image, the text is hardly visible.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can maybe find a small artifacts that hardly visit, so now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I told you also that there were some extensions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Permitted, which are here a bit.",
                    "label": 0
                },
                {
                    "sent": "And they will be published soon so.",
                    "label": 0
                },
                {
                    "sent": "We can add up committed to the form of non negative matrix vector is non non negative matrix factorization because it's very easy to incorporate positivity constraint on Alpha.",
                    "label": 0
                },
                {
                    "sent": "But it's also easy to add this study.",
                    "label": 0
                },
                {
                    "sent": "We have also let us say for instance that we want or dictionary to have some sparsity which can be useful for some application.",
                    "label": 0
                },
                {
                    "sent": "So one idea we can use is that will replace the constraints that we had on D by the constraint which will.",
                    "label": 0
                },
                {
                    "sent": "In use a city, so we propose here to use a mix between the L2 and one regularization using the set C prime and using some kind of elastic net constraints.",
                    "label": 0
                },
                {
                    "sent": "So here is not.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some images to show you how you can get, so we have applied this data to a database of faces an so the first image shows what you get using PCA.",
                    "label": 0
                },
                {
                    "sent": "So the the positive values are right and the negative values are blue.",
                    "label": 0
                },
                {
                    "sent": "So here we have a dictionary which contains maybe 49 elements.",
                    "label": 0
                },
                {
                    "sent": "An when you use NMF non negative matrix factorization we have these classical images which show that there is more locality in the features that you can learn on their faces when you use the classical dictionary learning method you obtain the image on the right.",
                    "label": 0
                },
                {
                    "sent": "And what is interesting is that using one new constraint which induce sparsity, indeed, we can obtain this scene of images where we can.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show the Society of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So from the left we have 70% of pixels which are nonzero, and on the right we have only 10%.",
                    "label": 0
                },
                {
                    "sent": "So what do we have when we apply all these techniques on patches from natural images?",
                    "label": 0
                },
                {
                    "sent": "So the first image on the left is CCA.",
                    "label": 0
                },
                {
                    "sent": "The Mail.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the middle is nonnegative matrix factorization an.",
                    "label": 0
                },
                {
                    "sent": "So maybe in this sale there is.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to comment on the structure alone by NMF.",
                    "label": 0
                },
                {
                    "sent": "Maybe on PC we can first see that we learn that natural patches have more vertical and horizontal structures, and This is why the first 2 components are basically this.",
                    "label": 0
                },
                {
                    "sent": "The dictionary learning shows some structures which.",
                    "label": 0
                },
                {
                    "sent": "Maybe look like like wavelets on teacher improve the subsidy to increase the safety of the dictionary using the extension.",
                    "label": 0
                },
                {
                    "sent": "Then you can obtain.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This image is again is 70% of pixels which are not there on the left and with only 10% on the right.",
                    "label": 0
                },
                {
                    "sent": "So now my take.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Message of all of this is that online techniques in general order to the dictionary learning problem that's using your method.",
                    "label": 0
                },
                {
                    "sent": "You can make some image processing task tractable using big images.",
                    "label": 0
                },
                {
                    "sent": "And also this method extends to values matrix factorization problems.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, good point.",
                    "label": 0
                },
                {
                    "sent": "Your online method explain the principle by which updating we take time to growth with T. Yes, so the question is if we come back just to the method.",
                    "label": 0
                },
                {
                    "sent": "The question is why the date of D doesn't grow with C, so the the actually.",
                    "label": 0
                },
                {
                    "sent": "Musician from here you can develop all of these least square, adopting looking initial point which will be written the trace of the console Z times metrics A minus the trace of the transpose B.",
                    "label": 0
                },
                {
                    "sent": "So we just have to update the metrics A and the metrics be so that it doesn't grow with T. Just wondering, after learning the dictionary new different language, yes, so for English the noises.",
                    "label": 0
                },
                {
                    "sent": "When you learn the diction and image, what happens when I does it work?",
                    "label": 0
                },
                {
                    "sent": "When I use it on the new English?",
                    "label": 0
                },
                {
                    "sent": "So for image analysing, there are two approaches.",
                    "label": 0
                },
                {
                    "sent": "You can learn the dictionary in the database of natural images and use it as a dictionary to the noisy image and this works OK when you use the dictionary which is learned on the noisy image itself, you improve.",
                    "label": 1
                },
                {
                    "sent": "So basically show images.",
                    "label": 0
                },
                {
                    "sent": "Has never ability.",
                    "label": 0
                },
                {
                    "sent": "You can learn a dictionary with, which will be generic enough to be good too for some other images.",
                    "label": 0
                },
                {
                    "sent": "If you have an image which has maybe only a single pattern, then it might be very bad for the other images.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, so a lot of people are used stochastic gradient issues.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So the question is when the Kurds who showed the difference wasn't that great in the statistical mention you comment on the on the sort of practical consequences differences, so actually so here in the data we argue that we don't have any power meter soon.",
                    "label": 0
                },
                {
                    "sent": "Actually we have the size that we touch, but it was also pretty California for this location.",
                    "label": 0
                },
                {
                    "sent": "Georgia Open it up.",
                    "label": 0
                },
                {
                    "sent": "So for this project.",
                    "label": 0
                },
                {
                    "sent": "And the end.",
                    "label": 0
                },
                {
                    "sent": "The size of the universe.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Now we re say this, the pastor that aren't at the parameter in your method which is not on this site which gives better results and there is a.",
                    "label": 0
                },
                {
                    "sent": "Thinking of you folks dependent stochastic automated one or metal.",
                    "label": 0
                },
                {
                    "sent": "Some people use your secondary approximation.",
                    "label": 0
                },
                {
                    "sent": "Is the option to figure out the stateside this yes, so so.",
                    "label": 0
                },
                {
                    "sent": "Yes, maybe maybe it was the basic version of the constituent method.",
                    "label": 0
                },
                {
                    "sent": "You could.",
                    "label": 0
                },
                {
                    "sent": "We could also imagine to have some adaptive learning right?",
                    "label": 0
                },
                {
                    "sent": "So we didn't go into that an but we could do the same for method 2.",
                    "label": 0
                },
                {
                    "sent": "The first data and doing some kind of learning rates.",
                    "label": 0
                },
                {
                    "sent": "We could also, but I don't know exactly how to do this.",
                    "label": 0
                },
                {
                    "sent": "We can do the same in the elastic network structures.",
                    "label": 0
                },
                {
                    "sent": "Yes, here we go.",
                    "label": 0
                },
                {
                    "sent": "So you said that stochastic gradient has the learning rate to tune, but what about the regularization parameter in this one?",
                    "label": 0
                },
                {
                    "sent": "Yes, so there there is two kind of parameters.",
                    "label": 0
                },
                {
                    "sent": "Are the parameters for the optimization and the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So here I was talking only about the parameter of the optimization.",
                    "label": 0
                },
                {
                    "sent": "So yes, this model has the parameter for the conferring with sparsity Ann for dealing with that.",
                    "label": 0
                },
                {
                    "sent": "He does do cross validation.",
                    "label": 0
                },
                {
                    "sent": "OK, we have to stop acting OK.",
                    "label": 0
                },
                {
                    "sent": "Happy OK. OK. OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}