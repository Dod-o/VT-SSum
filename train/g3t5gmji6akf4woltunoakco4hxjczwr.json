{
    "id": "g3t5gmji6akf4woltunoakco4hxjczwr",
    "title": "The Asymptotics of Semi-Supervised Learning in Discriminative Probabilistic Models",
    "info": {
        "author": [
            "Olivier Capp\u00e9, +LTCI, TELECOM ParisTech and CNRS"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_cappe_ass/",
    "segmentation": [
        [
            "I'm leaving capital will be presenting the this paper which is a joint work between and made myself Natalia so closely with.",
            "He ran, our colleague Francois evolves from University price sheet so there are various ways in which this work can be presented probably, and I'm presenting it from a statistical perspective which is more my background.",
            "So because there's also the poster session, you get an opportunity to have it presented.",
            "A bit differently at the poster so I can go to the poster as well if you're interested by the talks."
        ],
        [
            "So in the talk, I will insist perhaps more than in the paper on the.",
            "This idea of stratified sampling, which is a principle which is known in statistiken which forms the basis of the method that we are describing here.",
            "So the method is about semi supervised classification and we tried to answer the following question which in fact we could not really answer.",
            "So I will come to that later but.",
            "Our question was if we consider discriminative probabilistic model, is it possible to say?",
            "What should be the correct way or off estimating the parameters of such a model?",
            "If you have some information brought by unlabeled data?",
            "OK, is there an optimal way of doing this or is there a way which satisfies some requirement that I'm going to talk about so we don't have the answer to this question because we have the answer only in any case which is a bit more restricted than that, but at least in a case which is a bit more restricted in that we have the answer so we know what's an optimal method.",
            "In some sense an.",
            "What what are is its performances.",
            "I have to say also that this talk is a very related to the talk by parcel Young and in the few been yesterday to the word session.",
            "So there are a lot of things that if you attended this talk you will recognize.",
            "Also in particular is sure he showed an expression which is very important here and so we'll see where it's important for semi supervised."
        ],
        [
            "So this is a semi supervised, so since he's been the topic of this session, maybe it's not that useful to explain exactly what it is, so we have some training data an we'd like to improve.",
            "I think that this work.",
            "This word is important.",
            "Would like to improve what we can get from training data by some information on the.",
            "On what exactly?",
            "So I think that the information that we have information on the marginal distribution of the features so feature is the word that I use for the data.",
            "So in semi supervised estimation we get some additional information that we don't have only.",
            "Levels of data with levels.",
            "We have some additional information and it's additional information on the marginal distribution on the feature.",
            "So the question is how to do something with this additional information.",
            "So it's very important, of course, because in many learning application this information is really.",
            "Can be really precise, so it's not unlikely that for instance you have 100 or 1000 labeled data, but perhaps 10,000 unlabeled data, so in some sense you can say that this additional information that you have, maybe it's not that useful, but at least it can be rather precise in many application.",
            "And it's also the reason why I think it's different from something that is much studied in statistics, which is the context of missing data, which would more be the case.",
            "If you add some training samples but not all of them, a few of them where there something missing.",
            "Perhaps the label, or perhaps some attributes of the data would be missing, so that would be typical case of what people call missing data, but I don't think that it's exactly the same setting in machine learning, because in machine learning you have the training data and you have this additional information which you may or may not use.",
            "And also it's since it's based on a large number of unlabeled data, it can be.",
            "In fact, very precise."
        ],
        [
            "So I'm not going to really discuss very precisely what the literature says about this problem.",
            "I just want to say one of the reasons that I can do this is because many of the methods that have been proposed so far they are very dependent on the of the type of classifier that you're using.",
            "So, for instance, if you're using SVM, you can use one method.",
            "If you in this case will be focusing on discriminative probabilistic model.",
            "So you have some other method that are.",
            "Useful in this case, so there are some.",
            "Of course there are different, but if you if you try to think about what they have in common, I think that it's the.",
            "That when you new semi supervised learning this additional information that you get from the unlabeled data is mostly useful in the case where the classification error is very low.",
            "And so this is generally referred to as the cluster assumption.",
            "This is not very, very well defined notion, but when you look at the methods, usually this as cluster assumption is used, for instance by adding a penalty term to your normal supervised criterion, and this penalty term with do either do one of these two things, which in fact are very in some sense similar.",
            "Either you try to force the decision boundaries of the classifier that you estimating to cross.",
            "Only low density regions, so region of the space where you don't have much unlabeled data or you will try, which is somewhat equivalent to make the decision in identity region to be as unambiguous as possible.",
            "So of course there are.",
            "These have been some improvement improvement that have been demonstrated by this method, but there are some concern.",
            "One of them is the fact that of course, because this unlabeled data is additional information.",
            "I mean from a decision theory point of view, you could you should compare to the.",
            "Supervised learning only.",
            "I mean that this is an additional information.",
            "I can discard it if I want so in some sense, any method that doesn't that doesn't guarantee that I'm not doing worse with the additional information and without it.",
            "Is as a problem, and in fact when you look at the literature, this is really not easy to achieve.",
            "I have a method that always give you something that that would be best.",
            "Then using only the supervised data.",
            "For instance, you may think that of course this intuition is probably correct, that the semi supervise is most useful if the classification error is very low, but if your data if it's not true for your data.",
            "If it's not the case and you're trying to add this in your penalty term or something, are you really going to improve on that?",
            "So that's I think a problem in for us, since we'll be discussing this Community probabilistic model, there's another big problem, which is that in generative models, it's.",
            "Quite easy to see or you should use the unreliable information so at least I'm not claiming that it's easy to analyze them to say whether twitches, for instance, the goal of always improving but at least it's easy to see what you should do for discriminative probabilistic model.",
            "This is not at all the case, and there are even some claims and which are in part through that.",
            "In fact, for those models, maybe it's not useful at all.",
            "Maybe in trying to use some information brought by.",
            "Unlabeled data you are just messing this thing up and you shouldn't do that.",
            "And this ad also this has also generated some methods which try to combine both by using mixture of generative and discriminative probabilistic."
        ],
        [
            "So what this contribution does is we will be assuming that we have a probabilistic model which is given and it's a discriminative one.",
            "So it can be summarized by conditional distribution of the label given the search feature.",
            "And it has a parameter.",
            "This city is a parameter, so that's the only thing that you can do.",
            "Find a good value of Theta and we will push the situation.",
            "The semi supervised situation to the extreme because we will assume that we have so much.",
            "Unlabeled data that in fact we know exactly the marginal OK.",
            "So that can be a bit arbitrary, but I don't think actually that it's quite far from.",
            "The truth in many application and the third one is something that we like to avoid, but so the third one.",
            "Of course the level set is always finite.",
            "This is not a problem, but we have to assume also that the data take only a finite number of which is, I admit, restrictive.",
            "But.",
            "We have to assume that for reason that would be clear on the following.",
            "So our question is.",
            "So we we simplified.",
            "I say in two ways.",
            "The first one is what we in some sense, there's no unlabeled data anymore because we think that we do as if the information route by this is really the knowledge of the marginal probability.",
            "So that's one simplification and the other one is that the fact that we are restricting to finite state.",
            "For the UPS."
        ],
        [
            "Ovation.",
            "So these are on rotation, so some of them are already used and the only thing that I want to insist on is that this would be the.",
            "This all refers to the actual distribution of the data and labels, so by is the joint probability this one is the conditional and these are the marginals so I recall that this Q is assumed to be known.",
            "And this is what the model says about the conditional probability.",
            "I insist that it's very important in this setting to particular be aware that this thing might not be equal to this one, even if you choose a very good value of Sita.",
            "And this is a redundant notation, but we use it anyway.",
            "This is the minus the log of the condition."
        ],
        [
            "Like we would, so some inspiration from from statistics.",
            "So in fact there's a problem which is which resembles setting that we're considering here, which occurs in survey sampling.",
            "So if you assume that you you have two variables which are both finite value and you want to estimate their joint probability.",
            "Then in fact, if you know the marginal of one of them, you should not estimate the joint probability by this thing.",
            "So simply counting the number of time in your observation, you've seen the couple X&Y.",
            "You should be estimating this way, because in fact the only know the only thing that you don't know in this joint probability is the conditional, because you already know the marginal, so the only thing that is unknown in that needs to be estimated is the conditional.",
            "So of course this is the maximum likelihood estimator of the conditional probability of Y given.",
            "X.",
            "And so this is the good estimate and this good estimate what you can show is that it has an SMT violence which is given by this formula.",
            "So if you compare it to the variance of this one, so the variance of this one is easy to compute, it's very well known is the violence of Bernoulli variable which equals 01.",
            "So it's \u03c0 -- \u03c0.",
            "And here you find something which is a bit different because you have by y -- Y to, and it turns out that this thing is always smaller than this one.",
            "This has to do with the fact that of course, by Equal Eater Times Q, and since Q is smaller than one.",
            "It means that by is smaller than either, so it is closer to 1.",
            "So in fact this number, this one minus Theta is smaller than 1 -- 5 and it can even be that case that this thing is extremely small because it could be could be arbitrarily close to one, and in this case this factor would be almost zero.",
            "So it means that if there is one of the label forgiven value of X, one of the label is.",
            "Nearly always occur with that given value of the data.",
            "Then here you make almost no error, or at least an error which is not of the same order compared to the usual estimator, and so the."
        ],
        [
            "She.",
            "The usual way this is used in statistique.",
            "Is to estimate marginal probability of why?",
            "So?",
            "I'm not claiming that is that this is interesting for learning, but this is the way this unit statistiques.",
            "So if you want to estimate now, the marginal probability of Y, then the best estimator that you can use is the sum of the one that we had previously.",
            "And because these estimators despite at they tend to be decorrelated for different value of X.",
            "In fact, this so called stratified estimator, this one as an SMT violence with which is just the sum of the SMT violence that we saw on the previous on the previous slide.",
            "And here I just written that bit differently because this was by Y minus Eater.",
            "But you may also call this QE to 1 -- E to, and when you look at it a 1 -- E to this is the conditional variance.",
            "Of these things, or something that equal that cause one if capital Y equal to small lie or 0 otherwise, so ITA 1 minus ITA is exactly the this conditional variance in blue.",
            "So it means that the formula that we obtain is that this estimator as an aseptic violence, which is the expectation of a conditional variance.",
            "And why is it any better than using the normal estimator?",
            "I mean the one which doesn't even look at the X, so this is in some sense very.",
            "This is where you see that it sounds similar to the semi supervised setting, because this is the estimator that simply discard the X.",
            "Doesn't look at the X, only look at the Y and do the usual empirical average and this one has a variance which is.",
            "So when I say violence, perhaps this is not correct.",
            "I mean the variance of this estimator is this thing divided by N OK and the ascent attic variance is compatible with this thing.",
            "So this thing is just the variance of one of those Bernoulli variable and you see that what you compare is the expectation of the conditional variance with the variance.",
            "And so Percy mentioned that yesterday the difference between both is always positive and it can be expressed this way.",
            "So this formula said you have that violence is.",
            "The expectation of the conditional variance plus the variance of the conditional expectation so determine between both is exactly this thing.",
            "So it means that.",
            "Unless.",
            "The conditional probability does not depend on X.",
            "You will always gain something in using the stratified estimator rather than the usual estimator.",
            "You see also that it's fragile, I mean.",
            "You have to be careful because the improvement in this estimator is entirely based on the fact that you know cure fix, because if you were now to replace in this estimator Q are fixed by an estimated value."
        ],
        [
            "Then it's quite obvious that.",
            "If here I'm estimating Q, so I'm estimating, it's perhaps by this some normalized by N, then everything is lost.",
            "So the only thing?"
        ],
        [
            "That you can that you.",
            "The improvement that you get in this example is fully known by the fact that you know Q, and when you know Q, you can trade these variance for an expectation of conditional variance, which is small."
        ],
        [
            "So the now we come into our discriminative model and the question that we ask is what happens to this result if we have a discriminative model.",
            "So the first thing I should make clear is what are we expecting from our conditional models so.",
            "The model has a conditional probability which is given by G. N We will consider only method that try to maximize the log likelihood.",
            "OK, log likelihood based methods.",
            "So it means that those methods.",
            "What they do in the end is try to optimize this risk.",
            "We call this logarithmic risk so this Lu can be interpreted as a logarithmic loss.",
            "So this is the expectation under the actual data distribution, so this is the thing that you're trying to minimize, and in fact the point.",
            "There was, which you expect to converge.",
            "Is this is a star, which is the minimum of the expected risk and methods for doing this.",
            "Typically the usual semi supervised estimator.",
            "What you can expect from it is that when the number of observations, so this is N gets sufficiently large, it will converge to this little star an it will have given aseptic covariance matrix and this aseptic covariance matrix.",
            "This was also.",
            "Mentioned yesterday is useful because it gives an equivalent of the excess logarithmic risk, so this thing is the logarithmic risk between the estimated parameter minus the optimal one.",
            "The expectation here is taken with respect to all this sample that you may have observed.",
            "And this is multiplied by North, so this is decreasing as 1 / N and so here is an equivalent for this quantity.",
            "So it means that.",
            "To compare such method to compare methods that are doing this well, if there's if one of them as a smaller of synthetic variance Sigma, then it should be better than.",
            "Another should be preferred to another and in this setting in this emphatic analysis, that's how you should try to find the best performing method is trying to find the one which achieve the smallest covariance matrix possible.",
            "Covariance matrix.",
            "OK, so this may sound.",
            "Really dude."
        ],
        [
            "Change because of course there's a particular case which is often considered as being not particular but general case which is.",
            "We can add an assumption which will make this thing look very different.",
            "This assumption is if we assume that the model is well specified, that is, that all data distribution is taken from the model for a particular value sitestar.",
            "Of course, we're going to converge to Sister Star, but that's not the only thing that happens, and there's a lot of things that are very different in this case, and in particular, you see that if this is true.",
            "Then it means that yeah, you have an expectation, but you could imagine taking the expectation of a conditional expectation given X.",
            "And if you're well specified, even the conditional part itself will already be minimized at citrus to for any value of X that you may consider.",
            "So it means that in this case, of course.",
            "For instance, this logarithmic risk or it's it's minimum does not depend on Q.",
            "In fact it does not depend on the marginal, which is not true in general.",
            "OK, so if the model is well specified, it is true that this value of Sita star does not even depend on Q.",
            "So in some sense it's not very important.",
            "If you observe some data which has the correct conditional distribution but not the correct marginal, and we'll see it has been known for sometimes that in this case it's not important.",
            "Also, if you know things about.",
            "The marginal distribution, so we tend to view this as the general case, and this is a particular case.",
            "This is the.",
            "This case is what was called yesterday Misspecified model."
        ],
        [
            "So our result is the following.",
            "We claim that this thing.",
            "Is an interesting is estimator.",
            "I'm going to say why, but you see that it's only the usual thing that you would expect.",
            "Except that determine blue.",
            "Yeah, determine blue would be usually 1 / N OK. Usually what you would take is take all your training sample, weigh them equally and try to minimize this empirical some over the observed exam.",
            "The training example.",
            "And here you have a difference, which is that you have a weight.",
            "Which is not equal to 1 / N exactly and this weight is the difference between the.",
            "Proportion that you know should be.",
            "Corresponding to the marginal distribution of the X, and this is the empirical proportion that you really observed in your training sample."
        ],
        [
            "And so we think that it's good, because in fact it's the IT achieves the minimal accepted by and so you know from an aseptic POV you can't do any better than this estimator.",
            "So you have this expression in which the J metrics is the one we already met on the previous slide.",
            "An age is this one, and so in what sense is it better?",
            "Well, if you compare it to this usual unweighted maximum likelihood estimator, you have an expression which is almost the same, except that in place instead of H you have high.",
            "And so you have the same phenomenon that H is the expectation of a conditional covariance.",
            "This time it's a covariance matrix and I is a covariance matrix and it's the covariance matrix and of something which is known as the score.",
            "So it's the gradient of the conditional likelihood at the point at which you're converging.",
            "So you have almost the same thing as in stratified estimation, except that here it's the covariance of the score, but you have."
        ],
        [
            "Same phenomenon.",
            "The reason why it's the case is only because if you write the thing a bit differently, you'll see that in fact is the estimator can be written like this where despite at exactly this estimation of joint probability that we had before.",
            "And the reason why it's optimal is because this PIAT, despite at our maximum likelihood estimator of the joint probabilities under the constraint that we know the marginal.",
            "So this thing is also.",
            "Maximum likelihood estimator under this constraint."
        ],
        [
            "Maybe it's easier to see what this means in a in a particular case, so this is the case of binary logistic regression, so I didn't even write what was a logistic regression.",
            "Perhaps I should so seta is a parameter, that of the same dimension of X or X is assumed to be a vector.",
            "And so the matrices, the ones that are interesting H&IH is the covariance matrix.",
            "In the proposed method I is the usual one and you see that it's very interesting because the difference between the between both is exactly this term and this term has an easy interpretation because it's a difference between the conditional or at least the part that is in blue.",
            "Is the difference between the conditional probability of observing your one given that you're that you see X.",
            "When so, this is the actual probability of your data generating.",
            "Mechanism and this is what your model say, and so it means that of course, if your model is well specified, this is equal to that, and you never see this time.",
            "So it means that there can only be difference between those two methods.",
            "So you can only achieve an aseptic performance which is better than the usual maximum likelihood estimator.",
            "If you're if you're in misspecified scenario, you cannot observe this if you're if you're in a well specified scenario and also you see that.",
            "Yeah, eight, this I sorry is the sum of two terms.",
            "So if you want to see that H is really smaller than I, it also means that you need this term to be small.",
            "OK, if you want to make if you want the difference between this one and this one to be much larger than this one, it means that this one has to be small.",
            "But when you look at what this is this can be small only if beta is is close to zero close to one.",
            "So it means that the difference will be significant only in models for which you have many axes for which.",
            "The conditional probability, the true conditional probability is closed."
        ],
        [
            "201 so we have some experiments that shows that.",
            "Note absolutely ridiculous, so this is something that will be clearer too.",
            "Do people that went to the top by Percy yesterday because this is the excess logarithmic risk which is scaled by N. So it's multiplied by some by something that is growing.",
            "When you go from left to right.",
            "So that's why it stabilize at some value.",
            "But if you don't multiply it by end, of course you see the logarithmic risk is very small, so in this.",
            "Indeed, and so you see, this is the usual logistic regression and weighted, and this is the proposed estimator.",
            "And you see that when N is sufficiently large, it's exactly as expected anet between.",
            "It's a bit more complicated, but since I have only few time and rather not explain why, we seem that it's the case exactly here, so this is of course a model which is misspecified, because if the model was well specified, those two lines they would match and you wouldn't see much difference between the two things.",
            "So these are box plot and so the expected value, um, prequel expectations.",
            "Sorry, either read the thing.",
            "OK so this is at least when N is efficiently large.",
            "This is approximately where the."
        ],
        [
            "Reset.",
            "A few more things, so this is perhaps the nightmare slide, so we go very fast on this because it suddenly pops up another problem which you may or may not know, but I'm only saying to those people who know what the quiet shift problem is.",
            "If you have covi achieve.",
            "So if the marginal distribution in the test and training sample are different, you probably know this thing which is an estimator which.",
            "Takes it is inspiration from an importance weighting important sampling method, so this is the important ratio, and in fact it turns out that this one as a larger asymptotic variance than the proposed estimator.",
            "If you use it, of course with Q1.",
            "This is a bit surprising because in this estimator you need to know not only Q1 but also Q zero.",
            "It turns out that if you don't know Q0 and if you use the estimator that that we should have shown before, then it's actually better.",
            "Of course the difference there must be a different summer.",
            "The difference is that in the former scenario, if you don't have covariate shift the weights they tend to 1 / N here.",
            "Of course they would not tend to one of our end, but exactly to this correct factor 1, /, Q and secure.",
            "So this could be of interest also in the covariate shift."
        ],
        [
            "Situation.",
            "So final word of course.",
            "Is the problem in this?",
            "Which is that it's not very interesting in many practical problem to say that we have only finished finite value the data, so we must do something about this.",
            "So in the paper there's an.",
            "Something that I won't show.",
            "Era text classification example and we had to do something because even if you if you take words, there are too many words combination of course and you cannot pretend that they take a finite number of values.",
            "So what we did is to use clustering.",
            "So in fact we use the unlabeled data to perform clustering.",
            "If you're given number of cluster and in this way of expression we consider that this we use the weight not with the.",
            "X but with the clustering indicator variable.",
            "So it means that the queue is replaced by the empirical frequency of the cluster to which xib long.",
            "So this empirical refers to what we obtain from the unlabeled data.",
            "And here we divide by the number of points in the training sample that fold in the same cluster.",
            "So it means in fact, if you think about a bit of fits, if you.",
            "Down weight, for instance, the sample that comes from cluster that are too much represented in your training sample compared to what you know should be the long term frequency.",
            "In the opposite way, of course, if they are under represented."
        ],
        [
            "OK, so that's the end.",
            "I'm mainly said everything that's in this slide.",
            "I just want to say here that of course this is a bit disappointing in some cases because we see that we don't gain much, but at least in some of the situation, in particular, when the number of sample is sufficiently large.",
            "This is predicted by the theory, so it means that there are really settings in which you can't gain very much by having some unlabeled data.",
            "An I think it's important to know.",
            "OK, so at the end, thank you.",
            "Questions.",
            "Sorry, yes.",
            "Yeah.",
            "Yeah, of course."
        ],
        [
            "How do you make decisions?",
            "Very bad no no no.",
            "Of course this is a very bad question, but OK, I can tell you what happened if you do it badly.",
            "What happens if you do it badly?",
            "OK. Maybe I need to?"
        ],
        [
            "Do something that you should pay attention to.",
            "Is that yeah, you have a weight, so if you're unlucky, what could happen?",
            "It could happen that some samples in your observation would get almost 0 weight, so in practice it would mean that you're not only.",
            "Are you adding some other information?",
            "Other than your sample that you have, but you also discarding some of them, so this is a very bad situation.",
            "So it means that of course this method cannot work if this Q as an incredible dynamic.",
            "So it means that the thing you should be aware of is that if this Q is such that you have many values that are quite small and some value which has been very big, this will not work at all.",
            "So I mean that if you're training samples hey as an equal to 100.",
            "Data if you're going to use a clustering method with 1000 clusters, it would probably not be a very good idea.",
            "So in practice what we do is fix the number of cluster as a function of the number of observation that we really had, but I mean it's the part where they would just say I've known theory of why you should do that and why it's better or not."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm leaving capital will be presenting the this paper which is a joint work between and made myself Natalia so closely with.",
                    "label": 0
                },
                {
                    "sent": "He ran, our colleague Francois evolves from University price sheet so there are various ways in which this work can be presented probably, and I'm presenting it from a statistical perspective which is more my background.",
                    "label": 0
                },
                {
                    "sent": "So because there's also the poster session, you get an opportunity to have it presented.",
                    "label": 0
                },
                {
                    "sent": "A bit differently at the poster so I can go to the poster as well if you're interested by the talks.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the talk, I will insist perhaps more than in the paper on the.",
                    "label": 0
                },
                {
                    "sent": "This idea of stratified sampling, which is a principle which is known in statistiken which forms the basis of the method that we are describing here.",
                    "label": 0
                },
                {
                    "sent": "So the method is about semi supervised classification and we tried to answer the following question which in fact we could not really answer.",
                    "label": 0
                },
                {
                    "sent": "So I will come to that later but.",
                    "label": 0
                },
                {
                    "sent": "Our question was if we consider discriminative probabilistic model, is it possible to say?",
                    "label": 0
                },
                {
                    "sent": "What should be the correct way or off estimating the parameters of such a model?",
                    "label": 0
                },
                {
                    "sent": "If you have some information brought by unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "OK, is there an optimal way of doing this or is there a way which satisfies some requirement that I'm going to talk about so we don't have the answer to this question because we have the answer only in any case which is a bit more restricted than that, but at least in a case which is a bit more restricted in that we have the answer so we know what's an optimal method.",
                    "label": 0
                },
                {
                    "sent": "In some sense an.",
                    "label": 0
                },
                {
                    "sent": "What what are is its performances.",
                    "label": 0
                },
                {
                    "sent": "I have to say also that this talk is a very related to the talk by parcel Young and in the few been yesterday to the word session.",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of things that if you attended this talk you will recognize.",
                    "label": 0
                },
                {
                    "sent": "Also in particular is sure he showed an expression which is very important here and so we'll see where it's important for semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a semi supervised, so since he's been the topic of this session, maybe it's not that useful to explain exactly what it is, so we have some training data an we'd like to improve.",
                    "label": 0
                },
                {
                    "sent": "I think that this work.",
                    "label": 0
                },
                {
                    "sent": "This word is important.",
                    "label": 0
                },
                {
                    "sent": "Would like to improve what we can get from training data by some information on the.",
                    "label": 0
                },
                {
                    "sent": "On what exactly?",
                    "label": 0
                },
                {
                    "sent": "So I think that the information that we have information on the marginal distribution of the features so feature is the word that I use for the data.",
                    "label": 1
                },
                {
                    "sent": "So in semi supervised estimation we get some additional information that we don't have only.",
                    "label": 0
                },
                {
                    "sent": "Levels of data with levels.",
                    "label": 0
                },
                {
                    "sent": "We have some additional information and it's additional information on the marginal distribution on the feature.",
                    "label": 0
                },
                {
                    "sent": "So the question is how to do something with this additional information.",
                    "label": 1
                },
                {
                    "sent": "So it's very important, of course, because in many learning application this information is really.",
                    "label": 0
                },
                {
                    "sent": "Can be really precise, so it's not unlikely that for instance you have 100 or 1000 labeled data, but perhaps 10,000 unlabeled data, so in some sense you can say that this additional information that you have, maybe it's not that useful, but at least it can be rather precise in many application.",
                    "label": 1
                },
                {
                    "sent": "And it's also the reason why I think it's different from something that is much studied in statistics, which is the context of missing data, which would more be the case.",
                    "label": 1
                },
                {
                    "sent": "If you add some training samples but not all of them, a few of them where there something missing.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the label, or perhaps some attributes of the data would be missing, so that would be typical case of what people call missing data, but I don't think that it's exactly the same setting in machine learning, because in machine learning you have the training data and you have this additional information which you may or may not use.",
                    "label": 0
                },
                {
                    "sent": "And also it's since it's based on a large number of unlabeled data, it can be.",
                    "label": 0
                },
                {
                    "sent": "In fact, very precise.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm not going to really discuss very precisely what the literature says about this problem.",
                    "label": 0
                },
                {
                    "sent": "I just want to say one of the reasons that I can do this is because many of the methods that have been proposed so far they are very dependent on the of the type of classifier that you're using.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, if you're using SVM, you can use one method.",
                    "label": 0
                },
                {
                    "sent": "If you in this case will be focusing on discriminative probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So you have some other method that are.",
                    "label": 0
                },
                {
                    "sent": "Useful in this case, so there are some.",
                    "label": 1
                },
                {
                    "sent": "Of course there are different, but if you if you try to think about what they have in common, I think that it's the.",
                    "label": 0
                },
                {
                    "sent": "That when you new semi supervised learning this additional information that you get from the unlabeled data is mostly useful in the case where the classification error is very low.",
                    "label": 1
                },
                {
                    "sent": "And so this is generally referred to as the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "This is not very, very well defined notion, but when you look at the methods, usually this as cluster assumption is used, for instance by adding a penalty term to your normal supervised criterion, and this penalty term with do either do one of these two things, which in fact are very in some sense similar.",
                    "label": 1
                },
                {
                    "sent": "Either you try to force the decision boundaries of the classifier that you estimating to cross.",
                    "label": 1
                },
                {
                    "sent": "Only low density regions, so region of the space where you don't have much unlabeled data or you will try, which is somewhat equivalent to make the decision in identity region to be as unambiguous as possible.",
                    "label": 0
                },
                {
                    "sent": "So of course there are.",
                    "label": 0
                },
                {
                    "sent": "These have been some improvement improvement that have been demonstrated by this method, but there are some concern.",
                    "label": 0
                },
                {
                    "sent": "One of them is the fact that of course, because this unlabeled data is additional information.",
                    "label": 0
                },
                {
                    "sent": "I mean from a decision theory point of view, you could you should compare to the.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning only.",
                    "label": 0
                },
                {
                    "sent": "I mean that this is an additional information.",
                    "label": 0
                },
                {
                    "sent": "I can discard it if I want so in some sense, any method that doesn't that doesn't guarantee that I'm not doing worse with the additional information and without it.",
                    "label": 1
                },
                {
                    "sent": "Is as a problem, and in fact when you look at the literature, this is really not easy to achieve.",
                    "label": 0
                },
                {
                    "sent": "I have a method that always give you something that that would be best.",
                    "label": 0
                },
                {
                    "sent": "Then using only the supervised data.",
                    "label": 0
                },
                {
                    "sent": "For instance, you may think that of course this intuition is probably correct, that the semi supervise is most useful if the classification error is very low, but if your data if it's not true for your data.",
                    "label": 0
                },
                {
                    "sent": "If it's not the case and you're trying to add this in your penalty term or something, are you really going to improve on that?",
                    "label": 0
                },
                {
                    "sent": "So that's I think a problem in for us, since we'll be discussing this Community probabilistic model, there's another big problem, which is that in generative models, it's.",
                    "label": 0
                },
                {
                    "sent": "Quite easy to see or you should use the unreliable information so at least I'm not claiming that it's easy to analyze them to say whether twitches, for instance, the goal of always improving but at least it's easy to see what you should do for discriminative probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "This is not at all the case, and there are even some claims and which are in part through that.",
                    "label": 0
                },
                {
                    "sent": "In fact, for those models, maybe it's not useful at all.",
                    "label": 0
                },
                {
                    "sent": "Maybe in trying to use some information brought by.",
                    "label": 1
                },
                {
                    "sent": "Unlabeled data you are just messing this thing up and you shouldn't do that.",
                    "label": 0
                },
                {
                    "sent": "And this ad also this has also generated some methods which try to combine both by using mixture of generative and discriminative probabilistic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what this contribution does is we will be assuming that we have a probabilistic model which is given and it's a discriminative one.",
                    "label": 1
                },
                {
                    "sent": "So it can be summarized by conditional distribution of the label given the search feature.",
                    "label": 0
                },
                {
                    "sent": "And it has a parameter.",
                    "label": 0
                },
                {
                    "sent": "This city is a parameter, so that's the only thing that you can do.",
                    "label": 0
                },
                {
                    "sent": "Find a good value of Theta and we will push the situation.",
                    "label": 0
                },
                {
                    "sent": "The semi supervised situation to the extreme because we will assume that we have so much.",
                    "label": 1
                },
                {
                    "sent": "Unlabeled data that in fact we know exactly the marginal OK.",
                    "label": 0
                },
                {
                    "sent": "So that can be a bit arbitrary, but I don't think actually that it's quite far from.",
                    "label": 0
                },
                {
                    "sent": "The truth in many application and the third one is something that we like to avoid, but so the third one.",
                    "label": 0
                },
                {
                    "sent": "Of course the level set is always finite.",
                    "label": 0
                },
                {
                    "sent": "This is not a problem, but we have to assume also that the data take only a finite number of which is, I admit, restrictive.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "We have to assume that for reason that would be clear on the following.",
                    "label": 0
                },
                {
                    "sent": "So our question is.",
                    "label": 0
                },
                {
                    "sent": "So we we simplified.",
                    "label": 0
                },
                {
                    "sent": "I say in two ways.",
                    "label": 1
                },
                {
                    "sent": "The first one is what we in some sense, there's no unlabeled data anymore because we think that we do as if the information route by this is really the knowledge of the marginal probability.",
                    "label": 0
                },
                {
                    "sent": "So that's one simplification and the other one is that the fact that we are restricting to finite state.",
                    "label": 0
                },
                {
                    "sent": "For the UPS.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ovation.",
                    "label": 0
                },
                {
                    "sent": "So these are on rotation, so some of them are already used and the only thing that I want to insist on is that this would be the.",
                    "label": 0
                },
                {
                    "sent": "This all refers to the actual distribution of the data and labels, so by is the joint probability this one is the conditional and these are the marginals so I recall that this Q is assumed to be known.",
                    "label": 0
                },
                {
                    "sent": "And this is what the model says about the conditional probability.",
                    "label": 0
                },
                {
                    "sent": "I insist that it's very important in this setting to particular be aware that this thing might not be equal to this one, even if you choose a very good value of Sita.",
                    "label": 0
                },
                {
                    "sent": "And this is a redundant notation, but we use it anyway.",
                    "label": 0
                },
                {
                    "sent": "This is the minus the log of the condition.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like we would, so some inspiration from from statistics.",
                    "label": 0
                },
                {
                    "sent": "So in fact there's a problem which is which resembles setting that we're considering here, which occurs in survey sampling.",
                    "label": 1
                },
                {
                    "sent": "So if you assume that you you have two variables which are both finite value and you want to estimate their joint probability.",
                    "label": 1
                },
                {
                    "sent": "Then in fact, if you know the marginal of one of them, you should not estimate the joint probability by this thing.",
                    "label": 0
                },
                {
                    "sent": "So simply counting the number of time in your observation, you've seen the couple X&Y.",
                    "label": 0
                },
                {
                    "sent": "You should be estimating this way, because in fact the only know the only thing that you don't know in this joint probability is the conditional, because you already know the marginal, so the only thing that is unknown in that needs to be estimated is the conditional.",
                    "label": 1
                },
                {
                    "sent": "So of course this is the maximum likelihood estimator of the conditional probability of Y given.",
                    "label": 1
                },
                {
                    "sent": "X.",
                    "label": 0
                },
                {
                    "sent": "And so this is the good estimate and this good estimate what you can show is that it has an SMT violence which is given by this formula.",
                    "label": 0
                },
                {
                    "sent": "So if you compare it to the variance of this one, so the variance of this one is easy to compute, it's very well known is the violence of Bernoulli variable which equals 01.",
                    "label": 0
                },
                {
                    "sent": "So it's \u03c0 -- \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And here you find something which is a bit different because you have by y -- Y to, and it turns out that this thing is always smaller than this one.",
                    "label": 0
                },
                {
                    "sent": "This has to do with the fact that of course, by Equal Eater Times Q, and since Q is smaller than one.",
                    "label": 0
                },
                {
                    "sent": "It means that by is smaller than either, so it is closer to 1.",
                    "label": 0
                },
                {
                    "sent": "So in fact this number, this one minus Theta is smaller than 1 -- 5 and it can even be that case that this thing is extremely small because it could be could be arbitrarily close to one, and in this case this factor would be almost zero.",
                    "label": 0
                },
                {
                    "sent": "So it means that if there is one of the label forgiven value of X, one of the label is.",
                    "label": 0
                },
                {
                    "sent": "Nearly always occur with that given value of the data.",
                    "label": 0
                },
                {
                    "sent": "Then here you make almost no error, or at least an error which is not of the same order compared to the usual estimator, and so the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She.",
                    "label": 0
                },
                {
                    "sent": "The usual way this is used in statistique.",
                    "label": 0
                },
                {
                    "sent": "Is to estimate marginal probability of why?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that is that this is interesting for learning, but this is the way this unit statistiques.",
                    "label": 0
                },
                {
                    "sent": "So if you want to estimate now, the marginal probability of Y, then the best estimator that you can use is the sum of the one that we had previously.",
                    "label": 0
                },
                {
                    "sent": "And because these estimators despite at they tend to be decorrelated for different value of X.",
                    "label": 0
                },
                {
                    "sent": "In fact, this so called stratified estimator, this one as an SMT violence with which is just the sum of the SMT violence that we saw on the previous on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And here I just written that bit differently because this was by Y minus Eater.",
                    "label": 0
                },
                {
                    "sent": "But you may also call this QE to 1 -- E to, and when you look at it a 1 -- E to this is the conditional variance.",
                    "label": 0
                },
                {
                    "sent": "Of these things, or something that equal that cause one if capital Y equal to small lie or 0 otherwise, so ITA 1 minus ITA is exactly the this conditional variance in blue.",
                    "label": 0
                },
                {
                    "sent": "So it means that the formula that we obtain is that this estimator as an aseptic violence, which is the expectation of a conditional variance.",
                    "label": 0
                },
                {
                    "sent": "And why is it any better than using the normal estimator?",
                    "label": 0
                },
                {
                    "sent": "I mean the one which doesn't even look at the X, so this is in some sense very.",
                    "label": 0
                },
                {
                    "sent": "This is where you see that it sounds similar to the semi supervised setting, because this is the estimator that simply discard the X.",
                    "label": 0
                },
                {
                    "sent": "Doesn't look at the X, only look at the Y and do the usual empirical average and this one has a variance which is.",
                    "label": 0
                },
                {
                    "sent": "So when I say violence, perhaps this is not correct.",
                    "label": 0
                },
                {
                    "sent": "I mean the variance of this estimator is this thing divided by N OK and the ascent attic variance is compatible with this thing.",
                    "label": 0
                },
                {
                    "sent": "So this thing is just the variance of one of those Bernoulli variable and you see that what you compare is the expectation of the conditional variance with the variance.",
                    "label": 0
                },
                {
                    "sent": "And so Percy mentioned that yesterday the difference between both is always positive and it can be expressed this way.",
                    "label": 1
                },
                {
                    "sent": "So this formula said you have that violence is.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the conditional variance plus the variance of the conditional expectation so determine between both is exactly this thing.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "Unless.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability does not depend on X.",
                    "label": 1
                },
                {
                    "sent": "You will always gain something in using the stratified estimator rather than the usual estimator.",
                    "label": 0
                },
                {
                    "sent": "You see also that it's fragile, I mean.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful because the improvement in this estimator is entirely based on the fact that you know cure fix, because if you were now to replace in this estimator Q are fixed by an estimated value.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then it's quite obvious that.",
                    "label": 0
                },
                {
                    "sent": "If here I'm estimating Q, so I'm estimating, it's perhaps by this some normalized by N, then everything is lost.",
                    "label": 0
                },
                {
                    "sent": "So the only thing?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you can that you.",
                    "label": 0
                },
                {
                    "sent": "The improvement that you get in this example is fully known by the fact that you know Q, and when you know Q, you can trade these variance for an expectation of conditional variance, which is small.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the now we come into our discriminative model and the question that we ask is what happens to this result if we have a discriminative model.",
                    "label": 1
                },
                {
                    "sent": "So the first thing I should make clear is what are we expecting from our conditional models so.",
                    "label": 0
                },
                {
                    "sent": "The model has a conditional probability which is given by G. N We will consider only method that try to maximize the log likelihood.",
                    "label": 1
                },
                {
                    "sent": "OK, log likelihood based methods.",
                    "label": 0
                },
                {
                    "sent": "So it means that those methods.",
                    "label": 0
                },
                {
                    "sent": "What they do in the end is try to optimize this risk.",
                    "label": 0
                },
                {
                    "sent": "We call this logarithmic risk so this Lu can be interpreted as a logarithmic loss.",
                    "label": 0
                },
                {
                    "sent": "So this is the expectation under the actual data distribution, so this is the thing that you're trying to minimize, and in fact the point.",
                    "label": 0
                },
                {
                    "sent": "There was, which you expect to converge.",
                    "label": 0
                },
                {
                    "sent": "Is this is a star, which is the minimum of the expected risk and methods for doing this.",
                    "label": 0
                },
                {
                    "sent": "Typically the usual semi supervised estimator.",
                    "label": 0
                },
                {
                    "sent": "What you can expect from it is that when the number of observations, so this is N gets sufficiently large, it will converge to this little star an it will have given aseptic covariance matrix and this aseptic covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "This was also.",
                    "label": 0
                },
                {
                    "sent": "Mentioned yesterday is useful because it gives an equivalent of the excess logarithmic risk, so this thing is the logarithmic risk between the estimated parameter minus the optimal one.",
                    "label": 1
                },
                {
                    "sent": "The expectation here is taken with respect to all this sample that you may have observed.",
                    "label": 1
                },
                {
                    "sent": "And this is multiplied by North, so this is decreasing as 1 / N and so here is an equivalent for this quantity.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "To compare such method to compare methods that are doing this well, if there's if one of them as a smaller of synthetic variance Sigma, then it should be better than.",
                    "label": 0
                },
                {
                    "sent": "Another should be preferred to another and in this setting in this emphatic analysis, that's how you should try to find the best performing method is trying to find the one which achieve the smallest covariance matrix possible.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so this may sound.",
                    "label": 0
                },
                {
                    "sent": "Really dude.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Change because of course there's a particular case which is often considered as being not particular but general case which is.",
                    "label": 0
                },
                {
                    "sent": "We can add an assumption which will make this thing look very different.",
                    "label": 0
                },
                {
                    "sent": "This assumption is if we assume that the model is well specified, that is, that all data distribution is taken from the model for a particular value sitestar.",
                    "label": 0
                },
                {
                    "sent": "Of course, we're going to converge to Sister Star, but that's not the only thing that happens, and there's a lot of things that are very different in this case, and in particular, you see that if this is true.",
                    "label": 0
                },
                {
                    "sent": "Then it means that yeah, you have an expectation, but you could imagine taking the expectation of a conditional expectation given X.",
                    "label": 0
                },
                {
                    "sent": "And if you're well specified, even the conditional part itself will already be minimized at citrus to for any value of X that you may consider.",
                    "label": 1
                },
                {
                    "sent": "So it means that in this case, of course.",
                    "label": 0
                },
                {
                    "sent": "For instance, this logarithmic risk or it's it's minimum does not depend on Q.",
                    "label": 0
                },
                {
                    "sent": "In fact it does not depend on the marginal, which is not true in general.",
                    "label": 0
                },
                {
                    "sent": "OK, so if the model is well specified, it is true that this value of Sita star does not even depend on Q.",
                    "label": 1
                },
                {
                    "sent": "So in some sense it's not very important.",
                    "label": 0
                },
                {
                    "sent": "If you observe some data which has the correct conditional distribution but not the correct marginal, and we'll see it has been known for sometimes that in this case it's not important.",
                    "label": 0
                },
                {
                    "sent": "Also, if you know things about.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution, so we tend to view this as the general case, and this is a particular case.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This case is what was called yesterday Misspecified model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our result is the following.",
                    "label": 0
                },
                {
                    "sent": "We claim that this thing.",
                    "label": 0
                },
                {
                    "sent": "Is an interesting is estimator.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say why, but you see that it's only the usual thing that you would expect.",
                    "label": 0
                },
                {
                    "sent": "Except that determine blue.",
                    "label": 0
                },
                {
                    "sent": "Yeah, determine blue would be usually 1 / N OK. Usually what you would take is take all your training sample, weigh them equally and try to minimize this empirical some over the observed exam.",
                    "label": 0
                },
                {
                    "sent": "The training example.",
                    "label": 0
                },
                {
                    "sent": "And here you have a difference, which is that you have a weight.",
                    "label": 0
                },
                {
                    "sent": "Which is not equal to 1 / N exactly and this weight is the difference between the.",
                    "label": 0
                },
                {
                    "sent": "Proportion that you know should be.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to the marginal distribution of the X, and this is the empirical proportion that you really observed in your training sample.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we think that it's good, because in fact it's the IT achieves the minimal accepted by and so you know from an aseptic POV you can't do any better than this estimator.",
                    "label": 0
                },
                {
                    "sent": "So you have this expression in which the J metrics is the one we already met on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "An age is this one, and so in what sense is it better?",
                    "label": 0
                },
                {
                    "sent": "Well, if you compare it to this usual unweighted maximum likelihood estimator, you have an expression which is almost the same, except that in place instead of H you have high.",
                    "label": 0
                },
                {
                    "sent": "And so you have the same phenomenon that H is the expectation of a conditional covariance.",
                    "label": 0
                },
                {
                    "sent": "This time it's a covariance matrix and I is a covariance matrix and it's the covariance matrix and of something which is known as the score.",
                    "label": 0
                },
                {
                    "sent": "So it's the gradient of the conditional likelihood at the point at which you're converging.",
                    "label": 0
                },
                {
                    "sent": "So you have almost the same thing as in stratified estimation, except that here it's the covariance of the score, but you have.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same phenomenon.",
                    "label": 0
                },
                {
                    "sent": "The reason why it's the case is only because if you write the thing a bit differently, you'll see that in fact is the estimator can be written like this where despite at exactly this estimation of joint probability that we had before.",
                    "label": 0
                },
                {
                    "sent": "And the reason why it's optimal is because this PIAT, despite at our maximum likelihood estimator of the joint probabilities under the constraint that we know the marginal.",
                    "label": 1
                },
                {
                    "sent": "So this thing is also.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood estimator under this constraint.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe it's easier to see what this means in a in a particular case, so this is the case of binary logistic regression, so I didn't even write what was a logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Perhaps I should so seta is a parameter, that of the same dimension of X or X is assumed to be a vector.",
                    "label": 0
                },
                {
                    "sent": "And so the matrices, the ones that are interesting H&IH is the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "In the proposed method I is the usual one and you see that it's very interesting because the difference between the between both is exactly this term and this term has an easy interpretation because it's a difference between the conditional or at least the part that is in blue.",
                    "label": 0
                },
                {
                    "sent": "Is the difference between the conditional probability of observing your one given that you're that you see X.",
                    "label": 0
                },
                {
                    "sent": "When so, this is the actual probability of your data generating.",
                    "label": 0
                },
                {
                    "sent": "Mechanism and this is what your model say, and so it means that of course, if your model is well specified, this is equal to that, and you never see this time.",
                    "label": 0
                },
                {
                    "sent": "So it means that there can only be difference between those two methods.",
                    "label": 0
                },
                {
                    "sent": "So you can only achieve an aseptic performance which is better than the usual maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "If you're if you're in misspecified scenario, you cannot observe this if you're if you're in a well specified scenario and also you see that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, eight, this I sorry is the sum of two terms.",
                    "label": 0
                },
                {
                    "sent": "So if you want to see that H is really smaller than I, it also means that you need this term to be small.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to make if you want the difference between this one and this one to be much larger than this one, it means that this one has to be small.",
                    "label": 0
                },
                {
                    "sent": "But when you look at what this is this can be small only if beta is is close to zero close to one.",
                    "label": 1
                },
                {
                    "sent": "So it means that the difference will be significant only in models for which you have many axes for which.",
                    "label": 0
                },
                {
                    "sent": "The conditional probability, the true conditional probability is closed.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "201 so we have some experiments that shows that.",
                    "label": 0
                },
                {
                    "sent": "Note absolutely ridiculous, so this is something that will be clearer too.",
                    "label": 0
                },
                {
                    "sent": "Do people that went to the top by Percy yesterday because this is the excess logarithmic risk which is scaled by N. So it's multiplied by some by something that is growing.",
                    "label": 0
                },
                {
                    "sent": "When you go from left to right.",
                    "label": 0
                },
                {
                    "sent": "So that's why it stabilize at some value.",
                    "label": 0
                },
                {
                    "sent": "But if you don't multiply it by end, of course you see the logarithmic risk is very small, so in this.",
                    "label": 0
                },
                {
                    "sent": "Indeed, and so you see, this is the usual logistic regression and weighted, and this is the proposed estimator.",
                    "label": 0
                },
                {
                    "sent": "And you see that when N is sufficiently large, it's exactly as expected anet between.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated, but since I have only few time and rather not explain why, we seem that it's the case exactly here, so this is of course a model which is misspecified, because if the model was well specified, those two lines they would match and you wouldn't see much difference between the two things.",
                    "label": 0
                },
                {
                    "sent": "So these are box plot and so the expected value, um, prequel expectations.",
                    "label": 0
                },
                {
                    "sent": "Sorry, either read the thing.",
                    "label": 0
                },
                {
                    "sent": "OK so this is at least when N is efficiently large.",
                    "label": 0
                },
                {
                    "sent": "This is approximately where the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reset.",
                    "label": 0
                },
                {
                    "sent": "A few more things, so this is perhaps the nightmare slide, so we go very fast on this because it suddenly pops up another problem which you may or may not know, but I'm only saying to those people who know what the quiet shift problem is.",
                    "label": 0
                },
                {
                    "sent": "If you have covi achieve.",
                    "label": 0
                },
                {
                    "sent": "So if the marginal distribution in the test and training sample are different, you probably know this thing which is an estimator which.",
                    "label": 0
                },
                {
                    "sent": "Takes it is inspiration from an importance weighting important sampling method, so this is the important ratio, and in fact it turns out that this one as a larger asymptotic variance than the proposed estimator.",
                    "label": 0
                },
                {
                    "sent": "If you use it, of course with Q1.",
                    "label": 0
                },
                {
                    "sent": "This is a bit surprising because in this estimator you need to know not only Q1 but also Q zero.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you don't know Q0 and if you use the estimator that that we should have shown before, then it's actually better.",
                    "label": 0
                },
                {
                    "sent": "Of course the difference there must be a different summer.",
                    "label": 0
                },
                {
                    "sent": "The difference is that in the former scenario, if you don't have covariate shift the weights they tend to 1 / N here.",
                    "label": 1
                },
                {
                    "sent": "Of course they would not tend to one of our end, but exactly to this correct factor 1, /, Q and secure.",
                    "label": 1
                },
                {
                    "sent": "So this could be of interest also in the covariate shift.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Situation.",
                    "label": 0
                },
                {
                    "sent": "So final word of course.",
                    "label": 0
                },
                {
                    "sent": "Is the problem in this?",
                    "label": 0
                },
                {
                    "sent": "Which is that it's not very interesting in many practical problem to say that we have only finished finite value the data, so we must do something about this.",
                    "label": 0
                },
                {
                    "sent": "So in the paper there's an.",
                    "label": 1
                },
                {
                    "sent": "Something that I won't show.",
                    "label": 0
                },
                {
                    "sent": "Era text classification example and we had to do something because even if you if you take words, there are too many words combination of course and you cannot pretend that they take a finite number of values.",
                    "label": 0
                },
                {
                    "sent": "So what we did is to use clustering.",
                    "label": 0
                },
                {
                    "sent": "So in fact we use the unlabeled data to perform clustering.",
                    "label": 0
                },
                {
                    "sent": "If you're given number of cluster and in this way of expression we consider that this we use the weight not with the.",
                    "label": 0
                },
                {
                    "sent": "X but with the clustering indicator variable.",
                    "label": 0
                },
                {
                    "sent": "So it means that the queue is replaced by the empirical frequency of the cluster to which xib long.",
                    "label": 1
                },
                {
                    "sent": "So this empirical refers to what we obtain from the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And here we divide by the number of points in the training sample that fold in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "So it means in fact, if you think about a bit of fits, if you.",
                    "label": 0
                },
                {
                    "sent": "Down weight, for instance, the sample that comes from cluster that are too much represented in your training sample compared to what you know should be the long term frequency.",
                    "label": 0
                },
                {
                    "sent": "In the opposite way, of course, if they are under represented.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the end.",
                    "label": 0
                },
                {
                    "sent": "I'm mainly said everything that's in this slide.",
                    "label": 0
                },
                {
                    "sent": "I just want to say here that of course this is a bit disappointing in some cases because we see that we don't gain much, but at least in some of the situation, in particular, when the number of sample is sufficiently large.",
                    "label": 1
                },
                {
                    "sent": "This is predicted by the theory, so it means that there are really settings in which you can't gain very much by having some unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "An I think it's important to know.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the end, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Sorry, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you make decisions?",
                    "label": 0
                },
                {
                    "sent": "Very bad no no no.",
                    "label": 0
                },
                {
                    "sent": "Of course this is a very bad question, but OK, I can tell you what happened if you do it badly.",
                    "label": 0
                },
                {
                    "sent": "What happens if you do it badly?",
                    "label": 0
                },
                {
                    "sent": "OK. Maybe I need to?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do something that you should pay attention to.",
                    "label": 0
                },
                {
                    "sent": "Is that yeah, you have a weight, so if you're unlucky, what could happen?",
                    "label": 0
                },
                {
                    "sent": "It could happen that some samples in your observation would get almost 0 weight, so in practice it would mean that you're not only.",
                    "label": 0
                },
                {
                    "sent": "Are you adding some other information?",
                    "label": 0
                },
                {
                    "sent": "Other than your sample that you have, but you also discarding some of them, so this is a very bad situation.",
                    "label": 0
                },
                {
                    "sent": "So it means that of course this method cannot work if this Q as an incredible dynamic.",
                    "label": 0
                },
                {
                    "sent": "So it means that the thing you should be aware of is that if this Q is such that you have many values that are quite small and some value which has been very big, this will not work at all.",
                    "label": 0
                },
                {
                    "sent": "So I mean that if you're training samples hey as an equal to 100.",
                    "label": 0
                },
                {
                    "sent": "Data if you're going to use a clustering method with 1000 clusters, it would probably not be a very good idea.",
                    "label": 0
                },
                {
                    "sent": "So in practice what we do is fix the number of cluster as a function of the number of observation that we really had, but I mean it's the part where they would just say I've known theory of why you should do that and why it's better or not.",
                    "label": 0
                }
            ]
        }
    }
}