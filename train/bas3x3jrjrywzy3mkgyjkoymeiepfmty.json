{
    "id": "bas3x3jrjrywzy3mkgyjkoymeiepfmty",
    "title": "Leveraging Higher Order Dependencies Between Features for Text Classification",
    "info": {
        "author": [
            "William M. Pottenger, Rutgers, The State University of New Jersey"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_pottenger_lhodbftc/",
    "segmentation": [
        [
            "OK, I think we can start, so we will now.",
            "Higher order features for text application and I think they should be given by really fast enough.",
            "So yeah, good morning puncher, sorry.",
            "I guess you folks didn't know where I was, but I actually knew I was so OK.",
            "Thanks for your patience.",
            "OK so this is work joint work with a couple of my students, Nikita Lipkin and Morocco knees."
        ],
        [
            "Anne.",
            "Basically, we're addressing the problem where you've got the old problem of how do you label enough training data to learn a model.",
            "So this has applications naturally with online learning and one of the issues is the traditional learning approaches classify a single instance at a time, unlike collective classification.",
            "So we've got to deal with even less."
        ],
        [
            "I've got an issue with even less training data, so here's an outline of what I want to talk about today."
        ],
        [
            "Just briefly, some of the related work.",
            "The basic issue here is that the related work in statistical relational learning task are Lisa Couture and others have shown that if you leverage the relationships between features or between entities or attributes, you can build better models, so that's that's the basic message here.",
            "Active classifiers are have done a lot of good things.",
            "We're trying to achieve the performance equal to a collective classifier, but with a classifier that can do."
        ],
        [
            "For instance classification.",
            "There's some more interesting related work to our particular approach.",
            "This first paper goes all the way back to 1953.",
            "I don't know if any of you familiar with cats is work.",
            "Early work where he looked at leveraging multiple paths between nodes in a graph in order to identify social relations, and then one of my former PhD students.",
            "She and I looked at LSI latent semantic indexing, otherwise called latent semantic analysis, and we were able to prove mathematically that LSI actually depends.",
            "On higher order pads or higher order Co occurrences.",
            "And then there's some other work that a couple."
        ],
        [
            "Other students have done here, so let's dive."
        ],
        [
            "To the approach.",
            "OK, so first of all, we're just dealing with Boolean data, right?",
            "That's the first step, and in fact, this presentation is really focused just on on text data.",
            "That's why we're in the text mining thing here, so we're looking at a simple representation where you've got objects which we documents and features, which would be whatever words or entities, or whatever you like an on the right.",
            "You see, there's a little table.",
            "It shows the usual, you know.",
            "Basically, in this case, we've got documents listed on the Rose, and the features are in the columns.",
            "OK, so so this is basic."
        ],
        [
            "The formal definition of the representation.",
            "First of all, we wanted next thing to define is a first order path, and here you're looking at just cooccurrence.",
            "I mean, I think everybody in the room is quite familiar with what Co occurrences don't really need to define it, but just so you understand, we're looking at the occurrence of feature ion feature K in object or document L."
        ],
        [
            "So that's the 1st order, so, so what's a second order path?",
            "Well, it's a second order cooccurrence.",
            "These also have a long history.",
            "Lots of people have looked at 2nd order and higher order Co occurrences in a number of different contexts.",
            "So 2nd order path we're just looking at three features now.",
            "We've got INK that Co occur together in document or object L and then K an J. Co occur in some other context and you'll notice in our particular definition we actually don't disallow.",
            "I and.",
            "We do disallow Inj to Co occur actually in a first order Co occurrence.",
            "In the definition that we."
        ],
        [
            "There were working with here.",
            "OK, so this just gives you a brief graphic graphical picture of of what 1st order versus 2nd order looks like the the the lighter color that well the black are zeros.",
            "OK so when you look at at 1st order cooccurrence it's very very sparse and 2nd order.",
            "However the pictures completely opposite.",
            "So in other words there's a lot of higher order or second order paths in a text."
        ],
        [
            "Old data set.",
            "OK, so.",
            "Here's how we characterize probabilistic Lee Ann.",
            "We're starting with with a typical Bayesian approach here, but in bays you take your terms and you count how many times they occur in in your training data, and that gives you your prior.",
            "So we just did a little twist on that and we said OK instead of counting on getting the prior from a direct count in the document space, we're going to actually form this higher order path space, and then we're going to get the prior from that space.",
            "So we're going to count events, which could be a term or whatever in the higher order pass space instead of in the document space.",
            "So that's the basic found."
        ],
        [
            "Station for this Ann.",
            "You know, naturally, just like in Bayes you know you get your class priors.",
            "Well, we can get a class prior to and the class prior here is simply based on the number of paths that the term or the feature occurs in divided by the total number of pads in that particular class.",
            "OK, so if you're comfortable and familiar with naive Bayes, then this this is.",
            "This is analogous just doing a space."
        ],
        [
            "Transform.",
            "OK, so with that that was that previous slide was it was approach we called higher order naive Bayes because we changed the way we get the prior.",
            "So we wanted to generalize this to work with discriminative learners too.",
            "So we developed the transform on the data itself so we're not transforming the algorithm here.",
            "We're transforming the data and basically you can see in the upper half of the slide there it's a log likelihood of the higher order of probability we're going to do a little bit more here.",
            "We've got this.",
            "Normalization term here, which is just just a heuristic, and I'll explain why we do that here in a minute.",
            "But the bottom line is that will take this this binary data ones and zeros in the document set, and we're going to apply this transform to it, which which will then prepare it for use in virtually any discriminative learner like logistic regression or support vector machine.",
            "Whatever?",
            "OK, so really two different things.",
            "We're looking at higher order naive Bayes and then we did a general transform and we're going to.",
            "Show some results for how."
        ],
        [
            "So it works with the discriminative learner.",
            "OK, so we're down to the."
        ],
        [
            "Notes here's our experimental setup.",
            "We used SVM, 'cause it's well known for to be stable forms and have good performance, especially with text, high dimensional, sparse text data.",
            "We we use multi class classification.",
            "Notice that this representation in the data transform you know requires it's you know it's a log likelihood ratio, right?",
            "So you can only have two classes represented there, so we have to use one against one or some other scheme, one against all or one against one.",
            "We just won against one in the SVM we gave it every chance we could.",
            "I mean we used several different kernels we use.",
            "We optimized the soft margin cost across about 9 different values and so we gave SBM every opportunity to do its best in these experiments.",
            "We varied the training set size we we were.",
            "We were primarily interested in in in online learning case where we've got a very small amount of training data or where we've got a very small amount of labeled data.",
            "So we vary the training size from 5%."
        ],
        [
            "Up to 60%.",
            "These are the benchmarks that are reported in this paper.",
            "4 ECML this time.",
            "The usual suspects 20 newsgroups excites here, etc.",
            "We did some preprocessing.",
            "In this case, we're actually comparing to some prior work by Sloan and Tishby, which used word cluster to transform the feature space to try and generalize the model better, and so this is some of this is coming from the need to compare to them.",
            "But anyway, we took for the news groups in particular, we processed them and we selected eight sets randomly selected eight sets of 500 documents each.",
            "Work for compare."
        ],
        [
            "So the numbers you're looking at in this slide are actually all based, each.",
            "Each one of those data points is based on eight runs, and then you're looking at an average there.",
            "So here's the six different datasets that are presented in the paper, and it's a little hard to see.",
            "I'm sure you can't see it in the back, but basically there's four.",
            "First of all, there's four lines on each of these graphs, and the X axis is showing the percent of the training data that we took all the way from 5%.",
            "Like I said, up to 60%.",
            "An we started, you know when we train with 5% it means we test on the rest.",
            "So if we train with 10% we test on 90 and so that the accuracy is presented on the Y axis and these charts I'm just showing accuracy.",
            "You know we used several different metrics including precision recall and F beta and the trends are similar.",
            "So what are we looking at?",
            "Well for each of those lines we've got several data points.",
            "Excuse me an the higher order naive Bayes is in the 20 newsgroups, higher tonight bases leading OK, that's the top line that you can see in these charts.",
            "So it basically beat everything and these results here for Howard and I base are all statistically significant, at least at 5% or less.",
            "So that's higher than I based the next runner up, which is the second line that you see in most of these, is the higher order SVM.",
            "That's where we took transformed the data.",
            "With this this this transform to a higher order space and then just plug that into an SVM and optimized it, and then the trailer at the very bottom.",
            "In almost all these graphs is just plain a base and then the SVM actually does.",
            "It doesn't do as well and in the 20 newsgroups it does a little better in the in the sites here and Cora but.",
            "I'll show you a drill down now."
        ],
        [
            "Where you can.",
            "See, these are just a drill down just for the 5% sample size.",
            "What are the results look like?",
            "So again we got six rows for the six datasets here and then on the left half of this table we're comparing naive Bayes with higher naive Bayes, and then there's a thick dashed line in the middle and on the right half is SVM versus higher order SVM, and so the BF show the which one had the largest absolute performance an as I mentioned, for higher order naive Bayes it was.",
            "Statistically significant, at least a 5% across all experiments.",
            "The higher order SVM was also statistically significantly better than SVM in in five out of the six cases.",
            "For this.",
            "For these particular datasets, and there's a few more details here and also in the paper."
        ],
        [
            "OK, so this is really to me.",
            "This is the most interesting slide.",
            "I hope, maybe not, maybe for you too, but what we did here is we're trying to figure out why.",
            "Like you know, Scheiben David yesterday talked about the fact that there's more theory needed, right?",
            "And so we're trying to figure out why, you know, why does this work?",
            "And this is kind of gives a little bit of a clue what you're looking at here.",
            "Let's look at the graph on the left.",
            "Right now we're looking at.",
            "Here is a plot of the 5% sample from the religion data.",
            "Two of the classes only.",
            "I think it was all to Christianity and Alt Dot atheism and.",
            "Each little circle is is a term.",
            "It's one of the terms from from those classes, and if you follow over to the Y axis just just just pick a term followed to the Y axis, you'll get the naive Bayes log likelihood that the usual way of doing it if you drop down to the X axis, you get the higher order.",
            "Probability the log likelihood with a higher order probability ratio, so that's that's the plot.",
            "Now you notice it's very interesting, because if you project everything off onto the Y axis, there's really not much discrimination there if you project it down to the X axis, there's a lot of discrimination.",
            "Why it's because the stuff here on the lower left is all those terms are in one class.",
            "They are in one of the two classes and the stuff in the upper rides all in the other class.",
            "So it turns out that the log likelihood of the higher order probabilities really pulls apart the feature space really, really nicely.",
            "In fact, if you look at it, the X axis ranges from about minus four, up to four.",
            "OK, I think that's a typical ratio.",
            "A typical range for the log likelihood ratios for naive Bayes.",
            "Here we're going from minus 20.",
            "Up to 20 OK talking five times.",
            "Greater log likelihood ratio.",
            "So it's a much stronger discrimination in this case.",
            "So, so this is really kind of a clue of between behind what's happening on the right hand.",
            "What you're seeing is the same thing, but what we've done.",
            "Remember, I said there was a little normalization factor that we put into the transform for discriminative learners.",
            "Well, that's after applying that factor.",
            "So what does it do?",
            "Well, it leaves us with good discrimination between the classes, but the chunk of terms in the middle which are shared, those we can leverage a little bit better.",
            "OK, now I've got some.",
            "Some animations here which kind of show the same thing we're looking at.",
            "You know discriminators.",
            "As I said, for the two classes, and if I do that projection onto the Y axis for naive Bayes and onto the X axis for higher tonight, Bayes you clearly see the distribution of of the you know the terms there, so we're getting some very good discrimination out of this.",
            "Now, what this these slides are showing is if we start at 95%.",
            "Training that means almost the whole thing just for training, then this is the picture that we get.",
            "Again, this is for the same 5% sample from the legend.",
            "But as we go down now we're going to scale down from 95% right on down to 5%.",
            "And there you see 90 percent 85 up at the top.",
            "It's going down 70.",
            "And notice that despite the fact that we're steadily reducing the amount of training data that we're still keeping that discrimination constant.",
            "So the higher order information is actually quite valuable in terms of discrimination, right down to 5%.",
            "Now in this case, what is 5%?",
            "Well, it was 500 sample size, so."
        ],
        [
            "About 25 documents per class.",
            "So you know, in summary, these features give really powerful discrimination, especially when you've got very small training samples.",
            "And of course you know anybody can detect a good discriminator if it just occurs once in a given class.",
            "The problem is of course, what do you wait?",
            "Do you give it?",
            "And that's really the power here.",
            "Is that this really gives evidently gives a very good."
        ],
        [
            "A good way to good Pryor."
        ],
        [
            "OK, so in conclusion.",
            "There's a bunch of things we've done.",
            "Basically, we've got a couple of different approaches.",
            "The higher order naive Bayes approach, as well as this transform for discriminative learners.",
            "That second bullet is kind of a summary of how things you know of all the results.",
            "We found that it the higher transform whether it was HOSVM or H1B never performed worse than its competitor, and most of the time beat it at a statistically significant performance of less than or equal to 5% level.",
            "We compared it to Sloan and Tippy who used word clustering to try and transform the feature space.",
            "Very, very similar goals with small.",
            "Amount of training data?",
            "How do you transform the space an?",
            "In that case they had compared to a multinomial naive Bayes baseline, whereas ours is of course binomial.",
            "So we what we did is we just said what's the improvement over the baseline respectively, and we did about.",
            "I think we do about twice as good as they did anyway, the details are in the paper or please stop by the poster session.",
            "I think it's tomorrow we tried some other interesting things we've looked at.",
            "Just Co occurrences or I'm in fact lots of people look at trying to use Co occurrences to build a learning to build a model.",
            "And it just doesn't work.",
            "That's the bottom line.",
            "It just doesn't work.",
            "We also did something else interesting.",
            "We looked at orders higher than two.",
            "And we ask the questions that help, and it didn't make a statistically significant difference is something else we tried as we looked at, we looked at 2nd order disallowing 1st order, disallowing Co occurrence which which I mentioned and believe it or not that also didn't make any difference.",
            "If we disallow 1st order or even if we allow it, as long as there was second order.",
            "So that basically the outcome, the final conclusion is that the 2nd order is where the information is coming from.",
            "It's this second order information that's the valuable information.",
            "We've used this in.",
            "In a couple of other cases, to classify BGP data there was one reference earlier there.",
            "We've also used it to classify radio nuclear isotopes.",
            "This is a border crossing a border crossing security related application, Market Basket analysis and then most recently in community discovery in social networks.",
            "So I mean some of them."
        ],
        [
            "Future work.",
            "Here's the research team.",
            "Give them some credit.",
            "Actually, Nikita is on the far right there.",
            "He's he's about to defend his PhD next Friday.",
            "I think it is.",
            "So.",
            "That some of the key is actually developed and, uh."
        ],
        [
            "Supervised transform as part of his ongoing PhD work, where we do the same higher order transform in an unsupervised way which is pretty interesting.",
            "These are just some of the collaborators.",
            "Of course.",
            "You know this is work that was done at Rutgers University in New Jersey, so most of the collaborations are with.",
            "I mean in fact, all of them are with, you know."
        ],
        [
            "Organizations in the US.",
            "OK, that's it.",
            "Questions.",
            "So.",
            "Kind of 2nd order issues since.",
            "Capture and also by LSA.",
            "So you can spend some more due to clarify.",
            "OK, sure, so the question was how does LSA or LSI leverage this hierarchical currents in at least for part?",
            "I mean there's been a lot of people that have developed theoretical models for LSA, right?",
            "I mean lots of papers right and so the work I did with April was, you know, it's just another way to understand LSA, not the only way.",
            "It's just another way.",
            "But it turns out that we were able to prove theoretically.",
            "That in in a reduced representation where for any value of K, if you've got an entry in the reduced representation that was originally zero, that means there's no first order cooccurrence.",
            "Then we've proved that there must exist a higher order path, and that's actually kind of what sparked this work in learning is 'cause we looked at LSA and we said, you know, it works sometimes and not other times.",
            "Obviously there's something going on here with higher order relations with statistical learning, and so we thought.",
            "How can we put it down?",
            "Can we have?",
            "Sort of extract the key order.",
            "That's important was one question, 'cause if LSA that theorem proved it was higher order didn't prove what order.",
            "We also had some empirical evidence that showed that LSA most strongly depends on those higher order currencies.",
            "We want to show that as well.",
            "Next question in the back.",
            "I saw somebody put a hand up.",
            "OK yes please.",
            "Rubbish diffusion, We are graph based.",
            "OK. Possible I considered.",
            "Yeah, that's an interesting idea.",
            "In fact, we've done a little bit of work thinking about what's the relationship with kernel methods.",
            "In this case, you know that's a particular kernel method, I think, which is, which should be interesting to look at kernel methods in general have the problem that they do transform the space, but you don't know it's to an unknown dimensionality, right?",
            "So you really lose the interpretation of that space, and that's one advantage.",
            "At least we think intuitively of this Transformers that we actually maintain the understanding of the space.",
            "Transform.",
            "OK. Sure, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think we can start, so we will now.",
                    "label": 0
                },
                {
                    "sent": "Higher order features for text application and I think they should be given by really fast enough.",
                    "label": 1
                },
                {
                    "sent": "So yeah, good morning puncher, sorry.",
                    "label": 0
                },
                {
                    "sent": "I guess you folks didn't know where I was, but I actually knew I was so OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your patience.",
                    "label": 0
                },
                {
                    "sent": "OK so this is work joint work with a couple of my students, Nikita Lipkin and Morocco knees.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're addressing the problem where you've got the old problem of how do you label enough training data to learn a model.",
                    "label": 0
                },
                {
                    "sent": "So this has applications naturally with online learning and one of the issues is the traditional learning approaches classify a single instance at a time, unlike collective classification.",
                    "label": 0
                },
                {
                    "sent": "So we've got to deal with even less.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've got an issue with even less training data, so here's an outline of what I want to talk about today.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just briefly, some of the related work.",
                    "label": 0
                },
                {
                    "sent": "The basic issue here is that the related work in statistical relational learning task are Lisa Couture and others have shown that if you leverage the relationships between features or between entities or attributes, you can build better models, so that's that's the basic message here.",
                    "label": 0
                },
                {
                    "sent": "Active classifiers are have done a lot of good things.",
                    "label": 0
                },
                {
                    "sent": "We're trying to achieve the performance equal to a collective classifier, but with a classifier that can do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For instance classification.",
                    "label": 0
                },
                {
                    "sent": "There's some more interesting related work to our particular approach.",
                    "label": 0
                },
                {
                    "sent": "This first paper goes all the way back to 1953.",
                    "label": 0
                },
                {
                    "sent": "I don't know if any of you familiar with cats is work.",
                    "label": 0
                },
                {
                    "sent": "Early work where he looked at leveraging multiple paths between nodes in a graph in order to identify social relations, and then one of my former PhD students.",
                    "label": 0
                },
                {
                    "sent": "She and I looked at LSI latent semantic indexing, otherwise called latent semantic analysis, and we were able to prove mathematically that LSI actually depends.",
                    "label": 0
                },
                {
                    "sent": "On higher order pads or higher order Co occurrences.",
                    "label": 0
                },
                {
                    "sent": "And then there's some other work that a couple.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other students have done here, so let's dive.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the approach.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, we're just dealing with Boolean data, right?",
                    "label": 0
                },
                {
                    "sent": "That's the first step, and in fact, this presentation is really focused just on on text data.",
                    "label": 0
                },
                {
                    "sent": "That's why we're in the text mining thing here, so we're looking at a simple representation where you've got objects which we documents and features, which would be whatever words or entities, or whatever you like an on the right.",
                    "label": 0
                },
                {
                    "sent": "You see, there's a little table.",
                    "label": 0
                },
                {
                    "sent": "It shows the usual, you know.",
                    "label": 0
                },
                {
                    "sent": "Basically, in this case, we've got documents listed on the Rose, and the features are in the columns.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is basic.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The formal definition of the representation.",
                    "label": 0
                },
                {
                    "sent": "First of all, we wanted next thing to define is a first order path, and here you're looking at just cooccurrence.",
                    "label": 1
                },
                {
                    "sent": "I mean, I think everybody in the room is quite familiar with what Co occurrences don't really need to define it, but just so you understand, we're looking at the occurrence of feature ion feature K in object or document L.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the 1st order, so, so what's a second order path?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a second order cooccurrence.",
                    "label": 1
                },
                {
                    "sent": "These also have a long history.",
                    "label": 0
                },
                {
                    "sent": "Lots of people have looked at 2nd order and higher order Co occurrences in a number of different contexts.",
                    "label": 0
                },
                {
                    "sent": "So 2nd order path we're just looking at three features now.",
                    "label": 0
                },
                {
                    "sent": "We've got INK that Co occur together in document or object L and then K an J. Co occur in some other context and you'll notice in our particular definition we actually don't disallow.",
                    "label": 0
                },
                {
                    "sent": "I and.",
                    "label": 1
                },
                {
                    "sent": "We do disallow Inj to Co occur actually in a first order Co occurrence.",
                    "label": 0
                },
                {
                    "sent": "In the definition that we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There were working with here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this just gives you a brief graphic graphical picture of of what 1st order versus 2nd order looks like the the the lighter color that well the black are zeros.",
                    "label": 0
                },
                {
                    "sent": "OK so when you look at at 1st order cooccurrence it's very very sparse and 2nd order.",
                    "label": 0
                },
                {
                    "sent": "However the pictures completely opposite.",
                    "label": 0
                },
                {
                    "sent": "So in other words there's a lot of higher order or second order paths in a text.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Old data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here's how we characterize probabilistic Lee Ann.",
                    "label": 0
                },
                {
                    "sent": "We're starting with with a typical Bayesian approach here, but in bays you take your terms and you count how many times they occur in in your training data, and that gives you your prior.",
                    "label": 0
                },
                {
                    "sent": "So we just did a little twist on that and we said OK instead of counting on getting the prior from a direct count in the document space, we're going to actually form this higher order path space, and then we're going to get the prior from that space.",
                    "label": 0
                },
                {
                    "sent": "So we're going to count events, which could be a term or whatever in the higher order pass space instead of in the document space.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic found.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station for this Ann.",
                    "label": 0
                },
                {
                    "sent": "You know, naturally, just like in Bayes you know you get your class priors.",
                    "label": 0
                },
                {
                    "sent": "Well, we can get a class prior to and the class prior here is simply based on the number of paths that the term or the feature occurs in divided by the total number of pads in that particular class.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're comfortable and familiar with naive Bayes, then this this is.",
                    "label": 0
                },
                {
                    "sent": "This is analogous just doing a space.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transform.",
                    "label": 0
                },
                {
                    "sent": "OK, so with that that was that previous slide was it was approach we called higher order naive Bayes because we changed the way we get the prior.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to generalize this to work with discriminative learners too.",
                    "label": 0
                },
                {
                    "sent": "So we developed the transform on the data itself so we're not transforming the algorithm here.",
                    "label": 0
                },
                {
                    "sent": "We're transforming the data and basically you can see in the upper half of the slide there it's a log likelihood of the higher order of probability we're going to do a little bit more here.",
                    "label": 0
                },
                {
                    "sent": "We've got this.",
                    "label": 0
                },
                {
                    "sent": "Normalization term here, which is just just a heuristic, and I'll explain why we do that here in a minute.",
                    "label": 0
                },
                {
                    "sent": "But the bottom line is that will take this this binary data ones and zeros in the document set, and we're going to apply this transform to it, which which will then prepare it for use in virtually any discriminative learner like logistic regression or support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Whatever?",
                    "label": 0
                },
                {
                    "sent": "OK, so really two different things.",
                    "label": 0
                },
                {
                    "sent": "We're looking at higher order naive Bayes and then we did a general transform and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Show some results for how.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it works with the discriminative learner.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're down to the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notes here's our experimental setup.",
                    "label": 1
                },
                {
                    "sent": "We used SVM, 'cause it's well known for to be stable forms and have good performance, especially with text, high dimensional, sparse text data.",
                    "label": 0
                },
                {
                    "sent": "We we use multi class classification.",
                    "label": 0
                },
                {
                    "sent": "Notice that this representation in the data transform you know requires it's you know it's a log likelihood ratio, right?",
                    "label": 0
                },
                {
                    "sent": "So you can only have two classes represented there, so we have to use one against one or some other scheme, one against all or one against one.",
                    "label": 0
                },
                {
                    "sent": "We just won against one in the SVM we gave it every chance we could.",
                    "label": 0
                },
                {
                    "sent": "I mean we used several different kernels we use.",
                    "label": 0
                },
                {
                    "sent": "We optimized the soft margin cost across about 9 different values and so we gave SBM every opportunity to do its best in these experiments.",
                    "label": 1
                },
                {
                    "sent": "We varied the training set size we we were.",
                    "label": 1
                },
                {
                    "sent": "We were primarily interested in in in online learning case where we've got a very small amount of training data or where we've got a very small amount of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So we vary the training size from 5%.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up to 60%.",
                    "label": 0
                },
                {
                    "sent": "These are the benchmarks that are reported in this paper.",
                    "label": 0
                },
                {
                    "sent": "4 ECML this time.",
                    "label": 0
                },
                {
                    "sent": "The usual suspects 20 newsgroups excites here, etc.",
                    "label": 0
                },
                {
                    "sent": "We did some preprocessing.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're actually comparing to some prior work by Sloan and Tishby, which used word cluster to transform the feature space to try and generalize the model better, and so this is some of this is coming from the need to compare to them.",
                    "label": 0
                },
                {
                    "sent": "But anyway, we took for the news groups in particular, we processed them and we selected eight sets randomly selected eight sets of 500 documents each.",
                    "label": 0
                },
                {
                    "sent": "Work for compare.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the numbers you're looking at in this slide are actually all based, each.",
                    "label": 0
                },
                {
                    "sent": "Each one of those data points is based on eight runs, and then you're looking at an average there.",
                    "label": 0
                },
                {
                    "sent": "So here's the six different datasets that are presented in the paper, and it's a little hard to see.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you can't see it in the back, but basically there's four.",
                    "label": 0
                },
                {
                    "sent": "First of all, there's four lines on each of these graphs, and the X axis is showing the percent of the training data that we took all the way from 5%.",
                    "label": 0
                },
                {
                    "sent": "Like I said, up to 60%.",
                    "label": 0
                },
                {
                    "sent": "An we started, you know when we train with 5% it means we test on the rest.",
                    "label": 0
                },
                {
                    "sent": "So if we train with 10% we test on 90 and so that the accuracy is presented on the Y axis and these charts I'm just showing accuracy.",
                    "label": 0
                },
                {
                    "sent": "You know we used several different metrics including precision recall and F beta and the trends are similar.",
                    "label": 0
                },
                {
                    "sent": "So what are we looking at?",
                    "label": 0
                },
                {
                    "sent": "Well for each of those lines we've got several data points.",
                    "label": 0
                },
                {
                    "sent": "Excuse me an the higher order naive Bayes is in the 20 newsgroups, higher tonight bases leading OK, that's the top line that you can see in these charts.",
                    "label": 0
                },
                {
                    "sent": "So it basically beat everything and these results here for Howard and I base are all statistically significant, at least at 5% or less.",
                    "label": 0
                },
                {
                    "sent": "So that's higher than I based the next runner up, which is the second line that you see in most of these, is the higher order SVM.",
                    "label": 0
                },
                {
                    "sent": "That's where we took transformed the data.",
                    "label": 0
                },
                {
                    "sent": "With this this this transform to a higher order space and then just plug that into an SVM and optimized it, and then the trailer at the very bottom.",
                    "label": 0
                },
                {
                    "sent": "In almost all these graphs is just plain a base and then the SVM actually does.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do as well and in the 20 newsgroups it does a little better in the in the sites here and Cora but.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a drill down now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you can.",
                    "label": 0
                },
                {
                    "sent": "See, these are just a drill down just for the 5% sample size.",
                    "label": 0
                },
                {
                    "sent": "What are the results look like?",
                    "label": 0
                },
                {
                    "sent": "So again we got six rows for the six datasets here and then on the left half of this table we're comparing naive Bayes with higher naive Bayes, and then there's a thick dashed line in the middle and on the right half is SVM versus higher order SVM, and so the BF show the which one had the largest absolute performance an as I mentioned, for higher order naive Bayes it was.",
                    "label": 0
                },
                {
                    "sent": "Statistically significant, at least a 5% across all experiments.",
                    "label": 0
                },
                {
                    "sent": "The higher order SVM was also statistically significantly better than SVM in in five out of the six cases.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                },
                {
                    "sent": "For these particular datasets, and there's a few more details here and also in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is really to me.",
                    "label": 0
                },
                {
                    "sent": "This is the most interesting slide.",
                    "label": 0
                },
                {
                    "sent": "I hope, maybe not, maybe for you too, but what we did here is we're trying to figure out why.",
                    "label": 0
                },
                {
                    "sent": "Like you know, Scheiben David yesterday talked about the fact that there's more theory needed, right?",
                    "label": 0
                },
                {
                    "sent": "And so we're trying to figure out why, you know, why does this work?",
                    "label": 0
                },
                {
                    "sent": "And this is kind of gives a little bit of a clue what you're looking at here.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the graph on the left.",
                    "label": 0
                },
                {
                    "sent": "Right now we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Here is a plot of the 5% sample from the religion data.",
                    "label": 0
                },
                {
                    "sent": "Two of the classes only.",
                    "label": 0
                },
                {
                    "sent": "I think it was all to Christianity and Alt Dot atheism and.",
                    "label": 0
                },
                {
                    "sent": "Each little circle is is a term.",
                    "label": 0
                },
                {
                    "sent": "It's one of the terms from from those classes, and if you follow over to the Y axis just just just pick a term followed to the Y axis, you'll get the naive Bayes log likelihood that the usual way of doing it if you drop down to the X axis, you get the higher order.",
                    "label": 0
                },
                {
                    "sent": "Probability the log likelihood with a higher order probability ratio, so that's that's the plot.",
                    "label": 0
                },
                {
                    "sent": "Now you notice it's very interesting, because if you project everything off onto the Y axis, there's really not much discrimination there if you project it down to the X axis, there's a lot of discrimination.",
                    "label": 0
                },
                {
                    "sent": "Why it's because the stuff here on the lower left is all those terms are in one class.",
                    "label": 0
                },
                {
                    "sent": "They are in one of the two classes and the stuff in the upper rides all in the other class.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the log likelihood of the higher order probabilities really pulls apart the feature space really, really nicely.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you look at it, the X axis ranges from about minus four, up to four.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that's a typical ratio.",
                    "label": 0
                },
                {
                    "sent": "A typical range for the log likelihood ratios for naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Here we're going from minus 20.",
                    "label": 0
                },
                {
                    "sent": "Up to 20 OK talking five times.",
                    "label": 0
                },
                {
                    "sent": "Greater log likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "So it's a much stronger discrimination in this case.",
                    "label": 0
                },
                {
                    "sent": "So, so this is really kind of a clue of between behind what's happening on the right hand.",
                    "label": 0
                },
                {
                    "sent": "What you're seeing is the same thing, but what we've done.",
                    "label": 0
                },
                {
                    "sent": "Remember, I said there was a little normalization factor that we put into the transform for discriminative learners.",
                    "label": 0
                },
                {
                    "sent": "Well, that's after applying that factor.",
                    "label": 0
                },
                {
                    "sent": "So what does it do?",
                    "label": 0
                },
                {
                    "sent": "Well, it leaves us with good discrimination between the classes, but the chunk of terms in the middle which are shared, those we can leverage a little bit better.",
                    "label": 0
                },
                {
                    "sent": "OK, now I've got some.",
                    "label": 0
                },
                {
                    "sent": "Some animations here which kind of show the same thing we're looking at.",
                    "label": 0
                },
                {
                    "sent": "You know discriminators.",
                    "label": 0
                },
                {
                    "sent": "As I said, for the two classes, and if I do that projection onto the Y axis for naive Bayes and onto the X axis for higher tonight, Bayes you clearly see the distribution of of the you know the terms there, so we're getting some very good discrimination out of this.",
                    "label": 0
                },
                {
                    "sent": "Now, what this these slides are showing is if we start at 95%.",
                    "label": 0
                },
                {
                    "sent": "Training that means almost the whole thing just for training, then this is the picture that we get.",
                    "label": 0
                },
                {
                    "sent": "Again, this is for the same 5% sample from the legend.",
                    "label": 0
                },
                {
                    "sent": "But as we go down now we're going to scale down from 95% right on down to 5%.",
                    "label": 0
                },
                {
                    "sent": "And there you see 90 percent 85 up at the top.",
                    "label": 0
                },
                {
                    "sent": "It's going down 70.",
                    "label": 0
                },
                {
                    "sent": "And notice that despite the fact that we're steadily reducing the amount of training data that we're still keeping that discrimination constant.",
                    "label": 0
                },
                {
                    "sent": "So the higher order information is actually quite valuable in terms of discrimination, right down to 5%.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, what is 5%?",
                    "label": 0
                },
                {
                    "sent": "Well, it was 500 sample size, so.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About 25 documents per class.",
                    "label": 0
                },
                {
                    "sent": "So you know, in summary, these features give really powerful discrimination, especially when you've got very small training samples.",
                    "label": 0
                },
                {
                    "sent": "And of course you know anybody can detect a good discriminator if it just occurs once in a given class.",
                    "label": 0
                },
                {
                    "sent": "The problem is of course, what do you wait?",
                    "label": 0
                },
                {
                    "sent": "Do you give it?",
                    "label": 0
                },
                {
                    "sent": "And that's really the power here.",
                    "label": 0
                },
                {
                    "sent": "Is that this really gives evidently gives a very good.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A good way to good Pryor.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of things we've done.",
                    "label": 0
                },
                {
                    "sent": "Basically, we've got a couple of different approaches.",
                    "label": 0
                },
                {
                    "sent": "The higher order naive Bayes approach, as well as this transform for discriminative learners.",
                    "label": 1
                },
                {
                    "sent": "That second bullet is kind of a summary of how things you know of all the results.",
                    "label": 0
                },
                {
                    "sent": "We found that it the higher transform whether it was HOSVM or H1B never performed worse than its competitor, and most of the time beat it at a statistically significant performance of less than or equal to 5% level.",
                    "label": 1
                },
                {
                    "sent": "We compared it to Sloan and Tippy who used word clustering to try and transform the feature space.",
                    "label": 0
                },
                {
                    "sent": "Very, very similar goals with small.",
                    "label": 0
                },
                {
                    "sent": "Amount of training data?",
                    "label": 0
                },
                {
                    "sent": "How do you transform the space an?",
                    "label": 1
                },
                {
                    "sent": "In that case they had compared to a multinomial naive Bayes baseline, whereas ours is of course binomial.",
                    "label": 0
                },
                {
                    "sent": "So we what we did is we just said what's the improvement over the baseline respectively, and we did about.",
                    "label": 1
                },
                {
                    "sent": "I think we do about twice as good as they did anyway, the details are in the paper or please stop by the poster session.",
                    "label": 0
                },
                {
                    "sent": "I think it's tomorrow we tried some other interesting things we've looked at.",
                    "label": 0
                },
                {
                    "sent": "Just Co occurrences or I'm in fact lots of people look at trying to use Co occurrences to build a learning to build a model.",
                    "label": 0
                },
                {
                    "sent": "And it just doesn't work.",
                    "label": 0
                },
                {
                    "sent": "That's the bottom line.",
                    "label": 0
                },
                {
                    "sent": "It just doesn't work.",
                    "label": 0
                },
                {
                    "sent": "We also did something else interesting.",
                    "label": 0
                },
                {
                    "sent": "We looked at orders higher than two.",
                    "label": 0
                },
                {
                    "sent": "And we ask the questions that help, and it didn't make a statistically significant difference is something else we tried as we looked at, we looked at 2nd order disallowing 1st order, disallowing Co occurrence which which I mentioned and believe it or not that also didn't make any difference.",
                    "label": 0
                },
                {
                    "sent": "If we disallow 1st order or even if we allow it, as long as there was second order.",
                    "label": 1
                },
                {
                    "sent": "So that basically the outcome, the final conclusion is that the 2nd order is where the information is coming from.",
                    "label": 0
                },
                {
                    "sent": "It's this second order information that's the valuable information.",
                    "label": 0
                },
                {
                    "sent": "We've used this in.",
                    "label": 0
                },
                {
                    "sent": "In a couple of other cases, to classify BGP data there was one reference earlier there.",
                    "label": 0
                },
                {
                    "sent": "We've also used it to classify radio nuclear isotopes.",
                    "label": 1
                },
                {
                    "sent": "This is a border crossing a border crossing security related application, Market Basket analysis and then most recently in community discovery in social networks.",
                    "label": 0
                },
                {
                    "sent": "So I mean some of them.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future work.",
                    "label": 0
                },
                {
                    "sent": "Here's the research team.",
                    "label": 0
                },
                {
                    "sent": "Give them some credit.",
                    "label": 0
                },
                {
                    "sent": "Actually, Nikita is on the far right there.",
                    "label": 0
                },
                {
                    "sent": "He's he's about to defend his PhD next Friday.",
                    "label": 0
                },
                {
                    "sent": "I think it is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That some of the key is actually developed and, uh.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supervised transform as part of his ongoing PhD work, where we do the same higher order transform in an unsupervised way which is pretty interesting.",
                    "label": 0
                },
                {
                    "sent": "These are just some of the collaborators.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "You know this is work that was done at Rutgers University in New Jersey, so most of the collaborations are with.",
                    "label": 0
                },
                {
                    "sent": "I mean in fact, all of them are with, you know.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Organizations in the US.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Kind of 2nd order issues since.",
                    "label": 0
                },
                {
                    "sent": "Capture and also by LSA.",
                    "label": 0
                },
                {
                    "sent": "So you can spend some more due to clarify.",
                    "label": 0
                },
                {
                    "sent": "OK, sure, so the question was how does LSA or LSI leverage this hierarchical currents in at least for part?",
                    "label": 0
                },
                {
                    "sent": "I mean there's been a lot of people that have developed theoretical models for LSA, right?",
                    "label": 0
                },
                {
                    "sent": "I mean lots of papers right and so the work I did with April was, you know, it's just another way to understand LSA, not the only way.",
                    "label": 0
                },
                {
                    "sent": "It's just another way.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that we were able to prove theoretically.",
                    "label": 0
                },
                {
                    "sent": "That in in a reduced representation where for any value of K, if you've got an entry in the reduced representation that was originally zero, that means there's no first order cooccurrence.",
                    "label": 0
                },
                {
                    "sent": "Then we've proved that there must exist a higher order path, and that's actually kind of what sparked this work in learning is 'cause we looked at LSA and we said, you know, it works sometimes and not other times.",
                    "label": 0
                },
                {
                    "sent": "Obviously there's something going on here with higher order relations with statistical learning, and so we thought.",
                    "label": 0
                },
                {
                    "sent": "How can we put it down?",
                    "label": 0
                },
                {
                    "sent": "Can we have?",
                    "label": 0
                },
                {
                    "sent": "Sort of extract the key order.",
                    "label": 0
                },
                {
                    "sent": "That's important was one question, 'cause if LSA that theorem proved it was higher order didn't prove what order.",
                    "label": 0
                },
                {
                    "sent": "We also had some empirical evidence that showed that LSA most strongly depends on those higher order currencies.",
                    "label": 0
                },
                {
                    "sent": "We want to show that as well.",
                    "label": 0
                },
                {
                    "sent": "Next question in the back.",
                    "label": 0
                },
                {
                    "sent": "I saw somebody put a hand up.",
                    "label": 0
                },
                {
                    "sent": "OK yes please.",
                    "label": 0
                },
                {
                    "sent": "Rubbish diffusion, We are graph based.",
                    "label": 0
                },
                {
                    "sent": "OK. Possible I considered.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's an interesting idea.",
                    "label": 0
                },
                {
                    "sent": "In fact, we've done a little bit of work thinking about what's the relationship with kernel methods.",
                    "label": 0
                },
                {
                    "sent": "In this case, you know that's a particular kernel method, I think, which is, which should be interesting to look at kernel methods in general have the problem that they do transform the space, but you don't know it's to an unknown dimensionality, right?",
                    "label": 0
                },
                {
                    "sent": "So you really lose the interpretation of that space, and that's one advantage.",
                    "label": 0
                },
                {
                    "sent": "At least we think intuitively of this Transformers that we actually maintain the understanding of the space.",
                    "label": 0
                },
                {
                    "sent": "Transform.",
                    "label": 0
                },
                {
                    "sent": "OK. Sure, thank you.",
                    "label": 0
                }
            ]
        }
    }
}