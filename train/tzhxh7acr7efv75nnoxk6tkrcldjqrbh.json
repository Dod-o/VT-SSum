{
    "id": "tzhxh7acr7efv75nnoxk6tkrcldjqrbh",
    "title": "Multilayer Neural Networks",
    "info": {
        "author": [
            "L\u00e9on Bottou, Facebook"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_bottou_neural_networks/",
    "segmentation": [
        [
            "So thank you very much.",
            "Under so today I'm going to speak about Matillion, our network, the kind that was.",
            "Celebrated in the 50s, then forgotten in the 70s, then celebrated in the late 80s then.",
            "Mike is off no oh.",
            "No, it says on here.",
            "Oh, I see.",
            "OK, so these things well famous in the 60s forgotten in the 70s, then famous again in the late 80s, then forgotten in the 2000, then famous again now.",
            "So I'm not going to try to explain why, just explain what they are.",
            "And at that point, when I wrote this slide where I had them."
        ],
        [
            "Oh no, I had something about success stories, but now everybody knows it that these things have success stories and an important part that they also had success stories in the 80s.",
            "Like there's some check reading system where diploid and red checks for years with.",
            "Without the new problem.",
            "And then.",
            "Anyway, fine.",
            "So so actually three parts in this lecture, and they're going to be over the next two sessions now and tomorrow."
        ],
        [
            "The first one is what is the story of neural information processing like in nips you know, is neural information processing systems and what are the origins?",
            "And an example that I like very much colder email out position on network.",
            "And how you construct networks and you describe them."
        ],
        [
            "So the second part is how you train them and it's connected to optimization loosely connected to optimization, because we don't really optimize, but how you initialize what, why do stochastic methods and how you could do better."
        ],
        [
            "And the third one, which is a bit of a bonus, is to try to make networks for complicated tasks, like when you have structured problem, auxiliary tasks and things like that.",
            "So I'm going to start with the org."
        ],
        [
            "Since."
        ],
        [
            "And with the beginning, the percept."
        ],
        [
            "Home.",
            "So you probably seen that picture somewhere or picture like that.",
            "One of the things that people forget sometimes that the perceptron had first written now and then what they call the associative area, which is kind of random connection.",
            "Or so it actually had layers.",
            "Going to an associative area, this is known as X.",
            "Multiply by some weight so you complete the dot product of W&X, then the linear threshold, then you get the sign.",
            "It tells you whether.",
            "This is the object you want is a classification system.",
            "And you might think that the perceptron is an algorithm, or the perceptron is a software, but."
        ],
        [
            "It's important to understand that perception is a machine, so this is Rosenblatt.",
            "Is holding 8 weights in his hands.",
            "So each weight is a potential matter with a little electrical engine, and whenever there is a mistake, meaning the perceptron makes a mistake, you press the button and it's a bit all over the place and it adjusts.",
            "So one thing to understand is that the perceptron is not an algorithm that runs on a computer.",
            "The perceptron was the computer.",
            "And it was a cop."
        ],
        [
            "After that could do things that no the computer of the time could do, like recognize characters like this.",
            "You know more than we do image net and things like that, but at that time, well, there is no CCD to image something into your computer.",
            "You had to use something pretty sophisticated and large.",
            "And so, in the view of the authors of the perceptron.",
            "That was not something to run on a computer that was a computer that was an alternative computing model."
        ],
        [
            "And all this goes back to Vienna.",
            "So 1948 you see science fiction orders in science fiction is pretty clear that.",
            "That intelligence things all over the place and all these ID's there were not really.",
            "Present.",
            "In that sense, before Winner is the guy you said.",
            "Well, learning happens everywhere happens in biological systems, in hospitals, in Automatical system that we do, it happens.",
            "In societies he happened in sensitive process and should be studied as such.",
            "And the perception is the continuation of this.",
            "Studying learning as such.",
            "So study learning and information processing were not that separate at that time.",
            "And of course you have the."
        ],
        [
            "Notion of how to design computers.",
            "You could see the biological learning computer which we don't know how it should be done, so I put a little brand here, but that doesn't mean anything.",
            "And the mathematical computer, which is the ones we use nowadays, they're based on mathematical logic.",
            "And that model, the model of building computers on mathematical logic, starting with the Turing machines, the Fundament machines and everything is clearly one there everywhere.",
            "So if you."
        ],
        [
            "Look at this this computing with symbols you know everything you learn about computer science is based about this.",
            "Programming means reducing a complex tasks into a collection of simple tasks.",
            "Computer language has no meaning in the terms of learning.",
            "You don't need to program a learning machine debugging well, you don't need to debug learning machines at any very different from debugging a program or parting systems, libraries API.",
            "All these things now if you."
        ],
        [
            "Look at the brand.",
            "The running machine.",
            "The best learning machine.",
            "We know it's compact is 20 Watts, meaning is less than this laptop's.",
            "Quite a lot of cells and neurons and the volume is mostly wires.",
            "In fact, is very constrained.",
            "In the sense that packing all these wires in such a small volume is a real challenge and there is a cell use bias on the connectivity between units.",
            "Because of this challenge.",
            "So is it a general computing machine like a Turing machine?",
            "Not at all.",
            "It's very slow for mathematical logic, arithmetic and stuff like that.",
            "It's very fast for computer vision, speech languages, social interactions.",
            "And there is a kind of evolution in animals that start applying vision.",
            "Then there is a theory that we developed language by reusing some of the vision mechanism.",
            "And eventually we develop logic by reducing some of the language mechanics.",
            "So let's go on and continue."
        ],
        [
            "So in 43 McCulloch and Pitts invent a simplified model of the neuron.",
            "It's very simplified.",
            "It's ridiculously simplified.",
            "In fact, real neurons have nothing to do with that.",
            "So in this model you have some X one X2X3 this other inputs of the neurons they go inside you compute the sum of WWIXI that product, pass it to some nonlinear function that typically a saturation and you distribute this result to other units.",
            "And the best drone is based on this idea.",
            "There is only one of this unit, the classification unit and the nonlinearity is just a threshold unit.",
            "And people ask of course well, if the perceptron is the computer, can we use the perceptron to do all the things we want to do with the computer?",
            "Just silly question.",
            "The answer is obviously no, but it wasn't that clear."
        ],
        [
            "So it took a book and perceptrons in 68 to actually say that.",
            "And they say that with very elegant mathematics.",
            "And these are the basically the points made in the book, and they're all interesting, because there's still.",
            "Important today.",
            "So the first one is that linear threshold units.",
            "They're very similar to Boolean gates.",
            "Which is one of the building blocks of electronics.",
            "Now the secret theory is very poorly known.",
            "That we don't really much know how many gates we need to.",
            "Construct a certain function or or if you have a flat or deep systems, how many?",
            "How do you compare with the same number of gates?",
            "Flatten deep systems in terms of what they can build.",
            "This is not known very precisely.",
            "That learning a deep secret means solving the credit assignment problem, which is supposed to be quite hard.",
            "That if you don't know deep use, remain to you constraint to solving linearly separable problems that the few of the problems are linearly separable.",
            "I'm going to come back to how this has changed.",
            "What is the credit assignment problem?",
            "Well, you have an arrow at the top.",
            "Of your system at the output, and you want to know who is responsible for this error.",
            "So who should be corrected?",
            "Is it a unit close to the top or you need somewhere in the middle?",
            "And that's a priority.",
            "If you're in the system of Boolean gates, it's very hard to say.",
            "Then that identify no.",
            "Sorry.",
            "I've no idea this is.",
            "But I can't answer.",
            "So the identify a set of elementary problems that need complex circuits within West the best.",
            "None of these problems, the connectivity problem, where you have to decide whether a shape in an image is connected, is composed of a single connected component or not is something that's very difficult to do with learning system, and I'm not going to go into the details, but it's trivial to program.",
            "And the conclusion is, well, all these learning machines, the interesting because they learn.",
            "But what they can do is very limited.",
            "So we should just program computers instead of trying to train them.",
            "Do things and that led to going to symbolic computers and symbolic AI.",
            "So this is not a ridiculous book.",
            "You know it's a very small book in many respects."
        ],
        [
            "But things have changed a little bit.",
            "Secret theory is pulling on.",
            "This is still true or some progress, but not much.",
            "Learning deep secret means solving the credit assignment problems well.",
            "It's easier than expected, but still puzzling.",
            "We still don't know very much why.",
            "Linearly separable problems are few, but that means I have a lot of value judgments dimension and this is supposed to be good for generalization, so we start looking at this aspect as as much a problem as a blessing.",
            "Elementary problems need complex secrets.",
            "Litter Connect city weather.",
            "Bunch of Pixel is a single connected component well.",
            "If you think about the maze and you want to know whether the maze a solution, you can go from the left to the right by going through all the little things that's a connected component problem.",
            "We're not very good at it.",
            "So humans are not very good at that either.",
            "But have a simple organic solutions.",
            "Well sometimes the problem is not specified clearly.",
            "So it's very difficult to program something if you don't have a specification of the problem.",
            "Typical examples recognize the digit four in an image.",
            "You could say, OK, there are two strokes.",
            "Let's find vertical stroke, an L shape stroke such that the L shape stroke crosses the vertical structure in the in the right place, and then you have a problem.",
            "What is a stroke?",
            "And then you have plenty of other images for that car going to be so different that they're not going to fit that thing.",
            "So we need to add new rules and new things you just explode.",
            "But there is still something else that we don't know how to do in learning how to reduce complex learning problems into simple ones.",
            "And that's something I'm going to discuss a little bit at the very end, and it's not something we know very well.",
            "So now."
        ],
        [
            "I'm going to go into.",
            "One of the motivations for neural networks.",
            "That's the story of the regular propositional network, which is likely best described in the paper by McClelland and Rogers in 2003.",
            "Another phone number problem with phones?",
            "And so."
        ],
        [
            "Here is something that from 68, but you probably know today it's called quillons propositional model.",
            "So now it's not called that way is called knowledge graph.",
            "Now if you look in your search engine on the right very often there are some little information about your word.",
            "That's who are the parents.",
            "So look some well known person they're going to tell you the parents and their children, the jobs, the kind of graph.",
            "And so you can have a graph like this, while the top you have a living thing and the relation can grow is living is a animal can move her skin and you build all these triples between left relation right and you have a big table of triples and you have inheritance to like.",
            "Sunfish is yellow, but is a fish.",
            "That four is an animal, therefore Hoskin.",
            "And you can navigate these things so this is one of the tools to make computers.",
            "A little bit more knowledgeable than they used to be.",
            "This tool is a lot of problems."
        ],
        [
            "The first one is that.",
            "If you want to access a specific property should be faster than accessing a generic property.",
            "Like if you go back here, the Sunfish has a skin.",
            "Well, sunfish is yellow.",
            "Use immediately has a skin.",
            "You need to navigate and go there.",
            "And well, there is evidence that humans don't work like that.",
            "Usually if I ask you whether a dog's forelegs?",
            "You can tell me yes immediately for asking whether anomalous forelegs, you need to think about it.",
            "Because you're not so sure.",
            "So we really don't work that way.",
            "The problem #2 is that this graph is very static.",
            "If you want to change things or reorganize things or change the notion of easy, there is a relation little bit like you want to make it less.",
            "Childish and more biological.",
            "You need to reshuffle this network in a way that's very complex.",
            "And that humans do very easily."
        ],
        [
            "So.",
            "This idea of connection is in fact it comes from psychology in the 19th and 20th century.",
            "And some psychologists actually see connection is a regression compared to other models.",
            "And in the 80s there was a group called parallel Distributed Processing.",
            "They produce a number of books that were famous.",
            "And they try to push.",
            "Some ideas about how the brain works that neural representations are distributed?",
            "That neural computation is parallel.",
            "The processing units, connectivity propagation rules are learning rules define.",
            "How they work?",
            "So basically they push the idea that the brand is a very very different machine than your typical computer with a single CPU, memory and everything.",
            "And this is one of the founding members of this group is Jeff Hinton with this.",
            "Regular.",
            "Question how does the brain work?",
            "And if you ask, Jeff is still asking this actually.",
            "Some say that he claims every two years that he now understands how the brain works, but the truth is that he wants to know how the brain works, and that I'm sure of that.",
            "So in about 8687, I think I think it's 87.",
            "In fact, Ramella proposed.",
            "This note."
        ],
        [
            "That actually corresponds to exactly what was in the graph.",
            "So what do we have here?",
            "We have inputs on the left, so these items by Norcross, Daisy, Ruben, Canaries, and Fish.",
            "Salman, these are relations.",
            "Any other possible outputs?",
            "And you want to trend that thing so that if you put an item so you put camera?",
            "Can I redo one and all the rest to 0?",
            "And you select a particular relation.",
            "You want the proper.",
            "Properties to be illuminated on the output.",
            "Oh how convenient and modern.",
            "Yes, there is a button here, amazing.",
            "So that was 86.",
            "But what's interesting here is that if you look at the item here, we are a matrix of weight already goes into the representation layer here.",
            "So nowadays we would call this an embedding.",
            "So this is a in that case 8 dimensional space, in which each of these objects is going to have a point.",
            "This is combined with the relation goes into some hidden layer and produces some things.",
            "So how do you trend this?",
            "This is what I'm going to explain backdrop."
        ],
        [
            "Well, the first thing was.",
            "Replace the threshold unit by a sigmoid unit.",
            "Because it's differentiable sagon collect training example which other trifles item relation these outputs.",
            "But actually this is the whole set of these output.",
            "Found the mean squared error for every example I want.",
            "To compute the difference between the desired output and the actual output of my system squared.",
            "Initialize with random waste optimized by gradient descent.",
            "Batch gradient descent.",
            "Now there was a problem of computing the gradient."
        ],
        [
            "And in this structure, because while there was big for the time, no."
        ],
        [
            "So if I go back to my little Mac Lock pizza model, I say that AI AJ sorry, the activation for this unit J is the sum of WIG's eyewear exile.",
            "The outputs of what comes before WI J the connection.",
            "The strength of the connection between I&J.",
            "And XJ is some linear function of AJ."
        ],
        [
            "Now there is something called back propagation.",
            "And we have a cost function this squared error."
        ],
        [
            "That was here.",
            "I want to compute the daily relative of this with respect to the weights of all mile units."
        ],
        [
            "So for simplification I'm calling or called G dot is the directive of.",
            "This function respects to a dot for any unit.",
            "And just apply the chain rule and you see that GI is F prime of AI for some.",
            "For all the things that come in the past of WIJJ.",
            "And then derivative of E with respect to await is excited GJ.",
            "So you can just can just start on the last layer because you started directly from the cost function and you can back propagate the errors everywhere.",
            "This is well known by now.",
            "I'm not going to explain this too much because I'm going to explain it better later."
        ],
        [
            "And the training algorithm that they used was a completely batching clear the gradient accumulators for each example.",
            "Set the inputs as implied by item K in relation K. Computes all activations and states of all the units by forward propagation.",
            "For all outputs, units compute the derivative of the cost with respect to the activation of that output unit.",
            "Compute all the other JFK by back propagation.",
            "And accumulate the component of the gradient into the Delta IJ, which is how much you're going to move WRJ in the end and make a little step.",
            "It's a plan or fashion gradient descent that was very slow.",
            "In fact, Rumala didn't do that."
        ],
        [
            "But some people are couple years later about the same time he was young, young looking for instance, and I was in front of them.",
            "We didn't have computers powerful enough, so we did the stochastic way because.",
            "Instead.",
            "For each example, you do the same thing, but you accumulate after each example.",
            "Instead of going over all the examples, we didn't do that because we were smart with that because our computers were too slow.",
            "Anyway.",
            "So what do you?"
        ],
        [
            "See when the system learns well, you can look at the output of value.",
            "Things like the oranges Canary can grow, so input is Canary, relationships can, and you look at the output of this growth cell in the output and you see this when you learn it goes quite fast to one and stays there.",
            "Canary can move well.",
            "Can I we can fly?",
            "So of course interesting that depending on the specificity of the of the property, it goes slower.",
            "But the most interesting is this one.",
            "Pine has leaves initially.",
            "Yes, because pine is probably kind of a tree.",
            "And then I'll know pain is an exception.",
            "And you see this dynamic in a simple gradient training, which is quite interesting, because this is what you would expect.",
            "Then you can look at the."
        ],
        [
            "Presentation layer.",
            "So at the beginning, well, everything is random and not differentiated but after.",
            "2500 bucks you can see that there is kind of a pattern that shows for the vegetables.",
            "Animals.",
            "Features will be different.",
            "That just shows up.",
            "In the year was 87 I believe."
        ],
        [
            "You can do a little dendrogram of representations how things are ordered so at the beginning is a piece of crap, but very quickly you see the sun fission, the salmen go together well being and Canary go together, the trees go together, the Flowers grow together.",
            "And you can also."
        ],
        [
            "Take the representation layer 8 dimensional, do a PCA and try to plot in the two main components where the value things go.",
            "For the easy relation.",
            "So you see the fish circle with the Sunfish and Salman the bird, the Canary and Robin flower Rosen.",
            "Another thing that's interesting.",
            "Is the dynamic reconfiguration that you could see already becausw during learning?",
            "You see that there's some reconfiguration goes on, so the next image is a bit diff."
        ],
        [
            "Hunt is a typical animal things where you have dog God speaks and everything and if you have a child while the child is more likely to see a lot more dogs than pigs and goats at the beginning, everything is a dog, dog, dog, dog, dog, dog.",
            "Then every animal is a dog.",
            "And you see that the dog is a big circle here.",
            "But after a while the thing learns to cover little bit of space for the goat.",
            "Little bit of space for the pig, which are strange kinds of dogs.",
            "And then the kinds of dogs become become animal, so one of the properties of this model is that is completely reconfigurable.",
            "There is no need to shuffle pointers who change distributions or change pointers in memory cells and everything in the big tree.",
            "You just give it more examples that we find the concepts or example that give and it's going to reconfigure like if you look at the thing.",
            "Oh dog is a yes or I don't produce already.",
            "Yeah the dog is got.",
            "Which is a mistake of course, starts by being OK. Everything is a dog and then correct while dog is a dog is perfectly OK and got is got improves and we see that God is a God improves when you realize that this is not a dog anymore.",
            "And the things that don't work just don't work very quickly.",
            "Dog is a Canary too, is something that goes up and then goes down.",
            "And you can attract."
        ],
        [
            "Dynamic reconfiguration of values.",
            "A box quite amusing.",
            "So.",
            "What I've shown here is very, very old and very, very simple neural networks.",
            "Its goal is to represent what's called nowadays knowledge graph very small one because it's a small network and I wouldn't say it's the most efficient way to represent the Knowledge Graph.",
            "But what you can see that's quite interesting is that it's sort of.",
            "Address some of the problems.",
            "That the traditional knowledge drops how in terms of reconfiguration and in terms of speed of access.",
            "So the PDP people they knew they were onto something that is not completely unreasonable to think that humans don't think the same way computers do.",
            "So let's go back to the neural networks.",
            "I explained back prob very quickly.",
            "So a couple years later."
        ],
        [
            "So basically we came with the idea of a construction kit because there was too complex.",
            "We"
        ],
        [
            "Start with a linear brick.",
            "So by by neurons we have X.",
            "It's a vector, W is a matrix.",
            "Multiply X by the.",
            "And you get why.",
            "That's a forward propagation.",
            "Why is WX?",
            "And if you have DE over DY.",
            "You can compute the Y over the X.",
            "By multiplying by the way to the other side.",
            "Did I forget to transpose?",
            "No, because I assume the gradients are.",
            "Row vectors X the column vector gradient vectors in this structure.",
            "And the same way you can get the E over W with a relatively simple operation.",
            "In fact, you can observe an interesting symmetry between these three relations.",
            "Now you can have."
        ],
        [
            "Transfer function brick.",
            "You have a vector X vector Y for each other component of X.",
            "You apply your nonlinear function to Y and you can similarly right the backpropagation.",
            "And you."
        ],
        [
            "I have a catalog of transfer function of the sigmoid, which is a typical of the first one.",
            "With this kind of back propagation rule, the hyperbolic tangent.",
            "Boo.",
            "Really rectified linear unit which is fashionable nowadays.",
            "The fashion changes, you know, just not worry about that Max of 0 S which is and where you can write it.",
            "You have the ramp, which is a rule with things like this and you can write it.",
            "So this is an indicator actually fine."
        ],
        [
            "You can have a square loss brick.",
            "So the output given.",
            "Some values and some desired values while the output is actually the error is 1/2 of X minus the square, and you can also back propagate through this if you know DE over DY, which is the over the which is 1.",
            "You can compute the E over the X.",
            "And again, you."
        ],
        [
            "Can have a catalog of loose bricks.",
            "The square loss low gloss, the hinge loss, the looks of Max, the Max margins.",
            "People fight about the properties of all these losses and what's best and was not best.",
            "Well for classification we know the log loss and the hinge loss or even the locks of Max the.",
            "They're very close, actually."
        ],
        [
            "And the last brick is the sequential brick.",
            "So you have a sub bricks inside, be want the output of the one is the input of B2 and output of B2 is the input of the three, and so on, and the propagation is to apply the propagation rules to all of this in that order.",
            "The back propagation rule is to apply.",
            "The world backwards.",
            "So you could call this an automatic differentiation system.",
            "Is delivered zero over automatic differentiation because the real level comes when you have loops and stuff like that.",
            "But the benefits?"
        ],
        [
            "All that well, you're a flexible model or framework, and in fact many toolkits that have been produced for neural networks are based on that idea that you can test them.",
            "You can test each brick separately by finite differences.",
            "Make sure the guidance is correct.",
            "I'd like to give an anecdote, sometimes have people coming to me and tell me back Prop doesn't learn this horrible is terrible.",
            "I cannot adjust the parameters.",
            "You know, I cannot find the sweet spot for the parameters and my first question is did you try to put the learning words very small though it?",
            "Ah yes yes yes.",
            "Yes yes.",
            "I'll try and does the arrow go down?",
            "No no it doesn't go down.",
            "It's impossible.",
            "So my other things checked the gradients because the gradients are always wrong.",
            "I don't know.",
            "I checked them know check the gradients.",
            "It never fails.",
            "You can try it yourself.",
            "It's going to work.",
            "You'll see it makes you feel good sometimes but.",
            "Because the.",
            "Did they really know?",
            "I checked out, checked all the lines of the program 1 by 1.",
            "The gradients are correct, no?",
            "And you have plenty of possibilities, so I have examples.",
            "This is their new land, but I've touched code examples, but that's very similar indeed."
        ],
        [
            "So.",
            "Oh well, two Leonel Networks is a sequential brick inside the mystery shape because we want to transform these images into a single vector linear brick, I public tangent Anatolia, Britain.",
            "Lots of mass for classification and the criterion which is how you're going to define the cost function is negative log likelihood criterion, which is one of those I gave, and one that's good for multiclass problems."
        ],
        [
            "So here's an example where decide to trend the network.",
            "So what's important here is this function.",
            "Well, people don't do it that way there, but it's interesting to see.",
            "Which is something that's going to compute the gradient respect to the weights.",
            "So you get the parameters you copy X into the parameters.",
            "O the gradients.",
            "Then for all your inputs you do forward.",
            "You apply the criterion to compute the error.",
            "I just left another error.",
            "Go backward through the criterion to know the derivative of the outputs of the network.",
            "Go backward through the network to know the gradients everywhere.",
            "Accumulate the gradients and divide by the number of examples.",
            "Well, why not?",
            "And return the value of the function and the God of parameters is all you need to start an optimization program.",
            "OK, this time is wrong.",
            "OK, I'm going too fast."
        ],
        [
            "So now I'm going to go to the model that works the best.",
            "The convolutional neural networks is the one that historically works the best.",
            "And still the one that's responsible for most of the successful application of neural networks.",
            "There are many reasons why this is the case.",
            "One of the reasons is probably that it's closely related, is well suited to the task.",
            "And the task is typically vision or speech or perceptual problems.",
            "And so we need to explain something about vision.",
            "And."
        ],
        [
            "First thing I'm going to say that vision is fast.",
            "So that comes from an influential paper, or Simon Thorpe.",
            "And.",
            "What he did was the following to the subject and you show to the subject images of plenty of things, just pictures very quickly 10.",
            "Past 10 by second just flashes in front of your eyes.",
            "And at the end we asked them, did you see a dog?",
            "Did you see a car and try to read the error rates?",
            "And you want to know how fast you can flush the images.",
            "Until they started making lots of mistakes.",
            "And the conclusion of this is that you recognize an image about 0.1 seconds.",
            "Now, the propagation delay of signal through one neuron real neuron is about 100 milliseconds.",
            "So we have time to go to 10 layers.",
            "So there is no equilibration.",
            "There is no, there is no sleuthing.",
            "It looks that just out of timing constraints, this has to be 10 layers, completely feedforward.",
            "Well.",
            "How is it organized?",
            "And."
        ],
        [
            "62 Rebel in visual meta.",
            "Lot of experiments and.",
            "Yes."
        ],
        [
            "All possible, it's true.",
            "It's a good question is good remark is a good remark.",
            "Then you have to believe that you can store a lot of images in your new short term memory.",
            "Lot of complete images and at the time that was not considered very possible.",
            "We should have some results by all the levers, suggesting that you can actually record quite a lot of images.",
            "So yes, you objection is Israel.",
            "At that time, people didn't.",
            "We didn't believe it could be possible to store, let's say 100 images.",
            "In your in your short term memory, Now people like Old Oliva believes it's possible.",
            "OK, good point."
        ],
        [
            "I never thought of it that way, that's true.",
            "So well.",
            "Essentially these people studied.",
            "With the biological, you know, by looking at samples, staining things and staining neurons and looking at the patterns of connectivity, the connectivity patterns of the first neurons in the visual systems.",
            "And.",
            "They came with the conclusion that there are two kinds of cells that you could spot very easily.",
            "You have simple cells that take a little neighborhood of the.",
            "Or in the retina that detect local features?",
            "And you have complex cells that pull local features and compute things that are bigger distance.",
            "Of course people try to use this so."
        ],
        [
            "In the 70s, Fukushima invented this very complicated thing called the new core neutron.",
            "The dinner backdrop at the time so so the learning rule is to completely unsupervised method.",
            "That's very ad hoc.",
            "It's a bit complicated to get to work, but these things work know and have an input layer and then you have a contrast extraction which is made by taking local.",
            "Connections in this image is just each unit.",
            "Here seems a little neighborhood there and you do it again and again and again and these are made partly by hand in the construction edge extraction, partly by hand, and then some kind of supervised learning that's completely ad hoc, and at the end the recognition layer.",
            "That's essentially the input of a perceptron.",
            "And that thing actually works.",
            "You can recognize things that way.",
            "There is no backdrop, no nothing or all these.",
            "You can see the kind of unsupervised learning as a form of K means.",
            "Basically what it's doing is recording in the filters segments of the images are seen.",
            "And there is no enviance but."
        ],
        [
            "One of the ideas there is this idea of local connection.",
            "Instead of connecting to every unit there, you connect locally.",
            "Which is interesting because there is a notion of locality if you connect to everybody you could scramble all the pixel by permutation wouldn't be different.",
            "But if I show you a picture after scrambling all the pixels with a random permutation, not going to recognize anything.",
            "However, how do you work on it?",
            "The next step."
        ],
        [
            "It was to go to the convolution seiwell.",
            "In the new code neutron, you observe that when you present examples that are shifted because for this to work you have to present example that I did in every position.",
            "They tend to have the same weights in all positions.",
            "They tend to develop representations that do not move, so you could force it.",
            "You can say, well, I'm going to have this unit seeing three pixels here and this one C3 pixels, and they're going to use the same weight, so this unit computes the same thing relative to these three pixel as this unit relative to the three pixels and so on, and three pixels.",
            "This segment of an image, sorry.",
            "And of course."
        ],
        [
            "You can make multiple convolutions, so you have the black one, the red one.",
            "They have different thing.",
            "And here this is the convolutional network."
        ],
        [
            "So in the 90s there were things like this where you would take a character, do a bunch of convolution like 6.",
            "And then subsample and do something like 16 with carefully organized connectivity pattern to save some computation.",
            "Then sub sample and then do some linear bricks at the end.",
            "So in 89 young started to have a big success in isolated and written character recognition.",
            "In 91 there was face recognition systems that were working.",
            "I worked on one at that point and one of the problems we had is that we had to remove the colors of the people from the images because the thing would be very good at organizing the shots rather than the faces.",
            "Which is the problem?",
            "That's that's real.",
            "Oh in 90 three vehicle vehicle recognition that was in only row, which is in in France, in an in aerospace research lab.",
            "One of the thing Steve Souder Dinara at the time was quite difficult, was to have a PC of the time equipped with a camera, which was not the USB camera.",
            "That was a big thing and you would put little model of an airplane, and we tell you the position of the airplane.",
            "Is it coming towards you leaving and so on.",
            "And all this in near real time.",
            "In 90 three there was a mean feat just in terms of engineering that was hard to do.",
            "So in 94 Bell Labs had a zip code recognition that was working quite well, but didn't go into a product, but in 96 they had the check rating system.",
            "And the check reading system was diploid about in 96 and worked until until 2000 and the end of the 2000.",
            "Processed about 10 to 20% of the checks in the US.",
            "That means 1 billion per year.",
            "So that was a real application.",
            "This is not a joke.",
            "This was not laughing this."
        ],
        [
            "He worked.",
            "And they could recognize things like this.",
            "I told you that before is not just two strokes.",
            "That cross each other because you can have things like this.",
            "You can have a bunch of very noisy situations.",
            "They're not really trend for that, you know.",
            "We just try things.",
            "In fact, the way this works is that you have this big fat camera and then the set of slides like projector slides with the noise and everything will move them around to just see characters and then things."
        ],
        [
            "An idea that appear clearly doing this just compared to the beginning is this idea of pulling.",
            "So I told you that in the network sometimes you subsample because you want to trade.",
            "Special resolution for qualitative information.",
            "Initially you have an image is quite large, but each pixel is 3D, three numbers RGB.",
            "Then the more you go forward you want to trade special resolution because you want to keep the information limited, but you want information that's richer.",
            "It's going to be an object or there is a nature, or there's something like this.",
            "And the question is how you do the resolution reduction that is called pulling and pulling is just a convolution in fact.",
            "So the first one was average pooling.",
            "You take this unique, takes a little neighborhood, and then you shift the neighborhood, but not by one pixel by two, so that you get the subsampling by two and you go on.",
            "And then people discover that.",
            "Well, if you do Max pooling, it works better.",
            "Well, OK, why?",
            "Some people argue that we know in the brand there is something like this is compatible with Max pooling and rectification.",
            "It's a bit of a posteriori theory, so fine, you have the L2 pool.",
            "Tell people which is cover, so there is a catalogue of these things.",
            "Then"
        ],
        [
            "People started to add contrast normalization, so you know that if you have a strong light here and no light here, when you take a picture, the picture looks ugly.",
            "But when we see.",
            "If I have a lot of strong light on that side, I'm still going to see both sides of the of the room pretty well, so we have some contrast normalization built in, so we're not putting in the network, so.",
            "You can smooth lowpass smooth version of the layer is just another convolution with six coefficient.",
            "You have lots of violence per feature map across feature Maps.",
            "There's just there's a catalog of these things that you can see.",
            "Other people argue whether which one is better or not.",
            "Of note"
        ],
        [
            "And the conversion networks in the 2010.",
            "They look more like this convolution nonlinearity Max pull contrast normalization.",
            "I do it again, convolution.",
            "Nonlinearity Max pool consideration.",
            "I do it again and we can do that many many times."
        ],
        [
            "The code does not really change.",
            "Is all this brick system?",
            "Is sequential spatial convolution OK with all the filter size?",
            "The how much you shift things and everything.",
            "I probably tangent in that case special LP pulling then special subjective normalization OK, and I will start again and you have three layers and this is something with which you can recognize one of these kind of images for them.",
            "I think that's a Google Street number database."
        ],
        [
            "So we have the Street View House numbers that this one traffic sign recognition, pedestrian detection.",
            "Basically in a couple years.",
            "This machine started to work better than the typical vision competition."
        ],
        [
            "One of the turning point was the image net 2012 competition, where famous entry by by Alice Kryszewski, Leah and Jeff Hinton just beat everybody by a good margin.",
            "And so these were the typical competitors, like 6 procedures, two kinds of SVM with a lot of 100 hand engineer feature extraction, and the Internet was pretty much half the error.",
            "For classification, for detection was the same levels of shock in the vision community, and because they work really hard at this and what you see is something at work, that data Trump's engineering.",
            "Data Trump's programming, so if we go back to the ID of computers, the computer your program in the computer that learn if you have data.",
            "Computer done can be better.",
            "So the computer you program with a lot of engineered heuristics is just as good as the heuristics you engineer.",
            "And you.",
            "Can't see as much data.",
            "You cannot see all the cases."
        ],
        [
            "So the the mandatory little image it image if you compare to the previous one.",
            "Remember, I told you an input that was 28 by 28?",
            "This one is 220 by 220, so it's I'm not, I'm bigger.",
            "48 and 48 feature Maps on 96 instead of 6.",
            "256 here 344 here.",
            "So it's much much bigger than the networks of the 90s.",
            "So one of the big challenges of the networks are much, much bigger.",
            "There's no way we could run anything close to that.",
            "Even a 10th of that at that time."
        ],
        [
            "And they recognize stuff.",
            "OK, you've seen it.",
            "An important thing, however, is to realize that now we don't want to recognize object like image net in a small Patch.",
            "We want to take a complete image and apply a conversion network to everything."
        ],
        [
            "And there is a wrong way and there is a right way.",
            "The wrong way is you take your image network and take the Python code and you scan the image to try to detect objects in various positions.",
            "OK, that's the easiest to program.",
            "That's clearly the wrong way becausw.",
            "What you should do is just expand the convolutions.",
            "The convolutions are invariant operations.",
            "You can do them on a small image.",
            "You can do them on the larger image with the same weights.",
            "And if you expand the combination, you change the input image, which is a large image with three channels into an output image which is.",
            "Lower resolution image with a number of channels corresponding to the number of classes you want to detect and when you do this you share a lot of computation on the way.",
            "This goes all those of my teeth faster than this one."
        ],
        [
            "And so again, in the early 2000s are plenty of work on this, so that was a younger kind, friends.",
            "I think France include certainly Joshua others, including working with.",
            "We played with these things trying to recognize replicated faces.",
            "And that poses well in all kinds of images, and they were not trained on aliens.",
            "They were not trained on cartoon characters, but it worked quite well anyway.",
            "So that's one side of the story.",
            "That's the best known.",
            "Another one is CNN's for speech recognition."
        ],
        [
            "It's about the same time.",
            "Are in 88.",
            "There was a hint on in Lang and Alex Waibel and in did a big speaker independent from recognition, that was running on one of these fancy Alliant computers.",
            "That's lower than your cell phone.",
            "But that cost a lot more.",
            "And then you sure worked a lot on this, but with first like this and then we can, then there are speaker independent, worker permission, continuous speech recognition.",
            "All this with CNN's that were just very close."
        ],
        [
            "And then more and more complicated things with dynamic programming it with recurrent network, all kinds of things were tried.",
            "And when you show us that we did our thesis on pretty much the same topics, well that was it."
        ],
        [
            "And in the 19th they were competitive with the Goshen hidden Markov models.",
            "They're working pretty much as well, do it.",
            "More difficult to use because you had this strange learning process used with a couple parameters.",
            "You had to adjust.",
            "So they were not outperforming the existing system enough to justify your switch.",
            "2010 alot more data.",
            "Lot more compute power.",
            "Not more results.",
            "This kind of systems or variants of these systems just.",
            "Beat the performance of Russian hidden Markov model by good margin.",
            "Now pretty much all the major speech recognition systems are switched to neural network acoustic models that happened in the space of one year, one year and a half 2011 2012.",
            "So very often yes question.",
            "Do you know what happened is?",
            "Nobody knows.",
            "Nobody knows.",
            "The hydran Bible.",
            "Yes.",
            "Well, you know."
        ],
        [
            "Kind of things that are in an already is our Internet on top of a confident people do this kind of things.",
            "So so in the in the 90s that Tony Robinson did a very complicated system of that kind.",
            "Basically with the bigger running on top and for a couple years that was the best performing system around.",
            "You were just complicated to run.",
            "The engineering was complicated in comparison, running a hidden Markov model was very simple.",
            "We didn't have the LSD endo.",
            "That was not an idea that was the clean at that time."
        ],
        [
            "So now I'm going to enter my second part, which is the part that's more technical about the optimization part.",
            "How to train neural networks.",
            "And on."
        ],
        [
            "I'm going to start by some optimization basics.",
            "Things that are.",
            "Not that complicated, but they good to know about optimization.",
            "I'm going to start with batch algorithms because this simple.",
            "And the first thing about the."
        ],
        [
            "Story of convexity.",
            "Complexity is very important in optimization.",
            "Not all of optimization is convex, but it's a big part of it.",
            "And convexity means that if you take two points on this surface, the segment that connects the two points.",
            "Is able to surface.",
            "This is what this is.",
            "And the resulting properties at any local minimum is a global minimum.",
            "She's very convenient.",
            "And conclusion with optimization, libraries are easy to use because they always give the same result.",
            "And also easy to analyze mathematically.",
            "Example linear model with the convex loss function that's convex.",
            "That's basically if you add the regularizer, it's SVM basically.",
            "Curve fitting with mean squared all natural linear classification with loss of hinge loss.",
            "Yeah, OK. Now I know."
        ],
        [
            "Convex is something like this.",
            "You lose these properties you have now have a complicated landscape is low dimensional in high dimension is very hard to fathom.",
            "Local minima saddle points plateau.",
            "Big Platearius summer revines someone just what?",
            "All this stuff.",
            "An optimization algorithm usually finds the local minimum at best.",
            "Some are good, some are bad.",
            "And the results can depend on subtle details.",
            "It's possible very easily to build surfaces like this that are going to be almost impossible to optimize.",
            "And examples are material networks, clustering, learning features, mixture model hidden Markov models.",
            "There two are non convex.",
            "Silly feature selections.",
            "Basically all the important learn machine learning problems except for simple linear problems.",
            "Simple linear models tend to be non convex.",
            "So sometimes you find a very smart paper and it's a lot of smarts that try to transform a problem like this into a convex problems that can be analyzed.",
            "And that's not something negligible.",
            "It's an achievement.",
            "And usually a good interesting starting point.",
            "But well, I'm going to take the example feature selection.",
            "For instance, feature selection in the linear model you have just one unit linear log loss.",
            "Pick your classifier.",
            "Typical SVM and you would like to select the important features and people tell you well you can use L1 regularization.",
            "Because everyone regularization is the one that's going to select a small number of features and remains convex.",
            "Now the secret is that if use L1 half regularization instead of using just the sum of absolute value of the weight using the sum of the square root of the absolute value of the weights, it works better.",
            "You get less features for the same performance.",
            "Now the second secret is that if you want to learn something with L 1/2.",
            "Is better to start by learning your L1.",
            "And then you switch your regularizer to L 1/2.",
            "And you make a couple of steps.",
            "This is what's going to work the best, so the convex approximation was not useless.",
            "It was a very good starting point, a very interesting one.",
            "So.",
            "So I."
        ],
        [
            "Another aspect of optimization is this idea of derivatives.",
            "Stupid idea.",
            "R. The Liberty Vindicator general position of the closest local minimum, but was very important that Liberty is a local cue.",
            "You, in one point, you observe the derivative, and it tells you well go this way.",
            "This is probably the right way.",
            "This happens only when you have a differentiable function.",
            "If you have something like logic where everything is 1 and zeros.",
            "You know it's flat everywhere.",
            "Somewhere there is a Cliff, but when you take a point you don't know where the cliffs are, you don't know which way to go.",
            "And this is what makes discrete optimization a lot harder than continuous optimization.",
            "So one of the tricks of neural networks the changing the threshold you need by a sigmoid is very important because you create a situation where at every point you have a richer information that tells you.",
            "Weather in which direction?",
            "The closest local minimum is.",
            "It's not a very good information, sometimes not always perfect, but it's more.",
            "It's more than no information.",
            "Now you have a second derivative.",
            "Not only you have a general position, we have an estimate of the position because if you have a first and second derivative, you can fit a quadratic and it tells you well, probably around here is not a bad idea.",
            "You may."
        ],
        [
            "About linesearch suppose you have a line like this in one D. You want to find the minimum algorithmically.",
            "Well, the first thing you have to understand they have to bracket the minimum to set the minimum within two points.",
            "And to bracket the minimum you need three points.",
            "You need this one, this one and this one.",
            "The important thing that the value in this point is less than the value here and less than the value here.",
            "When you know this, if you assume the function is continuous.",
            "You know that you have a minimum between the two light blue points.",
            "You don't know where exactly."
        ],
        [
            "Then you can probe another point here.",
            "And see how.",
            "Its value.",
            "Ranks compared to the value of the others, and you discovered.",
            "Now you have a new bracket of the minimum here, smaller.",
            "And you can go on."
        ],
        [
            "Problem with the point here with the inside the new bracket and are.",
            "I think that one is the new bracket.",
            "And again."
        ],
        [
            "And again."
        ],
        [
            "And in fact, it's easy to prove that if you want to be mostly effective at this game, you should put your query point.",
            "In the biggest segment in your view bracket, at the position that obeys the Golden ratio.",
            "Just them.",
            "Normally just a little recursion is going to show you that.",
            "OK, but here I just use the value of the function.",
            "What about my local information?",
            "I just assume the function is continuous.",
            "Whichever is better than this continues.",
            "And they just use the values.",
            "So can I use the derivatives?",
            "Well."
        ],
        [
            "Actually I can use if I have three points, I could do something smarter, which is a parabolic interpolation and said now I'm going to take my points here.",
            "My estimate of the minimum.",
            "And say maybe it's going to go faster.",
            "Sometimes it goes faster."
        ],
        [
            "Here is a case where it's not going to go faster or something like this.",
            "This estimate.",
            "Is wrong?",
            "And so I'm going to have this bracket now and I'm going to do the same mistake all the time, so I'm going to progress to that minimum very slowly.",
            "So in fact, if you want and if."
        ],
        [
            "Patient line search or you have to use the brand method you alternate.",
            "You have two methods.",
            "One is.",
            "Well, maybe not the fastest, but you know how fast it goes.",
            "It goes that way all the time.",
            "The other one, the other one could be faster in common situations, but there's always, but you do just one step of each.",
            "That way you know more than twice slower than the slow one.",
            "But if the first one is faster while you fast is Siri ID, and of course you can use derivatives if you can compute F&F prime together.",
            "Bicep bicep together that's important if you.",
            "You don't want the cost of F prime to the computer experiment to be too high, but if you can compute F&F prime together, you can go faster.",
            "If you can compute F prime F so gone together, you can go faster again.",
            "So now I'm."
        ],
        [
            "To go back to my values layer.",
            "So I think my linear units here have an input XI compute WX, which I call a activation.",
            "I take a non linearity FANY.",
            "And I've got propagation equation in my backpropagation equations.",
            "That I've seen now I'm going to make a slight change.",
            "I'm going to make my non linearity twice steeper.",
            "And at the same times I'm going to make the weights twice smaller.",
            "So I'm computing the same thing with the same XI, get the same, why?",
            "So that leaves Y of X and change.",
            "So what can you say about the change to the weight?",
            "If you run the back propagation.",
            "So that's the bigger size.",
            "Any idea?",
            "So the weights have been divided by two.",
            "But the nonlinearity has been doubled.",
            "Now you back propagate.",
            "What do you see?",
            "Oh come on, come on.",
            "Yes.",
            "Is multiplied by 4, that's correct.",
            "If you go back.",
            "You go back here so if promised was steeper.",
            "So GA is double.",
            "Now.",
            "GX equals GW, so GX is twice bigger now.",
            "For the Cemex.",
            "Now Delta W is also twice bigger, but the weights are twice smaller.",
            "So now comparative to the size of the weights.",
            "The update is twice is 4 times bigger.",
            "Isn't that a bit annoying?",
            "You did a change in your system that changes nothing forward, but the learning is completely different.",
            "So what do you think about that?",
            "You know you take the steepest direction that the gradient is the steepest descent.",
            "And now the steepest descent is changed because I melted, I multiply the Empire met F twice deeper, and a divide W by two.",
            "OK, we come back to that."
        ],
        [
            "Just keep that somewhere in your head that there is something fishy there.",
            "So let's take a power bowler.",
            "What do you say?"
        ],
        [
            "I'm sorry I could understand.",
            "Well, you don't.",
            "You don't exactly know, but one side effect of being doing this change dividing the W by two and making FY steeper that are effectively multiply the learning rate by 4.",
            "Yeah.",
            "What well, I just showed one particular change, but I can make many changes that preserve the forward.",
            "The rule, especially with the role, is very easy to play because they know you divide the weights in one layer for one unit by two, multiply the output waste.",
            "For that you need by by two.",
            "You don't change anything but all the units in the similar could be done differently, so there are plenty of invariant.",
            "Transformations forward that are not invariant backward for the learning.",
            "So that tells you that the gradient algorithm doesn't go by itself."
        ],
        [
            "So let's take my Powerball are here.",
            "You have W = C is curvature over by two double square.",
            "The gradient decent is WT plus one equals WT minus Y to deliberative of East.",
            "Respect the W in WT so the size again questions how does it affect the convergence and what's the best value of data?",
            "Well, some of you know already yes.",
            "You can bounce.",
            "But you can calculate it better.",
            "Now The thing is that my plan was to ever blackballed and write things, but the blackboard is under the thing.",
            "So so.",
            "There's a fancy button somewhere I've seen you sure do that.",
            "Oh yeah, OK, thank you.",
            "Is it good enough?",
            "So.",
            "What is this?",
            "You have the function there WT plus one equal WT minus ITA.",
            "Would that be?",
            "WT.",
            "CWT equal 1 -- y to CWT.",
            "Haha.",
            "So that is a geometric thing.",
            "So if this as an absolute value less than one.",
            "You're going to go to the minimum.",
            "If this is negative.",
            "You bounce.",
            "But this could be minus 1/2.",
            "You're going to bounce, but go to the minimum.",
            "So if it's minus 1/2, you're going to do something like this.",
            "If it's 1/2 you're going to do something like this.",
            "Good.",
            "If it's.",
            "If this is more than one.",
            "You go.",
            "As well.",
            "If it's less than minus one, you go elsewhere, but on the other side.",
            "So now you know what is the range of values of E to the target.",
            "You want.",
            "Ether to be between.",
            "I'm awful to do.",
            "Help me please.",
            "So we want this to be like valkyr.",
            "Let's write it seriously.",
            "You want 1 -- E to C2B.",
            "Between one and minus one.",
            "So you can remove a one here screen minus two less than minus 8C0.",
            "Wait, what?",
            "Yes, should be strictly less than one that's correct strictly.",
            "So I removed one so I can put you on ether C between zero and two.",
            "Therefore you want it to to be between 2 / C and 0.",
            "But the learning rate is positive, seems normal.",
            "You want to go down.",
            "And that's the maximum learning rate to oversee that you can tolerate if it's bigger than that.",
            "Well, actually this is going to be the minus one side, and you're going to diverge.",
            "OK oh.",
            "My second question was.",
            "What's the best value of Eater?",
            "Yes, whenever she is the one that's going to, you're going to go straight to the minimum.",
            "Have 1 / C. So there was one guy."
        ],
        [
            "More dimensions.",
            "See quatic.",
            "I have two dimensions.",
            "And two different curvatures.",
            "So basically I have to see.",
            "So how does it affect the comment was the best value of beta.",
            "So I have a Cmax and cmin.",
            "Well.",
            "Wally, just it is not in the matrix is just this color.",
            "But it's clear that.",
            "You can write the same thing and diagonalize, so you can write the same thing along each of the axis.",
            "In that case update Axis parallel is simpler.",
            "So you want to eat that to satisfy both conditions should be less than two oversee mean and less of a two or C Max.",
            "So that they see what it should be.",
            "Yeah, so basically you're constrained now.",
            "The speed at which you're going to go.",
            "The speed is going to go is going to be controlled by these ratios.",
            "1 -- 8 C, So you want mine minus 8 AC to be in absolute value less than 1.",
            "Four in every direction.",
            "But what's going to control the speed?",
            "Is the slowest one.",
            "Because even if you converge in all the directions, let's be optimistic and all of the directions so fast you converge is the slow direction that's going to concern.",
            "And even if you converge in the fast directions, you cannot raise it to because a little bit of noise in the calculation, and you're going to diverge in the first direction.",
            "So when you have.",
            "An eccentric ellipse, your constraint.",
            "And basically you know the maximum learning rate is constrained by the strong curvature, and the speed is constrained by the low curvature."
        ],
        [
            "Is this something we should change?",
            "Access so basically?",
            "No, just that you can diagonalize and do the same thing.",
            "And I'd give."
        ],
        [
            "The idea of secondary scaling.",
            "So now I'm going to do one of these change of things that lifting some change.",
            "So I'm going to say Wu is H1, half of W. So my cost, which is 1/2 of W transpose, HWH is the Hessian matrix, become one half of WW.",
            "Now it's a circle.",
            "The two seas are the same.",
            "You go at maximum speed.",
            "So does my little exercise questions.",
            "OK, right gradient descent and W space, right?",
            "The equivalent W update.",
            "So turns out that."
        ],
        [
            "I've done them here so in the dominance space.",
            "Is minus Y to the E over DW?",
            "Because of this formula is minus little edge minus 1/2 the E over W. So if you do Delta W instead of Delta W new, you have another hash minus half to multiply with and you get cash minus one.",
            "This is Newton's rule.",
            "So.",
            "What we have here is a way to compute the minimum that you can see in two ways.",
            "One way is to say well, I'm going to fit aquatic bowl that has the same Asian.",
            "As my my decision that I have in my current point and the same gradient, so it's tangent and go to settle the minimum of this classic ball analytically.",
            "Another way is to say, well, he's just a rescaling of W. So what I did in my little network thing before."
        ],
        [
            "Which was a simple rescaling, is in fact the primitive operation that allows us to make learning better or worse.",
            "And all the game will be to do that in the best possible way."
        ],
        [
            "So the other."
        ],
        [
            "Ethical issues though.",
            "The first object function is not quadratic.",
            "So the local quasi approximation is reasonable, but the Haitian changed with the volume.",
            "Second, when the function is non convex.",
            "The Hessian can have negative curvature.",
            "If you do not only are going to go to the maximum, not the minimum.",
            "If you do that.",
            "Oh so there goes way to estimate the Hessian on the fly?",
            "Is often lost to store invert, yes.",
            "Yeah, you go to the saddle.",
            "Point is you go to the critical points.",
            "You don't know three minimum.",
            "No stand up."
        ],
        [
            "Solutions.",
            "The first one is to estimate the compact approximation of X -- 1.",
            "I'm going to give an example later, so you use the observed gradient GWT, GWT minus one GWT minus K and you know the gradients and you do the changes between the values.",
            "WT so you have some information here that tells you about the Haitian in that area.",
            "So in fact you can use directly the gradient and WT minus WT minus one, and so in the difference in W to actually do this multiplication by the inverse of the Haitian more or less directly.",
            "Simple ways to do a categorisation grammar sheet organization on the fly.",
            "There are more sophisticated way then you can use approximate line search along the direction.",
            "It's minus one estimated G of WT and you can go.",
            "And our values algorithms that use this kind of IDs.",
            "So under the ID is to use an exact line search and ensure that the conjugates direction.",
            "So so if DY T -- 1 D T -- 2 D T -- K are the last case of directions you want to choose a DT.",
            "That's a linear combination of the gradient and the past directions such that DT HD T minus one I = 0 for all eyes.",
            "So this gives you the conjugate gradient kind of algorithms, and in fact very good algorithms have been developed.",
            "Their conjugate gradient is one with K equal 1 and\nSearch.",
            "And one that's very famous limited storage BGS with greater than one but not too large.",
            "An approximate line search if you do the approximate line search well, you can go very fast.",
            "If you don't do it well.",
            "So so it's it would take a complete.",
            "Week to describe this algorithm, all the tricks that are inside, especially the Elbe GS one underestimated sometimes.",
            "But if you implement LB FGS like simple book tell you to do and you compare with no signals code, you discover that no, that's called this faster because this approximate line search matches exactly the property of the approximation in a way that extremely smart."
        ],
        [
            "But there are lots of reasons to remain suspicious.",
            "First, our cost function is a symbol for large number of similar terms.",
            "And this could be used to speed up the optimization.",
            "See if N is 1 billion.",
            "No evaluating the function is costly.",
            "The second is that a problem that's a random subset of the terms is informative.",
            "If you have 1 billion examples, well, you know you don't need to see the 1 billion example.",
            "You can start with 1,000,000.",
            "And you get closer to the solution and then you can use more.",
            "And the fact that this is true is because we expect generalization.",
            "If we expect that our system is going to generalize and we have 1 billion examples, we also expect that 1 million example is going to at least going to take us in the query direction.",
            "Is actually the same argument.",
            "The third one that quickly achieving a good tested performance is not the same as quickly achieving a good training set performance.",
            "Some of these algorithms can be so fast that they overfit faster.",
            "But before.",
            "This."
        ],
        [
            "Just a lot of things we can do.",
            "So we can pre condition the inputs, normalize everything in similar ranges.",
            "Because if you do so, the curvatures are more likely to be similar in all directions.",
            "You can use different learning rates in different layer on the basis of the average size.",
            "The gradient, an oversized weights.",
            "So it's very simple idea, but no, you want everything to look.",
            "Like a circle like like a sphere and a sphere, if you learn.",
            "You learn about the same speed in all dimensions in in along every axis.",
            "So now you have a new network.",
            "Stop once in awhile.",
            "Take some data.",
            "And do some forward backward without learning the weights.",
            "You see some weights compute the average number of the weights in one layer.",
            "Compute the average norm of the gradient.",
            "And if you look at the ratio between this, you would like that the change you're going to do to the weights has to be a small fraction of the normal ways.",
            "Let's say 10%.",
            "One percent of the side of the weights.",
            "That seems reasonable.",
            "What doesn't seem reasonable is that if you have one layer where the average update of the weights is 10 times bigger than the words themselves, another layer is 1 millionth of the size of the weights.",
            "That's probably not going to work.",
            "So it's a simple thing and I describe it not in terms of an algorithm, but in terms of practice.",
            "Laundromat stop.",
            "Collect statistics in all layers.",
            "Size of the weight, size of the gradients and everything and make sure they are reasonable.",
            "And that goes along way.",
            "Alot of the fancy algorithms that are discussed in the literature, whether they called adigrat Ms Prop and everything.",
            "This is what they do more and less automatically.",
            "But to do it they have to pay at each time you need to page time just every 1 million iterations.",
            "Stop and look."
        ],
        [
            "So that brings us to another problem.",
            "In multilayer networks is the initialization.",
            "We typically initialize the weights of multilayer networks randomly."
        ],
        [
            "And.",
            "Now it's time to ask the nasty question, why can we optimize such a complex nonconvex function will not realize optimizing.",
            "The problem is simpler than looks, otherwise we couldn't do that.",
            "So that means that.",
            "The initial weights they play a big role because they put us close to where we want.",
            "We want to be.",
            "So of course there's many symmetries in the networks and everything, but there are many ways to put the initial ways that are going to do my system.",
            "Amber.",
            "I'm going to look at the simple examples the first one."
        ],
        [
            "Is the simplest to layer network.",
            "Have an input, wait just one way to improve this color one wait W 1, the sigmoid another wait, another sigmoid yet why?",
            "And you turn into examples.",
            "This is the input.",
            "This into output 1/2 one half and minus 1/2 -- 1/2.",
            "So basically you want this network to replicate its input with only two examples.",
            "So the arrow is here in full.",
            "So you can plot it."
        ],
        [
            "Looks like this.",
            "I don't know if he's clear enough.",
            "So in the center you have a subtle point.",
            "In this other point, because you have two products, all the relatives.",
            "All zero.",
            "And the second derivatives that cannot 0.",
            "But what's interesting to know is at the center southern point with the hyperbolic tangent.",
            "Is a subtle point of the older 1 minus the number of layers.",
            "So if you have 15 layers, the saddle .0 devices.",
            "0 second abilities, 0 third abilities, and so on until the 14th derivative.",
            "So it's really flat.",
            "Yeah, you don't want to be here because there is no local signal.",
            "And then you have the solution without the black thing.",
            "There's somewhere here along two hyperballs, so maybe it's."
        ],
        [
            "Easier to represent like this, so that's one quadrant W1W2 and you get the kind of.",
            "Are being shaped revine here with the minimum about.",
            "Here is normal because if you didn't have the hyperbolic tangents this would just be W 1 equal times W 2 equal identity.",
            "So.",
            "The hyperbowl here or the Hyper Bowl is where the solution is likely to be and you want to be in the point of the sigmoid.",
            "That's interesting.",
            "So what do you know here?",
            "Well, if you know you size the weights so that W1W2 are here.",
            "Well, there's no gradient here.",
            "It's all flat.",
            "You're not going to run if you put them here.",
            "It's all flat.",
            "You're not going to learn.",
            "Well, you know you want to put them in a certain area here.",
            "At least if you just.",
            "Decide now Norm, that should be in the part that's active.",
            "So the capital rule of initialization is do not pick initial ways that kill the gradient.",
            "It's so simple.",
            "Now the transfer function plays a big role in this.",
            "If it's a Sigma media saturation, the gradient can be 0.",
            "And the.",
            "The meaning of this rule for value sensor function is that the distribution of the inputs of the transfer function should target the linear part, but should have a chance to exploit the non linearity.",
            "So I'm going to draw things again."
        ],
        [
            "So I think again this ID that I stop my network and I'm doing some food showing some examples and correcting statistiques, and I'll take a particular you need somewhere and I assume this unit is a kind of sigmoidal unit, something like this.",
            "OK. And I'm computing the distribution.",
            "Of the son of WIJ.",
            "XXXI so the input in the sigmoid.",
            "Well, if this distribution is like this.",
            "It's not going to work.",
            "OK, let's not do that.",
            "If the distribution is like this.",
            "Very small.",
            "Well, it's in the linear part of the sigmoid.",
            "So basically have a linear system.",
            "There is a gradient.",
            "But also means that my system is working linearly and that I'm not really going to use the fact that have a non linearity, which is something that differentiates multi and network from.",
            "1 million network so not good.",
            "If I put it like this.",
            "Well.",
            "Lot of zero gradients, not good.",
            "Well, if I put it like this.",
            "So the mode is in the linear part.",
            "But I have enough tail here so that I might be able to use the nonlinearities if I need them.",
            "Good.",
            "OK, now another example, the whole.",
            "The world is a different animal.",
            "It's like this.",
            "That's zero and OK. Look at my distribution.",
            "My decisions like this not good, OK?",
            "Sorry.",
            "The distribution of the activation function like this?",
            "Well, it's linear is not too bad, but could be better.",
            "I put it like this.",
            "Well, I have a strange hinge.",
            "There is not good for the gradients in other gradients.",
            "Don't like that too much.",
            "Notice that you take a gradient on one side.",
            "It moves a lot.",
            "It's just like having a very nasty curvature.",
            "So if you put it like this.",
            "So basically the mean is somewhere and the standard deviation is on the order of the mean.",
            "Little bit on the side good.",
            "Good you're going to be linear.",
            "You have the chance to use the nonlinearity if you need it.",
            "Simply simple idea.",
            "So when you initialize things like this, that constraints the ways to have certain norm.",
            "Because the more inputs you have to unit, the smaller the West we need to be to be in a range that you have defined.",
            "And by having the weights of the norm that tells you who my learning rate in that layer.",
            "They should have be such that when I back propagate the gradient from the top, which depends on what you do on the top.",
            "The gradient terms of learning, which should be a reasonable fraction of the weights.",
            "You start like that, like this.",
            "You already have your network in a good shape.",
            "We just go fine.",
            "So I think I'm going to stop here for this part and keep something for tomorrow.",
            "Oh I have a.",
            "Have any questions before for the last five minutes?",
            "I don't always do it that way.",
            "I've been doing it this way for 20 years.",
            "The fact is that in fact, if you look, for instance, the creature ski network, the code is not done that well.",
            "There's a lot of imbalance in various parts of the networks.",
            "You know, the fact is that most of the networks that, even if you don't do that right, they're going to work.",
            "But you can do it much better.",
            "By simple observations and then you know you start again later in the learning you used up your system.",
            "You collect some statistics and you look what's there in the various layers, the various activations, everything and you look what's wrong.",
            "Yes.",
            "Yes.",
            "They're not discontinuous the non differentiable in some point there continues.",
            "Franklin in the ranking algorithms you have.",
            "Maybe in the case of Jason West, and I'm pretty sure that continues.",
            "In the case of ranking in general, people have been trying to optimize violence of the end ECG criteria.",
            "The ranking criteria that rank at the top and they could be discontinuous.",
            "But what actually works best is to have a continuous variant of them.",
            "So if you look at all the work of Chris Burgess on Lambda rank, Lambda Max and so on, they all done in such a way that there continues, and in some cases differentiable.",
            "When this non continues, we really don't know what we do with optimization.",
            "The local information is not reliable so it could work, could be lucky.",
            "Yes.",
            "The parameters learning regimen.",
            "After running for.",
            "We are seeing this at 6 and then decide if it is a good initialization or not.",
            "Oh, but you can do that before.",
            "Like I told you how to initialize the weights and you you can do it incrementally.",
            "You start the first layer and you see what side of the weights you set the weights for that layer, and so that's going to be the size of the weights, and then you can start to go to the next layer and decide the size of the way so that the activation is right and you work up your network that way.",
            "Then you start computing gradients backwards and you set, which will be learning rates without learning anything.",
            "You just run it called.",
            "Well, you can initialize that altogether, but you look at the firstly your first and then you are going to re scale them to put everything in the right range.",
            "It's a very bad idea to start with the network where let's say some units are already dead, because basically you're going to saturate the hyperbolic tangent unit, so the sigmoid units, even even when you start.",
            "Do this at a rate is going to be a very long learning time.",
            "You just don't want to do that.",
            "Size of.",
            "For the learning rate, I'm suggesting something very simple that's.",
            "That's a gross, and we're going to discuss that later.",
            "Actually, that's a gross approximation of Newton, which says let's look at the average number of the weights in the layer.",
            "The average none of the of the gradient of the of the cost respect to the weights.",
            "That I'm going to multiply by some learning, right?",
            "And I want the product of the update I'm going to apply to my weights to be commensurable.",
            "With the size of the weights shouldn't should be smaller?",
            "Of course, let's say 1% or 10% of the size of the weights.",
            "But you certainly don't want them to be widely different.",
            "In various layers.",
            "And you can play with these numbers little bit.",
            "But then instead of having plenty of amateurs to adjust, you have one.",
            "What I didn't tell you is how to set the momentum, because I don't know.",
            "Yes.",
            "That changes typically when you initialize the way I suggest initially there not saturated because insulation that way.",
            "And they tend to saturate progressively.",
            "But what you want is then to saturate differently, like if you have a unit that saturated always on the same size all the time.",
            "Basically it's dead.",
            "So you want to avoid that, but it's also something you can check.",
            "So, so we learn all these things the hard way, because 20 years ago, computers well slower and there was a lot less margin for errors for four, but less margin for slow systems and not more time to look at them.",
            "So when when it runs for three weeks, you have time to look at the elementary statistics on the weights and everything, and I think we both wasted quite a.",
            "Quite a couple days or weeks at doing this.",
            "Just trying to see now network slowly learning networks something was wrong.",
            "That also created this culture of these ideas that you pilot the optimization.",
            "It lasted for weeks and you would come every morning and say OK, well my basic statistics, everything oh this is not good.",
            "Let's change it a little bit.",
            "Depends you do it every hour.",
            "Then you realize that it was not worth it so.",
            "But so that created this culture that all learning or networks is very tricky.",
            "You have to monitor things are just things and everything and all things like that.",
            "But you could automate that if you wanted to.",
            "No, in fact a lot of the algorithm that people propose are some kind of automation of these things instead, that sometimes it's the automation is a bit naive.",
            "I think you can do it better.",
            "But but what I'm describing here is not an algorithm that the principles that allow you to design an algorithm for each particular case, you're going to adjust a little bit with this principle and design an algorithm for to make it work, that's fine.",
            "Just.",
            "Yeah.",
            "Yes.",
            "Well, if you were to train a Christian Skynet with the old techniques.",
            "You would lose probably a couple person from the drop out.",
            "Uh, EE seems to be a bit easier to learn, so maybe you would learn a bit faster, even though you could balance the inside the internal statistics a bit better.",
            "You would lose in the pulling.",
            "Because the Max pooling is clearly better and contrast normalization helps.",
            "But you would still have been much better than the competing systems.",
            "So yes, there is a progress, but it's not a revolution is an evolution.",
            "Would you agree you sure?",
            "Well, I just say that because you know.",
            "They would, they would work there would.",
            "There is still in progress.",
            "There is progress there.",
            "Order direct ification of the Max pooling and the contrast normalization are clear effects and the drop party is a clear effect as well.",
            "There really the main argument for the reader you have to get the network bigger.",
            "But then it's a bit easier to trend, so this is what the paper says, and this seems to be true.",
            "So, so it's more evolution and revolution.",
            "One of the issues with the order, for instance, is that there was a well publicized paper saying that if you move just a little bit.",
            "One pixel in one image.",
            "You can make it any class you want, or pretty much like that now.",
            "Some of the observation is that when you have a role in network for image recognition, about 80% of the units in the flood zone at a given moment, they change.",
            "When you change example, but 2020% sparse basically.",
            "So now see what it means.",
            "If you want to preserve the signal you like the average signal to go up in each layer.",
            "To to be one.",
            "So that means that those are the activity must multiply the signal by 5 so that you get about the same kind of signal going up or otherwise going to go down and you get nothing.",
            "So that means that along the path of the active rollers, you multiply by 5 at actually every layer you have something like 10 layers.",
            "So that's very sensitive along this path.",
            "So of course if you move that pixel you can change anything on the output in a completely crazy way.",
            "So the story here is that.",
            "So what I'm describing is one of the drawbacks of whole lot of qualities, but that's one of the drawbacks is that along certain path is super sensitive to the inputs.",
            "So that answers your question directly.",
            "Saying well, you can have a very small change at the input that creates a large change on the output.",
            "If you choose your input right.",
            "Now, if you saturate with a sigmoid, well, you want not going to happen because the not going to be beyond the minus one.",
            "Oprah or zero and one or just designed that way.",
            "Yeah.",
            "Yes.",
            "Yes.",
            "This.",
            "This is me OK, this actually best seen in the context of convex optimization.",
            "The theorems about convex optimization use values assumption about the function.",
            "One of them is smoothness.",
            "And you can also optimise connections are not smooth.",
            "And typically the rates you can prove for nonsmooth function, so you have no clean curvature is lower.",
            "But when you do this, it's an essential tick rate.",
            "Meaning that you want to go to the very minimum of that function well when you're training in our net, you don't want to go to the minimum in the training set you want to meet testing set.",
            "You're going to stop before.",
            "And then more.",
            "Maybe I'll show some silly divisions that are not in the slides on the ball to to show how this happens in the stochastic gradient problem, where in fact you you have the aseptic retrospectively.",
            "One over thesis is smooth.",
            "And strongly convex.",
            "Can be one of a sqrt T if it's convex.",
            "But in fact you can also see that it's exponential up to a certain zone that depends on the size of the of the weights, and then it slows down.",
            "So if you play using right, you can go use you.",
            "You're not going to the end, and the fact that you have an inch is not a problem unless you algorithm looks for it.",
            "So if you have a particular engine you do linesearch, you're going to fall there.",
            "But if you have a stochastic noisy problem is not going to be like this.",
            "So this kind of issues that you have an actual hinge and an no real second derivative.",
            "Is less of a problem with algorithms?",
            "Are noisy because there is the effect you describe that at the scale you're looking at it looks moderately smooth.",
            "Or it could be could be smooth.",
            "It's plausibly smooth.",
            "Now, if you do a line search and you have a wedge like this, you do anyone optimization.",
            "Let's say anyone regularization with align search.",
            "The line search is going to be right to the point where there is no gradient and infinite curvature and you cooked.",
            "So you have to use all the algorithms for that.",
            "If you do it, stochastic is not the case, but you're not going to get sparsity that easily."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Under so today I'm going to speak about Matillion, our network, the kind that was.",
                    "label": 0
                },
                {
                    "sent": "Celebrated in the 50s, then forgotten in the 70s, then celebrated in the late 80s then.",
                    "label": 0
                },
                {
                    "sent": "Mike is off no oh.",
                    "label": 0
                },
                {
                    "sent": "No, it says on here.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "OK, so these things well famous in the 60s forgotten in the 70s, then famous again in the late 80s, then forgotten in the 2000, then famous again now.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to try to explain why, just explain what they are.",
                    "label": 0
                },
                {
                    "sent": "And at that point, when I wrote this slide where I had them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh no, I had something about success stories, but now everybody knows it that these things have success stories and an important part that they also had success stories in the 80s.",
                    "label": 0
                },
                {
                    "sent": "Like there's some check reading system where diploid and red checks for years with.",
                    "label": 0
                },
                {
                    "sent": "Without the new problem.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Anyway, fine.",
                    "label": 0
                },
                {
                    "sent": "So so actually three parts in this lecture, and they're going to be over the next two sessions now and tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one is what is the story of neural information processing like in nips you know, is neural information processing systems and what are the origins?",
                    "label": 1
                },
                {
                    "sent": "And an example that I like very much colder email out position on network.",
                    "label": 0
                },
                {
                    "sent": "And how you construct networks and you describe them.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second part is how you train them and it's connected to optimization loosely connected to optimization, because we don't really optimize, but how you initialize what, why do stochastic methods and how you could do better.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the third one, which is a bit of a bonus, is to try to make networks for complicated tasks, like when you have structured problem, auxiliary tasks and things like that.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to start with the org.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with the beginning, the percept.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Home.",
                    "label": 0
                },
                {
                    "sent": "So you probably seen that picture somewhere or picture like that.",
                    "label": 0
                },
                {
                    "sent": "One of the things that people forget sometimes that the perceptron had first written now and then what they call the associative area, which is kind of random connection.",
                    "label": 0
                },
                {
                    "sent": "Or so it actually had layers.",
                    "label": 0
                },
                {
                    "sent": "Going to an associative area, this is known as X.",
                    "label": 0
                },
                {
                    "sent": "Multiply by some weight so you complete the dot product of W&X, then the linear threshold, then you get the sign.",
                    "label": 0
                },
                {
                    "sent": "It tells you whether.",
                    "label": 0
                },
                {
                    "sent": "This is the object you want is a classification system.",
                    "label": 0
                },
                {
                    "sent": "And you might think that the perceptron is an algorithm, or the perceptron is a software, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's important to understand that perception is a machine, so this is Rosenblatt.",
                    "label": 0
                },
                {
                    "sent": "Is holding 8 weights in his hands.",
                    "label": 0
                },
                {
                    "sent": "So each weight is a potential matter with a little electrical engine, and whenever there is a mistake, meaning the perceptron makes a mistake, you press the button and it's a bit all over the place and it adjusts.",
                    "label": 0
                },
                {
                    "sent": "So one thing to understand is that the perceptron is not an algorithm that runs on a computer.",
                    "label": 0
                },
                {
                    "sent": "The perceptron was the computer.",
                    "label": 1
                },
                {
                    "sent": "And it was a cop.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After that could do things that no the computer of the time could do, like recognize characters like this.",
                    "label": 0
                },
                {
                    "sent": "You know more than we do image net and things like that, but at that time, well, there is no CCD to image something into your computer.",
                    "label": 0
                },
                {
                    "sent": "You had to use something pretty sophisticated and large.",
                    "label": 0
                },
                {
                    "sent": "And so, in the view of the authors of the perceptron.",
                    "label": 1
                },
                {
                    "sent": "That was not something to run on a computer that was a computer that was an alternative computing model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all this goes back to Vienna.",
                    "label": 0
                },
                {
                    "sent": "So 1948 you see science fiction orders in science fiction is pretty clear that.",
                    "label": 0
                },
                {
                    "sent": "That intelligence things all over the place and all these ID's there were not really.",
                    "label": 0
                },
                {
                    "sent": "Present.",
                    "label": 0
                },
                {
                    "sent": "In that sense, before Winner is the guy you said.",
                    "label": 0
                },
                {
                    "sent": "Well, learning happens everywhere happens in biological systems, in hospitals, in Automatical system that we do, it happens.",
                    "label": 0
                },
                {
                    "sent": "In societies he happened in sensitive process and should be studied as such.",
                    "label": 0
                },
                {
                    "sent": "And the perception is the continuation of this.",
                    "label": 0
                },
                {
                    "sent": "Studying learning as such.",
                    "label": 0
                },
                {
                    "sent": "So study learning and information processing were not that separate at that time.",
                    "label": 0
                },
                {
                    "sent": "And of course you have the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notion of how to design computers.",
                    "label": 0
                },
                {
                    "sent": "You could see the biological learning computer which we don't know how it should be done, so I put a little brand here, but that doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "And the mathematical computer, which is the ones we use nowadays, they're based on mathematical logic.",
                    "label": 0
                },
                {
                    "sent": "And that model, the model of building computers on mathematical logic, starting with the Turing machines, the Fundament machines and everything is clearly one there everywhere.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at this this computing with symbols you know everything you learn about computer science is based about this.",
                    "label": 0
                },
                {
                    "sent": "Programming means reducing a complex tasks into a collection of simple tasks.",
                    "label": 0
                },
                {
                    "sent": "Computer language has no meaning in the terms of learning.",
                    "label": 0
                },
                {
                    "sent": "You don't need to program a learning machine debugging well, you don't need to debug learning machines at any very different from debugging a program or parting systems, libraries API.",
                    "label": 0
                },
                {
                    "sent": "All these things now if you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the brand.",
                    "label": 0
                },
                {
                    "sent": "The running machine.",
                    "label": 0
                },
                {
                    "sent": "The best learning machine.",
                    "label": 0
                },
                {
                    "sent": "We know it's compact is 20 Watts, meaning is less than this laptop's.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of cells and neurons and the volume is mostly wires.",
                    "label": 0
                },
                {
                    "sent": "In fact, is very constrained.",
                    "label": 0
                },
                {
                    "sent": "In the sense that packing all these wires in such a small volume is a real challenge and there is a cell use bias on the connectivity between units.",
                    "label": 0
                },
                {
                    "sent": "Because of this challenge.",
                    "label": 0
                },
                {
                    "sent": "So is it a general computing machine like a Turing machine?",
                    "label": 1
                },
                {
                    "sent": "Not at all.",
                    "label": 0
                },
                {
                    "sent": "It's very slow for mathematical logic, arithmetic and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "It's very fast for computer vision, speech languages, social interactions.",
                    "label": 0
                },
                {
                    "sent": "And there is a kind of evolution in animals that start applying vision.",
                    "label": 0
                },
                {
                    "sent": "Then there is a theory that we developed language by reusing some of the vision mechanism.",
                    "label": 0
                },
                {
                    "sent": "And eventually we develop logic by reducing some of the language mechanics.",
                    "label": 0
                },
                {
                    "sent": "So let's go on and continue.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in 43 McCulloch and Pitts invent a simplified model of the neuron.",
                    "label": 0
                },
                {
                    "sent": "It's very simplified.",
                    "label": 0
                },
                {
                    "sent": "It's ridiculously simplified.",
                    "label": 0
                },
                {
                    "sent": "In fact, real neurons have nothing to do with that.",
                    "label": 0
                },
                {
                    "sent": "So in this model you have some X one X2X3 this other inputs of the neurons they go inside you compute the sum of WWIXI that product, pass it to some nonlinear function that typically a saturation and you distribute this result to other units.",
                    "label": 0
                },
                {
                    "sent": "And the best drone is based on this idea.",
                    "label": 0
                },
                {
                    "sent": "There is only one of this unit, the classification unit and the nonlinearity is just a threshold unit.",
                    "label": 0
                },
                {
                    "sent": "And people ask of course well, if the perceptron is the computer, can we use the perceptron to do all the things we want to do with the computer?",
                    "label": 0
                },
                {
                    "sent": "Just silly question.",
                    "label": 0
                },
                {
                    "sent": "The answer is obviously no, but it wasn't that clear.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it took a book and perceptrons in 68 to actually say that.",
                    "label": 0
                },
                {
                    "sent": "And they say that with very elegant mathematics.",
                    "label": 0
                },
                {
                    "sent": "And these are the basically the points made in the book, and they're all interesting, because there's still.",
                    "label": 0
                },
                {
                    "sent": "Important today.",
                    "label": 0
                },
                {
                    "sent": "So the first one is that linear threshold units.",
                    "label": 1
                },
                {
                    "sent": "They're very similar to Boolean gates.",
                    "label": 0
                },
                {
                    "sent": "Which is one of the building blocks of electronics.",
                    "label": 0
                },
                {
                    "sent": "Now the secret theory is very poorly known.",
                    "label": 0
                },
                {
                    "sent": "That we don't really much know how many gates we need to.",
                    "label": 0
                },
                {
                    "sent": "Construct a certain function or or if you have a flat or deep systems, how many?",
                    "label": 0
                },
                {
                    "sent": "How do you compare with the same number of gates?",
                    "label": 0
                },
                {
                    "sent": "Flatten deep systems in terms of what they can build.",
                    "label": 0
                },
                {
                    "sent": "This is not known very precisely.",
                    "label": 0
                },
                {
                    "sent": "That learning a deep secret means solving the credit assignment problem, which is supposed to be quite hard.",
                    "label": 0
                },
                {
                    "sent": "That if you don't know deep use, remain to you constraint to solving linearly separable problems that the few of the problems are linearly separable.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come back to how this has changed.",
                    "label": 0
                },
                {
                    "sent": "What is the credit assignment problem?",
                    "label": 0
                },
                {
                    "sent": "Well, you have an arrow at the top.",
                    "label": 0
                },
                {
                    "sent": "Of your system at the output, and you want to know who is responsible for this error.",
                    "label": 0
                },
                {
                    "sent": "So who should be corrected?",
                    "label": 0
                },
                {
                    "sent": "Is it a unit close to the top or you need somewhere in the middle?",
                    "label": 0
                },
                {
                    "sent": "And that's a priority.",
                    "label": 0
                },
                {
                    "sent": "If you're in the system of Boolean gates, it's very hard to say.",
                    "label": 0
                },
                {
                    "sent": "Then that identify no.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "I've no idea this is.",
                    "label": 0
                },
                {
                    "sent": "But I can't answer.",
                    "label": 0
                },
                {
                    "sent": "So the identify a set of elementary problems that need complex circuits within West the best.",
                    "label": 0
                },
                {
                    "sent": "None of these problems, the connectivity problem, where you have to decide whether a shape in an image is connected, is composed of a single connected component or not is something that's very difficult to do with learning system, and I'm not going to go into the details, but it's trivial to program.",
                    "label": 0
                },
                {
                    "sent": "And the conclusion is, well, all these learning machines, the interesting because they learn.",
                    "label": 0
                },
                {
                    "sent": "But what they can do is very limited.",
                    "label": 0
                },
                {
                    "sent": "So we should just program computers instead of trying to train them.",
                    "label": 0
                },
                {
                    "sent": "Do things and that led to going to symbolic computers and symbolic AI.",
                    "label": 0
                },
                {
                    "sent": "So this is not a ridiculous book.",
                    "label": 0
                },
                {
                    "sent": "You know it's a very small book in many respects.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But things have changed a little bit.",
                    "label": 0
                },
                {
                    "sent": "Secret theory is pulling on.",
                    "label": 0
                },
                {
                    "sent": "This is still true or some progress, but not much.",
                    "label": 0
                },
                {
                    "sent": "Learning deep secret means solving the credit assignment problems well.",
                    "label": 1
                },
                {
                    "sent": "It's easier than expected, but still puzzling.",
                    "label": 0
                },
                {
                    "sent": "We still don't know very much why.",
                    "label": 0
                },
                {
                    "sent": "Linearly separable problems are few, but that means I have a lot of value judgments dimension and this is supposed to be good for generalization, so we start looking at this aspect as as much a problem as a blessing.",
                    "label": 1
                },
                {
                    "sent": "Elementary problems need complex secrets.",
                    "label": 0
                },
                {
                    "sent": "Litter Connect city weather.",
                    "label": 0
                },
                {
                    "sent": "Bunch of Pixel is a single connected component well.",
                    "label": 0
                },
                {
                    "sent": "If you think about the maze and you want to know whether the maze a solution, you can go from the left to the right by going through all the little things that's a connected component problem.",
                    "label": 0
                },
                {
                    "sent": "We're not very good at it.",
                    "label": 0
                },
                {
                    "sent": "So humans are not very good at that either.",
                    "label": 0
                },
                {
                    "sent": "But have a simple organic solutions.",
                    "label": 0
                },
                {
                    "sent": "Well sometimes the problem is not specified clearly.",
                    "label": 0
                },
                {
                    "sent": "So it's very difficult to program something if you don't have a specification of the problem.",
                    "label": 0
                },
                {
                    "sent": "Typical examples recognize the digit four in an image.",
                    "label": 0
                },
                {
                    "sent": "You could say, OK, there are two strokes.",
                    "label": 0
                },
                {
                    "sent": "Let's find vertical stroke, an L shape stroke such that the L shape stroke crosses the vertical structure in the in the right place, and then you have a problem.",
                    "label": 0
                },
                {
                    "sent": "What is a stroke?",
                    "label": 0
                },
                {
                    "sent": "And then you have plenty of other images for that car going to be so different that they're not going to fit that thing.",
                    "label": 0
                },
                {
                    "sent": "So we need to add new rules and new things you just explode.",
                    "label": 0
                },
                {
                    "sent": "But there is still something else that we don't know how to do in learning how to reduce complex learning problems into simple ones.",
                    "label": 0
                },
                {
                    "sent": "And that's something I'm going to discuss a little bit at the very end, and it's not something we know very well.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to go into.",
                    "label": 0
                },
                {
                    "sent": "One of the motivations for neural networks.",
                    "label": 0
                },
                {
                    "sent": "That's the story of the regular propositional network, which is likely best described in the paper by McClelland and Rogers in 2003.",
                    "label": 0
                },
                {
                    "sent": "Another phone number problem with phones?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is something that from 68, but you probably know today it's called quillons propositional model.",
                    "label": 0
                },
                {
                    "sent": "So now it's not called that way is called knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Now if you look in your search engine on the right very often there are some little information about your word.",
                    "label": 0
                },
                {
                    "sent": "That's who are the parents.",
                    "label": 0
                },
                {
                    "sent": "So look some well known person they're going to tell you the parents and their children, the jobs, the kind of graph.",
                    "label": 0
                },
                {
                    "sent": "And so you can have a graph like this, while the top you have a living thing and the relation can grow is living is a animal can move her skin and you build all these triples between left relation right and you have a big table of triples and you have inheritance to like.",
                    "label": 0
                },
                {
                    "sent": "Sunfish is yellow, but is a fish.",
                    "label": 0
                },
                {
                    "sent": "That four is an animal, therefore Hoskin.",
                    "label": 0
                },
                {
                    "sent": "And you can navigate these things so this is one of the tools to make computers.",
                    "label": 0
                },
                {
                    "sent": "A little bit more knowledgeable than they used to be.",
                    "label": 0
                },
                {
                    "sent": "This tool is a lot of problems.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "If you want to access a specific property should be faster than accessing a generic property.",
                    "label": 0
                },
                {
                    "sent": "Like if you go back here, the Sunfish has a skin.",
                    "label": 0
                },
                {
                    "sent": "Well, sunfish is yellow.",
                    "label": 0
                },
                {
                    "sent": "Use immediately has a skin.",
                    "label": 0
                },
                {
                    "sent": "You need to navigate and go there.",
                    "label": 0
                },
                {
                    "sent": "And well, there is evidence that humans don't work like that.",
                    "label": 0
                },
                {
                    "sent": "Usually if I ask you whether a dog's forelegs?",
                    "label": 0
                },
                {
                    "sent": "You can tell me yes immediately for asking whether anomalous forelegs, you need to think about it.",
                    "label": 0
                },
                {
                    "sent": "Because you're not so sure.",
                    "label": 0
                },
                {
                    "sent": "So we really don't work that way.",
                    "label": 0
                },
                {
                    "sent": "The problem #2 is that this graph is very static.",
                    "label": 0
                },
                {
                    "sent": "If you want to change things or reorganize things or change the notion of easy, there is a relation little bit like you want to make it less.",
                    "label": 0
                },
                {
                    "sent": "Childish and more biological.",
                    "label": 0
                },
                {
                    "sent": "You need to reshuffle this network in a way that's very complex.",
                    "label": 0
                },
                {
                    "sent": "And that humans do very easily.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This idea of connection is in fact it comes from psychology in the 19th and 20th century.",
                    "label": 0
                },
                {
                    "sent": "And some psychologists actually see connection is a regression compared to other models.",
                    "label": 0
                },
                {
                    "sent": "And in the 80s there was a group called parallel Distributed Processing.",
                    "label": 0
                },
                {
                    "sent": "They produce a number of books that were famous.",
                    "label": 0
                },
                {
                    "sent": "And they try to push.",
                    "label": 0
                },
                {
                    "sent": "Some ideas about how the brain works that neural representations are distributed?",
                    "label": 0
                },
                {
                    "sent": "That neural computation is parallel.",
                    "label": 0
                },
                {
                    "sent": "The processing units, connectivity propagation rules are learning rules define.",
                    "label": 0
                },
                {
                    "sent": "How they work?",
                    "label": 0
                },
                {
                    "sent": "So basically they push the idea that the brand is a very very different machine than your typical computer with a single CPU, memory and everything.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the founding members of this group is Jeff Hinton with this.",
                    "label": 0
                },
                {
                    "sent": "Regular.",
                    "label": 0
                },
                {
                    "sent": "Question how does the brain work?",
                    "label": 0
                },
                {
                    "sent": "And if you ask, Jeff is still asking this actually.",
                    "label": 0
                },
                {
                    "sent": "Some say that he claims every two years that he now understands how the brain works, but the truth is that he wants to know how the brain works, and that I'm sure of that.",
                    "label": 0
                },
                {
                    "sent": "So in about 8687, I think I think it's 87.",
                    "label": 0
                },
                {
                    "sent": "In fact, Ramella proposed.",
                    "label": 0
                },
                {
                    "sent": "This note.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That actually corresponds to exactly what was in the graph.",
                    "label": 0
                },
                {
                    "sent": "So what do we have here?",
                    "label": 0
                },
                {
                    "sent": "We have inputs on the left, so these items by Norcross, Daisy, Ruben, Canaries, and Fish.",
                    "label": 0
                },
                {
                    "sent": "Salman, these are relations.",
                    "label": 0
                },
                {
                    "sent": "Any other possible outputs?",
                    "label": 0
                },
                {
                    "sent": "And you want to trend that thing so that if you put an item so you put camera?",
                    "label": 0
                },
                {
                    "sent": "Can I redo one and all the rest to 0?",
                    "label": 0
                },
                {
                    "sent": "And you select a particular relation.",
                    "label": 0
                },
                {
                    "sent": "You want the proper.",
                    "label": 0
                },
                {
                    "sent": "Properties to be illuminated on the output.",
                    "label": 0
                },
                {
                    "sent": "Oh how convenient and modern.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is a button here, amazing.",
                    "label": 0
                },
                {
                    "sent": "So that was 86.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting here is that if you look at the item here, we are a matrix of weight already goes into the representation layer here.",
                    "label": 0
                },
                {
                    "sent": "So nowadays we would call this an embedding.",
                    "label": 0
                },
                {
                    "sent": "So this is a in that case 8 dimensional space, in which each of these objects is going to have a point.",
                    "label": 0
                },
                {
                    "sent": "This is combined with the relation goes into some hidden layer and produces some things.",
                    "label": 0
                },
                {
                    "sent": "So how do you trend this?",
                    "label": 0
                },
                {
                    "sent": "This is what I'm going to explain backdrop.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the first thing was.",
                    "label": 0
                },
                {
                    "sent": "Replace the threshold unit by a sigmoid unit.",
                    "label": 0
                },
                {
                    "sent": "Because it's differentiable sagon collect training example which other trifles item relation these outputs.",
                    "label": 0
                },
                {
                    "sent": "But actually this is the whole set of these output.",
                    "label": 0
                },
                {
                    "sent": "Found the mean squared error for every example I want.",
                    "label": 0
                },
                {
                    "sent": "To compute the difference between the desired output and the actual output of my system squared.",
                    "label": 0
                },
                {
                    "sent": "Initialize with random waste optimized by gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Batch gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Now there was a problem of computing the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this structure, because while there was big for the time, no.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I go back to my little Mac Lock pizza model, I say that AI AJ sorry, the activation for this unit J is the sum of WIG's eyewear exile.",
                    "label": 0
                },
                {
                    "sent": "The outputs of what comes before WI J the connection.",
                    "label": 0
                },
                {
                    "sent": "The strength of the connection between I&J.",
                    "label": 0
                },
                {
                    "sent": "And XJ is some linear function of AJ.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there is something called back propagation.",
                    "label": 0
                },
                {
                    "sent": "And we have a cost function this squared error.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was here.",
                    "label": 0
                },
                {
                    "sent": "I want to compute the daily relative of this with respect to the weights of all mile units.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for simplification I'm calling or called G dot is the directive of.",
                    "label": 0
                },
                {
                    "sent": "This function respects to a dot for any unit.",
                    "label": 0
                },
                {
                    "sent": "And just apply the chain rule and you see that GI is F prime of AI for some.",
                    "label": 0
                },
                {
                    "sent": "For all the things that come in the past of WIJJ.",
                    "label": 0
                },
                {
                    "sent": "And then derivative of E with respect to await is excited GJ.",
                    "label": 0
                },
                {
                    "sent": "So you can just can just start on the last layer because you started directly from the cost function and you can back propagate the errors everywhere.",
                    "label": 0
                },
                {
                    "sent": "This is well known by now.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to explain this too much because I'm going to explain it better later.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the training algorithm that they used was a completely batching clear the gradient accumulators for each example.",
                    "label": 0
                },
                {
                    "sent": "Set the inputs as implied by item K in relation K. Computes all activations and states of all the units by forward propagation.",
                    "label": 0
                },
                {
                    "sent": "For all outputs, units compute the derivative of the cost with respect to the activation of that output unit.",
                    "label": 0
                },
                {
                    "sent": "Compute all the other JFK by back propagation.",
                    "label": 0
                },
                {
                    "sent": "And accumulate the component of the gradient into the Delta IJ, which is how much you're going to move WRJ in the end and make a little step.",
                    "label": 0
                },
                {
                    "sent": "It's a plan or fashion gradient descent that was very slow.",
                    "label": 0
                },
                {
                    "sent": "In fact, Rumala didn't do that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But some people are couple years later about the same time he was young, young looking for instance, and I was in front of them.",
                    "label": 0
                },
                {
                    "sent": "We didn't have computers powerful enough, so we did the stochastic way because.",
                    "label": 0
                },
                {
                    "sent": "Instead.",
                    "label": 0
                },
                {
                    "sent": "For each example, you do the same thing, but you accumulate after each example.",
                    "label": 1
                },
                {
                    "sent": "Instead of going over all the examples, we didn't do that because we were smart with that because our computers were too slow.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "So what do you?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See when the system learns well, you can look at the output of value.",
                    "label": 0
                },
                {
                    "sent": "Things like the oranges Canary can grow, so input is Canary, relationships can, and you look at the output of this growth cell in the output and you see this when you learn it goes quite fast to one and stays there.",
                    "label": 0
                },
                {
                    "sent": "Canary can move well.",
                    "label": 0
                },
                {
                    "sent": "Can I we can fly?",
                    "label": 0
                },
                {
                    "sent": "So of course interesting that depending on the specificity of the of the property, it goes slower.",
                    "label": 0
                },
                {
                    "sent": "But the most interesting is this one.",
                    "label": 0
                },
                {
                    "sent": "Pine has leaves initially.",
                    "label": 0
                },
                {
                    "sent": "Yes, because pine is probably kind of a tree.",
                    "label": 0
                },
                {
                    "sent": "And then I'll know pain is an exception.",
                    "label": 0
                },
                {
                    "sent": "And you see this dynamic in a simple gradient training, which is quite interesting, because this is what you would expect.",
                    "label": 0
                },
                {
                    "sent": "Then you can look at the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Presentation layer.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning, well, everything is random and not differentiated but after.",
                    "label": 0
                },
                {
                    "sent": "2500 bucks you can see that there is kind of a pattern that shows for the vegetables.",
                    "label": 0
                },
                {
                    "sent": "Animals.",
                    "label": 0
                },
                {
                    "sent": "Features will be different.",
                    "label": 0
                },
                {
                    "sent": "That just shows up.",
                    "label": 0
                },
                {
                    "sent": "In the year was 87 I believe.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do a little dendrogram of representations how things are ordered so at the beginning is a piece of crap, but very quickly you see the sun fission, the salmen go together well being and Canary go together, the trees go together, the Flowers grow together.",
                    "label": 0
                },
                {
                    "sent": "And you can also.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take the representation layer 8 dimensional, do a PCA and try to plot in the two main components where the value things go.",
                    "label": 0
                },
                {
                    "sent": "For the easy relation.",
                    "label": 0
                },
                {
                    "sent": "So you see the fish circle with the Sunfish and Salman the bird, the Canary and Robin flower Rosen.",
                    "label": 0
                },
                {
                    "sent": "Another thing that's interesting.",
                    "label": 0
                },
                {
                    "sent": "Is the dynamic reconfiguration that you could see already becausw during learning?",
                    "label": 0
                },
                {
                    "sent": "You see that there's some reconfiguration goes on, so the next image is a bit diff.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hunt is a typical animal things where you have dog God speaks and everything and if you have a child while the child is more likely to see a lot more dogs than pigs and goats at the beginning, everything is a dog, dog, dog, dog, dog, dog.",
                    "label": 0
                },
                {
                    "sent": "Then every animal is a dog.",
                    "label": 0
                },
                {
                    "sent": "And you see that the dog is a big circle here.",
                    "label": 0
                },
                {
                    "sent": "But after a while the thing learns to cover little bit of space for the goat.",
                    "label": 0
                },
                {
                    "sent": "Little bit of space for the pig, which are strange kinds of dogs.",
                    "label": 0
                },
                {
                    "sent": "And then the kinds of dogs become become animal, so one of the properties of this model is that is completely reconfigurable.",
                    "label": 0
                },
                {
                    "sent": "There is no need to shuffle pointers who change distributions or change pointers in memory cells and everything in the big tree.",
                    "label": 0
                },
                {
                    "sent": "You just give it more examples that we find the concepts or example that give and it's going to reconfigure like if you look at the thing.",
                    "label": 0
                },
                {
                    "sent": "Oh dog is a yes or I don't produce already.",
                    "label": 0
                },
                {
                    "sent": "Yeah the dog is got.",
                    "label": 0
                },
                {
                    "sent": "Which is a mistake of course, starts by being OK. Everything is a dog and then correct while dog is a dog is perfectly OK and got is got improves and we see that God is a God improves when you realize that this is not a dog anymore.",
                    "label": 0
                },
                {
                    "sent": "And the things that don't work just don't work very quickly.",
                    "label": 0
                },
                {
                    "sent": "Dog is a Canary too, is something that goes up and then goes down.",
                    "label": 0
                },
                {
                    "sent": "And you can attract.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dynamic reconfiguration of values.",
                    "label": 0
                },
                {
                    "sent": "A box quite amusing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I've shown here is very, very old and very, very simple neural networks.",
                    "label": 0
                },
                {
                    "sent": "Its goal is to represent what's called nowadays knowledge graph very small one because it's a small network and I wouldn't say it's the most efficient way to represent the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "But what you can see that's quite interesting is that it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Address some of the problems.",
                    "label": 0
                },
                {
                    "sent": "That the traditional knowledge drops how in terms of reconfiguration and in terms of speed of access.",
                    "label": 0
                },
                {
                    "sent": "So the PDP people they knew they were onto something that is not completely unreasonable to think that humans don't think the same way computers do.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the neural networks.",
                    "label": 0
                },
                {
                    "sent": "I explained back prob very quickly.",
                    "label": 0
                },
                {
                    "sent": "So a couple years later.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically we came with the idea of a construction kit because there was too complex.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with a linear brick.",
                    "label": 0
                },
                {
                    "sent": "So by by neurons we have X.",
                    "label": 0
                },
                {
                    "sent": "It's a vector, W is a matrix.",
                    "label": 0
                },
                {
                    "sent": "Multiply X by the.",
                    "label": 0
                },
                {
                    "sent": "And you get why.",
                    "label": 0
                },
                {
                    "sent": "That's a forward propagation.",
                    "label": 0
                },
                {
                    "sent": "Why is WX?",
                    "label": 0
                },
                {
                    "sent": "And if you have DE over DY.",
                    "label": 0
                },
                {
                    "sent": "You can compute the Y over the X.",
                    "label": 0
                },
                {
                    "sent": "By multiplying by the way to the other side.",
                    "label": 0
                },
                {
                    "sent": "Did I forget to transpose?",
                    "label": 0
                },
                {
                    "sent": "No, because I assume the gradients are.",
                    "label": 0
                },
                {
                    "sent": "Row vectors X the column vector gradient vectors in this structure.",
                    "label": 0
                },
                {
                    "sent": "And the same way you can get the E over W with a relatively simple operation.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can observe an interesting symmetry between these three relations.",
                    "label": 0
                },
                {
                    "sent": "Now you can have.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transfer function brick.",
                    "label": 0
                },
                {
                    "sent": "You have a vector X vector Y for each other component of X.",
                    "label": 0
                },
                {
                    "sent": "You apply your nonlinear function to Y and you can similarly right the backpropagation.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have a catalog of transfer function of the sigmoid, which is a typical of the first one.",
                    "label": 1
                },
                {
                    "sent": "With this kind of back propagation rule, the hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Boo.",
                    "label": 0
                },
                {
                    "sent": "Really rectified linear unit which is fashionable nowadays.",
                    "label": 0
                },
                {
                    "sent": "The fashion changes, you know, just not worry about that Max of 0 S which is and where you can write it.",
                    "label": 0
                },
                {
                    "sent": "You have the ramp, which is a rule with things like this and you can write it.",
                    "label": 0
                },
                {
                    "sent": "So this is an indicator actually fine.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can have a square loss brick.",
                    "label": 0
                },
                {
                    "sent": "So the output given.",
                    "label": 0
                },
                {
                    "sent": "Some values and some desired values while the output is actually the error is 1/2 of X minus the square, and you can also back propagate through this if you know DE over DY, which is the over the which is 1.",
                    "label": 0
                },
                {
                    "sent": "You can compute the E over the X.",
                    "label": 0
                },
                {
                    "sent": "And again, you.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can have a catalog of loose bricks.",
                    "label": 0
                },
                {
                    "sent": "The square loss low gloss, the hinge loss, the looks of Max, the Max margins.",
                    "label": 1
                },
                {
                    "sent": "People fight about the properties of all these losses and what's best and was not best.",
                    "label": 0
                },
                {
                    "sent": "Well for classification we know the log loss and the hinge loss or even the locks of Max the.",
                    "label": 0
                },
                {
                    "sent": "They're very close, actually.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last brick is the sequential brick.",
                    "label": 0
                },
                {
                    "sent": "So you have a sub bricks inside, be want the output of the one is the input of B2 and output of B2 is the input of the three, and so on, and the propagation is to apply the propagation rules to all of this in that order.",
                    "label": 0
                },
                {
                    "sent": "The back propagation rule is to apply.",
                    "label": 0
                },
                {
                    "sent": "The world backwards.",
                    "label": 0
                },
                {
                    "sent": "So you could call this an automatic differentiation system.",
                    "label": 0
                },
                {
                    "sent": "Is delivered zero over automatic differentiation because the real level comes when you have loops and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "But the benefits?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All that well, you're a flexible model or framework, and in fact many toolkits that have been produced for neural networks are based on that idea that you can test them.",
                    "label": 0
                },
                {
                    "sent": "You can test each brick separately by finite differences.",
                    "label": 0
                },
                {
                    "sent": "Make sure the guidance is correct.",
                    "label": 0
                },
                {
                    "sent": "I'd like to give an anecdote, sometimes have people coming to me and tell me back Prop doesn't learn this horrible is terrible.",
                    "label": 0
                },
                {
                    "sent": "I cannot adjust the parameters.",
                    "label": 0
                },
                {
                    "sent": "You know, I cannot find the sweet spot for the parameters and my first question is did you try to put the learning words very small though it?",
                    "label": 0
                },
                {
                    "sent": "Ah yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "I'll try and does the arrow go down?",
                    "label": 0
                },
                {
                    "sent": "No no it doesn't go down.",
                    "label": 0
                },
                {
                    "sent": "It's impossible.",
                    "label": 0
                },
                {
                    "sent": "So my other things checked the gradients because the gradients are always wrong.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I checked them know check the gradients.",
                    "label": 0
                },
                {
                    "sent": "It never fails.",
                    "label": 0
                },
                {
                    "sent": "You can try it yourself.",
                    "label": 0
                },
                {
                    "sent": "It's going to work.",
                    "label": 0
                },
                {
                    "sent": "You'll see it makes you feel good sometimes but.",
                    "label": 0
                },
                {
                    "sent": "Because the.",
                    "label": 0
                },
                {
                    "sent": "Did they really know?",
                    "label": 0
                },
                {
                    "sent": "I checked out, checked all the lines of the program 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "The gradients are correct, no?",
                    "label": 0
                },
                {
                    "sent": "And you have plenty of possibilities, so I have examples.",
                    "label": 0
                },
                {
                    "sent": "This is their new land, but I've touched code examples, but that's very similar indeed.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh well, two Leonel Networks is a sequential brick inside the mystery shape because we want to transform these images into a single vector linear brick, I public tangent Anatolia, Britain.",
                    "label": 0
                },
                {
                    "sent": "Lots of mass for classification and the criterion which is how you're going to define the cost function is negative log likelihood criterion, which is one of those I gave, and one that's good for multiclass problems.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example where decide to trend the network.",
                    "label": 0
                },
                {
                    "sent": "So what's important here is this function.",
                    "label": 0
                },
                {
                    "sent": "Well, people don't do it that way there, but it's interesting to see.",
                    "label": 0
                },
                {
                    "sent": "Which is something that's going to compute the gradient respect to the weights.",
                    "label": 0
                },
                {
                    "sent": "So you get the parameters you copy X into the parameters.",
                    "label": 0
                },
                {
                    "sent": "O the gradients.",
                    "label": 0
                },
                {
                    "sent": "Then for all your inputs you do forward.",
                    "label": 0
                },
                {
                    "sent": "You apply the criterion to compute the error.",
                    "label": 0
                },
                {
                    "sent": "I just left another error.",
                    "label": 0
                },
                {
                    "sent": "Go backward through the criterion to know the derivative of the outputs of the network.",
                    "label": 0
                },
                {
                    "sent": "Go backward through the network to know the gradients everywhere.",
                    "label": 0
                },
                {
                    "sent": "Accumulate the gradients and divide by the number of examples.",
                    "label": 0
                },
                {
                    "sent": "Well, why not?",
                    "label": 0
                },
                {
                    "sent": "And return the value of the function and the God of parameters is all you need to start an optimization program.",
                    "label": 0
                },
                {
                    "sent": "OK, this time is wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going too fast.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to go to the model that works the best.",
                    "label": 0
                },
                {
                    "sent": "The convolutional neural networks is the one that historically works the best.",
                    "label": 0
                },
                {
                    "sent": "And still the one that's responsible for most of the successful application of neural networks.",
                    "label": 0
                },
                {
                    "sent": "There are many reasons why this is the case.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons is probably that it's closely related, is well suited to the task.",
                    "label": 0
                },
                {
                    "sent": "And the task is typically vision or speech or perceptual problems.",
                    "label": 0
                },
                {
                    "sent": "And so we need to explain something about vision.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing I'm going to say that vision is fast.",
                    "label": 0
                },
                {
                    "sent": "So that comes from an influential paper, or Simon Thorpe.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What he did was the following to the subject and you show to the subject images of plenty of things, just pictures very quickly 10.",
                    "label": 0
                },
                {
                    "sent": "Past 10 by second just flashes in front of your eyes.",
                    "label": 0
                },
                {
                    "sent": "And at the end we asked them, did you see a dog?",
                    "label": 0
                },
                {
                    "sent": "Did you see a car and try to read the error rates?",
                    "label": 0
                },
                {
                    "sent": "And you want to know how fast you can flush the images.",
                    "label": 0
                },
                {
                    "sent": "Until they started making lots of mistakes.",
                    "label": 0
                },
                {
                    "sent": "And the conclusion of this is that you recognize an image about 0.1 seconds.",
                    "label": 0
                },
                {
                    "sent": "Now, the propagation delay of signal through one neuron real neuron is about 100 milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So we have time to go to 10 layers.",
                    "label": 0
                },
                {
                    "sent": "So there is no equilibration.",
                    "label": 0
                },
                {
                    "sent": "There is no, there is no sleuthing.",
                    "label": 0
                },
                {
                    "sent": "It looks that just out of timing constraints, this has to be 10 layers, completely feedforward.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "How is it organized?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "62 Rebel in visual meta.",
                    "label": 0
                },
                {
                    "sent": "Lot of experiments and.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All possible, it's true.",
                    "label": 0
                },
                {
                    "sent": "It's a good question is good remark is a good remark.",
                    "label": 0
                },
                {
                    "sent": "Then you have to believe that you can store a lot of images in your new short term memory.",
                    "label": 0
                },
                {
                    "sent": "Lot of complete images and at the time that was not considered very possible.",
                    "label": 0
                },
                {
                    "sent": "We should have some results by all the levers, suggesting that you can actually record quite a lot of images.",
                    "label": 0
                },
                {
                    "sent": "So yes, you objection is Israel.",
                    "label": 0
                },
                {
                    "sent": "At that time, people didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't believe it could be possible to store, let's say 100 images.",
                    "label": 0
                },
                {
                    "sent": "In your in your short term memory, Now people like Old Oliva believes it's possible.",
                    "label": 0
                },
                {
                    "sent": "OK, good point.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I never thought of it that way, that's true.",
                    "label": 0
                },
                {
                    "sent": "So well.",
                    "label": 0
                },
                {
                    "sent": "Essentially these people studied.",
                    "label": 0
                },
                {
                    "sent": "With the biological, you know, by looking at samples, staining things and staining neurons and looking at the patterns of connectivity, the connectivity patterns of the first neurons in the visual systems.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "They came with the conclusion that there are two kinds of cells that you could spot very easily.",
                    "label": 0
                },
                {
                    "sent": "You have simple cells that take a little neighborhood of the.",
                    "label": 0
                },
                {
                    "sent": "Or in the retina that detect local features?",
                    "label": 0
                },
                {
                    "sent": "And you have complex cells that pull local features and compute things that are bigger distance.",
                    "label": 0
                },
                {
                    "sent": "Of course people try to use this so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the 70s, Fukushima invented this very complicated thing called the new core neutron.",
                    "label": 1
                },
                {
                    "sent": "The dinner backdrop at the time so so the learning rule is to completely unsupervised method.",
                    "label": 0
                },
                {
                    "sent": "That's very ad hoc.",
                    "label": 0
                },
                {
                    "sent": "It's a bit complicated to get to work, but these things work know and have an input layer and then you have a contrast extraction which is made by taking local.",
                    "label": 0
                },
                {
                    "sent": "Connections in this image is just each unit.",
                    "label": 0
                },
                {
                    "sent": "Here seems a little neighborhood there and you do it again and again and again and these are made partly by hand in the construction edge extraction, partly by hand, and then some kind of supervised learning that's completely ad hoc, and at the end the recognition layer.",
                    "label": 0
                },
                {
                    "sent": "That's essentially the input of a perceptron.",
                    "label": 0
                },
                {
                    "sent": "And that thing actually works.",
                    "label": 0
                },
                {
                    "sent": "You can recognize things that way.",
                    "label": 0
                },
                {
                    "sent": "There is no backdrop, no nothing or all these.",
                    "label": 0
                },
                {
                    "sent": "You can see the kind of unsupervised learning as a form of K means.",
                    "label": 0
                },
                {
                    "sent": "Basically what it's doing is recording in the filters segments of the images are seen.",
                    "label": 0
                },
                {
                    "sent": "And there is no enviance but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the ideas there is this idea of local connection.",
                    "label": 0
                },
                {
                    "sent": "Instead of connecting to every unit there, you connect locally.",
                    "label": 0
                },
                {
                    "sent": "Which is interesting because there is a notion of locality if you connect to everybody you could scramble all the pixel by permutation wouldn't be different.",
                    "label": 0
                },
                {
                    "sent": "But if I show you a picture after scrambling all the pixels with a random permutation, not going to recognize anything.",
                    "label": 0
                },
                {
                    "sent": "However, how do you work on it?",
                    "label": 0
                },
                {
                    "sent": "The next step.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was to go to the convolution seiwell.",
                    "label": 0
                },
                {
                    "sent": "In the new code neutron, you observe that when you present examples that are shifted because for this to work you have to present example that I did in every position.",
                    "label": 0
                },
                {
                    "sent": "They tend to have the same weights in all positions.",
                    "label": 0
                },
                {
                    "sent": "They tend to develop representations that do not move, so you could force it.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, I'm going to have this unit seeing three pixels here and this one C3 pixels, and they're going to use the same weight, so this unit computes the same thing relative to these three pixel as this unit relative to the three pixels and so on, and three pixels.",
                    "label": 0
                },
                {
                    "sent": "This segment of an image, sorry.",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can make multiple convolutions, so you have the black one, the red one.",
                    "label": 0
                },
                {
                    "sent": "They have different thing.",
                    "label": 0
                },
                {
                    "sent": "And here this is the convolutional network.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the 90s there were things like this where you would take a character, do a bunch of convolution like 6.",
                    "label": 0
                },
                {
                    "sent": "And then subsample and do something like 16 with carefully organized connectivity pattern to save some computation.",
                    "label": 0
                },
                {
                    "sent": "Then sub sample and then do some linear bricks at the end.",
                    "label": 0
                },
                {
                    "sent": "So in 89 young started to have a big success in isolated and written character recognition.",
                    "label": 0
                },
                {
                    "sent": "In 91 there was face recognition systems that were working.",
                    "label": 0
                },
                {
                    "sent": "I worked on one at that point and one of the problems we had is that we had to remove the colors of the people from the images because the thing would be very good at organizing the shots rather than the faces.",
                    "label": 0
                },
                {
                    "sent": "Which is the problem?",
                    "label": 0
                },
                {
                    "sent": "That's that's real.",
                    "label": 0
                },
                {
                    "sent": "Oh in 90 three vehicle vehicle recognition that was in only row, which is in in France, in an in aerospace research lab.",
                    "label": 0
                },
                {
                    "sent": "One of the thing Steve Souder Dinara at the time was quite difficult, was to have a PC of the time equipped with a camera, which was not the USB camera.",
                    "label": 0
                },
                {
                    "sent": "That was a big thing and you would put little model of an airplane, and we tell you the position of the airplane.",
                    "label": 0
                },
                {
                    "sent": "Is it coming towards you leaving and so on.",
                    "label": 0
                },
                {
                    "sent": "And all this in near real time.",
                    "label": 0
                },
                {
                    "sent": "In 90 three there was a mean feat just in terms of engineering that was hard to do.",
                    "label": 0
                },
                {
                    "sent": "So in 94 Bell Labs had a zip code recognition that was working quite well, but didn't go into a product, but in 96 they had the check rating system.",
                    "label": 0
                },
                {
                    "sent": "And the check reading system was diploid about in 96 and worked until until 2000 and the end of the 2000.",
                    "label": 0
                },
                {
                    "sent": "Processed about 10 to 20% of the checks in the US.",
                    "label": 0
                },
                {
                    "sent": "That means 1 billion per year.",
                    "label": 0
                },
                {
                    "sent": "So that was a real application.",
                    "label": 0
                },
                {
                    "sent": "This is not a joke.",
                    "label": 0
                },
                {
                    "sent": "This was not laughing this.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He worked.",
                    "label": 0
                },
                {
                    "sent": "And they could recognize things like this.",
                    "label": 0
                },
                {
                    "sent": "I told you that before is not just two strokes.",
                    "label": 0
                },
                {
                    "sent": "That cross each other because you can have things like this.",
                    "label": 0
                },
                {
                    "sent": "You can have a bunch of very noisy situations.",
                    "label": 0
                },
                {
                    "sent": "They're not really trend for that, you know.",
                    "label": 0
                },
                {
                    "sent": "We just try things.",
                    "label": 0
                },
                {
                    "sent": "In fact, the way this works is that you have this big fat camera and then the set of slides like projector slides with the noise and everything will move them around to just see characters and then things.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An idea that appear clearly doing this just compared to the beginning is this idea of pulling.",
                    "label": 0
                },
                {
                    "sent": "So I told you that in the network sometimes you subsample because you want to trade.",
                    "label": 0
                },
                {
                    "sent": "Special resolution for qualitative information.",
                    "label": 0
                },
                {
                    "sent": "Initially you have an image is quite large, but each pixel is 3D, three numbers RGB.",
                    "label": 0
                },
                {
                    "sent": "Then the more you go forward you want to trade special resolution because you want to keep the information limited, but you want information that's richer.",
                    "label": 0
                },
                {
                    "sent": "It's going to be an object or there is a nature, or there's something like this.",
                    "label": 0
                },
                {
                    "sent": "And the question is how you do the resolution reduction that is called pulling and pulling is just a convolution in fact.",
                    "label": 0
                },
                {
                    "sent": "So the first one was average pooling.",
                    "label": 0
                },
                {
                    "sent": "You take this unique, takes a little neighborhood, and then you shift the neighborhood, but not by one pixel by two, so that you get the subsampling by two and you go on.",
                    "label": 0
                },
                {
                    "sent": "And then people discover that.",
                    "label": 0
                },
                {
                    "sent": "Well, if you do Max pooling, it works better.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, why?",
                    "label": 0
                },
                {
                    "sent": "Some people argue that we know in the brand there is something like this is compatible with Max pooling and rectification.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a posteriori theory, so fine, you have the L2 pool.",
                    "label": 0
                },
                {
                    "sent": "Tell people which is cover, so there is a catalogue of these things.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People started to add contrast normalization, so you know that if you have a strong light here and no light here, when you take a picture, the picture looks ugly.",
                    "label": 0
                },
                {
                    "sent": "But when we see.",
                    "label": 0
                },
                {
                    "sent": "If I have a lot of strong light on that side, I'm still going to see both sides of the of the room pretty well, so we have some contrast normalization built in, so we're not putting in the network, so.",
                    "label": 0
                },
                {
                    "sent": "You can smooth lowpass smooth version of the layer is just another convolution with six coefficient.",
                    "label": 0
                },
                {
                    "sent": "You have lots of violence per feature map across feature Maps.",
                    "label": 0
                },
                {
                    "sent": "There's just there's a catalog of these things that you can see.",
                    "label": 0
                },
                {
                    "sent": "Other people argue whether which one is better or not.",
                    "label": 0
                },
                {
                    "sent": "Of note",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the conversion networks in the 2010.",
                    "label": 0
                },
                {
                    "sent": "They look more like this convolution nonlinearity Max pull contrast normalization.",
                    "label": 0
                },
                {
                    "sent": "I do it again, convolution.",
                    "label": 0
                },
                {
                    "sent": "Nonlinearity Max pool consideration.",
                    "label": 0
                },
                {
                    "sent": "I do it again and we can do that many many times.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The code does not really change.",
                    "label": 0
                },
                {
                    "sent": "Is all this brick system?",
                    "label": 0
                },
                {
                    "sent": "Is sequential spatial convolution OK with all the filter size?",
                    "label": 0
                },
                {
                    "sent": "The how much you shift things and everything.",
                    "label": 0
                },
                {
                    "sent": "I probably tangent in that case special LP pulling then special subjective normalization OK, and I will start again and you have three layers and this is something with which you can recognize one of these kind of images for them.",
                    "label": 0
                },
                {
                    "sent": "I think that's a Google Street number database.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the Street View House numbers that this one traffic sign recognition, pedestrian detection.",
                    "label": 0
                },
                {
                    "sent": "Basically in a couple years.",
                    "label": 0
                },
                {
                    "sent": "This machine started to work better than the typical vision competition.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the turning point was the image net 2012 competition, where famous entry by by Alice Kryszewski, Leah and Jeff Hinton just beat everybody by a good margin.",
                    "label": 0
                },
                {
                    "sent": "And so these were the typical competitors, like 6 procedures, two kinds of SVM with a lot of 100 hand engineer feature extraction, and the Internet was pretty much half the error.",
                    "label": 0
                },
                {
                    "sent": "For classification, for detection was the same levels of shock in the vision community, and because they work really hard at this and what you see is something at work, that data Trump's engineering.",
                    "label": 0
                },
                {
                    "sent": "Data Trump's programming, so if we go back to the ID of computers, the computer your program in the computer that learn if you have data.",
                    "label": 0
                },
                {
                    "sent": "Computer done can be better.",
                    "label": 0
                },
                {
                    "sent": "So the computer you program with a lot of engineered heuristics is just as good as the heuristics you engineer.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                },
                {
                    "sent": "Can't see as much data.",
                    "label": 0
                },
                {
                    "sent": "You cannot see all the cases.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the the mandatory little image it image if you compare to the previous one.",
                    "label": 0
                },
                {
                    "sent": "Remember, I told you an input that was 28 by 28?",
                    "label": 0
                },
                {
                    "sent": "This one is 220 by 220, so it's I'm not, I'm bigger.",
                    "label": 0
                },
                {
                    "sent": "48 and 48 feature Maps on 96 instead of 6.",
                    "label": 0
                },
                {
                    "sent": "256 here 344 here.",
                    "label": 0
                },
                {
                    "sent": "So it's much much bigger than the networks of the 90s.",
                    "label": 0
                },
                {
                    "sent": "So one of the big challenges of the networks are much, much bigger.",
                    "label": 0
                },
                {
                    "sent": "There's no way we could run anything close to that.",
                    "label": 0
                },
                {
                    "sent": "Even a 10th of that at that time.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they recognize stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, you've seen it.",
                    "label": 0
                },
                {
                    "sent": "An important thing, however, is to realize that now we don't want to recognize object like image net in a small Patch.",
                    "label": 0
                },
                {
                    "sent": "We want to take a complete image and apply a conversion network to everything.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is a wrong way and there is a right way.",
                    "label": 0
                },
                {
                    "sent": "The wrong way is you take your image network and take the Python code and you scan the image to try to detect objects in various positions.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the easiest to program.",
                    "label": 0
                },
                {
                    "sent": "That's clearly the wrong way becausw.",
                    "label": 0
                },
                {
                    "sent": "What you should do is just expand the convolutions.",
                    "label": 0
                },
                {
                    "sent": "The convolutions are invariant operations.",
                    "label": 0
                },
                {
                    "sent": "You can do them on a small image.",
                    "label": 0
                },
                {
                    "sent": "You can do them on the larger image with the same weights.",
                    "label": 0
                },
                {
                    "sent": "And if you expand the combination, you change the input image, which is a large image with three channels into an output image which is.",
                    "label": 0
                },
                {
                    "sent": "Lower resolution image with a number of channels corresponding to the number of classes you want to detect and when you do this you share a lot of computation on the way.",
                    "label": 0
                },
                {
                    "sent": "This goes all those of my teeth faster than this one.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so again, in the early 2000s are plenty of work on this, so that was a younger kind, friends.",
                    "label": 0
                },
                {
                    "sent": "I think France include certainly Joshua others, including working with.",
                    "label": 0
                },
                {
                    "sent": "We played with these things trying to recognize replicated faces.",
                    "label": 0
                },
                {
                    "sent": "And that poses well in all kinds of images, and they were not trained on aliens.",
                    "label": 0
                },
                {
                    "sent": "They were not trained on cartoon characters, but it worked quite well anyway.",
                    "label": 0
                },
                {
                    "sent": "So that's one side of the story.",
                    "label": 0
                },
                {
                    "sent": "That's the best known.",
                    "label": 0
                },
                {
                    "sent": "Another one is CNN's for speech recognition.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's about the same time.",
                    "label": 0
                },
                {
                    "sent": "Are in 88.",
                    "label": 0
                },
                {
                    "sent": "There was a hint on in Lang and Alex Waibel and in did a big speaker independent from recognition, that was running on one of these fancy Alliant computers.",
                    "label": 0
                },
                {
                    "sent": "That's lower than your cell phone.",
                    "label": 0
                },
                {
                    "sent": "But that cost a lot more.",
                    "label": 0
                },
                {
                    "sent": "And then you sure worked a lot on this, but with first like this and then we can, then there are speaker independent, worker permission, continuous speech recognition.",
                    "label": 0
                },
                {
                    "sent": "All this with CNN's that were just very close.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then more and more complicated things with dynamic programming it with recurrent network, all kinds of things were tried.",
                    "label": 0
                },
                {
                    "sent": "And when you show us that we did our thesis on pretty much the same topics, well that was it.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the 19th they were competitive with the Goshen hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "They're working pretty much as well, do it.",
                    "label": 0
                },
                {
                    "sent": "More difficult to use because you had this strange learning process used with a couple parameters.",
                    "label": 0
                },
                {
                    "sent": "You had to adjust.",
                    "label": 0
                },
                {
                    "sent": "So they were not outperforming the existing system enough to justify your switch.",
                    "label": 0
                },
                {
                    "sent": "2010 alot more data.",
                    "label": 0
                },
                {
                    "sent": "Lot more compute power.",
                    "label": 0
                },
                {
                    "sent": "Not more results.",
                    "label": 0
                },
                {
                    "sent": "This kind of systems or variants of these systems just.",
                    "label": 0
                },
                {
                    "sent": "Beat the performance of Russian hidden Markov model by good margin.",
                    "label": 0
                },
                {
                    "sent": "Now pretty much all the major speech recognition systems are switched to neural network acoustic models that happened in the space of one year, one year and a half 2011 2012.",
                    "label": 1
                },
                {
                    "sent": "So very often yes question.",
                    "label": 0
                },
                {
                    "sent": "Do you know what happened is?",
                    "label": 0
                },
                {
                    "sent": "Nobody knows.",
                    "label": 0
                },
                {
                    "sent": "Nobody knows.",
                    "label": 0
                },
                {
                    "sent": "The hydran Bible.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, you know.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of things that are in an already is our Internet on top of a confident people do this kind of things.",
                    "label": 0
                },
                {
                    "sent": "So so in the in the 90s that Tony Robinson did a very complicated system of that kind.",
                    "label": 1
                },
                {
                    "sent": "Basically with the bigger running on top and for a couple years that was the best performing system around.",
                    "label": 0
                },
                {
                    "sent": "You were just complicated to run.",
                    "label": 1
                },
                {
                    "sent": "The engineering was complicated in comparison, running a hidden Markov model was very simple.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the LSD endo.",
                    "label": 0
                },
                {
                    "sent": "That was not an idea that was the clean at that time.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to enter my second part, which is the part that's more technical about the optimization part.",
                    "label": 0
                },
                {
                    "sent": "How to train neural networks.",
                    "label": 0
                },
                {
                    "sent": "And on.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to start by some optimization basics.",
                    "label": 0
                },
                {
                    "sent": "Things that are.",
                    "label": 0
                },
                {
                    "sent": "Not that complicated, but they good to know about optimization.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with batch algorithms because this simple.",
                    "label": 0
                },
                {
                    "sent": "And the first thing about the.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Story of convexity.",
                    "label": 0
                },
                {
                    "sent": "Complexity is very important in optimization.",
                    "label": 0
                },
                {
                    "sent": "Not all of optimization is convex, but it's a big part of it.",
                    "label": 0
                },
                {
                    "sent": "And convexity means that if you take two points on this surface, the segment that connects the two points.",
                    "label": 0
                },
                {
                    "sent": "Is able to surface.",
                    "label": 0
                },
                {
                    "sent": "This is what this is.",
                    "label": 0
                },
                {
                    "sent": "And the resulting properties at any local minimum is a global minimum.",
                    "label": 0
                },
                {
                    "sent": "She's very convenient.",
                    "label": 0
                },
                {
                    "sent": "And conclusion with optimization, libraries are easy to use because they always give the same result.",
                    "label": 0
                },
                {
                    "sent": "And also easy to analyze mathematically.",
                    "label": 0
                },
                {
                    "sent": "Example linear model with the convex loss function that's convex.",
                    "label": 0
                },
                {
                    "sent": "That's basically if you add the regularizer, it's SVM basically.",
                    "label": 0
                },
                {
                    "sent": "Curve fitting with mean squared all natural linear classification with loss of hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. Now I know.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Convex is something like this.",
                    "label": 0
                },
                {
                    "sent": "You lose these properties you have now have a complicated landscape is low dimensional in high dimension is very hard to fathom.",
                    "label": 0
                },
                {
                    "sent": "Local minima saddle points plateau.",
                    "label": 0
                },
                {
                    "sent": "Big Platearius summer revines someone just what?",
                    "label": 0
                },
                {
                    "sent": "All this stuff.",
                    "label": 0
                },
                {
                    "sent": "An optimization algorithm usually finds the local minimum at best.",
                    "label": 0
                },
                {
                    "sent": "Some are good, some are bad.",
                    "label": 0
                },
                {
                    "sent": "And the results can depend on subtle details.",
                    "label": 0
                },
                {
                    "sent": "It's possible very easily to build surfaces like this that are going to be almost impossible to optimize.",
                    "label": 0
                },
                {
                    "sent": "And examples are material networks, clustering, learning features, mixture model hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "There two are non convex.",
                    "label": 0
                },
                {
                    "sent": "Silly feature selections.",
                    "label": 0
                },
                {
                    "sent": "Basically all the important learn machine learning problems except for simple linear problems.",
                    "label": 0
                },
                {
                    "sent": "Simple linear models tend to be non convex.",
                    "label": 0
                },
                {
                    "sent": "So sometimes you find a very smart paper and it's a lot of smarts that try to transform a problem like this into a convex problems that can be analyzed.",
                    "label": 0
                },
                {
                    "sent": "And that's not something negligible.",
                    "label": 0
                },
                {
                    "sent": "It's an achievement.",
                    "label": 0
                },
                {
                    "sent": "And usually a good interesting starting point.",
                    "label": 0
                },
                {
                    "sent": "But well, I'm going to take the example feature selection.",
                    "label": 0
                },
                {
                    "sent": "For instance, feature selection in the linear model you have just one unit linear log loss.",
                    "label": 0
                },
                {
                    "sent": "Pick your classifier.",
                    "label": 0
                },
                {
                    "sent": "Typical SVM and you would like to select the important features and people tell you well you can use L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Because everyone regularization is the one that's going to select a small number of features and remains convex.",
                    "label": 0
                },
                {
                    "sent": "Now the secret is that if use L1 half regularization instead of using just the sum of absolute value of the weight using the sum of the square root of the absolute value of the weights, it works better.",
                    "label": 0
                },
                {
                    "sent": "You get less features for the same performance.",
                    "label": 0
                },
                {
                    "sent": "Now the second secret is that if you want to learn something with L 1/2.",
                    "label": 0
                },
                {
                    "sent": "Is better to start by learning your L1.",
                    "label": 0
                },
                {
                    "sent": "And then you switch your regularizer to L 1/2.",
                    "label": 0
                },
                {
                    "sent": "And you make a couple of steps.",
                    "label": 0
                },
                {
                    "sent": "This is what's going to work the best, so the convex approximation was not useless.",
                    "label": 0
                },
                {
                    "sent": "It was a very good starting point, a very interesting one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another aspect of optimization is this idea of derivatives.",
                    "label": 0
                },
                {
                    "sent": "Stupid idea.",
                    "label": 0
                },
                {
                    "sent": "R. The Liberty Vindicator general position of the closest local minimum, but was very important that Liberty is a local cue.",
                    "label": 0
                },
                {
                    "sent": "You, in one point, you observe the derivative, and it tells you well go this way.",
                    "label": 0
                },
                {
                    "sent": "This is probably the right way.",
                    "label": 0
                },
                {
                    "sent": "This happens only when you have a differentiable function.",
                    "label": 0
                },
                {
                    "sent": "If you have something like logic where everything is 1 and zeros.",
                    "label": 0
                },
                {
                    "sent": "You know it's flat everywhere.",
                    "label": 0
                },
                {
                    "sent": "Somewhere there is a Cliff, but when you take a point you don't know where the cliffs are, you don't know which way to go.",
                    "label": 0
                },
                {
                    "sent": "And this is what makes discrete optimization a lot harder than continuous optimization.",
                    "label": 0
                },
                {
                    "sent": "So one of the tricks of neural networks the changing the threshold you need by a sigmoid is very important because you create a situation where at every point you have a richer information that tells you.",
                    "label": 0
                },
                {
                    "sent": "Weather in which direction?",
                    "label": 0
                },
                {
                    "sent": "The closest local minimum is.",
                    "label": 0
                },
                {
                    "sent": "It's not a very good information, sometimes not always perfect, but it's more.",
                    "label": 0
                },
                {
                    "sent": "It's more than no information.",
                    "label": 0
                },
                {
                    "sent": "Now you have a second derivative.",
                    "label": 0
                },
                {
                    "sent": "Not only you have a general position, we have an estimate of the position because if you have a first and second derivative, you can fit a quadratic and it tells you well, probably around here is not a bad idea.",
                    "label": 0
                },
                {
                    "sent": "You may.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About linesearch suppose you have a line like this in one D. You want to find the minimum algorithmically.",
                    "label": 0
                },
                {
                    "sent": "Well, the first thing you have to understand they have to bracket the minimum to set the minimum within two points.",
                    "label": 0
                },
                {
                    "sent": "And to bracket the minimum you need three points.",
                    "label": 0
                },
                {
                    "sent": "You need this one, this one and this one.",
                    "label": 0
                },
                {
                    "sent": "The important thing that the value in this point is less than the value here and less than the value here.",
                    "label": 0
                },
                {
                    "sent": "When you know this, if you assume the function is continuous.",
                    "label": 0
                },
                {
                    "sent": "You know that you have a minimum between the two light blue points.",
                    "label": 0
                },
                {
                    "sent": "You don't know where exactly.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can probe another point here.",
                    "label": 0
                },
                {
                    "sent": "And see how.",
                    "label": 0
                },
                {
                    "sent": "Its value.",
                    "label": 0
                },
                {
                    "sent": "Ranks compared to the value of the others, and you discovered.",
                    "label": 0
                },
                {
                    "sent": "Now you have a new bracket of the minimum here, smaller.",
                    "label": 0
                },
                {
                    "sent": "And you can go on.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem with the point here with the inside the new bracket and are.",
                    "label": 0
                },
                {
                    "sent": "I think that one is the new bracket.",
                    "label": 0
                },
                {
                    "sent": "And again.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, it's easy to prove that if you want to be mostly effective at this game, you should put your query point.",
                    "label": 0
                },
                {
                    "sent": "In the biggest segment in your view bracket, at the position that obeys the Golden ratio.",
                    "label": 0
                },
                {
                    "sent": "Just them.",
                    "label": 0
                },
                {
                    "sent": "Normally just a little recursion is going to show you that.",
                    "label": 0
                },
                {
                    "sent": "OK, but here I just use the value of the function.",
                    "label": 0
                },
                {
                    "sent": "What about my local information?",
                    "label": 0
                },
                {
                    "sent": "I just assume the function is continuous.",
                    "label": 0
                },
                {
                    "sent": "Whichever is better than this continues.",
                    "label": 0
                },
                {
                    "sent": "And they just use the values.",
                    "label": 0
                },
                {
                    "sent": "So can I use the derivatives?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually I can use if I have three points, I could do something smarter, which is a parabolic interpolation and said now I'm going to take my points here.",
                    "label": 0
                },
                {
                    "sent": "My estimate of the minimum.",
                    "label": 0
                },
                {
                    "sent": "And say maybe it's going to go faster.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it goes faster.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a case where it's not going to go faster or something like this.",
                    "label": 0
                },
                {
                    "sent": "This estimate.",
                    "label": 0
                },
                {
                    "sent": "Is wrong?",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to have this bracket now and I'm going to do the same mistake all the time, so I'm going to progress to that minimum very slowly.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you want and if.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient line search or you have to use the brand method you alternate.",
                    "label": 0
                },
                {
                    "sent": "You have two methods.",
                    "label": 0
                },
                {
                    "sent": "One is.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe not the fastest, but you know how fast it goes.",
                    "label": 0
                },
                {
                    "sent": "It goes that way all the time.",
                    "label": 0
                },
                {
                    "sent": "The other one, the other one could be faster in common situations, but there's always, but you do just one step of each.",
                    "label": 0
                },
                {
                    "sent": "That way you know more than twice slower than the slow one.",
                    "label": 0
                },
                {
                    "sent": "But if the first one is faster while you fast is Siri ID, and of course you can use derivatives if you can compute F&F prime together.",
                    "label": 0
                },
                {
                    "sent": "Bicep bicep together that's important if you.",
                    "label": 0
                },
                {
                    "sent": "You don't want the cost of F prime to the computer experiment to be too high, but if you can compute F&F prime together, you can go faster.",
                    "label": 0
                },
                {
                    "sent": "If you can compute F prime F so gone together, you can go faster again.",
                    "label": 0
                },
                {
                    "sent": "So now I'm.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To go back to my values layer.",
                    "label": 0
                },
                {
                    "sent": "So I think my linear units here have an input XI compute WX, which I call a activation.",
                    "label": 0
                },
                {
                    "sent": "I take a non linearity FANY.",
                    "label": 0
                },
                {
                    "sent": "And I've got propagation equation in my backpropagation equations.",
                    "label": 0
                },
                {
                    "sent": "That I've seen now I'm going to make a slight change.",
                    "label": 0
                },
                {
                    "sent": "I'm going to make my non linearity twice steeper.",
                    "label": 0
                },
                {
                    "sent": "And at the same times I'm going to make the weights twice smaller.",
                    "label": 0
                },
                {
                    "sent": "So I'm computing the same thing with the same XI, get the same, why?",
                    "label": 0
                },
                {
                    "sent": "So that leaves Y of X and change.",
                    "label": 0
                },
                {
                    "sent": "So what can you say about the change to the weight?",
                    "label": 0
                },
                {
                    "sent": "If you run the back propagation.",
                    "label": 0
                },
                {
                    "sent": "So that's the bigger size.",
                    "label": 0
                },
                {
                    "sent": "Any idea?",
                    "label": 0
                },
                {
                    "sent": "So the weights have been divided by two.",
                    "label": 0
                },
                {
                    "sent": "But the nonlinearity has been doubled.",
                    "label": 0
                },
                {
                    "sent": "Now you back propagate.",
                    "label": 0
                },
                {
                    "sent": "What do you see?",
                    "label": 0
                },
                {
                    "sent": "Oh come on, come on.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is multiplied by 4, that's correct.",
                    "label": 0
                },
                {
                    "sent": "If you go back.",
                    "label": 0
                },
                {
                    "sent": "You go back here so if promised was steeper.",
                    "label": 0
                },
                {
                    "sent": "So GA is double.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "GX equals GW, so GX is twice bigger now.",
                    "label": 0
                },
                {
                    "sent": "For the Cemex.",
                    "label": 0
                },
                {
                    "sent": "Now Delta W is also twice bigger, but the weights are twice smaller.",
                    "label": 0
                },
                {
                    "sent": "So now comparative to the size of the weights.",
                    "label": 0
                },
                {
                    "sent": "The update is twice is 4 times bigger.",
                    "label": 0
                },
                {
                    "sent": "Isn't that a bit annoying?",
                    "label": 0
                },
                {
                    "sent": "You did a change in your system that changes nothing forward, but the learning is completely different.",
                    "label": 0
                },
                {
                    "sent": "So what do you think about that?",
                    "label": 0
                },
                {
                    "sent": "You know you take the steepest direction that the gradient is the steepest descent.",
                    "label": 0
                },
                {
                    "sent": "And now the steepest descent is changed because I melted, I multiply the Empire met F twice deeper, and a divide W by two.",
                    "label": 0
                },
                {
                    "sent": "OK, we come back to that.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just keep that somewhere in your head that there is something fishy there.",
                    "label": 0
                },
                {
                    "sent": "So let's take a power bowler.",
                    "label": 0
                },
                {
                    "sent": "What do you say?",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry I could understand.",
                    "label": 0
                },
                {
                    "sent": "Well, you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't exactly know, but one side effect of being doing this change dividing the W by two and making FY steeper that are effectively multiply the learning rate by 4.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "What well, I just showed one particular change, but I can make many changes that preserve the forward.",
                    "label": 0
                },
                {
                    "sent": "The rule, especially with the role, is very easy to play because they know you divide the weights in one layer for one unit by two, multiply the output waste.",
                    "label": 0
                },
                {
                    "sent": "For that you need by by two.",
                    "label": 0
                },
                {
                    "sent": "You don't change anything but all the units in the similar could be done differently, so there are plenty of invariant.",
                    "label": 0
                },
                {
                    "sent": "Transformations forward that are not invariant backward for the learning.",
                    "label": 0
                },
                {
                    "sent": "So that tells you that the gradient algorithm doesn't go by itself.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take my Powerball are here.",
                    "label": 0
                },
                {
                    "sent": "You have W = C is curvature over by two double square.",
                    "label": 0
                },
                {
                    "sent": "The gradient decent is WT plus one equals WT minus Y to deliberative of East.",
                    "label": 0
                },
                {
                    "sent": "Respect the W in WT so the size again questions how does it affect the convergence and what's the best value of data?",
                    "label": 1
                },
                {
                    "sent": "Well, some of you know already yes.",
                    "label": 0
                },
                {
                    "sent": "You can bounce.",
                    "label": 0
                },
                {
                    "sent": "But you can calculate it better.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is that my plan was to ever blackballed and write things, but the blackboard is under the thing.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "There's a fancy button somewhere I've seen you sure do that.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Is it good enough?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "You have the function there WT plus one equal WT minus ITA.",
                    "label": 0
                },
                {
                    "sent": "Would that be?",
                    "label": 0
                },
                {
                    "sent": "WT.",
                    "label": 0
                },
                {
                    "sent": "CWT equal 1 -- y to CWT.",
                    "label": 0
                },
                {
                    "sent": "Haha.",
                    "label": 0
                },
                {
                    "sent": "So that is a geometric thing.",
                    "label": 0
                },
                {
                    "sent": "So if this as an absolute value less than one.",
                    "label": 0
                },
                {
                    "sent": "You're going to go to the minimum.",
                    "label": 0
                },
                {
                    "sent": "If this is negative.",
                    "label": 0
                },
                {
                    "sent": "You bounce.",
                    "label": 0
                },
                {
                    "sent": "But this could be minus 1/2.",
                    "label": 0
                },
                {
                    "sent": "You're going to bounce, but go to the minimum.",
                    "label": 0
                },
                {
                    "sent": "So if it's minus 1/2, you're going to do something like this.",
                    "label": 0
                },
                {
                    "sent": "If it's 1/2 you're going to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "If it's.",
                    "label": 0
                },
                {
                    "sent": "If this is more than one.",
                    "label": 0
                },
                {
                    "sent": "You go.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "If it's less than minus one, you go elsewhere, but on the other side.",
                    "label": 0
                },
                {
                    "sent": "So now you know what is the range of values of E to the target.",
                    "label": 0
                },
                {
                    "sent": "You want.",
                    "label": 0
                },
                {
                    "sent": "Ether to be between.",
                    "label": 0
                },
                {
                    "sent": "I'm awful to do.",
                    "label": 0
                },
                {
                    "sent": "Help me please.",
                    "label": 0
                },
                {
                    "sent": "So we want this to be like valkyr.",
                    "label": 0
                },
                {
                    "sent": "Let's write it seriously.",
                    "label": 0
                },
                {
                    "sent": "You want 1 -- E to C2B.",
                    "label": 0
                },
                {
                    "sent": "Between one and minus one.",
                    "label": 0
                },
                {
                    "sent": "So you can remove a one here screen minus two less than minus 8C0.",
                    "label": 0
                },
                {
                    "sent": "Wait, what?",
                    "label": 0
                },
                {
                    "sent": "Yes, should be strictly less than one that's correct strictly.",
                    "label": 0
                },
                {
                    "sent": "So I removed one so I can put you on ether C between zero and two.",
                    "label": 0
                },
                {
                    "sent": "Therefore you want it to to be between 2 / C and 0.",
                    "label": 0
                },
                {
                    "sent": "But the learning rate is positive, seems normal.",
                    "label": 0
                },
                {
                    "sent": "You want to go down.",
                    "label": 0
                },
                {
                    "sent": "And that's the maximum learning rate to oversee that you can tolerate if it's bigger than that.",
                    "label": 0
                },
                {
                    "sent": "Well, actually this is going to be the minus one side, and you're going to diverge.",
                    "label": 0
                },
                {
                    "sent": "OK oh.",
                    "label": 0
                },
                {
                    "sent": "My second question was.",
                    "label": 0
                },
                {
                    "sent": "What's the best value of Eater?",
                    "label": 0
                },
                {
                    "sent": "Yes, whenever she is the one that's going to, you're going to go straight to the minimum.",
                    "label": 0
                },
                {
                    "sent": "Have 1 / C. So there was one guy.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More dimensions.",
                    "label": 0
                },
                {
                    "sent": "See quatic.",
                    "label": 0
                },
                {
                    "sent": "I have two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And two different curvatures.",
                    "label": 0
                },
                {
                    "sent": "So basically I have to see.",
                    "label": 0
                },
                {
                    "sent": "So how does it affect the comment was the best value of beta.",
                    "label": 0
                },
                {
                    "sent": "So I have a Cmax and cmin.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Wally, just it is not in the matrix is just this color.",
                    "label": 0
                },
                {
                    "sent": "But it's clear that.",
                    "label": 0
                },
                {
                    "sent": "You can write the same thing and diagonalize, so you can write the same thing along each of the axis.",
                    "label": 0
                },
                {
                    "sent": "In that case update Axis parallel is simpler.",
                    "label": 0
                },
                {
                    "sent": "So you want to eat that to satisfy both conditions should be less than two oversee mean and less of a two or C Max.",
                    "label": 0
                },
                {
                    "sent": "So that they see what it should be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so basically you're constrained now.",
                    "label": 0
                },
                {
                    "sent": "The speed at which you're going to go.",
                    "label": 0
                },
                {
                    "sent": "The speed is going to go is going to be controlled by these ratios.",
                    "label": 0
                },
                {
                    "sent": "1 -- 8 C, So you want mine minus 8 AC to be in absolute value less than 1.",
                    "label": 0
                },
                {
                    "sent": "Four in every direction.",
                    "label": 0
                },
                {
                    "sent": "But what's going to control the speed?",
                    "label": 0
                },
                {
                    "sent": "Is the slowest one.",
                    "label": 0
                },
                {
                    "sent": "Because even if you converge in all the directions, let's be optimistic and all of the directions so fast you converge is the slow direction that's going to concern.",
                    "label": 0
                },
                {
                    "sent": "And even if you converge in the fast directions, you cannot raise it to because a little bit of noise in the calculation, and you're going to diverge in the first direction.",
                    "label": 0
                },
                {
                    "sent": "So when you have.",
                    "label": 0
                },
                {
                    "sent": "An eccentric ellipse, your constraint.",
                    "label": 0
                },
                {
                    "sent": "And basically you know the maximum learning rate is constrained by the strong curvature, and the speed is constrained by the low curvature.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this something we should change?",
                    "label": 0
                },
                {
                    "sent": "Access so basically?",
                    "label": 0
                },
                {
                    "sent": "No, just that you can diagonalize and do the same thing.",
                    "label": 0
                },
                {
                    "sent": "And I'd give.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea of secondary scaling.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to do one of these change of things that lifting some change.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say Wu is H1, half of W. So my cost, which is 1/2 of W transpose, HWH is the Hessian matrix, become one half of WW.",
                    "label": 0
                },
                {
                    "sent": "Now it's a circle.",
                    "label": 0
                },
                {
                    "sent": "The two seas are the same.",
                    "label": 0
                },
                {
                    "sent": "You go at maximum speed.",
                    "label": 0
                },
                {
                    "sent": "So does my little exercise questions.",
                    "label": 0
                },
                {
                    "sent": "OK, right gradient descent and W space, right?",
                    "label": 0
                },
                {
                    "sent": "The equivalent W update.",
                    "label": 0
                },
                {
                    "sent": "So turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've done them here so in the dominance space.",
                    "label": 0
                },
                {
                    "sent": "Is minus Y to the E over DW?",
                    "label": 0
                },
                {
                    "sent": "Because of this formula is minus little edge minus 1/2 the E over W. So if you do Delta W instead of Delta W new, you have another hash minus half to multiply with and you get cash minus one.",
                    "label": 0
                },
                {
                    "sent": "This is Newton's rule.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we have here is a way to compute the minimum that you can see in two ways.",
                    "label": 0
                },
                {
                    "sent": "One way is to say well, I'm going to fit aquatic bowl that has the same Asian.",
                    "label": 0
                },
                {
                    "sent": "As my my decision that I have in my current point and the same gradient, so it's tangent and go to settle the minimum of this classic ball analytically.",
                    "label": 0
                },
                {
                    "sent": "Another way is to say, well, he's just a rescaling of W. So what I did in my little network thing before.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which was a simple rescaling, is in fact the primitive operation that allows us to make learning better or worse.",
                    "label": 0
                },
                {
                    "sent": "And all the game will be to do that in the best possible way.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ethical issues though.",
                    "label": 0
                },
                {
                    "sent": "The first object function is not quadratic.",
                    "label": 0
                },
                {
                    "sent": "So the local quasi approximation is reasonable, but the Haitian changed with the volume.",
                    "label": 0
                },
                {
                    "sent": "Second, when the function is non convex.",
                    "label": 0
                },
                {
                    "sent": "The Hessian can have negative curvature.",
                    "label": 0
                },
                {
                    "sent": "If you do not only are going to go to the maximum, not the minimum.",
                    "label": 0
                },
                {
                    "sent": "If you do that.",
                    "label": 0
                },
                {
                    "sent": "Oh so there goes way to estimate the Hessian on the fly?",
                    "label": 0
                },
                {
                    "sent": "Is often lost to store invert, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you go to the saddle.",
                    "label": 0
                },
                {
                    "sent": "Point is you go to the critical points.",
                    "label": 0
                },
                {
                    "sent": "You don't know three minimum.",
                    "label": 0
                },
                {
                    "sent": "No stand up.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions.",
                    "label": 0
                },
                {
                    "sent": "The first one is to estimate the compact approximation of X -- 1.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give an example later, so you use the observed gradient GWT, GWT minus one GWT minus K and you know the gradients and you do the changes between the values.",
                    "label": 0
                },
                {
                    "sent": "WT so you have some information here that tells you about the Haitian in that area.",
                    "label": 0
                },
                {
                    "sent": "So in fact you can use directly the gradient and WT minus WT minus one, and so in the difference in W to actually do this multiplication by the inverse of the Haitian more or less directly.",
                    "label": 0
                },
                {
                    "sent": "Simple ways to do a categorisation grammar sheet organization on the fly.",
                    "label": 0
                },
                {
                    "sent": "There are more sophisticated way then you can use approximate line search along the direction.",
                    "label": 0
                },
                {
                    "sent": "It's minus one estimated G of WT and you can go.",
                    "label": 0
                },
                {
                    "sent": "And our values algorithms that use this kind of IDs.",
                    "label": 0
                },
                {
                    "sent": "So under the ID is to use an exact line search and ensure that the conjugates direction.",
                    "label": 0
                },
                {
                    "sent": "So so if DY T -- 1 D T -- 2 D T -- K are the last case of directions you want to choose a DT.",
                    "label": 0
                },
                {
                    "sent": "That's a linear combination of the gradient and the past directions such that DT HD T minus one I = 0 for all eyes.",
                    "label": 0
                },
                {
                    "sent": "So this gives you the conjugate gradient kind of algorithms, and in fact very good algorithms have been developed.",
                    "label": 0
                },
                {
                    "sent": "Their conjugate gradient is one with K equal 1 and\nSearch.",
                    "label": 0
                },
                {
                    "sent": "And one that's very famous limited storage BGS with greater than one but not too large.",
                    "label": 0
                },
                {
                    "sent": "An approximate line search if you do the approximate line search well, you can go very fast.",
                    "label": 0
                },
                {
                    "sent": "If you don't do it well.",
                    "label": 0
                },
                {
                    "sent": "So so it's it would take a complete.",
                    "label": 0
                },
                {
                    "sent": "Week to describe this algorithm, all the tricks that are inside, especially the Elbe GS one underestimated sometimes.",
                    "label": 0
                },
                {
                    "sent": "But if you implement LB FGS like simple book tell you to do and you compare with no signals code, you discover that no, that's called this faster because this approximate line search matches exactly the property of the approximation in a way that extremely smart.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there are lots of reasons to remain suspicious.",
                    "label": 0
                },
                {
                    "sent": "First, our cost function is a symbol for large number of similar terms.",
                    "label": 0
                },
                {
                    "sent": "And this could be used to speed up the optimization.",
                    "label": 0
                },
                {
                    "sent": "See if N is 1 billion.",
                    "label": 0
                },
                {
                    "sent": "No evaluating the function is costly.",
                    "label": 1
                },
                {
                    "sent": "The second is that a problem that's a random subset of the terms is informative.",
                    "label": 0
                },
                {
                    "sent": "If you have 1 billion examples, well, you know you don't need to see the 1 billion example.",
                    "label": 0
                },
                {
                    "sent": "You can start with 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "And you get closer to the solution and then you can use more.",
                    "label": 0
                },
                {
                    "sent": "And the fact that this is true is because we expect generalization.",
                    "label": 0
                },
                {
                    "sent": "If we expect that our system is going to generalize and we have 1 billion examples, we also expect that 1 million example is going to at least going to take us in the query direction.",
                    "label": 0
                },
                {
                    "sent": "Is actually the same argument.",
                    "label": 1
                },
                {
                    "sent": "The third one that quickly achieving a good tested performance is not the same as quickly achieving a good training set performance.",
                    "label": 0
                },
                {
                    "sent": "Some of these algorithms can be so fast that they overfit faster.",
                    "label": 0
                },
                {
                    "sent": "But before.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a lot of things we can do.",
                    "label": 0
                },
                {
                    "sent": "So we can pre condition the inputs, normalize everything in similar ranges.",
                    "label": 0
                },
                {
                    "sent": "Because if you do so, the curvatures are more likely to be similar in all directions.",
                    "label": 0
                },
                {
                    "sent": "You can use different learning rates in different layer on the basis of the average size.",
                    "label": 0
                },
                {
                    "sent": "The gradient, an oversized weights.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple idea, but no, you want everything to look.",
                    "label": 0
                },
                {
                    "sent": "Like a circle like like a sphere and a sphere, if you learn.",
                    "label": 0
                },
                {
                    "sent": "You learn about the same speed in all dimensions in in along every axis.",
                    "label": 0
                },
                {
                    "sent": "So now you have a new network.",
                    "label": 0
                },
                {
                    "sent": "Stop once in awhile.",
                    "label": 0
                },
                {
                    "sent": "Take some data.",
                    "label": 0
                },
                {
                    "sent": "And do some forward backward without learning the weights.",
                    "label": 0
                },
                {
                    "sent": "You see some weights compute the average number of the weights in one layer.",
                    "label": 0
                },
                {
                    "sent": "Compute the average norm of the gradient.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the ratio between this, you would like that the change you're going to do to the weights has to be a small fraction of the normal ways.",
                    "label": 0
                },
                {
                    "sent": "Let's say 10%.",
                    "label": 0
                },
                {
                    "sent": "One percent of the side of the weights.",
                    "label": 0
                },
                {
                    "sent": "That seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "What doesn't seem reasonable is that if you have one layer where the average update of the weights is 10 times bigger than the words themselves, another layer is 1 millionth of the size of the weights.",
                    "label": 0
                },
                {
                    "sent": "That's probably not going to work.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple thing and I describe it not in terms of an algorithm, but in terms of practice.",
                    "label": 0
                },
                {
                    "sent": "Laundromat stop.",
                    "label": 0
                },
                {
                    "sent": "Collect statistics in all layers.",
                    "label": 0
                },
                {
                    "sent": "Size of the weight, size of the gradients and everything and make sure they are reasonable.",
                    "label": 0
                },
                {
                    "sent": "And that goes along way.",
                    "label": 0
                },
                {
                    "sent": "Alot of the fancy algorithms that are discussed in the literature, whether they called adigrat Ms Prop and everything.",
                    "label": 0
                },
                {
                    "sent": "This is what they do more and less automatically.",
                    "label": 0
                },
                {
                    "sent": "But to do it they have to pay at each time you need to page time just every 1 million iterations.",
                    "label": 0
                },
                {
                    "sent": "Stop and look.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that brings us to another problem.",
                    "label": 0
                },
                {
                    "sent": "In multilayer networks is the initialization.",
                    "label": 0
                },
                {
                    "sent": "We typically initialize the weights of multilayer networks randomly.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now it's time to ask the nasty question, why can we optimize such a complex nonconvex function will not realize optimizing.",
                    "label": 0
                },
                {
                    "sent": "The problem is simpler than looks, otherwise we couldn't do that.",
                    "label": 0
                },
                {
                    "sent": "So that means that.",
                    "label": 0
                },
                {
                    "sent": "The initial weights they play a big role because they put us close to where we want.",
                    "label": 0
                },
                {
                    "sent": "We want to be.",
                    "label": 0
                },
                {
                    "sent": "So of course there's many symmetries in the networks and everything, but there are many ways to put the initial ways that are going to do my system.",
                    "label": 0
                },
                {
                    "sent": "Amber.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at the simple examples the first one.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the simplest to layer network.",
                    "label": 0
                },
                {
                    "sent": "Have an input, wait just one way to improve this color one wait W 1, the sigmoid another wait, another sigmoid yet why?",
                    "label": 0
                },
                {
                    "sent": "And you turn into examples.",
                    "label": 0
                },
                {
                    "sent": "This is the input.",
                    "label": 0
                },
                {
                    "sent": "This into output 1/2 one half and minus 1/2 -- 1/2.",
                    "label": 0
                },
                {
                    "sent": "So basically you want this network to replicate its input with only two examples.",
                    "label": 0
                },
                {
                    "sent": "So the arrow is here in full.",
                    "label": 0
                },
                {
                    "sent": "So you can plot it.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like this.",
                    "label": 0
                },
                {
                    "sent": "I don't know if he's clear enough.",
                    "label": 0
                },
                {
                    "sent": "So in the center you have a subtle point.",
                    "label": 0
                },
                {
                    "sent": "In this other point, because you have two products, all the relatives.",
                    "label": 0
                },
                {
                    "sent": "All zero.",
                    "label": 0
                },
                {
                    "sent": "And the second derivatives that cannot 0.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting to know is at the center southern point with the hyperbolic tangent.",
                    "label": 0
                },
                {
                    "sent": "Is a subtle point of the older 1 minus the number of layers.",
                    "label": 0
                },
                {
                    "sent": "So if you have 15 layers, the saddle .0 devices.",
                    "label": 0
                },
                {
                    "sent": "0 second abilities, 0 third abilities, and so on until the 14th derivative.",
                    "label": 0
                },
                {
                    "sent": "So it's really flat.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you don't want to be here because there is no local signal.",
                    "label": 0
                },
                {
                    "sent": "And then you have the solution without the black thing.",
                    "label": 0
                },
                {
                    "sent": "There's somewhere here along two hyperballs, so maybe it's.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easier to represent like this, so that's one quadrant W1W2 and you get the kind of.",
                    "label": 0
                },
                {
                    "sent": "Are being shaped revine here with the minimum about.",
                    "label": 0
                },
                {
                    "sent": "Here is normal because if you didn't have the hyperbolic tangents this would just be W 1 equal times W 2 equal identity.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The hyperbowl here or the Hyper Bowl is where the solution is likely to be and you want to be in the point of the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "That's interesting.",
                    "label": 0
                },
                {
                    "sent": "So what do you know here?",
                    "label": 0
                },
                {
                    "sent": "Well, if you know you size the weights so that W1W2 are here.",
                    "label": 0
                },
                {
                    "sent": "Well, there's no gradient here.",
                    "label": 0
                },
                {
                    "sent": "It's all flat.",
                    "label": 0
                },
                {
                    "sent": "You're not going to run if you put them here.",
                    "label": 0
                },
                {
                    "sent": "It's all flat.",
                    "label": 0
                },
                {
                    "sent": "You're not going to learn.",
                    "label": 0
                },
                {
                    "sent": "Well, you know you want to put them in a certain area here.",
                    "label": 0
                },
                {
                    "sent": "At least if you just.",
                    "label": 0
                },
                {
                    "sent": "Decide now Norm, that should be in the part that's active.",
                    "label": 0
                },
                {
                    "sent": "So the capital rule of initialization is do not pick initial ways that kill the gradient.",
                    "label": 0
                },
                {
                    "sent": "It's so simple.",
                    "label": 0
                },
                {
                    "sent": "Now the transfer function plays a big role in this.",
                    "label": 0
                },
                {
                    "sent": "If it's a Sigma media saturation, the gradient can be 0.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The meaning of this rule for value sensor function is that the distribution of the inputs of the transfer function should target the linear part, but should have a chance to exploit the non linearity.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to draw things again.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think again this ID that I stop my network and I'm doing some food showing some examples and correcting statistiques, and I'll take a particular you need somewhere and I assume this unit is a kind of sigmoidal unit, something like this.",
                    "label": 0
                },
                {
                    "sent": "OK. And I'm computing the distribution.",
                    "label": 0
                },
                {
                    "sent": "Of the son of WIJ.",
                    "label": 0
                },
                {
                    "sent": "XXXI so the input in the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Well, if this distribution is like this.",
                    "label": 0
                },
                {
                    "sent": "It's not going to work.",
                    "label": 0
                },
                {
                    "sent": "OK, let's not do that.",
                    "label": 0
                },
                {
                    "sent": "If the distribution is like this.",
                    "label": 0
                },
                {
                    "sent": "Very small.",
                    "label": 0
                },
                {
                    "sent": "Well, it's in the linear part of the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "So basically have a linear system.",
                    "label": 0
                },
                {
                    "sent": "There is a gradient.",
                    "label": 0
                },
                {
                    "sent": "But also means that my system is working linearly and that I'm not really going to use the fact that have a non linearity, which is something that differentiates multi and network from.",
                    "label": 0
                },
                {
                    "sent": "1 million network so not good.",
                    "label": 0
                },
                {
                    "sent": "If I put it like this.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Lot of zero gradients, not good.",
                    "label": 0
                },
                {
                    "sent": "Well, if I put it like this.",
                    "label": 0
                },
                {
                    "sent": "So the mode is in the linear part.",
                    "label": 0
                },
                {
                    "sent": "But I have enough tail here so that I might be able to use the nonlinearities if I need them.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "OK, now another example, the whole.",
                    "label": 0
                },
                {
                    "sent": "The world is a different animal.",
                    "label": 0
                },
                {
                    "sent": "It's like this.",
                    "label": 0
                },
                {
                    "sent": "That's zero and OK. Look at my distribution.",
                    "label": 0
                },
                {
                    "sent": "My decisions like this not good, OK?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the activation function like this?",
                    "label": 0
                },
                {
                    "sent": "Well, it's linear is not too bad, but could be better.",
                    "label": 0
                },
                {
                    "sent": "I put it like this.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a strange hinge.",
                    "label": 0
                },
                {
                    "sent": "There is not good for the gradients in other gradients.",
                    "label": 0
                },
                {
                    "sent": "Don't like that too much.",
                    "label": 0
                },
                {
                    "sent": "Notice that you take a gradient on one side.",
                    "label": 0
                },
                {
                    "sent": "It moves a lot.",
                    "label": 0
                },
                {
                    "sent": "It's just like having a very nasty curvature.",
                    "label": 0
                },
                {
                    "sent": "So if you put it like this.",
                    "label": 0
                },
                {
                    "sent": "So basically the mean is somewhere and the standard deviation is on the order of the mean.",
                    "label": 0
                },
                {
                    "sent": "Little bit on the side good.",
                    "label": 0
                },
                {
                    "sent": "Good you're going to be linear.",
                    "label": 0
                },
                {
                    "sent": "You have the chance to use the nonlinearity if you need it.",
                    "label": 0
                },
                {
                    "sent": "Simply simple idea.",
                    "label": 0
                },
                {
                    "sent": "So when you initialize things like this, that constraints the ways to have certain norm.",
                    "label": 0
                },
                {
                    "sent": "Because the more inputs you have to unit, the smaller the West we need to be to be in a range that you have defined.",
                    "label": 0
                },
                {
                    "sent": "And by having the weights of the norm that tells you who my learning rate in that layer.",
                    "label": 0
                },
                {
                    "sent": "They should have be such that when I back propagate the gradient from the top, which depends on what you do on the top.",
                    "label": 0
                },
                {
                    "sent": "The gradient terms of learning, which should be a reasonable fraction of the weights.",
                    "label": 0
                },
                {
                    "sent": "You start like that, like this.",
                    "label": 0
                },
                {
                    "sent": "You already have your network in a good shape.",
                    "label": 0
                },
                {
                    "sent": "We just go fine.",
                    "label": 0
                },
                {
                    "sent": "So I think I'm going to stop here for this part and keep something for tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Oh I have a.",
                    "label": 0
                },
                {
                    "sent": "Have any questions before for the last five minutes?",
                    "label": 0
                },
                {
                    "sent": "I don't always do it that way.",
                    "label": 0
                },
                {
                    "sent": "I've been doing it this way for 20 years.",
                    "label": 0
                },
                {
                    "sent": "The fact is that in fact, if you look, for instance, the creature ski network, the code is not done that well.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of imbalance in various parts of the networks.",
                    "label": 0
                },
                {
                    "sent": "You know, the fact is that most of the networks that, even if you don't do that right, they're going to work.",
                    "label": 0
                },
                {
                    "sent": "But you can do it much better.",
                    "label": 0
                },
                {
                    "sent": "By simple observations and then you know you start again later in the learning you used up your system.",
                    "label": 0
                },
                {
                    "sent": "You collect some statistics and you look what's there in the various layers, the various activations, everything and you look what's wrong.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "They're not discontinuous the non differentiable in some point there continues.",
                    "label": 0
                },
                {
                    "sent": "Franklin in the ranking algorithms you have.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the case of Jason West, and I'm pretty sure that continues.",
                    "label": 0
                },
                {
                    "sent": "In the case of ranking in general, people have been trying to optimize violence of the end ECG criteria.",
                    "label": 0
                },
                {
                    "sent": "The ranking criteria that rank at the top and they could be discontinuous.",
                    "label": 0
                },
                {
                    "sent": "But what actually works best is to have a continuous variant of them.",
                    "label": 0
                },
                {
                    "sent": "So if you look at all the work of Chris Burgess on Lambda rank, Lambda Max and so on, they all done in such a way that there continues, and in some cases differentiable.",
                    "label": 0
                },
                {
                    "sent": "When this non continues, we really don't know what we do with optimization.",
                    "label": 0
                },
                {
                    "sent": "The local information is not reliable so it could work, could be lucky.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The parameters learning regimen.",
                    "label": 0
                },
                {
                    "sent": "After running for.",
                    "label": 0
                },
                {
                    "sent": "We are seeing this at 6 and then decide if it is a good initialization or not.",
                    "label": 0
                },
                {
                    "sent": "Oh, but you can do that before.",
                    "label": 0
                },
                {
                    "sent": "Like I told you how to initialize the weights and you you can do it incrementally.",
                    "label": 0
                },
                {
                    "sent": "You start the first layer and you see what side of the weights you set the weights for that layer, and so that's going to be the size of the weights, and then you can start to go to the next layer and decide the size of the way so that the activation is right and you work up your network that way.",
                    "label": 0
                },
                {
                    "sent": "Then you start computing gradients backwards and you set, which will be learning rates without learning anything.",
                    "label": 0
                },
                {
                    "sent": "You just run it called.",
                    "label": 0
                },
                {
                    "sent": "Well, you can initialize that altogether, but you look at the firstly your first and then you are going to re scale them to put everything in the right range.",
                    "label": 0
                },
                {
                    "sent": "It's a very bad idea to start with the network where let's say some units are already dead, because basically you're going to saturate the hyperbolic tangent unit, so the sigmoid units, even even when you start.",
                    "label": 0
                },
                {
                    "sent": "Do this at a rate is going to be a very long learning time.",
                    "label": 0
                },
                {
                    "sent": "You just don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "Size of.",
                    "label": 0
                },
                {
                    "sent": "For the learning rate, I'm suggesting something very simple that's.",
                    "label": 0
                },
                {
                    "sent": "That's a gross, and we're going to discuss that later.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's a gross approximation of Newton, which says let's look at the average number of the weights in the layer.",
                    "label": 0
                },
                {
                    "sent": "The average none of the of the gradient of the of the cost respect to the weights.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to multiply by some learning, right?",
                    "label": 0
                },
                {
                    "sent": "And I want the product of the update I'm going to apply to my weights to be commensurable.",
                    "label": 0
                },
                {
                    "sent": "With the size of the weights shouldn't should be smaller?",
                    "label": 0
                },
                {
                    "sent": "Of course, let's say 1% or 10% of the size of the weights.",
                    "label": 0
                },
                {
                    "sent": "But you certainly don't want them to be widely different.",
                    "label": 0
                },
                {
                    "sent": "In various layers.",
                    "label": 0
                },
                {
                    "sent": "And you can play with these numbers little bit.",
                    "label": 0
                },
                {
                    "sent": "But then instead of having plenty of amateurs to adjust, you have one.",
                    "label": 0
                },
                {
                    "sent": "What I didn't tell you is how to set the momentum, because I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That changes typically when you initialize the way I suggest initially there not saturated because insulation that way.",
                    "label": 0
                },
                {
                    "sent": "And they tend to saturate progressively.",
                    "label": 0
                },
                {
                    "sent": "But what you want is then to saturate differently, like if you have a unit that saturated always on the same size all the time.",
                    "label": 0
                },
                {
                    "sent": "Basically it's dead.",
                    "label": 0
                },
                {
                    "sent": "So you want to avoid that, but it's also something you can check.",
                    "label": 0
                },
                {
                    "sent": "So, so we learn all these things the hard way, because 20 years ago, computers well slower and there was a lot less margin for errors for four, but less margin for slow systems and not more time to look at them.",
                    "label": 0
                },
                {
                    "sent": "So when when it runs for three weeks, you have time to look at the elementary statistics on the weights and everything, and I think we both wasted quite a.",
                    "label": 0
                },
                {
                    "sent": "Quite a couple days or weeks at doing this.",
                    "label": 0
                },
                {
                    "sent": "Just trying to see now network slowly learning networks something was wrong.",
                    "label": 0
                },
                {
                    "sent": "That also created this culture of these ideas that you pilot the optimization.",
                    "label": 0
                },
                {
                    "sent": "It lasted for weeks and you would come every morning and say OK, well my basic statistics, everything oh this is not good.",
                    "label": 0
                },
                {
                    "sent": "Let's change it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Depends you do it every hour.",
                    "label": 0
                },
                {
                    "sent": "Then you realize that it was not worth it so.",
                    "label": 0
                },
                {
                    "sent": "But so that created this culture that all learning or networks is very tricky.",
                    "label": 0
                },
                {
                    "sent": "You have to monitor things are just things and everything and all things like that.",
                    "label": 0
                },
                {
                    "sent": "But you could automate that if you wanted to.",
                    "label": 0
                },
                {
                    "sent": "No, in fact a lot of the algorithm that people propose are some kind of automation of these things instead, that sometimes it's the automation is a bit naive.",
                    "label": 0
                },
                {
                    "sent": "I think you can do it better.",
                    "label": 0
                },
                {
                    "sent": "But but what I'm describing here is not an algorithm that the principles that allow you to design an algorithm for each particular case, you're going to adjust a little bit with this principle and design an algorithm for to make it work, that's fine.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, if you were to train a Christian Skynet with the old techniques.",
                    "label": 0
                },
                {
                    "sent": "You would lose probably a couple person from the drop out.",
                    "label": 0
                },
                {
                    "sent": "Uh, EE seems to be a bit easier to learn, so maybe you would learn a bit faster, even though you could balance the inside the internal statistics a bit better.",
                    "label": 0
                },
                {
                    "sent": "You would lose in the pulling.",
                    "label": 0
                },
                {
                    "sent": "Because the Max pooling is clearly better and contrast normalization helps.",
                    "label": 0
                },
                {
                    "sent": "But you would still have been much better than the competing systems.",
                    "label": 0
                },
                {
                    "sent": "So yes, there is a progress, but it's not a revolution is an evolution.",
                    "label": 0
                },
                {
                    "sent": "Would you agree you sure?",
                    "label": 0
                },
                {
                    "sent": "Well, I just say that because you know.",
                    "label": 0
                },
                {
                    "sent": "They would, they would work there would.",
                    "label": 0
                },
                {
                    "sent": "There is still in progress.",
                    "label": 0
                },
                {
                    "sent": "There is progress there.",
                    "label": 0
                },
                {
                    "sent": "Order direct ification of the Max pooling and the contrast normalization are clear effects and the drop party is a clear effect as well.",
                    "label": 0
                },
                {
                    "sent": "There really the main argument for the reader you have to get the network bigger.",
                    "label": 0
                },
                {
                    "sent": "But then it's a bit easier to trend, so this is what the paper says, and this seems to be true.",
                    "label": 0
                },
                {
                    "sent": "So, so it's more evolution and revolution.",
                    "label": 0
                },
                {
                    "sent": "One of the issues with the order, for instance, is that there was a well publicized paper saying that if you move just a little bit.",
                    "label": 0
                },
                {
                    "sent": "One pixel in one image.",
                    "label": 0
                },
                {
                    "sent": "You can make it any class you want, or pretty much like that now.",
                    "label": 0
                },
                {
                    "sent": "Some of the observation is that when you have a role in network for image recognition, about 80% of the units in the flood zone at a given moment, they change.",
                    "label": 0
                },
                {
                    "sent": "When you change example, but 2020% sparse basically.",
                    "label": 0
                },
                {
                    "sent": "So now see what it means.",
                    "label": 0
                },
                {
                    "sent": "If you want to preserve the signal you like the average signal to go up in each layer.",
                    "label": 0
                },
                {
                    "sent": "To to be one.",
                    "label": 0
                },
                {
                    "sent": "So that means that those are the activity must multiply the signal by 5 so that you get about the same kind of signal going up or otherwise going to go down and you get nothing.",
                    "label": 0
                },
                {
                    "sent": "So that means that along the path of the active rollers, you multiply by 5 at actually every layer you have something like 10 layers.",
                    "label": 0
                },
                {
                    "sent": "So that's very sensitive along this path.",
                    "label": 0
                },
                {
                    "sent": "So of course if you move that pixel you can change anything on the output in a completely crazy way.",
                    "label": 0
                },
                {
                    "sent": "So the story here is that.",
                    "label": 0
                },
                {
                    "sent": "So what I'm describing is one of the drawbacks of whole lot of qualities, but that's one of the drawbacks is that along certain path is super sensitive to the inputs.",
                    "label": 0
                },
                {
                    "sent": "So that answers your question directly.",
                    "label": 0
                },
                {
                    "sent": "Saying well, you can have a very small change at the input that creates a large change on the output.",
                    "label": 0
                },
                {
                    "sent": "If you choose your input right.",
                    "label": 0
                },
                {
                    "sent": "Now, if you saturate with a sigmoid, well, you want not going to happen because the not going to be beyond the minus one.",
                    "label": 0
                },
                {
                    "sent": "Oprah or zero and one or just designed that way.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This is me OK, this actually best seen in the context of convex optimization.",
                    "label": 0
                },
                {
                    "sent": "The theorems about convex optimization use values assumption about the function.",
                    "label": 0
                },
                {
                    "sent": "One of them is smoothness.",
                    "label": 0
                },
                {
                    "sent": "And you can also optimise connections are not smooth.",
                    "label": 0
                },
                {
                    "sent": "And typically the rates you can prove for nonsmooth function, so you have no clean curvature is lower.",
                    "label": 0
                },
                {
                    "sent": "But when you do this, it's an essential tick rate.",
                    "label": 0
                },
                {
                    "sent": "Meaning that you want to go to the very minimum of that function well when you're training in our net, you don't want to go to the minimum in the training set you want to meet testing set.",
                    "label": 0
                },
                {
                    "sent": "You're going to stop before.",
                    "label": 0
                },
                {
                    "sent": "And then more.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll show some silly divisions that are not in the slides on the ball to to show how this happens in the stochastic gradient problem, where in fact you you have the aseptic retrospectively.",
                    "label": 0
                },
                {
                    "sent": "One over thesis is smooth.",
                    "label": 0
                },
                {
                    "sent": "And strongly convex.",
                    "label": 0
                },
                {
                    "sent": "Can be one of a sqrt T if it's convex.",
                    "label": 0
                },
                {
                    "sent": "But in fact you can also see that it's exponential up to a certain zone that depends on the size of the of the weights, and then it slows down.",
                    "label": 0
                },
                {
                    "sent": "So if you play using right, you can go use you.",
                    "label": 0
                },
                {
                    "sent": "You're not going to the end, and the fact that you have an inch is not a problem unless you algorithm looks for it.",
                    "label": 0
                },
                {
                    "sent": "So if you have a particular engine you do linesearch, you're going to fall there.",
                    "label": 0
                },
                {
                    "sent": "But if you have a stochastic noisy problem is not going to be like this.",
                    "label": 0
                },
                {
                    "sent": "So this kind of issues that you have an actual hinge and an no real second derivative.",
                    "label": 0
                },
                {
                    "sent": "Is less of a problem with algorithms?",
                    "label": 0
                },
                {
                    "sent": "Are noisy because there is the effect you describe that at the scale you're looking at it looks moderately smooth.",
                    "label": 0
                },
                {
                    "sent": "Or it could be could be smooth.",
                    "label": 0
                },
                {
                    "sent": "It's plausibly smooth.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do a line search and you have a wedge like this, you do anyone optimization.",
                    "label": 0
                },
                {
                    "sent": "Let's say anyone regularization with align search.",
                    "label": 0
                },
                {
                    "sent": "The line search is going to be right to the point where there is no gradient and infinite curvature and you cooked.",
                    "label": 0
                },
                {
                    "sent": "So you have to use all the algorithms for that.",
                    "label": 0
                },
                {
                    "sent": "If you do it, stochastic is not the case, but you're not going to get sparsity that easily.",
                    "label": 0
                }
            ]
        }
    }
}