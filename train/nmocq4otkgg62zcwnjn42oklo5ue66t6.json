{
    "id": "nmocq4otkgg62zcwnjn42oklo5ue66t6",
    "title": "Co-regularized Spectral Clustering with Multiple Kernels",
    "info": {
        "author": [
            "Piyush Rai, University of Utah"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_rai_csc/",
    "segmentation": [
        [
            "I'm puce, Ray, welcome to the last last talk of this workshop, and I'm going to be talking about code regularised spectral clustering with multiple kernels, and this is joint work with Abhishek Kumar and held on May 3 from you."
        ],
        [
            "City of Maryland.",
            "OK, so I'll begin.",
            "So this problem setting that we have is that we have multiple views of the data and we want to somehow combine these views to learn better.",
            "So many of the real world datasets have multiple feature representations and these feature representations could be obtained naturally.",
            "For example, if you have a collection of web pages so each web page could be thought of as having some paste text and hyperlinks or social tags.",
            "So you have these different features that are naturally available for.",
            "These kind of datasets or these features these multiple representations could be derived from the same set of objects.",
            "For example, if you have images, so you can use different kind of you can extract different kind of features from the same image.",
            "For example, you can extract pixel features or you can use Fourier coefficients and so on.",
            "So basically each representation is some kind of a view of the data, and if I give you enough data then you each view individually is kind of good enough to learn from.",
            "And what multiple?"
        ],
        [
            "If you learning does is that it somehow tries to exploit these multiple views in order to learn better?",
            "And by when I say better, it means generally that use require less data to learn and you learn."
        ],
        [
            "Bitter as well.",
            "OK, so I just want to contrast or maybe compared with multiple kernel learning so at the end of the day they are essentially solving the same problem, both multi view learning and multiple kernel learning.",
            "Because if you are in a kernel and then each view can be thought of as giving you a Colonel from the data.",
            "So basically in the kernel kind of setting, these terms are basically synonymous so multi view learning is kind of doing the same thing as multiple kernel learning because multiple views can give you multiple multiple kernels and then.",
            "The goal is to somehow combine these kernels to do better on the."
        ],
        [
            "Any task that you have.",
            "OK, so I'll go.",
            "I'll go forward with some background for this work, so our work is based on a principle of colorization, so the correlation correlation idea has been proposed about 10 years back, so it was from the influential paper by Bloom and Michelle, so this idea of correlation basically tries to enforce agreement between learners that are trying to learn using different views of the data.",
            "So this notion was typically used for.",
            "Basically, it was originally used for semi supervised learning where you have some label data in some unlabeled data and the data comes in natural naturally multiple views.",
            "So what you do is you learn multiple classifiers.",
            "So let's say we only have 2 views V1 and V2.",
            "So what you do, you train 2 classifiers F1 and F2 and you want these classifiers to agree on unlabeled data.",
            "And by doing this you somehow shrink the set of hypothesis.",
            "The space of hypothesis that you are trying to learn.",
            "And there has been theoretical and empirical evidence that this kind of a learning setting.",
            "It reduces the amount of data that you need to learn, and at the end of the at the end of the day, you also learn better than the."
        ],
        [
            "Settings.",
            "So in this talk I'll talk about how to use code regularization in unsupervised learning setting.",
            "So as I said in the beginning, so typically code regularization has been used in semi supervised learning.",
            "But in this talk I'll talk about an unsupervised learning algorithm.",
            "So which in this talk I'll talk about a clustering problem which is the spectral clustering algorithm and spectral clustering.",
            "As many of you might be knowing, it's a kernel based clustering algorithm and our goal is to."
        ],
        [
            "These multiple kernels from different views to improve spectral clustering.",
            "So again the idea, just like in core training is that we're trying to learn clustering from different views of the data, and the idea is that we since all the views are representing the same set of objects, we want these clusterings to agree.",
            "OK. Again, I would reiterate that each view corresponds to a kernel, so it's just like what we do in multiple kernel learning."
        ],
        [
            "OK, and as I will show later during the later part of the talk, this in a sense is similar to combining multiple kernels like combining multiple views in our setting.",
            "It turns out to be the same similar as combining multiple kernels."
        ],
        [
            "OK so next give some brief background on spectral clustering.",
            "So the spectral clustering algorithm is based on the idea of spectral decomposition of graph data.",
            "So it's graph Laplacian of the data and it's a very theoretically well motivated algorithm.",
            "And the nice thing about this as compared to other algorithms such as K means is that spectral clustering can learn arbitrary shaped clusters, so your clusters need not be round shape, so you don't have these kind of restrictions on the shape of the clusters.",
            "So introduce some notations here.",
            "So I'll describe right K as in.",
            "And by in current metrics of the data which is lying in like R to the ND, and by D&KIJ denotes a similarity between 2 examples and D is again a diagonal matrix with the diagonal entries being the row sums of your kernel metrics.",
            "And then I define the normalized graph Laplacian as like this L which is D minus half K D -- 5.",
            "It's a normalized graph Laplacian.",
            "So if I give you the graph Laplacian, then what we do what we want is we want to keep partitions of the data and spectral clustering achieves this by doing."
        ],
        [
            "Vector decomposition of this graph Laplacian OK, so let me formally define the spectral clustering problem.",
            "So these are the objective I'm showing here is by any at all from NIPS 2002, so the spectral processing solves the following problem, so it just maximizes the trace of U transpose Lu where U is a matrix of size N by K and it's the columns are orthonormal.",
            "So as you can see it's just an eigenvalue problem which amounts to solving finding the top K eigenvectors of L. So you can think of US consisting of each column has eigenvector."
        ],
        [
            "Of the Laplacian L. OK, so if you think of this matrix you so it's N by K matrix, so you can also think of US giving you a new representation of the data.",
            "So each row of U corresponds to a row in the data metrics."
        ],
        [
            "And what is spectral clustering does in the final step is is that it just runs, normalizes the rules of this you metrics and runs the K means algorithm and that gives you the final clustering solution."
        ],
        [
            "OK, so now let's consider this setting.",
            "So what we have in our case is that we have multiple views of the data, so I'll denote each view as X superscript V. So, so this is the data from view V and we can denote the Laplacian for Vue VSLV and then I can write the similar kind of objective for a single, just like I had for the single view.",
            "So this is the objective for us.",
            "For the spectral clustering in Vue V. Now.",
            "So as you can see in this picture.",
            "So I have like different views of the data, so all these.",
            "Different matrices are representing different views of the data, and for each view I can think of I'm doing spectral clustering, so I'm getting these different views.",
            "Now.",
            "My goal is in colourization based spectral clustering would be to somehow reconcile these different views so as to reach on some sort of agreement.",
            "So I want to I want these used to look similar to each other in some."
        ],
        [
            "Which I'll define later on.",
            "OK, so let's look at our objective function.",
            "So what we have, let's assume just two views.",
            "So extension to more than two uses like it can be done in a similar fashion, so I'll call my views as V&W.",
            "So I'll write the joint problem with two views as the spectral objective function for Vue V, just like the one you case.",
            "Plus we have spectral objective for the second view W and then we have some Co regularizer term which tries to bring which tries to make.",
            "UV and you do you W the two representations of this.",
            "You metrics similar in some sense.",
            "OK, now the question is that we have this code regular either.",
            "D now what should be look like?",
            "So there's a question so."
        ],
        [
            "Litty now the intuition is that as I said.",
            "You use the new metrics to do clustering to run some K means clustering as the final step of your respect requesting algorithm.",
            "Now The thing is, whatever you use, it should lead to the same clustering because at the end of the day you were just collecting the same data but using different views, right?",
            "So you would expect that you should each you should lead to the same clustering."
        ],
        [
            "So what it implies that the kernel that you're using with the.",
            "So if we just take this U, the new representation of the data U and if you just define a kernel using U so all these kernels using the your representation should look similar to each other, right?"
        ],
        [
            "So which implies that if you have two kernels to find on the 1st and 2nd views, UV and UW, these should have a high degree of alignment between them.",
            "OK, So what we do is we just define a linear kernel in EU space as UV UV transpose and that defines."
        ],
        [
            "OK, so this is the Colonel in EU space.",
            "Now.",
            "The alignment term between these two matrices would be just the trace of the product of these two kernel matrices KUV&KUW.",
            "And since I defined KQ as a linear kernel, it just boils down to this term.",
            "So you have this UV transpose and you WW transpose, so that's our Co regularizer term so."
        ],
        [
            "Logging in this value of this expression of D in the spectral joint spectral clustering objective, we get the following objective function, and then we also have this hyperparameter Lambda, which tunes how much you want to, basically, how much you want your individual objectives to perform and how much how much you want, how much importance you want to give to the code regularization term so that basically is the trade off parameter.",
            "So that's all."
        ],
        [
            "The free parameter in our algorithm.",
            "No, this kind of an objective function.",
            "It has, as you can see, it's the optimization is over.",
            "UV and UW pose the matrices of size N by K and this can be solved using using an alternating optimization scheme.",
            "So if you just treat UW S fixed then you get the following.",
            "If you and then if you rearrange the above objective function you just get this this.",
            "Objective and if you just stare at it, it just looks like the spectral spectral clustering objective with a single view, except for the difference."
        ],
        [
            "That you have the Laplacian instead of a single Laplacian LV.",
            "You have this extra term right?",
            "So your Laplacian instead of justice being LV, it becomes LV plus Lambda times, uvu, WW, transpose.",
            "So if you just think of it, it's kind of similar to combining Laplacian, so combining graphs because you originally have the Laplacian from the View View V and then you bring in another component in terms of this this new kernel you WW transpose.",
            "OK so this is similar to.",
            "Combining two kernels."
        ],
        [
            "And then you can solve this optimization in an alternating fashion until convergence and you will converge to a local minima.",
            "In this case local Maxima, because we're doing optimization and we're guaranteed to."
        ],
        [
            "English.",
            "So in terms of experiment, so we have compared against the number of baselines, so the prominent ones are Canonical correlation analysis.",
            "The CCA, which what it does is that it combines multiple views of the data, it does a shared projection and then uses the features extracted as the final K means clustering in the final clustering step, and we also compare with another multiview agreement based disagreement based clustering by diesel from its recent algorithm so.",
            "I'm showing the results with on the UCI handwritten digit data set, so we also compared with another number of other baselines, the where we just concatenate the features from different views.",
            "So that's one baseline, the other is like just.",
            "Add two kernels or multiply 2 kernels and then the other two baselines are the CC and the minimizations disagreement algorithm, and the final is our core regularizer approach and we see like in all cases.",
            "So we have compared the clustering performance on a number of metrics that score precision.",
            "Recall entropion my adjusted Rand index an all these we do reasonably better than the other baselines."
        ],
        [
            "So I'm also showing some results from the writer multilingual data, so it's a multi level data set and your goal is to like you have documents in translations in different languages and you somehow want to combine these documents for clustering to her task.",
            "So again on this data set as well as you can see we do considerably better than the other baselines here.",
            "OK, so."
        ],
        [
            "As I said in the beginning, so we have a free parameter Lambda which controls how much to the degree to which we want to regularize the different views.",
            "So we wanted to see how sensitive our algorithm is for the choice of parameter Lambda.",
            "So the figure on the left shows the performance of, so we show the enemy scores.",
            "So we compare the performance of our algorithm with respect to the closest performing baselines.",
            "So the line in blue is the closest performing baseline in terms of the isquared gates, and we vary the regularization parameter from zero to.",
            ".1 and we see that for a wide range of Lambda we are still better than the second closest performing performing baselines.",
            "So it shows that it's kind of robust to.",
            "Of course you have a free parameter Lambda, but it's kind of robust for a wide range of Lambda and the other.",
            "It shows how we converge, so as you can see it's on the Celtic 101 data, so we converge pretty quickly, like four or five iterations."
        ],
        [
            "So to conclude, so we propose a code regularization based approach for an unsupervised learning problem.",
            "And as I showed, this approach can be thought of as combining multiple kernels, and the algorithm is pretty simple.",
            "So at each step you just solve spectral clustering objective with a modified Laplacian.",
            "So it's just like you can just use any existing Eigen solvers to which to solve that step.",
            "And I used this.",
            "People are either the form of the regular regularizer that we used was just the product, the trees of the product of two kernel matrices in EU space, but you could also think of other ways to measure similarities or to enforcing agreement between these two representations that I. I talked earlier and so this is a kind of a general set up, so we had, you know, at the end of the so our underlying objective or some kind of spectral objective function.",
            "So you could also imagine solving other kind of spectral algorithms which try to do spectral decomposition.",
            "For example kernel PCA.",
            "So you could combine multiple kernels in kernel PCA, kind of a setting and you can solve PC as well using this kind of setup.",
            "So I guess."
        ],
        [
            "That's all I had to say.",
            "Yeah, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm puce, Ray, welcome to the last last talk of this workshop, and I'm going to be talking about code regularised spectral clustering with multiple kernels, and this is joint work with Abhishek Kumar and held on May 3 from you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City of Maryland.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll begin.",
                    "label": 0
                },
                {
                    "sent": "So this problem setting that we have is that we have multiple views of the data and we want to somehow combine these views to learn better.",
                    "label": 0
                },
                {
                    "sent": "So many of the real world datasets have multiple feature representations and these feature representations could be obtained naturally.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a collection of web pages so each web page could be thought of as having some paste text and hyperlinks or social tags.",
                    "label": 0
                },
                {
                    "sent": "So you have these different features that are naturally available for.",
                    "label": 0
                },
                {
                    "sent": "These kind of datasets or these features these multiple representations could be derived from the same set of objects.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have images, so you can use different kind of you can extract different kind of features from the same image.",
                    "label": 0
                },
                {
                    "sent": "For example, you can extract pixel features or you can use Fourier coefficients and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically each representation is some kind of a view of the data, and if I give you enough data then you each view individually is kind of good enough to learn from.",
                    "label": 1
                },
                {
                    "sent": "And what multiple?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you learning does is that it somehow tries to exploit these multiple views in order to learn better?",
                    "label": 0
                },
                {
                    "sent": "And by when I say better, it means generally that use require less data to learn and you learn.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bitter as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just want to contrast or maybe compared with multiple kernel learning so at the end of the day they are essentially solving the same problem, both multi view learning and multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "Because if you are in a kernel and then each view can be thought of as giving you a Colonel from the data.",
                    "label": 1
                },
                {
                    "sent": "So basically in the kernel kind of setting, these terms are basically synonymous so multi view learning is kind of doing the same thing as multiple kernel learning because multiple views can give you multiple multiple kernels and then.",
                    "label": 0
                },
                {
                    "sent": "The goal is to somehow combine these kernels to do better on the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any task that you have.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll go.",
                    "label": 0
                },
                {
                    "sent": "I'll go forward with some background for this work, so our work is based on a principle of colorization, so the correlation correlation idea has been proposed about 10 years back, so it was from the influential paper by Bloom and Michelle, so this idea of correlation basically tries to enforce agreement between learners that are trying to learn using different views of the data.",
                    "label": 0
                },
                {
                    "sent": "So this notion was typically used for.",
                    "label": 0
                },
                {
                    "sent": "Basically, it was originally used for semi supervised learning where you have some label data in some unlabeled data and the data comes in natural naturally multiple views.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you learn multiple classifiers.",
                    "label": 0
                },
                {
                    "sent": "So let's say we only have 2 views V1 and V2.",
                    "label": 1
                },
                {
                    "sent": "So what you do, you train 2 classifiers F1 and F2 and you want these classifiers to agree on unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And by doing this you somehow shrink the set of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The space of hypothesis that you are trying to learn.",
                    "label": 0
                },
                {
                    "sent": "And there has been theoretical and empirical evidence that this kind of a learning setting.",
                    "label": 0
                },
                {
                    "sent": "It reduces the amount of data that you need to learn, and at the end of the at the end of the day, you also learn better than the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Settings.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll talk about how to use code regularization in unsupervised learning setting.",
                    "label": 0
                },
                {
                    "sent": "So as I said in the beginning, so typically code regularization has been used in semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "But in this talk I'll talk about an unsupervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So which in this talk I'll talk about a clustering problem which is the spectral clustering algorithm and spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "As many of you might be knowing, it's a kernel based clustering algorithm and our goal is to.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These multiple kernels from different views to improve spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "So again the idea, just like in core training is that we're trying to learn clustering from different views of the data, and the idea is that we since all the views are representing the same set of objects, we want these clusterings to agree.",
                    "label": 0
                },
                {
                    "sent": "OK. Again, I would reiterate that each view corresponds to a kernel, so it's just like what we do in multiple kernel learning.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and as I will show later during the later part of the talk, this in a sense is similar to combining multiple kernels like combining multiple views in our setting.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be the same similar as combining multiple kernels.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so next give some brief background on spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So the spectral clustering algorithm is based on the idea of spectral decomposition of graph data.",
                    "label": 1
                },
                {
                    "sent": "So it's graph Laplacian of the data and it's a very theoretically well motivated algorithm.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing about this as compared to other algorithms such as K means is that spectral clustering can learn arbitrary shaped clusters, so your clusters need not be round shape, so you don't have these kind of restrictions on the shape of the clusters.",
                    "label": 0
                },
                {
                    "sent": "So introduce some notations here.",
                    "label": 0
                },
                {
                    "sent": "So I'll describe right K as in.",
                    "label": 0
                },
                {
                    "sent": "And by in current metrics of the data which is lying in like R to the ND, and by D&KIJ denotes a similarity between 2 examples and D is again a diagonal matrix with the diagonal entries being the row sums of your kernel metrics.",
                    "label": 1
                },
                {
                    "sent": "And then I define the normalized graph Laplacian as like this L which is D minus half K D -- 5.",
                    "label": 0
                },
                {
                    "sent": "It's a normalized graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "So if I give you the graph Laplacian, then what we do what we want is we want to keep partitions of the data and spectral clustering achieves this by doing.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vector decomposition of this graph Laplacian OK, so let me formally define the spectral clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So these are the objective I'm showing here is by any at all from NIPS 2002, so the spectral processing solves the following problem, so it just maximizes the trace of U transpose Lu where U is a matrix of size N by K and it's the columns are orthonormal.",
                    "label": 0
                },
                {
                    "sent": "So as you can see it's just an eigenvalue problem which amounts to solving finding the top K eigenvectors of L. So you can think of US consisting of each column has eigenvector.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the Laplacian L. OK, so if you think of this matrix you so it's N by K matrix, so you can also think of US giving you a new representation of the data.",
                    "label": 0
                },
                {
                    "sent": "So each row of U corresponds to a row in the data metrics.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what is spectral clustering does in the final step is is that it just runs, normalizes the rules of this you metrics and runs the K means algorithm and that gives you the final clustering solution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's consider this setting.",
                    "label": 0
                },
                {
                    "sent": "So what we have in our case is that we have multiple views of the data, so I'll denote each view as X superscript V. So, so this is the data from view V and we can denote the Laplacian for Vue VSLV and then I can write the similar kind of objective for a single, just like I had for the single view.",
                    "label": 1
                },
                {
                    "sent": "So this is the objective for us.",
                    "label": 1
                },
                {
                    "sent": "For the spectral clustering in Vue V. Now.",
                    "label": 0
                },
                {
                    "sent": "So as you can see in this picture.",
                    "label": 0
                },
                {
                    "sent": "So I have like different views of the data, so all these.",
                    "label": 0
                },
                {
                    "sent": "Different matrices are representing different views of the data, and for each view I can think of I'm doing spectral clustering, so I'm getting these different views.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "My goal is in colourization based spectral clustering would be to somehow reconcile these different views so as to reach on some sort of agreement.",
                    "label": 0
                },
                {
                    "sent": "So I want to I want these used to look similar to each other in some.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which I'll define later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at our objective function.",
                    "label": 0
                },
                {
                    "sent": "So what we have, let's assume just two views.",
                    "label": 0
                },
                {
                    "sent": "So extension to more than two uses like it can be done in a similar fashion, so I'll call my views as V&W.",
                    "label": 1
                },
                {
                    "sent": "So I'll write the joint problem with two views as the spectral objective function for Vue V, just like the one you case.",
                    "label": 1
                },
                {
                    "sent": "Plus we have spectral objective for the second view W and then we have some Co regularizer term which tries to bring which tries to make.",
                    "label": 0
                },
                {
                    "sent": "UV and you do you W the two representations of this.",
                    "label": 0
                },
                {
                    "sent": "You metrics similar in some sense.",
                    "label": 0
                },
                {
                    "sent": "OK, now the question is that we have this code regular either.",
                    "label": 1
                },
                {
                    "sent": "D now what should be look like?",
                    "label": 0
                },
                {
                    "sent": "So there's a question so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Litty now the intuition is that as I said.",
                    "label": 0
                },
                {
                    "sent": "You use the new metrics to do clustering to run some K means clustering as the final step of your respect requesting algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is, whatever you use, it should lead to the same clustering because at the end of the day you were just collecting the same data but using different views, right?",
                    "label": 0
                },
                {
                    "sent": "So you would expect that you should each you should lead to the same clustering.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what it implies that the kernel that you're using with the.",
                    "label": 0
                },
                {
                    "sent": "So if we just take this U, the new representation of the data U and if you just define a kernel using U so all these kernels using the your representation should look similar to each other, right?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So which implies that if you have two kernels to find on the 1st and 2nd views, UV and UW, these should have a high degree of alignment between them.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is we just define a linear kernel in EU space as UV UV transpose and that defines.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the Colonel in EU space.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The alignment term between these two matrices would be just the trace of the product of these two kernel matrices KUV&KUW.",
                    "label": 1
                },
                {
                    "sent": "And since I defined KQ as a linear kernel, it just boils down to this term.",
                    "label": 0
                },
                {
                    "sent": "So you have this UV transpose and you WW transpose, so that's our Co regularizer term so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Logging in this value of this expression of D in the spectral joint spectral clustering objective, we get the following objective function, and then we also have this hyperparameter Lambda, which tunes how much you want to, basically, how much you want your individual objectives to perform and how much how much you want, how much importance you want to give to the code regularization term so that basically is the trade off parameter.",
                    "label": 0
                },
                {
                    "sent": "So that's all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The free parameter in our algorithm.",
                    "label": 0
                },
                {
                    "sent": "No, this kind of an objective function.",
                    "label": 0
                },
                {
                    "sent": "It has, as you can see, it's the optimization is over.",
                    "label": 0
                },
                {
                    "sent": "UV and UW pose the matrices of size N by K and this can be solved using using an alternating optimization scheme.",
                    "label": 1
                },
                {
                    "sent": "So if you just treat UW S fixed then you get the following.",
                    "label": 0
                },
                {
                    "sent": "If you and then if you rearrange the above objective function you just get this this.",
                    "label": 0
                },
                {
                    "sent": "Objective and if you just stare at it, it just looks like the spectral spectral clustering objective with a single view, except for the difference.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you have the Laplacian instead of a single Laplacian LV.",
                    "label": 0
                },
                {
                    "sent": "You have this extra term right?",
                    "label": 0
                },
                {
                    "sent": "So your Laplacian instead of justice being LV, it becomes LV plus Lambda times, uvu, WW, transpose.",
                    "label": 0
                },
                {
                    "sent": "So if you just think of it, it's kind of similar to combining Laplacian, so combining graphs because you originally have the Laplacian from the View View V and then you bring in another component in terms of this this new kernel you WW transpose.",
                    "label": 0
                },
                {
                    "sent": "OK so this is similar to.",
                    "label": 0
                },
                {
                    "sent": "Combining two kernels.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can solve this optimization in an alternating fashion until convergence and you will converge to a local minima.",
                    "label": 0
                },
                {
                    "sent": "In this case local Maxima, because we're doing optimization and we're guaranteed to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "So in terms of experiment, so we have compared against the number of baselines, so the prominent ones are Canonical correlation analysis.",
                    "label": 1
                },
                {
                    "sent": "The CCA, which what it does is that it combines multiple views of the data, it does a shared projection and then uses the features extracted as the final K means clustering in the final clustering step, and we also compare with another multiview agreement based disagreement based clustering by diesel from its recent algorithm so.",
                    "label": 0
                },
                {
                    "sent": "I'm showing the results with on the UCI handwritten digit data set, so we also compared with another number of other baselines, the where we just concatenate the features from different views.",
                    "label": 0
                },
                {
                    "sent": "So that's one baseline, the other is like just.",
                    "label": 0
                },
                {
                    "sent": "Add two kernels or multiply 2 kernels and then the other two baselines are the CC and the minimizations disagreement algorithm, and the final is our core regularizer approach and we see like in all cases.",
                    "label": 1
                },
                {
                    "sent": "So we have compared the clustering performance on a number of metrics that score precision.",
                    "label": 0
                },
                {
                    "sent": "Recall entropion my adjusted Rand index an all these we do reasonably better than the other baselines.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm also showing some results from the writer multilingual data, so it's a multi level data set and your goal is to like you have documents in translations in different languages and you somehow want to combine these documents for clustering to her task.",
                    "label": 0
                },
                {
                    "sent": "So again on this data set as well as you can see we do considerably better than the other baselines here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said in the beginning, so we have a free parameter Lambda which controls how much to the degree to which we want to regularize the different views.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to see how sensitive our algorithm is for the choice of parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "So the figure on the left shows the performance of, so we show the enemy scores.",
                    "label": 0
                },
                {
                    "sent": "So we compare the performance of our algorithm with respect to the closest performing baselines.",
                    "label": 1
                },
                {
                    "sent": "So the line in blue is the closest performing baseline in terms of the isquared gates, and we vary the regularization parameter from zero to.",
                    "label": 0
                },
                {
                    "sent": ".1 and we see that for a wide range of Lambda we are still better than the second closest performing performing baselines.",
                    "label": 1
                },
                {
                    "sent": "So it shows that it's kind of robust to.",
                    "label": 0
                },
                {
                    "sent": "Of course you have a free parameter Lambda, but it's kind of robust for a wide range of Lambda and the other.",
                    "label": 0
                },
                {
                    "sent": "It shows how we converge, so as you can see it's on the Celtic 101 data, so we converge pretty quickly, like four or five iterations.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, so we propose a code regularization based approach for an unsupervised learning problem.",
                    "label": 1
                },
                {
                    "sent": "And as I showed, this approach can be thought of as combining multiple kernels, and the algorithm is pretty simple.",
                    "label": 1
                },
                {
                    "sent": "So at each step you just solve spectral clustering objective with a modified Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So it's just like you can just use any existing Eigen solvers to which to solve that step.",
                    "label": 0
                },
                {
                    "sent": "And I used this.",
                    "label": 0
                },
                {
                    "sent": "People are either the form of the regular regularizer that we used was just the product, the trees of the product of two kernel matrices in EU space, but you could also think of other ways to measure similarities or to enforcing agreement between these two representations that I. I talked earlier and so this is a kind of a general set up, so we had, you know, at the end of the so our underlying objective or some kind of spectral objective function.",
                    "label": 0
                },
                {
                    "sent": "So you could also imagine solving other kind of spectral algorithms which try to do spectral decomposition.",
                    "label": 0
                },
                {
                    "sent": "For example kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "So you could combine multiple kernels in kernel PCA, kind of a setting and you can solve PC as well using this kind of setup.",
                    "label": 0
                },
                {
                    "sent": "So I guess.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all I had to say.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                }
            ]
        }
    }
}