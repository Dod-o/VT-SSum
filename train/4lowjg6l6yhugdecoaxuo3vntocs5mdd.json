{
    "id": "4lowjg6l6yhugdecoaxuo3vntocs5mdd",
    "title": "Spotlights 1",
    "info": {
        "published": "Dec. 3, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_various_s1/",
    "segmentation": [
        [
            "The first spotlight is kernels and learning curves for Gaussian process regression on random graphs OK.",
            "So Gaussian process regression is a Bayesian learning technique where we take a joint Gaussian distribution for the function for all possible inputs as our prior Gaussian process is then uniquely determined by its covariance matrix or its kernel loaning behavior.",
            "Gaussian process is relatively well understood in continuous spaces, but not much so much work that we've done in discrete spaces.",
            "So in our work we've looked at random regular graphs as a good example of a discrete space.",
            "We aim to learn a function that assigns real values to the nodes of the graph.",
            "An example of which can be seen in the bottom left.",
            "We look at learning curves, which is the mean square error as the number of examples increases, this is the dotted line in the top right and try and see if it is found accurate in the Euclidean cases, is applies in discrete space.",
            "Still we use the fact that random regular graphs of tree like we get a prediction that's the thick line and top top right.",
            "We find that it's good in the initial and final regime, but not so good in the crossover.",
            "Surprisingly, we find the learning curve is accurate in this town and the kernel tensor nontrivial limiting forms link scales increased.",
            "For further analysis, please come up, look it up."
        ],
        [
            "21 Next up robust value function approximation using bilinear programming.",
            "We proposed and analyzed bilinear programming.",
            "Approximate belinyu programming formulation for value function approximation in large scale MVP's.",
            "This is a new method.",
            "It has better theoretical properties than existing methods an it's in particular guaranteed to converge to a solution which minimizes the policy loss.",
            "The main idea is to optimize over policies and value functions simultaneously, which is unlike LSP.",
            "Which does this iteratively or approximate linear programming which ignores policies and approximate only value functions.",
            "We also show that an approximate algorithm for solving these approximate bilinear programs motivates and can be used to improve approximate policy iteration.",
            "Next we have an analysis of SVM with indefinite kernels."
        ],
        [
            "So in this work we started the problem of SVM classification with non positive definite kernels and this is the this formulation is proposed by Doctor Douglas and thus promote two years ago and basically previous optimization approaches.",
            "There's two optimization approaches.",
            "The first one assumes object function is not differentiable, and then we can, using smooth techniques to get approximated solutions and 2nd approach is to use simple Infinity, quadratic constraint linear programming.",
            "However, this approach could be inefficient because in each iteration you have to solve quadratic are quadratically constrained linear programming.",
            "So our main contribution is the following.",
            "We prove actually the objective function is differentiable.",
            "Moreover, so objective function actually has Lipschitz continuous gradient and based on the select code is out to be developed.",
            "Smooth optimization approaches to efficient solver SVM with independent kernels.",
            "So if you're interested, please, please come to our postsecondary much.",
            "Next is boosting with spatial regularization."
        ],
        [
            "So in the classification algorithm we use features to train a classifier.",
            "In some applications.",
            "fMRI being why example features have spatial structure and when this is the case, we are interested in addressing the following two challenges.",
            "First, still using fMRI as an example, if we use the activation levels unlock source as features, then there is a spatial structure which standard boosting fails to utilize.",
            "So how can we incorporate this spatial prior knowledge to enhance classification?",
            "Second, because of the spatial structure, every classifier now corresponds to a pattern in the space and we are interested in using classification as a tool to identify the patterns that contain discriminative information.",
            "So how can we combine classification and spatial pattern estimation into a single algorithm?",
            "So my poster describes a new algorithm called spatial regularised boosting that tries to address these two questions.",
            "So we.",
            "Modified the loss function of boosting to incorporate spatial information and theoretically it has a grouping effect similar to the elastic net method, so please stop by my poster at T 74 and I will talk to you more about this, thank you.",
            "Next is abstraction and relational learning."
        ],
        [
            "How do people learn concepts given just a single example, if you consider the example here on the left, you might notice that the dimensions of ball position, size, color are all aligned given just the single example of a group of this kind, we show that people are able to generate additional groups which which satisfy this regularity.",
            "We account for this result by developing a probabilistic model that's able to infer the abstract regularity's that all valid groups must must share.",
            "Our approach is related to previous work on relational learning, analogical reasoning, and theory formation.",
            "So please come and talk to us if you are interested in those topics.",
            "Next, we have probabilistic relational PCA."
        ],
        [
            "In relational data instances often linked with each other, which means that the instances are not IID.",
            "For example, the web pages of often linked with each other, and then bioinformatics.",
            "The proteins are often interacted with each other.",
            "Traditional dimensionality reduction methods such as PCA and probabilistic PC often assumes that the instances ID, which is unreasonable for relational data.",
            "In our work, we propose the log model.",
            "Could probabilistic relational PCA to eliminate the idea assumption?",
            "Experimental results showed that the PPC can outperform PC dramatically.",
            "The PCA can be applied for.",
            "Linked document analysis.",
            "Social network analysis and biological network modeling.",
            "If you're interested, Please remember that the Post ID is T3 fails.",
            "Next, we have decoupling sparsity and smoothness in the discrete hierarchical."
        ],
        [
            "It's a process, so in this paper we studied problem in the discrete hierarchical directory process for topping modeling.",
            "So the problem here is the coupling of the sparsity and smoothness control induced by single topic directly parameter.",
            "So which makes it difficult to achieve both the top sparse topics and smoothly smooth term probabilities we see in the topic distributions.",
            "So to this end, with two decoded sparsity and smoothness control we present sparse topic models in the sparse topic models.",
            "We introduced us back and slap Spike and slab prior for the topic distributions to select the terms that will appear in the topics using Bank of selector variables and then the smoothness will be only enforced over those selected terms after the out of the entire vocabulary.",
            "Through this the sparsity and smoothness control is decompiled and can be achieved together so that the capital model can give can give better predict.",
            "Performance and.",
            "By using, like a similar inferred models as shown in those scatter plots.",
            "Think that's it?",
            "Please come to our poster T4 tell you more about problem is and how we solve that, thanks.",
            "Next we have a joint maximum entropy model for binary neural population patterns and continuous signals.",
            "OK, so here we use 2nd order maximum entropy distribution to model at."
        ],
        [
            "Same time binary new populations as continuous valued sensory stimuli.",
            "So given mean and covariance of your data that joined maximum entropy distribution turns out to be a mixed galaxies in distribution.",
            "We have the margins over the continuous valued variables mixture of Gaussians where each Gaussian is restricted to have the same covariance matrix where some.",
            "The marginal distribution of other binary part is again of easing type.",
            "So nice thing here is that fitting these to extra data is a convex problem and imposes no additional costs compared to the pure binary setting.",
            "So in a neuroscience perspective this can be seen as a probability stick model for the spike triggered average inspector covariance and also in a new encoding setting as well as access to both conditional distributions.",
            "But you can model the encoding as well as the decoding part.",
            "So here we have an example where we generated spikes from leaky integrate and fire neuron and then fitted with a maximum entropy distribution.",
            "So if anything of this sounds interesting to you, please come through the post.",
            "Next up, efficient Bregman range search.",
            "This work is on the problem of rain search whenever."
        ],
        [
            "You have a notion of distance that is given by a pregnant vergence, and the best example of that would be the KL divergent.",
            "So rain searches this problem shown on the left.",
            "Here it's kind of a sibling of nearest neighbor search and the problem is that if you have some large database and anytime you get a query you want to return all points within a certain distance from that query in your database.",
            "And this is kind of a problem and a lot of learning algorithms.",
            "For instance whenever one method to build graphs for spectral based algorithms.",
            "Is to run range search repeatedly to build up this graph, and since it's kind of a core problem, it's been studied quite a bit in the case when you have metrics and is sometimes called enbody methods.",
            "It's kind of a general approach to solving these these things quickly, so in the case of Bregman divergences we need sort of a different set of techniques because we cannot rely on the triangle inequality, so we use this special kind of space decomposition in the center here that is specially attuned to whatever Bregman divergences you're interested in, but using it is not straight, totally straightforward, so the kind of core of this work is developing some of the basic geometry of Bregman divergences so that we can make this range search process efficient using this data structure.",
            "And that relies pretty heavily on convexity properties.",
            "Next is graphs data function in the bed of free energy and loopy belief propagation.",
            "Looking very for."
        ],
        [
            "Nation is an efficient method for inference and graphs.",
            "It provides approximate marginals of the even distribution.",
            "Ruby versus propagation is connected with better free energy.",
            "Actually the solutions of.",
            "Look, if you propagation is identified with the stationary points of the very free energy function.",
            "This is shown by Lydia out in NIPS 2000 to these things we add a new apparatus, grab their function.",
            "Rub their function is known in graph theory and extract informations of graph topology.",
            "And we established a formula that relates to previous propagation and debate.",
            "Free energy and grab their function.",
            "And the problem here gives us new results.",
            "The problems, such as convexity of the better free energy and the rupee.",
            "And the stability and uniqueness of the Ruby Bridge propagation.",
            "OK, thank you.",
            "This is for 1719.",
            "Thank you.",
            "Next we have online learning assignments.",
            "Hi so."
        ],
        [
            "Suppose you have this problem that you like blogs, but you can only read a limited number of them every day.",
            "OK, now you want each blog you read kind of rank high on your list of favorite blogs, but the same time you want the set of logs you read to be a cover a diverse range of topics.",
            "Alright, so these objectives are can be in somewhat conflicting.",
            "But the point is, so how should you decide which blogs to read?",
            "So we present an algorithm that helps you learn which blogs to read.",
            "It does this by presenting you with a list every day.",
            "You then read that list score with some utility and feedback that utility score to the algorithm.",
            "Now as long as your utility satisfy natural diminishing returns property called submodularity, our algorithm is guaranteed to get within 63% of the best fixed list of blogs with respect to utility.",
            "And this is the optimal ratio.",
            "More generally, our algorithm can be used to learn lists online to maximize a sequence of monotone submodular functions.",
            "Other applications of this, so we tested this algorithm on this blog selection problem and some sponsored search ad allocation problem and got good results.",
            "Other applications include news aggregation and recommendation systems and environmental monitoring.",
            "And our poster is at location T41 and I look forward to talking with people about it.",
            "Thank you.",
            "And finally, linear time algorithms for pairwise statistical problems so."
        ],
        [
            "As Lawrence was mentioning two slides ago that the N body problems require are like really important in machine learning.",
            "Examples of the anybody problems are like the all nearest neighbor.",
            "All current density estimation potential summation.",
            "So these problems have been studied in the last decades.",
            "You have the KD tree, you have the Barnes Hut algorithm.",
            "You have the FMM which is like one of the most famous algorithms in the 20th century.",
            "So thing is they have certain runtimes an inverse case, their order N squared an their algorithm like.",
            "Just mentioned are like order N log N. FMM is actually order N, but they have certain assumptions, so a few years back there was this paper.",
            "This proposed the dual tree algorithms, which is an extension of the FMM to different.",
            "Applications like nearest neighbors range search, density estimation.",
            "So and then after that there was this paper on cover trees which give a really good analysis of the runtime for nearest neighbor using three data structures.",
            "So what we do is we take the dual tree algorithm, we take the cover tree analysis and we show that these N body problems can be solved in order end time.",
            "And the algorithms we discussed in this paper, our nearest neighbors, condensed estimation and potential summation.",
            "Please come to a post.",
            "Sure, it's at T 44.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first spotlight is kernels and learning curves for Gaussian process regression on random graphs OK.",
                    "label": 1
                },
                {
                    "sent": "So Gaussian process regression is a Bayesian learning technique where we take a joint Gaussian distribution for the function for all possible inputs as our prior Gaussian process is then uniquely determined by its covariance matrix or its kernel loaning behavior.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process is relatively well understood in continuous spaces, but not much so much work that we've done in discrete spaces.",
                    "label": 0
                },
                {
                    "sent": "So in our work we've looked at random regular graphs as a good example of a discrete space.",
                    "label": 0
                },
                {
                    "sent": "We aim to learn a function that assigns real values to the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "An example of which can be seen in the bottom left.",
                    "label": 0
                },
                {
                    "sent": "We look at learning curves, which is the mean square error as the number of examples increases, this is the dotted line in the top right and try and see if it is found accurate in the Euclidean cases, is applies in discrete space.",
                    "label": 0
                },
                {
                    "sent": "Still we use the fact that random regular graphs of tree like we get a prediction that's the thick line and top top right.",
                    "label": 0
                },
                {
                    "sent": "We find that it's good in the initial and final regime, but not so good in the crossover.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, we find the learning curve is accurate in this town and the kernel tensor nontrivial limiting forms link scales increased.",
                    "label": 0
                },
                {
                    "sent": "For further analysis, please come up, look it up.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "21 Next up robust value function approximation using bilinear programming.",
                    "label": 1
                },
                {
                    "sent": "We proposed and analyzed bilinear programming.",
                    "label": 1
                },
                {
                    "sent": "Approximate belinyu programming formulation for value function approximation in large scale MVP's.",
                    "label": 0
                },
                {
                    "sent": "This is a new method.",
                    "label": 0
                },
                {
                    "sent": "It has better theoretical properties than existing methods an it's in particular guaranteed to converge to a solution which minimizes the policy loss.",
                    "label": 0
                },
                {
                    "sent": "The main idea is to optimize over policies and value functions simultaneously, which is unlike LSP.",
                    "label": 0
                },
                {
                    "sent": "Which does this iteratively or approximate linear programming which ignores policies and approximate only value functions.",
                    "label": 1
                },
                {
                    "sent": "We also show that an approximate algorithm for solving these approximate bilinear programs motivates and can be used to improve approximate policy iteration.",
                    "label": 0
                },
                {
                    "sent": "Next we have an analysis of SVM with indefinite kernels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work we started the problem of SVM classification with non positive definite kernels and this is the this formulation is proposed by Doctor Douglas and thus promote two years ago and basically previous optimization approaches.",
                    "label": 0
                },
                {
                    "sent": "There's two optimization approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one assumes object function is not differentiable, and then we can, using smooth techniques to get approximated solutions and 2nd approach is to use simple Infinity, quadratic constraint linear programming.",
                    "label": 0
                },
                {
                    "sent": "However, this approach could be inefficient because in each iteration you have to solve quadratic are quadratically constrained linear programming.",
                    "label": 1
                },
                {
                    "sent": "So our main contribution is the following.",
                    "label": 1
                },
                {
                    "sent": "We prove actually the objective function is differentiable.",
                    "label": 1
                },
                {
                    "sent": "Moreover, so objective function actually has Lipschitz continuous gradient and based on the select code is out to be developed.",
                    "label": 0
                },
                {
                    "sent": "Smooth optimization approaches to efficient solver SVM with independent kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, please, please come to our postsecondary much.",
                    "label": 0
                },
                {
                    "sent": "Next is boosting with spatial regularization.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the classification algorithm we use features to train a classifier.",
                    "label": 0
                },
                {
                    "sent": "In some applications.",
                    "label": 0
                },
                {
                    "sent": "fMRI being why example features have spatial structure and when this is the case, we are interested in addressing the following two challenges.",
                    "label": 0
                },
                {
                    "sent": "First, still using fMRI as an example, if we use the activation levels unlock source as features, then there is a spatial structure which standard boosting fails to utilize.",
                    "label": 0
                },
                {
                    "sent": "So how can we incorporate this spatial prior knowledge to enhance classification?",
                    "label": 1
                },
                {
                    "sent": "Second, because of the spatial structure, every classifier now corresponds to a pattern in the space and we are interested in using classification as a tool to identify the patterns that contain discriminative information.",
                    "label": 1
                },
                {
                    "sent": "So how can we combine classification and spatial pattern estimation into a single algorithm?",
                    "label": 0
                },
                {
                    "sent": "So my poster describes a new algorithm called spatial regularised boosting that tries to address these two questions.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Modified the loss function of boosting to incorporate spatial information and theoretically it has a grouping effect similar to the elastic net method, so please stop by my poster at T 74 and I will talk to you more about this, thank you.",
                    "label": 0
                },
                {
                    "sent": "Next is abstraction and relational learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do people learn concepts given just a single example, if you consider the example here on the left, you might notice that the dimensions of ball position, size, color are all aligned given just the single example of a group of this kind, we show that people are able to generate additional groups which which satisfy this regularity.",
                    "label": 0
                },
                {
                    "sent": "We account for this result by developing a probabilistic model that's able to infer the abstract regularity's that all valid groups must must share.",
                    "label": 0
                },
                {
                    "sent": "Our approach is related to previous work on relational learning, analogical reasoning, and theory formation.",
                    "label": 0
                },
                {
                    "sent": "So please come and talk to us if you are interested in those topics.",
                    "label": 0
                },
                {
                    "sent": "Next, we have probabilistic relational PCA.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In relational data instances often linked with each other, which means that the instances are not IID.",
                    "label": 1
                },
                {
                    "sent": "For example, the web pages of often linked with each other, and then bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "The proteins are often interacted with each other.",
                    "label": 0
                },
                {
                    "sent": "Traditional dimensionality reduction methods such as PCA and probabilistic PC often assumes that the instances ID, which is unreasonable for relational data.",
                    "label": 0
                },
                {
                    "sent": "In our work, we propose the log model.",
                    "label": 1
                },
                {
                    "sent": "Could probabilistic relational PCA to eliminate the idea assumption?",
                    "label": 0
                },
                {
                    "sent": "Experimental results showed that the PPC can outperform PC dramatically.",
                    "label": 0
                },
                {
                    "sent": "The PCA can be applied for.",
                    "label": 0
                },
                {
                    "sent": "Linked document analysis.",
                    "label": 1
                },
                {
                    "sent": "Social network analysis and biological network modeling.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, Please remember that the Post ID is T3 fails.",
                    "label": 0
                },
                {
                    "sent": "Next, we have decoupling sparsity and smoothness in the discrete hierarchical.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a process, so in this paper we studied problem in the discrete hierarchical directory process for topping modeling.",
                    "label": 1
                },
                {
                    "sent": "So the problem here is the coupling of the sparsity and smoothness control induced by single topic directly parameter.",
                    "label": 1
                },
                {
                    "sent": "So which makes it difficult to achieve both the top sparse topics and smoothly smooth term probabilities we see in the topic distributions.",
                    "label": 0
                },
                {
                    "sent": "So to this end, with two decoded sparsity and smoothness control we present sparse topic models in the sparse topic models.",
                    "label": 1
                },
                {
                    "sent": "We introduced us back and slap Spike and slab prior for the topic distributions to select the terms that will appear in the topics using Bank of selector variables and then the smoothness will be only enforced over those selected terms after the out of the entire vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Through this the sparsity and smoothness control is decompiled and can be achieved together so that the capital model can give can give better predict.",
                    "label": 0
                },
                {
                    "sent": "Performance and.",
                    "label": 0
                },
                {
                    "sent": "By using, like a similar inferred models as shown in those scatter plots.",
                    "label": 0
                },
                {
                    "sent": "Think that's it?",
                    "label": 0
                },
                {
                    "sent": "Please come to our poster T4 tell you more about problem is and how we solve that, thanks.",
                    "label": 0
                },
                {
                    "sent": "Next we have a joint maximum entropy model for binary neural population patterns and continuous signals.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we use 2nd order maximum entropy distribution to model at.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same time binary new populations as continuous valued sensory stimuli.",
                    "label": 0
                },
                {
                    "sent": "So given mean and covariance of your data that joined maximum entropy distribution turns out to be a mixed galaxies in distribution.",
                    "label": 0
                },
                {
                    "sent": "We have the margins over the continuous valued variables mixture of Gaussians where each Gaussian is restricted to have the same covariance matrix where some.",
                    "label": 0
                },
                {
                    "sent": "The marginal distribution of other binary part is again of easing type.",
                    "label": 0
                },
                {
                    "sent": "So nice thing here is that fitting these to extra data is a convex problem and imposes no additional costs compared to the pure binary setting.",
                    "label": 0
                },
                {
                    "sent": "So in a neuroscience perspective this can be seen as a probability stick model for the spike triggered average inspector covariance and also in a new encoding setting as well as access to both conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "But you can model the encoding as well as the decoding part.",
                    "label": 0
                },
                {
                    "sent": "So here we have an example where we generated spikes from leaky integrate and fire neuron and then fitted with a maximum entropy distribution.",
                    "label": 0
                },
                {
                    "sent": "So if anything of this sounds interesting to you, please come through the post.",
                    "label": 0
                },
                {
                    "sent": "Next up, efficient Bregman range search.",
                    "label": 0
                },
                {
                    "sent": "This work is on the problem of rain search whenever.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have a notion of distance that is given by a pregnant vergence, and the best example of that would be the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "So rain searches this problem shown on the left.",
                    "label": 0
                },
                {
                    "sent": "Here it's kind of a sibling of nearest neighbor search and the problem is that if you have some large database and anytime you get a query you want to return all points within a certain distance from that query in your database.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of a problem and a lot of learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "For instance whenever one method to build graphs for spectral based algorithms.",
                    "label": 0
                },
                {
                    "sent": "Is to run range search repeatedly to build up this graph, and since it's kind of a core problem, it's been studied quite a bit in the case when you have metrics and is sometimes called enbody methods.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a general approach to solving these these things quickly, so in the case of Bregman divergences we need sort of a different set of techniques because we cannot rely on the triangle inequality, so we use this special kind of space decomposition in the center here that is specially attuned to whatever Bregman divergences you're interested in, but using it is not straight, totally straightforward, so the kind of core of this work is developing some of the basic geometry of Bregman divergences so that we can make this range search process efficient using this data structure.",
                    "label": 0
                },
                {
                    "sent": "And that relies pretty heavily on convexity properties.",
                    "label": 0
                },
                {
                    "sent": "Next is graphs data function in the bed of free energy and loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Looking very for.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation is an efficient method for inference and graphs.",
                    "label": 0
                },
                {
                    "sent": "It provides approximate marginals of the even distribution.",
                    "label": 0
                },
                {
                    "sent": "Ruby versus propagation is connected with better free energy.",
                    "label": 0
                },
                {
                    "sent": "Actually the solutions of.",
                    "label": 0
                },
                {
                    "sent": "Look, if you propagation is identified with the stationary points of the very free energy function.",
                    "label": 0
                },
                {
                    "sent": "This is shown by Lydia out in NIPS 2000 to these things we add a new apparatus, grab their function.",
                    "label": 0
                },
                {
                    "sent": "Rub their function is known in graph theory and extract informations of graph topology.",
                    "label": 0
                },
                {
                    "sent": "And we established a formula that relates to previous propagation and debate.",
                    "label": 0
                },
                {
                    "sent": "Free energy and grab their function.",
                    "label": 1
                },
                {
                    "sent": "And the problem here gives us new results.",
                    "label": 0
                },
                {
                    "sent": "The problems, such as convexity of the better free energy and the rupee.",
                    "label": 1
                },
                {
                    "sent": "And the stability and uniqueness of the Ruby Bridge propagation.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "This is for 1719.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Next we have online learning assignments.",
                    "label": 0
                },
                {
                    "sent": "Hi so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose you have this problem that you like blogs, but you can only read a limited number of them every day.",
                    "label": 0
                },
                {
                    "sent": "OK, now you want each blog you read kind of rank high on your list of favorite blogs, but the same time you want the set of logs you read to be a cover a diverse range of topics.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these objectives are can be in somewhat conflicting.",
                    "label": 0
                },
                {
                    "sent": "But the point is, so how should you decide which blogs to read?",
                    "label": 0
                },
                {
                    "sent": "So we present an algorithm that helps you learn which blogs to read.",
                    "label": 0
                },
                {
                    "sent": "It does this by presenting you with a list every day.",
                    "label": 0
                },
                {
                    "sent": "You then read that list score with some utility and feedback that utility score to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now as long as your utility satisfy natural diminishing returns property called submodularity, our algorithm is guaranteed to get within 63% of the best fixed list of blogs with respect to utility.",
                    "label": 0
                },
                {
                    "sent": "And this is the optimal ratio.",
                    "label": 0
                },
                {
                    "sent": "More generally, our algorithm can be used to learn lists online to maximize a sequence of monotone submodular functions.",
                    "label": 0
                },
                {
                    "sent": "Other applications of this, so we tested this algorithm on this blog selection problem and some sponsored search ad allocation problem and got good results.",
                    "label": 0
                },
                {
                    "sent": "Other applications include news aggregation and recommendation systems and environmental monitoring.",
                    "label": 0
                },
                {
                    "sent": "And our poster is at location T41 and I look forward to talking with people about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "And finally, linear time algorithms for pairwise statistical problems so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As Lawrence was mentioning two slides ago that the N body problems require are like really important in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Examples of the anybody problems are like the all nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "All current density estimation potential summation.",
                    "label": 0
                },
                {
                    "sent": "So these problems have been studied in the last decades.",
                    "label": 0
                },
                {
                    "sent": "You have the KD tree, you have the Barnes Hut algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have the FMM which is like one of the most famous algorithms in the 20th century.",
                    "label": 0
                },
                {
                    "sent": "So thing is they have certain runtimes an inverse case, their order N squared an their algorithm like.",
                    "label": 0
                },
                {
                    "sent": "Just mentioned are like order N log N. FMM is actually order N, but they have certain assumptions, so a few years back there was this paper.",
                    "label": 0
                },
                {
                    "sent": "This proposed the dual tree algorithms, which is an extension of the FMM to different.",
                    "label": 0
                },
                {
                    "sent": "Applications like nearest neighbors range search, density estimation.",
                    "label": 0
                },
                {
                    "sent": "So and then after that there was this paper on cover trees which give a really good analysis of the runtime for nearest neighbor using three data structures.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take the dual tree algorithm, we take the cover tree analysis and we show that these N body problems can be solved in order end time.",
                    "label": 0
                },
                {
                    "sent": "And the algorithms we discussed in this paper, our nearest neighbors, condensed estimation and potential summation.",
                    "label": 0
                },
                {
                    "sent": "Please come to a post.",
                    "label": 0
                },
                {
                    "sent": "Sure, it's at T 44.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}