{
    "id": "cbqon37rom3ui6gx6n2if4rkrcev5eju",
    "title": "Efficient discriminative learning of Bayesian network classifier via Boosted Augmented Naive Bayes",
    "info": {
        "author": [
            "Yushi Jing, Georgia Institute of Technology"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml05_jing_edlbn/",
    "segmentation": [
        [
            "And we're going to have for talks in this session.",
            "The first part is actually a winner.",
            "Distinguished Student Paper award.",
            "Efficient, discriminative learning.",
            "Bayesian network classifier.",
            "Eugene Levy, Republic at James Ray.",
            "OK, so this goes on top of the.",
            "Hello.",
            "Hi, my name is Eugene and we're presenting boosted augmented naive base.",
            "Simple and efficient way to improve the classification accuracy of Bayesian network classifiers.",
            "This is a joint work with Vladimir Pavlovich from Rogers University, an James Ray from Georgia Tech.",
            "Are there too many?"
        ],
        [
            "Contributions of our work.",
            "First, we revisit the boost Naive Bayes classifier, first developed by Chance back in 1997.",
            "We showed that ensemble of sparse graphical model has very attractive properties, including very efficient training, computational complexity, and very good classification accuracy.",
            "We also present most comprehensive empirical evaluation of boosting lying face up to date.",
            "2nd, we present with post boosted augment United space efficient structure learning algorithm to further improve its classification classification accuracy.",
            "We show that is efficient and also have competitive classification accuracy are performing standard Bayesian network classifiers including my base 10 and compared to or are performing more recently proposed BMC and you are algorithm.",
            "I'll just start paging network."
        ],
        [
            "Popular mechanisms to model complex problem domain for its intuitive and modular graphical interpretation and its explicit incorporation of.",
            "Probabilistic relationship a patient.",
            "Network classifiers convert the estimated joint distribution into a conditional distribution which are labeled can be inferred by estimators such as map.",
            "The question is, how do we effectively and efficiently train generative model activation for classified into?",
            "If.",
            "How to return a generative model like Bayesian Network into effective classifier?",
            "The."
        ],
        [
            "Problem lies in the one of the usely and mostly used prior to learning technique.",
            "Maximum likely learning since the joint distribution of data can be decomposed over the structure of the network, maximum lack of learning extremely fast.",
            "However, it maximizes the log likelihood score rather than the conditional log likelihood score, which is more directly related to classification accuracy.",
            "As you can see here, Al score can be decomposed as CL score and the feature log likelihood.",
            "However, Seattle score does not decompose over the structure of the network, and optimizing it requires more complex methods.",
            "Also, the structure of."
        ],
        [
            "The network plays a crucial role in the optimality of the ML learning.",
            "If the base structure, say and I base is too sparse, then the base node classifier use the limited capacity to partially model the joint distribution of the feature, which is unnecessary and sometimes and therefore maximum.",
            "It does not maximize the CL score in this case.",
            "And quiner enjoy in 2002 proposed the ER algorithm that uses gradient descent and linesearch to directly find optimal set of parameters.",
            "On the other hand, given the optimal structure, Bayesian network classifiers able to find the accurate joint distribution, which in turn have obtain an accurate estimation of the conditional distribution.",
            "That's yml learning in this case maximizes CL score and Grossman domain goes last year.",
            "Using this this property to search through the optimal to search through space and structures to find the optimal structure.",
            "Both measures, both methods, improve the classification accuracy.",
            "However, because it involves a large evaluation of potential classifiers that can be computationally expensive in training.",
            "A third approach is to use our ensemble of sparser Model 2 as alternative to a more density connected Model B and use maximum like learning as a parameter to train the parameter of individual based network.",
            "And our goal is to."
        ],
        [
            "Combined parameter as optimization together with structure position at the same time avoid overfitting and more Interestingly, will carry develop efficient training algorithm.",
            "The the rest of talk is organized in the following.",
            "First, we propose to use exponential loss function for the minimization task.",
            "We followed by empirical evaluation with boosting like base, then followed by our proposal of boosted augmented naive Bayes or Ben algorithm will conclude with the empirical evaluation of band.",
            "So the exponential loss function."
        ],
        [
            "Your F is a very widely used last function for wider use.",
            "Upper bound on the training error here.",
            "Why is the class label an FX?",
            "Is functional Maps of feature into a prediction?",
            "And if we represent the estimator conditional distribution as logical function, then ROF can be easily shown as simply upper bound on the negative conditional log likelihood of ensemble classifier.",
            "In order to minimize the LF score while."
        ],
        [
            "Overfitting will simply use tables for for for the task.",
            "At each iteration of boosting will calculate.",
            "We use maximum maximum likelihood learning to calculate to end to calculate the parameter for each Beijing network classifier on the discriminatively weighted data.",
            "Because because of this feature, it is very fast.",
            "At the generation we continue this process until boosting convergence and the final classifier is linear ensemble of individual Bayesian network classifiers.",
            "Here is a evaluation of Post 9."
        ],
        [
            "Base against its competing methods, let me explain the graph.",
            "Little bit.",
            "the Y axis is the classification accuracy for boosting my face, and I'm sorry the X axis and the Y axis is the classification error for the competing methods or the crosses above the diagonal line favors boosted my base and the other classes underneath their favors.",
            "Naive bayes.",
            "We also included the number of datasets that are statistically statistically significant and.",
            "Edge of the graph.",
            "For example, in this case boosting I base has 10 datasets that are statistically statistically significant.",
            "Then are you based in vice versa?",
            "We also included the average classification error across 25 datasets and edge.",
            "And here is the full comparison."
        ],
        [
            "And as you can see, boosting I based on our performance knowledge base and 10 and slightly outperforms being C 2P and have comperable classification accuracy as the ER algorithm.",
            "Here's a brief evaluation of both."
        ],
        [
            ":) it's very efficient.",
            "Computation time is big of M&T where M is number of training data.",
            "N is number of features we have and T is number of boosting iteration.",
            "Since empirically we observe that she's number between 5 to 20 and the entirely computational complexity is essentially optimal.",
            "And since we since sparse structure combined with boosting achieves very good classification accuracy.",
            "I am actually.",
            "We also observed that boost tonight base do not outperform 10 or BMC algorithm in data sets where there were the features are very highly correlated.",
            "So then the next logical step is to augment both naive Bayes with most important edges.",
            "To further improve its classification performance.",
            "However, the challenges."
        ],
        [
            "Structure searches difficult problem.",
            "Exhaustive search over the structure spaces empty heart even heuristic methods including K2 or hillclimbing methods exams hundreds and thousands of structures.",
            "This is clearly infeasible when used in combination with parameter boosting.",
            "And also a complex structure if incorrect.",
            "If unless is up to model often leads to overfitting of the data.",
            "So we would like to keep.",
            "The structure as simple as possible found observation that a sparse model, combined with boosting achieves very good classification accuracy.",
            "We only add edges that are most important into our sparse model to form ensemble.",
            "That way we can cut down the number of structures for exam.",
            "The implementation is very easy in the first step we construct."
        ],
        [
            "Pairwise conditional mutual information and then from which a maximum spanning tree can be can be constructed.",
            "The pairwise conditional mutual information simply identifies features that are highly correlated with each other and maximum spanning tree explicitly controls the capacity of the classifier by saying that each feature can only have one parent besides its class.",
            "We start."
        ],
        [
            "With the boost in naive base and we evaluate the conditional log likelihood score of the boost in Naive Bayes, here is the base structure.",
            "And over here is the G tree we created in the previous slides and this all the edges are not augmented into the naive Bayes yet.",
            "The next step here is evaluation of both."
        ],
        [
            ":) next step we augment boost my base with the edge that has the highest condition, which information from the tree and we again apply Ada boost to this base structure to obtain the CL score.",
            "We continue this process until the CL squared no longer improves.",
            "And here example of the."
        ],
        [
            "Final assemble structure.",
            "Each model learned the way structure is often very sparse, containing empirically containing zero to five edges and and the final class structure is ensemble of individual based network classifier.",
            "It is usually more sparse than models of 10."
        ],
        [
            "From C10 and BMC algorithm.",
            "Which will show empirical in the next couple slides.",
            "The computational complexity of."
        ],
        [
            "Pam algorithm is broken down into following the computation of conditional mutual information and orangey tree takes the big of Ms squared and the structure search is simply a constant times the the estimation of naive Bayes, where here T is number of boosting iteration and S is number of structure we evaluated as in the worst case band algorithm evaluates number of structures however in.",
            "Empirically, van terminates after adding zero to five edges into network.",
            "Our observation, the training time of band is approximately 2500 times more excellent, expensive than the training of my base.",
            "Here is the results on SIM."
        ],
        [
            "The datasets we generated 25 distributions from the true structure, which is a chance structured graph.",
            "Is a different parameter setting and different number of nodes and we tested on naive Bayes.",
            "Cincinnati based does not capture the dependence relationship between the features.",
            "This is clearly incorrect model choice.",
            "Not surprisingly Ben, our performance Boost Bam outperforms naive Bayes 29, Two, 0."
        ],
        [
            "Sorry 19 to 0.",
            "The reason they have similar classification accuracy in six datasets is becausw.",
            "The distortion in the estimation of conditional log likelihood do not result in labeled this classification for Naive Bayes.",
            "Here's the comp."
        ],
        [
            "Harrison between booster Naive Bayes and Ben.",
            "Since this is a fairly simple classification problem, boosting my base achieved the optimal Bayes theorem in 22 datasets, but bandwidth shift based optimum based here in all 25 datasets, the edges band identified a correlate to the edges from the true structure.",
            "We also eval."
        ],
        [
            "Band on 25 UCI datasets, a commonly used for the evaluation of Beijing network classifiers.",
            "When implemented naive base 10, then, and BMC and BMD PNC algorithm and we obtained results for EOR from the published results.",
            "Here is."
        ],
        [
            "The comparison between ban against the standard Bayesian network classifier, Naive Bayes and 10.",
            "As you can see here, Bam clearly outperforms Naive Bayes and 10."
        ],
        [
            "More interesting comparison is between Ben and BMC structure learning algorithm.",
            "They are the classification accuracy, the average classification accuracy differs by 2% and.",
            "And the number of datasets were band are performing BMC's is status maccess significant although the base structure that ventures is much more sparse than the dental structure chosen by BBC 2P algorithm.",
            "However, Ben is ensemble of sparse models.",
            "And."
        ],
        [
            "Here is the comparison between Ben algorithm and you are banning.",
            "Your has come parable classification performance over here.",
            "Best slightly outperforms your argument render naive based.",
            "However it is not static, is significant.",
            "However, Bam is more efficient than you are too in training.",
            "Here's the final value."
        ],
        [
            "Ocean between Bannon, boosting our new base even though they only differ by 1% average average classification error about our performance, naive Bayes significantly in seven datasets were not boosting nine base are performing spend in two band was especially useful in datasets where features are highly correlated.",
            "For example, corral with difference between 2 to 5% and we did observe overfitting.",
            "For bandwidth comparing to boost my base in two datasets we would like to mention that.",
            "In some cases, band actually picked boost Naive Bayes as the optimal structure.",
            "For example in Iris and no phone.",
            "So and to compare the average classification error between the datasets about our performance boost night based 16 two 6.",
            "So in."
        ],
        [
            "Conclusion We showed that ensemble of sparse model can serve as alternative to search for more densely connected graph.",
            "Therefore it's very efficient to implement and for training and also it's very simple.",
            "It's very simple to implement and has compared to the classification accuracy as most standard Bayesian network classifiers as well as the more complex recently proposed Bayesian network classifiers.",
            "In the future we would like."
        ],
        [
            "Pipe Band to handle sequential data classification and we also would like to analyze.",
            "To analyze the relationship between the final dense structure with the ensemble of the more sparse model and also can we use band to generative models as well instead of constant instead of classifiers.",
            "Questions.",
            "This.",
            "I.",
            "How does it work on noisy data?",
            "Work on noise data so we.",
            "Right, so the simulated.",
            "So in the simulated datasets where we generate 4000 samples from the chain structure graph and sometimes the change is actually quite long, so there is the sparsity of data in few cases.",
            "That's why when you go back.",
            "That's why when you go back to the graph, you'll see oh one or two points that's actually below the diagonal bar.",
            "However, it's not statistically significant, so in some cases it's supposed to.",
            "The algorithm actually overfits or comparing boosting value base.",
            "So what about boosting?",
            "The is actually not much slower than pen, and it's more accurate, so I would guess that if we boost the insensitive thank, you may get better results.",
            "OK, so the question is why?",
            "Why can't we build ensemble of BMC model?",
            "So BMC 2P AP&C model learn from BMC 2P algorithm.",
            "P MC2P model is actually a very good model by itself.",
            "It has as we continue expanding the prior work, it has very good classification accuracy.",
            "However, to find that particular model involves structure search over hundreds and thousands of our offer classify evaluation and we think there are method is much simpler and more efficient to train.",
            "But since BNC model is already quite accurate and the way we did not try to experiment.",
            "But we are the the cases you may not further may not actually further improve its classification accuracy.",
            "But we would like to do some experiments.",
            "OK, let's take this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to have for talks in this session.",
                    "label": 0
                },
                {
                    "sent": "The first part is actually a winner.",
                    "label": 0
                },
                {
                    "sent": "Distinguished Student Paper award.",
                    "label": 0
                },
                {
                    "sent": "Efficient, discriminative learning.",
                    "label": 0
                },
                {
                    "sent": "Bayesian network classifier.",
                    "label": 0
                },
                {
                    "sent": "Eugene Levy, Republic at James Ray.",
                    "label": 0
                },
                {
                    "sent": "OK, so this goes on top of the.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Hi, my name is Eugene and we're presenting boosted augmented naive base.",
                    "label": 0
                },
                {
                    "sent": "Simple and efficient way to improve the classification accuracy of Bayesian network classifiers.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with Vladimir Pavlovich from Rogers University, an James Ray from Georgia Tech.",
                    "label": 0
                },
                {
                    "sent": "Are there too many?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Contributions of our work.",
                    "label": 0
                },
                {
                    "sent": "First, we revisit the boost Naive Bayes classifier, first developed by Chance back in 1997.",
                    "label": 0
                },
                {
                    "sent": "We showed that ensemble of sparse graphical model has very attractive properties, including very efficient training, computational complexity, and very good classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "We also present most comprehensive empirical evaluation of boosting lying face up to date.",
                    "label": 0
                },
                {
                    "sent": "2nd, we present with post boosted augment United space efficient structure learning algorithm to further improve its classification classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "We show that is efficient and also have competitive classification accuracy are performing standard Bayesian network classifiers including my base 10 and compared to or are performing more recently proposed BMC and you are algorithm.",
                    "label": 1
                },
                {
                    "sent": "I'll just start paging network.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Popular mechanisms to model complex problem domain for its intuitive and modular graphical interpretation and its explicit incorporation of.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic relationship a patient.",
                    "label": 0
                },
                {
                    "sent": "Network classifiers convert the estimated joint distribution into a conditional distribution which are labeled can be inferred by estimators such as map.",
                    "label": 1
                },
                {
                    "sent": "The question is, how do we effectively and efficiently train generative model activation for classified into?",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 1
                },
                {
                    "sent": "How to return a generative model like Bayesian Network into effective classifier?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem lies in the one of the usely and mostly used prior to learning technique.",
                    "label": 0
                },
                {
                    "sent": "Maximum likely learning since the joint distribution of data can be decomposed over the structure of the network, maximum lack of learning extremely fast.",
                    "label": 0
                },
                {
                    "sent": "However, it maximizes the log likelihood score rather than the conditional log likelihood score, which is more directly related to classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, Al score can be decomposed as CL score and the feature log likelihood.",
                    "label": 0
                },
                {
                    "sent": "However, Seattle score does not decompose over the structure of the network, and optimizing it requires more complex methods.",
                    "label": 0
                },
                {
                    "sent": "Also, the structure of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The network plays a crucial role in the optimality of the ML learning.",
                    "label": 0
                },
                {
                    "sent": "If the base structure, say and I base is too sparse, then the base node classifier use the limited capacity to partially model the joint distribution of the feature, which is unnecessary and sometimes and therefore maximum.",
                    "label": 0
                },
                {
                    "sent": "It does not maximize the CL score in this case.",
                    "label": 1
                },
                {
                    "sent": "And quiner enjoy in 2002 proposed the ER algorithm that uses gradient descent and linesearch to directly find optimal set of parameters.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, given the optimal structure, Bayesian network classifiers able to find the accurate joint distribution, which in turn have obtain an accurate estimation of the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "That's yml learning in this case maximizes CL score and Grossman domain goes last year.",
                    "label": 0
                },
                {
                    "sent": "Using this this property to search through the optimal to search through space and structures to find the optimal structure.",
                    "label": 1
                },
                {
                    "sent": "Both measures, both methods, improve the classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "However, because it involves a large evaluation of potential classifiers that can be computationally expensive in training.",
                    "label": 1
                },
                {
                    "sent": "A third approach is to use our ensemble of sparser Model 2 as alternative to a more density connected Model B and use maximum like learning as a parameter to train the parameter of individual based network.",
                    "label": 0
                },
                {
                    "sent": "And our goal is to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Combined parameter as optimization together with structure position at the same time avoid overfitting and more Interestingly, will carry develop efficient training algorithm.",
                    "label": 0
                },
                {
                    "sent": "The the rest of talk is organized in the following.",
                    "label": 0
                },
                {
                    "sent": "First, we propose to use exponential loss function for the minimization task.",
                    "label": 0
                },
                {
                    "sent": "We followed by empirical evaluation with boosting like base, then followed by our proposal of boosted augmented naive Bayes or Ben algorithm will conclude with the empirical evaluation of band.",
                    "label": 1
                },
                {
                    "sent": "So the exponential loss function.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your F is a very widely used last function for wider use.",
                    "label": 0
                },
                {
                    "sent": "Upper bound on the training error here.",
                    "label": 0
                },
                {
                    "sent": "Why is the class label an FX?",
                    "label": 0
                },
                {
                    "sent": "Is functional Maps of feature into a prediction?",
                    "label": 0
                },
                {
                    "sent": "And if we represent the estimator conditional distribution as logical function, then ROF can be easily shown as simply upper bound on the negative conditional log likelihood of ensemble classifier.",
                    "label": 0
                },
                {
                    "sent": "In order to minimize the LF score while.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overfitting will simply use tables for for for the task.",
                    "label": 0
                },
                {
                    "sent": "At each iteration of boosting will calculate.",
                    "label": 0
                },
                {
                    "sent": "We use maximum maximum likelihood learning to calculate to end to calculate the parameter for each Beijing network classifier on the discriminatively weighted data.",
                    "label": 0
                },
                {
                    "sent": "Because because of this feature, it is very fast.",
                    "label": 0
                },
                {
                    "sent": "At the generation we continue this process until boosting convergence and the final classifier is linear ensemble of individual Bayesian network classifiers.",
                    "label": 1
                },
                {
                    "sent": "Here is a evaluation of Post 9.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Base against its competing methods, let me explain the graph.",
                    "label": 0
                },
                {
                    "sent": "Little bit.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the classification accuracy for boosting my face, and I'm sorry the X axis and the Y axis is the classification error for the competing methods or the crosses above the diagonal line favors boosted my base and the other classes underneath their favors.",
                    "label": 0
                },
                {
                    "sent": "Naive bayes.",
                    "label": 0
                },
                {
                    "sent": "We also included the number of datasets that are statistically statistically significant and.",
                    "label": 0
                },
                {
                    "sent": "Edge of the graph.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case boosting I base has 10 datasets that are statistically statistically significant.",
                    "label": 0
                },
                {
                    "sent": "Then are you based in vice versa?",
                    "label": 0
                },
                {
                    "sent": "We also included the average classification error across 25 datasets and edge.",
                    "label": 0
                },
                {
                    "sent": "And here is the full comparison.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you can see, boosting I based on our performance knowledge base and 10 and slightly outperforms being C 2P and have comperable classification accuracy as the ER algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here's a brief evaluation of both.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ":) it's very efficient.",
                    "label": 0
                },
                {
                    "sent": "Computation time is big of M&T where M is number of training data.",
                    "label": 0
                },
                {
                    "sent": "N is number of features we have and T is number of boosting iteration.",
                    "label": 0
                },
                {
                    "sent": "Since empirically we observe that she's number between 5 to 20 and the entirely computational complexity is essentially optimal.",
                    "label": 0
                },
                {
                    "sent": "And since we since sparse structure combined with boosting achieves very good classification accuracy.",
                    "label": 1
                },
                {
                    "sent": "I am actually.",
                    "label": 0
                },
                {
                    "sent": "We also observed that boost tonight base do not outperform 10 or BMC algorithm in data sets where there were the features are very highly correlated.",
                    "label": 0
                },
                {
                    "sent": "So then the next logical step is to augment both naive Bayes with most important edges.",
                    "label": 0
                },
                {
                    "sent": "To further improve its classification performance.",
                    "label": 0
                },
                {
                    "sent": "However, the challenges.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure searches difficult problem.",
                    "label": 0
                },
                {
                    "sent": "Exhaustive search over the structure spaces empty heart even heuristic methods including K2 or hillclimbing methods exams hundreds and thousands of structures.",
                    "label": 0
                },
                {
                    "sent": "This is clearly infeasible when used in combination with parameter boosting.",
                    "label": 0
                },
                {
                    "sent": "And also a complex structure if incorrect.",
                    "label": 0
                },
                {
                    "sent": "If unless is up to model often leads to overfitting of the data.",
                    "label": 0
                },
                {
                    "sent": "So we would like to keep.",
                    "label": 0
                },
                {
                    "sent": "The structure as simple as possible found observation that a sparse model, combined with boosting achieves very good classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "We only add edges that are most important into our sparse model to form ensemble.",
                    "label": 1
                },
                {
                    "sent": "That way we can cut down the number of structures for exam.",
                    "label": 0
                },
                {
                    "sent": "The implementation is very easy in the first step we construct.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pairwise conditional mutual information and then from which a maximum spanning tree can be can be constructed.",
                    "label": 1
                },
                {
                    "sent": "The pairwise conditional mutual information simply identifies features that are highly correlated with each other and maximum spanning tree explicitly controls the capacity of the classifier by saying that each feature can only have one parent besides its class.",
                    "label": 0
                },
                {
                    "sent": "We start.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the boost in naive base and we evaluate the conditional log likelihood score of the boost in Naive Bayes, here is the base structure.",
                    "label": 0
                },
                {
                    "sent": "And over here is the G tree we created in the previous slides and this all the edges are not augmented into the naive Bayes yet.",
                    "label": 1
                },
                {
                    "sent": "The next step here is evaluation of both.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ":) next step we augment boost my base with the edge that has the highest condition, which information from the tree and we again apply Ada boost to this base structure to obtain the CL score.",
                    "label": 0
                },
                {
                    "sent": "We continue this process until the CL squared no longer improves.",
                    "label": 0
                },
                {
                    "sent": "And here example of the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Final assemble structure.",
                    "label": 0
                },
                {
                    "sent": "Each model learned the way structure is often very sparse, containing empirically containing zero to five edges and and the final class structure is ensemble of individual based network classifier.",
                    "label": 1
                },
                {
                    "sent": "It is usually more sparse than models of 10.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From C10 and BMC algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which will show empirical in the next couple slides.",
                    "label": 0
                },
                {
                    "sent": "The computational complexity of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pam algorithm is broken down into following the computation of conditional mutual information and orangey tree takes the big of Ms squared and the structure search is simply a constant times the the estimation of naive Bayes, where here T is number of boosting iteration and S is number of structure we evaluated as in the worst case band algorithm evaluates number of structures however in.",
                    "label": 1
                },
                {
                    "sent": "Empirically, van terminates after adding zero to five edges into network.",
                    "label": 0
                },
                {
                    "sent": "Our observation, the training time of band is approximately 2500 times more excellent, expensive than the training of my base.",
                    "label": 1
                },
                {
                    "sent": "Here is the results on SIM.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The datasets we generated 25 distributions from the true structure, which is a chance structured graph.",
                    "label": 0
                },
                {
                    "sent": "Is a different parameter setting and different number of nodes and we tested on naive Bayes.",
                    "label": 1
                },
                {
                    "sent": "Cincinnati based does not capture the dependence relationship between the features.",
                    "label": 0
                },
                {
                    "sent": "This is clearly incorrect model choice.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly Ben, our performance Boost Bam outperforms naive Bayes 29, Two, 0.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry 19 to 0.",
                    "label": 0
                },
                {
                    "sent": "The reason they have similar classification accuracy in six datasets is becausw.",
                    "label": 0
                },
                {
                    "sent": "The distortion in the estimation of conditional log likelihood do not result in labeled this classification for Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Here's the comp.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Harrison between booster Naive Bayes and Ben.",
                    "label": 0
                },
                {
                    "sent": "Since this is a fairly simple classification problem, boosting my base achieved the optimal Bayes theorem in 22 datasets, but bandwidth shift based optimum based here in all 25 datasets, the edges band identified a correlate to the edges from the true structure.",
                    "label": 1
                },
                {
                    "sent": "We also eval.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Band on 25 UCI datasets, a commonly used for the evaluation of Beijing network classifiers.",
                    "label": 1
                },
                {
                    "sent": "When implemented naive base 10, then, and BMC and BMD PNC algorithm and we obtained results for EOR from the published results.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The comparison between ban against the standard Bayesian network classifier, Naive Bayes and 10.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, Bam clearly outperforms Naive Bayes and 10.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More interesting comparison is between Ben and BMC structure learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "They are the classification accuracy, the average classification accuracy differs by 2% and.",
                    "label": 0
                },
                {
                    "sent": "And the number of datasets were band are performing BMC's is status maccess significant although the base structure that ventures is much more sparse than the dental structure chosen by BBC 2P algorithm.",
                    "label": 0
                },
                {
                    "sent": "However, Ben is ensemble of sparse models.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the comparison between Ben algorithm and you are banning.",
                    "label": 0
                },
                {
                    "sent": "Your has come parable classification performance over here.",
                    "label": 0
                },
                {
                    "sent": "Best slightly outperforms your argument render naive based.",
                    "label": 0
                },
                {
                    "sent": "However it is not static, is significant.",
                    "label": 0
                },
                {
                    "sent": "However, Bam is more efficient than you are too in training.",
                    "label": 1
                },
                {
                    "sent": "Here's the final value.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean between Bannon, boosting our new base even though they only differ by 1% average average classification error about our performance, naive Bayes significantly in seven datasets were not boosting nine base are performing spend in two band was especially useful in datasets where features are highly correlated.",
                    "label": 0
                },
                {
                    "sent": "For example, corral with difference between 2 to 5% and we did observe overfitting.",
                    "label": 0
                },
                {
                    "sent": "For bandwidth comparing to boost my base in two datasets we would like to mention that.",
                    "label": 0
                },
                {
                    "sent": "In some cases, band actually picked boost Naive Bayes as the optimal structure.",
                    "label": 0
                },
                {
                    "sent": "For example in Iris and no phone.",
                    "label": 0
                },
                {
                    "sent": "So and to compare the average classification error between the datasets about our performance boost night based 16 two 6.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion We showed that ensemble of sparse model can serve as alternative to search for more densely connected graph.",
                    "label": 1
                },
                {
                    "sent": "Therefore it's very efficient to implement and for training and also it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It's very simple to implement and has compared to the classification accuracy as most standard Bayesian network classifiers as well as the more complex recently proposed Bayesian network classifiers.",
                    "label": 0
                },
                {
                    "sent": "In the future we would like.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pipe Band to handle sequential data classification and we also would like to analyze.",
                    "label": 1
                },
                {
                    "sent": "To analyze the relationship between the final dense structure with the ensemble of the more sparse model and also can we use band to generative models as well instead of constant instead of classifiers.",
                    "label": 1
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "How does it work on noisy data?",
                    "label": 0
                },
                {
                    "sent": "Work on noise data so we.",
                    "label": 0
                },
                {
                    "sent": "Right, so the simulated.",
                    "label": 0
                },
                {
                    "sent": "So in the simulated datasets where we generate 4000 samples from the chain structure graph and sometimes the change is actually quite long, so there is the sparsity of data in few cases.",
                    "label": 0
                },
                {
                    "sent": "That's why when you go back.",
                    "label": 0
                },
                {
                    "sent": "That's why when you go back to the graph, you'll see oh one or two points that's actually below the diagonal bar.",
                    "label": 0
                },
                {
                    "sent": "However, it's not statistically significant, so in some cases it's supposed to.",
                    "label": 0
                },
                {
                    "sent": "The algorithm actually overfits or comparing boosting value base.",
                    "label": 0
                },
                {
                    "sent": "So what about boosting?",
                    "label": 0
                },
                {
                    "sent": "The is actually not much slower than pen, and it's more accurate, so I would guess that if we boost the insensitive thank, you may get better results.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is why?",
                    "label": 0
                },
                {
                    "sent": "Why can't we build ensemble of BMC model?",
                    "label": 0
                },
                {
                    "sent": "So BMC 2P AP&C model learn from BMC 2P algorithm.",
                    "label": 0
                },
                {
                    "sent": "P MC2P model is actually a very good model by itself.",
                    "label": 0
                },
                {
                    "sent": "It has as we continue expanding the prior work, it has very good classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "However, to find that particular model involves structure search over hundreds and thousands of our offer classify evaluation and we think there are method is much simpler and more efficient to train.",
                    "label": 0
                },
                {
                    "sent": "But since BNC model is already quite accurate and the way we did not try to experiment.",
                    "label": 0
                },
                {
                    "sent": "But we are the the cases you may not further may not actually further improve its classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "But we would like to do some experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, let's take this.",
                    "label": 0
                }
            ]
        }
    }
}