{
    "id": "5vn6tokqwyo33r7dggcopgaonu5osaga",
    "title": "Document Embedding Models on Environmental Legal Documents",
    "info": {
        "presenter": [
            "\u017diva Urban\u010di\u010d, Institut \"Jo\u017eef Stefan\""
        ],
        "published": "Nov. 14, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2019_urbancic_environmental_legal_documents/",
    "segmentation": [
        [
            "So my name is Ivan Qichen, together with some accrual Eric Novak incoming kinda.",
            "I've been working on document embedding models on environmental legal data set.",
            "This was done in scope of the in Violence Project.",
            "And the aim was to.",
            "See different ways how we can embed documents and how this preforms."
        ],
        [
            "OK, so this is this is a brief overview of my talk.",
            "First I will represent I will introduce the data we've been working with, so the sources of it, how we collected it and submitted data.",
            "It is attributed with.",
            "I will continue with describing the methods we used and and with some preliminary results an attempts of evaluating which model performs better."
        ],
        [
            "So some of this has been said by seated before me.",
            "We've been working with two main sources of data.",
            "One is equal X, which I believe hasn't been mentioned by the ship.",
            "This is the this is an online service for online information service led by Food and Agriculture Organization of the United Nations.",
            "The International Union for Conservation of Nature and United Nations environment.",
            "Program and the second one is early data set which has been mentioned by Bashir.",
            "Dan is the data set of entire European law and documents have been collected using dedicated web crawlers."
        ],
        [
            "So some characteristics of the documents we collected, 220,000 documents from equal X data set and 800,000 documents from your links.",
            "Now because a Rolex contains entire European, Lawson filtering had to be done.",
            "Since we are only interested in environmental data, so we only took documents which have.",
            "In their meta data, environmental works an also.",
            "We only took documents that are available in English, German and Slovene language.",
            "So some meta data.",
            "Those documents are attributed with our title authors certain important dates as dates of Publishment dates of the document coming into force and their two specific datasets, specific metadata types, keywords and descriptors, and even though they serve the same purpose which is.",
            "Describing the document, they are not completely the same, so it's not 1 to one correspondence.",
            "Certain keywords for equals data set do not appear in the descriptors corpus and vice versa.",
            "And also some really similar documents that are appearing.",
            "Both datasets are attributed with different with different keywords and descriptors."
        ],
        [
            "So the objective of this part of our project is to find a model which will enable us to search through our database and find the most similar documents.",
            "We decided to use word embeddings and later on document embeddings to do that.",
            "Now there has been a lot of things said about word embeddings today already, so I will be short.",
            "Maybe the most important thing is that they are.",
            "Able to capture what in what kinds different words are similar and this is what we are counting on in.",
            "In the further steps.",
            "I think this will be."
        ],
        [
            "Yeah, we chose to test two different word embedding models.",
            "We focused on fastex models because they have certain.",
            "Advantages student for example.",
            "They are available in different languages.",
            "And vectors in different languages are aligned with each other.",
            "But we also decided to train knew Fastex model on our data which resulted in a library that was much smaller and fitted to the vocabulary of our documents.",
            "So instead of 2.5 million tokens in the pre trained fastex model, we ended up with a vocabulary vocabulary of half a million tokens.",
            "An once."
        ],
        [
            "We had word embeddings we wanted to generate document embeddings from them, an one of the most simple methods is to take a set of words that appear in a document and then average their embeddings.",
            "Now there are different approaches that we decided to test.",
            "One is that we take for the set W. We take all the words that appear in the document.",
            "Another one is to use the keywords or descriptors of the document and the last one was a combination of the two approaches."
        ],
        [
            "So this is a planar projection of document embeddings for the first 15,000 English documents in air leaks, as we see, there are some clusters forming.",
            "Now this is 2D projections so.",
            "This might not be the case in the overall space, but we assume that those documents are close, so they are similar."
        ],
        [
            "Now, because we use different models, it is very difficult to.",
            "Assess their performance.",
            "Basically, the final metric that should be used is the user, so the user is by its actions on the server it's supposed to tell us.",
            "So if I search for this query an I get these documents like which of these documents is similar in a way that I want it to be similar.",
            "I will talk about this a bit later as well so that you see what I mean by that.",
            "But the so there is no metric right now that we could use to assess whether one model performs better than the other.",
            "But what we can do is.",
            "To choose an arbitrary source document, reform K nearest neighbor search and then manually evaluating the result.",
            "Of course, this you don't want to do this in the end, but this is kind of to get a few of approaches we want to take in the future."
        ],
        [
            "So this is an example of a result where the source document was an agreement on fisheries between the European Economic Community and the Government of Canada.",
            "The title is a bit longer, but it didn't seem.",
            "Necessary to put the whole thing on the slide.",
            "As you can see, there are the titles of the nearest documents found by our search are quite similar.",
            "And the thing I was talking about before about the metric needs to be the user.",
            "So if the user is searching for agreement on Fisheries Canada.",
            "They will not care about agreement or fisheries in Estonia, so this kind of things need to be improved."
        ],
        [
            "Hey I would like to conclude with some observations that we were able to.",
            "Make even with the.",
            "Handwaving approach that we had so even though the pre trained model performs better in general so the pre trained fasttext model, the one we trained or on our data performs better when the documents are the source document is.",
            "Different from the other documents in the database.",
            "Um, another thing which is a common problem with natural language processing.",
            "The length of the documents is important, so when we use all the words in the document, we will find that documents that have similar lengths will end up similar.",
            "This is not as important when we only use documents, keywords, or descriptors since the size of the set W varies less.",
            "And documents with similar legal expressions and structures have closer embeddings.",
            "With this I mean that for example, when you use, when you use the search query of Commission decision or something like that, it is very likely that you will get as a result Commission decision on something not necessarily connected to what you want.",
            "Since the language the legal language will be similar in the document, this is a thing we will do with with.",
            "Probably expanding stopwords list.",
            "MSU"
        ],
        [
            "In future work, so as I said, the objective is to provide the.",
            "Kind of a search service for legal experts, so we want to develop a service for searching through our database.",
            "We want to provide support for language other than just English and we also want to evaluate and improve our model using the users feedback and I believe this is that yes, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my name is Ivan Qichen, together with some accrual Eric Novak incoming kinda.",
                    "label": 0
                },
                {
                    "sent": "I've been working on document embedding models on environmental legal data set.",
                    "label": 1
                },
                {
                    "sent": "This was done in scope of the in Violence Project.",
                    "label": 0
                },
                {
                    "sent": "And the aim was to.",
                    "label": 0
                },
                {
                    "sent": "See different ways how we can embed documents and how this preforms.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is this is a brief overview of my talk.",
                    "label": 1
                },
                {
                    "sent": "First I will represent I will introduce the data we've been working with, so the sources of it, how we collected it and submitted data.",
                    "label": 0
                },
                {
                    "sent": "It is attributed with.",
                    "label": 1
                },
                {
                    "sent": "I will continue with describing the methods we used and and with some preliminary results an attempts of evaluating which model performs better.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some of this has been said by seated before me.",
                    "label": 0
                },
                {
                    "sent": "We've been working with two main sources of data.",
                    "label": 0
                },
                {
                    "sent": "One is equal X, which I believe hasn't been mentioned by the ship.",
                    "label": 0
                },
                {
                    "sent": "This is the this is an online service for online information service led by Food and Agriculture Organization of the United Nations.",
                    "label": 0
                },
                {
                    "sent": "The International Union for Conservation of Nature and United Nations environment.",
                    "label": 1
                },
                {
                    "sent": "Program and the second one is early data set which has been mentioned by Bashir.",
                    "label": 0
                },
                {
                    "sent": "Dan is the data set of entire European law and documents have been collected using dedicated web crawlers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some characteristics of the documents we collected, 220,000 documents from equal X data set and 800,000 documents from your links.",
                    "label": 0
                },
                {
                    "sent": "Now because a Rolex contains entire European, Lawson filtering had to be done.",
                    "label": 1
                },
                {
                    "sent": "Since we are only interested in environmental data, so we only took documents which have.",
                    "label": 0
                },
                {
                    "sent": "In their meta data, environmental works an also.",
                    "label": 0
                },
                {
                    "sent": "We only took documents that are available in English, German and Slovene language.",
                    "label": 0
                },
                {
                    "sent": "So some meta data.",
                    "label": 0
                },
                {
                    "sent": "Those documents are attributed with our title authors certain important dates as dates of Publishment dates of the document coming into force and their two specific datasets, specific metadata types, keywords and descriptors, and even though they serve the same purpose which is.",
                    "label": 1
                },
                {
                    "sent": "Describing the document, they are not completely the same, so it's not 1 to one correspondence.",
                    "label": 0
                },
                {
                    "sent": "Certain keywords for equals data set do not appear in the descriptors corpus and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And also some really similar documents that are appearing.",
                    "label": 0
                },
                {
                    "sent": "Both datasets are attributed with different with different keywords and descriptors.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the objective of this part of our project is to find a model which will enable us to search through our database and find the most similar documents.",
                    "label": 0
                },
                {
                    "sent": "We decided to use word embeddings and later on document embeddings to do that.",
                    "label": 1
                },
                {
                    "sent": "Now there has been a lot of things said about word embeddings today already, so I will be short.",
                    "label": 0
                },
                {
                    "sent": "Maybe the most important thing is that they are.",
                    "label": 0
                },
                {
                    "sent": "Able to capture what in what kinds different words are similar and this is what we are counting on in.",
                    "label": 0
                },
                {
                    "sent": "In the further steps.",
                    "label": 0
                },
                {
                    "sent": "I think this will be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, we chose to test two different word embedding models.",
                    "label": 1
                },
                {
                    "sent": "We focused on fastex models because they have certain.",
                    "label": 0
                },
                {
                    "sent": "Advantages student for example.",
                    "label": 0
                },
                {
                    "sent": "They are available in different languages.",
                    "label": 0
                },
                {
                    "sent": "And vectors in different languages are aligned with each other.",
                    "label": 0
                },
                {
                    "sent": "But we also decided to train knew Fastex model on our data which resulted in a library that was much smaller and fitted to the vocabulary of our documents.",
                    "label": 0
                },
                {
                    "sent": "So instead of 2.5 million tokens in the pre trained fastex model, we ended up with a vocabulary vocabulary of half a million tokens.",
                    "label": 0
                },
                {
                    "sent": "An once.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We had word embeddings we wanted to generate document embeddings from them, an one of the most simple methods is to take a set of words that appear in a document and then average their embeddings.",
                    "label": 1
                },
                {
                    "sent": "Now there are different approaches that we decided to test.",
                    "label": 0
                },
                {
                    "sent": "One is that we take for the set W. We take all the words that appear in the document.",
                    "label": 1
                },
                {
                    "sent": "Another one is to use the keywords or descriptors of the document and the last one was a combination of the two approaches.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a planar projection of document embeddings for the first 15,000 English documents in air leaks, as we see, there are some clusters forming.",
                    "label": 1
                },
                {
                    "sent": "Now this is 2D projections so.",
                    "label": 0
                },
                {
                    "sent": "This might not be the case in the overall space, but we assume that those documents are close, so they are similar.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, because we use different models, it is very difficult to.",
                    "label": 0
                },
                {
                    "sent": "Assess their performance.",
                    "label": 0
                },
                {
                    "sent": "Basically, the final metric that should be used is the user, so the user is by its actions on the server it's supposed to tell us.",
                    "label": 0
                },
                {
                    "sent": "So if I search for this query an I get these documents like which of these documents is similar in a way that I want it to be similar.",
                    "label": 0
                },
                {
                    "sent": "I will talk about this a bit later as well so that you see what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "But the so there is no metric right now that we could use to assess whether one model performs better than the other.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is.",
                    "label": 0
                },
                {
                    "sent": "To choose an arbitrary source document, reform K nearest neighbor search and then manually evaluating the result.",
                    "label": 1
                },
                {
                    "sent": "Of course, this you don't want to do this in the end, but this is kind of to get a few of approaches we want to take in the future.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an example of a result where the source document was an agreement on fisheries between the European Economic Community and the Government of Canada.",
                    "label": 1
                },
                {
                    "sent": "The title is a bit longer, but it didn't seem.",
                    "label": 0
                },
                {
                    "sent": "Necessary to put the whole thing on the slide.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there are the titles of the nearest documents found by our search are quite similar.",
                    "label": 0
                },
                {
                    "sent": "And the thing I was talking about before about the metric needs to be the user.",
                    "label": 0
                },
                {
                    "sent": "So if the user is searching for agreement on Fisheries Canada.",
                    "label": 0
                },
                {
                    "sent": "They will not care about agreement or fisheries in Estonia, so this kind of things need to be improved.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey I would like to conclude with some observations that we were able to.",
                    "label": 0
                },
                {
                    "sent": "Make even with the.",
                    "label": 0
                },
                {
                    "sent": "Handwaving approach that we had so even though the pre trained model performs better in general so the pre trained fasttext model, the one we trained or on our data performs better when the documents are the source document is.",
                    "label": 1
                },
                {
                    "sent": "Different from the other documents in the database.",
                    "label": 1
                },
                {
                    "sent": "Um, another thing which is a common problem with natural language processing.",
                    "label": 0
                },
                {
                    "sent": "The length of the documents is important, so when we use all the words in the document, we will find that documents that have similar lengths will end up similar.",
                    "label": 1
                },
                {
                    "sent": "This is not as important when we only use documents, keywords, or descriptors since the size of the set W varies less.",
                    "label": 1
                },
                {
                    "sent": "And documents with similar legal expressions and structures have closer embeddings.",
                    "label": 0
                },
                {
                    "sent": "With this I mean that for example, when you use, when you use the search query of Commission decision or something like that, it is very likely that you will get as a result Commission decision on something not necessarily connected to what you want.",
                    "label": 0
                },
                {
                    "sent": "Since the language the legal language will be similar in the document, this is a thing we will do with with.",
                    "label": 0
                },
                {
                    "sent": "Probably expanding stopwords list.",
                    "label": 0
                },
                {
                    "sent": "MSU",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In future work, so as I said, the objective is to provide the.",
                    "label": 0
                },
                {
                    "sent": "Kind of a search service for legal experts, so we want to develop a service for searching through our database.",
                    "label": 1
                },
                {
                    "sent": "We want to provide support for language other than just English and we also want to evaluate and improve our model using the users feedback and I believe this is that yes, thank you.",
                    "label": 1
                }
            ]
        }
    }
}