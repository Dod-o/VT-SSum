{
    "id": "ab2jkxqievjtn6vgd6jvdxafgxcqgxia",
    "title": "Basics of probability and statistics",
    "info": {
        "author": [
            "Mikaela Keller, IDIAP Research Institute"
        ],
        "published": "July 2, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_keller_bss/",
    "segmentation": [
        [
            "Deborah.",
            "I hope what I'm going to present it's not so obvious for you.",
            "I'm trying to put it in a way related to what are the machine learning needs more than a really theoretical in algebra course.",
            "I don't know you.",
            "I always had really theoretical in algebra courses and.",
            "What is needed in machine learning?",
            "It's few but you have to have more intuition."
        ],
        [
            "Or something like that.",
            "So to see a little bit, and for those who are not familiar with machine learning, will first go through.",
            "I'm going to motivate this need of of."
        ],
        [
            "Mathematical tool."
        ],
        [
            "And for that I'm going to present what are the different kinds of problems we tried to solve in machine learning.",
            "So first kind of problem is a regression problem.",
            "Which as a let's give a concrete example.",
            "Let's say that it's extremely important to know the age of abalones.",
            "This kind of shells for doing that.",
            "Usually what you do is you have to cut the shell and then go look through microscope and count the rings because it's ring, it's one one year.",
            "Of course, that's it's a really time consuming and boring task.",
            "But we know that we can imagine that there is a correlation between the age of the shell an his diameter, his weight, and so on.",
            "So which are measurement that are easy to obtain?",
            "Usually what we have in machine learning is that we have a combination of this easy measurement.",
            "And the the true.",
            "The difficult measurement for a set of examples, which are the training example and what we want is to have a model where we have the easy measurement and the target.",
            "Variable we want to to estimate and we have only these few points and we want to have to compute.",
            "Model which will give us for all the different measurements."
        ],
        [
            "OK.",
            "So another kind of example is a classification example.",
            "Which, for example is used in automatic recognition of Postal code scan for Mail.",
            "So imagine and that's something that is using for real, is that?",
            "You want to sort automatically Mail incoming Mail wear and the Postal oops.",
            "The Postal code is scan and.",
            "I want the computer to know which direction it has to sort Mail.",
            "From the numbers, so there's a.",
            "There's a variability in the way people.",
            "Write the numbers and so that's a source of difficulty for the computer to two.",
            "So that that's our statistical problem.",
            "It's in there, and so again we have.",
            "A set of training example with within the.",
            "The image scan and the true digit and we want to predict the correct digit for any new image.",
            "And as you can see, it's again a problem of finding a curve or separation related to the distance between the object."
        ],
        [
            "OK.",
            "So kind of problem is density estimation of clustering, which.",
            "What would the name of of this density estimation is like a?",
            "Previous step before the regulation of classify or classification problem usually often.",
            "In which we want to organize the data in some way for applying then other kind of algorithms.",
            "So here I have an example of a.",
            "It's a really traditional example of a geyser who is really popular because he shot really often an.",
            "We want to understand if there correlation between the time the eruptions last and the time between 2 two eruptions Ann.",
            "From doing density estimation, we see that there is kind of 2 two patterns.",
            "So if we here we have I think.",
            "Time between two eruptions and here.",
            "Note here is how long the eruption last and curious the time between two eruptions.",
            "So apparently we see that there's two kinds of patterns in here which I don't know can be used in for model model Ising this."
        ],
        [
            "Duh.",
            "OK, So what I want to show by this example is that most problems in machine learning that we want to I have shown end up reformulating into curve or surface to be discovered.",
            "And.",
            "That's it's usually model is a system of equations with unknowns to be solved, which is really easily done with mattress matrix manipulation, so that's that's what is linear algebra useful in machine learning.",
            "The second thing that comes out from this example is that there's diverse sources of uncertainty.",
            "One comes from the fact that we have a limited amount of data.",
            "And others can come from the the noise in the measurements and also of course of the randomness inherent to the observed phenomena.",
            "But what is really can be something randomly happening in there.",
            "So that's the second which I'm going to talk about tomorrow, which is probability theory.",
            "We will again really.",
            "Basic stuff introduction to two, probably."
        ],
        [
            "Eating.",
            "OK, let's go to linear algebra.",
            "So, as has been already discussed, today there is different kind of data vector.",
            "Most often, or at least what have been done most in machine learning, it's data are."
        ],
        [
            "Resented as victors so vectors, which EM features.",
            "Of course, I think I didn't take the same convention of Isabelle because we didn't so for me I have.",
            "Uh and vectors of end components, sorry.",
            "And, um.",
            "So.",
            "The inner product or which is in some condition in some cases called dot product or scalar product, some more or less the same.",
            "Is defined as that.",
            "So line time column.",
            "That's important then for matrix multiplication."
        ],
        [
            "To know that.",
            "Two vectors are safe to be orthogonal if there is color.",
            "Product is 0.",
            "And how is calling was saying just a few minutes ago?",
            "It says often interesting to have a metric, so in a usual expedient space like where matrix and vectors like most often it's we have a norm which is the square root of the scalar product of the vector by itself.",
            "And then usually the distance is defined with this norm as the norm of the difference of two vectors.",
            "And as you can see here in the definition of the distance, in fact there's the scalar product of the two vectors, which is included an, that's why.",
            "So we can see this scale produces a similarity or or that's why it's discovered produce comes all the time in machine learning because it is related to the distance between the objects in the plane or in the in the spacing which will the components."
        ],
        [
            "Face.",
            "Yep, so.",
            "I'd say that most problems can be ends up as a as an equation with them and owns.",
            "Which can be written like this.",
            "With that being the matrix N * M. Matrix multiply by the vector.",
            "And equals another."
        ],
        [
            "Victor.",
            "Ticket.",
            "So that's the 1st way of seeing the matrices is like.",
            "I take a row of matrix.",
            "I multiply it by the vector an.",
            "I obtained the first element of this vector and so on."
        ],
        [
            "So."
        ],
        [
            "Oh yeah.",
            "Let's have a very easy example.",
            "So let's imagine we have this.",
            "Set of equation with the.",
            "This first line defines in fact.",
            "The equation of."
        ],
        [
            "Line.",
            "If I win two points."
        ],
        [
            "Serving for two points, I have the line."
        ],
        [
            "Uh, this."
        ],
        [
            "Aim for the second one and solving an equation is in fact finding the point where the two points intersect."
        ],
        [
            "2 lines intersect.",
            "Another way of seeing the equation, the equation with Emma known is again, so we have this mattress kind of seeing it, but this time we're going to pay attention to the column of the matrix."
        ],
        [
            "Of the lines.",
            "So.",
            "Uh, an equation with emanon can be seen.",
            "In fact, as a linear combination.",
            "Of the column.",
            "Of of the Matrix is so.",
            "We're trying to find the vector the the coefficient here, which when I combine linearly the the column of my mattress give me this vector.",
            "And it's also a way of seeing that the matrices it's a linear transformation from the space Mer.",
            "Into air and the space where my vectors of the column define mattress."
        ],
        [
            "I.",
            "So easy example to show.",
            "This time looking this Jimmy."
        ],
        [
            "Actually this is I have a first vector."
        ],
        [
            "This vector.",
            "A second Victor."
        ],
        [
            "And I want to see which are the values of X&Y, which will give me this vector.",
            "So is also the projection of this vector on these two vectors.",
            "Are these value X1 and X2?",
            "And as you can see here any imagine I don't have this part.",
            "In fact, with these two vectors, which this one and this one in fact, I can reconstruct the whole space.",
            "So that's the property of two vectors being."
        ],
        [
            "In early independent, because in fact, let's say I change the second part of my equation.",
            "And I have another column vector which is in fact Colin are to the to the first one an.",
            "I I cannot this vector, I cannot construct.",
            "The only one I can is that the ones that are on this line.",
            "Which is seeing if we looked under in the row view the role of the matrix view.",
            "It looks like these two.",
            "Vectors are parallel.",
            "These two the two lines, so they never intersect, so there is no solution to to this equation.",
            "Um?",
            "To say something else."
        ],
        [
            "OK.",
            "So of course this it's easy to see that the fact that vectors are collinear or not in two dimension.",
            "But what happened when we are in 100 and made dimension?",
            "It's not, it's not possible anymore, so that's why determinants are are interesting.",
            "So they are defined for square matrices and they are defined recursively.",
            "As that's an easy definition there, there are more general ones, but this one is an easy one.",
            "Where the determinant is seen as a sum plus or minus one, the D element of the first line times the minor of the matrices of the matrix with this minor is.",
            "Matrix without line and without rock without the role without the column.",
            "For example, here for a three by three matches you see here we have first element.",
            "Then this minor second element and the matrix formed by this vector on this one.",
            "And so we're elements, and so on and so on.",
            "So what's interesting about determinants is I'm it's related to inverse."
        ],
        [
            "This is so I go to inverse matrices.",
            "So inverse matrices are defined so.",
            "A square matrix A is called nonsingular or even invertible if there exists a matrix P such that the product of these two metrics give the identity.",
            "And if this be exists, so the inverse of our is not.",
            "I mean this one and it's big.",
            "So what is interesting is that if the determinant of my matrix is non zero, then I know that my matrix is invertible.",
            "Which give me a.",
            "If I have A and an equation with an unknown's give me the solution of my equation by inverting the matrix so if I have a 100 by 100 matrix equation problem, I take the matrix.",
            "I look if his determinant is non zero, an if it's non zero.",
            "I directly have the here the solution of my equation."
        ],
        [
            "So let's let's go back to the to the geometrical view.",
            "In which the determinant in fact it's.",
            "At the surface formed by the two column vectors of my metrics in a two dimension.",
            "Of course in three dimension it's more complicated and."
        ],
        [
            "So so yeah, and it's easy verified if you if you go to polar polar code coordinates.",
            "Anne, what you see is if if my two vectors are collinear, as I saw before, the determinant would be 0 because the surface is a completely flat and.",
            "Just to verify that what we were saying before."
        ],
        [
            "Held for determinants.",
            "OK, So what happened when that was when Matrix A is square?",
            "It's easy we take the inverse, but what happened when it is rectangular?",
            "We take the pseudoinverse just called sort of inverse.",
            "Anne.",
            "Yeah, so that's help for any particular metrics.",
            "Providing that 80 times A is invertible, of course.",
            "Oh talking about rectangular Matriz, there is one which is really interesting, which is about already talk about today, which is the matrix where I take where each line is one of my examples.",
            "Anne.",
            "The the the as I say before the scalar product is something which are really interested in because it is related to the distance between our vectors.",
            "So one matrix which is also of interest Instagram matrix where each component is the component, IJ is the scalar product of the vector of the example AI times the.",
            "So the the the example J.",
            "An also something which you always read in books, which is the.",
            "The Matrix M considering must be positive semidefinite, which means in fact that.",
            "The billionaire form, which when I'm I'm taking a vector non zero vector an I multiply it.",
            "Left side and right side.",
            "By my metrics it must be positive or 0.",
            "And gram matrices are always positive semidefinite, so that's one one thing you always.",
            "There's a lot of theorem that held if mattress is positive semidefinite and we we want often in machine learning to apply it to gram matrices.",
            "Which hopefully are positive semi definite so.",
            "No.",
            "You don't need to see to think about it.",
            "Um?",
            "OK, so for example this matrix we sometimes want to to raise it to the power to see what would be the distance or something related to the distance, not between two vectors, but between this vector and the third one passing through the middle one.",
            "So the distance the path to go from one to the other.",
            "Entry in three times.",
            "So it's interesting to be able to raise it to the to the."
        ],
        [
            "To power.",
            "And for that metric, sterilization is really is really something you want to do.",
            "So.",
            "Eigenvector of a matrix A is a solution non zero solution of this equation.",
            "Which cannot see can also be seen as a.",
            "In fact, I want to find the invariants, kind of the invariants of my application A.",
            "So the if I look at the matrix as a as a linear transformation, what happened to these eigenvectors?",
            "They in fact are just stretch stretch but not don't change direction.",
            "Their honest stretch, so it's.",
            "Close to invariants idea.",
            "So of course this is equivalent to this, with I being the identity matrix.",
            "An eigenvector has an associated eigenvalue, so for solving that you first go to the through the characteristic polynomial an solve this.",
            "Which is a yeah, it's a polynomial and you find first which are the eigenvalues and then you replace the eigenvalue in here to find which is the corresponding eigenvector.",
            "So.",
            "When do the matrix A again, we're talking about square matrices.",
            "This mattress that generalization is for square matrices.",
            "When the matrix is real valued and symmetric, we know that our.",
            "Eigenvalues are also real value.",
            "And we know also that we can find.",
            "Basis of eigenvector which are all or that which are orthogonal one to each other.",
            "And we can force them to have a norm one, which is an interesting thing that you in fact can.",
            "You know that you can change.",
            "You can have a change of basis which will give you a only being diagonal matrix with the eigen values on the on the diagonal.",
            "Under yeah, so it can be written like that.",
            "Which.",
            "Which has a.",
            "Really interesting properties, since if I write.",
            "A. Pete, let's call it D. This diagonal matrix and I want to raise it to the power.",
            "For example, what does that mean?",
            "Let's say two to be to begin with.",
            "That would be so.",
            "Two times this matrix.",
            "And because.",
            "P It's a it's constructed with the eigenvectors which are all orthogonal between them.",
            "An with base one.",
            "This in fact end up to be identity.",
            "And this.",
            "This is only P the square.",
            "Be cheap.",
            "So for that generalize of course to end because you is just anytime the same thing and.",
            "So it's really easy to compute the.",
            "The power of a matrix that you have that they're gonna lies.",
            "Because the diagonal to the power to.",
            "Wait, Yep, Goren is only you.",
            "Take the diagonal elements and raise it to the power an.",
            "Um?",
            "And other things.",
            "Way of seeing that also which is interesting is.",
            "That you.",
            "You can in fact look at the.",
            "The transform the linear transformation of a vector X as in fact.",
            "Also.",
            "Something like that, which in fact will give you something like this.",
            "Um?",
            "So as a.",
            "That's what's called spectral decomposition.",
            "In which in fact you will have.",
            "Which you will have so you look at this linear transformation in terms of decomposition in the eigenvectors.",
            "And how to give you a intuition of that, let's say.",
            "I had a really easy emetrics which is this one.",
            "OK, so that will give me as eigenvector.",
            "Um?",
            "Did the.",
            "He won't.",
            "Which are the normal in the?",
            "Are the normal base do not do the usual base we take in which we decompose.",
            "Yeah, then the usual base you know about it?",
            "Anne.",
            "Let's say I have the 111.",
            "Victor.",
            "With this transformation, with this information, what happened?",
            "I can't, it's only.",
            "This don't move.",
            "With this, the 1st two dimension will not move, so I will keep exactly the same and the second one will be increased by two.",
            "So.",
            "Knowing that this decomposition.",
            "We will told me that on this on this plane things are not going to be stretched and they will keep their shape.",
            "Well, in this there going to be increased by two and for example if I had one of these which is a negative.",
            "So you see what will happen?",
            "This is going to be completely reverse, so I will have a rotation or.",
            "So knowing knowing the shape.",
            "The what's your eigenvalue looks like?",
            "Give you an idea of what the translator transformation is doing.",
            "Yeah, so having.",
            "A positive positive semidefinite matrix such as the gram matrix.",
            "Will give me that all my my eigenvalues are positive or or 0.",
            "Which is 0 is also another case in which.",
            "I don't have a if I know.",
            "One of my eigenvalues is 0.",
            "Then I will.",
            "I know that I don't have objection but an injection some some.",
            "Something disappear, can I do that?",
            "Yeah, I'm not sure I do.",
            "I let that for the.",
            "Practical part of the exercise."
        ],
        [
            "So that's the nice metrics generalization was used for square matrices, but we have again a generalization for rectangular matrix which is called the singular value decomposition.",
            "Anne Anne.",
            "Any any real value metrics can be rewritten like that with the this being the.",
            "The singular of the.",
            "Right, the left singular vectors.",
            "And here the.",
            "Right singular vectors.",
            "In line or an in column an in the middle.",
            "Something which is close to a diagonal matrix even if its mtime M so, but which has only element in the in the diagonal an.",
            "If you look at this equation.",
            "In fact, you can find it.",
            "You can find a relation with the.",
            "The gram matrix we we saw before because.",
            "And with the matrix organization.",
            "Of course.",
            "And if I take.",
            "Oh MTM.",
            "I will have, so the transpose is this which is.",
            "Up and what will I have in the end?",
            "I will have we so again, this matrices are orthogonal matrices, so when you make.",
            "One year identity matrix.",
            "So here.",
            "This end up being.",
            "And.",
            "You see, that's that's only a diagonalization, or like we had before much examination.",
            "Oops, sorry.",
            "And this Instagram matrix in the.",
            "In the witch space so.",
            "This one was.",
            "N * M. So this one is going to be.",
            "And things M. So it's a.",
            "It's the gram matrix of the variables.",
            "If I look at the variables.",
            "As examples.",
            "Define color for now.",
            "Square meter.",
            "You're right.",
            "So it's not.",
            "Thank you.",
            "You're right, it's not square.",
            "Yeah, yeah.",
            "Yeah, you're right.",
            "But if we look at this matrix as, this is a square matrix, so we have a matrix organization.",
            "OK, I I stop here.",
            "So you're in time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deborah.",
                    "label": 0
                },
                {
                    "sent": "I hope what I'm going to present it's not so obvious for you.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to put it in a way related to what are the machine learning needs more than a really theoretical in algebra course.",
                    "label": 0
                },
                {
                    "sent": "I don't know you.",
                    "label": 0
                },
                {
                    "sent": "I always had really theoretical in algebra courses and.",
                    "label": 0
                },
                {
                    "sent": "What is needed in machine learning?",
                    "label": 1
                },
                {
                    "sent": "It's few but you have to have more intuition.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "So to see a little bit, and for those who are not familiar with machine learning, will first go through.",
                    "label": 0
                },
                {
                    "sent": "I'm going to motivate this need of of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mathematical tool.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for that I'm going to present what are the different kinds of problems we tried to solve in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So first kind of problem is a regression problem.",
                    "label": 1
                },
                {
                    "sent": "Which as a let's give a concrete example.",
                    "label": 1
                },
                {
                    "sent": "Let's say that it's extremely important to know the age of abalones.",
                    "label": 0
                },
                {
                    "sent": "This kind of shells for doing that.",
                    "label": 0
                },
                {
                    "sent": "Usually what you do is you have to cut the shell and then go look through microscope and count the rings because it's ring, it's one one year.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's it's a really time consuming and boring task.",
                    "label": 0
                },
                {
                    "sent": "But we know that we can imagine that there is a correlation between the age of the shell an his diameter, his weight, and so on.",
                    "label": 1
                },
                {
                    "sent": "So which are measurement that are easy to obtain?",
                    "label": 0
                },
                {
                    "sent": "Usually what we have in machine learning is that we have a combination of this easy measurement.",
                    "label": 0
                },
                {
                    "sent": "And the the true.",
                    "label": 1
                },
                {
                    "sent": "The difficult measurement for a set of examples, which are the training example and what we want is to have a model where we have the easy measurement and the target.",
                    "label": 0
                },
                {
                    "sent": "Variable we want to to estimate and we have only these few points and we want to have to compute.",
                    "label": 0
                },
                {
                    "sent": "Model which will give us for all the different measurements.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So another kind of example is a classification example.",
                    "label": 0
                },
                {
                    "sent": "Which, for example is used in automatic recognition of Postal code scan for Mail.",
                    "label": 1
                },
                {
                    "sent": "So imagine and that's something that is using for real, is that?",
                    "label": 0
                },
                {
                    "sent": "You want to sort automatically Mail incoming Mail wear and the Postal oops.",
                    "label": 0
                },
                {
                    "sent": "The Postal code is scan and.",
                    "label": 0
                },
                {
                    "sent": "I want the computer to know which direction it has to sort Mail.",
                    "label": 0
                },
                {
                    "sent": "From the numbers, so there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a variability in the way people.",
                    "label": 0
                },
                {
                    "sent": "Write the numbers and so that's a source of difficulty for the computer to two.",
                    "label": 0
                },
                {
                    "sent": "So that that's our statistical problem.",
                    "label": 0
                },
                {
                    "sent": "It's in there, and so again we have.",
                    "label": 0
                },
                {
                    "sent": "A set of training example with within the.",
                    "label": 0
                },
                {
                    "sent": "The image scan and the true digit and we want to predict the correct digit for any new image.",
                    "label": 1
                },
                {
                    "sent": "And as you can see, it's again a problem of finding a curve or separation related to the distance between the object.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So kind of problem is density estimation of clustering, which.",
                    "label": 0
                },
                {
                    "sent": "What would the name of of this density estimation is like a?",
                    "label": 0
                },
                {
                    "sent": "Previous step before the regulation of classify or classification problem usually often.",
                    "label": 0
                },
                {
                    "sent": "In which we want to organize the data in some way for applying then other kind of algorithms.",
                    "label": 1
                },
                {
                    "sent": "So here I have an example of a.",
                    "label": 0
                },
                {
                    "sent": "It's a really traditional example of a geyser who is really popular because he shot really often an.",
                    "label": 0
                },
                {
                    "sent": "We want to understand if there correlation between the time the eruptions last and the time between 2 two eruptions Ann.",
                    "label": 0
                },
                {
                    "sent": "From doing density estimation, we see that there is kind of 2 two patterns.",
                    "label": 0
                },
                {
                    "sent": "So if we here we have I think.",
                    "label": 1
                },
                {
                    "sent": "Time between two eruptions and here.",
                    "label": 0
                },
                {
                    "sent": "Note here is how long the eruption last and curious the time between two eruptions.",
                    "label": 0
                },
                {
                    "sent": "So apparently we see that there's two kinds of patterns in here which I don't know can be used in for model model Ising this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Duh.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want to show by this example is that most problems in machine learning that we want to I have shown end up reformulating into curve or surface to be discovered.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's it's usually model is a system of equations with unknowns to be solved, which is really easily done with mattress matrix manipulation, so that's that's what is linear algebra useful in machine learning.",
                    "label": 1
                },
                {
                    "sent": "The second thing that comes out from this example is that there's diverse sources of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "One comes from the fact that we have a limited amount of data.",
                    "label": 0
                },
                {
                    "sent": "And others can come from the the noise in the measurements and also of course of the randomness inherent to the observed phenomena.",
                    "label": 1
                },
                {
                    "sent": "But what is really can be something randomly happening in there.",
                    "label": 0
                },
                {
                    "sent": "So that's the second which I'm going to talk about tomorrow, which is probability theory.",
                    "label": 0
                },
                {
                    "sent": "We will again really.",
                    "label": 0
                },
                {
                    "sent": "Basic stuff introduction to two, probably.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eating.",
                    "label": 0
                },
                {
                    "sent": "OK, let's go to linear algebra.",
                    "label": 1
                },
                {
                    "sent": "So, as has been already discussed, today there is different kind of data vector.",
                    "label": 0
                },
                {
                    "sent": "Most often, or at least what have been done most in machine learning, it's data are.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Resented as victors so vectors, which EM features.",
                    "label": 0
                },
                {
                    "sent": "Of course, I think I didn't take the same convention of Isabelle because we didn't so for me I have.",
                    "label": 0
                },
                {
                    "sent": "Uh and vectors of end components, sorry.",
                    "label": 1
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The inner product or which is in some condition in some cases called dot product or scalar product, some more or less the same.",
                    "label": 1
                },
                {
                    "sent": "Is defined as that.",
                    "label": 0
                },
                {
                    "sent": "So line time column.",
                    "label": 0
                },
                {
                    "sent": "That's important then for matrix multiplication.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To know that.",
                    "label": 0
                },
                {
                    "sent": "Two vectors are safe to be orthogonal if there is color.",
                    "label": 0
                },
                {
                    "sent": "Product is 0.",
                    "label": 0
                },
                {
                    "sent": "And how is calling was saying just a few minutes ago?",
                    "label": 0
                },
                {
                    "sent": "It says often interesting to have a metric, so in a usual expedient space like where matrix and vectors like most often it's we have a norm which is the square root of the scalar product of the vector by itself.",
                    "label": 0
                },
                {
                    "sent": "And then usually the distance is defined with this norm as the norm of the difference of two vectors.",
                    "label": 1
                },
                {
                    "sent": "And as you can see here in the definition of the distance, in fact there's the scalar product of the two vectors, which is included an, that's why.",
                    "label": 0
                },
                {
                    "sent": "So we can see this scale produces a similarity or or that's why it's discovered produce comes all the time in machine learning because it is related to the distance between the objects in the plane or in the in the spacing which will the components.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Face.",
                    "label": 0
                },
                {
                    "sent": "Yep, so.",
                    "label": 0
                },
                {
                    "sent": "I'd say that most problems can be ends up as a as an equation with them and owns.",
                    "label": 0
                },
                {
                    "sent": "Which can be written like this.",
                    "label": 0
                },
                {
                    "sent": "With that being the matrix N * M. Matrix multiply by the vector.",
                    "label": 0
                },
                {
                    "sent": "And equals another.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Victor.",
                    "label": 0
                },
                {
                    "sent": "Ticket.",
                    "label": 0
                },
                {
                    "sent": "So that's the 1st way of seeing the matrices is like.",
                    "label": 0
                },
                {
                    "sent": "I take a row of matrix.",
                    "label": 0
                },
                {
                    "sent": "I multiply it by the vector an.",
                    "label": 0
                },
                {
                    "sent": "I obtained the first element of this vector and so on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Let's have a very easy example.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine we have this.",
                    "label": 0
                },
                {
                    "sent": "Set of equation with the.",
                    "label": 0
                },
                {
                    "sent": "This first line defines in fact.",
                    "label": 0
                },
                {
                    "sent": "The equation of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Line.",
                    "label": 0
                },
                {
                    "sent": "If I win two points.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Serving for two points, I have the line.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aim for the second one and solving an equation is in fact finding the point where the two points intersect.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 lines intersect.",
                    "label": 0
                },
                {
                    "sent": "Another way of seeing the equation, the equation with Emma known is again, so we have this mattress kind of seeing it, but this time we're going to pay attention to the column of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the lines.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Uh, an equation with emanon can be seen.",
                    "label": 0
                },
                {
                    "sent": "In fact, as a linear combination.",
                    "label": 1
                },
                {
                    "sent": "Of the column.",
                    "label": 0
                },
                {
                    "sent": "Of of the Matrix is so.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find the vector the the coefficient here, which when I combine linearly the the column of my mattress give me this vector.",
                    "label": 0
                },
                {
                    "sent": "And it's also a way of seeing that the matrices it's a linear transformation from the space Mer.",
                    "label": 0
                },
                {
                    "sent": "Into air and the space where my vectors of the column define mattress.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So easy example to show.",
                    "label": 0
                },
                {
                    "sent": "This time looking this Jimmy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually this is I have a first vector.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This vector.",
                    "label": 0
                },
                {
                    "sent": "A second Victor.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I want to see which are the values of X&Y, which will give me this vector.",
                    "label": 0
                },
                {
                    "sent": "So is also the projection of this vector on these two vectors.",
                    "label": 0
                },
                {
                    "sent": "Are these value X1 and X2?",
                    "label": 0
                },
                {
                    "sent": "And as you can see here any imagine I don't have this part.",
                    "label": 0
                },
                {
                    "sent": "In fact, with these two vectors, which this one and this one in fact, I can reconstruct the whole space.",
                    "label": 0
                },
                {
                    "sent": "So that's the property of two vectors being.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In early independent, because in fact, let's say I change the second part of my equation.",
                    "label": 0
                },
                {
                    "sent": "And I have another column vector which is in fact Colin are to the to the first one an.",
                    "label": 0
                },
                {
                    "sent": "I I cannot this vector, I cannot construct.",
                    "label": 0
                },
                {
                    "sent": "The only one I can is that the ones that are on this line.",
                    "label": 0
                },
                {
                    "sent": "Which is seeing if we looked under in the row view the role of the matrix view.",
                    "label": 0
                },
                {
                    "sent": "It looks like these two.",
                    "label": 0
                },
                {
                    "sent": "Vectors are parallel.",
                    "label": 0
                },
                {
                    "sent": "These two the two lines, so they never intersect, so there is no solution to to this equation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To say something else.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So of course this it's easy to see that the fact that vectors are collinear or not in two dimension.",
                    "label": 0
                },
                {
                    "sent": "But what happened when we are in 100 and made dimension?",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not possible anymore, so that's why determinants are are interesting.",
                    "label": 0
                },
                {
                    "sent": "So they are defined for square matrices and they are defined recursively.",
                    "label": 0
                },
                {
                    "sent": "As that's an easy definition there, there are more general ones, but this one is an easy one.",
                    "label": 0
                },
                {
                    "sent": "Where the determinant is seen as a sum plus or minus one, the D element of the first line times the minor of the matrices of the matrix with this minor is.",
                    "label": 0
                },
                {
                    "sent": "Matrix without line and without rock without the role without the column.",
                    "label": 0
                },
                {
                    "sent": "For example, here for a three by three matches you see here we have first element.",
                    "label": 0
                },
                {
                    "sent": "Then this minor second element and the matrix formed by this vector on this one.",
                    "label": 0
                },
                {
                    "sent": "And so we're elements, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting about determinants is I'm it's related to inverse.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is so I go to inverse matrices.",
                    "label": 0
                },
                {
                    "sent": "So inverse matrices are defined so.",
                    "label": 0
                },
                {
                    "sent": "A square matrix A is called nonsingular or even invertible if there exists a matrix P such that the product of these two metrics give the identity.",
                    "label": 1
                },
                {
                    "sent": "And if this be exists, so the inverse of our is not.",
                    "label": 0
                },
                {
                    "sent": "I mean this one and it's big.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting is that if the determinant of my matrix is non zero, then I know that my matrix is invertible.",
                    "label": 0
                },
                {
                    "sent": "Which give me a.",
                    "label": 0
                },
                {
                    "sent": "If I have A and an equation with an unknown's give me the solution of my equation by inverting the matrix so if I have a 100 by 100 matrix equation problem, I take the matrix.",
                    "label": 0
                },
                {
                    "sent": "I look if his determinant is non zero, an if it's non zero.",
                    "label": 0
                },
                {
                    "sent": "I directly have the here the solution of my equation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let's go back to the to the geometrical view.",
                    "label": 0
                },
                {
                    "sent": "In which the determinant in fact it's.",
                    "label": 0
                },
                {
                    "sent": "At the surface formed by the two column vectors of my metrics in a two dimension.",
                    "label": 0
                },
                {
                    "sent": "Of course in three dimension it's more complicated and.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so yeah, and it's easy verified if you if you go to polar polar code coordinates.",
                    "label": 0
                },
                {
                    "sent": "Anne, what you see is if if my two vectors are collinear, as I saw before, the determinant would be 0 because the surface is a completely flat and.",
                    "label": 0
                },
                {
                    "sent": "Just to verify that what we were saying before.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Held for determinants.",
                    "label": 0
                },
                {
                    "sent": "OK, So what happened when that was when Matrix A is square?",
                    "label": 1
                },
                {
                    "sent": "It's easy we take the inverse, but what happened when it is rectangular?",
                    "label": 0
                },
                {
                    "sent": "We take the pseudoinverse just called sort of inverse.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's help for any particular metrics.",
                    "label": 0
                },
                {
                    "sent": "Providing that 80 times A is invertible, of course.",
                    "label": 1
                },
                {
                    "sent": "Oh talking about rectangular Matriz, there is one which is really interesting, which is about already talk about today, which is the matrix where I take where each line is one of my examples.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The the the as I say before the scalar product is something which are really interested in because it is related to the distance between our vectors.",
                    "label": 0
                },
                {
                    "sent": "So one matrix which is also of interest Instagram matrix where each component is the component, IJ is the scalar product of the vector of the example AI times the.",
                    "label": 0
                },
                {
                    "sent": "So the the the example J.",
                    "label": 0
                },
                {
                    "sent": "An also something which you always read in books, which is the.",
                    "label": 0
                },
                {
                    "sent": "The Matrix M considering must be positive semidefinite, which means in fact that.",
                    "label": 0
                },
                {
                    "sent": "The billionaire form, which when I'm I'm taking a vector non zero vector an I multiply it.",
                    "label": 0
                },
                {
                    "sent": "Left side and right side.",
                    "label": 0
                },
                {
                    "sent": "By my metrics it must be positive or 0.",
                    "label": 0
                },
                {
                    "sent": "And gram matrices are always positive semidefinite, so that's one one thing you always.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of theorem that held if mattress is positive semidefinite and we we want often in machine learning to apply it to gram matrices.",
                    "label": 0
                },
                {
                    "sent": "Which hopefully are positive semi definite so.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "You don't need to see to think about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so for example this matrix we sometimes want to to raise it to the power to see what would be the distance or something related to the distance, not between two vectors, but between this vector and the third one passing through the middle one.",
                    "label": 0
                },
                {
                    "sent": "So the distance the path to go from one to the other.",
                    "label": 0
                },
                {
                    "sent": "Entry in three times.",
                    "label": 0
                },
                {
                    "sent": "So it's interesting to be able to raise it to the to the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To power.",
                    "label": 0
                },
                {
                    "sent": "And for that metric, sterilization is really is really something you want to do.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Eigenvector of a matrix A is a solution non zero solution of this equation.",
                    "label": 1
                },
                {
                    "sent": "Which cannot see can also be seen as a.",
                    "label": 0
                },
                {
                    "sent": "In fact, I want to find the invariants, kind of the invariants of my application A.",
                    "label": 0
                },
                {
                    "sent": "So the if I look at the matrix as a as a linear transformation, what happened to these eigenvectors?",
                    "label": 0
                },
                {
                    "sent": "They in fact are just stretch stretch but not don't change direction.",
                    "label": 0
                },
                {
                    "sent": "Their honest stretch, so it's.",
                    "label": 0
                },
                {
                    "sent": "Close to invariants idea.",
                    "label": 0
                },
                {
                    "sent": "So of course this is equivalent to this, with I being the identity matrix.",
                    "label": 1
                },
                {
                    "sent": "An eigenvector has an associated eigenvalue, so for solving that you first go to the through the characteristic polynomial an solve this.",
                    "label": 0
                },
                {
                    "sent": "Which is a yeah, it's a polynomial and you find first which are the eigenvalues and then you replace the eigenvalue in here to find which is the corresponding eigenvector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When do the matrix A again, we're talking about square matrices.",
                    "label": 0
                },
                {
                    "sent": "This mattress that generalization is for square matrices.",
                    "label": 0
                },
                {
                    "sent": "When the matrix is real valued and symmetric, we know that our.",
                    "label": 1
                },
                {
                    "sent": "Eigenvalues are also real value.",
                    "label": 0
                },
                {
                    "sent": "And we know also that we can find.",
                    "label": 0
                },
                {
                    "sent": "Basis of eigenvector which are all or that which are orthogonal one to each other.",
                    "label": 0
                },
                {
                    "sent": "And we can force them to have a norm one, which is an interesting thing that you in fact can.",
                    "label": 0
                },
                {
                    "sent": "You know that you can change.",
                    "label": 0
                },
                {
                    "sent": "You can have a change of basis which will give you a only being diagonal matrix with the eigen values on the on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "Under yeah, so it can be written like that.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Which has a.",
                    "label": 0
                },
                {
                    "sent": "Really interesting properties, since if I write.",
                    "label": 0
                },
                {
                    "sent": "A. Pete, let's call it D. This diagonal matrix and I want to raise it to the power.",
                    "label": 0
                },
                {
                    "sent": "For example, what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Let's say two to be to begin with.",
                    "label": 0
                },
                {
                    "sent": "That would be so.",
                    "label": 0
                },
                {
                    "sent": "Two times this matrix.",
                    "label": 0
                },
                {
                    "sent": "And because.",
                    "label": 0
                },
                {
                    "sent": "P It's a it's constructed with the eigenvectors which are all orthogonal between them.",
                    "label": 0
                },
                {
                    "sent": "An with base one.",
                    "label": 0
                },
                {
                    "sent": "This in fact end up to be identity.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "This is only P the square.",
                    "label": 0
                },
                {
                    "sent": "Be cheap.",
                    "label": 0
                },
                {
                    "sent": "So for that generalize of course to end because you is just anytime the same thing and.",
                    "label": 0
                },
                {
                    "sent": "So it's really easy to compute the.",
                    "label": 0
                },
                {
                    "sent": "The power of a matrix that you have that they're gonna lies.",
                    "label": 0
                },
                {
                    "sent": "Because the diagonal to the power to.",
                    "label": 0
                },
                {
                    "sent": "Wait, Yep, Goren is only you.",
                    "label": 0
                },
                {
                    "sent": "Take the diagonal elements and raise it to the power an.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And other things.",
                    "label": 0
                },
                {
                    "sent": "Way of seeing that also which is interesting is.",
                    "label": 0
                },
                {
                    "sent": "That you.",
                    "label": 0
                },
                {
                    "sent": "You can in fact look at the.",
                    "label": 0
                },
                {
                    "sent": "The transform the linear transformation of a vector X as in fact.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Something like that, which in fact will give you something like this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So as a.",
                    "label": 0
                },
                {
                    "sent": "That's what's called spectral decomposition.",
                    "label": 0
                },
                {
                    "sent": "In which in fact you will have.",
                    "label": 0
                },
                {
                    "sent": "Which you will have so you look at this linear transformation in terms of decomposition in the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "And how to give you a intuition of that, let's say.",
                    "label": 0
                },
                {
                    "sent": "I had a really easy emetrics which is this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so that will give me as eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Did the.",
                    "label": 0
                },
                {
                    "sent": "He won't.",
                    "label": 0
                },
                {
                    "sent": "Which are the normal in the?",
                    "label": 0
                },
                {
                    "sent": "Are the normal base do not do the usual base we take in which we decompose.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then the usual base you know about it?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have the 111.",
                    "label": 0
                },
                {
                    "sent": "Victor.",
                    "label": 0
                },
                {
                    "sent": "With this transformation, with this information, what happened?",
                    "label": 0
                },
                {
                    "sent": "I can't, it's only.",
                    "label": 0
                },
                {
                    "sent": "This don't move.",
                    "label": 0
                },
                {
                    "sent": "With this, the 1st two dimension will not move, so I will keep exactly the same and the second one will be increased by two.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Knowing that this decomposition.",
                    "label": 0
                },
                {
                    "sent": "We will told me that on this on this plane things are not going to be stretched and they will keep their shape.",
                    "label": 0
                },
                {
                    "sent": "Well, in this there going to be increased by two and for example if I had one of these which is a negative.",
                    "label": 0
                },
                {
                    "sent": "So you see what will happen?",
                    "label": 0
                },
                {
                    "sent": "This is going to be completely reverse, so I will have a rotation or.",
                    "label": 0
                },
                {
                    "sent": "So knowing knowing the shape.",
                    "label": 1
                },
                {
                    "sent": "The what's your eigenvalue looks like?",
                    "label": 0
                },
                {
                    "sent": "Give you an idea of what the translator transformation is doing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so having.",
                    "label": 0
                },
                {
                    "sent": "A positive positive semidefinite matrix such as the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "Will give me that all my my eigenvalues are positive or or 0.",
                    "label": 0
                },
                {
                    "sent": "Which is 0 is also another case in which.",
                    "label": 0
                },
                {
                    "sent": "I don't have a if I know.",
                    "label": 0
                },
                {
                    "sent": "One of my eigenvalues is 0.",
                    "label": 0
                },
                {
                    "sent": "Then I will.",
                    "label": 0
                },
                {
                    "sent": "I know that I don't have objection but an injection some some.",
                    "label": 0
                },
                {
                    "sent": "Something disappear, can I do that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure I do.",
                    "label": 0
                },
                {
                    "sent": "I let that for the.",
                    "label": 0
                },
                {
                    "sent": "Practical part of the exercise.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the nice metrics generalization was used for square matrices, but we have again a generalization for rectangular matrix which is called the singular value decomposition.",
                    "label": 1
                },
                {
                    "sent": "Anne Anne.",
                    "label": 1
                },
                {
                    "sent": "Any any real value metrics can be rewritten like that with the this being the.",
                    "label": 0
                },
                {
                    "sent": "The singular of the.",
                    "label": 0
                },
                {
                    "sent": "Right, the left singular vectors.",
                    "label": 0
                },
                {
                    "sent": "And here the.",
                    "label": 0
                },
                {
                    "sent": "Right singular vectors.",
                    "label": 0
                },
                {
                    "sent": "In line or an in column an in the middle.",
                    "label": 0
                },
                {
                    "sent": "Something which is close to a diagonal matrix even if its mtime M so, but which has only element in the in the diagonal an.",
                    "label": 0
                },
                {
                    "sent": "If you look at this equation.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can find it.",
                    "label": 0
                },
                {
                    "sent": "You can find a relation with the.",
                    "label": 0
                },
                {
                    "sent": "The gram matrix we we saw before because.",
                    "label": 0
                },
                {
                    "sent": "And with the matrix organization.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "And if I take.",
                    "label": 0
                },
                {
                    "sent": "Oh MTM.",
                    "label": 0
                },
                {
                    "sent": "I will have, so the transpose is this which is.",
                    "label": 1
                },
                {
                    "sent": "Up and what will I have in the end?",
                    "label": 0
                },
                {
                    "sent": "I will have we so again, this matrices are orthogonal matrices, so when you make.",
                    "label": 0
                },
                {
                    "sent": "One year identity matrix.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "This end up being.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You see, that's that's only a diagonalization, or like we had before much examination.",
                    "label": 0
                },
                {
                    "sent": "Oops, sorry.",
                    "label": 0
                },
                {
                    "sent": "And this Instagram matrix in the.",
                    "label": 0
                },
                {
                    "sent": "In the witch space so.",
                    "label": 0
                },
                {
                    "sent": "This one was.",
                    "label": 0
                },
                {
                    "sent": "N * M. So this one is going to be.",
                    "label": 0
                },
                {
                    "sent": "And things M. So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's the gram matrix of the variables.",
                    "label": 0
                },
                {
                    "sent": "If I look at the variables.",
                    "label": 0
                },
                {
                    "sent": "As examples.",
                    "label": 0
                },
                {
                    "sent": "Define color for now.",
                    "label": 0
                },
                {
                    "sent": "Square meter.",
                    "label": 0
                },
                {
                    "sent": "You're right.",
                    "label": 0
                },
                {
                    "sent": "So it's not.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "You're right, it's not square.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "But if we look at this matrix as, this is a square matrix, so we have a matrix organization.",
                    "label": 0
                },
                {
                    "sent": "OK, I I stop here.",
                    "label": 0
                },
                {
                    "sent": "So you're in time.",
                    "label": 0
                }
            ]
        }
    }
}