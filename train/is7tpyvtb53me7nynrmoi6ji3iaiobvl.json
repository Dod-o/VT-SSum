{
    "id": "is7tpyvtb53me7nynrmoi6ji3iaiobvl",
    "title": "Denoising and Dimension Reduction in Feature Space",
    "info": {
        "author": [
            "Mikio Braun, Fraunhofer Institute for Intelligent Analysis and Information Systems",
            "Klaus-Robert M\u00fcller, Department of Software Engineering and Theoretical Computer Science, Technische Universit\u00e4t Berlin"
        ],
        "published": "Dec. 10, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/acs07_muller_braun_ddr/",
    "segmentation": [
        [
            "This is not going to be a do it, so we will speak sequentially.",
            "First I will.",
            "Briefly introduced, then Mickey will will talk about.",
            "His contribution to the research and then I will.",
            "Wrap up with some applications.",
            "Maybe it's better to.",
            "OK. Can we turn off the light a little bit or?",
            "So you can see it well then that's fine.",
            "OK, so and since this is a Pascal workshop, I believe we should also have the Pascal logo on our slide and this is actually not only joint work with in Berlin.",
            "I mean we're in the same Department but also withdrew buhman from ATL to Rick."
        ],
        [
            "OK, so I didn't really know what would be the audience, and it seems the audience is somewhat mixed, so I apologize for those of you who know VC three and PHMSA to all details and maybe the others will enjoy nevertheless.",
            "So there will be a crash course in this in the first couple of minutes and I will then.",
            "We will.",
            "Come too.",
            "Proofs and mainly applications in the end and it's always like if I give talks I always like to talk a little bit about brain computer interfacing, so that's in the end and maybe for the break.",
            "OK, so let's start.",
            "So we have some.",
            "X is an wise so there are.",
            "These are the data we have, any of them they live in our end.",
            "We have some labels or some just some data.",
            "If we do regression in RM generated from some P of XY, some underlying probability distribution we would like to learn from these examples.",
            "So we'd like to make inference find an F function F that can do.",
            "That can generalize well.",
            "So given all that data, we would like to see all the we would like to be able to predict all the future data as well.",
            "And this is the risk function as we commonly describe it.",
            "But you could also have other.",
            "You know this squared loss, but integrated over the risk.",
            "You could have something else there as well.",
            "Now of course.",
            "If you wouldn't know P of XY, then then you would be finished with the whole thing because you would just take the F that is that is optimal.",
            "But since you don't have it, you need some kind of induction principle and so in.",
            "I mean, we know empirical risk minimization where we just.",
            "Instead of taking the integral, we just take an average over the examples and this is just minimizing the training error.",
            "So we would like to find an F that minimizes the training error.",
            "So.",
            "That, of course, is very lousy.",
            "It doesn't generalize well in this sense, and is usually subject to gross overfitting.",
            "So that's not a good inference principle unless we have infinitely many data points."
        ],
        [
            "AM.",
            "So what has?",
            "Botnick contributed to that.",
            "And so.",
            "Anne.",
            "So first of all.",
            "If N goes to Infinity, then this empirical risk minimization goes to the true risk.",
            "So it's consistent, but does it actually give the same result?",
            "And the answer that Botnick gives is no, because we need uniform convergence over the whole function class.",
            "And.",
            "Taking this for granted.",
            "Pop Nick was able to prove a lot of different theorems one.",
            "Is here on this slide.",
            "Basically, the idea of the theorems is the following, so.",
            "Remember we would like to.",
            "Find a function from a function class that generalizes best OK.",
            "So.",
            "On this side we have the empirical risk.",
            "Something that is in our hands and we have some other term which is a square root term that basically describes the complexity of the classifier of the regressor.",
            "So D is the so called VC dimension and is the number of data points.",
            "Log is log.",
            "M and this probability holds with probability to 1 minus eater.",
            "OK, that's the bitter about, so I won't give any formal definition of the VC dimension.",
            "But maybe for the moment you can think of.",
            "VC dimension as a sum quantity that measures how complex your function classes.",
            "So linear function of function class made of linear functions would be very small VC dimension and some complicated spline basis would be.",
            "More complex or polynomial basis.",
            "And.",
            "So the idea of structural risk minimization is basically to say, well, we have some structure of the set of function and we would like to minimize this right hand side.",
            "So in other words, if we have something very.",
            "Simple then this term will be large, whereas this will be small if we have something very complicated as a function class, this will go to zero, whereas this will explode.",
            "So the truth is somewhere in the in the middle OK?",
            "So imagine you have.",
            "I mean it's going back to old neural networks, so you have.",
            "A function class made of networks with one hidden unit.",
            "That's one function class, next function Class 2 hidden units, and so on, so forth.",
            "So that would be some kind of a function class that that you could try to get the.",
            "An optimal part of the problem is of course you would like to give.",
            "You would have to have the D for that, the VC dimension and the bad news is that for neural networks or anything reasonable you might not have a closed form solution for the VC dimension.",
            "So the."
        ],
        [
            "The bad news?",
            "The good news is that there are some function classes for which you can compute the VC dimension.",
            "So.",
            "And in particular, one of those function classes is the linear hyperplane classifier.",
            "So this is not something particularly new.",
            "As you all know, of course.",
            "And.",
            "Um?",
            "So the idea is.",
            "You have a hyperplane and so these are the data points and you would like to classify the plus and the minus data points.",
            "And then.",
            "So this some space between these these data points this thing we call the margin.",
            "I mean not all people here in this room are from the machine learning community, so we use some jargon and.",
            "Margin if I say margin then this this thing here OK.",
            "So for hyperplanes in Canonical form, so the equation for the hyperplane would be W which is the norm vector to the hyperplane.",
            "Times X is the data, plus BB is the offset.",
            "And then we take the sign of it and we get the classification.",
            "If we have this hyperplane classifier.",
            "So Canonical form means that we remove the scaling freedom.",
            "So in principle here we could, you know multiply this by 100,000 and B also by 100,000 we get the same solution.",
            "So we just by by taking this.",
            "Form into account we remove the scaling freedom.",
            "Just set the scale arbitrarily to some things so we know what is the optimal generalizing hyperplane classifier that that is well known from the 60s of 1960s.",
            "So basically the larger the margin that we have.",
            "The better it is for our generalization.",
            "And we can just compute.",
            "This is a background of an envelope computation.",
            "How large is the margin?",
            "So it's basically inversely proportional to the norm of the hyperplane vector.",
            "So.",
            "If we have a large hyperplane norm vector then then this is a.",
            "You know small margin.",
            "And if it's a small non large mansion, OK, so.",
            "The optimal generalizing hyperplane has the maximum margin."
        ],
        [
            "So.",
            "For these hyperplanes in Canonical form.",
            "But Nick is able to compute the VC dimension and it's basically R-squared.",
            "So R is the ball in which the data lies.",
            "W is the norm of the hyperplane vector.",
            "A + 1 and this is just N + 1.",
            "And is the dimensionality of the space OK?",
            "So.",
            "In fact.",
            "If if W is not not small, then we get N + 1, which is very bad OK?",
            "So typically W squared is much smaller than this side.",
            "OK, so in fact what we see is that the VC dimension of a Canonical hyperplane classifier is basically.",
            "Smallest if the if it.",
            "If the system has a large margin or a small norm right?",
            "So basically the VC result states the same as the results known from the 60s.",
            "And now we can.",
            "We can actually plug this result into the theorem of Vatnik and then we could in principle at least decide what is the best generalizing hyperplane classifier OK?",
            "So again to put this.",
            "To state this very briefly, maximum margin means small W squared means good generalization error, meaning no low risk via this.",
            "So all we have to do is optimize this thing here.",
            "Subject to classifying correctly.",
            "So in a sense, this is somewhat independent of the dimensionality of the space, because this got lost in the mean time so.",
            "So I have to turn this off, sorry, forgot it.",
            "Anyway.",
            "M. OK, independent of the dimension."
        ],
        [
            "City of the space.",
            "So so the VC is so the support vector world makes some.",
            "Take some mileage out of that.",
            "AM.",
            "Because if you are anyway independent of the dimensionality of the space, you can just.",
            "You might as well just blow up the space to something infinite dimensional.",
            "OK, so.",
            "Say you have extra data.",
            "You map this into some feature space which is much larger and larger than the original space, and then you have to solve a problem here in this feature space so.",
            "Basically you have to classify all these pairs in the feature space correctly.",
            "So in other words.",
            "The we now not don't construct the function F which we did in the beginning, but it's rather a concatenation of some F~ and this mapping, right?",
            "So usually we always thought that this.",
            "Is actually harder because suddenly the data lives in a high dimensional space.",
            "But under some conditions this can be simpler.",
            "And what are the conditions?",
            "And this was on the last slide.",
            "So the conditions are basically.",
            "Within this inequality.",
            "So if this file this mapping is chosen that we can have a small training error and a low VC dimension.",
            "Then we we can prove that.",
            "We can guarantee good generalization.",
            "So in other words, it's the complexity that matters and not the dimensionality of the space.",
            "And.",
            "That"
        ],
        [
            "It's very abstract, but all of us have been doing this.",
            "This kind of things.",
            "Blowing up our data, for example.",
            "I give a very brief and simple example, so we want to classify between the blue points and the red points.",
            "And of course, just eyeballing this, you take an ellipse and that solves the problem, right?",
            "So now we take a mapping which is from R2 to R3 just.",
            "Increasing the dimensionality of the data.",
            "But taking now polynomials to the axis OK then all of a sudden this becomes linearly separable.",
            "OK, in this space we can take a linear separate separating hyperplane.",
            "We know how to optimally generalize.",
            "And.",
            "In this space, so we know in this space how to optimally generalize with something simple linear.",
            "And in this space here we have something nonlinear with which corresponds to the linear curve.",
            "Because this is just equivalent to some ellipsoid in the original space.",
            "So in other words, we know on the right side how to do optimal, and this is also optimal here.",
            "The thing that should be worrying us is of course, how to compute this into the how to compute scalar products here, because we might as well not map this into a 3 dimensional space, but into an infinite dimensional space.",
            "So how can we compute scalar products in an infinite dimensional space without an infinite amount of time?"
        ],
        [
            "And that's where.",
            "The kernel trick comes into the game because you can convince yourself that you can compute the scalar product easily by computing the scalar product in the original space and applying some nonlinearity to it.",
            "For this particular example.",
            "And we call this the kernel trick.",
            "And of course this doesn't work for all such mappings, but only for particular ones.",
            "In particular, for, this works only for so-called Mercer kernels because."
        ],
        [
            "We can then.",
            "Represent.",
            "If.",
            "The kernel is Mercer, IE fulfills this positivity condition, then we can.",
            "Basically have a representation in expansion and define, defy and basically get the kernel trick right there.",
            "So and.",
            "For those of you who like reproducing kernel Hilbert spaces, this is like a reproducing kernel, but."
        ],
        [
            "So what are typical kernels, polynomial kernels, RBF, kernel, splines, quadrics?",
            "Kernels."
        ],
        [
            "Respond to certain regulation properties.",
            "So.",
            "That's.",
            "Basically very.",
            "Brief wrap up of of VC theory and some SVM ideas.",
            "So.",
            "I have been telling this story many times and I always find that it's quite interesting in the end of the day that these machines actually work in practice.",
            "So imagine you have 1000 dimensional space where the data lives and you have 50 data points.",
            "You classify with some SVM with some kernel and you get something that really generalizes well, makes great predictions.",
            "So if I look for an explanation why this is really working so well, I cannot find it in this theory, I cannot find it within these bounds because the bounds are just general statements of how learning works.",
            "But it is.",
            "It doesn't tell me for my particular example, why why this actually works?",
            "And the idea of this talk is actually to explain to contributes, so to say.",
            "What I think is a missing part of the theory.",
            "Why are SVM's actually work or any kind of kernel methods in fact?",
            "Anne.",
            "So.",
            "OK, what we know so far what I've been telling you is, is that we have to trade off the error versus the complexity and the bound measures that.",
            "And we also know that the VC dimension is independent of the space dimensionality of the space, and we have some introduced some kernels that preprocess the data too.",
            "To increase the discriminative power so the question is really.",
            "So what what is the interesting?",
            "What is the important contribution in in kernel machines, is it?",
            "That we have these VC bounds or is it the kernel itself or what is it?",
            "OK, and we're trying to converge to finding an explanation and make you will do that, but in a couple of minutes.",
            "So how can we realize a large margin which we."
        ],
        [
            "No, is good for.",
            "Classification so so far the picture has been basically not considering the labels very much.",
            "So if we have been mapping our X into some high dimensional space, we know that we need some complexity control.",
            "We know that since VM's work that there's some low complexity, but what?",
            "Is there any link to the labels?",
            "And this is what we're working on.",
            "Now."
        ],
        [
            "OK, so.",
            "I will be now very fast.",
            "So first of all, many of you may know the kernel PCA, which is a nonlinear version of the original PCA.",
            "So you basically the idea is that instead of doing PCA in the original space, we map into some appropriate feature space.",
            "And then do something linear there and.",
            "In the original space we get nonlinear principle component analysis decomposition."
        ],
        [
            "So I'm just.",
            "Briefly, scanning through this, So what you need for that is you map the data into the feature space.",
            "Then you solve an eye problem there and basically you get an equation which is very similar to the original 1.",
            "Anne should put put here.",
            "And maybe just to show that the trick again.",
            "So we know that the solutions to the kernel PCA should be we should be able to expand this in terms of the data.",
            "And so we're only looking into this parameter space that's Alpha."
        ],
        [
            "Actors that you know saw on these slides.",
            "We have to know."
        ],
        [
            "Realize this and the good thing is that.",
            "Centering projecting everything is possible with current."
        ],
        [
            "OK.",
            "So.",
            "Knowing that.",
            "We now know the kernel PCA basis and we can take this kernel PCA basis.",
            "Which basically describes everything that the kernel is about in terms of the data.",
            "And construct something which is called empirical kernel map.",
            "OK.",
            "So this is this is also old.",
            "I mean I think we coined the term empirical kernel map 99, so we should cook for smaller.",
            "So, so this is now the kernel matrix.",
            "Now the kernel matrix is decomposed like that.",
            "So this is a diagonal matrix.",
            "Then this is basically the empirical kernel map, so we're just.",
            "Projecting to this decomposition right?",
            "So that FF transposed now becomes K again, OK?",
            "Now."
        ],
        [
            "So let's.",
            "Take a picture of F. OK, so that's.",
            "We have 40 data points.",
            "So this means that we have 40 kernel PCA components, OK?",
            "And also we have.",
            "This is the number of data points OK?",
            "Now you see which I just ordered them.",
            "These are the ones, and these are the 7th from optical character recognition.",
            "And if you look at this a little bit, there's something happening here.",
            "There's something happening here and that not a lot.",
            "Furthermore OK.",
            "So the very."
        ],
        [
            "It's become smaller and smaller.",
            "There's some proofs due to real and to meet you and two others also.",
            "John Troy Taylor contributed to that.",
            "So saying that.",
            "The.",
            "Eigenvalues of the kernel matrix.",
            "Anne, stay concentrated.",
            "So, so they are actually concentrated around their asymptotical values.",
            "Even for small N. And also this is to do to jail.",
            "So the differences in the tail bounds are not so great.",
            "It's not only that they are concentrating, so the errors that we make are proportional to their icon.",
            "Value so it's not that we do 10% error.",
            "But if we do it according to the value of the eye.",
            "K. I .1 error say."
        ],
        [
            "OK.",
            "So no.",
            "So far we've been looking at.",
            "Just.",
            "Colonel principle components, so just that variances.",
            "And now I want to look at something else which is labels.",
            "So remember, these use are the kernel PCA basis.",
            "OK, so we just computing so UI is the ice eigen vector of the kernel PCA basis and we are.",
            "Projecting to the labels Y.",
            "This is SI.",
            "So basically we're asking how much is this particular?",
            "I think I can.",
            "Factor contributing to explaining the data.",
            "OK. And we could also ask, well, how if we take M of those eigenvectors, all MM of them?",
            "How do all these M. Guys contribute to the explanation of the.",
            "Of the labels.",
            "And so the hope would be that we get some small M's.",
            "So very few components actually responsible for explaining."
        ],
        [
            "So.",
            "Principle we should remember so in.",
            "Why the heck should this be the case?",
            "So.",
            "If.",
            "We look at just the variances, then clearly the direction of largest variance would not do any any good for classification here.",
            "So, but the hope would be still that.",
            "The largest variances would contribute to the classification.",
            "So that's the hope."
        ],
        [
            "And if we look at the same picture that you saw before.",
            "This is just.",
            "If the empirical kernel PCA matrix.",
            "And this is the projection to the labels.",
            "So you see that this component not the first, but the second component contributes most.",
            "Everything else is basically decaying, so that's an empirical finding, and the hope is that this empirical fine."
        ],
        [
            "Can also be.",
            "Proved in theory.",
            "And this is what Mickey is going to do in the next minute.",
            "So so the the idea is really to say it is the.",
            "Very few of the kernel PCA components contain all variants necessary to explain the classification or regression.",
            "OK.",
            "Cancel thanks a lot for the first part clause.",
            "Right so and of course, so the question is, is there a theoretical result?",
            "And of course this theoretical result.",
            "Otherwise you wouldn't be here.",
            "So actually, So what?",
            "I'm trying to explain how you can prove that under certain conditions on the fitness between the kernel and your data, all the relevant information about the labels is actually contained in the first few kernel PCA directions, right?",
            "So irrespective of the dimensionality of the feature space itself, so the feature space can be infinite dimensional, But the relevant information about the learning problem is really contained in the first few direction.",
            "Right, so for example.",
            "So this is a regression data set.",
            "The window sync function, right?",
            "There's a bit of noise.",
            "And actually, if you look at the function you try to learn and the noise and you separate them and then you plot their how they contributed to kernel PCA directions when you see that the interesting part, So what you really actually want to learn is containing the first 11 directions.",
            "And the rest is really just noise, right?",
            "So this is very important for this whole.",
            "How can learning with kernels in infinite dimensional spaces really?"
        ],
        [
            "Work.",
            "OK, so.",
            "Very briefly, so the whole idea is based on defining what relevant information means, right?",
            "So I'm saying the relevant information is contained.",
            "Then copy, say, but we try to define this in a very generic way, which can be used both for classification regression.",
            "And then the whole result in the end is just some sort of approximation result, so we will first go to the asymptotic setting right of having infinitely many data points or just knowing the probability distribution where it's very easy to see.",
            "Why and when the data will be concentrated in the first few directions and then we show that there is we can derive a bound to show that this same thing also already holds for in the finite sample setting.",
            "So for really like 100 samples or thousand samples and then the last step is to also consider how noise will contribute to the."
        ],
        [
            "To the labels.",
            "OK.",
            "So he said, so we were in some regression or classification settings.",
            "So this can be treated in the same way and the basic idea is that the labels which we see is giving by by sampling some unknown function G which we actually want to learn an additive noise, right?",
            "So basically it says the output which we see the wiser given this some smooth part and some noise.",
            "And when you do this, then sorry for regression, which you take the two norm.",
            "It's very clear that the G is just given as the.",
            "That's the conditional expectation of the Y given the X.",
            "So this sort of say the mean label, which you would expect a certain point.",
            "Right and for regression, right?",
            "This is really just a function you try to learn.",
            "And for classification this boils down to two times the probability that you have Class 1 -- 1.",
            "Like like this, some alright?",
            "So this is the these are the class densities, two Gaussians and these are this is the.",
            "So to say the class posterior for the left of these two, and then you scale it such to minus 1 + 1, right?",
            "So this would then be the relevant information for the classification problem, and you can already see that you should just look at the sign of this relevant information.",
            "Then you can also.",
            "Then you get the optimal decision boundaries between the two classes, right?",
            "So the relevant information really is the something like the smooth, smooth, denoised version of the labels which still contains all the information which you need to learn well."
        ],
        [
            "Your data.",
            "Right, and so in order to study it, we want to somehow first think about what happens if a lot of data and then say if if you can somehow relate this Antarctic settings.",
            "A finite sample setting.",
            "So what is the finite sample setting?",
            "So there are certain.",
            "Wait so we have this kernel matrix K. Right, and this defines us a linear operator on vectors.",
            "I mean, it's just a matrix, right?",
            "But also in functions, so we could plug for this VJ.",
            "We could plug some sample functions, sample vectors of some functions in there, and then we could place this XI with a free variable and then we really get a mapping from functions to functions which is defined by this kernel matrix.",
            "Right and the this operator has eigenvectors which are also the kernel PCA components of which we talked before.",
            "And the contribution of these component of so the computer components just eigenvector is again given by this by the scalar product here, which you also saw before."
        ],
        [
            "OK, so and if you can all get more and more data right this.",
            "This sum here will converge to an integral operator.",
            "Of course.",
            "Where we integrate against the probability distribution which generated or training points.",
            "The Islanders converge to eigenfunctions, so I hear.",
            "And this contribution converge to the scalar product with the eigenfunctions.",
            "Right, so in the automatic setting basically, so we are now not talking about single points, but actually about."
        ],
        [
            "Functions.",
            "OK, so this is just summary so much.",
            "I hope you are."
        ],
        [
            "So it's so hard actually.",
            "OK, and so now the interesting question is.",
            "In this entotic setting, what are reasonable, reasonable assumptions such that is relevant function?",
            "This way of information you want to learn this this function G. Is really contained in the space which is spent by the first few of these eigenfunctions right?",
            "In a very natural assumption is to say that this G can absolutely be represented by this integral operator, TK."
        ],
        [
            "OK, which I write this so I mean which basically just means that this function G lies in the in the range of the TK here."
        ],
        [
            "OK, whoops.",
            "Or in other words, there exists some function H such as GST, KH and then I can just write everything down in the with respect to the eigen decomposition and I get this G has a series expansion in terms of the eigenfunctions also integrate operator which reads like this.",
            "Right and here of course, the Alpha is a bounded sequence, so I get that these contributions are scalar product between G and the eigenfunctions.",
            "Just decays as quickly as the eigenvalues of the of the kernel.",
            "Right and typically so this depends on the smoothness of the kernel.",
            "But since we want to do something useful, we usually have smooth kernels.",
            "So this means that this function that this coefficients actually decay quite quickly.",
            "And it isn't otic.",
            "Setting G is really contained in this space, which is spent by the first few eigenfunctions.",
            "Alpha cell phone.",
            "Do I get I get this here so this exists some function H right which has finite known and then so the length of H. So since this is a.",
            "So saying.",
            "Yeah, I I know there was.",
            "So this sounds like a basis, right?",
            "So then H as this is off.",
            "I responded here, it's actually it's a square summable.",
            "OK.",
            "Right, so I mean the idea is that you if you take the currency is she lots and lots of later than that.",
            "Then eventually you can actually represent what you are trying to learn.",
            "Use this kernel right?",
            "So this is really not too much of it."
        ],
        [
            "Option.",
            "OK, and then the question is of course does."
        ],
        [
            "So we are interested in.",
            "And showing that that this rate here, right?",
            "So these scalar products, they should decay as I look at it later components they should decay with the rate of the eigenvalues or eigenvalues of the kernel and the question is if this also holds for this finite."
        ],
        [
            "The setting.",
            "OK, and actually this is so this is the source of the result.",
            "So if we assume that this G has this form here.",
            "Right, and we look at the sample vector, then with high probability right?",
            "So this is something like this.",
            "Probability minus one minus Delta where it's all been suppressed here for simplicity.",
            "Scalar product between the eyes eigenvector of the kernel matrix and the center vector can be bounded by by this long bond here.",
            "And the most important things have been highlighted in red, right?",
            "So it actually consists of two parts.",
            "The first part, which depends on the eigenvalue of the kernel matrix, right?",
            "So this part goes to zero quickly.",
            "And then I have lots of lots of additional error terms here, but which are all small.",
            "OK, so this whole bound also depends on R, which I come later, but basically as R goes to Infinity, all of these terms go to zero very quickly.",
            "For example, this Lambda are here.",
            "This is just the tail sum of eigenvalues smaller than Lambda.",
            "Under R. So these are very small and these functions also occur from some translation operations.",
            "Right, So what I get is that the contribution.",
            "Of this relevant information.",
            "To the kernel, PCA components decays essentially as quickly as the eigenvalues."
        ],
        [
            "OK, so I tried to give a give and give an idea how this how this is proven very quickly or very not too much detail but the so the main idea what is the main idea right?",
            "So the I mean initially we had this, this Jesus these are all G these were all infinite expansions.",
            "So G actually the son went to Infinity and for K we have this formula here which comes from Earth's theorem.",
            "There's also went to Infinity and the basic idea is first to truncate all these.",
            "This series so that I really have something which is more less finite dimensional right?",
            "And then I will treat the truncation error in a very rough manner, and for this finite dimensional objects that can really do apply some linear algebra bounds to bond the error.",
            "Right, so this is what we're interested in.",
            "The first thing is OK, so this~ explains the truncation everywhere, so when you see something, some object which has been tilted.",
            "It has to have some serious expansion.",
            "It has been truncated.",
            "So in the first step we replace this G by G~ and then we get this additional truncation error here, right?",
            "So now we already have I vectors of the full kernel matrix, and this simpler function, and then the next step.",
            "We also replace these UI by the eigenvectors of the truncated kernel.",
            "Function so of the curtain matrix using the truncated cone right and then I sort of get like an error estimate here and I end up with.",
            "I can with the scalar product between sample vectors of the full eigenfunctions and eigenvectors of the kernel matrix for the truncated kernel.",
            "Function, yes, so this is not something which I can treat very basically."
        ],
        [
            "I mean, I'm very.",
            "Elementary so basically right?",
            "So this is just the definition of all the.",
            "Since L&L is an eigenvalue and use an eigenvector right, this holds.",
            "And then I replace K here basically.",
            "By this right?",
            "So this is sort of like."
        ],
        [
            "If I apply this formula here.",
            "I can also write it down in matrix form between.",
            "This is something like a multiplication between the sample."
        ],
        [
            "Matrices which collect this sample vectors of the eigen functions and diagonal matrix which only contains the true eigenvalues right and then I can.",
            "Sort of multiply by PSI transpose.",
            "Then I assume that this is invertible, so I get the pseudoinverse here.",
            "Right, and then if I take norms on both sides here?",
            "And actually I get an upper bound for this simple scalar product of eigen functions and eigenvectors.",
            "Right and here so this is the basic step where I get the.",
            "Decay rate, which is.",
            "Yeah, proportional to the eigenvalue."
        ],
        [
            "OK.",
            "So in the next step that I have somehow to take care of these these errors, which I get by truncating the kernel matrix.",
            "So here you can apply one of these synthetase theorems, which basically says that the angle between eigenvectors under an additive perturbation is bounded by the.",
            "The norm of the perturbation and the separation of these two eigenvalues, right?",
            "But then we will multiply this here also again by the eigenvalue.",
            "So we can actually bound this.",
            "So using something like the by looking at the size of certain clusters around eigenvalues."
        ],
        [
            "OK, so we have to take care of the truncation of this function G, But basically this just converges to this term and then we.",
            "Temperature if we get something like this year.",
            "Right?"
        ],
        [
            "So first we have decomposed the thing we wanted to look at, so we end up with eigenfunction center vectors and these sort of a truncated eigenvectors right?",
            "And we get the already the basic bond which convert which converges like we wished.",
            "Then we have to also take care of the perturbation of the truncation of the kernel function.",
            "Then we get this term.",
            "And then we get another term for the truncation function G and then basically we end up already end up with the term which we wanted to have here, right?",
            "So so basically, it's not.",
            "It's not so complicated.",
            "Maybe apart from the formulas, but.",
            "Sorry, so it's just basic linear algebra."
        ],
        [
            "Hollis OK, so let's look at some examples finally.",
            "We have two functions here.",
            "Again, the sync function and then we have another function which is cosine times son of 5X, so it's a bit more complex function and then we take an RBF kernel.",
            "So Gaussian kernel with a certain width and we look at we compute these kernel PCA components and just as predicted by the theorem you can see that the contribution of these kernel PCA components to the smooth functions is actually really concentrated in the first 10 or 20.",
            "Dimensions.",
            "Right, so it even makes it makes sense somehow cause this red function is a bit more complex.",
            "It also needs more components to be represented well.",
            "Right and everything else here is completely negligible.",
            "So in principle we have a problem in 100 dimensions, but only twenty of these dimensions are really in."
        ],
        [
            "OK, and then the last question is so far we only dealt with the with the noise free case.",
            "So what happens with the noise, right?",
            "So we have some noise in here, some IID noise.",
            "OK, but basically.",
            "When you compute the scalar product with the eigenvectors because they are talking about you, just computing an orthogonal rotation is something like an orthogonal basis transform.",
            "And this means if it's noise before that, then it will also be noise after this transformation, right?",
            "So the noise does not concentrate in this kind of PCA directions, but the noise stays distributed over all of them off the all over the."
        ],
        [
            "Components.",
            "Right, so the red one, so this would be the smooth part.",
            "This is the noise as you can just search see so the the interesting part in the data is really contained in the leading dimensions and the rest is really spread over all of the data.",
            "OK. No.",
            "Good, so the question is of course, so this is theoretically very interesting, right?",
            "So you say I can really learn something that cause the problem is really just 11 dimensional, and if I have one other data points and do something linear and F 11 dimensional space, then everything is well, but it's actually I mean if it's just a theoretical result, maybe not that interesting, But what we actually want to do is we want to.",
            "If you have a concrete data set and we choose some kernel, you want to know, how good does this kernel fit the data, right?",
            "So we want to really.",
            "Estimate."
        ],
        [
            "With this dimension.",
            "Of the data set to see how good the kernel fits the data.",
            "Right, and so this is actually possible, very easy manner.",
            "So usually you don't have the smooth part and the noise separated.",
            "So when you look at this, so you compute the kernel PCA components which are really just the eigenvectors of the kernel matrix.",
            "You compute the scalar product and then compute what you get is something like this right?",
            "And you know that the interesting part is somewhere contained within these blue points, but you don't know where.",
            "OK, but by the theorem now you know that that the interesting contribution will decay very quickly.",
            "So right?",
            "So what you can if you see this and you have a nice step like this and you can be pretty sure that all of the relevant information is really contained in these large leading large component in the leading components and the rest is really just noise and can we discard discard all of it?",
            "OK, So what we're trying to do to find this cut off point is to Esther to fit two Gaussians to these points with two different variances.",
            "So we both have mean zero and one one variance.",
            "For the later part and one for the first part."
        ],
        [
            "Right, so in formulas, so both have mean zero for the first dimension, we assume one variance in for this.",
            "The negative log likelihood isn't proportional to this term here.",
            "You can compute it very easily.",
            "And then we choose."
        ],
        [
            "Just choose the dimension which minimizes this likelihood.",
            "I will negative look like so, so in this case you very nicely see OK, it's just 11 dimensions.",
            "Right so and this actually works very well, so this is very."
        ],
        [
            "Simple idea, but.",
            "Works well and you can even use this log likelihood which you get for model selection, right?",
            "So in this picture, what you see here, these are two datasets.",
            "Is classification data set in the regression data set and on the X axis you have the kernel grid.",
            "So we take our requires many different kernels and what you see what you see here in blue.",
            "This is the test error.",
            "Right, so this would be so this is the area where you have a good kernel and here you you you.",
            "I think on this side you over fit in here under fit very severely.",
            "And what you see in red is the log likelihood, right?",
            "So the optimal log likelihood which you get but then very nicely coincide, right?",
            "So which somehow means if?"
        ],
        [
            "If in this image, if you can really separate the signal from the noise really well.",
            "Right then we will."
        ],
        [
            "Principle have a good car."
        ],
        [
            "OK. OK, So what we could also do is you can just take this number of dimensions and project your labels to this dimension.",
            "Right to denoise to get denoised versions of this layer, and this is actually worth works.",
            "Can see here.",
            "So what we did is we computed this number of dimensions for a number of benchmark datasets.",
            "OK. And then we just did an unregularized fit on this on this data set on this denoise data.",
            "And as you can see, the results which you get just as good as if you take a support vector machine on the whole data set.",
            "So which really proves that we have extracted that information, the relevant information is really contained in this low dimensional space."
        ],
        [
            "OK.",
            "So close right now I'll talk about some more applications of this.",
            "So this was all on some benchmark data which is available in.",
            "These are the either datasets.",
            "Now.",
            "So we didn't have these ideas for a very long time, so our paper is currently under revision in Jamil Arm, so, But what we have been working on for a very long time is splice site detection, and so we've been going through.",
            "A couple of iterations over many years to improve the classification rates for supply side detection.",
            "So spicy.",
            "So basically you would like to.",
            "Distinguish between ages that are instrumental as splice.",
            "Site Santa eggs that are just there but have nothing to do with the splicing process.",
            "And this is a very simple classification problem.",
            "You take all the edges that are splice sites and all the edges that are non splice sites take some windows around them.",
            "And then.",
            "Code these 80 GS somehow compute kernels and classify.",
            "So.",
            "That's the general."
        ],
        [
            "Problem.",
            "So you could do an E3 coding coding these as 1234, so to say.",
            "Which.",
            "May not be the proper thing because these would be closer together than than these and that this doesn't actually make any sense.",
            "But you could compute the same thing that you saw all the time in Micco Slide 4 for a kernel that that this is an RBF kernel that takes these such encoded strings, right?",
            "And you see that.",
            "Basically, there's 1000 kernel PCA components and you you see that there's a lot of this is quite high dimensional and contains a lot of noise, right?",
            "So.",
            "Next iteration, so you get a test error of 12."
        ],
        [
            "1212% so now you encode them as such, which is much smarter already.",
            "That your test error goes down to 7% and you only have a much lower dimensional thing and the noise is also smaller.",
            "And now."
        ],
        [
            "You do what we did in recent paper and plus computational biology that appeared in February.",
            "Which AM.",
            "Asks whether there are some substrings that match.",
            "So imagine you have a substring made of one letter of two letters, or three letters, or 4 letters, and so on, and you weight them accordingly, OK?",
            "So maybe the four letters are not so as high weighted as the one letter similarities.",
            "Just.",
            "So here you get a very.",
            "I mean, this is a very nice modeling and you can also talk a long time about the biological relevance of that."
        ],
        [
            "But I won't.",
            "And then just show you this plot so the test error is five, 5.5% and again you see something here.",
            "Now what I just want to show.",
            "With this example is.",
            "So this was a retrospective analysis, so our paper has.",
            "I mean, even the plus paper had appeared and we took the kernel matrices and asks, you know, in this sense of the theory just presented, can we distinguish between these three cases?",
            "And what eyeballing these kind of plots makes actually most sense, because we would like to get the lowest dimensionality at the lowest noise rate in principle.",
            "Right?",
            "So because in that sense.",
            "A good kernel tries to.",
            "Improve the signal to noise ratio at keeping the dimensionality low.",
            "I mean that is the basic finding of the whole thing.",
            "So basically forgiven.",
            "But Colonel basically boils everything down to something low dimensional.",
            "So the example that I made in the beginning with 50 data points in 1000 dimensions.",
            "So maybe only three or four dimensions of those or 10 might actually be relevant in the end.",
            "So all these kernel methods, be IT support vector machines or Fisher disk kernel, Fisher Discriminant or Gauss processes or methods that use kernels they actually reduce.",
            "The data to spaces where you actually get the lowest dimension at the highest signal to noise ratio, so to say for the Buck.",
            "If you do it right.",
            "And then of course.",
            "I mean it's it's only 1 remaining problem.",
            "Of course always that is model selection.",
            "So what is the good choice of the kernel and just show you this?",
            "I would like to propose this as a diagnosis tool that we can use in order to get a very nice precise understanding."
        ],
        [
            "OK, so maybe I am slightly running out of time and do I have 5 more minutes or OK so.",
            "So.",
            "Just.",
            "So what is the?",
            "Now for something completely different.",
            "It's not quite different.",
            "You see it in the end.",
            "So brain computer interfacing something that I'm very interested in.",
            "We have typically brain a measuring device 128 channel, EG, say.",
            "We extract some features we classify.",
            "We control a wheelchair computer game, something.",
            "An and the one of the very.",
            "Important points in the BCI world is that?",
            "So first of all, machine learning has contributed to the BCI.",
            "Research in that.",
            "No longer the subjects have to train to manipulate their brain signals such that their thoughts can be read out.",
            "But rather the machine is learning to interpret the thoughts.",
            "So the training goes from.",
            "100 hours of subject training to about 15 minutes of subject training.",
            "15 minutes of subject training so it's a big step forward that machine learning and signal processing has contributed to, however.",
            "About 1/3 of the population.",
            "We call BCO illiterate so they are unable to communicate.",
            "For an obvious reasons and one important question for us is can we see this in their data before we do long studies with them?",
            "OK, that's a question and now I just Fast forward.",
            "OK. And it just showed this plot.",
            "So.",
            "On this side you see.",
            "The offline classification error.",
            "So this is a very lousy subject here.",
            "20% errors.",
            "This is almost 0% error, so you see some blue crosses that are very good and this is the estimate of the.",
            "Cannot stop the noise level.",
            "So that's the estimate of Mickey's noise level, not Mickey's noise level, but makers estimate of the noise level of the data of the respective subject.",
            "So this is you see a number of different subjects and you see that.",
            "And it's somehow clear if you.",
            "Have subjects with a low, not a signal.",
            "I mean with a high signal to noise ratio with a low.",
            "Noise level, in the sense that it's estimated, then you get very high performance consecutively.",
            "You get worse performance if the noise level is higher.",
            "This is somewhat common sense.",
            "M. What is somewhat counter intuitive is that dimensionality.",
            "Is high for those guys.",
            "And low for the guys that cannot communicate.",
            "And this has to be put with a grain of salt, because we're always, you know, in this plot we always have two measurements.",
            "Noise level signal to noise level and the dimensionality.",
            "So of course we can, you know.",
            "So in this case, we say that the dimensionality is low, but the signal to noise ratio is high.",
            "Sorry as the noise level is high and the dimensionality is low, so I mean it's in principle what this lets us interpret is that we cannot really pinpoint a proper subspace where we can classify because there's mostly mainly noise, there's no real signal in that.",
            "And if you now plot the kernel width as a function of the dimensionality.",
            "So this is a very broad simulation for illiterate and for non illiterate so you can see just the difference and so the interesting point of course is can we?",
            "In the future, and this is very new result from last week, can we in the future actually be used that method in order to test very early on whether or not to pursue a certain subject further and?",
            "Can we?",
            "Of course, so usually in brain computer interfacing we do linear classifiers all the time.",
            "And so for some of the in particular, for some of the illiterates, it seems that a nonlinear classification, as suggested by this method, might make some sense, as suggested by the kernel found by this method.",
            "Makes makes some sense, so we will be pursuing that in order to actually improve our.",
            "Transmission rates and for those of you who haven't enjoyed this video, this is just a video of.",
            "Yeah myself.",
            "Playing a game of brain Pong by thinking about my right hand, I move the cursor to the right by thinking about my left hand.",
            "I move the cursor to the left.",
            "I'm not moving anything, I'm just thinking I'm just imagining the movement of my hands.",
            "So.",
            "In I would be one of the guys where that is very far from an illiterate, but there are many people that never get that far.",
            "We would like to save our experimenting time, it's it's really a serious experiment for every subject we need to put the G cap and we need to do some preparations, some training and so on, so forth.",
            "So anyway, with this very.",
            "Practical aspect."
        ],
        [
            "I would like to.",
            "Close, I think we have started to shed some some light in this direction, so I think this we gave some proof and some intuition and some practical evidence on practical algorithm.",
            "With which we can see why kernel methods would work very well.",
            "They make very economical use of the dimensionality of the kernel space.",
            "And I think that is a very interesting insight, and I think it's also something that is an independent type of explanation from the usual VC picture and I found it somewhat interesting myself.",
            "Thank you for you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is not going to be a do it, so we will speak sequentially.",
                    "label": 0
                },
                {
                    "sent": "First I will.",
                    "label": 0
                },
                {
                    "sent": "Briefly introduced, then Mickey will will talk about.",
                    "label": 0
                },
                {
                    "sent": "His contribution to the research and then I will.",
                    "label": 0
                },
                {
                    "sent": "Wrap up with some applications.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's better to.",
                    "label": 0
                },
                {
                    "sent": "OK. Can we turn off the light a little bit or?",
                    "label": 0
                },
                {
                    "sent": "So you can see it well then that's fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so and since this is a Pascal workshop, I believe we should also have the Pascal logo on our slide and this is actually not only joint work with in Berlin.",
                    "label": 0
                },
                {
                    "sent": "I mean we're in the same Department but also withdrew buhman from ATL to Rick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I didn't really know what would be the audience, and it seems the audience is somewhat mixed, so I apologize for those of you who know VC three and PHMSA to all details and maybe the others will enjoy nevertheless.",
                    "label": 0
                },
                {
                    "sent": "So there will be a crash course in this in the first couple of minutes and I will then.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                },
                {
                    "sent": "Come too.",
                    "label": 0
                },
                {
                    "sent": "Proofs and mainly applications in the end and it's always like if I give talks I always like to talk a little bit about brain computer interfacing, so that's in the end and maybe for the break.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start.",
                    "label": 0
                },
                {
                    "sent": "So we have some.",
                    "label": 0
                },
                {
                    "sent": "X is an wise so there are.",
                    "label": 0
                },
                {
                    "sent": "These are the data we have, any of them they live in our end.",
                    "label": 0
                },
                {
                    "sent": "We have some labels or some just some data.",
                    "label": 0
                },
                {
                    "sent": "If we do regression in RM generated from some P of XY, some underlying probability distribution we would like to learn from these examples.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to make inference find an F function F that can do.",
                    "label": 0
                },
                {
                    "sent": "That can generalize well.",
                    "label": 0
                },
                {
                    "sent": "So given all that data, we would like to see all the we would like to be able to predict all the future data as well.",
                    "label": 0
                },
                {
                    "sent": "And this is the risk function as we commonly describe it.",
                    "label": 0
                },
                {
                    "sent": "But you could also have other.",
                    "label": 0
                },
                {
                    "sent": "You know this squared loss, but integrated over the risk.",
                    "label": 0
                },
                {
                    "sent": "You could have something else there as well.",
                    "label": 0
                },
                {
                    "sent": "Now of course.",
                    "label": 0
                },
                {
                    "sent": "If you wouldn't know P of XY, then then you would be finished with the whole thing because you would just take the F that is that is optimal.",
                    "label": 0
                },
                {
                    "sent": "But since you don't have it, you need some kind of induction principle and so in.",
                    "label": 1
                },
                {
                    "sent": "I mean, we know empirical risk minimization where we just.",
                    "label": 1
                },
                {
                    "sent": "Instead of taking the integral, we just take an average over the examples and this is just minimizing the training error.",
                    "label": 1
                },
                {
                    "sent": "So we would like to find an F that minimizes the training error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That, of course, is very lousy.",
                    "label": 0
                },
                {
                    "sent": "It doesn't generalize well in this sense, and is usually subject to gross overfitting.",
                    "label": 0
                },
                {
                    "sent": "So that's not a good inference principle unless we have infinitely many data points.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "AM.",
                    "label": 0
                },
                {
                    "sent": "So what has?",
                    "label": 0
                },
                {
                    "sent": "Botnick contributed to that.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "If N goes to Infinity, then this empirical risk minimization goes to the true risk.",
                    "label": 0
                },
                {
                    "sent": "So it's consistent, but does it actually give the same result?",
                    "label": 1
                },
                {
                    "sent": "And the answer that Botnick gives is no, because we need uniform convergence over the whole function class.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Taking this for granted.",
                    "label": 0
                },
                {
                    "sent": "Pop Nick was able to prove a lot of different theorems one.",
                    "label": 0
                },
                {
                    "sent": "Is here on this slide.",
                    "label": 0
                },
                {
                    "sent": "Basically, the idea of the theorems is the following, so.",
                    "label": 0
                },
                {
                    "sent": "Remember we would like to.",
                    "label": 0
                },
                {
                    "sent": "Find a function from a function class that generalizes best OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On this side we have the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Something that is in our hands and we have some other term which is a square root term that basically describes the complexity of the classifier of the regressor.",
                    "label": 1
                },
                {
                    "sent": "So D is the so called VC dimension and is the number of data points.",
                    "label": 0
                },
                {
                    "sent": "Log is log.",
                    "label": 0
                },
                {
                    "sent": "M and this probability holds with probability to 1 minus eater.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the bitter about, so I won't give any formal definition of the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "But maybe for the moment you can think of.",
                    "label": 0
                },
                {
                    "sent": "VC dimension as a sum quantity that measures how complex your function classes.",
                    "label": 0
                },
                {
                    "sent": "So linear function of function class made of linear functions would be very small VC dimension and some complicated spline basis would be.",
                    "label": 0
                },
                {
                    "sent": "More complex or polynomial basis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the idea of structural risk minimization is basically to say, well, we have some structure of the set of function and we would like to minimize this right hand side.",
                    "label": 1
                },
                {
                    "sent": "So in other words, if we have something very.",
                    "label": 0
                },
                {
                    "sent": "Simple then this term will be large, whereas this will be small if we have something very complicated as a function class, this will go to zero, whereas this will explode.",
                    "label": 0
                },
                {
                    "sent": "So the truth is somewhere in the in the middle OK?",
                    "label": 0
                },
                {
                    "sent": "So imagine you have.",
                    "label": 0
                },
                {
                    "sent": "I mean it's going back to old neural networks, so you have.",
                    "label": 0
                },
                {
                    "sent": "A function class made of networks with one hidden unit.",
                    "label": 0
                },
                {
                    "sent": "That's one function class, next function Class 2 hidden units, and so on, so forth.",
                    "label": 0
                },
                {
                    "sent": "So that would be some kind of a function class that that you could try to get the.",
                    "label": 0
                },
                {
                    "sent": "An optimal part of the problem is of course you would like to give.",
                    "label": 0
                },
                {
                    "sent": "You would have to have the D for that, the VC dimension and the bad news is that for neural networks or anything reasonable you might not have a closed form solution for the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bad news?",
                    "label": 0
                },
                {
                    "sent": "The good news is that there are some function classes for which you can compute the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And in particular, one of those function classes is the linear hyperplane classifier.",
                    "label": 1
                },
                {
                    "sent": "So this is not something particularly new.",
                    "label": 0
                },
                {
                    "sent": "As you all know, of course.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So the idea is.",
                    "label": 0
                },
                {
                    "sent": "You have a hyperplane and so these are the data points and you would like to classify the plus and the minus data points.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So this some space between these these data points this thing we call the margin.",
                    "label": 0
                },
                {
                    "sent": "I mean not all people here in this room are from the machine learning community, so we use some jargon and.",
                    "label": 0
                },
                {
                    "sent": "Margin if I say margin then this this thing here OK.",
                    "label": 1
                },
                {
                    "sent": "So for hyperplanes in Canonical form, so the equation for the hyperplane would be W which is the norm vector to the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Times X is the data, plus BB is the offset.",
                    "label": 0
                },
                {
                    "sent": "And then we take the sign of it and we get the classification.",
                    "label": 0
                },
                {
                    "sent": "If we have this hyperplane classifier.",
                    "label": 0
                },
                {
                    "sent": "So Canonical form means that we remove the scaling freedom.",
                    "label": 1
                },
                {
                    "sent": "So in principle here we could, you know multiply this by 100,000 and B also by 100,000 we get the same solution.",
                    "label": 0
                },
                {
                    "sent": "So we just by by taking this.",
                    "label": 0
                },
                {
                    "sent": "Form into account we remove the scaling freedom.",
                    "label": 0
                },
                {
                    "sent": "Just set the scale arbitrarily to some things so we know what is the optimal generalizing hyperplane classifier that that is well known from the 60s of 1960s.",
                    "label": 0
                },
                {
                    "sent": "So basically the larger the margin that we have.",
                    "label": 0
                },
                {
                    "sent": "The better it is for our generalization.",
                    "label": 0
                },
                {
                    "sent": "And we can just compute.",
                    "label": 0
                },
                {
                    "sent": "This is a background of an envelope computation.",
                    "label": 0
                },
                {
                    "sent": "How large is the margin?",
                    "label": 0
                },
                {
                    "sent": "So it's basically inversely proportional to the norm of the hyperplane vector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we have a large hyperplane norm vector then then this is a.",
                    "label": 0
                },
                {
                    "sent": "You know small margin.",
                    "label": 0
                },
                {
                    "sent": "And if it's a small non large mansion, OK, so.",
                    "label": 0
                },
                {
                    "sent": "The optimal generalizing hyperplane has the maximum margin.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For these hyperplanes in Canonical form.",
                    "label": 1
                },
                {
                    "sent": "But Nick is able to compute the VC dimension and it's basically R-squared.",
                    "label": 0
                },
                {
                    "sent": "So R is the ball in which the data lies.",
                    "label": 0
                },
                {
                    "sent": "W is the norm of the hyperplane vector.",
                    "label": 0
                },
                {
                    "sent": "A + 1 and this is just N + 1.",
                    "label": 0
                },
                {
                    "sent": "And is the dimensionality of the space OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "If if W is not not small, then we get N + 1, which is very bad OK?",
                    "label": 0
                },
                {
                    "sent": "So typically W squared is much smaller than this side.",
                    "label": 0
                },
                {
                    "sent": "OK, so in fact what we see is that the VC dimension of a Canonical hyperplane classifier is basically.",
                    "label": 0
                },
                {
                    "sent": "Smallest if the if it.",
                    "label": 0
                },
                {
                    "sent": "If the system has a large margin or a small norm right?",
                    "label": 0
                },
                {
                    "sent": "So basically the VC result states the same as the results known from the 60s.",
                    "label": 0
                },
                {
                    "sent": "And now we can.",
                    "label": 0
                },
                {
                    "sent": "We can actually plug this result into the theorem of Vatnik and then we could in principle at least decide what is the best generalizing hyperplane classifier OK?",
                    "label": 0
                },
                {
                    "sent": "So again to put this.",
                    "label": 0
                },
                {
                    "sent": "To state this very briefly, maximum margin means small W squared means good generalization error, meaning no low risk via this.",
                    "label": 0
                },
                {
                    "sent": "So all we have to do is optimize this thing here.",
                    "label": 0
                },
                {
                    "sent": "Subject to classifying correctly.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, this is somewhat independent of the dimensionality of the space, because this got lost in the mean time so.",
                    "label": 1
                },
                {
                    "sent": "So I have to turn this off, sorry, forgot it.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "M. OK, independent of the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "City of the space.",
                    "label": 0
                },
                {
                    "sent": "So so the VC is so the support vector world makes some.",
                    "label": 1
                },
                {
                    "sent": "Take some mileage out of that.",
                    "label": 0
                },
                {
                    "sent": "AM.",
                    "label": 0
                },
                {
                    "sent": "Because if you are anyway independent of the dimensionality of the space, you can just.",
                    "label": 0
                },
                {
                    "sent": "You might as well just blow up the space to something infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Say you have extra data.",
                    "label": 0
                },
                {
                    "sent": "You map this into some feature space which is much larger and larger than the original space, and then you have to solve a problem here in this feature space so.",
                    "label": 1
                },
                {
                    "sent": "Basically you have to classify all these pairs in the feature space correctly.",
                    "label": 0
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "The we now not don't construct the function F which we did in the beginning, but it's rather a concatenation of some F~ and this mapping, right?",
                    "label": 0
                },
                {
                    "sent": "So usually we always thought that this.",
                    "label": 0
                },
                {
                    "sent": "Is actually harder because suddenly the data lives in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But under some conditions this can be simpler.",
                    "label": 0
                },
                {
                    "sent": "And what are the conditions?",
                    "label": 0
                },
                {
                    "sent": "And this was on the last slide.",
                    "label": 0
                },
                {
                    "sent": "So the conditions are basically.",
                    "label": 1
                },
                {
                    "sent": "Within this inequality.",
                    "label": 0
                },
                {
                    "sent": "So if this file this mapping is chosen that we can have a small training error and a low VC dimension.",
                    "label": 1
                },
                {
                    "sent": "Then we we can prove that.",
                    "label": 0
                },
                {
                    "sent": "We can guarantee good generalization.",
                    "label": 1
                },
                {
                    "sent": "So in other words, it's the complexity that matters and not the dimensionality of the space.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very abstract, but all of us have been doing this.",
                    "label": 0
                },
                {
                    "sent": "This kind of things.",
                    "label": 0
                },
                {
                    "sent": "Blowing up our data, for example.",
                    "label": 0
                },
                {
                    "sent": "I give a very brief and simple example, so we want to classify between the blue points and the red points.",
                    "label": 0
                },
                {
                    "sent": "And of course, just eyeballing this, you take an ellipse and that solves the problem, right?",
                    "label": 0
                },
                {
                    "sent": "So now we take a mapping which is from R2 to R3 just.",
                    "label": 0
                },
                {
                    "sent": "Increasing the dimensionality of the data.",
                    "label": 0
                },
                {
                    "sent": "But taking now polynomials to the axis OK then all of a sudden this becomes linearly separable.",
                    "label": 0
                },
                {
                    "sent": "OK, in this space we can take a linear separate separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "We know how to optimally generalize.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In this space, so we know in this space how to optimally generalize with something simple linear.",
                    "label": 0
                },
                {
                    "sent": "And in this space here we have something nonlinear with which corresponds to the linear curve.",
                    "label": 0
                },
                {
                    "sent": "Because this is just equivalent to some ellipsoid in the original space.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we know on the right side how to do optimal, and this is also optimal here.",
                    "label": 0
                },
                {
                    "sent": "The thing that should be worrying us is of course, how to compute this into the how to compute scalar products here, because we might as well not map this into a 3 dimensional space, but into an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So how can we compute scalar products in an infinite dimensional space without an infinite amount of time?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's where.",
                    "label": 0
                },
                {
                    "sent": "The kernel trick comes into the game because you can convince yourself that you can compute the scalar product easily by computing the scalar product in the original space and applying some nonlinearity to it.",
                    "label": 1
                },
                {
                    "sent": "For this particular example.",
                    "label": 1
                },
                {
                    "sent": "And we call this the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "And of course this doesn't work for all such mappings, but only for particular ones.",
                    "label": 1
                },
                {
                    "sent": "In particular, for, this works only for so-called Mercer kernels because.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can then.",
                    "label": 0
                },
                {
                    "sent": "Represent.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "The kernel is Mercer, IE fulfills this positivity condition, then we can.",
                    "label": 1
                },
                {
                    "sent": "Basically have a representation in expansion and define, defy and basically get the kernel trick right there.",
                    "label": 1
                },
                {
                    "sent": "So and.",
                    "label": 0
                },
                {
                    "sent": "For those of you who like reproducing kernel Hilbert spaces, this is like a reproducing kernel, but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are typical kernels, polynomial kernels, RBF, kernel, splines, quadrics?",
                    "label": 0
                },
                {
                    "sent": "Kernels.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Respond to certain regulation properties.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "Basically very.",
                    "label": 0
                },
                {
                    "sent": "Brief wrap up of of VC theory and some SVM ideas.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I have been telling this story many times and I always find that it's quite interesting in the end of the day that these machines actually work in practice.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have 1000 dimensional space where the data lives and you have 50 data points.",
                    "label": 0
                },
                {
                    "sent": "You classify with some SVM with some kernel and you get something that really generalizes well, makes great predictions.",
                    "label": 0
                },
                {
                    "sent": "So if I look for an explanation why this is really working so well, I cannot find it in this theory, I cannot find it within these bounds because the bounds are just general statements of how learning works.",
                    "label": 0
                },
                {
                    "sent": "But it is.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell me for my particular example, why why this actually works?",
                    "label": 0
                },
                {
                    "sent": "And the idea of this talk is actually to explain to contributes, so to say.",
                    "label": 0
                },
                {
                    "sent": "What I think is a missing part of the theory.",
                    "label": 0
                },
                {
                    "sent": "Why are SVM's actually work or any kind of kernel methods in fact?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, what we know so far what I've been telling you is, is that we have to trade off the error versus the complexity and the bound measures that.",
                    "label": 0
                },
                {
                    "sent": "And we also know that the VC dimension is independent of the space dimensionality of the space, and we have some introduced some kernels that preprocess the data too.",
                    "label": 1
                },
                {
                    "sent": "To increase the discriminative power so the question is really.",
                    "label": 0
                },
                {
                    "sent": "So what what is the interesting?",
                    "label": 0
                },
                {
                    "sent": "What is the important contribution in in kernel machines, is it?",
                    "label": 0
                },
                {
                    "sent": "That we have these VC bounds or is it the kernel itself or what is it?",
                    "label": 0
                },
                {
                    "sent": "OK, and we're trying to converge to finding an explanation and make you will do that, but in a couple of minutes.",
                    "label": 1
                },
                {
                    "sent": "So how can we realize a large margin which we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, is good for.",
                    "label": 0
                },
                {
                    "sent": "Classification so so far the picture has been basically not considering the labels very much.",
                    "label": 0
                },
                {
                    "sent": "So if we have been mapping our X into some high dimensional space, we know that we need some complexity control.",
                    "label": 0
                },
                {
                    "sent": "We know that since VM's work that there's some low complexity, but what?",
                    "label": 0
                },
                {
                    "sent": "Is there any link to the labels?",
                    "label": 0
                },
                {
                    "sent": "And this is what we're working on.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I will be now very fast.",
                    "label": 0
                },
                {
                    "sent": "So first of all, many of you may know the kernel PCA, which is a nonlinear version of the original PCA.",
                    "label": 1
                },
                {
                    "sent": "So you basically the idea is that instead of doing PCA in the original space, we map into some appropriate feature space.",
                    "label": 0
                },
                {
                    "sent": "And then do something linear there and.",
                    "label": 0
                },
                {
                    "sent": "In the original space we get nonlinear principle component analysis decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm just.",
                    "label": 0
                },
                {
                    "sent": "Briefly, scanning through this, So what you need for that is you map the data into the feature space.",
                    "label": 1
                },
                {
                    "sent": "Then you solve an eye problem there and basically you get an equation which is very similar to the original 1.",
                    "label": 0
                },
                {
                    "sent": "Anne should put put here.",
                    "label": 0
                },
                {
                    "sent": "And maybe just to show that the trick again.",
                    "label": 0
                },
                {
                    "sent": "So we know that the solutions to the kernel PCA should be we should be able to expand this in terms of the data.",
                    "label": 1
                },
                {
                    "sent": "And so we're only looking into this parameter space that's Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actors that you know saw on these slides.",
                    "label": 0
                },
                {
                    "sent": "We have to know.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Realize this and the good thing is that.",
                    "label": 0
                },
                {
                    "sent": "Centering projecting everything is possible with current.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Knowing that.",
                    "label": 0
                },
                {
                    "sent": "We now know the kernel PCA basis and we can take this kernel PCA basis.",
                    "label": 1
                },
                {
                    "sent": "Which basically describes everything that the kernel is about in terms of the data.",
                    "label": 1
                },
                {
                    "sent": "And construct something which is called empirical kernel map.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is this is also old.",
                    "label": 0
                },
                {
                    "sent": "I mean I think we coined the term empirical kernel map 99, so we should cook for smaller.",
                    "label": 0
                },
                {
                    "sent": "So, so this is now the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Now the kernel matrix is decomposed like that.",
                    "label": 1
                },
                {
                    "sent": "So this is a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Then this is basically the empirical kernel map, so we're just.",
                    "label": 0
                },
                {
                    "sent": "Projecting to this decomposition right?",
                    "label": 0
                },
                {
                    "sent": "So that FF transposed now becomes K again, OK?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "Take a picture of F. OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "We have 40 data points.",
                    "label": 0
                },
                {
                    "sent": "So this means that we have 40 kernel PCA components, OK?",
                    "label": 1
                },
                {
                    "sent": "And also we have.",
                    "label": 0
                },
                {
                    "sent": "This is the number of data points OK?",
                    "label": 1
                },
                {
                    "sent": "Now you see which I just ordered them.",
                    "label": 0
                },
                {
                    "sent": "These are the ones, and these are the 7th from optical character recognition.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this a little bit, there's something happening here.",
                    "label": 0
                },
                {
                    "sent": "There's something happening here and that not a lot.",
                    "label": 0
                },
                {
                    "sent": "Furthermore OK.",
                    "label": 0
                },
                {
                    "sent": "So the very.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's become smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "There's some proofs due to real and to meet you and two others also.",
                    "label": 0
                },
                {
                    "sent": "John Troy Taylor contributed to that.",
                    "label": 0
                },
                {
                    "sent": "So saying that.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Eigenvalues of the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "Anne, stay concentrated.",
                    "label": 0
                },
                {
                    "sent": "So, so they are actually concentrated around their asymptotical values.",
                    "label": 0
                },
                {
                    "sent": "Even for small N. And also this is to do to jail.",
                    "label": 0
                },
                {
                    "sent": "So the differences in the tail bounds are not so great.",
                    "label": 0
                },
                {
                    "sent": "It's not only that they are concentrating, so the errors that we make are proportional to their icon.",
                    "label": 0
                },
                {
                    "sent": "Value so it's not that we do 10% error.",
                    "label": 0
                },
                {
                    "sent": "But if we do it according to the value of the eye.",
                    "label": 0
                },
                {
                    "sent": "K. I .1 error say.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So no.",
                    "label": 0
                },
                {
                    "sent": "So far we've been looking at.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Colonel principle components, so just that variances.",
                    "label": 0
                },
                {
                    "sent": "And now I want to look at something else which is labels.",
                    "label": 0
                },
                {
                    "sent": "So remember, these use are the kernel PCA basis.",
                    "label": 0
                },
                {
                    "sent": "OK, so we just computing so UI is the ice eigen vector of the kernel PCA basis and we are.",
                    "label": 1
                },
                {
                    "sent": "Projecting to the labels Y.",
                    "label": 0
                },
                {
                    "sent": "This is SI.",
                    "label": 0
                },
                {
                    "sent": "So basically we're asking how much is this particular?",
                    "label": 0
                },
                {
                    "sent": "I think I can.",
                    "label": 0
                },
                {
                    "sent": "Factor contributing to explaining the data.",
                    "label": 0
                },
                {
                    "sent": "OK. And we could also ask, well, how if we take M of those eigenvectors, all MM of them?",
                    "label": 0
                },
                {
                    "sent": "How do all these M. Guys contribute to the explanation of the.",
                    "label": 0
                },
                {
                    "sent": "Of the labels.",
                    "label": 0
                },
                {
                    "sent": "And so the hope would be that we get some small M's.",
                    "label": 0
                },
                {
                    "sent": "So very few components actually responsible for explaining.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Principle we should remember so in.",
                    "label": 0
                },
                {
                    "sent": "Why the heck should this be the case?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "We look at just the variances, then clearly the direction of largest variance would not do any any good for classification here.",
                    "label": 0
                },
                {
                    "sent": "So, but the hope would be still that.",
                    "label": 0
                },
                {
                    "sent": "The largest variances would contribute to the classification.",
                    "label": 0
                },
                {
                    "sent": "So that's the hope.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we look at the same picture that you saw before.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "If the empirical kernel PCA matrix.",
                    "label": 1
                },
                {
                    "sent": "And this is the projection to the labels.",
                    "label": 0
                },
                {
                    "sent": "So you see that this component not the first, but the second component contributes most.",
                    "label": 0
                },
                {
                    "sent": "Everything else is basically decaying, so that's an empirical finding, and the hope is that this empirical fine.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can also be.",
                    "label": 0
                },
                {
                    "sent": "Proved in theory.",
                    "label": 0
                },
                {
                    "sent": "And this is what Mickey is going to do in the next minute.",
                    "label": 0
                },
                {
                    "sent": "So so the the idea is really to say it is the.",
                    "label": 0
                },
                {
                    "sent": "Very few of the kernel PCA components contain all variants necessary to explain the classification or regression.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Cancel thanks a lot for the first part clause.",
                    "label": 0
                },
                {
                    "sent": "Right so and of course, so the question is, is there a theoretical result?",
                    "label": 0
                },
                {
                    "sent": "And of course this theoretical result.",
                    "label": 1
                },
                {
                    "sent": "Otherwise you wouldn't be here.",
                    "label": 0
                },
                {
                    "sent": "So actually, So what?",
                    "label": 0
                },
                {
                    "sent": "I'm trying to explain how you can prove that under certain conditions on the fitness between the kernel and your data, all the relevant information about the labels is actually contained in the first few kernel PCA directions, right?",
                    "label": 0
                },
                {
                    "sent": "So irrespective of the dimensionality of the feature space itself, so the feature space can be infinite dimensional, But the relevant information about the learning problem is really contained in the first few direction.",
                    "label": 1
                },
                {
                    "sent": "Right, so for example.",
                    "label": 0
                },
                {
                    "sent": "So this is a regression data set.",
                    "label": 0
                },
                {
                    "sent": "The window sync function, right?",
                    "label": 0
                },
                {
                    "sent": "There's a bit of noise.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you look at the function you try to learn and the noise and you separate them and then you plot their how they contributed to kernel PCA directions when you see that the interesting part, So what you really actually want to learn is containing the first 11 directions.",
                    "label": 0
                },
                {
                    "sent": "And the rest is really just noise, right?",
                    "label": 0
                },
                {
                    "sent": "So this is very important for this whole.",
                    "label": 0
                },
                {
                    "sent": "How can learning with kernels in infinite dimensional spaces really?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Very briefly, so the whole idea is based on defining what relevant information means, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm saying the relevant information is contained.",
                    "label": 1
                },
                {
                    "sent": "Then copy, say, but we try to define this in a very generic way, which can be used both for classification regression.",
                    "label": 0
                },
                {
                    "sent": "And then the whole result in the end is just some sort of approximation result, so we will first go to the asymptotic setting right of having infinitely many data points or just knowing the probability distribution where it's very easy to see.",
                    "label": 0
                },
                {
                    "sent": "Why and when the data will be concentrated in the first few directions and then we show that there is we can derive a bound to show that this same thing also already holds for in the finite sample setting.",
                    "label": 1
                },
                {
                    "sent": "So for really like 100 samples or thousand samples and then the last step is to also consider how noise will contribute to the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the labels.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So he said, so we were in some regression or classification settings.",
                    "label": 0
                },
                {
                    "sent": "So this can be treated in the same way and the basic idea is that the labels which we see is giving by by sampling some unknown function G which we actually want to learn an additive noise, right?",
                    "label": 0
                },
                {
                    "sent": "So basically it says the output which we see the wiser given this some smooth part and some noise.",
                    "label": 1
                },
                {
                    "sent": "And when you do this, then sorry for regression, which you take the two norm.",
                    "label": 0
                },
                {
                    "sent": "It's very clear that the G is just given as the.",
                    "label": 0
                },
                {
                    "sent": "That's the conditional expectation of the Y given the X.",
                    "label": 0
                },
                {
                    "sent": "So this sort of say the mean label, which you would expect a certain point.",
                    "label": 0
                },
                {
                    "sent": "Right and for regression, right?",
                    "label": 0
                },
                {
                    "sent": "This is really just a function you try to learn.",
                    "label": 0
                },
                {
                    "sent": "And for classification this boils down to two times the probability that you have Class 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "Like like this, some alright?",
                    "label": 0
                },
                {
                    "sent": "So this is the these are the class densities, two Gaussians and these are this is the.",
                    "label": 0
                },
                {
                    "sent": "So to say the class posterior for the left of these two, and then you scale it such to minus 1 + 1, right?",
                    "label": 0
                },
                {
                    "sent": "So this would then be the relevant information for the classification problem, and you can already see that you should just look at the sign of this relevant information.",
                    "label": 1
                },
                {
                    "sent": "Then you can also.",
                    "label": 0
                },
                {
                    "sent": "Then you get the optimal decision boundaries between the two classes, right?",
                    "label": 1
                },
                {
                    "sent": "So the relevant information really is the something like the smooth, smooth, denoised version of the labels which still contains all the information which you need to learn well.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your data.",
                    "label": 0
                },
                {
                    "sent": "Right, and so in order to study it, we want to somehow first think about what happens if a lot of data and then say if if you can somehow relate this Antarctic settings.",
                    "label": 0
                },
                {
                    "sent": "A finite sample setting.",
                    "label": 0
                },
                {
                    "sent": "So what is the finite sample setting?",
                    "label": 1
                },
                {
                    "sent": "So there are certain.",
                    "label": 1
                },
                {
                    "sent": "Wait so we have this kernel matrix K. Right, and this defines us a linear operator on vectors.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just a matrix, right?",
                    "label": 0
                },
                {
                    "sent": "But also in functions, so we could plug for this VJ.",
                    "label": 0
                },
                {
                    "sent": "We could plug some sample functions, sample vectors of some functions in there, and then we could place this XI with a free variable and then we really get a mapping from functions to functions which is defined by this kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Right and the this operator has eigenvectors which are also the kernel PCA components of which we talked before.",
                    "label": 1
                },
                {
                    "sent": "And the contribution of these component of so the computer components just eigenvector is again given by this by the scalar product here, which you also saw before.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so and if you can all get more and more data right this.",
                    "label": 0
                },
                {
                    "sent": "This sum here will converge to an integral operator.",
                    "label": 1
                },
                {
                    "sent": "Of course.",
                    "label": 1
                },
                {
                    "sent": "Where we integrate against the probability distribution which generated or training points.",
                    "label": 1
                },
                {
                    "sent": "The Islanders converge to eigenfunctions, so I hear.",
                    "label": 1
                },
                {
                    "sent": "And this contribution converge to the scalar product with the eigenfunctions.",
                    "label": 0
                },
                {
                    "sent": "Right, so in the automatic setting basically, so we are now not talking about single points, but actually about.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just summary so much.",
                    "label": 0
                },
                {
                    "sent": "I hope you are.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's so hard actually.",
                    "label": 0
                },
                {
                    "sent": "OK, and so now the interesting question is.",
                    "label": 0
                },
                {
                    "sent": "In this entotic setting, what are reasonable, reasonable assumptions such that is relevant function?",
                    "label": 0
                },
                {
                    "sent": "This way of information you want to learn this this function G. Is really contained in the space which is spent by the first few of these eigenfunctions right?",
                    "label": 0
                },
                {
                    "sent": "In a very natural assumption is to say that this G can absolutely be represented by this integral operator, TK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, which I write this so I mean which basically just means that this function G lies in the in the range of the TK here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, whoops.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, there exists some function H such as GST, KH and then I can just write everything down in the with respect to the eigen decomposition and I get this G has a series expansion in terms of the eigenfunctions also integrate operator which reads like this.",
                    "label": 0
                },
                {
                    "sent": "Right and here of course, the Alpha is a bounded sequence, so I get that these contributions are scalar product between G and the eigenfunctions.",
                    "label": 0
                },
                {
                    "sent": "Just decays as quickly as the eigenvalues of the of the kernel.",
                    "label": 1
                },
                {
                    "sent": "Right and typically so this depends on the smoothness of the kernel.",
                    "label": 0
                },
                {
                    "sent": "But since we want to do something useful, we usually have smooth kernels.",
                    "label": 0
                },
                {
                    "sent": "So this means that this function that this coefficients actually decay quite quickly.",
                    "label": 0
                },
                {
                    "sent": "And it isn't otic.",
                    "label": 0
                },
                {
                    "sent": "Setting G is really contained in this space, which is spent by the first few eigenfunctions.",
                    "label": 0
                },
                {
                    "sent": "Alpha cell phone.",
                    "label": 0
                },
                {
                    "sent": "Do I get I get this here so this exists some function H right which has finite known and then so the length of H. So since this is a.",
                    "label": 0
                },
                {
                    "sent": "So saying.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I know there was.",
                    "label": 0
                },
                {
                    "sent": "So this sounds like a basis, right?",
                    "label": 0
                },
                {
                    "sent": "So then H as this is off.",
                    "label": 0
                },
                {
                    "sent": "I responded here, it's actually it's a square summable.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so I mean the idea is that you if you take the currency is she lots and lots of later than that.",
                    "label": 0
                },
                {
                    "sent": "Then eventually you can actually represent what you are trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Use this kernel right?",
                    "label": 0
                },
                {
                    "sent": "So this is really not too much of it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Option.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the question is of course does.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are interested in.",
                    "label": 0
                },
                {
                    "sent": "And showing that that this rate here, right?",
                    "label": 0
                },
                {
                    "sent": "So these scalar products, they should decay as I look at it later components they should decay with the rate of the eigenvalues or eigenvalues of the kernel and the question is if this also holds for this finite.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setting.",
                    "label": 0
                },
                {
                    "sent": "OK, and actually this is so this is the source of the result.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that this G has this form here.",
                    "label": 0
                },
                {
                    "sent": "Right, and we look at the sample vector, then with high probability right?",
                    "label": 1
                },
                {
                    "sent": "So this is something like this.",
                    "label": 0
                },
                {
                    "sent": "Probability minus one minus Delta where it's all been suppressed here for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Scalar product between the eyes eigenvector of the kernel matrix and the center vector can be bounded by by this long bond here.",
                    "label": 0
                },
                {
                    "sent": "And the most important things have been highlighted in red, right?",
                    "label": 0
                },
                {
                    "sent": "So it actually consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part, which depends on the eigenvalue of the kernel matrix, right?",
                    "label": 1
                },
                {
                    "sent": "So this part goes to zero quickly.",
                    "label": 0
                },
                {
                    "sent": "And then I have lots of lots of additional error terms here, but which are all small.",
                    "label": 0
                },
                {
                    "sent": "OK, so this whole bound also depends on R, which I come later, but basically as R goes to Infinity, all of these terms go to zero very quickly.",
                    "label": 0
                },
                {
                    "sent": "For example, this Lambda are here.",
                    "label": 1
                },
                {
                    "sent": "This is just the tail sum of eigenvalues smaller than Lambda.",
                    "label": 0
                },
                {
                    "sent": "Under R. So these are very small and these functions also occur from some translation operations.",
                    "label": 1
                },
                {
                    "sent": "Right, So what I get is that the contribution.",
                    "label": 0
                },
                {
                    "sent": "Of this relevant information.",
                    "label": 0
                },
                {
                    "sent": "To the kernel, PCA components decays essentially as quickly as the eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I tried to give a give and give an idea how this how this is proven very quickly or very not too much detail but the so the main idea what is the main idea right?",
                    "label": 0
                },
                {
                    "sent": "So the I mean initially we had this, this Jesus these are all G these were all infinite expansions.",
                    "label": 0
                },
                {
                    "sent": "So G actually the son went to Infinity and for K we have this formula here which comes from Earth's theorem.",
                    "label": 0
                },
                {
                    "sent": "There's also went to Infinity and the basic idea is first to truncate all these.",
                    "label": 0
                },
                {
                    "sent": "This series so that I really have something which is more less finite dimensional right?",
                    "label": 0
                },
                {
                    "sent": "And then I will treat the truncation error in a very rough manner, and for this finite dimensional objects that can really do apply some linear algebra bounds to bond the error.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "The first thing is OK, so this~ explains the truncation everywhere, so when you see something, some object which has been tilted.",
                    "label": 0
                },
                {
                    "sent": "It has to have some serious expansion.",
                    "label": 0
                },
                {
                    "sent": "It has been truncated.",
                    "label": 0
                },
                {
                    "sent": "So in the first step we replace this G by G~ and then we get this additional truncation error here, right?",
                    "label": 0
                },
                {
                    "sent": "So now we already have I vectors of the full kernel matrix, and this simpler function, and then the next step.",
                    "label": 0
                },
                {
                    "sent": "We also replace these UI by the eigenvectors of the truncated kernel.",
                    "label": 0
                },
                {
                    "sent": "Function so of the curtain matrix using the truncated cone right and then I sort of get like an error estimate here and I end up with.",
                    "label": 0
                },
                {
                    "sent": "I can with the scalar product between sample vectors of the full eigenfunctions and eigenvectors of the kernel matrix for the truncated kernel.",
                    "label": 1
                },
                {
                    "sent": "Function, yes, so this is not something which I can treat very basically.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, I'm very.",
                    "label": 0
                },
                {
                    "sent": "Elementary so basically right?",
                    "label": 0
                },
                {
                    "sent": "So this is just the definition of all the.",
                    "label": 0
                },
                {
                    "sent": "Since L&L is an eigenvalue and use an eigenvector right, this holds.",
                    "label": 0
                },
                {
                    "sent": "And then I replace K here basically.",
                    "label": 0
                },
                {
                    "sent": "By this right?",
                    "label": 0
                },
                {
                    "sent": "So this is sort of like.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I apply this formula here.",
                    "label": 0
                },
                {
                    "sent": "I can also write it down in matrix form between.",
                    "label": 0
                },
                {
                    "sent": "This is something like a multiplication between the sample.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrices which collect this sample vectors of the eigen functions and diagonal matrix which only contains the true eigenvalues right and then I can.",
                    "label": 0
                },
                {
                    "sent": "Sort of multiply by PSI transpose.",
                    "label": 0
                },
                {
                    "sent": "Then I assume that this is invertible, so I get the pseudoinverse here.",
                    "label": 0
                },
                {
                    "sent": "Right, and then if I take norms on both sides here?",
                    "label": 0
                },
                {
                    "sent": "And actually I get an upper bound for this simple scalar product of eigen functions and eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Right and here so this is the basic step where I get the.",
                    "label": 0
                },
                {
                    "sent": "Decay rate, which is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, proportional to the eigenvalue.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in the next step that I have somehow to take care of these these errors, which I get by truncating the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "So here you can apply one of these synthetase theorems, which basically says that the angle between eigenvectors under an additive perturbation is bounded by the.",
                    "label": 1
                },
                {
                    "sent": "The norm of the perturbation and the separation of these two eigenvalues, right?",
                    "label": 1
                },
                {
                    "sent": "But then we will multiply this here also again by the eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So we can actually bound this.",
                    "label": 0
                },
                {
                    "sent": "So using something like the by looking at the size of certain clusters around eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have to take care of the truncation of this function G, But basically this just converges to this term and then we.",
                    "label": 1
                },
                {
                    "sent": "Temperature if we get something like this year.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we have decomposed the thing we wanted to look at, so we end up with eigenfunction center vectors and these sort of a truncated eigenvectors right?",
                    "label": 1
                },
                {
                    "sent": "And we get the already the basic bond which convert which converges like we wished.",
                    "label": 0
                },
                {
                    "sent": "Then we have to also take care of the perturbation of the truncation of the kernel function.",
                    "label": 1
                },
                {
                    "sent": "Then we get this term.",
                    "label": 0
                },
                {
                    "sent": "And then we get another term for the truncation function G and then basically we end up already end up with the term which we wanted to have here, right?",
                    "label": 0
                },
                {
                    "sent": "So so basically, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not so complicated.",
                    "label": 0
                },
                {
                    "sent": "Maybe apart from the formulas, but.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so it's just basic linear algebra.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hollis OK, so let's look at some examples finally.",
                    "label": 0
                },
                {
                    "sent": "We have two functions here.",
                    "label": 0
                },
                {
                    "sent": "Again, the sync function and then we have another function which is cosine times son of 5X, so it's a bit more complex function and then we take an RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian kernel with a certain width and we look at we compute these kernel PCA components and just as predicted by the theorem you can see that the contribution of these kernel PCA components to the smooth functions is actually really concentrated in the first 10 or 20.",
                    "label": 1
                },
                {
                    "sent": "Dimensions.",
                    "label": 0
                },
                {
                    "sent": "Right, so it even makes it makes sense somehow cause this red function is a bit more complex.",
                    "label": 0
                },
                {
                    "sent": "It also needs more components to be represented well.",
                    "label": 0
                },
                {
                    "sent": "Right and everything else here is completely negligible.",
                    "label": 0
                },
                {
                    "sent": "So in principle we have a problem in 100 dimensions, but only twenty of these dimensions are really in.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then the last question is so far we only dealt with the with the noise free case.",
                    "label": 0
                },
                {
                    "sent": "So what happens with the noise, right?",
                    "label": 0
                },
                {
                    "sent": "So we have some noise in here, some IID noise.",
                    "label": 0
                },
                {
                    "sent": "OK, but basically.",
                    "label": 0
                },
                {
                    "sent": "When you compute the scalar product with the eigenvectors because they are talking about you, just computing an orthogonal rotation is something like an orthogonal basis transform.",
                    "label": 0
                },
                {
                    "sent": "And this means if it's noise before that, then it will also be noise after this transformation, right?",
                    "label": 0
                },
                {
                    "sent": "So the noise does not concentrate in this kind of PCA directions, but the noise stays distributed over all of them off the all over the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Components.",
                    "label": 0
                },
                {
                    "sent": "Right, so the red one, so this would be the smooth part.",
                    "label": 0
                },
                {
                    "sent": "This is the noise as you can just search see so the the interesting part in the data is really contained in the leading dimensions and the rest is really spread over all of the data.",
                    "label": 0
                },
                {
                    "sent": "OK. No.",
                    "label": 0
                },
                {
                    "sent": "Good, so the question is of course, so this is theoretically very interesting, right?",
                    "label": 0
                },
                {
                    "sent": "So you say I can really learn something that cause the problem is really just 11 dimensional, and if I have one other data points and do something linear and F 11 dimensional space, then everything is well, but it's actually I mean if it's just a theoretical result, maybe not that interesting, But what we actually want to do is we want to.",
                    "label": 0
                },
                {
                    "sent": "If you have a concrete data set and we choose some kernel, you want to know, how good does this kernel fit the data, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to really.",
                    "label": 0
                },
                {
                    "sent": "Estimate.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this dimension.",
                    "label": 0
                },
                {
                    "sent": "Of the data set to see how good the kernel fits the data.",
                    "label": 0
                },
                {
                    "sent": "Right, and so this is actually possible, very easy manner.",
                    "label": 0
                },
                {
                    "sent": "So usually you don't have the smooth part and the noise separated.",
                    "label": 0
                },
                {
                    "sent": "So when you look at this, so you compute the kernel PCA components which are really just the eigenvectors of the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "You compute the scalar product and then compute what you get is something like this right?",
                    "label": 0
                },
                {
                    "sent": "And you know that the interesting part is somewhere contained within these blue points, but you don't know where.",
                    "label": 0
                },
                {
                    "sent": "OK, but by the theorem now you know that that the interesting contribution will decay very quickly.",
                    "label": 0
                },
                {
                    "sent": "So right?",
                    "label": 0
                },
                {
                    "sent": "So what you can if you see this and you have a nice step like this and you can be pretty sure that all of the relevant information is really contained in these large leading large component in the leading components and the rest is really just noise and can we discard discard all of it?",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're trying to do to find this cut off point is to Esther to fit two Gaussians to these points with two different variances.",
                    "label": 0
                },
                {
                    "sent": "So we both have mean zero and one one variance.",
                    "label": 0
                },
                {
                    "sent": "For the later part and one for the first part.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so in formulas, so both have mean zero for the first dimension, we assume one variance in for this.",
                    "label": 0
                },
                {
                    "sent": "The negative log likelihood isn't proportional to this term here.",
                    "label": 0
                },
                {
                    "sent": "You can compute it very easily.",
                    "label": 0
                },
                {
                    "sent": "And then we choose.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just choose the dimension which minimizes this likelihood.",
                    "label": 0
                },
                {
                    "sent": "I will negative look like so, so in this case you very nicely see OK, it's just 11 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Right so and this actually works very well, so this is very.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple idea, but.",
                    "label": 0
                },
                {
                    "sent": "Works well and you can even use this log likelihood which you get for model selection, right?",
                    "label": 0
                },
                {
                    "sent": "So in this picture, what you see here, these are two datasets.",
                    "label": 0
                },
                {
                    "sent": "Is classification data set in the regression data set and on the X axis you have the kernel grid.",
                    "label": 0
                },
                {
                    "sent": "So we take our requires many different kernels and what you see what you see here in blue.",
                    "label": 0
                },
                {
                    "sent": "This is the test error.",
                    "label": 0
                },
                {
                    "sent": "Right, so this would be so this is the area where you have a good kernel and here you you you.",
                    "label": 0
                },
                {
                    "sent": "I think on this side you over fit in here under fit very severely.",
                    "label": 0
                },
                {
                    "sent": "And what you see in red is the log likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So the optimal log likelihood which you get but then very nicely coincide, right?",
                    "label": 0
                },
                {
                    "sent": "So which somehow means if?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If in this image, if you can really separate the signal from the noise really well.",
                    "label": 0
                },
                {
                    "sent": "Right then we will.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Principle have a good car.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, So what we could also do is you can just take this number of dimensions and project your labels to this dimension.",
                    "label": 0
                },
                {
                    "sent": "Right to denoise to get denoised versions of this layer, and this is actually worth works.",
                    "label": 0
                },
                {
                    "sent": "Can see here.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we computed this number of dimensions for a number of benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "OK. And then we just did an unregularized fit on this on this data set on this denoise data.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the results which you get just as good as if you take a support vector machine on the whole data set.",
                    "label": 0
                },
                {
                    "sent": "So which really proves that we have extracted that information, the relevant information is really contained in this low dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So close right now I'll talk about some more applications of this.",
                    "label": 0
                },
                {
                    "sent": "So this was all on some benchmark data which is available in.",
                    "label": 0
                },
                {
                    "sent": "These are the either datasets.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So we didn't have these ideas for a very long time, so our paper is currently under revision in Jamil Arm, so, But what we have been working on for a very long time is splice site detection, and so we've been going through.",
                    "label": 0
                },
                {
                    "sent": "A couple of iterations over many years to improve the classification rates for supply side detection.",
                    "label": 0
                },
                {
                    "sent": "So spicy.",
                    "label": 0
                },
                {
                    "sent": "So basically you would like to.",
                    "label": 0
                },
                {
                    "sent": "Distinguish between ages that are instrumental as splice.",
                    "label": 0
                },
                {
                    "sent": "Site Santa eggs that are just there but have nothing to do with the splicing process.",
                    "label": 0
                },
                {
                    "sent": "And this is a very simple classification problem.",
                    "label": 0
                },
                {
                    "sent": "You take all the edges that are splice sites and all the edges that are non splice sites take some windows around them.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Code these 80 GS somehow compute kernels and classify.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's the general.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So you could do an E3 coding coding these as 1234, so to say.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "May not be the proper thing because these would be closer together than than these and that this doesn't actually make any sense.",
                    "label": 0
                },
                {
                    "sent": "But you could compute the same thing that you saw all the time in Micco Slide 4 for a kernel that that this is an RBF kernel that takes these such encoded strings, right?",
                    "label": 0
                },
                {
                    "sent": "And you see that.",
                    "label": 0
                },
                {
                    "sent": "Basically, there's 1000 kernel PCA components and you you see that there's a lot of this is quite high dimensional and contains a lot of noise, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Next iteration, so you get a test error of 12.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1212% so now you encode them as such, which is much smarter already.",
                    "label": 0
                },
                {
                    "sent": "That your test error goes down to 7% and you only have a much lower dimensional thing and the noise is also smaller.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You do what we did in recent paper and plus computational biology that appeared in February.",
                    "label": 0
                },
                {
                    "sent": "Which AM.",
                    "label": 0
                },
                {
                    "sent": "Asks whether there are some substrings that match.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a substring made of one letter of two letters, or three letters, or 4 letters, and so on, and you weight them accordingly, OK?",
                    "label": 0
                },
                {
                    "sent": "So maybe the four letters are not so as high weighted as the one letter similarities.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "So here you get a very.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is a very nice modeling and you can also talk a long time about the biological relevance of that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I won't.",
                    "label": 0
                },
                {
                    "sent": "And then just show you this plot so the test error is five, 5.5% and again you see something here.",
                    "label": 1
                },
                {
                    "sent": "Now what I just want to show.",
                    "label": 0
                },
                {
                    "sent": "With this example is.",
                    "label": 0
                },
                {
                    "sent": "So this was a retrospective analysis, so our paper has.",
                    "label": 0
                },
                {
                    "sent": "I mean, even the plus paper had appeared and we took the kernel matrices and asks, you know, in this sense of the theory just presented, can we distinguish between these three cases?",
                    "label": 0
                },
                {
                    "sent": "And what eyeballing these kind of plots makes actually most sense, because we would like to get the lowest dimensionality at the lowest noise rate in principle.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So because in that sense.",
                    "label": 0
                },
                {
                    "sent": "A good kernel tries to.",
                    "label": 0
                },
                {
                    "sent": "Improve the signal to noise ratio at keeping the dimensionality low.",
                    "label": 1
                },
                {
                    "sent": "I mean that is the basic finding of the whole thing.",
                    "label": 0
                },
                {
                    "sent": "So basically forgiven.",
                    "label": 0
                },
                {
                    "sent": "But Colonel basically boils everything down to something low dimensional.",
                    "label": 0
                },
                {
                    "sent": "So the example that I made in the beginning with 50 data points in 1000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So maybe only three or four dimensions of those or 10 might actually be relevant in the end.",
                    "label": 0
                },
                {
                    "sent": "So all these kernel methods, be IT support vector machines or Fisher disk kernel, Fisher Discriminant or Gauss processes or methods that use kernels they actually reduce.",
                    "label": 0
                },
                {
                    "sent": "The data to spaces where you actually get the lowest dimension at the highest signal to noise ratio, so to say for the Buck.",
                    "label": 0
                },
                {
                    "sent": "If you do it right.",
                    "label": 0
                },
                {
                    "sent": "And then of course.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's only 1 remaining problem.",
                    "label": 0
                },
                {
                    "sent": "Of course always that is model selection.",
                    "label": 1
                },
                {
                    "sent": "So what is the good choice of the kernel and just show you this?",
                    "label": 1
                },
                {
                    "sent": "I would like to propose this as a diagnosis tool that we can use in order to get a very nice precise understanding.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so maybe I am slightly running out of time and do I have 5 more minutes or OK so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "So what is the?",
                    "label": 0
                },
                {
                    "sent": "Now for something completely different.",
                    "label": 0
                },
                {
                    "sent": "It's not quite different.",
                    "label": 0
                },
                {
                    "sent": "You see it in the end.",
                    "label": 0
                },
                {
                    "sent": "So brain computer interfacing something that I'm very interested in.",
                    "label": 0
                },
                {
                    "sent": "We have typically brain a measuring device 128 channel, EG, say.",
                    "label": 0
                },
                {
                    "sent": "We extract some features we classify.",
                    "label": 0
                },
                {
                    "sent": "We control a wheelchair computer game, something.",
                    "label": 0
                },
                {
                    "sent": "An and the one of the very.",
                    "label": 0
                },
                {
                    "sent": "Important points in the BCI world is that?",
                    "label": 0
                },
                {
                    "sent": "So first of all, machine learning has contributed to the BCI.",
                    "label": 0
                },
                {
                    "sent": "Research in that.",
                    "label": 0
                },
                {
                    "sent": "No longer the subjects have to train to manipulate their brain signals such that their thoughts can be read out.",
                    "label": 0
                },
                {
                    "sent": "But rather the machine is learning to interpret the thoughts.",
                    "label": 0
                },
                {
                    "sent": "So the training goes from.",
                    "label": 0
                },
                {
                    "sent": "100 hours of subject training to about 15 minutes of subject training.",
                    "label": 0
                },
                {
                    "sent": "15 minutes of subject training so it's a big step forward that machine learning and signal processing has contributed to, however.",
                    "label": 0
                },
                {
                    "sent": "About 1/3 of the population.",
                    "label": 0
                },
                {
                    "sent": "We call BCO illiterate so they are unable to communicate.",
                    "label": 0
                },
                {
                    "sent": "For an obvious reasons and one important question for us is can we see this in their data before we do long studies with them?",
                    "label": 0
                },
                {
                    "sent": "OK, that's a question and now I just Fast forward.",
                    "label": 0
                },
                {
                    "sent": "OK. And it just showed this plot.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On this side you see.",
                    "label": 0
                },
                {
                    "sent": "The offline classification error.",
                    "label": 0
                },
                {
                    "sent": "So this is a very lousy subject here.",
                    "label": 0
                },
                {
                    "sent": "20% errors.",
                    "label": 0
                },
                {
                    "sent": "This is almost 0% error, so you see some blue crosses that are very good and this is the estimate of the.",
                    "label": 0
                },
                {
                    "sent": "Cannot stop the noise level.",
                    "label": 0
                },
                {
                    "sent": "So that's the estimate of Mickey's noise level, not Mickey's noise level, but makers estimate of the noise level of the data of the respective subject.",
                    "label": 0
                },
                {
                    "sent": "So this is you see a number of different subjects and you see that.",
                    "label": 0
                },
                {
                    "sent": "And it's somehow clear if you.",
                    "label": 0
                },
                {
                    "sent": "Have subjects with a low, not a signal.",
                    "label": 0
                },
                {
                    "sent": "I mean with a high signal to noise ratio with a low.",
                    "label": 0
                },
                {
                    "sent": "Noise level, in the sense that it's estimated, then you get very high performance consecutively.",
                    "label": 0
                },
                {
                    "sent": "You get worse performance if the noise level is higher.",
                    "label": 0
                },
                {
                    "sent": "This is somewhat common sense.",
                    "label": 0
                },
                {
                    "sent": "M. What is somewhat counter intuitive is that dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Is high for those guys.",
                    "label": 0
                },
                {
                    "sent": "And low for the guys that cannot communicate.",
                    "label": 0
                },
                {
                    "sent": "And this has to be put with a grain of salt, because we're always, you know, in this plot we always have two measurements.",
                    "label": 0
                },
                {
                    "sent": "Noise level signal to noise level and the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So of course we can, you know.",
                    "label": 0
                },
                {
                    "sent": "So in this case, we say that the dimensionality is low, but the signal to noise ratio is high.",
                    "label": 0
                },
                {
                    "sent": "Sorry as the noise level is high and the dimensionality is low, so I mean it's in principle what this lets us interpret is that we cannot really pinpoint a proper subspace where we can classify because there's mostly mainly noise, there's no real signal in that.",
                    "label": 0
                },
                {
                    "sent": "And if you now plot the kernel width as a function of the dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So this is a very broad simulation for illiterate and for non illiterate so you can see just the difference and so the interesting point of course is can we?",
                    "label": 0
                },
                {
                    "sent": "In the future, and this is very new result from last week, can we in the future actually be used that method in order to test very early on whether or not to pursue a certain subject further and?",
                    "label": 0
                },
                {
                    "sent": "Can we?",
                    "label": 0
                },
                {
                    "sent": "Of course, so usually in brain computer interfacing we do linear classifiers all the time.",
                    "label": 0
                },
                {
                    "sent": "And so for some of the in particular, for some of the illiterates, it seems that a nonlinear classification, as suggested by this method, might make some sense, as suggested by the kernel found by this method.",
                    "label": 0
                },
                {
                    "sent": "Makes makes some sense, so we will be pursuing that in order to actually improve our.",
                    "label": 0
                },
                {
                    "sent": "Transmission rates and for those of you who haven't enjoyed this video, this is just a video of.",
                    "label": 0
                },
                {
                    "sent": "Yeah myself.",
                    "label": 0
                },
                {
                    "sent": "Playing a game of brain Pong by thinking about my right hand, I move the cursor to the right by thinking about my left hand.",
                    "label": 0
                },
                {
                    "sent": "I move the cursor to the left.",
                    "label": 0
                },
                {
                    "sent": "I'm not moving anything, I'm just thinking I'm just imagining the movement of my hands.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In I would be one of the guys where that is very far from an illiterate, but there are many people that never get that far.",
                    "label": 0
                },
                {
                    "sent": "We would like to save our experimenting time, it's it's really a serious experiment for every subject we need to put the G cap and we need to do some preparations, some training and so on, so forth.",
                    "label": 0
                },
                {
                    "sent": "So anyway, with this very.",
                    "label": 0
                },
                {
                    "sent": "Practical aspect.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to.",
                    "label": 0
                },
                {
                    "sent": "Close, I think we have started to shed some some light in this direction, so I think this we gave some proof and some intuition and some practical evidence on practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "With which we can see why kernel methods would work very well.",
                    "label": 1
                },
                {
                    "sent": "They make very economical use of the dimensionality of the kernel space.",
                    "label": 1
                },
                {
                    "sent": "And I think that is a very interesting insight, and I think it's also something that is an independent type of explanation from the usual VC picture and I found it somewhat interesting myself.",
                    "label": 0
                },
                {
                    "sent": "Thank you for you.",
                    "label": 0
                }
            ]
        }
    }
}