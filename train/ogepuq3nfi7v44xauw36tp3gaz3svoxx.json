{
    "id": "ogepuq3nfi7v44xauw36tp3gaz3svoxx",
    "title": "Cross Domain Distribution Adaptation via Kernel Mapping",
    "info": {
        "author": [
            "Wei Fan, Baidu, Inc."
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/kdd09_zhong_cddavkm/",
    "segmentation": [
        [
            "Hi Mrs Joyner.",
            "Work with our Hangzhou and gentlemen from science investing in China and my colleague Depetro Lager or leave ashore from IBM and gene come from Montclair New Ver City, New Jersey and Quinn John from Louisiana.",
            "So the OK."
        ],
        [
            "The chapter paper is about transfer learning.",
            "Here is 1 example to show the type of problems we are trying to solve.",
            "Here we have a data from two different domains.",
            "Can see that they look quite different.",
            "One is a 2 circle problem, the others like a two month problem.",
            "Do not only different in the conditional distribution in the shape of the.",
            "Prophete decision boundary also use.",
            "Look at the margin distribution.",
            "The feature one and feature two.",
            "They also quite different, so the question is can we do some mapping in the right space?",
            "We can use the data from one domain to solve the learning problem in the other domain."
        ],
        [
            "Here is the motivation of transparent learning.",
            "OK, so we want to do a test categorization, an application, and on York Times.",
            "So typically we want to get label information also from New York Times.",
            "When you get enough data, you play a model and getting accuracy bout say 85% and that's not bad then reality is you may not be able to get enough labeled information from the domain of interest.",
            "And for example, the only labeled information you have is from right?"
        ],
        [
            "It's.",
            "Then the way using Reuters labeled the data, training a model and apply the model on New York Times articles, you may get a much worse accuracy.",
            "For example, may go down to 64% and that's now."
        ],
        [
            "Good, so the reality is no.",
            "You have application you want to get a Model 1 but you have no enough training data.",
            "You get data from a different domain.",
            "Then you have the dropping accuracy.",
            "So the main challenge in transfer learning is can we do something to make so have some improvement on the learning accuracy using data from."
        ],
        [
            "Different domain.",
            "So here is the motivating example I discussed earlier and the one on the left.",
            "Is the domain we want to train a model to distinguish the red from the blue and the only labeled data we have are the big squares?",
            "Either the blue or the red.",
            "And but in the same time we have a lot of label information from a different domain.",
            "The two more example."
        ],
        [
            "So for this data set, if you apply your SVM, losing the polynomial kernel.",
            "So here is what you will get by using only the labeled data from the domain for interest.",
            "So obviously there are many examples like the error here.",
            "Also the error the grain.",
            "Also the Brown error here.",
            "There's old misclassified, but if you put.",
            "The data from the different domain just put them together without any adjustments and that's what you get.",
            "We get another boundary in the same time.",
            "The shape is different.",
            "There also many examples that are still misclassified, so that's not good."
        ],
        [
            "So that is so, how can we solve this problem?",
            "So in this scenario, both the marginal and condition distribution, two domains are quite different.",
            "So how can we get rid of these differences?",
            "And so can we find a different feature space to map those two problems into a common feature space where they look more similar?",
            "And can we also get rid of those examples in the South Domain path are different from the target domain, so the method we propose is on using a kernel mapping to the feature marginal distribution.",
            "Mapping and we also do a proposed feature selection of instance election approach."
        ],
        [
            "Based on clustering.",
            "So here is the flow of the algorithm.",
            "So first we use the labeled small number of target domain data and we are user data to compute using the kernel marking method, compute a separate feature space.",
            "And then both the in domain and our domain data are mapped into the space.",
            "Then we do a feature select instance selection using clustering methods so.",
            "In this example here, suppose the labeled instances from the target domain and the examples from our domain are mapped into the same space.",
            "Then we run, run a clustering and the clusters label.",
            "We use the majority label from the examples from your target domain.",
            "We only choose those.",
            "A source demand date examples only if the label are the same or the target domain.",
            "Then we build a classifier using the labeled data from the source domain and target domain, and then we this process iterates.",
            "We generate multiple ensembles, then the final prediction will be the voting of the multiple classifiers."
        ],
        [
            "So here is a example show how this works.",
            "So here is the original labeled data of the target domain.",
            "And the red and the blue are.",
            "Those labeled data.",
            "So these are two separate mappings into the kernel space.",
            "And the first mapping only uses the label data from the in domain and the secular 1 uses the label data from in domain.",
            "Plus this selected examples from our domain."
        ],
        [
            "And here is he.",
            "Example of the source domain data, which is different from the in domain you want to classify.",
            "The two more example after the kernel mapping.",
            "That's how it looks like in the kernel space and you can see that it looks more like the two clouds.",
            "And these are the examples using the clustering based approach.",
            "These are example of being chosen to join the learning."
        ],
        [
            "And after the.",
            "Example and sample using two models from one using the mapping from the first mapping space, the other from the second mapping space after the assemble using voting.",
            "Here are the predictions on the source domain that we want to classify on only this green areas are examples we have misclassified."
        ],
        [
            "So there are some more properties we have shown in the paper and first of all we can show we show that the kernel mapping can reduce the difference in marginal distribution between the source and target domain and we show that the after the kernel mapping the both domains look somehow like ocean.",
            "Then when we have a sufficient number of labeled data from the source domain than their differences can be bounded.",
            "And we also show that using the cloud between assumption and basis based on the the one examples in the same marginal distribution should have a similar conditional distribution.",
            "We also bound the differences in the conditional distribution.",
            "And also in the paper.",
            "Found the.",
            "Found the errors of trying to transfer the assault and in the target domain.",
            "Then we further show that using some Balkan reducing the errors even more."
        ],
        [
            "We ran the proposal approach on three sets of data, so Reuters and 20 newsgroup and the skills Cisco Webboard datasets, all of them.",
            "Our high dimension over 1000.",
            "In the 20 newsgroup also, the Reuters data sets the data have a higher level hierarchy like computer and recreation, so the two domains that when domain is taken from one sub category from when the computer from systems and the other label taken from the from support from recreation and the South domain from a different sub category graphic versus auto.",
            "And in the Cisco Web website, and these are the some users rankings, hardcode don't care on some web pages and they have a different categories as well.",
            "So we take one document from one domain, then combined with the other domain also training.",
            "Then we use a another one to be."
        ],
        [
            "Testing so we compared with non transfer learning single classifiers and strengthen learning algorithm, trader Boost and base learners are KNN, SVM and naive Bayes."
        ],
        [
            "And these are.",
            "Summarize results, so we have altogether 9 datasets reported in the paper.",
            "So these are using the cane and then as the base classifier can see that.",
            "The two approach approach performed the best.",
            "Either kernel ensemble Colonel W is a kernel assemble but with some waiting on us.",
            "Like Buddha boosting type of weighting measures on each base classifier, so obviously the kernel and sample perform the best compared to other baseline methods.",
            "Both trader boost and single classifiers.",
            "And here is using Naive Bayes as a base learner an have a similar performance as a King and and these are the results of using SVN.",
            "Again, it has a very similar performance in terms in terms of their ranking of different algorithm and so overall the kernel based ensemble have a 20, four wins and three losses."
        ],
        [
            "So here's a quick summary.",
            "So basically domain transfer.",
            "We propose a kernel based approach to transfer knowledge when the both marginal and conditional distributions are quite different.",
            "So the step is first we do a kernel mapping and to bring the marginal distribution between two domains closer.",
            "Then we use a cluster based instance selection approach to choose those examples from our domain that are most likely similar.",
            "Then we'll do the ensemble to reduce error even more.",
            "And the data set and the code are available from our research website.",
            "Thank you.",
            "Is the choice of the kernel dependent on how well the projected data their labels are consistent with the labeled data in the target space?",
            "We make choices now when we do the kernel, we only use the data itself.",
            "We do is not based on the base learners.",
            "OK, right by the performance when you do the classifier dependent on your face on your base models.",
            "But why do that when we do the feature mapping that's independent from the kernel as either SVM were or were different classifier of your preference.",
            "Using Lumia using SVM.",
            "Yeah yeah you had the.",
            "I think that's a common problem, not dependent from this paper, right?",
            "So forever?",
            "Whatever data set will be interesting to look at.",
            "Choice based on the projected data.",
            "So once you protect data into the program based on the class for distribution and the labels of the possible point, because yeah, that's a good question because they use after the mapping in the kernel space.",
            "The data are look like Gaussian away, so if you choose, the kernel can distinguish the data.",
            "Yeah, the cluster is looking more Gaussian that be more more appropriate than 10 and currently is not.",
            "Any?",
            "Susan, New York experimenting to test you.",
            "Some synthetic data set, and I'm wondering if you apply you or make certain some real world applications such as you mentioned example use the web use from router to credit the one news from New York.",
            "Yeah, the three set of data sets reporting the data in the paper are either on the Reuters.",
            "Or 20 newsgroup or the Cisco website and the paper talk about synthetic example just to show the idea how it works, but they reported detailed results are all on the nine web pages classification problem.",
            "Anymore questions.",
            "Or less active speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi Mrs Joyner.",
                    "label": 0
                },
                {
                    "sent": "Work with our Hangzhou and gentlemen from science investing in China and my colleague Depetro Lager or leave ashore from IBM and gene come from Montclair New Ver City, New Jersey and Quinn John from Louisiana.",
                    "label": 0
                },
                {
                    "sent": "So the OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The chapter paper is about transfer learning.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 example to show the type of problems we are trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Here we have a data from two different domains.",
                    "label": 0
                },
                {
                    "sent": "Can see that they look quite different.",
                    "label": 0
                },
                {
                    "sent": "One is a 2 circle problem, the others like a two month problem.",
                    "label": 0
                },
                {
                    "sent": "Do not only different in the conditional distribution in the shape of the.",
                    "label": 0
                },
                {
                    "sent": "Prophete decision boundary also use.",
                    "label": 0
                },
                {
                    "sent": "Look at the margin distribution.",
                    "label": 0
                },
                {
                    "sent": "The feature one and feature two.",
                    "label": 0
                },
                {
                    "sent": "They also quite different, so the question is can we do some mapping in the right space?",
                    "label": 0
                },
                {
                    "sent": "We can use the data from one domain to solve the learning problem in the other domain.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the motivation of transparent learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to do a test categorization, an application, and on York Times.",
                    "label": 0
                },
                {
                    "sent": "So typically we want to get label information also from New York Times.",
                    "label": 1
                },
                {
                    "sent": "When you get enough data, you play a model and getting accuracy bout say 85% and that's not bad then reality is you may not be able to get enough labeled information from the domain of interest.",
                    "label": 0
                },
                {
                    "sent": "And for example, the only labeled information you have is from right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Then the way using Reuters labeled the data, training a model and apply the model on New York Times articles, you may get a much worse accuracy.",
                    "label": 1
                },
                {
                    "sent": "For example, may go down to 64% and that's now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so the reality is no.",
                    "label": 0
                },
                {
                    "sent": "You have application you want to get a Model 1 but you have no enough training data.",
                    "label": 0
                },
                {
                    "sent": "You get data from a different domain.",
                    "label": 0
                },
                {
                    "sent": "Then you have the dropping accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the main challenge in transfer learning is can we do something to make so have some improvement on the learning accuracy using data from.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different domain.",
                    "label": 0
                },
                {
                    "sent": "So here is the motivating example I discussed earlier and the one on the left.",
                    "label": 0
                },
                {
                    "sent": "Is the domain we want to train a model to distinguish the red from the blue and the only labeled data we have are the big squares?",
                    "label": 0
                },
                {
                    "sent": "Either the blue or the red.",
                    "label": 0
                },
                {
                    "sent": "And but in the same time we have a lot of label information from a different domain.",
                    "label": 0
                },
                {
                    "sent": "The two more example.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this data set, if you apply your SVM, losing the polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "So here is what you will get by using only the labeled data from the domain for interest.",
                    "label": 0
                },
                {
                    "sent": "So obviously there are many examples like the error here.",
                    "label": 0
                },
                {
                    "sent": "Also the error the grain.",
                    "label": 0
                },
                {
                    "sent": "Also the Brown error here.",
                    "label": 0
                },
                {
                    "sent": "There's old misclassified, but if you put.",
                    "label": 0
                },
                {
                    "sent": "The data from the different domain just put them together without any adjustments and that's what you get.",
                    "label": 0
                },
                {
                    "sent": "We get another boundary in the same time.",
                    "label": 0
                },
                {
                    "sent": "The shape is different.",
                    "label": 0
                },
                {
                    "sent": "There also many examples that are still misclassified, so that's not good.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that is so, how can we solve this problem?",
                    "label": 0
                },
                {
                    "sent": "So in this scenario, both the marginal and condition distribution, two domains are quite different.",
                    "label": 1
                },
                {
                    "sent": "So how can we get rid of these differences?",
                    "label": 1
                },
                {
                    "sent": "And so can we find a different feature space to map those two problems into a common feature space where they look more similar?",
                    "label": 0
                },
                {
                    "sent": "And can we also get rid of those examples in the South Domain path are different from the target domain, so the method we propose is on using a kernel mapping to the feature marginal distribution.",
                    "label": 0
                },
                {
                    "sent": "Mapping and we also do a proposed feature selection of instance election approach.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based on clustering.",
                    "label": 0
                },
                {
                    "sent": "So here is the flow of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So first we use the labeled small number of target domain data and we are user data to compute using the kernel marking method, compute a separate feature space.",
                    "label": 1
                },
                {
                    "sent": "And then both the in domain and our domain data are mapped into the space.",
                    "label": 0
                },
                {
                    "sent": "Then we do a feature select instance selection using clustering methods so.",
                    "label": 0
                },
                {
                    "sent": "In this example here, suppose the labeled instances from the target domain and the examples from our domain are mapped into the same space.",
                    "label": 0
                },
                {
                    "sent": "Then we run, run a clustering and the clusters label.",
                    "label": 0
                },
                {
                    "sent": "We use the majority label from the examples from your target domain.",
                    "label": 0
                },
                {
                    "sent": "We only choose those.",
                    "label": 0
                },
                {
                    "sent": "A source demand date examples only if the label are the same or the target domain.",
                    "label": 0
                },
                {
                    "sent": "Then we build a classifier using the labeled data from the source domain and target domain, and then we this process iterates.",
                    "label": 1
                },
                {
                    "sent": "We generate multiple ensembles, then the final prediction will be the voting of the multiple classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a example show how this works.",
                    "label": 0
                },
                {
                    "sent": "So here is the original labeled data of the target domain.",
                    "label": 0
                },
                {
                    "sent": "And the red and the blue are.",
                    "label": 0
                },
                {
                    "sent": "Those labeled data.",
                    "label": 0
                },
                {
                    "sent": "So these are two separate mappings into the kernel space.",
                    "label": 0
                },
                {
                    "sent": "And the first mapping only uses the label data from the in domain and the secular 1 uses the label data from in domain.",
                    "label": 0
                },
                {
                    "sent": "Plus this selected examples from our domain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is he.",
                    "label": 0
                },
                {
                    "sent": "Example of the source domain data, which is different from the in domain you want to classify.",
                    "label": 0
                },
                {
                    "sent": "The two more example after the kernel mapping.",
                    "label": 0
                },
                {
                    "sent": "That's how it looks like in the kernel space and you can see that it looks more like the two clouds.",
                    "label": 0
                },
                {
                    "sent": "And these are the examples using the clustering based approach.",
                    "label": 0
                },
                {
                    "sent": "These are example of being chosen to join the learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And after the.",
                    "label": 0
                },
                {
                    "sent": "Example and sample using two models from one using the mapping from the first mapping space, the other from the second mapping space after the assemble using voting.",
                    "label": 0
                },
                {
                    "sent": "Here are the predictions on the source domain that we want to classify on only this green areas are examples we have misclassified.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some more properties we have shown in the paper and first of all we can show we show that the kernel mapping can reduce the difference in marginal distribution between the source and target domain and we show that the after the kernel mapping the both domains look somehow like ocean.",
                    "label": 1
                },
                {
                    "sent": "Then when we have a sufficient number of labeled data from the source domain than their differences can be bounded.",
                    "label": 0
                },
                {
                    "sent": "And we also show that using the cloud between assumption and basis based on the the one examples in the same marginal distribution should have a similar conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "We also bound the differences in the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "And also in the paper.",
                    "label": 0
                },
                {
                    "sent": "Found the.",
                    "label": 0
                },
                {
                    "sent": "Found the errors of trying to transfer the assault and in the target domain.",
                    "label": 0
                },
                {
                    "sent": "Then we further show that using some Balkan reducing the errors even more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We ran the proposal approach on three sets of data, so Reuters and 20 newsgroup and the skills Cisco Webboard datasets, all of them.",
                    "label": 1
                },
                {
                    "sent": "Our high dimension over 1000.",
                    "label": 1
                },
                {
                    "sent": "In the 20 newsgroup also, the Reuters data sets the data have a higher level hierarchy like computer and recreation, so the two domains that when domain is taken from one sub category from when the computer from systems and the other label taken from the from support from recreation and the South domain from a different sub category graphic versus auto.",
                    "label": 0
                },
                {
                    "sent": "And in the Cisco Web website, and these are the some users rankings, hardcode don't care on some web pages and they have a different categories as well.",
                    "label": 0
                },
                {
                    "sent": "So we take one document from one domain, then combined with the other domain also training.",
                    "label": 0
                },
                {
                    "sent": "Then we use a another one to be.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Testing so we compared with non transfer learning single classifiers and strengthen learning algorithm, trader Boost and base learners are KNN, SVM and naive Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are.",
                    "label": 0
                },
                {
                    "sent": "Summarize results, so we have altogether 9 datasets reported in the paper.",
                    "label": 0
                },
                {
                    "sent": "So these are using the cane and then as the base classifier can see that.",
                    "label": 0
                },
                {
                    "sent": "The two approach approach performed the best.",
                    "label": 0
                },
                {
                    "sent": "Either kernel ensemble Colonel W is a kernel assemble but with some waiting on us.",
                    "label": 0
                },
                {
                    "sent": "Like Buddha boosting type of weighting measures on each base classifier, so obviously the kernel and sample perform the best compared to other baseline methods.",
                    "label": 0
                },
                {
                    "sent": "Both trader boost and single classifiers.",
                    "label": 0
                },
                {
                    "sent": "And here is using Naive Bayes as a base learner an have a similar performance as a King and and these are the results of using SVN.",
                    "label": 0
                },
                {
                    "sent": "Again, it has a very similar performance in terms in terms of their ranking of different algorithm and so overall the kernel based ensemble have a 20, four wins and three losses.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a quick summary.",
                    "label": 0
                },
                {
                    "sent": "So basically domain transfer.",
                    "label": 0
                },
                {
                    "sent": "We propose a kernel based approach to transfer knowledge when the both marginal and conditional distributions are quite different.",
                    "label": 1
                },
                {
                    "sent": "So the step is first we do a kernel mapping and to bring the marginal distribution between two domains closer.",
                    "label": 0
                },
                {
                    "sent": "Then we use a cluster based instance selection approach to choose those examples from our domain that are most likely similar.",
                    "label": 1
                },
                {
                    "sent": "Then we'll do the ensemble to reduce error even more.",
                    "label": 0
                },
                {
                    "sent": "And the data set and the code are available from our research website.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Is the choice of the kernel dependent on how well the projected data their labels are consistent with the labeled data in the target space?",
                    "label": 0
                },
                {
                    "sent": "We make choices now when we do the kernel, we only use the data itself.",
                    "label": 0
                },
                {
                    "sent": "We do is not based on the base learners.",
                    "label": 0
                },
                {
                    "sent": "OK, right by the performance when you do the classifier dependent on your face on your base models.",
                    "label": 0
                },
                {
                    "sent": "But why do that when we do the feature mapping that's independent from the kernel as either SVM were or were different classifier of your preference.",
                    "label": 0
                },
                {
                    "sent": "Using Lumia using SVM.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah you had the.",
                    "label": 0
                },
                {
                    "sent": "I think that's a common problem, not dependent from this paper, right?",
                    "label": 0
                },
                {
                    "sent": "So forever?",
                    "label": 0
                },
                {
                    "sent": "Whatever data set will be interesting to look at.",
                    "label": 0
                },
                {
                    "sent": "Choice based on the projected data.",
                    "label": 0
                },
                {
                    "sent": "So once you protect data into the program based on the class for distribution and the labels of the possible point, because yeah, that's a good question because they use after the mapping in the kernel space.",
                    "label": 0
                },
                {
                    "sent": "The data are look like Gaussian away, so if you choose, the kernel can distinguish the data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the cluster is looking more Gaussian that be more more appropriate than 10 and currently is not.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "Susan, New York experimenting to test you.",
                    "label": 0
                },
                {
                    "sent": "Some synthetic data set, and I'm wondering if you apply you or make certain some real world applications such as you mentioned example use the web use from router to credit the one news from New York.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the three set of data sets reporting the data in the paper are either on the Reuters.",
                    "label": 0
                },
                {
                    "sent": "Or 20 newsgroup or the Cisco website and the paper talk about synthetic example just to show the idea how it works, but they reported detailed results are all on the nine web pages classification problem.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Or less active speaker.",
                    "label": 0
                }
            ]
        }
    }
}