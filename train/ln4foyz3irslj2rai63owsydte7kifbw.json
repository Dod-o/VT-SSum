{
    "id": "ln4foyz3irslj2rai63owsydte7kifbw",
    "title": "Analyzing Word Frequencies in Large Text Corpora using Inter-arrival Times and Bootstrapping",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Jefrey Lijffijt, Department of Information and Computer Science, Aalto University"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_lijffijt_bootstrapping/",
    "segmentation": [
        [
            "2 examples that we have discussed in the paper as well.",
            "I will also be discussing now is comparing male against female authors of fiction books and finding news items.",
            "So for example, you compare today's news against the past year and then you can find upcoming topics, or at least that's the hypothesis that you can do."
        ],
        [
            "So because I have some reading examples I will introduce you to the data right away that we use so we model text as a sequence of words.",
            "And for a male versus female authors we have, we used the British National Corpus.",
            "Which contains 100 million words in 4000 texts.",
            "So actually this example I will use less data, but in the paper we use the full corpus in some experiments.",
            "And.",
            "In the final stage presentation of o'clock, consider finding dates of important events.",
            "Here we use separate historical newspaper.",
            "Corpus has been introduced by.",
            "Lava in Katie Piper 2009 contains less works but stop words have been removed in this corpus and it's quite a different corpus because it contains eight 380,000 articles where here we have only 4000 text with more words.",
            "Actually for corporate data."
        ],
        [
            "Quite different.",
            "So let's consider proper setting in more technical perspective.",
            "So we have a usually find significance threshold between zero and one.",
            "Usually it's quite low, like 0.050401 or even lower.",
            "So this is the definition that we use.",
            "Work queue is dominant in essence level Alpha if and only if the P value was smaller than or equal to some level Alpha.",
            "At the value gives the probability of the work of Cuban S normalized frequency being lower than the normalized frequency in Corpus Christi.",
            "Committed value with load and we have a significant difference."
        ],
        [
            "More frequently.",
            "So most of the tests that have been previously used there are based on bag of words model.",
            "Here I'm considering the binomial test which is 1 example of an exact test.",
            "There are others like Chi Square test for independence etc.",
            "So this significance test is very easy.",
            "You only need these four numbers and then we can compute what the value is.",
            "So in this case here, the people is computed.",
            "Expression that gives the probability of observing exactly K times this work.",
            "So the probability for a word is based on the frequency in T. Evaluates the significance of the work being more frequent in S, and then we serve over case the frequency until the maximum possible size of the forms, and here we see that the P value is quite low, so this suggests that there is a large significant difference."
        ],
        [
            "So what is this data actually about?",
            "So here we have male fiction authors.",
            "Female authors of fiction books.",
            "And of course it will be no surprise that later on my new methods will suggest that this difference is not so significant."
        ],
        [
            "So.",
            "Why in the 1st place that we think that we need a new model?",
            "Well, because text structure and the bag of words model is completely ignores.",
            "This assumes that all words are completely independent."
        ],
        [
            "So here we have 400,000 samples and here we have 410,000 samples.",
            "But of course between the words in one text there is high correlation."
        ],
        [
            "So.",
            "So maybe we should take this into account.",
            "So why is why is it model used in the 1st place?",
            "Because it's simple mathematically and it's computationally very efficient.",
            "And of course, well people have been using this pervasively and it has been shown that, well, you can do very useful things with this model.",
            "So now the question is, can we provide more realistic models and does it matter?",
            "Well for significance testing if we do so."
        ],
        [
            "Well here I have a motivation of why we think that would be a real difference.",
            "If we have a more realistic model.",
            "On the left side we were considering word four in the English language.",
            "In this histogram of word frequencies from the X axis we have normalized frequency going from zero to 0.1 on the Y axis we have number of texts or meaning that we counted the number of texts where frequency is around 1:00 and around 1.05 etc etc.",
            "Now we see that this is kind of a possible or Gaussian distribution that looks like it, which is also the result if you use the bag of words model, so the predictions will look like that the distribution should follow up something like this.",
            "Over here, if we look at the program, I.",
            "Which is.",
            "Like the frequencies quite close in the corpus as a whole, occurs almost 900,000 times both of them.",
            "But the frequency distribution is hugely different.",
            "We see that 1/4 of the text they have near 0 frequency, and in some cases it's like almost 10%.",
            "So if you have a very short text about somebody who is a conversation about himself or something like that.",
            "So we see that.",
            "The frequency distribution it differs for work and it depends on both the frequency that was already known of course, but also to work type.",
            "So."
        ],
        [
            "Tell me proposal method based on Inter arrival times which we used to model this.",
            "Well, and in fact that we called burstiness.",
            "So what are interarrival times?",
            "So we come to space between consecutive occurrences on the same word.",
            "So here we have some text about Finnair resuming the scheduled flights to New York after the very game.",
            "I really I mark the work and here it occurs four times.",
            "So now we want to obtain interarrival distribution.",
            "How do we do that?",
            "We start here at the first occurrence and we count how many words are there between the 1st and the 2nd occurrence.",
            "So it's 28 days arrive 28 + 1.",
            "So here are 8 words, so it becomes 9.",
            "Here are 38 words between the 3rd and 4th occurrence of therapy.",
            "At night and then between the lost and the first occurrence where we just continue counting because we want to take into account the whole corpus we have until arrival times 29.",
            "So we see that here the distribution is going to even work and occurs with them very different places so and so far to text.",
            "How to hypothesis is that if you model these into rival time, this can capture the behavior pattern of an individual word.",
            "So how do we go through a significance test foundation?",
            "But we've come to the space between the technical current, so we will take that into arrival time distribution.",
            "And I would love to do resampling based on the observed distribution in Corpse team.",
            "Well, we start.",
            "We can either use simply the empirical distribution."
        ],
        [
            "Sure, or we can fix on parametric model.",
            "For example a Weibull distribution.",
            "I don't have time to go into details with that, but I will say something small about it later.",
            "Throughout the presentation I will assume that we actually fit this Bible distribution.",
            "So how do we now create a random text we rent?",
            "We pick a random first index example.",
            "It would be the first and it will be 3.",
            "Or this has some details.",
            "You can find it in the paper, that's why I specified it separately.",
            "And then we just sample random interarrival times.",
            "So once twice again until we run out of the length of the corpus.",
            "And then we simply count how often this work occurs.",
            "Well.",
            "In this way we produce an random corpora.",
            "So we obtain an word frequencies.",
            "Now we can make a P value out of this.",
            "You can get away, so we simply compare each of the random corpora against the frequency in F and then we count how often is a personal one.",
            "Plus is because it's an African Barrick Gold estimate on the actual P value."
        ],
        [
            "So before I go to any examples will tell you about the second method.",
            "It's quite simple, so be fast about it.",
            "Um?",
            "The idea is that we would bootstrapping of the word frequency distribution.",
            "Again, we saw we do recently based on the on corpus T and number of samples is equal to the number of samples in S. It's similar where the number of samples is the number of texts, or the number of words.",
            "We don't example words, but that's.",
            "I don't repeat this pattern times and here we have normalized frequency rather than the frequent on other than the normal frequency because we cannot guarantee that both corpora so that the random corpses on the same size as the number of text number tokens."
        ],
        [
            "Let's see some results.",
            "So now here we have the same table as we saw before, and obviously that both the interarrival model and the Bootstrap test they give her quite well because they open till 8.",
            "So maybe it's not so significant as the binomial model.",
            "And of course here we see that there is a huge difference between these models.",
            "So maybe the difference is not so significant.",
            "Why is this?",
            "Well, because it works started, it actually occurs in very few texts.",
            "I think there's a lot in this text.",
            "And in that sense, these new models they take this kind of bursty pattern into account."
        ],
        [
            "So let's see some more in depth examples.",
            "We compute the frequency thresholds in a hypothetical text of 2000 words using Alfonso can see one, but the frequency listed here on the right hand side for the three methods.",
            "Set the frequency is at least this high.",
            "Then it's significant at this level.",
            "So if we have a word like off, which is grammatical, it occurs like 2 million times.",
            "So it's quite frequent.",
            "In his way home model.",
            "Last two parameters, one is laptop.",
            "I didn't even specify it because it's just related to the frequency.",
            "Of course we can count toward directly, so it's not be interesting for the second parameter kinds of get the shape distribution of the shape of the distribution of the Inter arrival times.",
            "So if it's one, then the interarrival times they follow an exponential distribution.",
            "Which is kind of as random as it's gonna get.",
            "So if they follow exponential distribution, then actually you expect the prediction to be exactly the same as the bag of words model.",
            "Because this is a memoryless distribution and indeed we see that they give the same prediction and bootstrapping.",
            "It's a little bit more conservative, so it requires a higher frequency.",
            "Or again, we saw before work the frequency distribution Fort Worth 4.",
            "Indeed, it's quite close to one, so the predictions for these two models they are quite the same.",
            "Again, bootstrapping is a little bit more conservative.",
            "I program we see that zero point 57.",
            "Which is quite low, so it has a theoretical minimum of zero, which means that it's actually a power law distribution rather than exponential.",
            "Here we see that prediction states start to diverge quite a lot.",
            "So meaning that if the smaller debate thought the larger the differences between the methods, which is nice as I do so."
        ],
        [
            "Finally, the final example that I want to show you is about another corpus, but finding significant new defense.",
            "In the paper by laws at all in 2009, they introduce the search framework, where you can find.",
            "Intervals where to buy newspapers or well where the sentences work?",
            "Also is 1 newspaper where there has been significant discussion about some event.",
            "And we are not competing with the search framework in itself, but rather one step of the search framework.",
            "They've used the thresholding method.",
            "Based just on the expectation.",
            "And then you see that you get.",
            "Quite a lot of hits for here.",
            "We're discussing great fire at Jacksonville.",
            "May 3rd, 1901 here.",
            "So the data is from 1900 to 1910 I think.",
            "Um?",
            "Initially this thresholding method is quick, gives quite a lot of days where there are significant discussion.",
            "Now in this search framework, it could be improved if we perhaps reduce this set of significant dates.",
            "So we see that if we use significance testing, first of all we have.",
            "Well, we can very, very of course the parameter all file to get shorter or longer intervals.",
            "And then we see again the same thing that the binomial model it has more significant results than the other three.",
            "Um, so well?"
        ],
        [
            "I can conclude the paper.",
            "So what we show in our paper or the bag of words, Model 40, presents frequency distributions or bursty works which are most works in the English language at least.",
            "We've introduced 2 new models, interarrival times and books and method and they get sometimes.",
            "A lot more conservative values and this way will be tight predicts also the difference between these methods so we know when we want to use or when we should be careful about our significance tests.",
            "So some possible future work is used to interact in other settings, such as information retrieval.",
            "And study also other statistics network frequencies, because of course.",
            "Well it tells you only so much about the differences between the two corpora.",
            "OK, well thank you.",
            "There's an alternative approach to modeling the same phenomenon that I think gives the same results, and it was published at ICM L in 2005 and 2006, and the idea is to use a distribution is different from the multinomial that's called in volume distribution.",
            "It's also called the daily compound multinomial, and it directly models the fact that words adversity, and it gives essentially the same results.",
            "It distinguishes between content words.",
            "And what you called grammatical words.",
            "So grammatical words have interarrival times that are statistically even, whereas content words are clustered in content.",
            "Words are thirsty for grammatical words, adopt bursty, they can be high or low frequency, but they don't thirsty.",
            "And the Power Distribution captures that precisely.",
            "And the polio distribution has been used a couple years ago in a couple of papers and stick ioffer information between also.",
            "OK, well.",
            "Thank you for this comment.",
            "I'm not familiar with this work, so.",
            "I'll have a look at this.",
            "Yes, I guess modeling this as a mixture of also.",
            "Like you just take a mixture where each text gives a independent prediction.",
            "Then it's kind of the same as bootstrapping.",
            "It's possible to get these same results.",
            "It's possible to get the same results without leaving the bag of words model.",
            "The multinomial is simply not a good fit to how were distributed, but there are alternative distributions that can be used on the same bag of words representation.",
            "OK, well thank you.",
            "I'll take a look at it.",
            "Hi Chrissy."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 examples that we have discussed in the paper as well.",
                    "label": 0
                },
                {
                    "sent": "I will also be discussing now is comparing male against female authors of fiction books and finding news items.",
                    "label": 0
                },
                {
                    "sent": "So for example, you compare today's news against the past year and then you can find upcoming topics, or at least that's the hypothesis that you can do.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So because I have some reading examples I will introduce you to the data right away that we use so we model text as a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "And for a male versus female authors we have, we used the British National Corpus.",
                    "label": 1
                },
                {
                    "sent": "Which contains 100 million words in 4000 texts.",
                    "label": 1
                },
                {
                    "sent": "So actually this example I will use less data, but in the paper we use the full corpus in some experiments.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "In the final stage presentation of o'clock, consider finding dates of important events.",
                    "label": 0
                },
                {
                    "sent": "Here we use separate historical newspaper.",
                    "label": 0
                },
                {
                    "sent": "Corpus has been introduced by.",
                    "label": 0
                },
                {
                    "sent": "Lava in Katie Piper 2009 contains less works but stop words have been removed in this corpus and it's quite a different corpus because it contains eight 380,000 articles where here we have only 4000 text with more words.",
                    "label": 0
                },
                {
                    "sent": "Actually for corporate data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite different.",
                    "label": 0
                },
                {
                    "sent": "So let's consider proper setting in more technical perspective.",
                    "label": 0
                },
                {
                    "sent": "So we have a usually find significance threshold between zero and one.",
                    "label": 1
                },
                {
                    "sent": "Usually it's quite low, like 0.050401 or even lower.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition that we use.",
                    "label": 0
                },
                {
                    "sent": "Work queue is dominant in essence level Alpha if and only if the P value was smaller than or equal to some level Alpha.",
                    "label": 1
                },
                {
                    "sent": "At the value gives the probability of the work of Cuban S normalized frequency being lower than the normalized frequency in Corpus Christi.",
                    "label": 0
                },
                {
                    "sent": "Committed value with load and we have a significant difference.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More frequently.",
                    "label": 0
                },
                {
                    "sent": "So most of the tests that have been previously used there are based on bag of words model.",
                    "label": 0
                },
                {
                    "sent": "Here I'm considering the binomial test which is 1 example of an exact test.",
                    "label": 0
                },
                {
                    "sent": "There are others like Chi Square test for independence etc.",
                    "label": 0
                },
                {
                    "sent": "So this significance test is very easy.",
                    "label": 0
                },
                {
                    "sent": "You only need these four numbers and then we can compute what the value is.",
                    "label": 0
                },
                {
                    "sent": "So in this case here, the people is computed.",
                    "label": 0
                },
                {
                    "sent": "Expression that gives the probability of observing exactly K times this work.",
                    "label": 1
                },
                {
                    "sent": "So the probability for a word is based on the frequency in T. Evaluates the significance of the work being more frequent in S, and then we serve over case the frequency until the maximum possible size of the forms, and here we see that the P value is quite low, so this suggests that there is a large significant difference.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is this data actually about?",
                    "label": 0
                },
                {
                    "sent": "So here we have male fiction authors.",
                    "label": 0
                },
                {
                    "sent": "Female authors of fiction books.",
                    "label": 0
                },
                {
                    "sent": "And of course it will be no surprise that later on my new methods will suggest that this difference is not so significant.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Why in the 1st place that we think that we need a new model?",
                    "label": 0
                },
                {
                    "sent": "Well, because text structure and the bag of words model is completely ignores.",
                    "label": 0
                },
                {
                    "sent": "This assumes that all words are completely independent.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have 400,000 samples and here we have 410,000 samples.",
                    "label": 0
                },
                {
                    "sent": "But of course between the words in one text there is high correlation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So maybe we should take this into account.",
                    "label": 0
                },
                {
                    "sent": "So why is why is it model used in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "Because it's simple mathematically and it's computationally very efficient.",
                    "label": 0
                },
                {
                    "sent": "And of course, well people have been using this pervasively and it has been shown that, well, you can do very useful things with this model.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, can we provide more realistic models and does it matter?",
                    "label": 1
                },
                {
                    "sent": "Well for significance testing if we do so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well here I have a motivation of why we think that would be a real difference.",
                    "label": 0
                },
                {
                    "sent": "If we have a more realistic model.",
                    "label": 0
                },
                {
                    "sent": "On the left side we were considering word four in the English language.",
                    "label": 0
                },
                {
                    "sent": "In this histogram of word frequencies from the X axis we have normalized frequency going from zero to 0.1 on the Y axis we have number of texts or meaning that we counted the number of texts where frequency is around 1:00 and around 1.05 etc etc.",
                    "label": 1
                },
                {
                    "sent": "Now we see that this is kind of a possible or Gaussian distribution that looks like it, which is also the result if you use the bag of words model, so the predictions will look like that the distribution should follow up something like this.",
                    "label": 0
                },
                {
                    "sent": "Over here, if we look at the program, I.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Like the frequencies quite close in the corpus as a whole, occurs almost 900,000 times both of them.",
                    "label": 0
                },
                {
                    "sent": "But the frequency distribution is hugely different.",
                    "label": 1
                },
                {
                    "sent": "We see that 1/4 of the text they have near 0 frequency, and in some cases it's like almost 10%.",
                    "label": 0
                },
                {
                    "sent": "So if you have a very short text about somebody who is a conversation about himself or something like that.",
                    "label": 0
                },
                {
                    "sent": "So we see that.",
                    "label": 1
                },
                {
                    "sent": "The frequency distribution it differs for work and it depends on both the frequency that was already known of course, but also to work type.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell me proposal method based on Inter arrival times which we used to model this.",
                    "label": 0
                },
                {
                    "sent": "Well, and in fact that we called burstiness.",
                    "label": 0
                },
                {
                    "sent": "So what are interarrival times?",
                    "label": 1
                },
                {
                    "sent": "So we come to space between consecutive occurrences on the same word.",
                    "label": 1
                },
                {
                    "sent": "So here we have some text about Finnair resuming the scheduled flights to New York after the very game.",
                    "label": 0
                },
                {
                    "sent": "I really I mark the work and here it occurs four times.",
                    "label": 0
                },
                {
                    "sent": "So now we want to obtain interarrival distribution.",
                    "label": 0
                },
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "We start here at the first occurrence and we count how many words are there between the 1st and the 2nd occurrence.",
                    "label": 0
                },
                {
                    "sent": "So it's 28 days arrive 28 + 1.",
                    "label": 0
                },
                {
                    "sent": "So here are 8 words, so it becomes 9.",
                    "label": 0
                },
                {
                    "sent": "Here are 38 words between the 3rd and 4th occurrence of therapy.",
                    "label": 0
                },
                {
                    "sent": "At night and then between the lost and the first occurrence where we just continue counting because we want to take into account the whole corpus we have until arrival times 29.",
                    "label": 0
                },
                {
                    "sent": "So we see that here the distribution is going to even work and occurs with them very different places so and so far to text.",
                    "label": 0
                },
                {
                    "sent": "How to hypothesis is that if you model these into rival time, this can capture the behavior pattern of an individual word.",
                    "label": 1
                },
                {
                    "sent": "So how do we go through a significance test foundation?",
                    "label": 0
                },
                {
                    "sent": "But we've come to the space between the technical current, so we will take that into arrival time distribution.",
                    "label": 0
                },
                {
                    "sent": "And I would love to do resampling based on the observed distribution in Corpse team.",
                    "label": 0
                },
                {
                    "sent": "Well, we start.",
                    "label": 0
                },
                {
                    "sent": "We can either use simply the empirical distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure, or we can fix on parametric model.",
                    "label": 0
                },
                {
                    "sent": "For example a Weibull distribution.",
                    "label": 1
                },
                {
                    "sent": "I don't have time to go into details with that, but I will say something small about it later.",
                    "label": 0
                },
                {
                    "sent": "Throughout the presentation I will assume that we actually fit this Bible distribution.",
                    "label": 0
                },
                {
                    "sent": "So how do we now create a random text we rent?",
                    "label": 0
                },
                {
                    "sent": "We pick a random first index example.",
                    "label": 1
                },
                {
                    "sent": "It would be the first and it will be 3.",
                    "label": 0
                },
                {
                    "sent": "Or this has some details.",
                    "label": 0
                },
                {
                    "sent": "You can find it in the paper, that's why I specified it separately.",
                    "label": 0
                },
                {
                    "sent": "And then we just sample random interarrival times.",
                    "label": 1
                },
                {
                    "sent": "So once twice again until we run out of the length of the corpus.",
                    "label": 1
                },
                {
                    "sent": "And then we simply count how often this work occurs.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "In this way we produce an random corpora.",
                    "label": 0
                },
                {
                    "sent": "So we obtain an word frequencies.",
                    "label": 0
                },
                {
                    "sent": "Now we can make a P value out of this.",
                    "label": 0
                },
                {
                    "sent": "You can get away, so we simply compare each of the random corpora against the frequency in F and then we count how often is a personal one.",
                    "label": 0
                },
                {
                    "sent": "Plus is because it's an African Barrick Gold estimate on the actual P value.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before I go to any examples will tell you about the second method.",
                    "label": 0
                },
                {
                    "sent": "It's quite simple, so be fast about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The idea is that we would bootstrapping of the word frequency distribution.",
                    "label": 1
                },
                {
                    "sent": "Again, we saw we do recently based on the on corpus T and number of samples is equal to the number of samples in S. It's similar where the number of samples is the number of texts, or the number of words.",
                    "label": 1
                },
                {
                    "sent": "We don't example words, but that's.",
                    "label": 0
                },
                {
                    "sent": "I don't repeat this pattern times and here we have normalized frequency rather than the frequent on other than the normal frequency because we cannot guarantee that both corpora so that the random corpses on the same size as the number of text number tokens.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see some results.",
                    "label": 0
                },
                {
                    "sent": "So now here we have the same table as we saw before, and obviously that both the interarrival model and the Bootstrap test they give her quite well because they open till 8.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's not so significant as the binomial model.",
                    "label": 0
                },
                {
                    "sent": "And of course here we see that there is a huge difference between these models.",
                    "label": 0
                },
                {
                    "sent": "So maybe the difference is not so significant.",
                    "label": 1
                },
                {
                    "sent": "Why is this?",
                    "label": 0
                },
                {
                    "sent": "Well, because it works started, it actually occurs in very few texts.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot in this text.",
                    "label": 0
                },
                {
                    "sent": "And in that sense, these new models they take this kind of bursty pattern into account.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see some more in depth examples.",
                    "label": 0
                },
                {
                    "sent": "We compute the frequency thresholds in a hypothetical text of 2000 words using Alfonso can see one, but the frequency listed here on the right hand side for the three methods.",
                    "label": 1
                },
                {
                    "sent": "Set the frequency is at least this high.",
                    "label": 0
                },
                {
                    "sent": "Then it's significant at this level.",
                    "label": 0
                },
                {
                    "sent": "So if we have a word like off, which is grammatical, it occurs like 2 million times.",
                    "label": 0
                },
                {
                    "sent": "So it's quite frequent.",
                    "label": 0
                },
                {
                    "sent": "In his way home model.",
                    "label": 0
                },
                {
                    "sent": "Last two parameters, one is laptop.",
                    "label": 0
                },
                {
                    "sent": "I didn't even specify it because it's just related to the frequency.",
                    "label": 1
                },
                {
                    "sent": "Of course we can count toward directly, so it's not be interesting for the second parameter kinds of get the shape distribution of the shape of the distribution of the Inter arrival times.",
                    "label": 0
                },
                {
                    "sent": "So if it's one, then the interarrival times they follow an exponential distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of as random as it's gonna get.",
                    "label": 0
                },
                {
                    "sent": "So if they follow exponential distribution, then actually you expect the prediction to be exactly the same as the bag of words model.",
                    "label": 0
                },
                {
                    "sent": "Because this is a memoryless distribution and indeed we see that they give the same prediction and bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more conservative, so it requires a higher frequency.",
                    "label": 0
                },
                {
                    "sent": "Or again, we saw before work the frequency distribution Fort Worth 4.",
                    "label": 0
                },
                {
                    "sent": "Indeed, it's quite close to one, so the predictions for these two models they are quite the same.",
                    "label": 0
                },
                {
                    "sent": "Again, bootstrapping is a little bit more conservative.",
                    "label": 0
                },
                {
                    "sent": "I program we see that zero point 57.",
                    "label": 0
                },
                {
                    "sent": "Which is quite low, so it has a theoretical minimum of zero, which means that it's actually a power law distribution rather than exponential.",
                    "label": 0
                },
                {
                    "sent": "Here we see that prediction states start to diverge quite a lot.",
                    "label": 0
                },
                {
                    "sent": "So meaning that if the smaller debate thought the larger the differences between the methods, which is nice as I do so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, the final example that I want to show you is about another corpus, but finding significant new defense.",
                    "label": 0
                },
                {
                    "sent": "In the paper by laws at all in 2009, they introduce the search framework, where you can find.",
                    "label": 1
                },
                {
                    "sent": "Intervals where to buy newspapers or well where the sentences work?",
                    "label": 1
                },
                {
                    "sent": "Also is 1 newspaper where there has been significant discussion about some event.",
                    "label": 0
                },
                {
                    "sent": "And we are not competing with the search framework in itself, but rather one step of the search framework.",
                    "label": 0
                },
                {
                    "sent": "They've used the thresholding method.",
                    "label": 0
                },
                {
                    "sent": "Based just on the expectation.",
                    "label": 0
                },
                {
                    "sent": "And then you see that you get.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of hits for here.",
                    "label": 0
                },
                {
                    "sent": "We're discussing great fire at Jacksonville.",
                    "label": 1
                },
                {
                    "sent": "May 3rd, 1901 here.",
                    "label": 0
                },
                {
                    "sent": "So the data is from 1900 to 1910 I think.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Initially this thresholding method is quick, gives quite a lot of days where there are significant discussion.",
                    "label": 0
                },
                {
                    "sent": "Now in this search framework, it could be improved if we perhaps reduce this set of significant dates.",
                    "label": 0
                },
                {
                    "sent": "So we see that if we use significance testing, first of all we have.",
                    "label": 0
                },
                {
                    "sent": "Well, we can very, very of course the parameter all file to get shorter or longer intervals.",
                    "label": 1
                },
                {
                    "sent": "And then we see again the same thing that the binomial model it has more significant results than the other three.",
                    "label": 0
                },
                {
                    "sent": "Um, so well?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I can conclude the paper.",
                    "label": 0
                },
                {
                    "sent": "So what we show in our paper or the bag of words, Model 40, presents frequency distributions or bursty works which are most works in the English language at least.",
                    "label": 0
                },
                {
                    "sent": "We've introduced 2 new models, interarrival times and books and method and they get sometimes.",
                    "label": 1
                },
                {
                    "sent": "A lot more conservative values and this way will be tight predicts also the difference between these methods so we know when we want to use or when we should be careful about our significance tests.",
                    "label": 1
                },
                {
                    "sent": "So some possible future work is used to interact in other settings, such as information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And study also other statistics network frequencies, because of course.",
                    "label": 0
                },
                {
                    "sent": "Well it tells you only so much about the differences between the two corpora.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you.",
                    "label": 0
                },
                {
                    "sent": "There's an alternative approach to modeling the same phenomenon that I think gives the same results, and it was published at ICM L in 2005 and 2006, and the idea is to use a distribution is different from the multinomial that's called in volume distribution.",
                    "label": 0
                },
                {
                    "sent": "It's also called the daily compound multinomial, and it directly models the fact that words adversity, and it gives essentially the same results.",
                    "label": 0
                },
                {
                    "sent": "It distinguishes between content words.",
                    "label": 0
                },
                {
                    "sent": "And what you called grammatical words.",
                    "label": 0
                },
                {
                    "sent": "So grammatical words have interarrival times that are statistically even, whereas content words are clustered in content.",
                    "label": 0
                },
                {
                    "sent": "Words are thirsty for grammatical words, adopt bursty, they can be high or low frequency, but they don't thirsty.",
                    "label": 0
                },
                {
                    "sent": "And the Power Distribution captures that precisely.",
                    "label": 0
                },
                {
                    "sent": "And the polio distribution has been used a couple years ago in a couple of papers and stick ioffer information between also.",
                    "label": 0
                },
                {
                    "sent": "OK, well.",
                    "label": 0
                },
                {
                    "sent": "Thank you for this comment.",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with this work, so.",
                    "label": 0
                },
                {
                    "sent": "I'll have a look at this.",
                    "label": 0
                },
                {
                    "sent": "Yes, I guess modeling this as a mixture of also.",
                    "label": 0
                },
                {
                    "sent": "Like you just take a mixture where each text gives a independent prediction.",
                    "label": 0
                },
                {
                    "sent": "Then it's kind of the same as bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "It's possible to get these same results.",
                    "label": 0
                },
                {
                    "sent": "It's possible to get the same results without leaving the bag of words model.",
                    "label": 0
                },
                {
                    "sent": "The multinomial is simply not a good fit to how were distributed, but there are alternative distributions that can be used on the same bag of words representation.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you.",
                    "label": 0
                },
                {
                    "sent": "I'll take a look at it.",
                    "label": 0
                },
                {
                    "sent": "Hi Chrissy.",
                    "label": 0
                }
            ]
        }
    }
}