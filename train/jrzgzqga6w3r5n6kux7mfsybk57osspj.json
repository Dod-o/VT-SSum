{
    "id": "jrzgzqga6w3r5n6kux7mfsybk57osspj",
    "title": "Semantic text features from small world graphs",
    "info": {
        "author": [
            "Jure Leskovec, Computer Science Department, Stanford University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2005",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/slsfs05_leskovec_stfsw/",
    "segmentation": [
        [
            "So John, and what I'll present is like first steps towards getting rid of TF IDF basically."
        ],
        [
            "Um so.",
            "I mean today when we're dealing with talk text documents, what we usually do is just represent in this bag of words manner as a sparse vectors.",
            "And when we do the cost and then when we want to measure the similarity between the documents, we do the inner product.",
            "And I mean the obvious thing is that this bag of words is like.",
            "A stupid, stupid representation, right?",
            "It doesn't take into account any like semantic relations between words like synonyms and things like that.",
            "And we also know that like work frequencies in documents like they follow, they follow a power law.",
            "So they follow these steps, distribution and what we also know that like this IDF, when we do the TF IDF.",
            "So this inverse document frequency works really well.",
            "And what is like what we do there is we take a log right and.",
            "A power law is basically like a polynomial, so taking a lock we get rid of that exponent up there.",
            "So basically there is no.",
            "There is no good notion why why this IDF helps helps so much or know know any real explanation, but it works really well.",
            "So to get like over the IDF people have been trying various techniques and like things like LSI stream, kernel semantic kernels and so on.",
            "And so we start from here we are basically trying to get over this TF IDF scheme and the other thing is that in like this small world graphs we also observed this like.",
            "Or degree distributions and so our idea is to having a set of documents will try to create some kind of small world graph and then use this graph to measure the distance between the features and try to better estimate the document similarity.",
            "And basically just."
        ],
        [
            "The basic idea.",
            "So given a set of tech text units like documents or paragraphs, will in some simple manner will organize them into some kind of tree or a graph where each node will contain a set of set of words and our hope is that the words which will reside in the note will be like semantically related and given this topology and this works in which reside in the notes will then use this to measure to measure feature similarity and then use this for further for document similarity.",
            "Just."
        ],
        [
            "To show a toy example, so as I said, what we hope so we hope to organize for instance.",
            "Our works, our features into some kind of a tree structure where we where we think that like.",
            "A child, a child of apparently like extent, extend the vocabulary in some in some certain certain sense, meaning like that will get more and more fine grained vocabularies as we move down the tree, and I mean the analogies like with this directories on the web where we have like this taxonomies or hierarchies of topics when on the top of the tree we have like this.",
            "Very broad, common, common, common topics, common web pages.",
            "And then we move down.",
            "We are getting increasingly fine grained.",
            "And more and more specific web pages and the other distinction is that we are not trying to build like ontologies or taxonomies or something like that.",
            "So we are searching for a simple approach how to how to transform a set of documents into a some kind of a tree or a graph and then use this to measure document similarity.",
            "Just to make this more concrete, cares are small small example.",
            "So what we want to do for instance is that like in the root of the tree we want something like stop words and then we want like.",
            "In the like lower levels we want more and more specific words to appear, so we expect that I know from stop words will get to computer science and then like AI will extend the vocabulary of computer science in some in some certain way and then vocabulary of AI will be like extended into two different parts like machine learning and robotics and.",
            "So we can organize it into a tree or we can organize it into a graph.",
            "So there is no reason why machine learning wouldn't like extend the vocabulary of like computer science.",
            "And then for instance statistics.",
            "Or for robotics probably there will be also turns from electrical, mechanical engineering and like from from AI.",
            "So we aim to create something like this and then we can.",
            "We can better estimate unload the distance between the words which reside in these nodes.",
            "So now I."
        ],
        [
            "Represent like 3 simple algorithms which we like, used and compared.",
            "And so we start with something which builds like a really simple 3.",
            "Then we'll use some some like a bit better version and at the end I'll show.",
            "Procedure which creates a graph and then and then in later in the talk I'll explain how we how we evaluated all these algorithms and so on.",
            "Um?",
            "So what we basically do is we will be taking documents in some random order and for each document will create a node.",
            "A node in the tree and then we'll connect this node to some some of pre existing nodes in the tree.",
            "And what we try to do is we'll try to maximize maximize this score which is basically nothing else then the."
        ],
        [
            "Words which appear in the document.",
            "These are like all the words, all the words which appear in a path from the node which we are comparing to the root of the tree and we like divide by the union of all words in this path and.",
            "Just like a side note, we tested also like other scoring function or like similar form and there is not much difference between them.",
            "And so, like going back to the explaining the algorithm.",
            "So when we create this node, the words we will put in the notes are basically the words the document brought into this, like vocabulary which is like spans from the root of the tree to the leaf to this new node.",
            "And just like a small example.",
            "So if you want to insert insert."
        ],
        [
            "This small guy here will compare.",
            "It will compare it to all nodes and when we compare it to this note.",
            "For instance, we compare it to all the words which are which are in this path or when we compare it to this to this node we we calculate the score function using all words which appear in this path and then we connect it to the best scoring load.",
            "Um so."
        ],
        [
            "I mean just showing the simple algorithm.",
            "There are a lot of things you can do with it.",
            "For instance, you can introduce a stop words node, so it makes sense to have in the root of the tree to have like a load with most common sense like common words like stop words and also some other words.",
            "And we experimented with lots of different lists of stop words and at the end we decided to use like 8 English stop words like and so on.",
            "And we also added words which appeared like in more than 80% of the documents in the end.",
            "This root of the tree.",
            "Consisted about 20 words."
        ],
        [
            "So.",
            "One drawback of this basic tree, as we call it, is that so that structure rebuilt is like depends on the ordering of the document.",
            "So what we do then you say instead of like taking documents in some random order we like do this like greedy greedy search innocence.",
            "So we always add, add, add a document into this already build structure which like has the maximal score out of all documents.",
            "So just like some simple greedy approach.",
            "And the third."
        ],
        [
            "Think which is again nothing special is that.",
            "I mean, we notice that like these hierarchies are not really hierarchies, but they're basically graph.",
            "So not even this web directories, they're not, they're not taxonomies or hierarchies.",
            "But there are graphs you have this cross links among the categories, and here we also like.",
            "Try to imitate that.",
            "So again we like start with the stop words node and then we'll be adding notes, notes, loads to documents as nodes and will connect to all to all the nodes where the score.",
            "So the like the number of common of common words is greater than some predefined threshold and.",
            "What will usually happen is that this graph will be like a star, so we need at the end we need to remove this stop words notes so that the distance between the document will be something greater than two, right what?",
            "So if I have a stop words node, so I mean the hypothesis is every every document will have a staff person.",
            "So what can happen is that every document is is attached to this center of a star and then the distances are almost meaningless, so we need to remove this software it snowed."
        ],
        [
            "So having described all the algorithms will decide how to measure the similarity between features of documents.",
            "So basically we're having we have two documents and we do like.",
            "For each feature in first document, we'll compare it.",
            "All other features in the second document, and this is like it's not.",
            "It's not linear with the size of the document.",
            "And since we have a topology we use, we won't treat features as independent, but will use this technology to measure to measure the distance is and what we can simple thing to do.",
            "This is to use like shortest paths on the graph and then we can do like the unrated version, meaning that the length of each edge is 1 or we can do the.",
            "The weighted version where each edge edge is weighted by 1 minus score, meaning that like if the documents are two more similar than the length of the edge is shorter.",
            "And.",
            "And basically so basically this this says exactly as I said here.",
            "So basically for every for every pairs of features from the documents we calculated similarity and sum it up.",
            "Um so."
        ],
        [
            "For the experimental setup we were using this the new writers and we just like consider the random sample of 1000 documents.",
            "So it was a small experience and we did like 10 fold cross validation and we evaluated like the the.",
            "The quality of our distance measure using the kernel alignment and what this thing basically does is that it compares the distances among the documents from the same class but with the distance is among the documents from different classes and you basically want this ratio to be as large as possible, so alignment of 1 means that like all the documents from the same class have distance zero and from different classes have distance one.",
            "Um and."
        ],
        [
            "So now I'll show like 333 slides of with results feature distance, meaning that quadratic algorithm I showed or since every every document is basically also on edge.",
            "So when we are adding a document, we first create an edge and then link it.",
            "Um?",
            "We can also just do like this direct shortest path between the document.",
            "So when you say compared to documents or just find where in the graph they reside and I calculate the path and what I mean.",
            "So what we can see from here is like we get a small improvement, but it's not significant over the cosine similarity.",
            "And notice that here is no stop words now so."
        ],
        [
            "Going forward and introducing stopwords node really increases the performance.",
            "So what we what we observe is that this so this combination.",
            "So using the feature distance and waited path later, shortest paths on the graph is like the best.",
            "So this is still basically.",
            "But we introduced the stuff where it snowed the optimal tree, meaning that we are using these greedy heuristics to select documents is a bit better.",
            "We can also observe that for instance.",
            "No, distance is always is always worse than using feature distance, which also makes sense.",
            "And for instance, for the last.",
            "So this is where we created a graph."
        ],
        [
            "Here the results are different.",
            "Basically see that just like the length.",
            "The length of the shortest path and using the feature distance does does the best.",
            "And all these like differences are.",
            "Significant so just."
        ],
        [
            "So a summary, what we what I showed?",
            "So this is this is how random would do so.",
            "I mean, you couldn't do worse than this.",
            "So then this is the like standard cosine similarity.",
            "This is when you use the basic tree when and when you use a basic 3 plus stop words node, you really get an increasing performance if you use like optimal tree, but no stop where it snowed.",
            "Then you don't get like significant performance.",
            "So basically the ordering of the document does not like change much in terms of performance and from all the algorithms.",
            "Basically this optimal tree with.",
            "We stop with this node does does does the best, but I mean but basically all these all the last three are in the same range of performances.",
            "So."
        ],
        [
            "As I said, stop words improve results.",
            "The document of the ordering of documents of how we like, insert them into the this hierarchy or history does not degrade performance.",
            "That optimal.",
            "3 meaning that greedy strategy of picking documents with stop words performance best.",
            "Using feature distance.",
            "Outperforms the node distance, but feature distance is quite expensive to calculate.",
            "Um and weighted paths are like.",
            "I mean quite bad.",
            "Alot better than than just using the simple distance on a graph.",
            "And the other thing is that we also like tried so we try to create this simple simple trees first, withhold documents and the second with paragraph.",
            "So you could say that like a paragraph paragraph is like a coherent unit talking about the one thing right.",
            "But what we observe that basically paragraphs do worse than the documents.",
            "And."
        ],
        [
            "So just conclusions and further work.",
            "So basically represented just like first few steps.",
            "Towards I know building something which may eventually be better than TF, IDF and 1st next step to do would basically be to observe whether we can do like some build, some kind of probabilistic generation mechanism which would generate documents which would eventually have like this power law, degree distribution, power law, distribution of the words and this would probably like motivate them.",
            "The choice of document similarity and things like that.",
            "So.",
            "Yeah, done, thanks.",
            "Question.",
            "Stupid question, why not optimize over the?",
            "The order of insertion into the graph.",
            "Oh I think I. I think I did that, but there was like.",
            "I mean I should have done that, but I don't remember.",
            "Be really honest.",
            "I think I tried it, but there wasn't like anything anything.",
            "But yeah, you could.",
            "You could do that.",
            "I mean with this graph the problem is like like it's really simple, right?",
            "You have to like guess this threshold and that's like.",
            "I mean it's.",
            "Yes, you put on that.",
            "You have to.",
            "You have to.",
            "You have to find and.",
            "So I know.",
            "I mean, we have to find something better there.",
            "OK, so the second question really to comment.",
            "So you found that removing Stopwords helped a lot, and those are also the high degree nodes.",
            "I mean, it had kept the help of it right because it's like separated the documents in some sense, so it helps anyway.",
            "And there's also the high degree nodes.",
            "OK, so one thing that people find which doing sort of segmentation of graph based structures is that if you do normalization.",
            "Sort of down weight, highly weighted things that could help quite a lot.",
            "So you should positive possibly decide whether you should somehow normalizing for the number of incoming links.",
            "Links, OK, good idea, although isn't this just more like 3 than a graph?",
            "Search algorithm, I mean you have like a star.",
            "So basically everything more or less everything is connected to this stuff where Snowden then you also have like connections among among the.",
            "But that I can't measure the degree distribution of this of this, that would be also like.",
            "I like the last points by probabilistic generation generating documents, but of course that's going to be.",
            "I don't quite see how you fit that in with what you want to do here, because if you construct an old tree then you're going to be in a bit of trouble around for generating document holistically.",
            "I mean you could.",
            "You could always do is to have some kind of.",
            "I know random walk something on this graph.",
            "I mean in a tree it's obvious right start from some.",
            "From some child and you would like go go out for instance.",
            "For instance.",
            "I mean, you can do many ways.",
            "I don't have an answer.",
            "I mean, it's just like the first steps and seems promising.",
            "It's all.",
            "I mean, it's a bit ad hoc as I as I showed it so far, but.",
            "Really, really like to do is have something more that you could fit the probabilistic mechanism to the data you know.",
            "We used likelihood and you would learn the structure and then zip slow with just be.",
            "I think that's a good wish.",
            "Yeah, the idea is that the power law would come out of the generation methods that you were suggesting that you know you pick a random node in the tree and then clearly you know the chances are.",
            "If it's quite low in the tree, it will be very infrequent, but then when you look at the park going back up, the higher you go up, the more frequently those nodes will be used when you in the generation mechanism.",
            "And then I mean, you just randomly select some words from each of the nodes as you move guys move up the tree so it would give you the power or coming out of that, But you're saying that we would want to actually have the the generation mechanism somehow influence how you design the tree original.",
            "Yeah, well I'm gonna guess.",
            "It also doesn't guess one of the things that's nice.",
            "How about policyholders Gen things.",
            "I'd like it.",
            "Hurts that you can use to school.",
            "Sure documents under how you could get that didn't have a proper probability thing and if you don't have a tree, if you have an on tree.",
            "Capital with a senior probability distribution.",
            "If you could direct links.",
            "Yes, if you don't.",
            "If you have a problem, don't have a treat and then you know there are problems, but you could imagine that you're seeing a random walk you know following.",
            "Certain may be biased towards certain.",
            "So I think, yeah, it's it's certainly one of the next things to really thinking.",
            "Generated.",
            "Uh huh.",
            "On the order of The Alchemist.",
            "I did I did this.",
            "I did this experience so the trees were not so deep.",
            "They were like something like 16 levels, maybe four.",
            "I was building up like 10,000 documents.",
            "So the problem the problem here is not building a tree.",
            "Would be would be would be expensive expenses then calculating this quadratic.",
            "Features I mean yeah distances, so the trees they were like so obviously the highest degree, the highest degree node was like the root and then and then I think there was like about 20 levels of of this of this end.",
            "But I didn't like really, really draw.",
            "I was like we were.",
            "We were inspecting the words in the in the nodes.",
            "That was more important to us, seeing that if there are there words which like make sense to be to be present together.",
            "So that was more that we focused on.",
            "We have an algorithm for calculating distances in the tree.",
            "So I don't know whether that would help or not.",
            "Let me.",
            "If you want to use it, that's it's called the.",
            "We need to make some chemistry and that's what you can do in your time.",
            "You OK, yeah.",
            "I have a short comment.",
            "Yeah, basically in the beginning you said that you don't want to build this ontology, but in some way you you are approximating some kind of ontology.",
            "So so we have discussion.",
            "Yes is already on this so maybe even interesting to try first to compare.",
            "How the same approach would work on ontology?",
            "Because basically entology or this taxonomy of terms have seen this kind of hierarchical structure and somehow would be able to compensate this power law in the same way, except that it's more kind of reliable.",
            "At least you would call it and told you at least you have some sense.",
            "Well, here we are building something.",
            "Which way did you try if the structure becomes similar to something like topic structure in the industry?",
            "Before I could bring some like really shallow right for the new one?",
            "Yeah, but just by looking for the the three structure did you get let's say I don't know financial documents and then different you get like sets of sets of words which makes sense like you get a lot of noise.",
            "But there are also like in the nodes that are there are sets of words which which are which are there and you get some kind of like more and more specialized.",
            "But I mean if you would like, the idea was to get something simple so that people could use it in a sentence, right?",
            "If you come up with the two big machinery, them.",
            "OK. OK, OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So John, and what I'll present is like first steps towards getting rid of TF IDF basically.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "I mean today when we're dealing with talk text documents, what we usually do is just represent in this bag of words manner as a sparse vectors.",
                    "label": 1
                },
                {
                    "sent": "And when we do the cost and then when we want to measure the similarity between the documents, we do the inner product.",
                    "label": 0
                },
                {
                    "sent": "And I mean the obvious thing is that this bag of words is like.",
                    "label": 0
                },
                {
                    "sent": "A stupid, stupid representation, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't take into account any like semantic relations between words like synonyms and things like that.",
                    "label": 0
                },
                {
                    "sent": "And we also know that like work frequencies in documents like they follow, they follow a power law.",
                    "label": 0
                },
                {
                    "sent": "So they follow these steps, distribution and what we also know that like this IDF, when we do the TF IDF.",
                    "label": 0
                },
                {
                    "sent": "So this inverse document frequency works really well.",
                    "label": 0
                },
                {
                    "sent": "And what is like what we do there is we take a log right and.",
                    "label": 0
                },
                {
                    "sent": "A power law is basically like a polynomial, so taking a lock we get rid of that exponent up there.",
                    "label": 0
                },
                {
                    "sent": "So basically there is no.",
                    "label": 0
                },
                {
                    "sent": "There is no good notion why why this IDF helps helps so much or know know any real explanation, but it works really well.",
                    "label": 1
                },
                {
                    "sent": "So to get like over the IDF people have been trying various techniques and like things like LSI stream, kernel semantic kernels and so on.",
                    "label": 1
                },
                {
                    "sent": "And so we start from here we are basically trying to get over this TF IDF scheme and the other thing is that in like this small world graphs we also observed this like.",
                    "label": 0
                },
                {
                    "sent": "Or degree distributions and so our idea is to having a set of documents will try to create some kind of small world graph and then use this graph to measure the distance between the features and try to better estimate the document similarity.",
                    "label": 0
                },
                {
                    "sent": "And basically just.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea.",
                    "label": 0
                },
                {
                    "sent": "So given a set of tech text units like documents or paragraphs, will in some simple manner will organize them into some kind of tree or a graph where each node will contain a set of set of words and our hope is that the words which will reside in the note will be like semantically related and given this topology and this works in which reside in the notes will then use this to measure to measure feature similarity and then use this for further for document similarity.",
                    "label": 1
                },
                {
                    "sent": "Just.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To show a toy example, so as I said, what we hope so we hope to organize for instance.",
                    "label": 0
                },
                {
                    "sent": "Our works, our features into some kind of a tree structure where we where we think that like.",
                    "label": 0
                },
                {
                    "sent": "A child, a child of apparently like extent, extend the vocabulary in some in some certain certain sense, meaning like that will get more and more fine grained vocabularies as we move down the tree, and I mean the analogies like with this directories on the web where we have like this taxonomies or hierarchies of topics when on the top of the tree we have like this.",
                    "label": 1
                },
                {
                    "sent": "Very broad, common, common, common topics, common web pages.",
                    "label": 0
                },
                {
                    "sent": "And then we move down.",
                    "label": 1
                },
                {
                    "sent": "We are getting increasingly fine grained.",
                    "label": 1
                },
                {
                    "sent": "And more and more specific web pages and the other distinction is that we are not trying to build like ontologies or taxonomies or something like that.",
                    "label": 1
                },
                {
                    "sent": "So we are searching for a simple approach how to how to transform a set of documents into a some kind of a tree or a graph and then use this to measure document similarity.",
                    "label": 0
                },
                {
                    "sent": "Just to make this more concrete, cares are small small example.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do for instance is that like in the root of the tree we want something like stop words and then we want like.",
                    "label": 0
                },
                {
                    "sent": "In the like lower levels we want more and more specific words to appear, so we expect that I know from stop words will get to computer science and then like AI will extend the vocabulary of computer science in some in some certain way and then vocabulary of AI will be like extended into two different parts like machine learning and robotics and.",
                    "label": 0
                },
                {
                    "sent": "So we can organize it into a tree or we can organize it into a graph.",
                    "label": 0
                },
                {
                    "sent": "So there is no reason why machine learning wouldn't like extend the vocabulary of like computer science.",
                    "label": 0
                },
                {
                    "sent": "And then for instance statistics.",
                    "label": 0
                },
                {
                    "sent": "Or for robotics probably there will be also turns from electrical, mechanical engineering and like from from AI.",
                    "label": 0
                },
                {
                    "sent": "So we aim to create something like this and then we can.",
                    "label": 0
                },
                {
                    "sent": "We can better estimate unload the distance between the words which reside in these nodes.",
                    "label": 0
                },
                {
                    "sent": "So now I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Represent like 3 simple algorithms which we like, used and compared.",
                    "label": 0
                },
                {
                    "sent": "And so we start with something which builds like a really simple 3.",
                    "label": 0
                },
                {
                    "sent": "Then we'll use some some like a bit better version and at the end I'll show.",
                    "label": 0
                },
                {
                    "sent": "Procedure which creates a graph and then and then in later in the talk I'll explain how we how we evaluated all these algorithms and so on.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what we basically do is we will be taking documents in some random order and for each document will create a node.",
                    "label": 0
                },
                {
                    "sent": "A node in the tree and then we'll connect this node to some some of pre existing nodes in the tree.",
                    "label": 0
                },
                {
                    "sent": "And what we try to do is we'll try to maximize maximize this score which is basically nothing else then the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Words which appear in the document.",
                    "label": 0
                },
                {
                    "sent": "These are like all the words, all the words which appear in a path from the node which we are comparing to the root of the tree and we like divide by the union of all words in this path and.",
                    "label": 1
                },
                {
                    "sent": "Just like a side note, we tested also like other scoring function or like similar form and there is not much difference between them.",
                    "label": 0
                },
                {
                    "sent": "And so, like going back to the explaining the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So when we create this node, the words we will put in the notes are basically the words the document brought into this, like vocabulary which is like spans from the root of the tree to the leaf to this new node.",
                    "label": 0
                },
                {
                    "sent": "And just like a small example.",
                    "label": 0
                },
                {
                    "sent": "So if you want to insert insert.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This small guy here will compare.",
                    "label": 0
                },
                {
                    "sent": "It will compare it to all nodes and when we compare it to this note.",
                    "label": 1
                },
                {
                    "sent": "For instance, we compare it to all the words which are which are in this path or when we compare it to this to this node we we calculate the score function using all words which appear in this path and then we connect it to the best scoring load.",
                    "label": 1
                },
                {
                    "sent": "Um so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean just showing the simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of things you can do with it.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can introduce a stop words node, so it makes sense to have in the root of the tree to have like a load with most common sense like common words like stop words and also some other words.",
                    "label": 1
                },
                {
                    "sent": "And we experimented with lots of different lists of stop words and at the end we decided to use like 8 English stop words like and so on.",
                    "label": 1
                },
                {
                    "sent": "And we also added words which appeared like in more than 80% of the documents in the end.",
                    "label": 0
                },
                {
                    "sent": "This root of the tree.",
                    "label": 0
                },
                {
                    "sent": "Consisted about 20 words.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One drawback of this basic tree, as we call it, is that so that structure rebuilt is like depends on the ordering of the document.",
                    "label": 1
                },
                {
                    "sent": "So what we do then you say instead of like taking documents in some random order we like do this like greedy greedy search innocence.",
                    "label": 1
                },
                {
                    "sent": "So we always add, add, add a document into this already build structure which like has the maximal score out of all documents.",
                    "label": 0
                },
                {
                    "sent": "So just like some simple greedy approach.",
                    "label": 0
                },
                {
                    "sent": "And the third.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think which is again nothing special is that.",
                    "label": 0
                },
                {
                    "sent": "I mean, we notice that like these hierarchies are not really hierarchies, but they're basically graph.",
                    "label": 1
                },
                {
                    "sent": "So not even this web directories, they're not, they're not taxonomies or hierarchies.",
                    "label": 0
                },
                {
                    "sent": "But there are graphs you have this cross links among the categories, and here we also like.",
                    "label": 0
                },
                {
                    "sent": "Try to imitate that.",
                    "label": 0
                },
                {
                    "sent": "So again we like start with the stop words node and then we'll be adding notes, notes, loads to documents as nodes and will connect to all to all the nodes where the score.",
                    "label": 1
                },
                {
                    "sent": "So the like the number of common of common words is greater than some predefined threshold and.",
                    "label": 0
                },
                {
                    "sent": "What will usually happen is that this graph will be like a star, so we need at the end we need to remove this stop words notes so that the distance between the document will be something greater than two, right what?",
                    "label": 0
                },
                {
                    "sent": "So if I have a stop words node, so I mean the hypothesis is every every document will have a staff person.",
                    "label": 0
                },
                {
                    "sent": "So what can happen is that every document is is attached to this center of a star and then the distances are almost meaningless, so we need to remove this software it snowed.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So having described all the algorithms will decide how to measure the similarity between features of documents.",
                    "label": 1
                },
                {
                    "sent": "So basically we're having we have two documents and we do like.",
                    "label": 1
                },
                {
                    "sent": "For each feature in first document, we'll compare it.",
                    "label": 0
                },
                {
                    "sent": "All other features in the second document, and this is like it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not linear with the size of the document.",
                    "label": 0
                },
                {
                    "sent": "And since we have a topology we use, we won't treat features as independent, but will use this technology to measure to measure the distance is and what we can simple thing to do.",
                    "label": 1
                },
                {
                    "sent": "This is to use like shortest paths on the graph and then we can do like the unrated version, meaning that the length of each edge is 1 or we can do the.",
                    "label": 0
                },
                {
                    "sent": "The weighted version where each edge edge is weighted by 1 minus score, meaning that like if the documents are two more similar than the length of the edge is shorter.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "And basically so basically this this says exactly as I said here.",
                    "label": 0
                },
                {
                    "sent": "So basically for every for every pairs of features from the documents we calculated similarity and sum it up.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the experimental setup we were using this the new writers and we just like consider the random sample of 1000 documents.",
                    "label": 0
                },
                {
                    "sent": "So it was a small experience and we did like 10 fold cross validation and we evaluated like the the.",
                    "label": 1
                },
                {
                    "sent": "The quality of our distance measure using the kernel alignment and what this thing basically does is that it compares the distances among the documents from the same class but with the distance is among the documents from different classes and you basically want this ratio to be as large as possible, so alignment of 1 means that like all the documents from the same class have distance zero and from different classes have distance one.",
                    "label": 1
                },
                {
                    "sent": "Um and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'll show like 333 slides of with results feature distance, meaning that quadratic algorithm I showed or since every every document is basically also on edge.",
                    "label": 0
                },
                {
                    "sent": "So when we are adding a document, we first create an edge and then link it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We can also just do like this direct shortest path between the document.",
                    "label": 1
                },
                {
                    "sent": "So when you say compared to documents or just find where in the graph they reside and I calculate the path and what I mean.",
                    "label": 0
                },
                {
                    "sent": "So what we can see from here is like we get a small improvement, but it's not significant over the cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "And notice that here is no stop words now so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going forward and introducing stopwords node really increases the performance.",
                    "label": 0
                },
                {
                    "sent": "So what we what we observe is that this so this combination.",
                    "label": 0
                },
                {
                    "sent": "So using the feature distance and waited path later, shortest paths on the graph is like the best.",
                    "label": 0
                },
                {
                    "sent": "So this is still basically.",
                    "label": 0
                },
                {
                    "sent": "But we introduced the stuff where it snowed the optimal tree, meaning that we are using these greedy heuristics to select documents is a bit better.",
                    "label": 0
                },
                {
                    "sent": "We can also observe that for instance.",
                    "label": 0
                },
                {
                    "sent": "No, distance is always is always worse than using feature distance, which also makes sense.",
                    "label": 0
                },
                {
                    "sent": "And for instance, for the last.",
                    "label": 0
                },
                {
                    "sent": "So this is where we created a graph.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here the results are different.",
                    "label": 0
                },
                {
                    "sent": "Basically see that just like the length.",
                    "label": 0
                },
                {
                    "sent": "The length of the shortest path and using the feature distance does does the best.",
                    "label": 0
                },
                {
                    "sent": "And all these like differences are.",
                    "label": 0
                },
                {
                    "sent": "Significant so just.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a summary, what we what I showed?",
                    "label": 0
                },
                {
                    "sent": "So this is this is how random would do so.",
                    "label": 0
                },
                {
                    "sent": "I mean, you couldn't do worse than this.",
                    "label": 0
                },
                {
                    "sent": "So then this is the like standard cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "This is when you use the basic tree when and when you use a basic 3 plus stop words node, you really get an increasing performance if you use like optimal tree, but no stop where it snowed.",
                    "label": 0
                },
                {
                    "sent": "Then you don't get like significant performance.",
                    "label": 0
                },
                {
                    "sent": "So basically the ordering of the document does not like change much in terms of performance and from all the algorithms.",
                    "label": 1
                },
                {
                    "sent": "Basically this optimal tree with.",
                    "label": 1
                },
                {
                    "sent": "We stop with this node does does does the best, but I mean but basically all these all the last three are in the same range of performances.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said, stop words improve results.",
                    "label": 0
                },
                {
                    "sent": "The document of the ordering of documents of how we like, insert them into the this hierarchy or history does not degrade performance.",
                    "label": 1
                },
                {
                    "sent": "That optimal.",
                    "label": 0
                },
                {
                    "sent": "3 meaning that greedy strategy of picking documents with stop words performance best.",
                    "label": 0
                },
                {
                    "sent": "Using feature distance.",
                    "label": 1
                },
                {
                    "sent": "Outperforms the node distance, but feature distance is quite expensive to calculate.",
                    "label": 0
                },
                {
                    "sent": "Um and weighted paths are like.",
                    "label": 0
                },
                {
                    "sent": "I mean quite bad.",
                    "label": 0
                },
                {
                    "sent": "Alot better than than just using the simple distance on a graph.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that we also like tried so we try to create this simple simple trees first, withhold documents and the second with paragraph.",
                    "label": 0
                },
                {
                    "sent": "So you could say that like a paragraph paragraph is like a coherent unit talking about the one thing right.",
                    "label": 0
                },
                {
                    "sent": "But what we observe that basically paragraphs do worse than the documents.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just conclusions and further work.",
                    "label": 1
                },
                {
                    "sent": "So basically represented just like first few steps.",
                    "label": 0
                },
                {
                    "sent": "Towards I know building something which may eventually be better than TF, IDF and 1st next step to do would basically be to observe whether we can do like some build, some kind of probabilistic generation mechanism which would generate documents which would eventually have like this power law, degree distribution, power law, distribution of the words and this would probably like motivate them.",
                    "label": 0
                },
                {
                    "sent": "The choice of document similarity and things like that.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, done, thanks.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Stupid question, why not optimize over the?",
                    "label": 0
                },
                {
                    "sent": "The order of insertion into the graph.",
                    "label": 0
                },
                {
                    "sent": "Oh I think I. I think I did that, but there was like.",
                    "label": 0
                },
                {
                    "sent": "I mean I should have done that, but I don't remember.",
                    "label": 0
                },
                {
                    "sent": "Be really honest.",
                    "label": 0
                },
                {
                    "sent": "I think I tried it, but there wasn't like anything anything.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you could.",
                    "label": 0
                },
                {
                    "sent": "You could do that.",
                    "label": 0
                },
                {
                    "sent": "I mean with this graph the problem is like like it's really simple, right?",
                    "label": 0
                },
                {
                    "sent": "You have to like guess this threshold and that's like.",
                    "label": 0
                },
                {
                    "sent": "I mean it's.",
                    "label": 0
                },
                {
                    "sent": "Yes, you put on that.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                },
                {
                    "sent": "You have to find and.",
                    "label": 0
                },
                {
                    "sent": "So I know.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have to find something better there.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second question really to comment.",
                    "label": 0
                },
                {
                    "sent": "So you found that removing Stopwords helped a lot, and those are also the high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "I mean, it had kept the help of it right because it's like separated the documents in some sense, so it helps anyway.",
                    "label": 0
                },
                {
                    "sent": "And there's also the high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing that people find which doing sort of segmentation of graph based structures is that if you do normalization.",
                    "label": 0
                },
                {
                    "sent": "Sort of down weight, highly weighted things that could help quite a lot.",
                    "label": 0
                },
                {
                    "sent": "So you should positive possibly decide whether you should somehow normalizing for the number of incoming links.",
                    "label": 0
                },
                {
                    "sent": "Links, OK, good idea, although isn't this just more like 3 than a graph?",
                    "label": 0
                },
                {
                    "sent": "Search algorithm, I mean you have like a star.",
                    "label": 0
                },
                {
                    "sent": "So basically everything more or less everything is connected to this stuff where Snowden then you also have like connections among among the.",
                    "label": 0
                },
                {
                    "sent": "But that I can't measure the degree distribution of this of this, that would be also like.",
                    "label": 0
                },
                {
                    "sent": "I like the last points by probabilistic generation generating documents, but of course that's going to be.",
                    "label": 0
                },
                {
                    "sent": "I don't quite see how you fit that in with what you want to do here, because if you construct an old tree then you're going to be in a bit of trouble around for generating document holistically.",
                    "label": 0
                },
                {
                    "sent": "I mean you could.",
                    "label": 0
                },
                {
                    "sent": "You could always do is to have some kind of.",
                    "label": 0
                },
                {
                    "sent": "I know random walk something on this graph.",
                    "label": 0
                },
                {
                    "sent": "I mean in a tree it's obvious right start from some.",
                    "label": 0
                },
                {
                    "sent": "From some child and you would like go go out for instance.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can do many ways.",
                    "label": 0
                },
                {
                    "sent": "I don't have an answer.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's just like the first steps and seems promising.",
                    "label": 0
                },
                {
                    "sent": "It's all.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a bit ad hoc as I as I showed it so far, but.",
                    "label": 0
                },
                {
                    "sent": "Really, really like to do is have something more that you could fit the probabilistic mechanism to the data you know.",
                    "label": 0
                },
                {
                    "sent": "We used likelihood and you would learn the structure and then zip slow with just be.",
                    "label": 0
                },
                {
                    "sent": "I think that's a good wish.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the idea is that the power law would come out of the generation methods that you were suggesting that you know you pick a random node in the tree and then clearly you know the chances are.",
                    "label": 0
                },
                {
                    "sent": "If it's quite low in the tree, it will be very infrequent, but then when you look at the park going back up, the higher you go up, the more frequently those nodes will be used when you in the generation mechanism.",
                    "label": 0
                },
                {
                    "sent": "And then I mean, you just randomly select some words from each of the nodes as you move guys move up the tree so it would give you the power or coming out of that, But you're saying that we would want to actually have the the generation mechanism somehow influence how you design the tree original.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well I'm gonna guess.",
                    "label": 0
                },
                {
                    "sent": "It also doesn't guess one of the things that's nice.",
                    "label": 0
                },
                {
                    "sent": "How about policyholders Gen things.",
                    "label": 0
                },
                {
                    "sent": "I'd like it.",
                    "label": 0
                },
                {
                    "sent": "Hurts that you can use to school.",
                    "label": 0
                },
                {
                    "sent": "Sure documents under how you could get that didn't have a proper probability thing and if you don't have a tree, if you have an on tree.",
                    "label": 0
                },
                {
                    "sent": "Capital with a senior probability distribution.",
                    "label": 0
                },
                {
                    "sent": "If you could direct links.",
                    "label": 0
                },
                {
                    "sent": "Yes, if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you have a problem, don't have a treat and then you know there are problems, but you could imagine that you're seeing a random walk you know following.",
                    "label": 0
                },
                {
                    "sent": "Certain may be biased towards certain.",
                    "label": 0
                },
                {
                    "sent": "So I think, yeah, it's it's certainly one of the next things to really thinking.",
                    "label": 1
                },
                {
                    "sent": "Generated.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "On the order of The Alchemist.",
                    "label": 0
                },
                {
                    "sent": "I did I did this.",
                    "label": 0
                },
                {
                    "sent": "I did this experience so the trees were not so deep.",
                    "label": 0
                },
                {
                    "sent": "They were like something like 16 levels, maybe four.",
                    "label": 0
                },
                {
                    "sent": "I was building up like 10,000 documents.",
                    "label": 0
                },
                {
                    "sent": "So the problem the problem here is not building a tree.",
                    "label": 0
                },
                {
                    "sent": "Would be would be would be expensive expenses then calculating this quadratic.",
                    "label": 0
                },
                {
                    "sent": "Features I mean yeah distances, so the trees they were like so obviously the highest degree, the highest degree node was like the root and then and then I think there was like about 20 levels of of this of this end.",
                    "label": 0
                },
                {
                    "sent": "But I didn't like really, really draw.",
                    "label": 0
                },
                {
                    "sent": "I was like we were.",
                    "label": 0
                },
                {
                    "sent": "We were inspecting the words in the in the nodes.",
                    "label": 0
                },
                {
                    "sent": "That was more important to us, seeing that if there are there words which like make sense to be to be present together.",
                    "label": 0
                },
                {
                    "sent": "So that was more that we focused on.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm for calculating distances in the tree.",
                    "label": 0
                },
                {
                    "sent": "So I don't know whether that would help or not.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                },
                {
                    "sent": "If you want to use it, that's it's called the.",
                    "label": 0
                },
                {
                    "sent": "We need to make some chemistry and that's what you can do in your time.",
                    "label": 0
                },
                {
                    "sent": "You OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "I have a short comment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically in the beginning you said that you don't want to build this ontology, but in some way you you are approximating some kind of ontology.",
                    "label": 0
                },
                {
                    "sent": "So so we have discussion.",
                    "label": 0
                },
                {
                    "sent": "Yes is already on this so maybe even interesting to try first to compare.",
                    "label": 0
                },
                {
                    "sent": "How the same approach would work on ontology?",
                    "label": 0
                },
                {
                    "sent": "Because basically entology or this taxonomy of terms have seen this kind of hierarchical structure and somehow would be able to compensate this power law in the same way, except that it's more kind of reliable.",
                    "label": 0
                },
                {
                    "sent": "At least you would call it and told you at least you have some sense.",
                    "label": 0
                },
                {
                    "sent": "Well, here we are building something.",
                    "label": 0
                },
                {
                    "sent": "Which way did you try if the structure becomes similar to something like topic structure in the industry?",
                    "label": 0
                },
                {
                    "sent": "Before I could bring some like really shallow right for the new one?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but just by looking for the the three structure did you get let's say I don't know financial documents and then different you get like sets of sets of words which makes sense like you get a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "But there are also like in the nodes that are there are sets of words which which are which are there and you get some kind of like more and more specialized.",
                    "label": 0
                },
                {
                    "sent": "But I mean if you would like, the idea was to get something simple so that people could use it in a sentence, right?",
                    "label": 0
                },
                {
                    "sent": "If you come up with the two big machinery, them.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, OK.",
                    "label": 0
                }
            ]
        }
    }
}