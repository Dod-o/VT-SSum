{
    "id": "m3ad2gkwwyefresykvm35qzuibwhctrk",
    "title": "Feature Selection for Support Vector Regression Using Probabilistic Prediction",
    "info": {
        "author": [
            "Jian-Bo Yang, Department of Mechanical Engineering, National University of Singapore"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/kdd2010_yang_fssv/",
    "segmentation": [
        [
            "OK, good morning everyone.",
            "It's my honor to make a presentation of our recent work feature selection for support vector regression using probabilistic prediction.",
            "This is my supervisor perform changing on.",
            "We are from National University of Singapore."
        ],
        [
            "This is outline of this talk."
        ],
        [
            "OK. Let's go to the."
        ],
        [
            "Introduction Here I briefly introduce some background of the feature selection for regression problem.",
            "Basically, feature selection is a technique of selecting optimal feature among the original feature set by removing, removing irrelevant or redundant features.",
            "There are many benefit for feature selections such as increasing the system interpretability, improving generalization performance, or minimizing the overfitting for some learning algorithm.",
            "As the previous presentation mentioned, there are basically feature selection can be classified as two types, filter method and rapid method and generally represented performs better than filter method but carry with them have your computational loop.",
            "OK, the challenge feature selection for regression problem.",
            "Know that the most existing feature selection method.",
            "For classification problems, however, the target in the regression problem is continuous rather than discrete.",
            "So if using the feature selection for classification or regression problem, it may not work well becausw it may cause the potential loss of the important ordinal information."
        ],
        [
            "Here the regression problem is chosen as a support vector regression.",
            "It's also possible to use other regression learning algorithm.",
            "Give me a data set the standard as they are basically solve this optimization problem and the regression function is given here.",
            "Ivax is a continuous variable.",
            "The problem of this regression function is provided an estimate FX for output Y for any X.",
            "But cannot provide the information on the confidence level of of this estimate.",
            "To solve this problem, many effort has been done."
        ],
        [
            "One popular.",
            "Approach birthday given by the shop is too late, though why you call to FX plus some noisy data here.",
            "This noisy Delta is belonging to the loss distribution Gaussian distribution with IID assumption.",
            "Equivalently.",
            "This this expression implies that the density function of Y forgiven X can be expressed like this.",
            "The first one is loveless distribution.",
            "The second one is a Gaussian distribution.",
            "So the here the Parimeter Sigma is obtained by the maximizing the likelihood functions.",
            "In the proposed method, this two conditional density function will be fully used.",
            "Make use of."
        ],
        [
            "OK proposed matter."
        ],
        [
            "This is our feature ranking criteria.",
            "We measure the importance of feature J.",
            "By evaluating the differences of this List 2.",
            "And the functions over the whole feature SpaceX.",
            "Where X -- G is the sample X.",
            "With this feature removed.",
            "There are many ways to measure the differences of two density distributions.",
            "Here we choose a QR damages.",
            "The motivation of the proposed criteria is very clear.",
            "The greater the KL divergences between these two density functions.",
            "Over the access space, the greater the importance of the gist feature.",
            "Since there Buddy did the features in this data set, so if a full ranking list of feature is needed, this as the matter has to be evaluated three times, each time with a different X -- J since the conditional density function is estimated by the trend as VM as I mentioned previously.",
            "So if you train the train, the SVR 4D times.",
            "This is obviously a very expensive procedure, so next."
        ],
        [
            "We use a random permutation technique to avoid this shortcoming.",
            "The idea of the random permutation is very simple.",
            "This data set D. If you want to random permuted Jess feature in the dataset E, you only need 2 random permute all the elements in the future G under the uniform distribution.",
            "After with this random permutation, we can get a very important result is shown in the theorem in our previous work, assuming data set samples are sufficiently rich, these two density functions are same.",
            "That means removing one feature J is equivalent to random permute feature G."
        ],
        [
            "So with the theorem we can equivalent Re Express the proposed criteria as this form.",
            "So in this way, this equation we only need to train SVR once and random permute.",
            "Feature J 4D times is known that the random permutation is much computationally cheaper than the training as they are.",
            "So the computational advantage is very clear.",
            "This is a demonstration of the proposed criteria when D equal to 1.",
            "So in our criteria we measure this two.",
            "Density the difference of these two density functions over the whole feature space."
        ],
        [
            "To make the proposed criteria computable, we do the following two approximations.",
            "First one is approximated integration by summation.",
            "Second one is we approximate this two density function by the proximity.",
            "Laplace distribution argument is to distribution as I mentioned previously."
        ],
        [
            "OK, since the hear the expression for Loblaws distribution and caused this distribution is known, so the explicit form of the proposed criteria is known.",
            "Here I just give you an example.",
            "When P is approximated by Pierre.",
            "Is given here.",
            "So finally as D matter can be used together with the standard recursive feature elimination, it started with all features.",
            "Then delete the feature with the smallest value of SD until no feature left.",
            "So with this criteria, with this scheme, we can yield a ranking list of the features."
        ],
        [
            "Experiment in this experiment will compare our proposed method with this three existing matter.",
            "First one is the filter method.",
            "This one is also filter method.",
            "The last one is rapid method.",
            "We choose a mean square error rate as a performance evaluation criteria and we also compare the.",
            "We are the parents student test between the proposed method and benchmark method conducted.",
            "These are two hypothesis in the test and the significance level is chosen as a 0.05.",
            "The experiment is done on the artificial problems and real world problem for each problem where you have to run the experiment for 30 relations."
        ],
        [
            "Here is a artificial problem.",
            "This is a description of the three artificial problems.",
            "And this is target concept for each problem.",
            "We can see that the first problem 2 features are important.",
            "The second one is 531 is also five, and for each problem we use a different values.",
            "Different size of the training sample size.",
            "To do the experiment."
        ],
        [
            "Here is the result for the artificial problem.",
            "This table shows the number of relation between the of the known important features ranked in that operation over the 30 relations.",
            "So from these three tables we can see that the proposed method is significantly better than the.",
            "Of other three benchmark method and the advantage is more significant.",
            "When the training sample size is small."
        ],
        [
            "There is a some figure shows the mean square error rate against number of Top Rank features for six setting of the first artificial problems from this figure.",
            "This figure we can see that the for the proposed method it attend the lowest, most mean mean squared error rate when only two features are given.",
            "However, this is not the case for others, we matter."
        ],
        [
            "Here is a six year word in asset is a description of the data."
        ],
        [
            "Set.",
            "I only show the result for the one data set.",
            "This is figure shows the mean square error rate against the number of Top Rank features.",
            "Again, proposed method generally yield the lowest mean square error and this is confirmed by the student T test.",
            "This table of this kind item means that proposed method significantly better than the benchmark method."
        ],
        [
            "So from this result we can see that proposed method generally perform at least as well, if not better than other methods and other datasets.",
            "So the similar patterns and more details can be found in the paper."
        ],
        [
            "Conclusion.",
            "In this paper, we present a new wrapper based feature selection method by using the probabilistic output of SVR.",
            "I know the experiment experimental results show that the proposed method perform at least as well, if not better than some existing benchmark method.",
            "And the advantage of the proposed method is significant when their training data set is sparse.",
            "Finally, as a wrapper, measure, the computational cost is moderate.",
            "OK, that's all.",
            "Thank you very much."
        ],
        [
            "The random permutation.",
            "Here we need assumption that data set is subject sufficiently rich.",
            "So, but there is.",
            "We don't know how much is subset exactly, but according to our X."
        ],
        [
            "German.",
            "Is it even for the training sample size?",
            "Is only 20 or 30.",
            "Our method is still can yield the satisfying result.",
            "So I think that if we want to analyze more theoretically, it will be the our future work.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "It's my honor to make a presentation of our recent work feature selection for support vector regression using probabilistic prediction.",
                    "label": 1
                },
                {
                    "sent": "This is my supervisor perform changing on.",
                    "label": 1
                },
                {
                    "sent": "We are from National University of Singapore.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is outline of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Let's go to the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction Here I briefly introduce some background of the feature selection for regression problem.",
                    "label": 0
                },
                {
                    "sent": "Basically, feature selection is a technique of selecting optimal feature among the original feature set by removing, removing irrelevant or redundant features.",
                    "label": 1
                },
                {
                    "sent": "There are many benefit for feature selections such as increasing the system interpretability, improving generalization performance, or minimizing the overfitting for some learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "As the previous presentation mentioned, there are basically feature selection can be classified as two types, filter method and rapid method and generally represented performs better than filter method but carry with them have your computational loop.",
                    "label": 0
                },
                {
                    "sent": "OK, the challenge feature selection for regression problem.",
                    "label": 0
                },
                {
                    "sent": "Know that the most existing feature selection method.",
                    "label": 0
                },
                {
                    "sent": "For classification problems, however, the target in the regression problem is continuous rather than discrete.",
                    "label": 1
                },
                {
                    "sent": "So if using the feature selection for classification or regression problem, it may not work well becausw it may cause the potential loss of the important ordinal information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the regression problem is chosen as a support vector regression.",
                    "label": 0
                },
                {
                    "sent": "It's also possible to use other regression learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Give me a data set the standard as they are basically solve this optimization problem and the regression function is given here.",
                    "label": 0
                },
                {
                    "sent": "Ivax is a continuous variable.",
                    "label": 0
                },
                {
                    "sent": "The problem of this regression function is provided an estimate FX for output Y for any X.",
                    "label": 1
                },
                {
                    "sent": "But cannot provide the information on the confidence level of of this estimate.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem, many effort has been done.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One popular.",
                    "label": 0
                },
                {
                    "sent": "Approach birthday given by the shop is too late, though why you call to FX plus some noisy data here.",
                    "label": 0
                },
                {
                    "sent": "This noisy Delta is belonging to the loss distribution Gaussian distribution with IID assumption.",
                    "label": 0
                },
                {
                    "sent": "Equivalently.",
                    "label": 0
                },
                {
                    "sent": "This this expression implies that the density function of Y forgiven X can be expressed like this.",
                    "label": 1
                },
                {
                    "sent": "The first one is loveless distribution.",
                    "label": 0
                },
                {
                    "sent": "The second one is a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So the here the Parimeter Sigma is obtained by the maximizing the likelihood functions.",
                    "label": 1
                },
                {
                    "sent": "In the proposed method, this two conditional density function will be fully used.",
                    "label": 0
                },
                {
                    "sent": "Make use of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK proposed matter.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is our feature ranking criteria.",
                    "label": 0
                },
                {
                    "sent": "We measure the importance of feature J.",
                    "label": 0
                },
                {
                    "sent": "By evaluating the differences of this List 2.",
                    "label": 0
                },
                {
                    "sent": "And the functions over the whole feature SpaceX.",
                    "label": 0
                },
                {
                    "sent": "Where X -- G is the sample X.",
                    "label": 1
                },
                {
                    "sent": "With this feature removed.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to measure the differences of two density distributions.",
                    "label": 0
                },
                {
                    "sent": "Here we choose a QR damages.",
                    "label": 0
                },
                {
                    "sent": "The motivation of the proposed criteria is very clear.",
                    "label": 0
                },
                {
                    "sent": "The greater the KL divergences between these two density functions.",
                    "label": 0
                },
                {
                    "sent": "Over the access space, the greater the importance of the gist feature.",
                    "label": 1
                },
                {
                    "sent": "Since there Buddy did the features in this data set, so if a full ranking list of feature is needed, this as the matter has to be evaluated three times, each time with a different X -- J since the conditional density function is estimated by the trend as VM as I mentioned previously.",
                    "label": 0
                },
                {
                    "sent": "So if you train the train, the SVR 4D times.",
                    "label": 0
                },
                {
                    "sent": "This is obviously a very expensive procedure, so next.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use a random permutation technique to avoid this shortcoming.",
                    "label": 0
                },
                {
                    "sent": "The idea of the random permutation is very simple.",
                    "label": 1
                },
                {
                    "sent": "This data set D. If you want to random permuted Jess feature in the dataset E, you only need 2 random permute all the elements in the future G under the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "After with this random permutation, we can get a very important result is shown in the theorem in our previous work, assuming data set samples are sufficiently rich, these two density functions are same.",
                    "label": 1
                },
                {
                    "sent": "That means removing one feature J is equivalent to random permute feature G.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with the theorem we can equivalent Re Express the proposed criteria as this form.",
                    "label": 0
                },
                {
                    "sent": "So in this way, this equation we only need to train SVR once and random permute.",
                    "label": 0
                },
                {
                    "sent": "Feature J 4D times is known that the random permutation is much computationally cheaper than the training as they are.",
                    "label": 0
                },
                {
                    "sent": "So the computational advantage is very clear.",
                    "label": 0
                },
                {
                    "sent": "This is a demonstration of the proposed criteria when D equal to 1.",
                    "label": 1
                },
                {
                    "sent": "So in our criteria we measure this two.",
                    "label": 0
                },
                {
                    "sent": "Density the difference of these two density functions over the whole feature space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To make the proposed criteria computable, we do the following two approximations.",
                    "label": 0
                },
                {
                    "sent": "First one is approximated integration by summation.",
                    "label": 0
                },
                {
                    "sent": "Second one is we approximate this two density function by the proximity.",
                    "label": 0
                },
                {
                    "sent": "Laplace distribution argument is to distribution as I mentioned previously.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, since the hear the expression for Loblaws distribution and caused this distribution is known, so the explicit form of the proposed criteria is known.",
                    "label": 0
                },
                {
                    "sent": "Here I just give you an example.",
                    "label": 0
                },
                {
                    "sent": "When P is approximated by Pierre.",
                    "label": 1
                },
                {
                    "sent": "Is given here.",
                    "label": 0
                },
                {
                    "sent": "So finally as D matter can be used together with the standard recursive feature elimination, it started with all features.",
                    "label": 1
                },
                {
                    "sent": "Then delete the feature with the smallest value of SD until no feature left.",
                    "label": 0
                },
                {
                    "sent": "So with this criteria, with this scheme, we can yield a ranking list of the features.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment in this experiment will compare our proposed method with this three existing matter.",
                    "label": 0
                },
                {
                    "sent": "First one is the filter method.",
                    "label": 0
                },
                {
                    "sent": "This one is also filter method.",
                    "label": 0
                },
                {
                    "sent": "The last one is rapid method.",
                    "label": 0
                },
                {
                    "sent": "We choose a mean square error rate as a performance evaluation criteria and we also compare the.",
                    "label": 0
                },
                {
                    "sent": "We are the parents student test between the proposed method and benchmark method conducted.",
                    "label": 1
                },
                {
                    "sent": "These are two hypothesis in the test and the significance level is chosen as a 0.05.",
                    "label": 0
                },
                {
                    "sent": "The experiment is done on the artificial problems and real world problem for each problem where you have to run the experiment for 30 relations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a artificial problem.",
                    "label": 0
                },
                {
                    "sent": "This is a description of the three artificial problems.",
                    "label": 1
                },
                {
                    "sent": "And this is target concept for each problem.",
                    "label": 0
                },
                {
                    "sent": "We can see that the first problem 2 features are important.",
                    "label": 0
                },
                {
                    "sent": "The second one is 531 is also five, and for each problem we use a different values.",
                    "label": 0
                },
                {
                    "sent": "Different size of the training sample size.",
                    "label": 0
                },
                {
                    "sent": "To do the experiment.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the result for the artificial problem.",
                    "label": 0
                },
                {
                    "sent": "This table shows the number of relation between the of the known important features ranked in that operation over the 30 relations.",
                    "label": 1
                },
                {
                    "sent": "So from these three tables we can see that the proposed method is significantly better than the.",
                    "label": 0
                },
                {
                    "sent": "Of other three benchmark method and the advantage is more significant.",
                    "label": 0
                },
                {
                    "sent": "When the training sample size is small.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a some figure shows the mean square error rate against number of Top Rank features for six setting of the first artificial problems from this figure.",
                    "label": 0
                },
                {
                    "sent": "This figure we can see that the for the proposed method it attend the lowest, most mean mean squared error rate when only two features are given.",
                    "label": 0
                },
                {
                    "sent": "However, this is not the case for others, we matter.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a six year word in asset is a description of the data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "I only show the result for the one data set.",
                    "label": 0
                },
                {
                    "sent": "This is figure shows the mean square error rate against the number of Top Rank features.",
                    "label": 1
                },
                {
                    "sent": "Again, proposed method generally yield the lowest mean square error and this is confirmed by the student T test.",
                    "label": 1
                },
                {
                    "sent": "This table of this kind item means that proposed method significantly better than the benchmark method.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from this result we can see that proposed method generally perform at least as well, if not better than other methods and other datasets.",
                    "label": 0
                },
                {
                    "sent": "So the similar patterns and more details can be found in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion.",
                    "label": 0
                },
                {
                    "sent": "In this paper, we present a new wrapper based feature selection method by using the probabilistic output of SVR.",
                    "label": 1
                },
                {
                    "sent": "I know the experiment experimental results show that the proposed method perform at least as well, if not better than some existing benchmark method.",
                    "label": 1
                },
                {
                    "sent": "And the advantage of the proposed method is significant when their training data set is sparse.",
                    "label": 0
                },
                {
                    "sent": "Finally, as a wrapper, measure, the computational cost is moderate.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The random permutation.",
                    "label": 0
                },
                {
                    "sent": "Here we need assumption that data set is subject sufficiently rich.",
                    "label": 0
                },
                {
                    "sent": "So, but there is.",
                    "label": 0
                },
                {
                    "sent": "We don't know how much is subset exactly, but according to our X.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "German.",
                    "label": 0
                },
                {
                    "sent": "Is it even for the training sample size?",
                    "label": 0
                },
                {
                    "sent": "Is only 20 or 30.",
                    "label": 0
                },
                {
                    "sent": "Our method is still can yield the satisfying result.",
                    "label": 0
                },
                {
                    "sent": "So I think that if we want to analyze more theoretically, it will be the our future work.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}