{
    "id": "wigdkelkspou42h6lqbndfjsrx3vtt26",
    "title": "The Regularization Frontier in Machine Learning",
    "info": {
        "author": [
            "Gilles Gasso, INSA of Rouen"
        ],
        "published": "Oct. 10, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd08_gasso_trfim/",
    "segmentation": [
        [
            "So let's start.",
            "Welcome everybody, and first of all I want to thank everybody to be here after this long week of working.",
            "And to be here despite the blanket, the poster reception and the social events we will have during this week of conference.",
            "So this afternoon I will try to.",
            "Introduce you the user formulation path in machine learning algorithm.",
            "These techniques was recently proposed in order to deal with model selection issues in an efficient way in machine learning algorithm.",
            "So this work is joint work with professors, different canoe and.",
            "Let's see the wood."
        ],
        [
            "Map of the afternoon.",
            "So first I will provide an introduction.",
            "And which will settle the framework for my talk and which will introduce also the model selection problem and the way we want to solve it using regularization path.",
            "After this we try to make some connection between regularization path and the parito optimality Frontier, which is well known in optimization community.",
            "Also, this preliminary notion are introduced.",
            "We can focus on the condition on that which we can have an efficient way to explore our regularization path.",
            "That is an efficient way to obtain the set of our solution according to a trade off.",
            "We want to make when we are when we want to serve a learning problem.",
            "And this condition will lead.",
            "To some famous algorithm we use usually integration.",
            "An example is the lesser algorithm and in classification binary classification is the SVM algorithm and I will show you in details how to derive the relation path for this kind of algorithm.",
            "Anne.",
            "I will show also that.",
            "The efficiency of this algorithm is related to the sparsity of the criteria we use in this kind of algorithm.",
            "And finally I will provide some empirical evaluation of the efficiency of the algorithm according to computational time, so it's strength and also some limits of the algorithm.",
            "Let's begin."
        ],
        [
            "If the introduction so here is the framework of the classical statistical learning problem.",
            "I assume here that I have a set of data XIYI where X is the input vector and why and why is the target output?",
            "So we assume also that.",
            "The pair XY is going from a distribution PXY, which is of course unknown.",
            "When we consider again world data.",
            "For too easy my talk and for simplicity sake I will just focus in this search on two kinds of problems and these two kinds are supervised learning binary classification using some SVM algorithm for example.",
            "So binary classification.",
            "In this case the outputs can take only two values minus one and plus one and regression problems where the outputs can have any value in their airspace.",
            "So to address our learning problem, we can use two kinds of approach.",
            "The first one is the generative approach, where which I will try to estimate the unknown distribution and to infer the output from the knowledge of this distribution.",
            "The second approach is to directly identify a learning function F which is able to predict correctly the value of my target output.",
            "So in the second I will suppose that my function F belongs to an input space H, so this space can be the space of linear function polynomial function defined over.",
            "Additionally whatever you want."
        ],
        [
            "What I want is to identify to build a learning function F which must be accurate on the training data, but also are all unseen data.",
            "So in this case we classically consider a loss function L and to achieve this objective to find a learning function which will perform very well.",
            "We had to minimize the expected priest, but in this definition of the expected risk, we considered the expectation of X&Y as our distribution is unknown, we can't.",
            "Do this minimization so people usually.",
            "Use the empirical loss function L. And in this case, this empirical function is defined like this in this case.",
            "To.",
            "In for some nice properties of our function F like like sparsity solution is something like this.",
            "You know that to avoid some overtraining phenomenon what we do, we put a regularizer or penalty or colleagues penalty term in the second which is called PF.",
            "So in this condition we have our learning problem which is given by this.",
            "When the power to Lambda realises tradeoff between the loss function, the quality of the model.",
            "And.",
            "PF.",
            "And this parameter is positive.",
            "And how to choose this parameter so?",
            "Sorry.",
            "Yep.",
            "Yes.",
            "I think I can do this if you consider some some kind of argument like manifold learning.",
            "Yeah.",
            "Again, you can apply it because in the in this framework will consider as a loss function.",
            "The loss on the label data and you add another cost point according to the manifold modeling of your data.",
            "So you can use it.",
            "Josh.",
            "Supervise yeah I see that too easy presentation at firstly focus on this kind of algorithm, But you can extend the framework to this kind of semi supervised learning.",
            "So choosing the best model F star.",
            "In this framework, is just equivalent to find the best value Lambda star according to this tradeoff."
        ],
        [
            "2 cities just one simple example.",
            "So here I have a nonlinear function to be estimate, so I don't know if you see where the tool function is.",
            "The way the green curve and the data the Black Stars.",
            "So I want to identify a nonlinear model using this data.",
            "What I do I choose a Canyon Ridge regression.",
            "I use a Gaussian kernel an.",
            "I see the value of Lambda in this case when I pick up a small value of Lambda.",
            "What happens you have overfitting which appear here.",
            "Choosing a small value of Lambda means that we don't give importance to the penalty term, so we prefer a precise model according to the data.",
            "So we have these over and over fitting.",
            "Phenomenal.",
            "When we pick up a high value of Lambda, we have two smooth solution.",
            "So what we want to retrieve is this solution where we choose.",
            "According to the value of Lambda and will obtain a fair wood solution according to our two function in winter."
        ],
        [
            "So to select the best value of Lambda, what we can do?",
            "We can use two approaches."
        ],
        [
            "The first one, the brute Force One, is to use in research.",
            "In this case, we will ask the user to provide a set of value, Lambda one to Lambda K to our algorithm, and for each value of Lambda we identify a model F. I've had Lambda and will.",
            "Test this model according to some generalization performance.",
            "Whatever you want, you can choose your generously performance and from this.",
            "We can pick up the best estimation of our model so.",
            "This estimation will depend on.",
            "The closeness of the grid we choose.",
            "So if the bill is very close, will have a half approximation of our Jesus in performance.",
            "And in this case we cannot have a good solution here.",
            "If you choose a fan, read what you will have.",
            "You know that we have a good approximation of generalization error, but in this case we will spend a lot of time in order to identify our model.",
            "What we want to do?"
        ],
        [
            "And this is the second approach we can apply is.",
            "To compute the organization path, what is what is this path?",
            "We want.",
            "The user not to specify any value.",
            "We won't automatically find the set of our solution according to the different value of Lambda.",
            "So here Lambda is between zero and Infinity, so for each value of Lambda we want the argument to find automatic and dissolution.",
            "So we can validate each solution according to our journalism performance.",
            "And we can retrieve the best one.",
            "This is the formal definition of the realization.",
            "Tough how to obtain it will see this later but before this.",
            "I will try just to make a connection."
        ],
        [
            "Between this definition of regularization path and the operator frontier, which is well known, ocean in optimization community."
        ],
        [
            "So to do this, let's take a simple example.",
            "Here we have a linear regression example, so I just consider linear model defined like this with better the parameter vector and better is the dimensional vector.",
            "The problem we want to serve is to minimize discourse.",
            "At least workers under the constraints of.",
            "Square last night on our character data.",
            "So the solution is well known is given by this.",
            "Where here is an identity matrix, X is the matrix of the covariance and the vector Y is the vector of the target outputs.",
            "The organization path in this context is just defined by the value of beta according to the difference value of Lambda.",
            "So here what we can remark, if we set Lambda equal to 0 does mean that we drop down the penalty, so we get the square solution, which is well known.",
            "If we set Lambda equal to Infinity, we enforce our constraint.",
            "So to avoid that this criterion blows up, we will set better equal to 0.",
            "So this these are well known results."
        ],
        [
            "And to emphasize things, I will just take.",
            "1 dimensional example.",
            "In this one dimensional example of what I do, so better is just scalar here.",
            "And I can write my last L and my penalty better.",
            "I can relate things like this using my distribution, so here AB&C are parameters which will depend on the XIYI and the chosen value of Lambda.",
            "So.",
            "This relation.",
            "Is shown by the red curve here.",
            "And this curve.",
            "So on the X axis we have the penalty and Y axis.",
            "You have the loss.",
            "On this curve.",
            "We have about which is very interesting in.",
            "This part is just from here from here WHI let's see this point, the green one.",
            "This green points correspond to the solution beta equal to 0.",
            "So indeed here we have the penalty which is equal to 0.",
            "And here we have the loss which just equal to the norm of the output vector Y.",
            "And so these correspond according to our previous analysis.",
            "This correspond just to the solution when we set Lambda equal to Infinity.",
            "When we set Lambda equal to 0, so here we get the list for solution and this solution corresponds to the minimum of our loss here.",
            "So between those two solution when we vary the value of Lambda, we can see that.",
            "We will explore this part of the curve.",
            "So intuitively.",
            "The blue part of my curve.",
            "Is somehow the definition of migration path, so finding my my path is just finding this part of my girl."
        ],
        [
            "Let's relate this to the concept of Battle Frontier.",
            "So in my optimization problem I have two conflicting goals, the loss and the penalty.",
            "So according to my curve here.",
            "All this area corresponds to the set of admissible solution.",
            "And in this set, let's.",
            "Analyze this quote, the Red one and we compare this point to this one.",
            "This word is most interesting for us, why?",
            "If you look at this point, we have the penalty and the loss which are greater than the penalty and the loss of this point.",
            "So we've saved at this point the purple One dominates the right .1.",
            "And formally, the notion of dominance is defined by this definition.",
            "So if we pick up two solutions, better one and better too.",
            "Would say that better one dominates better too if for both.",
            "Better one performs well done better too.",
            "So according to this definition, if you look at the set of all the minutes solution, we get the Parrot frontier, which is the optimality frontier between our two criteria so.",
            "Here we saw that you can see that the power to frontier is nothing else than the regularization path, so doing this is equivalent to.",
            "Looking at the paragraph on here."
        ],
        [
            "This way of exploiting the party frontier.",
            "So in our classical formulation of learning problem, we use this formulation.",
            "This language and formulation, that is, we take a linear combination of the loss and the penalty.",
            "And this linear combination is given by this.",
            "So in the plane LP, what we do we just?",
            "Consider that the line which is 1000 tangents to our part of culture in at a certain point.",
            "So if we change the value of Lambda will change the slope of the line and we will change our solution.",
            "So this is the classical way to consider options."
        ],
        [
            "The second formulation is to minimize the loss under inequality constraints on the norm of our vector.",
            "So by constraining the penalty term in this case, this correspond to this situation and you can see that according to this formulation, if you just.",
            "You can be able to find a value of Lambda corresponding to these constants C and we can.",
            "Lucia, the land which correspond to this situation.",
            "So formulation one information, two equivalence."
        ],
        [
            "And the last formulation, the third one, is just this.",
            "When we minimize the penalty term under the constraints on the least square.",
            "So here this formulation can be interesting in.",
            "For instance, in some application like compressive sensing, when we have some idea about the noise inside our data, so we can set this C prime constant here, like the noise we guess in our data.",
            "So this situation correspond to these kinds of constraints and we see that we are able also to find.",
            "A line which is 10,000 two at this point.",
            "So this reformulation are equivalent.",
            "But be aware that this works.",
            "Order the cost and that's all.",
            "The criteria be colleagues.",
            "What happens?"
        ],
        [
            "When you are not in the non convex case when you are in the North of skis.",
            "Here we have an example when we consider.",
            "A convex penalty function and here and on convex penalty function.",
            "So here we can see that.",
            "We have a concave part in our Title Frontier and when we use the linear combination of our loss L&P.",
            "In this situation, we can retrieve our solution.",
            "Along the path.",
            "But here we can retrieve this solution.",
            "This one, but in the concave part we are not able to retrieve all the solution.",
            "So be aware that if you are using the language and formulation of your learning problem.",
            "Everything is OK when you are in this situation when you are in this situation, be aware that you lose the solution which are there."
        ],
        [
            "So make like let me make a short summarize.",
            "Learning.",
            "Everybody knows is a multi objective optimization problem.",
            "Computing the organization path is equivalent to explore the parity frontier.",
            "Our reformulation are equivalent.",
            "If you consider convex functions, otherwise will not retrieve all solution.",
            "And we can extend these two more than two criteria as I said.",
            "At the beginning of the talk, so we define all this notion.",
            "To do what?",
            "What we want to do?",
            "Is the way we can efficiently too in the value of Lambda?",
            "And to do this.",
            "Let's see."
        ],
        [
            "The third step of my talk and this step."
        ],
        [
            "To introduce this step, let's see this example.",
            "So I come back to my regression example.",
            "So he called that.",
            "The last function, the Fidelity and the organization parameter.",
            "I want to optimize.",
            "If I use the brute force."
        ],
        [
            "Methods that is, I consider only research algorithm.",
            "I specify K value of Lambda and for each value of Lambda Lambda one to Linda Kay.",
            "I compute this solution.",
            "The computation of this solution involves a complexity of the.",
            "Cubic.",
            "Discount my complexity is cubic according to the number of parameters I want to identify as I do this K times I have.",
            "K * D complexity.",
            "So this is the first one, but I can try to improve things.",
            "How to do it?"
        ],
        [
            "It's tricky way to do this is to use.",
            "The previous estimation better team I know, minus one to compute the current estimation better.",
            "T in this case I can use some warm start.",
            "Technique in order.",
            "To have very quickly my solution so I can apply some gradient conjugate integration and I can get my solution.",
            "So bad doing this.",
            "I drop the complexity of my algorithm.",
            "So instead of hiring different here, I just have a square complexity.",
            "Time is all the norm.",
            "The number of conjugate gradient and times K. So I can improve my complexity.",
            "Can I?"
        ],
        [
            "Do better lady.",
            "By the knowledge of my problem, when I have a solution with 80 -- 1.",
            "I can apply a gradient descent.",
            "Algorithm so I just take.",
            "The gradient descent direction and I calculate my solution according to this direction and using Lambda T which correspond to the solution I want to retrieve for better.",
            "So I guess here first solution.",
            "And from this solution I apply also my warm start.",
            "Iteration and I can get my solution metate.",
            "So here I suppose that.",
            "And crime here is less than L becausw I'm, I suppose that I'm close to the solution when I apply my conjugate gradient iteration.",
            "So.",
            "As you can see I improved the complexity for my algorithm, but."
        ],
        [
            "What I want is to do.",
            "More simple.",
            "That is here.",
            "Instead of using warm start and position step.",
            "If I'm able to use only the position step so I can calculate better T according to better T -- 1 using a linear relation.",
            "I'm the King of oil.",
            "Because I will have.",
            "Decrease the complexity of my algorithm in 2K the square.",
            "So in this case I have again of complexity in my algorithm.",
            "So what we want to do?",
            "If you want to efficiently explore our regularization path.",
            "If you can have this situation, it is very good for us because this situation correspond to the most.",
            "Decreasing complexity of our problem.",
            "So under which condition?"
        ],
        [
            "And can we do this?",
            "The answer was provided by Rossi in his paper one year ago.",
            "And this answer says that.",
            "If you have L&P and you want to have.",
            "A solution path which is piecewise linear.",
            "That is, we just apply the prediction step.",
            "The only condition is that one of your costs.",
            "Must be at most quadratic or piecewise quadratic, and the order cost must be at least this was linear.",
            "So L must be quadratic and P. Linear or key must be quadratic and linear.",
            "Both Atlantic and linear, and it works.",
            "This was linearity to sketch the proof of this theorem.",
            "Let's see, that is why linearity means that if we take.",
            "The variation of our solution better according to the variation of Lambda.",
            "If you have piecewise reality, we have this derivative which is quite constant.",
            "So on the average condition we have this equality.",
            "This equation.",
            "So little bit algebra.",
            "To show it.",
            "Consider a value Lambda known.",
            "So according to our optimization problem here.",
            "We can take the normal equation of this minimization problem, so here."
        ],
        [
            "You write down the optimality condition for Lambda, so we have these equation and we do the same for Lambda plus epsilon.",
            "The small variation of our key parameter.",
            "So we have this equation.",
            "Food.",
            "What we do next.",
            "We take a first order approximation of this equation around the solution beta Lambda.",
            "So this solution corresponds to the value of Lambda, and we do this.",
            "So you add on this first order approximation, so we get this solution and you can remark that here I drop them all terms which are of square_of epsilon becausw.",
            "I considered at this time are very small.",
            "And in this equation you have the Haitian of Defense system according to better Lambda and the Hessian of the last term according to Lambda, Beta, Lambda and so on.",
            "So combining this equation with this one.",
            "You can.",
            "See that this quantity is just equal to this.",
            "And if you want to have a piecewise linear behavior.",
            "Which condition we need?",
            "We need firstly that all this equation, the independent form Lambda.",
            "So I don't want to have Lambda here.",
            "So to have this condition I said that I would say that this region will be equal to 0.",
            "So here we don't have any dependence on Lambda.",
            "And to have this constant just need that this quantity, but the constant according to Lambda.",
            "If you do so, we can see that.",
            "This derivative is just equal to a constant, so here this content can deal in this case, you know that your loss function is just linear, and here this case means that your penalty function is also linear or piecewise linear.",
            "By this we showed the condition of our theorem.",
            "So to summarize it.",
            "If you want to compute very efficiently to your solution path, you want two condition.",
            "At most your loss or your penalty term must be quadratic, and at least they must be.",
            "Linear."
        ],
        [
            "Examples of function which.",
            "Fulfill this condition.",
            "So here we have.",
            "Lost.",
            "Plus for classification problem and here we have lots for regression problem.",
            "So for classification we have here 01 cost.",
            "Discourse is piecewise linear, but.",
            "It doesn't fulfill the condition of the theory of Rossi Becausw.",
            "Discussed is not convex.",
            "And our theorem said that we have we need convex function.",
            "The logistic loss here.",
            "Look like quadratic, but not the case.",
            "If you take the 2nd order derivative derivative is not equal to a constant, so this last doesn't.",
            "Meet the requirements of the term.",
            "The hinge loss.",
            "Which is very used in SVM algorithm is allows which fulfill this condition and its square version also met this requirement disagreement.",
            "From regression we have the epsilon insensitive loss which is defined here which is used in support vector regression algorithm and these kinds of loss fulfill this condition.",
            "And So what can I say?",
            "Also we have.",
            "The least square loss or the square penalty term, which is also piecewise linear OK. You have the Huber function, which is piecewise quadratic and used for unifying robust model."
        ],
        [
            "So if I play just a game of combination, I will see that I can retrieve these table where if I consider an L2 type loss what I call in two tab is called quadratic loss and one type penalty I get this well known algorithm.",
            "So typically for regression I have the algorithms of laser which consists in optimizing at least square criterion under the consequent.",
            "On the absolute value percent on your parameters.",
            "If you consider anyone type loss an L2 type penalty, you get the SVM.",
            "The counter methods algorithm so you can derive other kinds of algorithms according to this.",
            "Just remark one thing in my in the beginning of my presentation I focus on one example which is the example of regression.",
            "Record that for this example, we use an L2 tie plus an L2.",
            "A square penalty.",
            "So if you use L2 and L2 penalty, you are not in the condition of the theorem, so we cannot apply this efficient regularization path.",
            "We don't have this piecewise linear solution.",
            "What you have you will need to the to do some prediction and correction step in order to get your solution.",
            "So you spend some time you know that we compute this solution, But if you consider only this kind on this kind, you are lucky and you can do your optimization very efficiently."
        ],
        [
            "So.",
            "Second, summarize.",
            "We want to solve this problem.",
            "We want to find the organization path and we need for these convexity.",
            "You can have efficient computation.",
            "If you can use some piecewise linear variation, and finally to have this piecewise linear behavior, we need that our loss and penalty must be L1 type.",
            "So this algorithm was.",
            "This condition was proposed recently.",
            "And the people I send them to many kinds of algorithm.",
            "In SVM something like this.",
            "And for me it was.",
            "OK, sound technique and I tried to do some checking in the literature and what I."
        ],
        [
            "Find.",
            "I found that this is an old result.",
            "And the first one proposed this kind of.",
            "Computation of regularization path is this guy backwards?",
            "Who try to maximize the gain according to the investment in the case of asset management, this can be useful nowadays.",
            "And it's sure that.",
            "You have to solve quadratic programming problem and to get the set of optimal solution you have to compute the critical path which is this was linear and for for this work he gets.",
            "The Nobel Prize.",
            "So this is the reason that you have this picture on there.",
            "And two years further you have her who works also also on sensitivity, analysis of analysis of quadratic or linear programming problems.",
            "And he shows also that this can be done using some piecewise linear algorithm.",
            "And indeed, this is true, because when you consider SVM problem, you can show that it's equivalent to solve.",
            "Some quadratic programming algorithm and the same also for the laser, which is a well known algorithm.",
            "So we are in the condition of these two guys who have proposed this result.",
            "The last century.",
            "So what is what was presented than novelty in machine learning community in this recent years is just an old story told again."
        ],
        [
            "So.",
            "Let's focus on the way we apply this general principle to two kinds of algorithms.",
            "Well known algorithm.",
            "The first one is the laser problem and the second one will be in the SVM problem, so we'll see in details how to do it.",
            "I."
        ],
        [
            "I apologize, I will focus on some technical things so we see some algebra, but it's not quite complicated.",
            "Just follow and you can do it.",
            "You can understand, so for the lesser we consider this problem.",
            "The square loss.",
            "L1 penalty.",
            "And as we said previously, as this penalty function is convex and this one is convex, this is equivalent to this optimization, so we minimize our loss under the constraints on the penalty.",
            "Here I will define the variable XJ, like the Jeff: of the Matrix.",
            "X is the metrics of covariance.",
            "I assume also that my all valves and why which is the target output vector normalized?",
            "To illustrate this algorithm.",
            "In 2 dimensional example one, so we have here the last function.",
            "And the penalty which is this demo.",
            "So typically the least squares solution is here, which.",
            "Is obvious, obviously not sparse.",
            "What we want to do by using this aggressively, so we want sparsity.",
            "So in this situation if we set see this value, we see that only the parameter better too is different is different from zero and better one is equal to 0.",
            "In this situation we have some sparsity.",
            "And you see that if you increase the value of C, We will increase the size of our diamond here.",
            "So in this situation we will have.",
            "The solution, which will be the least square solution.",
            "In the order.",
            "In the opposite, if we decrease the value of C will decrease the size of the diamond, and in this situation we get very very sparse solution.",
            "That is, all the parameters are equal to 0.",
            "What is your subscription?",
            "Yeah, this one here.",
            "OK, so.",
            "I mean 2 dimensional.",
            "So I have two parameters better one better too.",
            "I'm sorry.",
            "OK, so in my model here I just have two variants.",
            "And I just put a penalty on my value so so the this figure corresponds to this formulation of my problem.",
            "So solving this one is equivalent to solving this one.",
            "We can find some some paper who gives gives you the direct direct correlation between C and Lambda here, so."
        ],
        [
            "So this is the formation of the problem so.",
            "I tried to wait the normality the normal equation according to this problem.",
            "So the optimality condition for the variable XJ.",
            "So for the parameters, better Jay is just defend like this.",
            "Just take the Liberty of this and this.",
            "Everything is OK if my parameter is different from zero because I have this penalty term.",
            "So if my partner is positive, we know that the derivative of the absolute value of this parameter is just equal to the sign of this parameter.",
            "If my parameter is positive, I get one.",
            "If my partner is negative against minus one, so we think it is OK, but.",
            "Here we can see that our penalty term is not smooth at 0, so we get in trouble.",
            "How to solve it?",
            "As we have our priority which is convex, we can use some materials of convex optimization in order to find a solution.",
            "So in convex case what we do?",
            "We can define an extension of the gradient of my function at zero.",
            "In this case I use a sub differential.",
            "Formally, the subdifferential of convex function is the set of all subgradients at this point.",
            "If my function is deliverable is differentiable, I just have one element in my set.",
            "Otherwise, I have a lot of value.",
            "So in this case, if I consider my absolute value at zero, I have my derivative which can evolve from minus one to one.",
            "So I have here some credit Alpha G which is between this interval.",
            "I don't know the exact value, but I know that I have.",
            "A gradient Alpha G which is in this interval.",
            "If my parameter is equal to 0.",
            "You can see the subgradients as the slope.",
            "Of the lower bounding function of our colleagues.",
            "Function so by this definition.",
            "Again, why done the optimality condition of my problems?"
        ],
        [
            "So.",
            "From this definition I can split.",
            "My parameters.",
            "In two sets, the first one is the set of active parameter.",
            "So I suppose that I better is just the set of better G different from zero.",
            "And according to this definition of my automatic condition and this definition of my sub differential for this parameter, I know that this quantity, the absolute value of the correlation.",
            "Here I have the correlation, because I suppose that all my variables and why normalized so under this condition?",
            "I know that this quantity is just the correlation of via XJ with residual.",
            "So this correlation is exactly equal to Lambda.",
            "According to this definition, an using this equation.",
            "What about the set of?",
            "Parameters.",
            "Which are equal to 0.",
            "This inductive set by zero.",
            "Is characterized by this relation, that is, using the fact that my subjects Alpha J is between this interval and using this equation.",
            "I see very easily that this quantity my correlation is less or equal to Lambda.",
            "So here Lambda is a positive value of course.",
            "So I have these two definitions of my my sets and from this definition.",
            "I can play again.",
            "If I want to estimate apparitor.",
            "Better J belonging to these sets.",
            "I just use this kind of condition to check that my parameter is different from zero and I can just calculate this matter if my parameter beta G is equal to zero.",
            "I don't care about it.",
            "This writer doesn't.",
            "Fulfill this condition so I don't care.",
            "I don't take any care of this parameter.",
            "I leave his value equal to 0."
        ],
        [
            "So let's use some matrix notation to defend the problem.",
            "I just define this vector.",
            "As the resolution of my overall vector to the set of active verbs.",
            "And the corresponding metrics X better here.",
            "So using my optimality condition, I can write down all my automatic condition in this matrix form."
        ],
        [
            "No.",
            "How can I get my solution path from this definition?",
            "I'm close to the solution.",
            "To see this.",
            "I have this optimality condition.",
            "So if I have a particular value, Lambda T at step T of my algorithm.",
            "This means that I know the parameters which are different from zero.",
            "I know their value.",
            "And I also know the set of parameters are different from zero, not normal.",
            "And I know that for Lambda T better T an ITI have this optimality condition.",
            "Missing a key here?",
            "So I have this optimality condition.",
            "No.",
            "Let's assume.",
            "Lambda equal to Lambda T plus gamma gamma, which is a small quantity I assume.",
            "And this quantity is such that.",
            "The set.",
            "I found here at the previous.",
            "Intuition and the parameters of the printer integration.",
            "They're saying doesn't change.",
            "What this mean?",
            "So this is very easy to see.",
            "This means that if my parameter is different from zero, it's still be different from zero.",
            "When I change slightly, the value of Lambda.",
            "This condition for this new is that if I have a positive positive parameter.",
            "I do not allow this parameter to become negative without.",
            "Taking the value 0.",
            "What I do?",
            "Why I do this?",
            "Just I I'm I'm positive I want to go to negative parts so when I'm running I will just pass by by zero and at zero I will remove my parameter from the set of active variable, that's all.",
            "So under this condition.",
            "We know that.",
            "This optimality condition also holds.",
            "And here because of the signpost points, I can write this.",
            "So.",
            "Grouping together these two equation I get.",
            "This relation, which is very simple and from this relation I obtained my piecewise linear variation.",
            "That is my goal.",
            "That is my estimation at step T. Is my estimation at the current step is obtained from the estimation of step T?",
            "Plus a certain quantity.",
            "The difference of the two values of Lambda.",
            "Multiplied by times W&W is nothing else that the gradient descent direction in this direction is easily obtained.",
            "So in this equation.",
            "This guys is known.",
            "This one is also known.",
            "As we know, the set of active lives, we can compute W because we know the matrix X right here and we know the sign of our parameters, so everything is OK.",
            "The only thing is unknown here.",
            "It's the value of Lambda, that is to find the step size gamma here.",
            "So how?",
            "If I have a current solution, better T corresponding to Lambda T. How can I get?"
        ],
        [
            "The next solution.",
            "To retrieve the next solution, OK.",
            "I know that this linear variation holds until a change in my sets occur, so we can have two kinds of change.",
            "The first one is that.",
            "On an active virus becomes inactive.",
            "That is a parameter which is different from zero becomes equal to 0.",
            "In this situation.",
            "To check this condition, it's very easy.",
            "Just take this equation.",
            "Set.",
            "The corresponding value to zero and you can calculate.",
            "The value of Lambda of gamma, the step size so you have.",
            "The value of gamma on which the parameter L become becomes equal to 0.",
            "So this is the easy case.",
            "The second type of event is.",
            "An inactive vial becomes active.",
            "How to check it?",
            "He told that from the optimality condition we have this condition, which is true for active area.",
            "And this one.",
            "For inactive files.",
            "So if an elective bias becomes active, it means that this quantity becomes equal exactly to Lambda.",
            "So.",
            "Just set this equation service in gamma and you have.",
            "The corresponding value of GABA, which gives you an idea on which condition better J becomes active.",
            "So if we do this.",
            "By checking these events for all the parameters in, I better and all the parameters in IO you would have a set of gamma.",
            "And from this set of gamma just pick up.",
            "So in this case, if I suppose that I dig in with higher value of Lambda and I decreasing Lambda, just take the smallest value of Lambda of gamma and that's it.",
            "So at each iteration you compute a set of the step size gamma and choose the small one.",
            "Just a few here.",
            "If you want to test this for all parameters in IO.",
            "Don't focus on all parameters, just take.",
            "The variable with the most correlation with the actual residual.",
            "This will give you.",
            "The corresponding value of gamma, such as these parameter interests your active volume."
        ],
        [
            "So we can derive the algorithm.",
            "For the lasso, the way to solve this problem using regularization path that is.",
            "At the beginning to do similar, you can say it's all your character equal to zero.",
            "That means that we set a high value of Lambda.",
            "And what you do at the first step is we check which variables is most correlated to your output and add these variables in your active set and from this you hit all the steps.",
            "You have one active apps, so you know in which direction you have to go.",
            "We can compute this decent direction.",
            "At step T, you compute the step size gamma, so notice that this step size gamma changes at each iteration is not a constant.",
            "You compute this step gamma by detecting which kind of events occurs is your active viral becomes inactive or an inactive virus becomes active, will check it and according to the events you return here, you can update your set.",
            "And you iterate until convergence until some termination criterion is met.",
            "By this way you get all solution.",
            "So here.",
            "Quiet is important to do this."
        ],
        [
            "If you know better T, but that correspond to Lambda T. And you want to know our solution between Lambda Tyann Lambda which correspond to the solution.",
            "Just use this linear relation.",
            "You don't need to.",
            "Compute.",
            "The solution solving the square or something like this.",
            "You know that you have a linear relation, so from the knowledge of this linear relation between the interval, Lambda Chi Lambda, we can compute all your solution in this interval using this linear relation.",
            "So if you have your generalization criteria, you just apply this year.",
            "Just plug this relation in your algorithm and you can compute the generation performance of your algorithm."
        ],
        [
            "To give.",
            "An intuitive interpretation of.",
            "The algorithm I show you.",
            "Just consider an example.",
            "In tweet demotion.",
            "So we have three values for X1X2 and X3.",
            "Here is our target output.",
            "At the beginning or.",
            "Arbiters are set equal to 0.",
            "What will do?",
            "As our orbiters are equal to zero, we know that the residual is just the output vector online, so we project this residual on this space of the virus."
        ],
        [
            "Doing this production.",
            "We find that.",
            "We obtained the most.",
            "The high value of correlation for the variable X one.",
            "So what we will do will take X1 and will put X1 in the set of active volumes.",
            "And we know that X one will give.",
            "The decent direction so will follow this direction.",
            "Point before."
        ],
        [
            "This direction.",
            "One question is.",
            "At which point we will stop.",
            "The algorithm says that.",
            "Follow this direction and stop here.",
            "Where your coalition of X2 and X1 with the current residual are equal.",
            "Just stop there.",
            "These two volumes can have the same explanation of your residual, so you don't have any reason to go to this point which correspond to the least square estimation.",
            "If you do just list where you will go through this solution before adding a new vibe in your active set, so the less would suggest you to stop here, so stop here.",
            "And at next stage, as this variable X2 is.",
            "A quick correlated with the residual XX one will add X2 in the active site."
        ],
        [
            "So when we are exploring in the active set.",
            "You have now this direction to follow.",
            "And we will follow this direction and will calculate our estimation."
        ],
        [
            "Until.",
            "We get this point.",
            "And at this point we have the correlation of the residual according to X1 and X2, which is equal to the correlation to extreme with the residual.",
            "So at this point we add extra in the active set and we can."
        ],
        [
            "New our estimation.",
            "OK here, let's have a look.",
            "On the way the parameters.",
            "Change so as we can see here, we begin with all parameters equal to 0 and at the first step the hydrometer.",
            "Becomes different from zero at the second step at this one, third said this one, and so on.",
            "And here we will see that at the moment this parameters is positive and becomes negative.",
            "So to do this we need that this parameter becomes equal to 0.",
            "So we move this parameter at this point from the active set and this parameter can enter again active sets after."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start.",
                    "label": 0
                },
                {
                    "sent": "Welcome everybody, and first of all I want to thank everybody to be here after this long week of working.",
                    "label": 0
                },
                {
                    "sent": "And to be here despite the blanket, the poster reception and the social events we will have during this week of conference.",
                    "label": 0
                },
                {
                    "sent": "So this afternoon I will try to.",
                    "label": 0
                },
                {
                    "sent": "Introduce you the user formulation path in machine learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "These techniques was recently proposed in order to deal with model selection issues in an efficient way in machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this work is joint work with professors, different canoe and.",
                    "label": 0
                },
                {
                    "sent": "Let's see the wood.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Map of the afternoon.",
                    "label": 0
                },
                {
                    "sent": "So first I will provide an introduction.",
                    "label": 0
                },
                {
                    "sent": "And which will settle the framework for my talk and which will introduce also the model selection problem and the way we want to solve it using regularization path.",
                    "label": 1
                },
                {
                    "sent": "After this we try to make some connection between regularization path and the parito optimality Frontier, which is well known in optimization community.",
                    "label": 1
                },
                {
                    "sent": "Also, this preliminary notion are introduced.",
                    "label": 0
                },
                {
                    "sent": "We can focus on the condition on that which we can have an efficient way to explore our regularization path.",
                    "label": 0
                },
                {
                    "sent": "That is an efficient way to obtain the set of our solution according to a trade off.",
                    "label": 1
                },
                {
                    "sent": "We want to make when we are when we want to serve a learning problem.",
                    "label": 0
                },
                {
                    "sent": "And this condition will lead.",
                    "label": 0
                },
                {
                    "sent": "To some famous algorithm we use usually integration.",
                    "label": 0
                },
                {
                    "sent": "An example is the lesser algorithm and in classification binary classification is the SVM algorithm and I will show you in details how to derive the relation path for this kind of algorithm.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I will show also that.",
                    "label": 0
                },
                {
                    "sent": "The efficiency of this algorithm is related to the sparsity of the criteria we use in this kind of algorithm.",
                    "label": 0
                },
                {
                    "sent": "And finally I will provide some empirical evaluation of the efficiency of the algorithm according to computational time, so it's strength and also some limits of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let's begin.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If the introduction so here is the framework of the classical statistical learning problem.",
                    "label": 0
                },
                {
                    "sent": "I assume here that I have a set of data XIYI where X is the input vector and why and why is the target output?",
                    "label": 0
                },
                {
                    "sent": "So we assume also that.",
                    "label": 0
                },
                {
                    "sent": "The pair XY is going from a distribution PXY, which is of course unknown.",
                    "label": 0
                },
                {
                    "sent": "When we consider again world data.",
                    "label": 0
                },
                {
                    "sent": "For too easy my talk and for simplicity sake I will just focus in this search on two kinds of problems and these two kinds are supervised learning binary classification using some SVM algorithm for example.",
                    "label": 0
                },
                {
                    "sent": "So binary classification.",
                    "label": 0
                },
                {
                    "sent": "In this case the outputs can take only two values minus one and plus one and regression problems where the outputs can have any value in their airspace.",
                    "label": 0
                },
                {
                    "sent": "So to address our learning problem, we can use two kinds of approach.",
                    "label": 0
                },
                {
                    "sent": "The first one is the generative approach, where which I will try to estimate the unknown distribution and to infer the output from the knowledge of this distribution.",
                    "label": 0
                },
                {
                    "sent": "The second approach is to directly identify a learning function F which is able to predict correctly the value of my target output.",
                    "label": 0
                },
                {
                    "sent": "So in the second I will suppose that my function F belongs to an input space H, so this space can be the space of linear function polynomial function defined over.",
                    "label": 1
                },
                {
                    "sent": "Additionally whatever you want.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I want is to identify to build a learning function F which must be accurate on the training data, but also are all unseen data.",
                    "label": 0
                },
                {
                    "sent": "So in this case we classically consider a loss function L and to achieve this objective to find a learning function which will perform very well.",
                    "label": 0
                },
                {
                    "sent": "We had to minimize the expected priest, but in this definition of the expected risk, we considered the expectation of X&Y as our distribution is unknown, we can't.",
                    "label": 0
                },
                {
                    "sent": "Do this minimization so people usually.",
                    "label": 0
                },
                {
                    "sent": "Use the empirical loss function L. And in this case, this empirical function is defined like this in this case.",
                    "label": 1
                },
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "In for some nice properties of our function F like like sparsity solution is something like this.",
                    "label": 1
                },
                {
                    "sent": "You know that to avoid some overtraining phenomenon what we do, we put a regularizer or penalty or colleagues penalty term in the second which is called PF.",
                    "label": 1
                },
                {
                    "sent": "So in this condition we have our learning problem which is given by this.",
                    "label": 0
                },
                {
                    "sent": "When the power to Lambda realises tradeoff between the loss function, the quality of the model.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "PF.",
                    "label": 0
                },
                {
                    "sent": "And this parameter is positive.",
                    "label": 0
                },
                {
                    "sent": "And how to choose this parameter so?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I think I can do this if you consider some some kind of argument like manifold learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Again, you can apply it because in the in this framework will consider as a loss function.",
                    "label": 0
                },
                {
                    "sent": "The loss on the label data and you add another cost point according to the manifold modeling of your data.",
                    "label": 0
                },
                {
                    "sent": "So you can use it.",
                    "label": 0
                },
                {
                    "sent": "Josh.",
                    "label": 0
                },
                {
                    "sent": "Supervise yeah I see that too easy presentation at firstly focus on this kind of algorithm, But you can extend the framework to this kind of semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So choosing the best model F star.",
                    "label": 0
                },
                {
                    "sent": "In this framework, is just equivalent to find the best value Lambda star according to this tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 cities just one simple example.",
                    "label": 0
                },
                {
                    "sent": "So here I have a nonlinear function to be estimate, so I don't know if you see where the tool function is.",
                    "label": 0
                },
                {
                    "sent": "The way the green curve and the data the Black Stars.",
                    "label": 0
                },
                {
                    "sent": "So I want to identify a nonlinear model using this data.",
                    "label": 0
                },
                {
                    "sent": "What I do I choose a Canyon Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "I use a Gaussian kernel an.",
                    "label": 0
                },
                {
                    "sent": "I see the value of Lambda in this case when I pick up a small value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "What happens you have overfitting which appear here.",
                    "label": 0
                },
                {
                    "sent": "Choosing a small value of Lambda means that we don't give importance to the penalty term, so we prefer a precise model according to the data.",
                    "label": 0
                },
                {
                    "sent": "So we have these over and over fitting.",
                    "label": 0
                },
                {
                    "sent": "Phenomenal.",
                    "label": 0
                },
                {
                    "sent": "When we pick up a high value of Lambda, we have two smooth solution.",
                    "label": 0
                },
                {
                    "sent": "So what we want to retrieve is this solution where we choose.",
                    "label": 0
                },
                {
                    "sent": "According to the value of Lambda and will obtain a fair wood solution according to our two function in winter.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to select the best value of Lambda, what we can do?",
                    "label": 0
                },
                {
                    "sent": "We can use two approaches.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one, the brute Force One, is to use in research.",
                    "label": 0
                },
                {
                    "sent": "In this case, we will ask the user to provide a set of value, Lambda one to Lambda K to our algorithm, and for each value of Lambda we identify a model F. I've had Lambda and will.",
                    "label": 0
                },
                {
                    "sent": "Test this model according to some generalization performance.",
                    "label": 1
                },
                {
                    "sent": "Whatever you want, you can choose your generously performance and from this.",
                    "label": 1
                },
                {
                    "sent": "We can pick up the best estimation of our model so.",
                    "label": 0
                },
                {
                    "sent": "This estimation will depend on.",
                    "label": 1
                },
                {
                    "sent": "The closeness of the grid we choose.",
                    "label": 0
                },
                {
                    "sent": "So if the bill is very close, will have a half approximation of our Jesus in performance.",
                    "label": 0
                },
                {
                    "sent": "And in this case we cannot have a good solution here.",
                    "label": 0
                },
                {
                    "sent": "If you choose a fan, read what you will have.",
                    "label": 0
                },
                {
                    "sent": "You know that we have a good approximation of generalization error, but in this case we will spend a lot of time in order to identify our model.",
                    "label": 0
                },
                {
                    "sent": "What we want to do?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is the second approach we can apply is.",
                    "label": 0
                },
                {
                    "sent": "To compute the organization path, what is what is this path?",
                    "label": 0
                },
                {
                    "sent": "We want.",
                    "label": 0
                },
                {
                    "sent": "The user not to specify any value.",
                    "label": 1
                },
                {
                    "sent": "We won't automatically find the set of our solution according to the different value of Lambda.",
                    "label": 1
                },
                {
                    "sent": "So here Lambda is between zero and Infinity, so for each value of Lambda we want the argument to find automatic and dissolution.",
                    "label": 0
                },
                {
                    "sent": "So we can validate each solution according to our journalism performance.",
                    "label": 0
                },
                {
                    "sent": "And we can retrieve the best one.",
                    "label": 0
                },
                {
                    "sent": "This is the formal definition of the realization.",
                    "label": 0
                },
                {
                    "sent": "Tough how to obtain it will see this later but before this.",
                    "label": 0
                },
                {
                    "sent": "I will try just to make a connection.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between this definition of regularization path and the operator frontier, which is well known, ocean in optimization community.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to do this, let's take a simple example.",
                    "label": 0
                },
                {
                    "sent": "Here we have a linear regression example, so I just consider linear model defined like this with better the parameter vector and better is the dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "The problem we want to serve is to minimize discourse.",
                    "label": 0
                },
                {
                    "sent": "At least workers under the constraints of.",
                    "label": 0
                },
                {
                    "sent": "Square last night on our character data.",
                    "label": 0
                },
                {
                    "sent": "So the solution is well known is given by this.",
                    "label": 0
                },
                {
                    "sent": "Where here is an identity matrix, X is the matrix of the covariance and the vector Y is the vector of the target outputs.",
                    "label": 0
                },
                {
                    "sent": "The organization path in this context is just defined by the value of beta according to the difference value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So here what we can remark, if we set Lambda equal to 0 does mean that we drop down the penalty, so we get the square solution, which is well known.",
                    "label": 0
                },
                {
                    "sent": "If we set Lambda equal to Infinity, we enforce our constraint.",
                    "label": 0
                },
                {
                    "sent": "So to avoid that this criterion blows up, we will set better equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this these are well known results.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to emphasize things, I will just take.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional example.",
                    "label": 0
                },
                {
                    "sent": "In this one dimensional example of what I do, so better is just scalar here.",
                    "label": 0
                },
                {
                    "sent": "And I can write my last L and my penalty better.",
                    "label": 1
                },
                {
                    "sent": "I can relate things like this using my distribution, so here AB&C are parameters which will depend on the XIYI and the chosen value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This relation.",
                    "label": 0
                },
                {
                    "sent": "Is shown by the red curve here.",
                    "label": 0
                },
                {
                    "sent": "And this curve.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis we have the penalty and Y axis.",
                    "label": 0
                },
                {
                    "sent": "You have the loss.",
                    "label": 0
                },
                {
                    "sent": "On this curve.",
                    "label": 0
                },
                {
                    "sent": "We have about which is very interesting in.",
                    "label": 0
                },
                {
                    "sent": "This part is just from here from here WHI let's see this point, the green one.",
                    "label": 0
                },
                {
                    "sent": "This green points correspond to the solution beta equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So indeed here we have the penalty which is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And here we have the loss which just equal to the norm of the output vector Y.",
                    "label": 1
                },
                {
                    "sent": "And so these correspond according to our previous analysis.",
                    "label": 0
                },
                {
                    "sent": "This correspond just to the solution when we set Lambda equal to Infinity.",
                    "label": 0
                },
                {
                    "sent": "When we set Lambda equal to 0, so here we get the list for solution and this solution corresponds to the minimum of our loss here.",
                    "label": 0
                },
                {
                    "sent": "So between those two solution when we vary the value of Lambda, we can see that.",
                    "label": 0
                },
                {
                    "sent": "We will explore this part of the curve.",
                    "label": 0
                },
                {
                    "sent": "So intuitively.",
                    "label": 0
                },
                {
                    "sent": "The blue part of my curve.",
                    "label": 0
                },
                {
                    "sent": "Is somehow the definition of migration path, so finding my my path is just finding this part of my girl.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's relate this to the concept of Battle Frontier.",
                    "label": 0
                },
                {
                    "sent": "So in my optimization problem I have two conflicting goals, the loss and the penalty.",
                    "label": 0
                },
                {
                    "sent": "So according to my curve here.",
                    "label": 0
                },
                {
                    "sent": "All this area corresponds to the set of admissible solution.",
                    "label": 0
                },
                {
                    "sent": "And in this set, let's.",
                    "label": 0
                },
                {
                    "sent": "Analyze this quote, the Red one and we compare this point to this one.",
                    "label": 0
                },
                {
                    "sent": "This word is most interesting for us, why?",
                    "label": 0
                },
                {
                    "sent": "If you look at this point, we have the penalty and the loss which are greater than the penalty and the loss of this point.",
                    "label": 0
                },
                {
                    "sent": "So we've saved at this point the purple One dominates the right .1.",
                    "label": 0
                },
                {
                    "sent": "And formally, the notion of dominance is defined by this definition.",
                    "label": 1
                },
                {
                    "sent": "So if we pick up two solutions, better one and better too.",
                    "label": 0
                },
                {
                    "sent": "Would say that better one dominates better too if for both.",
                    "label": 0
                },
                {
                    "sent": "Better one performs well done better too.",
                    "label": 1
                },
                {
                    "sent": "So according to this definition, if you look at the set of all the minutes solution, we get the Parrot frontier, which is the optimality frontier between our two criteria so.",
                    "label": 1
                },
                {
                    "sent": "Here we saw that you can see that the power to frontier is nothing else than the regularization path, so doing this is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Looking at the paragraph on here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This way of exploiting the party frontier.",
                    "label": 0
                },
                {
                    "sent": "So in our classical formulation of learning problem, we use this formulation.",
                    "label": 0
                },
                {
                    "sent": "This language and formulation, that is, we take a linear combination of the loss and the penalty.",
                    "label": 1
                },
                {
                    "sent": "And this linear combination is given by this.",
                    "label": 0
                },
                {
                    "sent": "So in the plane LP, what we do we just?",
                    "label": 0
                },
                {
                    "sent": "Consider that the line which is 1000 tangents to our part of culture in at a certain point.",
                    "label": 0
                },
                {
                    "sent": "So if we change the value of Lambda will change the slope of the line and we will change our solution.",
                    "label": 0
                },
                {
                    "sent": "So this is the classical way to consider options.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second formulation is to minimize the loss under inequality constraints on the norm of our vector.",
                    "label": 0
                },
                {
                    "sent": "So by constraining the penalty term in this case, this correspond to this situation and you can see that according to this formulation, if you just.",
                    "label": 0
                },
                {
                    "sent": "You can be able to find a value of Lambda corresponding to these constants C and we can.",
                    "label": 0
                },
                {
                    "sent": "Lucia, the land which correspond to this situation.",
                    "label": 0
                },
                {
                    "sent": "So formulation one information, two equivalence.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last formulation, the third one, is just this.",
                    "label": 0
                },
                {
                    "sent": "When we minimize the penalty term under the constraints on the least square.",
                    "label": 0
                },
                {
                    "sent": "So here this formulation can be interesting in.",
                    "label": 0
                },
                {
                    "sent": "For instance, in some application like compressive sensing, when we have some idea about the noise inside our data, so we can set this C prime constant here, like the noise we guess in our data.",
                    "label": 0
                },
                {
                    "sent": "So this situation correspond to these kinds of constraints and we see that we are able also to find.",
                    "label": 0
                },
                {
                    "sent": "A line which is 10,000 two at this point.",
                    "label": 0
                },
                {
                    "sent": "So this reformulation are equivalent.",
                    "label": 0
                },
                {
                    "sent": "But be aware that this works.",
                    "label": 0
                },
                {
                    "sent": "Order the cost and that's all.",
                    "label": 0
                },
                {
                    "sent": "The criteria be colleagues.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you are not in the non convex case when you are in the North of skis.",
                    "label": 1
                },
                {
                    "sent": "Here we have an example when we consider.",
                    "label": 0
                },
                {
                    "sent": "A convex penalty function and here and on convex penalty function.",
                    "label": 0
                },
                {
                    "sent": "So here we can see that.",
                    "label": 0
                },
                {
                    "sent": "We have a concave part in our Title Frontier and when we use the linear combination of our loss L&P.",
                    "label": 0
                },
                {
                    "sent": "In this situation, we can retrieve our solution.",
                    "label": 0
                },
                {
                    "sent": "Along the path.",
                    "label": 0
                },
                {
                    "sent": "But here we can retrieve this solution.",
                    "label": 0
                },
                {
                    "sent": "This one, but in the concave part we are not able to retrieve all the solution.",
                    "label": 0
                },
                {
                    "sent": "So be aware that if you are using the language and formulation of your learning problem.",
                    "label": 0
                },
                {
                    "sent": "Everything is OK when you are in this situation when you are in this situation, be aware that you lose the solution which are there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So make like let me make a short summarize.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Everybody knows is a multi objective optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Computing the organization path is equivalent to explore the parity frontier.",
                    "label": 0
                },
                {
                    "sent": "Our reformulation are equivalent.",
                    "label": 0
                },
                {
                    "sent": "If you consider convex functions, otherwise will not retrieve all solution.",
                    "label": 0
                },
                {
                    "sent": "And we can extend these two more than two criteria as I said.",
                    "label": 0
                },
                {
                    "sent": "At the beginning of the talk, so we define all this notion.",
                    "label": 0
                },
                {
                    "sent": "To do what?",
                    "label": 1
                },
                {
                    "sent": "What we want to do?",
                    "label": 0
                },
                {
                    "sent": "Is the way we can efficiently too in the value of Lambda?",
                    "label": 0
                },
                {
                    "sent": "And to do this.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third step of my talk and this step.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To introduce this step, let's see this example.",
                    "label": 0
                },
                {
                    "sent": "So I come back to my regression example.",
                    "label": 1
                },
                {
                    "sent": "So he called that.",
                    "label": 0
                },
                {
                    "sent": "The last function, the Fidelity and the organization parameter.",
                    "label": 0
                },
                {
                    "sent": "I want to optimize.",
                    "label": 0
                },
                {
                    "sent": "If I use the brute force.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Methods that is, I consider only research algorithm.",
                    "label": 0
                },
                {
                    "sent": "I specify K value of Lambda and for each value of Lambda Lambda one to Linda Kay.",
                    "label": 0
                },
                {
                    "sent": "I compute this solution.",
                    "label": 0
                },
                {
                    "sent": "The computation of this solution involves a complexity of the.",
                    "label": 0
                },
                {
                    "sent": "Cubic.",
                    "label": 0
                },
                {
                    "sent": "Discount my complexity is cubic according to the number of parameters I want to identify as I do this K times I have.",
                    "label": 0
                },
                {
                    "sent": "K * D complexity.",
                    "label": 0
                },
                {
                    "sent": "So this is the first one, but I can try to improve things.",
                    "label": 0
                },
                {
                    "sent": "How to do it?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's tricky way to do this is to use.",
                    "label": 0
                },
                {
                    "sent": "The previous estimation better team I know, minus one to compute the current estimation better.",
                    "label": 0
                },
                {
                    "sent": "T in this case I can use some warm start.",
                    "label": 1
                },
                {
                    "sent": "Technique in order.",
                    "label": 0
                },
                {
                    "sent": "To have very quickly my solution so I can apply some gradient conjugate integration and I can get my solution.",
                    "label": 0
                },
                {
                    "sent": "So bad doing this.",
                    "label": 0
                },
                {
                    "sent": "I drop the complexity of my algorithm.",
                    "label": 0
                },
                {
                    "sent": "So instead of hiring different here, I just have a square complexity.",
                    "label": 0
                },
                {
                    "sent": "Time is all the norm.",
                    "label": 1
                },
                {
                    "sent": "The number of conjugate gradient and times K. So I can improve my complexity.",
                    "label": 0
                },
                {
                    "sent": "Can I?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do better lady.",
                    "label": 0
                },
                {
                    "sent": "By the knowledge of my problem, when I have a solution with 80 -- 1.",
                    "label": 0
                },
                {
                    "sent": "I can apply a gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Algorithm so I just take.",
                    "label": 0
                },
                {
                    "sent": "The gradient descent direction and I calculate my solution according to this direction and using Lambda T which correspond to the solution I want to retrieve for better.",
                    "label": 0
                },
                {
                    "sent": "So I guess here first solution.",
                    "label": 0
                },
                {
                    "sent": "And from this solution I apply also my warm start.",
                    "label": 0
                },
                {
                    "sent": "Iteration and I can get my solution metate.",
                    "label": 0
                },
                {
                    "sent": "So here I suppose that.",
                    "label": 0
                },
                {
                    "sent": "And crime here is less than L becausw I'm, I suppose that I'm close to the solution when I apply my conjugate gradient iteration.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As you can see I improved the complexity for my algorithm, but.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I want is to do.",
                    "label": 0
                },
                {
                    "sent": "More simple.",
                    "label": 0
                },
                {
                    "sent": "That is here.",
                    "label": 0
                },
                {
                    "sent": "Instead of using warm start and position step.",
                    "label": 1
                },
                {
                    "sent": "If I'm able to use only the position step so I can calculate better T according to better T -- 1 using a linear relation.",
                    "label": 0
                },
                {
                    "sent": "I'm the King of oil.",
                    "label": 0
                },
                {
                    "sent": "Because I will have.",
                    "label": 0
                },
                {
                    "sent": "Decrease the complexity of my algorithm in 2K the square.",
                    "label": 0
                },
                {
                    "sent": "So in this case I have again of complexity in my algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do?",
                    "label": 1
                },
                {
                    "sent": "If you want to efficiently explore our regularization path.",
                    "label": 0
                },
                {
                    "sent": "If you can have this situation, it is very good for us because this situation correspond to the most.",
                    "label": 0
                },
                {
                    "sent": "Decreasing complexity of our problem.",
                    "label": 0
                },
                {
                    "sent": "So under which condition?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And can we do this?",
                    "label": 0
                },
                {
                    "sent": "The answer was provided by Rossi in his paper one year ago.",
                    "label": 0
                },
                {
                    "sent": "And this answer says that.",
                    "label": 0
                },
                {
                    "sent": "If you have L&P and you want to have.",
                    "label": 0
                },
                {
                    "sent": "A solution path which is piecewise linear.",
                    "label": 1
                },
                {
                    "sent": "That is, we just apply the prediction step.",
                    "label": 0
                },
                {
                    "sent": "The only condition is that one of your costs.",
                    "label": 1
                },
                {
                    "sent": "Must be at most quadratic or piecewise quadratic, and the order cost must be at least this was linear.",
                    "label": 0
                },
                {
                    "sent": "So L must be quadratic and P. Linear or key must be quadratic and linear.",
                    "label": 0
                },
                {
                    "sent": "Both Atlantic and linear, and it works.",
                    "label": 0
                },
                {
                    "sent": "This was linearity to sketch the proof of this theorem.",
                    "label": 0
                },
                {
                    "sent": "Let's see, that is why linearity means that if we take.",
                    "label": 0
                },
                {
                    "sent": "The variation of our solution better according to the variation of Lambda.",
                    "label": 0
                },
                {
                    "sent": "If you have piecewise reality, we have this derivative which is quite constant.",
                    "label": 0
                },
                {
                    "sent": "So on the average condition we have this equality.",
                    "label": 0
                },
                {
                    "sent": "This equation.",
                    "label": 0
                },
                {
                    "sent": "So little bit algebra.",
                    "label": 0
                },
                {
                    "sent": "To show it.",
                    "label": 0
                },
                {
                    "sent": "Consider a value Lambda known.",
                    "label": 0
                },
                {
                    "sent": "So according to our optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "We can take the normal equation of this minimization problem, so here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You write down the optimality condition for Lambda, so we have these equation and we do the same for Lambda plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "The small variation of our key parameter.",
                    "label": 0
                },
                {
                    "sent": "So we have this equation.",
                    "label": 0
                },
                {
                    "sent": "Food.",
                    "label": 0
                },
                {
                    "sent": "What we do next.",
                    "label": 0
                },
                {
                    "sent": "We take a first order approximation of this equation around the solution beta Lambda.",
                    "label": 0
                },
                {
                    "sent": "So this solution corresponds to the value of Lambda, and we do this.",
                    "label": 0
                },
                {
                    "sent": "So you add on this first order approximation, so we get this solution and you can remark that here I drop them all terms which are of square_of epsilon becausw.",
                    "label": 0
                },
                {
                    "sent": "I considered at this time are very small.",
                    "label": 0
                },
                {
                    "sent": "And in this equation you have the Haitian of Defense system according to better Lambda and the Hessian of the last term according to Lambda, Beta, Lambda and so on.",
                    "label": 0
                },
                {
                    "sent": "So combining this equation with this one.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "See that this quantity is just equal to this.",
                    "label": 0
                },
                {
                    "sent": "And if you want to have a piecewise linear behavior.",
                    "label": 0
                },
                {
                    "sent": "Which condition we need?",
                    "label": 0
                },
                {
                    "sent": "We need firstly that all this equation, the independent form Lambda.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to have Lambda here.",
                    "label": 0
                },
                {
                    "sent": "So to have this condition I said that I would say that this region will be equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So here we don't have any dependence on Lambda.",
                    "label": 0
                },
                {
                    "sent": "And to have this constant just need that this quantity, but the constant according to Lambda.",
                    "label": 0
                },
                {
                    "sent": "If you do so, we can see that.",
                    "label": 0
                },
                {
                    "sent": "This derivative is just equal to a constant, so here this content can deal in this case, you know that your loss function is just linear, and here this case means that your penalty function is also linear or piecewise linear.",
                    "label": 0
                },
                {
                    "sent": "By this we showed the condition of our theorem.",
                    "label": 0
                },
                {
                    "sent": "So to summarize it.",
                    "label": 0
                },
                {
                    "sent": "If you want to compute very efficiently to your solution path, you want two condition.",
                    "label": 0
                },
                {
                    "sent": "At most your loss or your penalty term must be quadratic, and at least they must be.",
                    "label": 0
                },
                {
                    "sent": "Linear.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples of function which.",
                    "label": 0
                },
                {
                    "sent": "Fulfill this condition.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "Lost.",
                    "label": 0
                },
                {
                    "sent": "Plus for classification problem and here we have lots for regression problem.",
                    "label": 0
                },
                {
                    "sent": "So for classification we have here 01 cost.",
                    "label": 0
                },
                {
                    "sent": "Discourse is piecewise linear, but.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fulfill the condition of the theory of Rossi Becausw.",
                    "label": 0
                },
                {
                    "sent": "Discussed is not convex.",
                    "label": 0
                },
                {
                    "sent": "And our theorem said that we have we need convex function.",
                    "label": 0
                },
                {
                    "sent": "The logistic loss here.",
                    "label": 0
                },
                {
                    "sent": "Look like quadratic, but not the case.",
                    "label": 0
                },
                {
                    "sent": "If you take the 2nd order derivative derivative is not equal to a constant, so this last doesn't.",
                    "label": 0
                },
                {
                    "sent": "Meet the requirements of the term.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Which is very used in SVM algorithm is allows which fulfill this condition and its square version also met this requirement disagreement.",
                    "label": 0
                },
                {
                    "sent": "From regression we have the epsilon insensitive loss which is defined here which is used in support vector regression algorithm and these kinds of loss fulfill this condition.",
                    "label": 0
                },
                {
                    "sent": "And So what can I say?",
                    "label": 0
                },
                {
                    "sent": "Also we have.",
                    "label": 0
                },
                {
                    "sent": "The least square loss or the square penalty term, which is also piecewise linear OK. You have the Huber function, which is piecewise quadratic and used for unifying robust model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if I play just a game of combination, I will see that I can retrieve these table where if I consider an L2 type loss what I call in two tab is called quadratic loss and one type penalty I get this well known algorithm.",
                    "label": 0
                },
                {
                    "sent": "So typically for regression I have the algorithms of laser which consists in optimizing at least square criterion under the consequent.",
                    "label": 0
                },
                {
                    "sent": "On the absolute value percent on your parameters.",
                    "label": 0
                },
                {
                    "sent": "If you consider anyone type loss an L2 type penalty, you get the SVM.",
                    "label": 0
                },
                {
                    "sent": "The counter methods algorithm so you can derive other kinds of algorithms according to this.",
                    "label": 0
                },
                {
                    "sent": "Just remark one thing in my in the beginning of my presentation I focus on one example which is the example of regression.",
                    "label": 0
                },
                {
                    "sent": "Record that for this example, we use an L2 tie plus an L2.",
                    "label": 0
                },
                {
                    "sent": "A square penalty.",
                    "label": 0
                },
                {
                    "sent": "So if you use L2 and L2 penalty, you are not in the condition of the theorem, so we cannot apply this efficient regularization path.",
                    "label": 0
                },
                {
                    "sent": "We don't have this piecewise linear solution.",
                    "label": 0
                },
                {
                    "sent": "What you have you will need to the to do some prediction and correction step in order to get your solution.",
                    "label": 0
                },
                {
                    "sent": "So you spend some time you know that we compute this solution, But if you consider only this kind on this kind, you are lucky and you can do your optimization very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Second, summarize.",
                    "label": 0
                },
                {
                    "sent": "We want to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "We want to find the organization path and we need for these convexity.",
                    "label": 0
                },
                {
                    "sent": "You can have efficient computation.",
                    "label": 0
                },
                {
                    "sent": "If you can use some piecewise linear variation, and finally to have this piecewise linear behavior, we need that our loss and penalty must be L1 type.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm was.",
                    "label": 0
                },
                {
                    "sent": "This condition was proposed recently.",
                    "label": 0
                },
                {
                    "sent": "And the people I send them to many kinds of algorithm.",
                    "label": 0
                },
                {
                    "sent": "In SVM something like this.",
                    "label": 0
                },
                {
                    "sent": "And for me it was.",
                    "label": 0
                },
                {
                    "sent": "OK, sound technique and I tried to do some checking in the literature and what I.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find.",
                    "label": 0
                },
                {
                    "sent": "I found that this is an old result.",
                    "label": 1
                },
                {
                    "sent": "And the first one proposed this kind of.",
                    "label": 1
                },
                {
                    "sent": "Computation of regularization path is this guy backwards?",
                    "label": 0
                },
                {
                    "sent": "Who try to maximize the gain according to the investment in the case of asset management, this can be useful nowadays.",
                    "label": 0
                },
                {
                    "sent": "And it's sure that.",
                    "label": 0
                },
                {
                    "sent": "You have to solve quadratic programming problem and to get the set of optimal solution you have to compute the critical path which is this was linear and for for this work he gets.",
                    "label": 0
                },
                {
                    "sent": "The Nobel Prize.",
                    "label": 1
                },
                {
                    "sent": "So this is the reason that you have this picture on there.",
                    "label": 1
                },
                {
                    "sent": "And two years further you have her who works also also on sensitivity, analysis of analysis of quadratic or linear programming problems.",
                    "label": 0
                },
                {
                    "sent": "And he shows also that this can be done using some piecewise linear algorithm.",
                    "label": 0
                },
                {
                    "sent": "And indeed, this is true, because when you consider SVM problem, you can show that it's equivalent to solve.",
                    "label": 0
                },
                {
                    "sent": "Some quadratic programming algorithm and the same also for the laser, which is a well known algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we are in the condition of these two guys who have proposed this result.",
                    "label": 0
                },
                {
                    "sent": "The last century.",
                    "label": 0
                },
                {
                    "sent": "So what is what was presented than novelty in machine learning community in this recent years is just an old story told again.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's focus on the way we apply this general principle to two kinds of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Well known algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first one is the laser problem and the second one will be in the SVM problem, so we'll see in details how to do it.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I apologize, I will focus on some technical things so we see some algebra, but it's not quite complicated.",
                    "label": 0
                },
                {
                    "sent": "Just follow and you can do it.",
                    "label": 0
                },
                {
                    "sent": "You can understand, so for the lesser we consider this problem.",
                    "label": 0
                },
                {
                    "sent": "The square loss.",
                    "label": 0
                },
                {
                    "sent": "L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "And as we said previously, as this penalty function is convex and this one is convex, this is equivalent to this optimization, so we minimize our loss under the constraints on the penalty.",
                    "label": 0
                },
                {
                    "sent": "Here I will define the variable XJ, like the Jeff: of the Matrix.",
                    "label": 0
                },
                {
                    "sent": "X is the metrics of covariance.",
                    "label": 0
                },
                {
                    "sent": "I assume also that my all valves and why which is the target output vector normalized?",
                    "label": 0
                },
                {
                    "sent": "To illustrate this algorithm.",
                    "label": 0
                },
                {
                    "sent": "In 2 dimensional example one, so we have here the last function.",
                    "label": 1
                },
                {
                    "sent": "And the penalty which is this demo.",
                    "label": 0
                },
                {
                    "sent": "So typically the least squares solution is here, which.",
                    "label": 1
                },
                {
                    "sent": "Is obvious, obviously not sparse.",
                    "label": 0
                },
                {
                    "sent": "What we want to do by using this aggressively, so we want sparsity.",
                    "label": 0
                },
                {
                    "sent": "So in this situation if we set see this value, we see that only the parameter better too is different is different from zero and better one is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "In this situation we have some sparsity.",
                    "label": 0
                },
                {
                    "sent": "And you see that if you increase the value of C, We will increase the size of our diamond here.",
                    "label": 0
                },
                {
                    "sent": "So in this situation we will have.",
                    "label": 0
                },
                {
                    "sent": "The solution, which will be the least square solution.",
                    "label": 0
                },
                {
                    "sent": "In the order.",
                    "label": 0
                },
                {
                    "sent": "In the opposite, if we decrease the value of C will decrease the size of the diamond, and in this situation we get very very sparse solution.",
                    "label": 0
                },
                {
                    "sent": "That is, all the parameters are equal to 0.",
                    "label": 0
                },
                {
                    "sent": "What is your subscription?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this one here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I mean 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So I have two parameters better one better too.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, so in my model here I just have two variants.",
                    "label": 0
                },
                {
                    "sent": "And I just put a penalty on my value so so the this figure corresponds to this formulation of my problem.",
                    "label": 0
                },
                {
                    "sent": "So solving this one is equivalent to solving this one.",
                    "label": 0
                },
                {
                    "sent": "We can find some some paper who gives gives you the direct direct correlation between C and Lambda here, so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the formation of the problem so.",
                    "label": 0
                },
                {
                    "sent": "I tried to wait the normality the normal equation according to this problem.",
                    "label": 0
                },
                {
                    "sent": "So the optimality condition for the variable XJ.",
                    "label": 1
                },
                {
                    "sent": "So for the parameters, better Jay is just defend like this.",
                    "label": 0
                },
                {
                    "sent": "Just take the Liberty of this and this.",
                    "label": 0
                },
                {
                    "sent": "Everything is OK if my parameter is different from zero because I have this penalty term.",
                    "label": 0
                },
                {
                    "sent": "So if my partner is positive, we know that the derivative of the absolute value of this parameter is just equal to the sign of this parameter.",
                    "label": 0
                },
                {
                    "sent": "If my parameter is positive, I get one.",
                    "label": 0
                },
                {
                    "sent": "If my partner is negative against minus one, so we think it is OK, but.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that our penalty term is not smooth at 0, so we get in trouble.",
                    "label": 0
                },
                {
                    "sent": "How to solve it?",
                    "label": 0
                },
                {
                    "sent": "As we have our priority which is convex, we can use some materials of convex optimization in order to find a solution.",
                    "label": 0
                },
                {
                    "sent": "So in convex case what we do?",
                    "label": 0
                },
                {
                    "sent": "We can define an extension of the gradient of my function at zero.",
                    "label": 0
                },
                {
                    "sent": "In this case I use a sub differential.",
                    "label": 0
                },
                {
                    "sent": "Formally, the subdifferential of convex function is the set of all subgradients at this point.",
                    "label": 0
                },
                {
                    "sent": "If my function is deliverable is differentiable, I just have one element in my set.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I have a lot of value.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if I consider my absolute value at zero, I have my derivative which can evolve from minus one to one.",
                    "label": 0
                },
                {
                    "sent": "So I have here some credit Alpha G which is between this interval.",
                    "label": 0
                },
                {
                    "sent": "I don't know the exact value, but I know that I have.",
                    "label": 0
                },
                {
                    "sent": "A gradient Alpha G which is in this interval.",
                    "label": 0
                },
                {
                    "sent": "If my parameter is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "You can see the subgradients as the slope.",
                    "label": 0
                },
                {
                    "sent": "Of the lower bounding function of our colleagues.",
                    "label": 0
                },
                {
                    "sent": "Function so by this definition.",
                    "label": 0
                },
                {
                    "sent": "Again, why done the optimality condition of my problems?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "From this definition I can split.",
                    "label": 0
                },
                {
                    "sent": "My parameters.",
                    "label": 0
                },
                {
                    "sent": "In two sets, the first one is the set of active parameter.",
                    "label": 0
                },
                {
                    "sent": "So I suppose that I better is just the set of better G different from zero.",
                    "label": 0
                },
                {
                    "sent": "And according to this definition of my automatic condition and this definition of my sub differential for this parameter, I know that this quantity, the absolute value of the correlation.",
                    "label": 0
                },
                {
                    "sent": "Here I have the correlation, because I suppose that all my variables and why normalized so under this condition?",
                    "label": 0
                },
                {
                    "sent": "I know that this quantity is just the correlation of via XJ with residual.",
                    "label": 0
                },
                {
                    "sent": "So this correlation is exactly equal to Lambda.",
                    "label": 0
                },
                {
                    "sent": "According to this definition, an using this equation.",
                    "label": 0
                },
                {
                    "sent": "What about the set of?",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 0
                },
                {
                    "sent": "Which are equal to 0.",
                    "label": 0
                },
                {
                    "sent": "This inductive set by zero.",
                    "label": 0
                },
                {
                    "sent": "Is characterized by this relation, that is, using the fact that my subjects Alpha J is between this interval and using this equation.",
                    "label": 0
                },
                {
                    "sent": "I see very easily that this quantity my correlation is less or equal to Lambda.",
                    "label": 0
                },
                {
                    "sent": "So here Lambda is a positive value of course.",
                    "label": 0
                },
                {
                    "sent": "So I have these two definitions of my my sets and from this definition.",
                    "label": 0
                },
                {
                    "sent": "I can play again.",
                    "label": 0
                },
                {
                    "sent": "If I want to estimate apparitor.",
                    "label": 0
                },
                {
                    "sent": "Better J belonging to these sets.",
                    "label": 0
                },
                {
                    "sent": "I just use this kind of condition to check that my parameter is different from zero and I can just calculate this matter if my parameter beta G is equal to zero.",
                    "label": 0
                },
                {
                    "sent": "I don't care about it.",
                    "label": 0
                },
                {
                    "sent": "This writer doesn't.",
                    "label": 0
                },
                {
                    "sent": "Fulfill this condition so I don't care.",
                    "label": 0
                },
                {
                    "sent": "I don't take any care of this parameter.",
                    "label": 0
                },
                {
                    "sent": "I leave his value equal to 0.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's use some matrix notation to defend the problem.",
                    "label": 0
                },
                {
                    "sent": "I just define this vector.",
                    "label": 0
                },
                {
                    "sent": "As the resolution of my overall vector to the set of active verbs.",
                    "label": 0
                },
                {
                    "sent": "And the corresponding metrics X better here.",
                    "label": 0
                },
                {
                    "sent": "So using my optimality condition, I can write down all my automatic condition in this matrix form.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "How can I get my solution path from this definition?",
                    "label": 0
                },
                {
                    "sent": "I'm close to the solution.",
                    "label": 1
                },
                {
                    "sent": "To see this.",
                    "label": 1
                },
                {
                    "sent": "I have this optimality condition.",
                    "label": 0
                },
                {
                    "sent": "So if I have a particular value, Lambda T at step T of my algorithm.",
                    "label": 0
                },
                {
                    "sent": "This means that I know the parameters which are different from zero.",
                    "label": 0
                },
                {
                    "sent": "I know their value.",
                    "label": 0
                },
                {
                    "sent": "And I also know the set of parameters are different from zero, not normal.",
                    "label": 0
                },
                {
                    "sent": "And I know that for Lambda T better T an ITI have this optimality condition.",
                    "label": 0
                },
                {
                    "sent": "Missing a key here?",
                    "label": 0
                },
                {
                    "sent": "So I have this optimality condition.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Let's assume.",
                    "label": 0
                },
                {
                    "sent": "Lambda equal to Lambda T plus gamma gamma, which is a small quantity I assume.",
                    "label": 1
                },
                {
                    "sent": "And this quantity is such that.",
                    "label": 0
                },
                {
                    "sent": "The set.",
                    "label": 0
                },
                {
                    "sent": "I found here at the previous.",
                    "label": 0
                },
                {
                    "sent": "Intuition and the parameters of the printer integration.",
                    "label": 0
                },
                {
                    "sent": "They're saying doesn't change.",
                    "label": 0
                },
                {
                    "sent": "What this mean?",
                    "label": 0
                },
                {
                    "sent": "So this is very easy to see.",
                    "label": 0
                },
                {
                    "sent": "This means that if my parameter is different from zero, it's still be different from zero.",
                    "label": 0
                },
                {
                    "sent": "When I change slightly, the value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "This condition for this new is that if I have a positive positive parameter.",
                    "label": 0
                },
                {
                    "sent": "I do not allow this parameter to become negative without.",
                    "label": 0
                },
                {
                    "sent": "Taking the value 0.",
                    "label": 0
                },
                {
                    "sent": "What I do?",
                    "label": 0
                },
                {
                    "sent": "Why I do this?",
                    "label": 0
                },
                {
                    "sent": "Just I I'm I'm positive I want to go to negative parts so when I'm running I will just pass by by zero and at zero I will remove my parameter from the set of active variable, that's all.",
                    "label": 0
                },
                {
                    "sent": "So under this condition.",
                    "label": 0
                },
                {
                    "sent": "We know that.",
                    "label": 0
                },
                {
                    "sent": "This optimality condition also holds.",
                    "label": 0
                },
                {
                    "sent": "And here because of the signpost points, I can write this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Grouping together these two equation I get.",
                    "label": 0
                },
                {
                    "sent": "This relation, which is very simple and from this relation I obtained my piecewise linear variation.",
                    "label": 0
                },
                {
                    "sent": "That is my goal.",
                    "label": 0
                },
                {
                    "sent": "That is my estimation at step T. Is my estimation at the current step is obtained from the estimation of step T?",
                    "label": 0
                },
                {
                    "sent": "Plus a certain quantity.",
                    "label": 0
                },
                {
                    "sent": "The difference of the two values of Lambda.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by times W&W is nothing else that the gradient descent direction in this direction is easily obtained.",
                    "label": 0
                },
                {
                    "sent": "So in this equation.",
                    "label": 0
                },
                {
                    "sent": "This guys is known.",
                    "label": 0
                },
                {
                    "sent": "This one is also known.",
                    "label": 0
                },
                {
                    "sent": "As we know, the set of active lives, we can compute W because we know the matrix X right here and we know the sign of our parameters, so everything is OK.",
                    "label": 0
                },
                {
                    "sent": "The only thing is unknown here.",
                    "label": 0
                },
                {
                    "sent": "It's the value of Lambda, that is to find the step size gamma here.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "If I have a current solution, better T corresponding to Lambda T. How can I get?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next solution.",
                    "label": 0
                },
                {
                    "sent": "To retrieve the next solution, OK.",
                    "label": 0
                },
                {
                    "sent": "I know that this linear variation holds until a change in my sets occur, so we can have two kinds of change.",
                    "label": 1
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "On an active virus becomes inactive.",
                    "label": 0
                },
                {
                    "sent": "That is a parameter which is different from zero becomes equal to 0.",
                    "label": 0
                },
                {
                    "sent": "In this situation.",
                    "label": 0
                },
                {
                    "sent": "To check this condition, it's very easy.",
                    "label": 0
                },
                {
                    "sent": "Just take this equation.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "The corresponding value to zero and you can calculate.",
                    "label": 1
                },
                {
                    "sent": "The value of Lambda of gamma, the step size so you have.",
                    "label": 0
                },
                {
                    "sent": "The value of gamma on which the parameter L become becomes equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So this is the easy case.",
                    "label": 0
                },
                {
                    "sent": "The second type of event is.",
                    "label": 0
                },
                {
                    "sent": "An inactive vial becomes active.",
                    "label": 0
                },
                {
                    "sent": "How to check it?",
                    "label": 0
                },
                {
                    "sent": "He told that from the optimality condition we have this condition, which is true for active area.",
                    "label": 0
                },
                {
                    "sent": "And this one.",
                    "label": 0
                },
                {
                    "sent": "For inactive files.",
                    "label": 0
                },
                {
                    "sent": "So if an elective bias becomes active, it means that this quantity becomes equal exactly to Lambda.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just set this equation service in gamma and you have.",
                    "label": 0
                },
                {
                    "sent": "The corresponding value of GABA, which gives you an idea on which condition better J becomes active.",
                    "label": 0
                },
                {
                    "sent": "So if we do this.",
                    "label": 0
                },
                {
                    "sent": "By checking these events for all the parameters in, I better and all the parameters in IO you would have a set of gamma.",
                    "label": 0
                },
                {
                    "sent": "And from this set of gamma just pick up.",
                    "label": 0
                },
                {
                    "sent": "So in this case, if I suppose that I dig in with higher value of Lambda and I decreasing Lambda, just take the smallest value of Lambda of gamma and that's it.",
                    "label": 0
                },
                {
                    "sent": "So at each iteration you compute a set of the step size gamma and choose the small one.",
                    "label": 0
                },
                {
                    "sent": "Just a few here.",
                    "label": 0
                },
                {
                    "sent": "If you want to test this for all parameters in IO.",
                    "label": 1
                },
                {
                    "sent": "Don't focus on all parameters, just take.",
                    "label": 1
                },
                {
                    "sent": "The variable with the most correlation with the actual residual.",
                    "label": 0
                },
                {
                    "sent": "This will give you.",
                    "label": 0
                },
                {
                    "sent": "The corresponding value of gamma, such as these parameter interests your active volume.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can derive the algorithm.",
                    "label": 0
                },
                {
                    "sent": "For the lasso, the way to solve this problem using regularization path that is.",
                    "label": 1
                },
                {
                    "sent": "At the beginning to do similar, you can say it's all your character equal to zero.",
                    "label": 0
                },
                {
                    "sent": "That means that we set a high value of Lambda.",
                    "label": 0
                },
                {
                    "sent": "And what you do at the first step is we check which variables is most correlated to your output and add these variables in your active set and from this you hit all the steps.",
                    "label": 0
                },
                {
                    "sent": "You have one active apps, so you know in which direction you have to go.",
                    "label": 0
                },
                {
                    "sent": "We can compute this decent direction.",
                    "label": 0
                },
                {
                    "sent": "At step T, you compute the step size gamma, so notice that this step size gamma changes at each iteration is not a constant.",
                    "label": 1
                },
                {
                    "sent": "You compute this step gamma by detecting which kind of events occurs is your active viral becomes inactive or an inactive virus becomes active, will check it and according to the events you return here, you can update your set.",
                    "label": 0
                },
                {
                    "sent": "And you iterate until convergence until some termination criterion is met.",
                    "label": 0
                },
                {
                    "sent": "By this way you get all solution.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Quiet is important to do this.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you know better T, but that correspond to Lambda T. And you want to know our solution between Lambda Tyann Lambda which correspond to the solution.",
                    "label": 0
                },
                {
                    "sent": "Just use this linear relation.",
                    "label": 0
                },
                {
                    "sent": "You don't need to.",
                    "label": 0
                },
                {
                    "sent": "Compute.",
                    "label": 0
                },
                {
                    "sent": "The solution solving the square or something like this.",
                    "label": 0
                },
                {
                    "sent": "You know that you have a linear relation, so from the knowledge of this linear relation between the interval, Lambda Chi Lambda, we can compute all your solution in this interval using this linear relation.",
                    "label": 0
                },
                {
                    "sent": "So if you have your generalization criteria, you just apply this year.",
                    "label": 0
                },
                {
                    "sent": "Just plug this relation in your algorithm and you can compute the generation performance of your algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To give.",
                    "label": 0
                },
                {
                    "sent": "An intuitive interpretation of.",
                    "label": 0
                },
                {
                    "sent": "The algorithm I show you.",
                    "label": 0
                },
                {
                    "sent": "Just consider an example.",
                    "label": 0
                },
                {
                    "sent": "In tweet demotion.",
                    "label": 0
                },
                {
                    "sent": "So we have three values for X1X2 and X3.",
                    "label": 0
                },
                {
                    "sent": "Here is our target output.",
                    "label": 0
                },
                {
                    "sent": "At the beginning or.",
                    "label": 0
                },
                {
                    "sent": "Arbiters are set equal to 0.",
                    "label": 1
                },
                {
                    "sent": "What will do?",
                    "label": 0
                },
                {
                    "sent": "As our orbiters are equal to zero, we know that the residual is just the output vector online, so we project this residual on this space of the virus.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing this production.",
                    "label": 0
                },
                {
                    "sent": "We find that.",
                    "label": 0
                },
                {
                    "sent": "We obtained the most.",
                    "label": 0
                },
                {
                    "sent": "The high value of correlation for the variable X one.",
                    "label": 0
                },
                {
                    "sent": "So what we will do will take X1 and will put X1 in the set of active volumes.",
                    "label": 0
                },
                {
                    "sent": "And we know that X one will give.",
                    "label": 0
                },
                {
                    "sent": "The decent direction so will follow this direction.",
                    "label": 0
                },
                {
                    "sent": "Point before.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This direction.",
                    "label": 0
                },
                {
                    "sent": "One question is.",
                    "label": 0
                },
                {
                    "sent": "At which point we will stop.",
                    "label": 0
                },
                {
                    "sent": "The algorithm says that.",
                    "label": 0
                },
                {
                    "sent": "Follow this direction and stop here.",
                    "label": 0
                },
                {
                    "sent": "Where your coalition of X2 and X1 with the current residual are equal.",
                    "label": 0
                },
                {
                    "sent": "Just stop there.",
                    "label": 0
                },
                {
                    "sent": "These two volumes can have the same explanation of your residual, so you don't have any reason to go to this point which correspond to the least square estimation.",
                    "label": 0
                },
                {
                    "sent": "If you do just list where you will go through this solution before adding a new vibe in your active set, so the less would suggest you to stop here, so stop here.",
                    "label": 0
                },
                {
                    "sent": "And at next stage, as this variable X2 is.",
                    "label": 0
                },
                {
                    "sent": "A quick correlated with the residual XX one will add X2 in the active site.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we are exploring in the active set.",
                    "label": 1
                },
                {
                    "sent": "You have now this direction to follow.",
                    "label": 0
                },
                {
                    "sent": "And we will follow this direction and will calculate our estimation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until.",
                    "label": 0
                },
                {
                    "sent": "We get this point.",
                    "label": 0
                },
                {
                    "sent": "And at this point we have the correlation of the residual according to X1 and X2, which is equal to the correlation to extreme with the residual.",
                    "label": 0
                },
                {
                    "sent": "So at this point we add extra in the active set and we can.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New our estimation.",
                    "label": 0
                },
                {
                    "sent": "OK here, let's have a look.",
                    "label": 0
                },
                {
                    "sent": "On the way the parameters.",
                    "label": 0
                },
                {
                    "sent": "Change so as we can see here, we begin with all parameters equal to 0 and at the first step the hydrometer.",
                    "label": 0
                },
                {
                    "sent": "Becomes different from zero at the second step at this one, third said this one, and so on.",
                    "label": 0
                },
                {
                    "sent": "And here we will see that at the moment this parameters is positive and becomes negative.",
                    "label": 0
                },
                {
                    "sent": "So to do this we need that this parameter becomes equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So we move this parameter at this point from the active set and this parameter can enter again active sets after.",
                    "label": 0
                }
            ]
        }
    }
}