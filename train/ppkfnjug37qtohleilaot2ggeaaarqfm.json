{
    "id": "ppkfnjug37qtohleilaot2ggeaaarqfm",
    "title": "Hierarchical Clustering",
    "info": {
        "author": [
            "Yee Whye Teh, University College London"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_teh_hc/",
    "segmentation": [
        [
            "Well, OK, so I'm going to talk about hierarchical clustering today.",
            "It's going to be quite a kind of high level view.",
            "I think that's probably like you can probably have whole conferences on hierarchical clustering because there's all sorts of hierarchical clustering algorithms out there, and this is really kind of like a little idiosyncratic journey to hierarchical clustering landscape.",
            "So I'll tell you about a few of the more popular ones, and a few of the hierarchical clustering algorithms which I think is.",
            "Promising.",
            "OK, so Neil has said that I'm kind of a Bayesian, so indeed I'm going to start with the traditional hierarchical clustering algorithms.",
            "I'm going to slowly move towards more and more Bayesian things as we as the talk."
        ],
        [
            "OK.",
            "So let's start with hierarchical representation.",
            "I think this morning Neil has told you about.",
            "Representing data with like nonlinear.",
            "Manifold right so dimensionality reduction or linear or nonlinear manifolds?",
            "So another very popular way of representing data is using hierarchies.",
            "So representing data using trees basically.",
            "So an basically there's many different types of data that has a hierarchical nature.",
            "So, for example, in natural language processing, you might want to represent sentences, English sentences using its path tree, right?",
            "So each of the interior nodes of this sentence corresponds to a little phrase which describes that part of the sentence.",
            "And the words are kind of the leaves of the tree basically."
        ],
        [
            "So another example of hierarchical representations is in, so here I've kind of shown you a little drawing I got from Internet of the language tree of the world, right?",
            "So languages are the leaves of the tree and then the tree itself kind of describes the evolutionary history of languages, how languages develop and change and split off from each other.",
            "So it's kind of also a hierarchical representation of what believe actually happened.",
            "I'm."
        ],
        [
            "The past.",
            "So another example of hierarchical Rep."
        ],
        [
            "Tentations is in.",
            "You know a phylogenetic trees, so I hope that I think all of us believe in evolution, and if you do believe in evolution, then you do believe that you know all of life on Earth came kind of evolved from one single source, and then if you trace truly evolutionary history of this species then you basically get a evolutionary tree which describes the relationships among all the living organisms on earth."
        ],
        [
            "Still more examples, so here we don't necessarily believe that the data we see actually has any underlying hierarchical structure, but will still like to somehow represent our data in a hierarchical manner.",
            "To visualize it better.",
            "So here I have a little data set in which you have micro micro array data in which you have patients cancer patients, and then you have jeans an each little.",
            "Square here tells you how much that Gene was expressed in that patient.",
            "You might want to cluster the patients.",
            "In such a way that it might be informative about, for example, which stage of cancer did the patient have?",
            "So this might be the stage one cancer patients, and these are the stage three cancer patients, the blue ones.",
            "And you see that there's kind of certain relationships that's being extracted that's being.",
            "Extract it by the tree and on the side of the jeans.",
            "You might want to have some.",
            "Hierarchical representation of the genes such that genes which are more similar to each other in terms of the expression levels are put in the same part of the tree."
        ],
        [
            "OK, still more examples.",
            "So in image processing, an in signal processing, you might represent the images and the signals using things like quadtrees and wavelet decompositions.",
            "So here we have an image like this and then if we.",
            "We can model this image as follows.",
            "We start off with the whole picture.",
            "We split it up into four quarters.",
            "And then within each quarter.",
            "If the.",
            "Image within that quarter is not.",
            "It's not uniform.",
            "Then we we keep on splitting it.",
            "OK, we can recursively split it until within each little region.",
            "Here the image is basically of a uniform color or grayscale distribution I guess."
        ],
        [
            "So in this talk, I'll talk about hierarchical clustering.",
            "So the idea.",
            "Is that if you have some data which comes in this form?",
            "Right, and you believe that your data is clustered, so it comes from it comes the data can be described as a whole bunch of clusters, so you might have one cluster here, another cluster there, another cluster is there.",
            "But within each cluster, you might still you might believe that the cluster also has some sort of internal structure in which you could.",
            "We model with subclusters.",
            "So now if you take your data, you split into clusters within each cluster split into sub clusters.",
            "So that's basically a hierarchical representation of your data.",
            "So that leads to the notion of hierarchical clustering."
        ],
        [
            "So, So what hierarchical?",
            "So that's what I'm going to be talking about today, I guess.",
            "So what are the users of hierarchical clustering?",
            "That's, I think, probably a few important users.",
            "Some of which I described here.",
            "So firstly, it's very useful in terms of visualizing data, right?",
            "So given a set of patients, we might cluster the patients into.",
            "Into you know, cancer patients of different stages, right so?",
            "We might be interested in summarizing data, so we might use the hierarchical clustering as a way of finding the clusters in the data and then instead of representing the data itself, we simply represent each data by which cluster belong to right?",
            "So we don't really.",
            "Sometimes we don't really care about the the.",
            "A lot of the detailed information about about the data about the data points, but simply where it is generally in the space.",
            "Another reason why why we might be interested in hierarchical clustering is that we do believe that our data come came from some sort of hierarchical underlying tree structure, and we actually want to.",
            "We are actually interested in recovering what is the underlying tree structure which gave rise to the data, so this is.",
            "This is example examples of this are things like the language tree example and the phylogenetic tree example.",
            "Oh, by the way, if there's any questions to ask questions."
        ],
        [
            "OK.",
            "So that's a number of jets of say that there's many different types of hierarchical clustering algorithms out there, and they can generally be split into like different sort of approaches.",
            "They could be described in terms of different dimensions to describe the algorithm, so one dimension is in terms of how you construct your hierarchy, so there's two general approaches.",
            "One is a top down approach and one is a bottom.",
            "Approach in the top down approach.",
            "We start off by saying that all our data points belong to one single cluster and then we're going to recursive recursively split this cluster up into smaller and smaller clusters until we decided.",
            "Until we did decide to stop.",
            "The in the bottom up approach, which is also called agglomerative.",
            "We start off with one cluster for every data point, so we start with small clusters an we iteratively find 2 clusters to merge and we can keep on merging the clusters until we get one big cluster for the whole data point for the whole data set.",
            "So it turns out that the dominant approach is the bottom of 1, because it turns out to be much easier to construct to find good trees when you start off with the bottom up rather than top down an.",
            "Um?",
            "I guess one way to think about it is is that when you start off with one big cluster and you want to.",
            "To split it into two smaller clusters.",
            "Without knowing the internal structure of the clusters isn't very hard to decide how to split that one big cluster into smaller clusters.",
            "Another way of looking at it is as follows.",
            "If you have one cluster with N data points, then to find the optimal split into 2 sub clusters right, you need to search through.",
            "How many how many possible splits?",
            "I think it's exponential is 2 to the N number of possible splits of.",
            "Of one cluster into two smaller clusters, right?",
            "But in the case of the bottom up approach, if you have N clusters and you want to decide which two cluster to merge, there's only a quadratic number of them, so it's much easier.",
            "It's much easier to search for the locally optimal pair of clusters to merge then to search for the locally optimal split of 1 cluster into 2.",
            "Yes.",
            "Yes, I'll I'll come to that.",
            "In fact, you could.",
            "Many of this of agglomerative clustering approaches.",
            "The way they differ is in terms of what distance measure you use and how you define distances on clusters.",
            "Given distances on data points.",
            "OK.",
            "Yes.",
            "It doesn't have to be split one into two, but could be split one into multiple.",
            "Choosing the number is.",
            "Anne.",
            "Well depends.",
            "There's also many different ways.",
            "In terms of choosing the number, I guess you could.",
            "If you're basing like me, then you would use things like directly process mixtures.",
            "Because.",
            "When you split a data point to multiple clusters an you don't use some penalty for more clusters than in a sense you have more use more parameters to describe your data.",
            "An if you don't use something, some sort of penalty to penalize for a larger number of clusters, then the best way of describing the data is simply to have every data point being its own cluster.",
            "So then that's different ways of.",
            "Of penalizing for the number of parameters that you've gotten.",
            "One way is to use Bayesian methods.",
            "I think tomorrow problem, maybe that was it zoom call who might either zuben or call will talk about some of the basic methodology and then within that you have algorithms which help you to decide how many clusters should one be split into.",
            "Yes?",
            "Fault sorry OK. Oh, I see.",
            "So there's two dimensions, right?",
            "So each data point could be high dimensional.",
            "And then you could have lots of data points.",
            "In terms of the high dimensionality of the data points, yes you could do dimensionality reduction.",
            "In fact, one of the examples which are given at the end is on handwritten digits, and what I did was I used PCA to reduce it to a low dimensional space and then do hierarchical clustering in the low dimensional space."
        ],
        [
            "OK.",
            "So that's a different.",
            "Dimension to the to the different hierarchical clustering approaches of that which is along the lines of more or less Bayesian algorithms.",
            "OK, so let's start off with the traditional ones, which are called linkage algorithms.",
            "So these are.",
            "You might have heard of them like single linkage average linkage and completely engaged.",
            "And these are algorithms which in a sense is prescriptive prescriptive.",
            "So the.",
            "Is you're simply given the algorithm which produces for you a tree a hierarchy, and it's not clear from a probabilistic perspective.",
            "It's not clear what this tree actually means.",
            "Um?",
            "But you know, it's very easy to implement as very simple to use, so it's very popular.",
            "Um?",
            "Coming up, being more basic now so.",
            "The next set of hierarchical clustering algorithms is kind of based on probabilistic models because the probabilistic models gives you a way in which to judge how good your hierarchy is based on how good it is a generative process of the data, and then you could be more Bayesian and you could do things like so.",
            "These are probabilistic approaches which returned to you a single tree, an if you're interested in recovering a tree from your data.",
            "I hierarchy from your data.",
            "Then if you don't have enough data then you have to be uncertain about what the tree is an in order to capture that uncertainty.",
            "What you like to do is to take a more Bayesian approach, in which you have a whole distribution over trees and then some with higher probability than others, but you want to actually report the distribution of the trees.",
            "So that all is basically going to go through this series of different.",
            "Hierarchical clustering algorithm."
        ],
        [
            "So let's start with the linkage algorithm is very easy to just tell you the algorithm and then we'll go on from that.",
            "So the input.",
            "Is you're given a set of data points X one to XN.",
            "You're also you also need to tell the algorithm what's up distance measure to use, so this distance is this.",
            "D function is simply a function.",
            "Returns to you distance between data points X&Y, so this could be things like L1 distance Euclidean distance.",
            "It could be other sorts of distance measures as well.",
            "And then given this distance measures, you also want some way of combining.",
            "Distance is so the distance between two clusters so C&D here are subsets of your data points.",
            "Anne.",
            "The distance between two subsets two clusters.",
            "Here is some function of this distance between the data points within each subset.",
            "OK. Then the algorithm is really simple.",
            "You start, you initialize each data point in its own separate cluster.",
            "So you initialize XI in cluster CI, which contains only XI.",
            "And then we iterate for N -- 1 iterations.",
            "In each iteration we simply find a pair of clusters C&D.",
            "That minimizes the distance between them, so the idea is that if two clusters are similar to each other, then you want to merge them right?",
            "And the more similar they are, the more you want to prioritize merging them, so you pick a pair of clusters with a minimum distance.",
            "And then simply much these two clusters together by basically removing C&D from your list of clusters and then adding the Union of the two clusters together into your list of clusters, and you just repeat this right?",
            "So in each iteration so you started off with North clusters.",
            "In each iteration we remove two clusters at one cluster, so we subtract the number of clusters by one and after my afternoon minus one iterations.",
            "Although you only have one cluster left.",
            "And this is this class that contains all your data points.",
            "So this is a."
        ],
        [
            "How?",
            "Very kalfa popular class of hierarchal clustering algorithms.",
            "So that's the different language algorithms here.",
            "I've told you.",
            "So single, complete an average and they simply correspond to different distance measures on clusters, so a different function to define distances between clusters given distances between data points.",
            "So in single linkage the distance between two clusters is simply the minimum distance between pair of data points, one from each cluster of the distance.",
            "In complete linkage, the distance between two classes is the maximum distance between data points.",
            "In the cluster.",
            "And then in average linkage is simply the average.",
            "OK, there's of course many different ways of of defining a distance between.",
            "Two clusters, given distances between data points, so these are kind of some of the other examples mean and Central and Walton weighted versions of this so.",
            "But the most popular history in fact, the most popular is probably average linkage.",
            "Just to give you a bit of intuition about what this to this tree."
        ],
        [
            "Irhythm stairs.",
            "So in single linkage saying that the distance between two clusters where one cluster here and another cluster that then the distance between these two clusters is simply the minimum distance of data points.",
            "One from each.",
            "Of a path of data points, one from each cluster right, and notice that.",
            "According to this distance measure, these two clusters are very close to very close together.",
            "If they have two data points which are close together, it doesn't really matter that this data point is very far away from that data point.",
            "Right, So what this single linkage algorithm tend to produce is that it tends to produce long string clusters, because if you have one, if you have a string of data points which belong to one cluster, and you have another string of data points which belong to another cluster, so long as the string of data points.",
            "At some point meet each other.",
            "Then they are considered close and then there would be.",
            "They would have high priority to be merged at the next iteration, so then you get an even long, longer and stringere cluster.",
            "OK."
        ],
        [
            "So for complete linkage.",
            "The distance between two classes in state defined to be the maximum distance between data points in the clusters.",
            "So now the situation is completely different because you cannot really have long stringy clusters right because?",
            "The.",
            "In order for you to form this long stringy cluster, you must have had I draw this.",
            "Maybe so if you have like 1 cluster like this.",
            "And another cluster like that.",
            "Right then, even though the minimum distance between them is very close, the maximum distance between the data point here and a data point here this distance.",
            "Is very far away and so is this two clusters is considered to be very far away and they would much and they would not.",
            "They would not be merged basically.",
            "So you don't get to form like long clusters like this.",
            "Of course, average linkages.",
            "The average of the distances, so it is behavior is somewhat in between single linkage and completely engaged.",
            "I'll give you some.",
            "Yes.",
            "Checks.",
            "No, it's that you don't actually check.",
            "So basically you tell the algorithm which one you want to use and it produces for your tree.",
            "And then you probably want to take a look at the tree to see whether it corresponds to what you believe the tree should be, and if it doesn't, you might want to try a different linkage algorithm.",
            "Just to give you.",
            "Oh wait.",
            "I think I've said something more here.",
            "Let's see.",
            "We'll see.",
            "OK, so here's a.",
            "Still dumb.",
            "Oops.",
            "So I have a.",
            "A data set which consists of data points which look like that, and we're going to use the Euclidean distance for our distance measure between two data points, and this is going to be single linkage complete linkage and average linkage.",
            "OK. We just smaller.",
            "OK, so I think the first iteration has been done and I think some.",
            "Probably either this pair of this pair of data points or either disparate data points or that pair data points have been much.",
            "I can't really tell because they are too close to each other.",
            "OK.",
            "So let's see what happens.",
            "So we see that kind of the clusters are getting much together and you see that in single linkage is really starting to form little strings like that.",
            "OK, so.",
            "So you see that single linkage actually in the end.",
            "If you do this all the way through here.",
            "So you see that the clusters that is formed, so these are the connected components.",
            "In this graph.",
            "There they formed this long stringy clusters.",
            "Long stringy connected components of the graph.",
            "While in the case of complete linkage then they tend to be kind of more.",
            "Cluster together like that.",
            "Right?",
            "While in average linkage is somewhat in between this and that.",
            "So now finally, it's.",
            "That's after it's all been much together.",
            "So basically all the other data points from one cluster now.",
            "The way I'm visualizing single linkage is somewhat different than the way I'm visualizing.",
            "I'm visualizing complete linkage.",
            "Let's just start again.",
            "In single linkage, whenever I merge two clusters, I draw a line between the two data points with the minimum distance, while in the case of completely in case I draw a line between all pairs of data points.",
            "OK, so now you see the incomplete linkage.",
            "It kind of likes to have very tightly coupled clusters like that.",
            "Well, in single linkage is kind of long stringy clusters OK. Yep.",
            "Yeah anyways.",
            "This little demo."
        ],
        [
            "OK, So what are the pros of pros and cons of the linkage algorithms?",
            "Firstly, they are very easy and very fast to use.",
            "I just show you the algorithm, you can see you can imagine it being very easy to implement.",
            "It is also very fast, so the runtime is quadratic in the number of data points.",
            "If you have a huge data set then you could do other things like.",
            "Using probably things like KD trees and stuff to to speed up from quadratic to N log N speed.",
            "It's also very well known and and very well accepted in the community, so if you show somebody visualization of your data set and say this is average linkage, they will understand and you can just move on from that.",
            "OK, so that's a good thing, because it's it's very easy to describe to people what you're actually doing.",
            "But there's negative points to it.",
            "Firstly, the distance metric is really sometimes quite unclear.",
            "Sometimes it's very hard to come up with a good distance metric which actually corresponds to what you believe is.",
            "The distance between the two data points right so?",
            "Neil, did you do the nonlinear dimensionality reduction stuff like Isomap and stuff?",
            "OK. OK, so.",
            "I'm not sure that I'm not sure where the new has told you but, but basically with the nonlinear dimensionality reduction stuff.",
            "It's in fact quite easy to determine.",
            "Weather 2 data items are very similar to each other, but often is very hard to determine how far away the two data points are.",
            "If they are far away, you just know that they are far away, but you don't really.",
            "You can't really have a very good measure of notion of distance between them unless you know the true underlying manifold structure of the data, of course.",
            "Secondly, falsely, I guess so.",
            "With this distance metrics, in order for you to have to define a distance between between two data points, you have to have observed the whole day.",
            "The whole vector, right?",
            "If each data points vector, it has to observe the whole every entry in it.",
            "In the vector, but if some of the entries of the vector are on observed, then you can't really define a distance function.",
            "On two partially observed factors.",
            "There's also no clear semantic for the optimality of the constructor tree.",
            "As a saying, it's kind of a prescriptive approach, you know, is this an algorithm which produces for your tree, you can't really say what is this tree optimizing?",
            "What does what does this tree mean?",
            "And finally, there's no uncertainty about it about the tree structure, so if you're really uncertain about the tree structure, this algorithm cannot tell you that.",
            "It cannot tell you like what part of the tree is.",
            "It's confident about what Power Tree is not confident about, for example."
        ],
        [
            "So to address some of these issues.",
            "Some people have pulled most onto using probabilistic models to to define linkage algorithms, and the idea here is that will use probabilistic models to define the class to define the cluster distance.",
            "So the distance between two clusters.",
            "It's going to be defined as the negative log probability negative log of a ratio of two probabilities.",
            "The first one is the probability of.",
            "The data points in the merged cluster.",
            "And in the denominator we have a probability of the data points in one cluster multiplied by the probability of data points in the other cluster.",
            "And basically what this is saying is that.",
            "If the two sets of data points.",
            "Similar to each other, then it's probably going to be more advantages to model them with a single model, because then you have you can have sharing of information between the two clusters, right?",
            "And if it's advantages, then this probability should be high.",
            "Higher than the product, the probabilities of those two.",
            "So what?",
            "What the denominator is saying is that you want to model this cluster separately from.",
            "From that cluster, kind of in an independent fashion.",
            "An if the two clusters are similar to each other than this probability should be higher than the product of these probabilities and so the distance should be close to be small small.",
            "In fact, this is actually not quite a distance metric, because this thing could be negative.",
            "But you could exponentiate it if you want to have a positive distance matrix doesn't really matter."
        ],
        [
            "OK, so common probabilistic models are so give you 2 examples here.",
            "The first one is a Gaussian, so this is just a normal distribution.",
            "So the probability of the of a cluster is simply the product of the data points in that cluster of the probability of the data points under a Gaussian in normal distribution.",
            "And this normal distribution has a mean and the.",
            "Standard deviation and mean understanding division could be fit to your to the data, and if you're a Bayesian you would integrate the mean and the standard division out.",
            "This is to prevent overfitting.",
            "So another common model is Bernoulli and this is used for if your data comes in the form of binary vectors.",
            "Basically it's saying that each entry of the.",
            "Of the vector is either one or zero is one with the case entry in the vector.",
            "XK is going to be.",
            "Equal to 1 with probability Pi K and it's going to be equal to zero with probability 1 -- \u03c0 K. And the \u03a0 case are the parameters of the.",
            "Multivariate Bernoulli distribution given by here.",
            "An that basically gives you the probability of the cluster."
        ],
        [
            "So let's just look at the Gaussian distribution.",
            "Now.",
            "It basically imposes a pretty strong constraint on what it thinks the cluster looks like.",
            "Clusters have to be shaped like this.",
            "It has verical.",
            "And basically the cluster distance simply measures how spherical or how Gaussian the cluster is.",
            "When Merge Tan before and after merging.",
            "Basically, if by merging the two clusters you get something which looks more Gaussian.",
            "Then this distance will be small.",
            "And basically clusters are merged if the merger produces a more Gaussian looking cluster at each iteration.",
            "So, so that's kind of the.",
            "Oh, description of this probabilistic hierarchical clustering as given by Nir Friedman in 2000."
        ],
        [
            "7.",
            "That's a different way of interpreting this.",
            "Are people here familiar with mixture models?",
            "Somewhat OK, so make sure models are basically.",
            "Models in which?",
            "Is there probabilistic models?",
            "Anne, what you're saying is that your data set is heterogeneous.",
            "They could be coming from different sources.",
            "And you model each source.",
            "So you model the data points coming from each source as coming from some probabilistic model for example.",
            "A Gaussian and the generative model is as follows.",
            "For each data point, you pick which source it came from, and then given the source you pick, you actually generate a data point given that source.",
            "Basically, let's imagine that.",
            "We will model our data set with us with a standard mixture model, say a mixture of Gaussians.",
            "And we'll start each data point XI in its own cluster.",
            "So this is the algorithm for this thing.",
            "So in each iteration we'll find a pair of clusters such that the probability of the data under.",
            "That so such that the probability of the data is maximized if we merge the pair of clusters together.",
            "Turns out that if you workout through the math that this is equivalent to finding a pair of clusters, search that the social disting is maximum."
        ],
        [
            "Or rather, this distance is minimum."
        ],
        [
            "A sign that.",
            "An if this quantity is positive, it means that.",
            "It's more advantages for the model to merge those two clusters together, and if it's negative then it's actually disadvantages to merge those two clusters together, which means that if this is positive then we should simply merge the two classes together, and if this thing is negative, if the maximum of this quantity over all pairs of clusters that you have is already negative, then we should simply stop right because it's this advantages for the algorithm to match any.",
            "Pair of pluses together.",
            "Yes.",
            "Just wondering.",
            "Yes.",
            "Yeah, so in fact.",
            "What is actually done is actually he had two.",
            "Two public two possibilities.",
            "One is to use the maximum posteriori estimates for the mean and variance, and that guts somewhat against overfitting, and the other approaches to actually integrate out the mean and the variance, which in fact.",
            "Totally got you against overfitting and basically.",
            "Instead of just modeling the cost of the data under the model.",
            "Instead of modeling the cost of the data, is simply the negative log probability of the data given the mean and variance we model.",
            "We also pay a penalty too.",
            "To describe the mean and variance basically.",
            "So there's an additional cost to it.",
            "So when you merge 2.",
            "When if you have two data points which are close together and you merge them right, so instead of describing these two data points as using 2 means and two variances, you not only need to describe the pair of data points with 1,000,000 one variance, so you actually paying less cost in terms of coding.",
            "The in terms of describing the parameters of this one single Gaussian.",
            "Makes sense.",
            "So you have less parameters in the model.",
            "I think tomorrow you see more of this Bayesian method methodology.",
            "Yes."
        ],
        [
            "Right?",
            "Yes, so it is.",
            "In some way related to Euclidean distance, because what it what it measures is.",
            "The distance from the data points.",
            "2D cluster means.",
            "Alright, so if you have say.",
            "If you have a bunch of data points here, right, there mean is this thing at this point, and you have another bunch of data points here and there.",
            "Mean is here.",
            "So the total distance from the data points to their mean is very small.",
            "The total distance of the data points to their mean is very small as well, right?",
            "But if you merge them together?",
            "Then the mean is going to be somewhere here.",
            "And the total distance from the data points to the means can be very far away now.",
            "So it's somewhat related.",
            "In fact, if you take the lock, the cancel with exponential and what you have in there is.",
            "The squared distance is Euclidean distance.",
            "OK.",
            "Yes.",
            "Yeah, that's right.",
            "So the mean of the merge cluster is simply the mean of all the data points in the merged cluster.",
            "Yes."
        ],
        [
            "OK.",
            "So, So what is this so just to move on I guess.",
            "What is this interpretation saying?",
            "This is saying that?",
            "The underlying model here is really a mixture model.",
            "And this way of merging classes together is in fact a way of finding a.",
            "Good description of the data in terms of this model.",
            "OK, so basically the clusters here correspond to the different sources in your.",
            "In your mixture model basically."
        ],
        [
            "So if you have a partially constructed tree like this, right?",
            "So you have merged.",
            "This data points into some cluster and if merge those data points into some cluster and those as well, and those around us as well, then what?",
            "The what the model is saying is that the data points within each connected component here.",
            "Is model with a single Gaussian.",
            "So that's of course a single Gaussian is is a very simple model.",
            "Probabilistic model is simply a spherical shape, right?",
            "Um?",
            "Anne Helen Ghahramani so Zuben is going to give a talk tomorrow here, but I'm not sure whether you talk about this thing.",
            "I really like this model so.",
            "What they say is that instead of assuming that each of the constructed subtree here.",
            "Is modelled by a single Gaussian.",
            "Each of the subtrees is in fact going to correspond itself to a mixture model.",
            "So for example, this subtree, right?",
            "So we have left ABC and D is going to be modeled with a mixture where in one component all the data points belong to one Gaussian.",
            "And in another component these two data points belong to one Gaussian and those two data points belong to one Gaussian.",
            "And then in another component, this data point belongs to his own Gaussian, B belongs to its own Gaussian.",
            "And C&D are much together, so these are what's called tree consistent partitions.",
            "And what this is basically, if you take this tree and you kind of like.",
            "Cut the line through this thing.",
            "Then you'll get one of these partitions of your data set.",
            "Um?",
            "Yeah, or one really go into details because it's quite."
        ],
        [
            "Mathematical.",
            "But it turns out that the algorithm is becomes very similar.",
            "The probability of the data under subtree.",
            "Can it can still be computed recursively?",
            "It's not going to be the same as before.",
            "In fact, the probability of a subtree here is going to be a sum of two terms.",
            "One is a kind of probability of the individual subtrees multiplied together with multiplied by some factor and then the other one is.",
            "The problem is probability of all the data points under both subtrees.",
            "Um?",
            "And this approach can be used to obtain a lower bound on the probability of data and directly process mixture.",
            "So that's just the.",
            "A throwaway statement."
        ],
        [
            "OK, So what are the pros and cons of probabilistic hierarchical clustering?",
            "It is still in fact very easy and efficient too.",
            "To implement as well."
        ],
        [
            "The algorithms given to you like this.",
            "So the question is how do you define this problem?",
            "Probability of the Merge cluster versus the probability of the individual clusters."
        ],
        [
            "And you could compute those probabilities efficiently, because you could compute them in a."
        ],
        [
            "Cursive fashion as well.",
            "It really is basically the same framework as the normal linkage algorithms that use that.",
            "I've told you about.",
            "So in a sense it's still easy to describe this.",
            "The output of this algorithms to people because they again still understand language English algorithms.",
            "So the probabilistic models are more interpretable because it's basically gives you a generative process an A tree with a higher probability is simply.",
            "Mall so basically the tree switch.",
            "The algorithm returns to you try to maximize the probability probability of the data.",
            "But unfortunately because this thing has to be probabilistic models.",
            "They are in fact less flexible than then distance metrics, right?",
            "You can define any distance metric that you want and it will work, but in terms of probabilistic models is important that you actually write down a probability distribution.",
            "It has to sum to one over all possible datasets.",
            "So that gives you a constraint basically.",
            "It also deals nicely with partially observed data because."
        ],
        [
            "Um?",
            "The probability of the of the data here.",
            "Probabilities can be computed even if you don't observe.",
            "Every entry in your data vector."
        ],
        [
            "Um?",
            "And there is a coherent measure of goodness of fit for the resulting for the resulting tree, right?",
            "But there's still no notion of uncertainty, because it still again is.",
            "If you if you give it a data set, it only returns to you one single tree.",
            "It doesn't really tell you how much it actually believes that tree is.",
            "The truth is the true true free."
        ],
        [
            "So, um.",
            "Just to reiterate what I said in the beginning, there's in fact two distinct.",
            "Ways in which you could believe the the the.",
            "Listing ways in which you could believe.",
            "The data is generated OK.",
            "There's two possible generative processes for data.",
            "The first one is that we believe that the data comes from unrelated groups or sources.",
            "OK, so this is the mixture modeling approach where you in fact believe that the data point is heterogeneous, so it has come from multiple groups, but within each group it's very simple.",
            "It doesn't have any internal structure at all, it's just one Gaussian for example.",
            "An in this.",
            "In this approach we simply use hierarchical clustering as an efficient search research procedure to find a good way of partitioning our data set into different groups or different sources.",
            "At the other belief is that we might believe that the data in fact has an underlying tree structure, so this is the case for example, in Phylogenetic's an Hilo linguistics as well.",
            "So in this case we want to use hierarchical clustering to in fact find the tree.",
            "Um?",
            "And the.",
            "The following algorithms I'll tell you about it, so the previous algorithm which I've told you about all use.",
            "As the underlying model, a mixture model, so it in fact.",
            "Correspond to that particular to that belief, while the two algorithms which I'll tell you about, not two.",
            "I guess the one framework which I'll tell you about next in fact follows the 2nd.",
            "A set of beliefs.",
            "In fact, we believe that there's an underlying tree and we want to find that underlying tree."
        ],
        [
            "So.",
            "If we do believe that there is an underlying tree to describe the data, then we could simply model the data.",
            "Under the tree using a tree structure model.",
            "So what this means is that if our data is here, so we have four data points here given by ABCD, and if we do believe that there's a tree underlying tree, then there should be some latent tree up here, which is an observed.",
            "Ann, you have some latent variables EF&G that lives on the tree, and the probability of the.",
            "Of the data under the tree is simply the sum over the latent variables of the probability of the roots, and then the multiplied by the probability of each variable given its parent."
        ],
        [
            "OK, so then the algorithm is basically as follows.",
            "We'll start with each data point.",
            "Data Points XI, in its own subtree.",
            "And we will find so an we again E trade.",
            "And in each iteration we take two subtrees and we merge them into a larger tree.",
            "But the subtree which we pick.",
            "We want to find a pair of subtrees such that the likelihood of the data is maximum is maximized after the merger, so this is equivalent to finding a pair of subtrees SMT search that this thing is maximized and this is basically the log of ratio here the top here we have the probability of the data under the much subtree.",
            "And in the bottom again we have a product of the probability of the data under the first subtree and the second subtree.",
            "Ann again, if this lock probability is positive, then it is more.",
            "Advantages for us to merge the two subtrees together so we will merge them, otherwise we'll stop.",
            "OK, so this is a very similar and it turns out that this probability of the data under the subtree can can also be computed efficiently if in a recursive manner.",
            "If you're familiar with it, it just uses belief propagation.",
            "OK, so moving and finally so this algorithm here is.",
            "Again, only finds for you a single tree, right?",
            "Because you know it, just it.",
            "It just iterates and it keeps on merging subtree and until and then at the end of the day you just have a single tree."
        ],
        [
            "Describe your data.",
            "But if you actually want to model uncertainty over trees, then in fact the Bayesian approach is to say that we should use the distribution for trees.",
            "So which?",
            "What we want to compute is the posterior distribution of over trees given our data.",
            "Where this posterior distribution is using base rule here is going to be a product of a prior distribution over trees.",
            "Multiplied by the probability of the data under the tree.",
            "And divided by a normalization constant?",
            "And basically the model for data this P of data given tree is."
        ],
        [
            "Tree structure model that."
        ],
        [
            "They have described here.",
            "It turns out that unfortunately for us basins, the posterior is often intractable and.",
            "That's different approaches to doing approximate inference under this framework."
        ],
        [
            "You could in fact treat this algorithm here as a greedy."
        ],
        [
            "As a greedy approximate inference algorithm in which you construct the tree greedily.",
            "There's also things like you could use Markov chain Monte Carlo, so that's what Chris Williams and revenue uses.",
            "An as well sequential Monte Carlo algorithms, so this is what we proposed as well.",
            "The multi color algorithms are more intricate and expensive.",
            "However they do give you some notion of.",
            "The posterior distribution of the trees.",
            "It actually tells you how much the model actually believes in.",
            "Uh.",
            "In the trees which.",
            "Has constructed basically.",
            "There's also some interesting theoretical work on like.",
            "Nonparametric Bayesian priors over trees.",
            "Wish I can have one time running out of time right someone?",
            "Yeah, but in fact I'm almost finished actually."
        ],
        [
            "So to conclude, actually I'll just do some simple comparisons on a few datasets.",
            "Um?",
            "So I here I've compared average linkage, so this is the traditional algorithm versus this.",
            "This is Helen Ghahramani, so this is that probabilistic.",
            "Hierarchical clustering algorithm.",
            "And then this is the greedy Bayesian approach in which we used restructured probabilistic models.",
            "An we competitive using a few measures of quality of the tree.",
            "One is called priority, one is called subtree an.",
            "The last ones leave one out accuracy.",
            "So on the first data set, it's a very simple data set, so you have.",
            "I think.",
            "Something like 10 or 100.",
            "Examples of handwritten digits between.",
            "090123456789 and they are.",
            "Images of handwritten digits.",
            "An the images are PCA reduced to a low dimensional space and then use we simply use Euclidean distance in the low dimensional space.",
            "As the distance measure OK, or in the case of these two algorithms, use use Gaussians.",
            "And basically this costs are all between zero and one and there.",
            "One means good and 0 means bad.",
            "And you know on on both.",
            "Both this.",
            "A datasets Bayesian, the greedy basin algorithm.",
            "This is the best basically.",
            "Yes.",
            "OK, so the priority is the following so.",
            "Once you've constructed a tree, say something like.",
            "This OK?",
            "And let's say that you're.",
            "You're trying to discriminate between zero and one, so this might be a zero.",
            "That's a 0.",
            "And then there's a 0.",
            "And this one OK.",
            "The purity is the.",
            "Is the probability that if you pick.",
            "Two images of the same class.",
            "That they would belong to the same subtree.",
            "So if this is like.",
            "0 zero and one one right so?",
            "Opath of images of the same class would either be this or that right and they would belong to the same subtree.",
            "So then the Paris.",
            "The priority score is considered high, but if the street that you've constructed looks like 0101, which intuitively kind of is bad, right?",
            "Because it's not really.",
            "Giving you a very discriminative.",
            "Tree in terms of the class in terms of classification.",
            "Then if you pick a pair of zeros.",
            "Then the subtree that.",
            "Is this that kind of contains the smaller subtree which contains this pair of 0?",
            "Is the whole tree itself and this smaller subtree is not pure right?",
            "So if you average over this, then the priority is in fact zero.",
            "In this case an is.",
            "One in that case.",
            "The subtree score is basically.",
            "It is related to the number of pure subtrees.",
            "So.",
            "In this case, the purity score is the subtree scores again one.",
            "Out.",
            "Sorry.",
            "The entropy.",
            "No, it's not.",
            "It's not the entropy.",
            "I'm not sure what you mean by entropy hearing actually, so I'll tell you what the subtree score is so.",
            "I need to draw it for you.",
            "A more complicated tree.",
            "In fact it's going to look like this.",
            "How?",
            "OK, so if you have a.",
            "0011 OK, so all the leaves under here are zeros.",
            "All the lifts under here one or the listeners here zero and all their lives under here, one the subtree score in this case is.",
            "I think is.",
            ".5 I think.",
            "Because the number of pure subtrees here is, this one is full right?",
            "So this is a pure subtree, another subtree, another person, another peer subtree basically pure in the sense that the all the leaves in that subtree have the same label.",
            "And.",
            "Let me see.",
            "So if you have two classes than the minimum number of peers subtrees is 2.",
            "So if your tree managed to split off.",
            "The two classes.",
            "How very well then you would.",
            "It would put all the zeros here.",
            "An older ones here.",
            "Right then the subtree score is simply one.",
            "Discount defined linearly related to the number of years of trees, basically.",
            "Yeah, yeah.",
            "Um?",
            "Yes."
        ],
        [
            "So another data set, so this is.",
            "From a fellow linguistics, so this is wall stands for World Atlas of Language Structures I believe.",
            "And here what we what we do is we construct a tree A to describe the languages.",
            "And then.",
            "We classify each language in terms of which genus it belongs to, which language family belongs to.",
            "Basically and again we use the same scores, and again the kind of degree D basin approach does pretty well except for this particular case, in which somehow average linkage did really well."
        ],
        [
            "So actually it's kind of interesting to look at the trees constructed by the Bayesian.",
            "The greedy Bayesian algorithm, so this is on the Indo European languages and it actually does pretty well so.",
            "Language names are given on this site and this is the language family name.",
            "And you can see that it is basically able to figure out that all the Germanic languages should go together.",
            "All the Romance languages should go together, and then the Celtic ones are up here.",
            "The Slavic ones are here.",
            "In fact, Slavian, but it got a bit confused, but then slowly and body language is is, I believe, quite closely related.",
            "Here it has the Iranian and Indian languages as well.",
            "The information things like.",
            "What's called.",
            "So somebody actually, some linguists actually.",
            "Went off an and collected quite a large database of linguistic features so the linguistic features are things like whether the noun comes whether the subject comes before the verb, whether the object comes after the verb and stuff, and this is whole bunch of binary features that describe the grammatical structure in the language, and we basically represent each language as a binary vector.",
            "We can zoom."
        ],
        [
            "OK, you can zoom in a little bit here to the Germany Ann and romance ones and it's basically got it right.",
            "I think.",
            "Basically it's saying that the Germany languages are very different than the Romance languages because there's a split earlier on here, right?",
            "And then within the Germanic languages then we see that the Scandinavian languages are grouped together.",
            "Dutch and German are quite close to each other.",
            "Anne, the English and Icelandic are quite different than.",
            "Either the jump, either German and Dutch, or the Scandinavian languages.",
            "In on the Roman side, it believes that Italian and Catalan are very similar, which I think is also correct.",
            "And Portuguese and French are more closely related to each other than they are to Spanish.",
            "Which seems to be pretty, is doing reasonably well basically."
        ],
        [
            "So the Bayesian approaches does again some pros and cons, so the it's kind of reversed from the from the linkage algorithms, so it can be very expensive if you actually want to compute the full posterior distribution over trees, you need to use a lot of MCMC algorithm sampling algorithms and stuff.",
            "It's less common.",
            "An less well understood, so if you show somebody the tree and you say that, oh, I'm using.",
            "Directly diffusion trees.",
            "There will be like what right?",
            "So you need to put more effort if you're actually going to.",
            "I use this in some applications.",
            "But Fortunately that they have positive sites.",
            "They have advantages as well.",
            "They have positive things, so they in fact give fully generative probabilistic models and their deals nicely with partially observed data.",
            "And there is coherent measure of whether the trees are.",
            "Of basically there's a measure of goodness of fit for the trees that you've obtained an they could give you uncertain T over the tree structures which you.",
            "I obtained from your data."
        ],
        [
            "OK.",
            "So just to summarize, basically, so I've given a kind of idiosyncratic overview of hierarchical clustering.",
            "And there's a few dimensions.",
            "That I've described first one is top down versus bottom up versus Monte Carlo search, so these are ways in which you could construct your tree.",
            "And the.",
            "The dominant approach is bottom up because it seems to have a much better search landscape than the top down ones, and multicolor searches of this quite expensive basically.",
            "The second dimension here is in terms of flat clustering, so mixture models.",
            "So what is the underlying model which you use?",
            "In your algorithm, so it's mixture models versus tree structured models.",
            "So you should use mixture models if you don't believe that your data is hierarchical, but you just want to use hierarchical clustering as a way of finding.",
            "A good clustering of your data.",
            "If your data is in fact restructured, in fact has this hierarchical structure like in the following linguistics case or the OR with phylogenetic data, then you should in fact used restructured models.",
            "To describe them, yes.",
            "Well, I guess you.",
            "I guess you could.",
            "It's just this is it just doesn't quite correspond to.",
            "Your model which you use should correspond to what you believe about the data, and if you believe that the data is simply clustered into different clusters and there's no relationships between the clusters that you should in fact use something which is based on a mixture model.",
            "I'm.",
            "But of course, you could take one of the Bayesian algorithms to produce where you are clustering.",
            "But yeah, that's fine, but you don't really have things like you know what is meant by.",
            "A good tree anymore basically right?",
            "Because your notion of a good tree in your case should be a good tree is good if it produces good clusters.",
            "But under the model, a good tree is 1, which kind of describes the data the best under tree structure model, which is different.",
            "Yes.",
            "It is possible, it's just.",
            "Yeah, it it, it is possible, is just that what the model is trying to optimize an what.",
            "Well, what you?",
            "Want to what you consider is good trees is they are not quite the same thing basically.",
            "The third dimension is basically, you know you have less space and once and more Bayesian ones.",
            "So yeah, so that's it basically.",
            "Yeah, thank you.",
            "Yes.",
            "Um?",
            "I believe.",
            "So the.",
            "Not all the features were observed, so it's a very sparsely observed data set and I believe what we do is.",
            "If you have a feature for language which you don't observe, then you simply set that.",
            "Feature to become the mean of the feature among all languages for which that features observed.",
            "So this is kind of like, you know, if you have partially observed data.",
            "That's just kind of no good way of assigning.",
            "Distances.",
            "Between the vectors basically so and that's what we do.",
            "Yes."
        ],
        [
            "So.",
            "This one.",
            "I used angle brackets to mean that the merger of these two subtrees.",
            "So.",
            "So you have.",
            "Two subtrees say.",
            "Once a piece here, it might look like this.",
            "OK, and I'm another one is here.",
            "So you have you have treated points here X one X2X3 and then this is X412X567.",
            "So in the bottom line we have the probability of X1 until three given this.",
            "Let's call this S&ST given S. Right where this probability is given by.",
            "That thing OK?",
            "And over here we have probability of X4 until 7.",
            "Given T. So those are the two bottom terms.",
            "And in the top 10, here is the probability of.",
            "X1.",
            "Until X7, so all the data points here and the subtree for which you have much to do.",
            "Basically that's it.",
            "Yes.",
            "Stereo.",
            "That's in fact probably no way of computing the full posterior.",
            "Yes, this is a lot of trees out there.",
            "It would be nice if the world is in fact true.",
            "It still has lots of trees, but I guess the world doesn't have that many trees anymore, but that's in this basin world.",
            "There's allot trees and you cannot really compute the full posterior.",
            "So in fact what people do is too.",
            "As approximate the posterior using samples from basically.",
            "And then within the sample you can collect statistics like so for example.",
            "In this case, right, you might say.",
            "Maybe English should be somewhat related to French because there's lot of French words in English, so maybe you think that if you actually run your your Bayesian tree algorithm right, it should produce for you lots of trees and then some of them English should be belonging to the Romance languages and but in most of them it should still be Germanic because the grammatical structure of English is still.",
            "Mostly Germanic.",
            "Right, yeah?",
            "Yes.",
            "This is purely grammatic I think yeah.",
            "Yeah, it would probably give you a difference answers if you use phonetic structure I think.",
            "Yes, in fact, that's one of the interesting things about this data set is because most people who have done a follow linguistics used.",
            "Not not fanatic.",
            "God.",
            "Ask what again is along with.",
            "Clock.",
            "Lotto something is basically they use sets of words for cognate sites sets.",
            "I think it's called basically sell words, one from each language for which they know from linguists, from historical studies that they had the same roots in an ancient language.",
            "And then they try to model the.",
            "Phonologic final logical changes.",
            "From between the words.",
            "So the English word for.",
            "Let me.",
            "Try to come up with some example.",
            "I don't know like Constitution and the French word for constitutions somewhat similar, and how similar they are tells you about how far away in the past did English and French diverge basically, and use sets of words like that, so that's the typical approach, but in our approach is quite different.",
            "We actually use grammatical information rather than all this account sets.",
            "Yes.",
            "Using their.",
            "Validating the feature space that you use.",
            "So what do you mean by that so?",
            "Backwards.",
            "That's true.",
            "I'm not sure about this case, but I'll tell you about what we what we found."
        ],
        [
            "In the.",
            "About in the Agnes case.",
            "So what I said was that the PCA reduced the images right and in a PCA reduction the first dimensions.",
            "Of course, the most informative one, followed by the second one and the third one and so forth.",
            "But because with PCA reduced it, and in fact we whiten it as well.",
            "The when you look at the vectors, the covariance structure of the of the vectors, there's no information in there.",
            "That tells you that the first dimension is more important than the 2nd and 3rd, but turns out that you could fit the hyper parameters of the model.",
            "I have this model.",
            "To the data and it actually found that indeed that it has settings of the hyperparameters such that the first dimension has the most effect on the construction of the tree.",
            "I followed by the second one, followed by the third one and so forth.",
            "So it actually updates the first dimensions and down with the letter dimensions.",
            "So it somehow figured out that the first few features are more important.",
            "Then the features at the end.",
            "So that's kind of it.",
            "Did manage to do it in that case, but we haven't really looked into the linguistic case.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, OK, so I'm going to talk about hierarchical clustering today.",
                    "label": 1
                },
                {
                    "sent": "It's going to be quite a kind of high level view.",
                    "label": 0
                },
                {
                    "sent": "I think that's probably like you can probably have whole conferences on hierarchical clustering because there's all sorts of hierarchical clustering algorithms out there, and this is really kind of like a little idiosyncratic journey to hierarchical clustering landscape.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you about a few of the more popular ones, and a few of the hierarchical clustering algorithms which I think is.",
                    "label": 0
                },
                {
                    "sent": "Promising.",
                    "label": 0
                },
                {
                    "sent": "OK, so Neil has said that I'm kind of a Bayesian, so indeed I'm going to start with the traditional hierarchical clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'm going to slowly move towards more and more Bayesian things as we as the talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's start with hierarchical representation.",
                    "label": 0
                },
                {
                    "sent": "I think this morning Neil has told you about.",
                    "label": 0
                },
                {
                    "sent": "Representing data with like nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Manifold right so dimensionality reduction or linear or nonlinear manifolds?",
                    "label": 0
                },
                {
                    "sent": "So another very popular way of representing data is using hierarchies.",
                    "label": 0
                },
                {
                    "sent": "So representing data using trees basically.",
                    "label": 0
                },
                {
                    "sent": "So an basically there's many different types of data that has a hierarchical nature.",
                    "label": 1
                },
                {
                    "sent": "So, for example, in natural language processing, you might want to represent sentences, English sentences using its path tree, right?",
                    "label": 0
                },
                {
                    "sent": "So each of the interior nodes of this sentence corresponds to a little phrase which describes that part of the sentence.",
                    "label": 0
                },
                {
                    "sent": "And the words are kind of the leaves of the tree basically.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another example of hierarchical representations is in, so here I've kind of shown you a little drawing I got from Internet of the language tree of the world, right?",
                    "label": 0
                },
                {
                    "sent": "So languages are the leaves of the tree and then the tree itself kind of describes the evolutionary history of languages, how languages develop and change and split off from each other.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of also a hierarchical representation of what believe actually happened.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The past.",
                    "label": 0
                },
                {
                    "sent": "So another example of hierarchical Rep.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tentations is in.",
                    "label": 0
                },
                {
                    "sent": "You know a phylogenetic trees, so I hope that I think all of us believe in evolution, and if you do believe in evolution, then you do believe that you know all of life on Earth came kind of evolved from one single source, and then if you trace truly evolutionary history of this species then you basically get a evolutionary tree which describes the relationships among all the living organisms on earth.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still more examples, so here we don't necessarily believe that the data we see actually has any underlying hierarchical structure, but will still like to somehow represent our data in a hierarchical manner.",
                    "label": 0
                },
                {
                    "sent": "To visualize it better.",
                    "label": 0
                },
                {
                    "sent": "So here I have a little data set in which you have micro micro array data in which you have patients cancer patients, and then you have jeans an each little.",
                    "label": 0
                },
                {
                    "sent": "Square here tells you how much that Gene was expressed in that patient.",
                    "label": 0
                },
                {
                    "sent": "You might want to cluster the patients.",
                    "label": 0
                },
                {
                    "sent": "In such a way that it might be informative about, for example, which stage of cancer did the patient have?",
                    "label": 0
                },
                {
                    "sent": "So this might be the stage one cancer patients, and these are the stage three cancer patients, the blue ones.",
                    "label": 0
                },
                {
                    "sent": "And you see that there's kind of certain relationships that's being extracted that's being.",
                    "label": 0
                },
                {
                    "sent": "Extract it by the tree and on the side of the jeans.",
                    "label": 0
                },
                {
                    "sent": "You might want to have some.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical representation of the genes such that genes which are more similar to each other in terms of the expression levels are put in the same part of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, still more examples.",
                    "label": 0
                },
                {
                    "sent": "So in image processing, an in signal processing, you might represent the images and the signals using things like quadtrees and wavelet decompositions.",
                    "label": 1
                },
                {
                    "sent": "So here we have an image like this and then if we.",
                    "label": 0
                },
                {
                    "sent": "We can model this image as follows.",
                    "label": 0
                },
                {
                    "sent": "We start off with the whole picture.",
                    "label": 0
                },
                {
                    "sent": "We split it up into four quarters.",
                    "label": 0
                },
                {
                    "sent": "And then within each quarter.",
                    "label": 0
                },
                {
                    "sent": "If the.",
                    "label": 0
                },
                {
                    "sent": "Image within that quarter is not.",
                    "label": 0
                },
                {
                    "sent": "It's not uniform.",
                    "label": 0
                },
                {
                    "sent": "Then we we keep on splitting it.",
                    "label": 0
                },
                {
                    "sent": "OK, we can recursively split it until within each little region.",
                    "label": 0
                },
                {
                    "sent": "Here the image is basically of a uniform color or grayscale distribution I guess.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk, I'll talk about hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "So the idea.",
                    "label": 0
                },
                {
                    "sent": "Is that if you have some data which comes in this form?",
                    "label": 0
                },
                {
                    "sent": "Right, and you believe that your data is clustered, so it comes from it comes the data can be described as a whole bunch of clusters, so you might have one cluster here, another cluster there, another cluster is there.",
                    "label": 0
                },
                {
                    "sent": "But within each cluster, you might still you might believe that the cluster also has some sort of internal structure in which you could.",
                    "label": 0
                },
                {
                    "sent": "We model with subclusters.",
                    "label": 0
                },
                {
                    "sent": "So now if you take your data, you split into clusters within each cluster split into sub clusters.",
                    "label": 0
                },
                {
                    "sent": "So that's basically a hierarchical representation of your data.",
                    "label": 1
                },
                {
                    "sent": "So that leads to the notion of hierarchical clustering.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what hierarchical?",
                    "label": 0
                },
                {
                    "sent": "So that's what I'm going to be talking about today, I guess.",
                    "label": 0
                },
                {
                    "sent": "So what are the users of hierarchical clustering?",
                    "label": 1
                },
                {
                    "sent": "That's, I think, probably a few important users.",
                    "label": 0
                },
                {
                    "sent": "Some of which I described here.",
                    "label": 0
                },
                {
                    "sent": "So firstly, it's very useful in terms of visualizing data, right?",
                    "label": 0
                },
                {
                    "sent": "So given a set of patients, we might cluster the patients into.",
                    "label": 0
                },
                {
                    "sent": "Into you know, cancer patients of different stages, right so?",
                    "label": 0
                },
                {
                    "sent": "We might be interested in summarizing data, so we might use the hierarchical clustering as a way of finding the clusters in the data and then instead of representing the data itself, we simply represent each data by which cluster belong to right?",
                    "label": 1
                },
                {
                    "sent": "So we don't really.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we don't really care about the the.",
                    "label": 0
                },
                {
                    "sent": "A lot of the detailed information about about the data about the data points, but simply where it is generally in the space.",
                    "label": 0
                },
                {
                    "sent": "Another reason why why we might be interested in hierarchical clustering is that we do believe that our data come came from some sort of hierarchical underlying tree structure, and we actually want to.",
                    "label": 1
                },
                {
                    "sent": "We are actually interested in recovering what is the underlying tree structure which gave rise to the data, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is example examples of this are things like the language tree example and the phylogenetic tree example.",
                    "label": 0
                },
                {
                    "sent": "Oh, by the way, if there's any questions to ask questions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's a number of jets of say that there's many different types of hierarchical clustering algorithms out there, and they can generally be split into like different sort of approaches.",
                    "label": 0
                },
                {
                    "sent": "They could be described in terms of different dimensions to describe the algorithm, so one dimension is in terms of how you construct your hierarchy, so there's two general approaches.",
                    "label": 0
                },
                {
                    "sent": "One is a top down approach and one is a bottom.",
                    "label": 0
                },
                {
                    "sent": "Approach in the top down approach.",
                    "label": 0
                },
                {
                    "sent": "We start off by saying that all our data points belong to one single cluster and then we're going to recursive recursively split this cluster up into smaller and smaller clusters until we decided.",
                    "label": 0
                },
                {
                    "sent": "Until we did decide to stop.",
                    "label": 0
                },
                {
                    "sent": "The in the bottom up approach, which is also called agglomerative.",
                    "label": 0
                },
                {
                    "sent": "We start off with one cluster for every data point, so we start with small clusters an we iteratively find 2 clusters to merge and we can keep on merging the clusters until we get one big cluster for the whole data point for the whole data set.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that the dominant approach is the bottom of 1, because it turns out to be much easier to construct to find good trees when you start off with the bottom up rather than top down an.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I guess one way to think about it is is that when you start off with one big cluster and you want to.",
                    "label": 0
                },
                {
                    "sent": "To split it into two smaller clusters.",
                    "label": 0
                },
                {
                    "sent": "Without knowing the internal structure of the clusters isn't very hard to decide how to split that one big cluster into smaller clusters.",
                    "label": 0
                },
                {
                    "sent": "Another way of looking at it is as follows.",
                    "label": 0
                },
                {
                    "sent": "If you have one cluster with N data points, then to find the optimal split into 2 sub clusters right, you need to search through.",
                    "label": 0
                },
                {
                    "sent": "How many how many possible splits?",
                    "label": 0
                },
                {
                    "sent": "I think it's exponential is 2 to the N number of possible splits of.",
                    "label": 0
                },
                {
                    "sent": "Of one cluster into two smaller clusters, right?",
                    "label": 0
                },
                {
                    "sent": "But in the case of the bottom up approach, if you have N clusters and you want to decide which two cluster to merge, there's only a quadratic number of them, so it's much easier.",
                    "label": 0
                },
                {
                    "sent": "It's much easier to search for the locally optimal pair of clusters to merge then to search for the locally optimal split of 1 cluster into 2.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'll I'll come to that.",
                    "label": 0
                },
                {
                    "sent": "In fact, you could.",
                    "label": 0
                },
                {
                    "sent": "Many of this of agglomerative clustering approaches.",
                    "label": 0
                },
                {
                    "sent": "The way they differ is in terms of what distance measure you use and how you define distances on clusters.",
                    "label": 0
                },
                {
                    "sent": "Given distances on data points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be split one into two, but could be split one into multiple.",
                    "label": 0
                },
                {
                    "sent": "Choosing the number is.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well depends.",
                    "label": 0
                },
                {
                    "sent": "There's also many different ways.",
                    "label": 0
                },
                {
                    "sent": "In terms of choosing the number, I guess you could.",
                    "label": 0
                },
                {
                    "sent": "If you're basing like me, then you would use things like directly process mixtures.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "When you split a data point to multiple clusters an you don't use some penalty for more clusters than in a sense you have more use more parameters to describe your data.",
                    "label": 0
                },
                {
                    "sent": "An if you don't use something, some sort of penalty to penalize for a larger number of clusters, then the best way of describing the data is simply to have every data point being its own cluster.",
                    "label": 0
                },
                {
                    "sent": "So then that's different ways of.",
                    "label": 0
                },
                {
                    "sent": "Of penalizing for the number of parameters that you've gotten.",
                    "label": 0
                },
                {
                    "sent": "One way is to use Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "I think tomorrow problem, maybe that was it zoom call who might either zuben or call will talk about some of the basic methodology and then within that you have algorithms which help you to decide how many clusters should one be split into.",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                },
                {
                    "sent": "Fault sorry OK. Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "So there's two dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "So each data point could be high dimensional.",
                    "label": 0
                },
                {
                    "sent": "And then you could have lots of data points.",
                    "label": 0
                },
                {
                    "sent": "In terms of the high dimensionality of the data points, yes you could do dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "In fact, one of the examples which are given at the end is on handwritten digits, and what I did was I used PCA to reduce it to a low dimensional space and then do hierarchical clustering in the low dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's a different.",
                    "label": 0
                },
                {
                    "sent": "Dimension to the to the different hierarchical clustering approaches of that which is along the lines of more or less Bayesian algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's start off with the traditional ones, which are called linkage algorithms.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "You might have heard of them like single linkage average linkage and completely engaged.",
                    "label": 0
                },
                {
                    "sent": "And these are algorithms which in a sense is prescriptive prescriptive.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Is you're simply given the algorithm which produces for you a tree a hierarchy, and it's not clear from a probabilistic perspective.",
                    "label": 0
                },
                {
                    "sent": "It's not clear what this tree actually means.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But you know, it's very easy to implement as very simple to use, so it's very popular.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Coming up, being more basic now so.",
                    "label": 0
                },
                {
                    "sent": "The next set of hierarchical clustering algorithms is kind of based on probabilistic models because the probabilistic models gives you a way in which to judge how good your hierarchy is based on how good it is a generative process of the data, and then you could be more Bayesian and you could do things like so.",
                    "label": 0
                },
                {
                    "sent": "These are probabilistic approaches which returned to you a single tree, an if you're interested in recovering a tree from your data.",
                    "label": 0
                },
                {
                    "sent": "I hierarchy from your data.",
                    "label": 0
                },
                {
                    "sent": "Then if you don't have enough data then you have to be uncertain about what the tree is an in order to capture that uncertainty.",
                    "label": 0
                },
                {
                    "sent": "What you like to do is to take a more Bayesian approach, in which you have a whole distribution over trees and then some with higher probability than others, but you want to actually report the distribution of the trees.",
                    "label": 0
                },
                {
                    "sent": "So that all is basically going to go through this series of different.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical clustering algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with the linkage algorithm is very easy to just tell you the algorithm and then we'll go on from that.",
                    "label": 0
                },
                {
                    "sent": "So the input.",
                    "label": 0
                },
                {
                    "sent": "Is you're given a set of data points X one to XN.",
                    "label": 0
                },
                {
                    "sent": "You're also you also need to tell the algorithm what's up distance measure to use, so this distance is this.",
                    "label": 0
                },
                {
                    "sent": "D function is simply a function.",
                    "label": 0
                },
                {
                    "sent": "Returns to you distance between data points X&Y, so this could be things like L1 distance Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "It could be other sorts of distance measures as well.",
                    "label": 0
                },
                {
                    "sent": "And then given this distance measures, you also want some way of combining.",
                    "label": 0
                },
                {
                    "sent": "Distance is so the distance between two clusters so C&D here are subsets of your data points.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The distance between two subsets two clusters.",
                    "label": 0
                },
                {
                    "sent": "Here is some function of this distance between the data points within each subset.",
                    "label": 0
                },
                {
                    "sent": "OK. Then the algorithm is really simple.",
                    "label": 0
                },
                {
                    "sent": "You start, you initialize each data point in its own separate cluster.",
                    "label": 1
                },
                {
                    "sent": "So you initialize XI in cluster CI, which contains only XI.",
                    "label": 1
                },
                {
                    "sent": "And then we iterate for N -- 1 iterations.",
                    "label": 0
                },
                {
                    "sent": "In each iteration we simply find a pair of clusters C&D.",
                    "label": 0
                },
                {
                    "sent": "That minimizes the distance between them, so the idea is that if two clusters are similar to each other, then you want to merge them right?",
                    "label": 0
                },
                {
                    "sent": "And the more similar they are, the more you want to prioritize merging them, so you pick a pair of clusters with a minimum distance.",
                    "label": 0
                },
                {
                    "sent": "And then simply much these two clusters together by basically removing C&D from your list of clusters and then adding the Union of the two clusters together into your list of clusters, and you just repeat this right?",
                    "label": 0
                },
                {
                    "sent": "So in each iteration so you started off with North clusters.",
                    "label": 0
                },
                {
                    "sent": "In each iteration we remove two clusters at one cluster, so we subtract the number of clusters by one and after my afternoon minus one iterations.",
                    "label": 0
                },
                {
                    "sent": "Although you only have one cluster left.",
                    "label": 0
                },
                {
                    "sent": "And this is this class that contains all your data points.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Very kalfa popular class of hierarchal clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "So that's the different language algorithms here.",
                    "label": 0
                },
                {
                    "sent": "I've told you.",
                    "label": 0
                },
                {
                    "sent": "So single, complete an average and they simply correspond to different distance measures on clusters, so a different function to define distances between clusters given distances between data points.",
                    "label": 0
                },
                {
                    "sent": "So in single linkage the distance between two clusters is simply the minimum distance between pair of data points, one from each cluster of the distance.",
                    "label": 0
                },
                {
                    "sent": "In complete linkage, the distance between two classes is the maximum distance between data points.",
                    "label": 0
                },
                {
                    "sent": "In the cluster.",
                    "label": 0
                },
                {
                    "sent": "And then in average linkage is simply the average.",
                    "label": 1
                },
                {
                    "sent": "OK, there's of course many different ways of of defining a distance between.",
                    "label": 1
                },
                {
                    "sent": "Two clusters, given distances between data points, so these are kind of some of the other examples mean and Central and Walton weighted versions of this so.",
                    "label": 0
                },
                {
                    "sent": "But the most popular history in fact, the most popular is probably average linkage.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a bit of intuition about what this to this tree.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Irhythm stairs.",
                    "label": 0
                },
                {
                    "sent": "So in single linkage saying that the distance between two clusters where one cluster here and another cluster that then the distance between these two clusters is simply the minimum distance of data points.",
                    "label": 0
                },
                {
                    "sent": "One from each.",
                    "label": 0
                },
                {
                    "sent": "Of a path of data points, one from each cluster right, and notice that.",
                    "label": 0
                },
                {
                    "sent": "According to this distance measure, these two clusters are very close to very close together.",
                    "label": 0
                },
                {
                    "sent": "If they have two data points which are close together, it doesn't really matter that this data point is very far away from that data point.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this single linkage algorithm tend to produce is that it tends to produce long string clusters, because if you have one, if you have a string of data points which belong to one cluster, and you have another string of data points which belong to another cluster, so long as the string of data points.",
                    "label": 0
                },
                {
                    "sent": "At some point meet each other.",
                    "label": 0
                },
                {
                    "sent": "Then they are considered close and then there would be.",
                    "label": 0
                },
                {
                    "sent": "They would have high priority to be merged at the next iteration, so then you get an even long, longer and stringere cluster.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for complete linkage.",
                    "label": 0
                },
                {
                    "sent": "The distance between two classes in state defined to be the maximum distance between data points in the clusters.",
                    "label": 0
                },
                {
                    "sent": "So now the situation is completely different because you cannot really have long stringy clusters right because?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "In order for you to form this long stringy cluster, you must have had I draw this.",
                    "label": 0
                },
                {
                    "sent": "Maybe so if you have like 1 cluster like this.",
                    "label": 0
                },
                {
                    "sent": "And another cluster like that.",
                    "label": 0
                },
                {
                    "sent": "Right then, even though the minimum distance between them is very close, the maximum distance between the data point here and a data point here this distance.",
                    "label": 0
                },
                {
                    "sent": "Is very far away and so is this two clusters is considered to be very far away and they would much and they would not.",
                    "label": 0
                },
                {
                    "sent": "They would not be merged basically.",
                    "label": 0
                },
                {
                    "sent": "So you don't get to form like long clusters like this.",
                    "label": 0
                },
                {
                    "sent": "Of course, average linkages.",
                    "label": 0
                },
                {
                    "sent": "The average of the distances, so it is behavior is somewhat in between single linkage and completely engaged.",
                    "label": 0
                },
                {
                    "sent": "I'll give you some.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Checks.",
                    "label": 0
                },
                {
                    "sent": "No, it's that you don't actually check.",
                    "label": 0
                },
                {
                    "sent": "So basically you tell the algorithm which one you want to use and it produces for your tree.",
                    "label": 0
                },
                {
                    "sent": "And then you probably want to take a look at the tree to see whether it corresponds to what you believe the tree should be, and if it doesn't, you might want to try a different linkage algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just to give you.",
                    "label": 0
                },
                {
                    "sent": "Oh wait.",
                    "label": 0
                },
                {
                    "sent": "I think I've said something more here.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "We'll see.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a.",
                    "label": 0
                },
                {
                    "sent": "Still dumb.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "So I have a.",
                    "label": 0
                },
                {
                    "sent": "A data set which consists of data points which look like that, and we're going to use the Euclidean distance for our distance measure between two data points, and this is going to be single linkage complete linkage and average linkage.",
                    "label": 0
                },
                {
                    "sent": "OK. We just smaller.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think the first iteration has been done and I think some.",
                    "label": 0
                },
                {
                    "sent": "Probably either this pair of this pair of data points or either disparate data points or that pair data points have been much.",
                    "label": 0
                },
                {
                    "sent": "I can't really tell because they are too close to each other.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "So we see that kind of the clusters are getting much together and you see that in single linkage is really starting to form little strings like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So you see that single linkage actually in the end.",
                    "label": 0
                },
                {
                    "sent": "If you do this all the way through here.",
                    "label": 0
                },
                {
                    "sent": "So you see that the clusters that is formed, so these are the connected components.",
                    "label": 0
                },
                {
                    "sent": "In this graph.",
                    "label": 0
                },
                {
                    "sent": "There they formed this long stringy clusters.",
                    "label": 0
                },
                {
                    "sent": "Long stringy connected components of the graph.",
                    "label": 0
                },
                {
                    "sent": "While in the case of complete linkage then they tend to be kind of more.",
                    "label": 0
                },
                {
                    "sent": "Cluster together like that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "While in average linkage is somewhat in between this and that.",
                    "label": 0
                },
                {
                    "sent": "So now finally, it's.",
                    "label": 0
                },
                {
                    "sent": "That's after it's all been much together.",
                    "label": 0
                },
                {
                    "sent": "So basically all the other data points from one cluster now.",
                    "label": 0
                },
                {
                    "sent": "The way I'm visualizing single linkage is somewhat different than the way I'm visualizing.",
                    "label": 0
                },
                {
                    "sent": "I'm visualizing complete linkage.",
                    "label": 0
                },
                {
                    "sent": "Let's just start again.",
                    "label": 0
                },
                {
                    "sent": "In single linkage, whenever I merge two clusters, I draw a line between the two data points with the minimum distance, while in the case of completely in case I draw a line between all pairs of data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you see the incomplete linkage.",
                    "label": 0
                },
                {
                    "sent": "It kind of likes to have very tightly coupled clusters like that.",
                    "label": 0
                },
                {
                    "sent": "Well, in single linkage is kind of long stringy clusters OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah anyways.",
                    "label": 0
                },
                {
                    "sent": "This little demo.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are the pros of pros and cons of the linkage algorithms?",
                    "label": 1
                },
                {
                    "sent": "Firstly, they are very easy and very fast to use.",
                    "label": 0
                },
                {
                    "sent": "I just show you the algorithm, you can see you can imagine it being very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "It is also very fast, so the runtime is quadratic in the number of data points.",
                    "label": 0
                },
                {
                    "sent": "If you have a huge data set then you could do other things like.",
                    "label": 0
                },
                {
                    "sent": "Using probably things like KD trees and stuff to to speed up from quadratic to N log N speed.",
                    "label": 0
                },
                {
                    "sent": "It's also very well known and and very well accepted in the community, so if you show somebody visualization of your data set and say this is average linkage, they will understand and you can just move on from that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a good thing, because it's it's very easy to describe to people what you're actually doing.",
                    "label": 0
                },
                {
                    "sent": "But there's negative points to it.",
                    "label": 0
                },
                {
                    "sent": "Firstly, the distance metric is really sometimes quite unclear.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's very hard to come up with a good distance metric which actually corresponds to what you believe is.",
                    "label": 0
                },
                {
                    "sent": "The distance between the two data points right so?",
                    "label": 0
                },
                {
                    "sent": "Neil, did you do the nonlinear dimensionality reduction stuff like Isomap and stuff?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that I'm not sure where the new has told you but, but basically with the nonlinear dimensionality reduction stuff.",
                    "label": 0
                },
                {
                    "sent": "It's in fact quite easy to determine.",
                    "label": 0
                },
                {
                    "sent": "Weather 2 data items are very similar to each other, but often is very hard to determine how far away the two data points are.",
                    "label": 0
                },
                {
                    "sent": "If they are far away, you just know that they are far away, but you don't really.",
                    "label": 0
                },
                {
                    "sent": "You can't really have a very good measure of notion of distance between them unless you know the true underlying manifold structure of the data, of course.",
                    "label": 0
                },
                {
                    "sent": "Secondly, falsely, I guess so.",
                    "label": 0
                },
                {
                    "sent": "With this distance metrics, in order for you to have to define a distance between between two data points, you have to have observed the whole day.",
                    "label": 0
                },
                {
                    "sent": "The whole vector, right?",
                    "label": 0
                },
                {
                    "sent": "If each data points vector, it has to observe the whole every entry in it.",
                    "label": 0
                },
                {
                    "sent": "In the vector, but if some of the entries of the vector are on observed, then you can't really define a distance function.",
                    "label": 1
                },
                {
                    "sent": "On two partially observed factors.",
                    "label": 0
                },
                {
                    "sent": "There's also no clear semantic for the optimality of the constructor tree.",
                    "label": 1
                },
                {
                    "sent": "As a saying, it's kind of a prescriptive approach, you know, is this an algorithm which produces for your tree, you can't really say what is this tree optimizing?",
                    "label": 0
                },
                {
                    "sent": "What does what does this tree mean?",
                    "label": 1
                },
                {
                    "sent": "And finally, there's no uncertainty about it about the tree structure, so if you're really uncertain about the tree structure, this algorithm cannot tell you that.",
                    "label": 0
                },
                {
                    "sent": "It cannot tell you like what part of the tree is.",
                    "label": 0
                },
                {
                    "sent": "It's confident about what Power Tree is not confident about, for example.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to address some of these issues.",
                    "label": 0
                },
                {
                    "sent": "Some people have pulled most onto using probabilistic models to to define linkage algorithms, and the idea here is that will use probabilistic models to define the class to define the cluster distance.",
                    "label": 1
                },
                {
                    "sent": "So the distance between two clusters.",
                    "label": 0
                },
                {
                    "sent": "It's going to be defined as the negative log probability negative log of a ratio of two probabilities.",
                    "label": 0
                },
                {
                    "sent": "The first one is the probability of.",
                    "label": 0
                },
                {
                    "sent": "The data points in the merged cluster.",
                    "label": 0
                },
                {
                    "sent": "And in the denominator we have a probability of the data points in one cluster multiplied by the probability of data points in the other cluster.",
                    "label": 0
                },
                {
                    "sent": "And basically what this is saying is that.",
                    "label": 0
                },
                {
                    "sent": "If the two sets of data points.",
                    "label": 0
                },
                {
                    "sent": "Similar to each other, then it's probably going to be more advantages to model them with a single model, because then you have you can have sharing of information between the two clusters, right?",
                    "label": 0
                },
                {
                    "sent": "And if it's advantages, then this probability should be high.",
                    "label": 0
                },
                {
                    "sent": "Higher than the product, the probabilities of those two.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What the denominator is saying is that you want to model this cluster separately from.",
                    "label": 0
                },
                {
                    "sent": "From that cluster, kind of in an independent fashion.",
                    "label": 0
                },
                {
                    "sent": "An if the two clusters are similar to each other than this probability should be higher than the product of these probabilities and so the distance should be close to be small small.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is actually not quite a distance metric, because this thing could be negative.",
                    "label": 0
                },
                {
                    "sent": "But you could exponentiate it if you want to have a positive distance matrix doesn't really matter.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so common probabilistic models are so give you 2 examples here.",
                    "label": 0
                },
                {
                    "sent": "The first one is a Gaussian, so this is just a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So the probability of the of a cluster is simply the product of the data points in that cluster of the probability of the data points under a Gaussian in normal distribution.",
                    "label": 0
                },
                {
                    "sent": "And this normal distribution has a mean and the.",
                    "label": 0
                },
                {
                    "sent": "Standard deviation and mean understanding division could be fit to your to the data, and if you're a Bayesian you would integrate the mean and the standard division out.",
                    "label": 1
                },
                {
                    "sent": "This is to prevent overfitting.",
                    "label": 0
                },
                {
                    "sent": "So another common model is Bernoulli and this is used for if your data comes in the form of binary vectors.",
                    "label": 1
                },
                {
                    "sent": "Basically it's saying that each entry of the.",
                    "label": 0
                },
                {
                    "sent": "Of the vector is either one or zero is one with the case entry in the vector.",
                    "label": 0
                },
                {
                    "sent": "XK is going to be.",
                    "label": 0
                },
                {
                    "sent": "Equal to 1 with probability Pi K and it's going to be equal to zero with probability 1 -- \u03c0 K. And the \u03a0 case are the parameters of the.",
                    "label": 0
                },
                {
                    "sent": "Multivariate Bernoulli distribution given by here.",
                    "label": 0
                },
                {
                    "sent": "An that basically gives you the probability of the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's just look at the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "It basically imposes a pretty strong constraint on what it thinks the cluster looks like.",
                    "label": 0
                },
                {
                    "sent": "Clusters have to be shaped like this.",
                    "label": 0
                },
                {
                    "sent": "It has verical.",
                    "label": 0
                },
                {
                    "sent": "And basically the cluster distance simply measures how spherical or how Gaussian the cluster is.",
                    "label": 1
                },
                {
                    "sent": "When Merge Tan before and after merging.",
                    "label": 0
                },
                {
                    "sent": "Basically, if by merging the two clusters you get something which looks more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then this distance will be small.",
                    "label": 0
                },
                {
                    "sent": "And basically clusters are merged if the merger produces a more Gaussian looking cluster at each iteration.",
                    "label": 1
                },
                {
                    "sent": "So, so that's kind of the.",
                    "label": 0
                },
                {
                    "sent": "Oh, description of this probabilistic hierarchical clustering as given by Nir Friedman in 2000.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "7.",
                    "label": 0
                },
                {
                    "sent": "That's a different way of interpreting this.",
                    "label": 0
                },
                {
                    "sent": "Are people here familiar with mixture models?",
                    "label": 0
                },
                {
                    "sent": "Somewhat OK, so make sure models are basically.",
                    "label": 0
                },
                {
                    "sent": "Models in which?",
                    "label": 0
                },
                {
                    "sent": "Is there probabilistic models?",
                    "label": 0
                },
                {
                    "sent": "Anne, what you're saying is that your data set is heterogeneous.",
                    "label": 0
                },
                {
                    "sent": "They could be coming from different sources.",
                    "label": 0
                },
                {
                    "sent": "And you model each source.",
                    "label": 0
                },
                {
                    "sent": "So you model the data points coming from each source as coming from some probabilistic model for example.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian and the generative model is as follows.",
                    "label": 0
                },
                {
                    "sent": "For each data point, you pick which source it came from, and then given the source you pick, you actually generate a data point given that source.",
                    "label": 0
                },
                {
                    "sent": "Basically, let's imagine that.",
                    "label": 0
                },
                {
                    "sent": "We will model our data set with us with a standard mixture model, say a mixture of Gaussians.",
                    "label": 1
                },
                {
                    "sent": "And we'll start each data point XI in its own cluster.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm for this thing.",
                    "label": 0
                },
                {
                    "sent": "So in each iteration we'll find a pair of clusters such that the probability of the data under.",
                    "label": 1
                },
                {
                    "sent": "That so such that the probability of the data is maximized if we merge the pair of clusters together.",
                    "label": 0
                },
                {
                    "sent": "Turns out that if you workout through the math that this is equivalent to finding a pair of clusters, search that the social disting is maximum.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or rather, this distance is minimum.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sign that.",
                    "label": 0
                },
                {
                    "sent": "An if this quantity is positive, it means that.",
                    "label": 0
                },
                {
                    "sent": "It's more advantages for the model to merge those two clusters together, and if it's negative then it's actually disadvantages to merge those two clusters together, which means that if this is positive then we should simply merge the two classes together, and if this thing is negative, if the maximum of this quantity over all pairs of clusters that you have is already negative, then we should simply stop right because it's this advantages for the algorithm to match any.",
                    "label": 0
                },
                {
                    "sent": "Pair of pluses together.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Just wondering.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in fact.",
                    "label": 0
                },
                {
                    "sent": "What is actually done is actually he had two.",
                    "label": 0
                },
                {
                    "sent": "Two public two possibilities.",
                    "label": 0
                },
                {
                    "sent": "One is to use the maximum posteriori estimates for the mean and variance, and that guts somewhat against overfitting, and the other approaches to actually integrate out the mean and the variance, which in fact.",
                    "label": 0
                },
                {
                    "sent": "Totally got you against overfitting and basically.",
                    "label": 0
                },
                {
                    "sent": "Instead of just modeling the cost of the data under the model.",
                    "label": 0
                },
                {
                    "sent": "Instead of modeling the cost of the data, is simply the negative log probability of the data given the mean and variance we model.",
                    "label": 0
                },
                {
                    "sent": "We also pay a penalty too.",
                    "label": 0
                },
                {
                    "sent": "To describe the mean and variance basically.",
                    "label": 0
                },
                {
                    "sent": "So there's an additional cost to it.",
                    "label": 0
                },
                {
                    "sent": "So when you merge 2.",
                    "label": 0
                },
                {
                    "sent": "When if you have two data points which are close together and you merge them right, so instead of describing these two data points as using 2 means and two variances, you not only need to describe the pair of data points with 1,000,000 one variance, so you actually paying less cost in terms of coding.",
                    "label": 0
                },
                {
                    "sent": "The in terms of describing the parameters of this one single Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Makes sense.",
                    "label": 0
                },
                {
                    "sent": "So you have less parameters in the model.",
                    "label": 0
                },
                {
                    "sent": "I think tomorrow you see more of this Bayesian method methodology.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so it is.",
                    "label": 0
                },
                {
                    "sent": "In some way related to Euclidean distance, because what it what it measures is.",
                    "label": 0
                },
                {
                    "sent": "The distance from the data points.",
                    "label": 0
                },
                {
                    "sent": "2D cluster means.",
                    "label": 0
                },
                {
                    "sent": "Alright, so if you have say.",
                    "label": 0
                },
                {
                    "sent": "If you have a bunch of data points here, right, there mean is this thing at this point, and you have another bunch of data points here and there.",
                    "label": 0
                },
                {
                    "sent": "Mean is here.",
                    "label": 0
                },
                {
                    "sent": "So the total distance from the data points to their mean is very small.",
                    "label": 0
                },
                {
                    "sent": "The total distance of the data points to their mean is very small as well, right?",
                    "label": 0
                },
                {
                    "sent": "But if you merge them together?",
                    "label": 0
                },
                {
                    "sent": "Then the mean is going to be somewhere here.",
                    "label": 0
                },
                {
                    "sent": "And the total distance from the data points to the means can be very far away now.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhat related.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you take the lock, the cancel with exponential and what you have in there is.",
                    "label": 0
                },
                {
                    "sent": "The squared distance is Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So the mean of the merge cluster is simply the mean of all the data points in the merged cluster.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, So what is this so just to move on I guess.",
                    "label": 0
                },
                {
                    "sent": "What is this interpretation saying?",
                    "label": 0
                },
                {
                    "sent": "This is saying that?",
                    "label": 0
                },
                {
                    "sent": "The underlying model here is really a mixture model.",
                    "label": 1
                },
                {
                    "sent": "And this way of merging classes together is in fact a way of finding a.",
                    "label": 0
                },
                {
                    "sent": "Good description of the data in terms of this model.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically the clusters here correspond to the different sources in your.",
                    "label": 1
                },
                {
                    "sent": "In your mixture model basically.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you have a partially constructed tree like this, right?",
                    "label": 1
                },
                {
                    "sent": "So you have merged.",
                    "label": 0
                },
                {
                    "sent": "This data points into some cluster and if merge those data points into some cluster and those as well, and those around us as well, then what?",
                    "label": 0
                },
                {
                    "sent": "The what the model is saying is that the data points within each connected component here.",
                    "label": 1
                },
                {
                    "sent": "Is model with a single Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So that's of course a single Gaussian is is a very simple model.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic model is simply a spherical shape, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Anne Helen Ghahramani so Zuben is going to give a talk tomorrow here, but I'm not sure whether you talk about this thing.",
                    "label": 0
                },
                {
                    "sent": "I really like this model so.",
                    "label": 0
                },
                {
                    "sent": "What they say is that instead of assuming that each of the constructed subtree here.",
                    "label": 0
                },
                {
                    "sent": "Is modelled by a single Gaussian.",
                    "label": 1
                },
                {
                    "sent": "Each of the subtrees is in fact going to correspond itself to a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So for example, this subtree, right?",
                    "label": 0
                },
                {
                    "sent": "So we have left ABC and D is going to be modeled with a mixture where in one component all the data points belong to one Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And in another component these two data points belong to one Gaussian and those two data points belong to one Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And then in another component, this data point belongs to his own Gaussian, B belongs to its own Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And C&D are much together, so these are what's called tree consistent partitions.",
                    "label": 0
                },
                {
                    "sent": "And what this is basically, if you take this tree and you kind of like.",
                    "label": 0
                },
                {
                    "sent": "Cut the line through this thing.",
                    "label": 0
                },
                {
                    "sent": "Then you'll get one of these partitions of your data set.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, or one really go into details because it's quite.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mathematical.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that the algorithm is becomes very similar.",
                    "label": 0
                },
                {
                    "sent": "The probability of the data under subtree.",
                    "label": 1
                },
                {
                    "sent": "Can it can still be computed recursively?",
                    "label": 0
                },
                {
                    "sent": "It's not going to be the same as before.",
                    "label": 0
                },
                {
                    "sent": "In fact, the probability of a subtree here is going to be a sum of two terms.",
                    "label": 0
                },
                {
                    "sent": "One is a kind of probability of the individual subtrees multiplied together with multiplied by some factor and then the other one is.",
                    "label": 0
                },
                {
                    "sent": "The problem is probability of all the data points under both subtrees.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this approach can be used to obtain a lower bound on the probability of data and directly process mixture.",
                    "label": 1
                },
                {
                    "sent": "So that's just the.",
                    "label": 0
                },
                {
                    "sent": "A throwaway statement.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are the pros and cons of probabilistic hierarchical clustering?",
                    "label": 1
                },
                {
                    "sent": "It is still in fact very easy and efficient too.",
                    "label": 0
                },
                {
                    "sent": "To implement as well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithms given to you like this.",
                    "label": 0
                },
                {
                    "sent": "So the question is how do you define this problem?",
                    "label": 0
                },
                {
                    "sent": "Probability of the Merge cluster versus the probability of the individual clusters.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you could compute those probabilities efficiently, because you could compute them in a.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cursive fashion as well.",
                    "label": 0
                },
                {
                    "sent": "It really is basically the same framework as the normal linkage algorithms that use that.",
                    "label": 1
                },
                {
                    "sent": "I've told you about.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it's still easy to describe this.",
                    "label": 0
                },
                {
                    "sent": "The output of this algorithms to people because they again still understand language English algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the probabilistic models are more interpretable because it's basically gives you a generative process an A tree with a higher probability is simply.",
                    "label": 1
                },
                {
                    "sent": "Mall so basically the tree switch.",
                    "label": 0
                },
                {
                    "sent": "The algorithm returns to you try to maximize the probability probability of the data.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately because this thing has to be probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "They are in fact less flexible than then distance metrics, right?",
                    "label": 0
                },
                {
                    "sent": "You can define any distance metric that you want and it will work, but in terms of probabilistic models is important that you actually write down a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "It has to sum to one over all possible datasets.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a constraint basically.",
                    "label": 0
                },
                {
                    "sent": "It also deals nicely with partially observed data because.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The probability of the of the data here.",
                    "label": 0
                },
                {
                    "sent": "Probabilities can be computed even if you don't observe.",
                    "label": 0
                },
                {
                    "sent": "Every entry in your data vector.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And there is a coherent measure of goodness of fit for the resulting for the resulting tree, right?",
                    "label": 1
                },
                {
                    "sent": "But there's still no notion of uncertainty, because it still again is.",
                    "label": 0
                },
                {
                    "sent": "If you if you give it a data set, it only returns to you one single tree.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really tell you how much it actually believes that tree is.",
                    "label": 0
                },
                {
                    "sent": "The truth is the true true free.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Just to reiterate what I said in the beginning, there's in fact two distinct.",
                    "label": 0
                },
                {
                    "sent": "Ways in which you could believe the the the.",
                    "label": 0
                },
                {
                    "sent": "Listing ways in which you could believe.",
                    "label": 0
                },
                {
                    "sent": "The data is generated OK.",
                    "label": 0
                },
                {
                    "sent": "There's two possible generative processes for data.",
                    "label": 0
                },
                {
                    "sent": "The first one is that we believe that the data comes from unrelated groups or sources.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the mixture modeling approach where you in fact believe that the data point is heterogeneous, so it has come from multiple groups, but within each group it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have any internal structure at all, it's just one Gaussian for example.",
                    "label": 0
                },
                {
                    "sent": "An in this.",
                    "label": 0
                },
                {
                    "sent": "In this approach we simply use hierarchical clustering as an efficient search research procedure to find a good way of partitioning our data set into different groups or different sources.",
                    "label": 1
                },
                {
                    "sent": "At the other belief is that we might believe that the data in fact has an underlying tree structure, so this is the case for example, in Phylogenetic's an Hilo linguistics as well.",
                    "label": 1
                },
                {
                    "sent": "So in this case we want to use hierarchical clustering to in fact find the tree.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The following algorithms I'll tell you about it, so the previous algorithm which I've told you about all use.",
                    "label": 0
                },
                {
                    "sent": "As the underlying model, a mixture model, so it in fact.",
                    "label": 0
                },
                {
                    "sent": "Correspond to that particular to that belief, while the two algorithms which I'll tell you about, not two.",
                    "label": 1
                },
                {
                    "sent": "I guess the one framework which I'll tell you about next in fact follows the 2nd.",
                    "label": 0
                },
                {
                    "sent": "A set of beliefs.",
                    "label": 0
                },
                {
                    "sent": "In fact, we believe that there's an underlying tree and we want to find that underlying tree.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we do believe that there is an underlying tree to describe the data, then we could simply model the data.",
                    "label": 0
                },
                {
                    "sent": "Under the tree using a tree structure model.",
                    "label": 1
                },
                {
                    "sent": "So what this means is that if our data is here, so we have four data points here given by ABCD, and if we do believe that there's a tree underlying tree, then there should be some latent tree up here, which is an observed.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have some latent variables EF&G that lives on the tree, and the probability of the.",
                    "label": 0
                },
                {
                    "sent": "Of the data under the tree is simply the sum over the latent variables of the probability of the roots, and then the multiplied by the probability of each variable given its parent.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then the algorithm is basically as follows.",
                    "label": 0
                },
                {
                    "sent": "We'll start with each data point.",
                    "label": 1
                },
                {
                    "sent": "Data Points XI, in its own subtree.",
                    "label": 1
                },
                {
                    "sent": "And we will find so an we again E trade.",
                    "label": 0
                },
                {
                    "sent": "And in each iteration we take two subtrees and we merge them into a larger tree.",
                    "label": 0
                },
                {
                    "sent": "But the subtree which we pick.",
                    "label": 0
                },
                {
                    "sent": "We want to find a pair of subtrees such that the likelihood of the data is maximum is maximized after the merger, so this is equivalent to finding a pair of subtrees SMT search that this thing is maximized and this is basically the log of ratio here the top here we have the probability of the data under the much subtree.",
                    "label": 1
                },
                {
                    "sent": "And in the bottom again we have a product of the probability of the data under the first subtree and the second subtree.",
                    "label": 1
                },
                {
                    "sent": "Ann again, if this lock probability is positive, then it is more.",
                    "label": 0
                },
                {
                    "sent": "Advantages for us to merge the two subtrees together so we will merge them, otherwise we'll stop.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very similar and it turns out that this probability of the data under the subtree can can also be computed efficiently if in a recursive manner.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with it, it just uses belief propagation.",
                    "label": 0
                },
                {
                    "sent": "OK, so moving and finally so this algorithm here is.",
                    "label": 0
                },
                {
                    "sent": "Again, only finds for you a single tree, right?",
                    "label": 0
                },
                {
                    "sent": "Because you know it, just it.",
                    "label": 0
                },
                {
                    "sent": "It just iterates and it keeps on merging subtree and until and then at the end of the day you just have a single tree.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Describe your data.",
                    "label": 0
                },
                {
                    "sent": "But if you actually want to model uncertainty over trees, then in fact the Bayesian approach is to say that we should use the distribution for trees.",
                    "label": 1
                },
                {
                    "sent": "So which?",
                    "label": 1
                },
                {
                    "sent": "What we want to compute is the posterior distribution of over trees given our data.",
                    "label": 1
                },
                {
                    "sent": "Where this posterior distribution is using base rule here is going to be a product of a prior distribution over trees.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by the probability of the data under the tree.",
                    "label": 0
                },
                {
                    "sent": "And divided by a normalization constant?",
                    "label": 1
                },
                {
                    "sent": "And basically the model for data this P of data given tree is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tree structure model that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have described here.",
                    "label": 0
                },
                {
                    "sent": "It turns out that unfortunately for us basins, the posterior is often intractable and.",
                    "label": 1
                },
                {
                    "sent": "That's different approaches to doing approximate inference under this framework.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could in fact treat this algorithm here as a greedy.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a greedy approximate inference algorithm in which you construct the tree greedily.",
                    "label": 0
                },
                {
                    "sent": "There's also things like you could use Markov chain Monte Carlo, so that's what Chris Williams and revenue uses.",
                    "label": 0
                },
                {
                    "sent": "An as well sequential Monte Carlo algorithms, so this is what we proposed as well.",
                    "label": 1
                },
                {
                    "sent": "The multi color algorithms are more intricate and expensive.",
                    "label": 1
                },
                {
                    "sent": "However they do give you some notion of.",
                    "label": 0
                },
                {
                    "sent": "The posterior distribution of the trees.",
                    "label": 0
                },
                {
                    "sent": "It actually tells you how much the model actually believes in.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "In the trees which.",
                    "label": 0
                },
                {
                    "sent": "Has constructed basically.",
                    "label": 0
                },
                {
                    "sent": "There's also some interesting theoretical work on like.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric Bayesian priors over trees.",
                    "label": 1
                },
                {
                    "sent": "Wish I can have one time running out of time right someone?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but in fact I'm almost finished actually.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, actually I'll just do some simple comparisons on a few datasets.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I here I've compared average linkage, so this is the traditional algorithm versus this.",
                    "label": 0
                },
                {
                    "sent": "This is Helen Ghahramani, so this is that probabilistic.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then this is the greedy Bayesian approach in which we used restructured probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "An we competitive using a few measures of quality of the tree.",
                    "label": 0
                },
                {
                    "sent": "One is called priority, one is called subtree an.",
                    "label": 0
                },
                {
                    "sent": "The last ones leave one out accuracy.",
                    "label": 0
                },
                {
                    "sent": "So on the first data set, it's a very simple data set, so you have.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Something like 10 or 100.",
                    "label": 0
                },
                {
                    "sent": "Examples of handwritten digits between.",
                    "label": 0
                },
                {
                    "sent": "090123456789 and they are.",
                    "label": 0
                },
                {
                    "sent": "Images of handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "An the images are PCA reduced to a low dimensional space and then use we simply use Euclidean distance in the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "As the distance measure OK, or in the case of these two algorithms, use use Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And basically this costs are all between zero and one and there.",
                    "label": 0
                },
                {
                    "sent": "One means good and 0 means bad.",
                    "label": 0
                },
                {
                    "sent": "And you know on on both.",
                    "label": 0
                },
                {
                    "sent": "Both this.",
                    "label": 0
                },
                {
                    "sent": "A datasets Bayesian, the greedy basin algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the best basically.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the priority is the following so.",
                    "label": 0
                },
                {
                    "sent": "Once you've constructed a tree, say something like.",
                    "label": 0
                },
                {
                    "sent": "This OK?",
                    "label": 0
                },
                {
                    "sent": "And let's say that you're.",
                    "label": 0
                },
                {
                    "sent": "You're trying to discriminate between zero and one, so this might be a zero.",
                    "label": 0
                },
                {
                    "sent": "That's a 0.",
                    "label": 0
                },
                {
                    "sent": "And then there's a 0.",
                    "label": 0
                },
                {
                    "sent": "And this one OK.",
                    "label": 0
                },
                {
                    "sent": "The purity is the.",
                    "label": 0
                },
                {
                    "sent": "Is the probability that if you pick.",
                    "label": 0
                },
                {
                    "sent": "Two images of the same class.",
                    "label": 0
                },
                {
                    "sent": "That they would belong to the same subtree.",
                    "label": 0
                },
                {
                    "sent": "So if this is like.",
                    "label": 0
                },
                {
                    "sent": "0 zero and one one right so?",
                    "label": 0
                },
                {
                    "sent": "Opath of images of the same class would either be this or that right and they would belong to the same subtree.",
                    "label": 0
                },
                {
                    "sent": "So then the Paris.",
                    "label": 0
                },
                {
                    "sent": "The priority score is considered high, but if the street that you've constructed looks like 0101, which intuitively kind of is bad, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's not really.",
                    "label": 0
                },
                {
                    "sent": "Giving you a very discriminative.",
                    "label": 0
                },
                {
                    "sent": "Tree in terms of the class in terms of classification.",
                    "label": 0
                },
                {
                    "sent": "Then if you pick a pair of zeros.",
                    "label": 0
                },
                {
                    "sent": "Then the subtree that.",
                    "label": 0
                },
                {
                    "sent": "Is this that kind of contains the smaller subtree which contains this pair of 0?",
                    "label": 0
                },
                {
                    "sent": "Is the whole tree itself and this smaller subtree is not pure right?",
                    "label": 0
                },
                {
                    "sent": "So if you average over this, then the priority is in fact zero.",
                    "label": 0
                },
                {
                    "sent": "In this case an is.",
                    "label": 0
                },
                {
                    "sent": "One in that case.",
                    "label": 0
                },
                {
                    "sent": "The subtree score is basically.",
                    "label": 0
                },
                {
                    "sent": "It is related to the number of pure subtrees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In this case, the purity score is the subtree scores again one.",
                    "label": 0
                },
                {
                    "sent": "Out.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "The entropy.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the entropy.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure what you mean by entropy hearing actually, so I'll tell you what the subtree score is so.",
                    "label": 0
                },
                {
                    "sent": "I need to draw it for you.",
                    "label": 0
                },
                {
                    "sent": "A more complicated tree.",
                    "label": 0
                },
                {
                    "sent": "In fact it's going to look like this.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have a.",
                    "label": 0
                },
                {
                    "sent": "0011 OK, so all the leaves under here are zeros.",
                    "label": 0
                },
                {
                    "sent": "All the lifts under here one or the listeners here zero and all their lives under here, one the subtree score in this case is.",
                    "label": 0
                },
                {
                    "sent": "I think is.",
                    "label": 0
                },
                {
                    "sent": ".5 I think.",
                    "label": 0
                },
                {
                    "sent": "Because the number of pure subtrees here is, this one is full right?",
                    "label": 0
                },
                {
                    "sent": "So this is a pure subtree, another subtree, another person, another peer subtree basically pure in the sense that the all the leaves in that subtree have the same label.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "So if you have two classes than the minimum number of peers subtrees is 2.",
                    "label": 0
                },
                {
                    "sent": "So if your tree managed to split off.",
                    "label": 0
                },
                {
                    "sent": "The two classes.",
                    "label": 0
                },
                {
                    "sent": "How very well then you would.",
                    "label": 0
                },
                {
                    "sent": "It would put all the zeros here.",
                    "label": 0
                },
                {
                    "sent": "An older ones here.",
                    "label": 0
                },
                {
                    "sent": "Right then the subtree score is simply one.",
                    "label": 0
                },
                {
                    "sent": "Discount defined linearly related to the number of years of trees, basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another data set, so this is.",
                    "label": 0
                },
                {
                    "sent": "From a fellow linguistics, so this is wall stands for World Atlas of Language Structures I believe.",
                    "label": 0
                },
                {
                    "sent": "And here what we what we do is we construct a tree A to describe the languages.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We classify each language in terms of which genus it belongs to, which language family belongs to.",
                    "label": 0
                },
                {
                    "sent": "Basically and again we use the same scores, and again the kind of degree D basin approach does pretty well except for this particular case, in which somehow average linkage did really well.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually it's kind of interesting to look at the trees constructed by the Bayesian.",
                    "label": 0
                },
                {
                    "sent": "The greedy Bayesian algorithm, so this is on the Indo European languages and it actually does pretty well so.",
                    "label": 0
                },
                {
                    "sent": "Language names are given on this site and this is the language family name.",
                    "label": 0
                },
                {
                    "sent": "And you can see that it is basically able to figure out that all the Germanic languages should go together.",
                    "label": 0
                },
                {
                    "sent": "All the Romance languages should go together, and then the Celtic ones are up here.",
                    "label": 0
                },
                {
                    "sent": "The Slavic ones are here.",
                    "label": 0
                },
                {
                    "sent": "In fact, Slavian, but it got a bit confused, but then slowly and body language is is, I believe, quite closely related.",
                    "label": 0
                },
                {
                    "sent": "Here it has the Iranian and Indian languages as well.",
                    "label": 0
                },
                {
                    "sent": "The information things like.",
                    "label": 0
                },
                {
                    "sent": "What's called.",
                    "label": 0
                },
                {
                    "sent": "So somebody actually, some linguists actually.",
                    "label": 0
                },
                {
                    "sent": "Went off an and collected quite a large database of linguistic features so the linguistic features are things like whether the noun comes whether the subject comes before the verb, whether the object comes after the verb and stuff, and this is whole bunch of binary features that describe the grammatical structure in the language, and we basically represent each language as a binary vector.",
                    "label": 0
                },
                {
                    "sent": "We can zoom.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you can zoom in a little bit here to the Germany Ann and romance ones and it's basically got it right.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Basically it's saying that the Germany languages are very different than the Romance languages because there's a split earlier on here, right?",
                    "label": 0
                },
                {
                    "sent": "And then within the Germanic languages then we see that the Scandinavian languages are grouped together.",
                    "label": 0
                },
                {
                    "sent": "Dutch and German are quite close to each other.",
                    "label": 0
                },
                {
                    "sent": "Anne, the English and Icelandic are quite different than.",
                    "label": 0
                },
                {
                    "sent": "Either the jump, either German and Dutch, or the Scandinavian languages.",
                    "label": 0
                },
                {
                    "sent": "In on the Roman side, it believes that Italian and Catalan are very similar, which I think is also correct.",
                    "label": 0
                },
                {
                    "sent": "And Portuguese and French are more closely related to each other than they are to Spanish.",
                    "label": 0
                },
                {
                    "sent": "Which seems to be pretty, is doing reasonably well basically.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the Bayesian approaches does again some pros and cons, so the it's kind of reversed from the from the linkage algorithms, so it can be very expensive if you actually want to compute the full posterior distribution over trees, you need to use a lot of MCMC algorithm sampling algorithms and stuff.",
                    "label": 0
                },
                {
                    "sent": "It's less common.",
                    "label": 0
                },
                {
                    "sent": "An less well understood, so if you show somebody the tree and you say that, oh, I'm using.",
                    "label": 0
                },
                {
                    "sent": "Directly diffusion trees.",
                    "label": 0
                },
                {
                    "sent": "There will be like what right?",
                    "label": 0
                },
                {
                    "sent": "So you need to put more effort if you're actually going to.",
                    "label": 0
                },
                {
                    "sent": "I use this in some applications.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately that they have positive sites.",
                    "label": 0
                },
                {
                    "sent": "They have advantages as well.",
                    "label": 0
                },
                {
                    "sent": "They have positive things, so they in fact give fully generative probabilistic models and their deals nicely with partially observed data.",
                    "label": 1
                },
                {
                    "sent": "And there is coherent measure of whether the trees are.",
                    "label": 0
                },
                {
                    "sent": "Of basically there's a measure of goodness of fit for the trees that you've obtained an they could give you uncertain T over the tree structures which you.",
                    "label": 0
                },
                {
                    "sent": "I obtained from your data.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just to summarize, basically, so I've given a kind of idiosyncratic overview of hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "And there's a few dimensions.",
                    "label": 0
                },
                {
                    "sent": "That I've described first one is top down versus bottom up versus Monte Carlo search, so these are ways in which you could construct your tree.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The dominant approach is bottom up because it seems to have a much better search landscape than the top down ones, and multicolor searches of this quite expensive basically.",
                    "label": 0
                },
                {
                    "sent": "The second dimension here is in terms of flat clustering, so mixture models.",
                    "label": 0
                },
                {
                    "sent": "So what is the underlying model which you use?",
                    "label": 0
                },
                {
                    "sent": "In your algorithm, so it's mixture models versus tree structured models.",
                    "label": 0
                },
                {
                    "sent": "So you should use mixture models if you don't believe that your data is hierarchical, but you just want to use hierarchical clustering as a way of finding.",
                    "label": 0
                },
                {
                    "sent": "A good clustering of your data.",
                    "label": 0
                },
                {
                    "sent": "If your data is in fact restructured, in fact has this hierarchical structure like in the following linguistics case or the OR with phylogenetic data, then you should in fact used restructured models.",
                    "label": 0
                },
                {
                    "sent": "To describe them, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess you.",
                    "label": 0
                },
                {
                    "sent": "I guess you could.",
                    "label": 0
                },
                {
                    "sent": "It's just this is it just doesn't quite correspond to.",
                    "label": 0
                },
                {
                    "sent": "Your model which you use should correspond to what you believe about the data, and if you believe that the data is simply clustered into different clusters and there's no relationships between the clusters that you should in fact use something which is based on a mixture model.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "But of course, you could take one of the Bayesian algorithms to produce where you are clustering.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's fine, but you don't really have things like you know what is meant by.",
                    "label": 0
                },
                {
                    "sent": "A good tree anymore basically right?",
                    "label": 0
                },
                {
                    "sent": "Because your notion of a good tree in your case should be a good tree is good if it produces good clusters.",
                    "label": 0
                },
                {
                    "sent": "But under the model, a good tree is 1, which kind of describes the data the best under tree structure model, which is different.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It is possible, it's just.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it it, it is possible, is just that what the model is trying to optimize an what.",
                    "label": 0
                },
                {
                    "sent": "Well, what you?",
                    "label": 0
                },
                {
                    "sent": "Want to what you consider is good trees is they are not quite the same thing basically.",
                    "label": 0
                },
                {
                    "sent": "The third dimension is basically, you know you have less space and once and more Bayesian ones.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so that's it basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I believe.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Not all the features were observed, so it's a very sparsely observed data set and I believe what we do is.",
                    "label": 0
                },
                {
                    "sent": "If you have a feature for language which you don't observe, then you simply set that.",
                    "label": 0
                },
                {
                    "sent": "Feature to become the mean of the feature among all languages for which that features observed.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of like, you know, if you have partially observed data.",
                    "label": 0
                },
                {
                    "sent": "That's just kind of no good way of assigning.",
                    "label": 0
                },
                {
                    "sent": "Distances.",
                    "label": 0
                },
                {
                    "sent": "Between the vectors basically so and that's what we do.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "I used angle brackets to mean that the merger of these two subtrees.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "Two subtrees say.",
                    "label": 0
                },
                {
                    "sent": "Once a piece here, it might look like this.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'm another one is here.",
                    "label": 0
                },
                {
                    "sent": "So you have you have treated points here X one X2X3 and then this is X412X567.",
                    "label": 0
                },
                {
                    "sent": "So in the bottom line we have the probability of X1 until three given this.",
                    "label": 0
                },
                {
                    "sent": "Let's call this S&ST given S. Right where this probability is given by.",
                    "label": 0
                },
                {
                    "sent": "That thing OK?",
                    "label": 0
                },
                {
                    "sent": "And over here we have probability of X4 until 7.",
                    "label": 0
                },
                {
                    "sent": "Given T. So those are the two bottom terms.",
                    "label": 0
                },
                {
                    "sent": "And in the top 10, here is the probability of.",
                    "label": 0
                },
                {
                    "sent": "X1.",
                    "label": 0
                },
                {
                    "sent": "Until X7, so all the data points here and the subtree for which you have much to do.",
                    "label": 0
                },
                {
                    "sent": "Basically that's it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Stereo.",
                    "label": 0
                },
                {
                    "sent": "That's in fact probably no way of computing the full posterior.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is a lot of trees out there.",
                    "label": 0
                },
                {
                    "sent": "It would be nice if the world is in fact true.",
                    "label": 0
                },
                {
                    "sent": "It still has lots of trees, but I guess the world doesn't have that many trees anymore, but that's in this basin world.",
                    "label": 0
                },
                {
                    "sent": "There's allot trees and you cannot really compute the full posterior.",
                    "label": 0
                },
                {
                    "sent": "So in fact what people do is too.",
                    "label": 0
                },
                {
                    "sent": "As approximate the posterior using samples from basically.",
                    "label": 0
                },
                {
                    "sent": "And then within the sample you can collect statistics like so for example.",
                    "label": 0
                },
                {
                    "sent": "In this case, right, you might say.",
                    "label": 0
                },
                {
                    "sent": "Maybe English should be somewhat related to French because there's lot of French words in English, so maybe you think that if you actually run your your Bayesian tree algorithm right, it should produce for you lots of trees and then some of them English should be belonging to the Romance languages and but in most of them it should still be Germanic because the grammatical structure of English is still.",
                    "label": 0
                },
                {
                    "sent": "Mostly Germanic.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This is purely grammatic I think yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it would probably give you a difference answers if you use phonetic structure I think.",
                    "label": 0
                },
                {
                    "sent": "Yes, in fact, that's one of the interesting things about this data set is because most people who have done a follow linguistics used.",
                    "label": 0
                },
                {
                    "sent": "Not not fanatic.",
                    "label": 0
                },
                {
                    "sent": "God.",
                    "label": 0
                },
                {
                    "sent": "Ask what again is along with.",
                    "label": 0
                },
                {
                    "sent": "Clock.",
                    "label": 0
                },
                {
                    "sent": "Lotto something is basically they use sets of words for cognate sites sets.",
                    "label": 0
                },
                {
                    "sent": "I think it's called basically sell words, one from each language for which they know from linguists, from historical studies that they had the same roots in an ancient language.",
                    "label": 0
                },
                {
                    "sent": "And then they try to model the.",
                    "label": 0
                },
                {
                    "sent": "Phonologic final logical changes.",
                    "label": 0
                },
                {
                    "sent": "From between the words.",
                    "label": 0
                },
                {
                    "sent": "So the English word for.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                },
                {
                    "sent": "Try to come up with some example.",
                    "label": 0
                },
                {
                    "sent": "I don't know like Constitution and the French word for constitutions somewhat similar, and how similar they are tells you about how far away in the past did English and French diverge basically, and use sets of words like that, so that's the typical approach, but in our approach is quite different.",
                    "label": 0
                },
                {
                    "sent": "We actually use grammatical information rather than all this account sets.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Using their.",
                    "label": 0
                },
                {
                    "sent": "Validating the feature space that you use.",
                    "label": 0
                },
                {
                    "sent": "So what do you mean by that so?",
                    "label": 0
                },
                {
                    "sent": "Backwards.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure about this case, but I'll tell you about what we what we found.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "About in the Agnes case.",
                    "label": 0
                },
                {
                    "sent": "So what I said was that the PCA reduced the images right and in a PCA reduction the first dimensions.",
                    "label": 0
                },
                {
                    "sent": "Of course, the most informative one, followed by the second one and the third one and so forth.",
                    "label": 0
                },
                {
                    "sent": "But because with PCA reduced it, and in fact we whiten it as well.",
                    "label": 0
                },
                {
                    "sent": "The when you look at the vectors, the covariance structure of the of the vectors, there's no information in there.",
                    "label": 0
                },
                {
                    "sent": "That tells you that the first dimension is more important than the 2nd and 3rd, but turns out that you could fit the hyper parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "I have this model.",
                    "label": 0
                },
                {
                    "sent": "To the data and it actually found that indeed that it has settings of the hyperparameters such that the first dimension has the most effect on the construction of the tree.",
                    "label": 0
                },
                {
                    "sent": "I followed by the second one, followed by the third one and so forth.",
                    "label": 0
                },
                {
                    "sent": "So it actually updates the first dimensions and down with the letter dimensions.",
                    "label": 0
                },
                {
                    "sent": "So it somehow figured out that the first few features are more important.",
                    "label": 0
                },
                {
                    "sent": "Then the features at the end.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of it.",
                    "label": 0
                },
                {
                    "sent": "Did manage to do it in that case, but we haven't really looked into the linguistic case.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}