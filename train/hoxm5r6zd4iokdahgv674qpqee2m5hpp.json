{
    "id": "hoxm5r6zd4iokdahgv674qpqee2m5hpp",
    "title": "Language-agnostic Relation Extraction from Wikipedia Abstracts",
    "info": {
        "author": [
            "Heiko Paulheim, Institut f\u00fcr Informatik, University of Mannheim"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_paulheim_wikipedia_abstracts/",
    "segmentation": [
        [
            "OK, thank you very much and this is joint work with the former Masters student of Mine, Nicholas and whatever is cool about this is his merit and whatever is wrong about it is my fault.",
            "Just to put that up front, OK?"
        ],
        [
            "Um, sometimes these knowledge graphs, like Pedia and Vicki data and the like.",
            "They do something which you would not expect them to do because you assume that the stuff you want to look for is in there.",
            "And then sometimes you find out that things are weird.",
            "So for example if you look for all the."
        ],
        [
            "Towns in Germany together with their state, you find that there are 790 something results, but actually there are more towns in Germany in DB Pedia.",
            "So there is roughly 2000.",
            "So the discrepancy comes from there are 1000 is roughly 1200 for Wichita State is missing so it's just not in there.",
            "There may be different reasons for the state missing for retirement Germany, so the value may not be in the infobox.",
            "The town may not have an infobox at all for example or the key.",
            "Used in the infobox is used wrongly or not mapped to the ontology, so there are various various things that may go wrong here in that case.",
            "So the question is how can we fill that gap?",
            "How can we fill in stuff which is not in the info box which is not correctly map for example?"
        ],
        [
            "On the other hand, if you look at the Wikipedia page that the that the data came from, it's usually in theirs, and it's usually in the abstract.",
            "For example, here you see a town in Germany, and then you see there is a sentence in the abstract process.",
            "Is the town in the damn study book district in Hesse in Germany, and there is a certain pattern in here, and the pattern is that if we see the sentence at first mentions the municipality, then it mentions the state and then it mentions the country.",
            "And the interesting thing is that this is a recurring pattern.",
            "So whenever you see articles about talents, this is a very frequent pattern that they use.",
            "This structure first mentioned the municipality, then the state, then the country, and for some of the occurrences of this, patterns devalue the corresponding value for the state.",
            "For example, is already in DB pedia.",
            "For others it's not.",
            "So the idea of this work is if we can discover this pattern from the examples where the information is already in DB pedia.",
            "Apply it to the rest of the abstract about this pattern also occurs, but the information is not yet in DB pedia and then we can fill in the slots from that."
        ],
        [
            "So if you think about that, you will notice that there are quite a few of those patterns, so we already have this town pattern.",
            "If you have a rider or a musician, and there are some genres which are mentioned in the abstract about this writer or musician, it's likely that these are the genres that this writer or musician is associated with.",
            "Or if you have a person and there is a place mentioned, usually the 1st place that is mentioned in an abstract about a person is that person's birthplace.",
            "To 2nd place is usually the death place.",
            "If there is a second place mentioned, so these patterns, they are quite frequent and we want to exploit those parents.",
            "If you look at these patterns, you see that there is some piece of information which they all have in common and that is the type of the entity which is linked.",
            "But this information is something we can already get from DB pedia.",
            "It points to a different page and there's different pages associated with the DB Pedia entity.",
            "And then we already have some information about this entity and this is something we can exploit.",
            "So the idea is discovering these patterns using the information that is already in DB pedia.",
            "So we have examples for pages on which these patterns occur.",
            "We know the corresponding relations and the pedia, and we can use that as a training data to train detector for those patterns and what we apply to also create negative examples.",
            "Of course we cannot assume that everything which is not there is negative, but we can use something which is quite frequently used in such experiments, which is the local closed world assumption.",
            "So if we know that a person already has a birthplace, we assume that this is the only birth place and then all the other places which are mentioned are not the person.",
            "Birthplace, and this is what it's called local closed world assumption, and it's quite frequently used in settings like these to generate negative examples."
        ],
        [
            "So to see whether this approach works at all, we did a small priest pre study where we picked the 10 most frequent relations and DV pedia for each of those we drove 50,000 examples with the corresponding abstracts and then created some features on those and these features.",
            "They are very simple, simplistic features, so we look at the position of an entity.",
            "We look at the type of an entity and then we count how many entities of that type are in the abstract, which is the position of this entity in the abstract.",
            "If you look at all the entities of that type and so on and so forth, and this is the set of training, this is the set of features we used for training and then we experimented with different learning algorithms to see whether we can train such a model.",
            "For that, the out."
        ],
        [
            "Time is that out of these there was one which turned out work quite well.",
            "There was the random forest classifier which also outperforms some simple baselines.",
            "So we tried four different simple baselines, and it outperformed all of them and delivered quite reasonable results.",
            "So we decided to stick with that classifier."
        ],
        [
            "Um?",
            "What we do to make sure that the classifier actually does something useful is we train one classifier per relation, and then we first check does this classifier work actually so we perform some cross validation and only if we exceed a certain precision, we assume that we can use this classifier.",
            "And of course the cross validation will not give us the exact precision and recall because we use some tricks here like the like, the local closed world assumption for example, we may have a bias because we only use.",
            "Abstracts for which the relation is known, and so on and so forth.",
            "So we also double check these precision and recall estimates by taking small samples for five relations and seeing what the actual precision and recall should be in that case, and what we saw is that the recall values they are usually not so trustable."
        ],
        [
            "But that's due to the experiment design, but the good thing is that the precision is usually almost in the range that the that we also estimate automatically.",
            "This means we can trust the precision, although it's a bit overly optimistic, but at least it's usually just two to three percentage points lower than the one which has been estimated.",
            "So what we did then is we roll that out on all of the pedia.",
            "So learning a model for each single relational DB pedia using all the data that is in there?",
            "Or the abstracts that we could get and then to to make this automatically applicable, we check."
        ],
        [
            "The precision of the model as it is estimated and only kept those models which have a precision higher than 95%.",
            "Because of course we want additional information in the in the Knowledge Graph.",
            "But what we don't want is additional noise in the Knowledge Graph, so this was the cut off we took and with that with that constraint we could learn models for almost 100 relations and those produce roughly 1 million additional statements to DB pedia.",
            "So this is what happens if we applied this approach to to DB pedia and.",
            "To the abstracts which we have in Wikipedia.",
            "And then we stepped back and looked again at the features we used and the features we used were solely independent from the language we just looked at.",
            "The position of an entity.",
            "We just looked.",
            "How many entities are there in the abstract?",
            "But we didn't try to pass the language.",
            "We didn't do any POS tagging or the like.",
            "In turn.",
            "This means we can just apply this to any language because we don't make any assumption of the underlying language.",
            "So the second experiment we did is we used the 12 largest language editions of Wikipedia.",
            "At that time, there were some that you would expect, like German.",
            "English, Spanish, French, Italian.",
            "There are also some languages, but we never heard before about which was achieved.",
            "10 and Buckeye.",
            "Obviously some languages spoken in Indonesia, you always learn certain things when you do research right, and so we roll this out to these twelve largest language additions.",
            "At that time we exploited the Inter language links to find the entities and DB pedia.",
            "In the end.",
            "And we could, we could get some boost there.",
            "So before we had 99 models which we could learn with the precision of 95%.",
            "Now we could learn models for 187 relations so that almost doubled and we could learn a total of 1.6 million actions, which was before 1 million when we applied as 95% precision threshold and you see in these graphs, the left hand side shows the number of models which we learn.",
            "So the number of relations for which we can extract the right hand side shows the growth in the number of actions that we get and the right the red line above is the accumulated numbers so.",
            "If we go from one language to two languages to three languages, how much does it add up?",
            "And you see that pretty much after 12 language there is a saturation, so we could go onto taking the 1521 hundred most frequent languages.",
            "But actually you see that the line already saturates there.",
            "What we also observed is that the number of statements we can create for a language is strongly correlated with the number of interlanguage links to English, which is what we what we used to identify the entities and DV pedia so."
        ],
        [
            "We can assume that these approaches also work in most languages, and the crucial part we need as these Inter language links and whenever they are more Inter language links, the approach works better if there are less Inter language links.",
            "The approach works worth.",
            "So what we were interested in next is finding out is there any?",
            "Is there any notable difference with respect to what is extracted from these different language extractions?",
            "So are there may be different relation types we can extract?",
            "Can we extract information about places better from one language and information about persons better from another?",
            "It turns out there is not so much difference there so the most the most information we get is information about places that the blue bars in the right hand side diagram and then from English we also get a significant amount of information about persons and musical works.",
            "These are rather underrepresented in the other languages.",
            "For the remaining languages, what we get is mainly geographic information.",
            "That being said, we wanted to know is there any?",
            "Is there any difference in with respect to the entities as such?",
            "So something you could assume is if we go for the German DB Pedia.",
            "We will mainly find information about German places if you go to the French mommy mainly find information about places in France and their overseas departments.",
            "So this is something we analyzed next."
        ],
        [
            "So for each of the entities for which we could find out in DB pedia which country they are in, we try to estimate the distribution for those entities.",
            "So is there any?",
            "Is there any pattern there?",
            "So the first thing we see from this diagram, so the each line and there is the one language addition and then the colored bars are the different countries.",
            "So the first observation we make is also from the other languages.",
            "Surprisingly most of this stuff is about things in the UK in the US right?",
            "Also most US the blue bars on the left hand side is US.",
            "Nevertheless, if you look in the in the second and third positions, you find these patterns so the second line shows Swedish and the red bar, which is the second largest bar.",
            "Is Sweden.",
            "Then for Germany there is for German there is Germany, the light blue bar and also Switzerland, the Orange bar and you see these patterns for Polish.",
            "It's pretty, it's pretty dominant.",
            "So for Polish Poland is really the most common source of origin of the subjects for which we extract information."
        ],
        [
            "So what we've seen is if we if you take this approach where we don't make any assumptions from the language using solely positional features and types existing in the knowledge graph, and nothing language specific, we can develop an approach that allows us to extract relations from Wikipedia.",
            "Abstracts in any sort of language.",
            "This was for the moment it was a positive result.",
            "What we did is we train some random forest classifier.",
            "We use the existing relations and types in the Knowledge Graph.",
            "Can be used to local closed world assumption to create negative examples and what we get is roughly 1.6 million additional relations in DB pedia at present.",
            "Estimated precision of 95% and we got them from 12 languages."
        ],
        [
            "Thing we could do in future work.",
            "So for the moment we looked only at relations between entities, but you could also recognize dates and numbers in abstracts.",
            "That's fairly easy.",
            "There are named entity recognition engines that do that for you, and there are probably also some patterns that you could exploit there.",
            "So for example, if you find a date in an article about a person, the first state you find is probably that person's birth date or the first number in an article about a country is probably the population of that country, and so on.",
            "So finding numbers and dates and then.",
            "Applying the same approach to those is probably also also an interesting way to go to also expand this to literals.",
            "But you could also do is using using this for not extending the Knowledge Graph, but day after addressing its dual, which is checking whether stuff in the Knowledge graph is correct or not.",
            "So if you find additional evidence for a statement in the abstract, we can trust that statement more than if it's solely in the infobox but not in the abstract.",
            "And finally, we can also extend this to other sources.",
            "We saw that Wikipedia abstracts.",
            "They follow certain patterns that they're probably not the type of text sources.",
            "They are not the only type of text sources that follow those patterns, so if you find text sources that expose so those parents, we could process them, for example with something like PDF Spotlight and then apply the same approach there.",
            "Or we could also apply it on other wikis.",
            "We have this poster presentation last night on DB quick where we extracted from different wikis and extracted a knowledge graph from there, and we observe that also in those Vicky's which are not Wikipedia wikis but very different wikis.",
            "Certain patterns of the kind I just discussed.",
            "Also exist there, so there is many opportunities of applying this approach and this also closes my talk and I'm happy to take questions now."
        ],
        [
            "Thank you.",
            "I was wondering you you actually something you mentioned on your last slide?",
            "How much do you think this generalizes beyond a genre of writing definitions or having an abstract of an encyclopedia?",
            "And at the same time, how much do you think this generalizes beyond?",
            "Let's call it Western languages.",
            "You know, in your Germanic, Roman languages, etc.",
            "OK, so let's start with the second one first, as some of these languages are actually not.",
            "Into Germanic languages, so we had Russian in there.",
            "We had those Indonesian language is in there, so they are fairly different.",
            "And since we learn a model which is specific to us to a language, I think we can capture different patterns.",
            "So the models we learn will not be the same.",
            "But I think there are also parents in other languages which are totally different from European language like Asian or Arabic languages.",
            "As for the first question, I think there are quite a few text sources out there which expose patterns, so take for example these.",
            "These news issued by companies about stock developments and stuff like that.",
            "So whenever you see something like that, there's some text which follows strict strict patterns or less strict but mostly obvious patterns you can.",
            "You can apply the approach there.",
            "This is what I assume.",
            "So as soon as you identify the entities and as you identify there is a certain pattern in there, then you probably can apply the approach.",
            "I was wondering if you had some problems, country specific information for the.",
            "Geographic location for example and Germany have lender and French of Department is kind of things.",
            "Oh as long as it's reflected in DB pedia, we can find this information and actually these things also country specific information.",
            "There are properties in the PDF that reflect that.",
            "So there are properties for departments in France and so on and so we can also discover these relations and then you only discover them there.",
            "But sometimes you get some noise of course, because then the extractor things that they should also be departments in the US and then extract some noisy stuff.",
            "So this is rather a problem with false positives than in the end.",
            "Thank you for your talk.",
            "What are the inputs of your classifier?",
            "The inputs are the abstracts.",
            "On these features we extracted from the abstract, so there was this.",
            "There was this list of features.",
            "It's basically a fairly fairly simple list, so we look at what are the candidates or candidates are.",
            "Everything that has a matching type.",
            "So for example, if you're looking for a birthplace then that's all the places we look at.",
            "All the entities, regardless of their type.",
            "And then we look at where are they positioned in the sentence in the abstract and where is the where is the sentence position in the abstract?",
            "Is there a link back from the?",
            "The linked entity to the one we have at hand, and then we look at the vector of the instances types and this is the features that the classifier gets.",
            "So you say you you build models which are language specific.",
            "Have you looked at how much the patterns overlap between languages and how much they differ?",
            "But also if you are using other sources, can you have you look at whether the patterns are Wikipedia specific or if they actually come back in other sources of writing?",
            "OK, we didn't do the 2nd, so we didn't look beyond Wikipedia.",
            "But we in fact look at the similarity of patterns, and we found that for some relations there are there are patterns which are universal.",
            "So for example, if musicians have genre and every genre in the abstract, follow musician is is a general of that musician.",
            "That's a universal pattern that occurs in every language and for other other relations, the patterns in which people describe this in different languages differ.",
            "But there are some relations for which you also can find universal patterns and what will be interesting if we find such a universal pattern is down to apply.",
            "It also to other languages.",
            "In which we couldn't find a pattern in the first place.",
            "Very quick technical question, what do you exactly mean by an abstract?",
            "Is it the first prog paragraph, barriers of language or does it change it according to the different languages?",
            "It's the text that stands before the 1st headline comes, so in Wikipedia, when the first headline starts, then the abstract nouns.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much and this is joint work with the former Masters student of Mine, Nicholas and whatever is cool about this is his merit and whatever is wrong about it is my fault.",
                    "label": 0
                },
                {
                    "sent": "Just to put that up front, OK?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, sometimes these knowledge graphs, like Pedia and Vicki data and the like.",
                    "label": 0
                },
                {
                    "sent": "They do something which you would not expect them to do because you assume that the stuff you want to look for is in there.",
                    "label": 0
                },
                {
                    "sent": "And then sometimes you find out that things are weird.",
                    "label": 0
                },
                {
                    "sent": "So for example if you look for all the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Towns in Germany together with their state, you find that there are 790 something results, but actually there are more towns in Germany in DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "So there is roughly 2000.",
                    "label": 0
                },
                {
                    "sent": "So the discrepancy comes from there are 1000 is roughly 1200 for Wichita State is missing so it's just not in there.",
                    "label": 0
                },
                {
                    "sent": "There may be different reasons for the state missing for retirement Germany, so the value may not be in the infobox.",
                    "label": 0
                },
                {
                    "sent": "The town may not have an infobox at all for example or the key.",
                    "label": 1
                },
                {
                    "sent": "Used in the infobox is used wrongly or not mapped to the ontology, so there are various various things that may go wrong here in that case.",
                    "label": 0
                },
                {
                    "sent": "So the question is how can we fill that gap?",
                    "label": 0
                },
                {
                    "sent": "How can we fill in stuff which is not in the info box which is not correctly map for example?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, if you look at the Wikipedia page that the that the data came from, it's usually in theirs, and it's usually in the abstract.",
                    "label": 1
                },
                {
                    "sent": "For example, here you see a town in Germany, and then you see there is a sentence in the abstract process.",
                    "label": 0
                },
                {
                    "sent": "Is the town in the damn study book district in Hesse in Germany, and there is a certain pattern in here, and the pattern is that if we see the sentence at first mentions the municipality, then it mentions the state and then it mentions the country.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that this is a recurring pattern.",
                    "label": 0
                },
                {
                    "sent": "So whenever you see articles about talents, this is a very frequent pattern that they use.",
                    "label": 0
                },
                {
                    "sent": "This structure first mentioned the municipality, then the state, then the country, and for some of the occurrences of this, patterns devalue the corresponding value for the state.",
                    "label": 0
                },
                {
                    "sent": "For example, is already in DB pedia.",
                    "label": 1
                },
                {
                    "sent": "For others it's not.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this work is if we can discover this pattern from the examples where the information is already in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Apply it to the rest of the abstract about this pattern also occurs, but the information is not yet in DB pedia and then we can fill in the slots from that.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you think about that, you will notice that there are quite a few of those patterns, so we already have this town pattern.",
                    "label": 0
                },
                {
                    "sent": "If you have a rider or a musician, and there are some genres which are mentioned in the abstract about this writer or musician, it's likely that these are the genres that this writer or musician is associated with.",
                    "label": 0
                },
                {
                    "sent": "Or if you have a person and there is a place mentioned, usually the 1st place that is mentioned in an abstract about a person is that person's birthplace.",
                    "label": 1
                },
                {
                    "sent": "To 2nd place is usually the death place.",
                    "label": 0
                },
                {
                    "sent": "If there is a second place mentioned, so these patterns, they are quite frequent and we want to exploit those parents.",
                    "label": 0
                },
                {
                    "sent": "If you look at these patterns, you see that there is some piece of information which they all have in common and that is the type of the entity which is linked.",
                    "label": 0
                },
                {
                    "sent": "But this information is something we can already get from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "It points to a different page and there's different pages associated with the DB Pedia entity.",
                    "label": 0
                },
                {
                    "sent": "And then we already have some information about this entity and this is something we can exploit.",
                    "label": 0
                },
                {
                    "sent": "So the idea is discovering these patterns using the information that is already in DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So we have examples for pages on which these patterns occur.",
                    "label": 1
                },
                {
                    "sent": "We know the corresponding relations and the pedia, and we can use that as a training data to train detector for those patterns and what we apply to also create negative examples.",
                    "label": 0
                },
                {
                    "sent": "Of course we cannot assume that everything which is not there is negative, but we can use something which is quite frequently used in such experiments, which is the local closed world assumption.",
                    "label": 1
                },
                {
                    "sent": "So if we know that a person already has a birthplace, we assume that this is the only birth place and then all the other places which are mentioned are not the person.",
                    "label": 0
                },
                {
                    "sent": "Birthplace, and this is what it's called local closed world assumption, and it's quite frequently used in settings like these to generate negative examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to see whether this approach works at all, we did a small priest pre study where we picked the 10 most frequent relations and DV pedia for each of those we drove 50,000 examples with the corresponding abstracts and then created some features on those and these features.",
                    "label": 1
                },
                {
                    "sent": "They are very simple, simplistic features, so we look at the position of an entity.",
                    "label": 1
                },
                {
                    "sent": "We look at the type of an entity and then we count how many entities of that type are in the abstract, which is the position of this entity in the abstract.",
                    "label": 0
                },
                {
                    "sent": "If you look at all the entities of that type and so on and so forth, and this is the set of training, this is the set of features we used for training and then we experimented with different learning algorithms to see whether we can train such a model.",
                    "label": 0
                },
                {
                    "sent": "For that, the out.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time is that out of these there was one which turned out work quite well.",
                    "label": 0
                },
                {
                    "sent": "There was the random forest classifier which also outperforms some simple baselines.",
                    "label": 1
                },
                {
                    "sent": "So we tried four different simple baselines, and it outperformed all of them and delivered quite reasonable results.",
                    "label": 0
                },
                {
                    "sent": "So we decided to stick with that classifier.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What we do to make sure that the classifier actually does something useful is we train one classifier per relation, and then we first check does this classifier work actually so we perform some cross validation and only if we exceed a certain precision, we assume that we can use this classifier.",
                    "label": 0
                },
                {
                    "sent": "And of course the cross validation will not give us the exact precision and recall because we use some tricks here like the like, the local closed world assumption for example, we may have a bias because we only use.",
                    "label": 0
                },
                {
                    "sent": "Abstracts for which the relation is known, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So we also double check these precision and recall estimates by taking small samples for five relations and seeing what the actual precision and recall should be in that case, and what we saw is that the recall values they are usually not so trustable.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's due to the experiment design, but the good thing is that the precision is usually almost in the range that the that we also estimate automatically.",
                    "label": 0
                },
                {
                    "sent": "This means we can trust the precision, although it's a bit overly optimistic, but at least it's usually just two to three percentage points lower than the one which has been estimated.",
                    "label": 0
                },
                {
                    "sent": "So what we did then is we roll that out on all of the pedia.",
                    "label": 0
                },
                {
                    "sent": "So learning a model for each single relational DB pedia using all the data that is in there?",
                    "label": 0
                },
                {
                    "sent": "Or the abstracts that we could get and then to to make this automatically applicable, we check.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The precision of the model as it is estimated and only kept those models which have a precision higher than 95%.",
                    "label": 0
                },
                {
                    "sent": "Because of course we want additional information in the in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "But what we don't want is additional noise in the Knowledge Graph, so this was the cut off we took and with that with that constraint we could learn models for almost 100 relations and those produce roughly 1 million additional statements to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens if we applied this approach to to DB pedia and.",
                    "label": 0
                },
                {
                    "sent": "To the abstracts which we have in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "And then we stepped back and looked again at the features we used and the features we used were solely independent from the language we just looked at.",
                    "label": 0
                },
                {
                    "sent": "The position of an entity.",
                    "label": 0
                },
                {
                    "sent": "We just looked.",
                    "label": 0
                },
                {
                    "sent": "How many entities are there in the abstract?",
                    "label": 0
                },
                {
                    "sent": "But we didn't try to pass the language.",
                    "label": 0
                },
                {
                    "sent": "We didn't do any POS tagging or the like.",
                    "label": 0
                },
                {
                    "sent": "In turn.",
                    "label": 0
                },
                {
                    "sent": "This means we can just apply this to any language because we don't make any assumption of the underlying language.",
                    "label": 1
                },
                {
                    "sent": "So the second experiment we did is we used the 12 largest language editions of Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "At that time, there were some that you would expect, like German.",
                    "label": 0
                },
                {
                    "sent": "English, Spanish, French, Italian.",
                    "label": 0
                },
                {
                    "sent": "There are also some languages, but we never heard before about which was achieved.",
                    "label": 0
                },
                {
                    "sent": "10 and Buckeye.",
                    "label": 1
                },
                {
                    "sent": "Obviously some languages spoken in Indonesia, you always learn certain things when you do research right, and so we roll this out to these twelve largest language additions.",
                    "label": 0
                },
                {
                    "sent": "At that time we exploited the Inter language links to find the entities and DB pedia.",
                    "label": 0
                },
                {
                    "sent": "In the end.",
                    "label": 0
                },
                {
                    "sent": "And we could, we could get some boost there.",
                    "label": 0
                },
                {
                    "sent": "So before we had 99 models which we could learn with the precision of 95%.",
                    "label": 0
                },
                {
                    "sent": "Now we could learn models for 187 relations so that almost doubled and we could learn a total of 1.6 million actions, which was before 1 million when we applied as 95% precision threshold and you see in these graphs, the left hand side shows the number of models which we learn.",
                    "label": 0
                },
                {
                    "sent": "So the number of relations for which we can extract the right hand side shows the growth in the number of actions that we get and the right the red line above is the accumulated numbers so.",
                    "label": 0
                },
                {
                    "sent": "If we go from one language to two languages to three languages, how much does it add up?",
                    "label": 0
                },
                {
                    "sent": "And you see that pretty much after 12 language there is a saturation, so we could go onto taking the 1521 hundred most frequent languages.",
                    "label": 0
                },
                {
                    "sent": "But actually you see that the line already saturates there.",
                    "label": 0
                },
                {
                    "sent": "What we also observed is that the number of statements we can create for a language is strongly correlated with the number of interlanguage links to English, which is what we what we used to identify the entities and DV pedia so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can assume that these approaches also work in most languages, and the crucial part we need as these Inter language links and whenever they are more Inter language links, the approach works better if there are less Inter language links.",
                    "label": 0
                },
                {
                    "sent": "The approach works worth.",
                    "label": 0
                },
                {
                    "sent": "So what we were interested in next is finding out is there any?",
                    "label": 0
                },
                {
                    "sent": "Is there any notable difference with respect to what is extracted from these different language extractions?",
                    "label": 0
                },
                {
                    "sent": "So are there may be different relation types we can extract?",
                    "label": 0
                },
                {
                    "sent": "Can we extract information about places better from one language and information about persons better from another?",
                    "label": 0
                },
                {
                    "sent": "It turns out there is not so much difference there so the most the most information we get is information about places that the blue bars in the right hand side diagram and then from English we also get a significant amount of information about persons and musical works.",
                    "label": 1
                },
                {
                    "sent": "These are rather underrepresented in the other languages.",
                    "label": 1
                },
                {
                    "sent": "For the remaining languages, what we get is mainly geographic information.",
                    "label": 0
                },
                {
                    "sent": "That being said, we wanted to know is there any?",
                    "label": 0
                },
                {
                    "sent": "Is there any difference in with respect to the entities as such?",
                    "label": 0
                },
                {
                    "sent": "So something you could assume is if we go for the German DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "We will mainly find information about German places if you go to the French mommy mainly find information about places in France and their overseas departments.",
                    "label": 0
                },
                {
                    "sent": "So this is something we analyzed next.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for each of the entities for which we could find out in DB pedia which country they are in, we try to estimate the distribution for those entities.",
                    "label": 1
                },
                {
                    "sent": "So is there any?",
                    "label": 1
                },
                {
                    "sent": "Is there any pattern there?",
                    "label": 0
                },
                {
                    "sent": "So the first thing we see from this diagram, so the each line and there is the one language addition and then the colored bars are the different countries.",
                    "label": 0
                },
                {
                    "sent": "So the first observation we make is also from the other languages.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly most of this stuff is about things in the UK in the US right?",
                    "label": 0
                },
                {
                    "sent": "Also most US the blue bars on the left hand side is US.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, if you look in the in the second and third positions, you find these patterns so the second line shows Swedish and the red bar, which is the second largest bar.",
                    "label": 0
                },
                {
                    "sent": "Is Sweden.",
                    "label": 0
                },
                {
                    "sent": "Then for Germany there is for German there is Germany, the light blue bar and also Switzerland, the Orange bar and you see these patterns for Polish.",
                    "label": 0
                },
                {
                    "sent": "It's pretty, it's pretty dominant.",
                    "label": 0
                },
                {
                    "sent": "So for Polish Poland is really the most common source of origin of the subjects for which we extract information.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we've seen is if we if you take this approach where we don't make any assumptions from the language using solely positional features and types existing in the knowledge graph, and nothing language specific, we can develop an approach that allows us to extract relations from Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "Abstracts in any sort of language.",
                    "label": 0
                },
                {
                    "sent": "This was for the moment it was a positive result.",
                    "label": 0
                },
                {
                    "sent": "What we did is we train some random forest classifier.",
                    "label": 1
                },
                {
                    "sent": "We use the existing relations and types in the Knowledge Graph.",
                    "label": 1
                },
                {
                    "sent": "Can be used to local closed world assumption to create negative examples and what we get is roughly 1.6 million additional relations in DB pedia at present.",
                    "label": 0
                },
                {
                    "sent": "Estimated precision of 95% and we got them from 12 languages.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thing we could do in future work.",
                    "label": 1
                },
                {
                    "sent": "So for the moment we looked only at relations between entities, but you could also recognize dates and numbers in abstracts.",
                    "label": 1
                },
                {
                    "sent": "That's fairly easy.",
                    "label": 0
                },
                {
                    "sent": "There are named entity recognition engines that do that for you, and there are probably also some patterns that you could exploit there.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you find a date in an article about a person, the first state you find is probably that person's birth date or the first number in an article about a country is probably the population of that country, and so on.",
                    "label": 0
                },
                {
                    "sent": "So finding numbers and dates and then.",
                    "label": 0
                },
                {
                    "sent": "Applying the same approach to those is probably also also an interesting way to go to also expand this to literals.",
                    "label": 0
                },
                {
                    "sent": "But you could also do is using using this for not extending the Knowledge Graph, but day after addressing its dual, which is checking whether stuff in the Knowledge graph is correct or not.",
                    "label": 1
                },
                {
                    "sent": "So if you find additional evidence for a statement in the abstract, we can trust that statement more than if it's solely in the infobox but not in the abstract.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can also extend this to other sources.",
                    "label": 1
                },
                {
                    "sent": "We saw that Wikipedia abstracts.",
                    "label": 0
                },
                {
                    "sent": "They follow certain patterns that they're probably not the type of text sources.",
                    "label": 0
                },
                {
                    "sent": "They are not the only type of text sources that follow those patterns, so if you find text sources that expose so those parents, we could process them, for example with something like PDF Spotlight and then apply the same approach there.",
                    "label": 0
                },
                {
                    "sent": "Or we could also apply it on other wikis.",
                    "label": 0
                },
                {
                    "sent": "We have this poster presentation last night on DB quick where we extracted from different wikis and extracted a knowledge graph from there, and we observe that also in those Vicky's which are not Wikipedia wikis but very different wikis.",
                    "label": 0
                },
                {
                    "sent": "Certain patterns of the kind I just discussed.",
                    "label": 0
                },
                {
                    "sent": "Also exist there, so there is many opportunities of applying this approach and this also closes my talk and I'm happy to take questions now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "I was wondering you you actually something you mentioned on your last slide?",
                    "label": 0
                },
                {
                    "sent": "How much do you think this generalizes beyond a genre of writing definitions or having an abstract of an encyclopedia?",
                    "label": 0
                },
                {
                    "sent": "And at the same time, how much do you think this generalizes beyond?",
                    "label": 0
                },
                {
                    "sent": "Let's call it Western languages.",
                    "label": 0
                },
                {
                    "sent": "You know, in your Germanic, Roman languages, etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's start with the second one first, as some of these languages are actually not.",
                    "label": 0
                },
                {
                    "sent": "Into Germanic languages, so we had Russian in there.",
                    "label": 0
                },
                {
                    "sent": "We had those Indonesian language is in there, so they are fairly different.",
                    "label": 0
                },
                {
                    "sent": "And since we learn a model which is specific to us to a language, I think we can capture different patterns.",
                    "label": 0
                },
                {
                    "sent": "So the models we learn will not be the same.",
                    "label": 0
                },
                {
                    "sent": "But I think there are also parents in other languages which are totally different from European language like Asian or Arabic languages.",
                    "label": 0
                },
                {
                    "sent": "As for the first question, I think there are quite a few text sources out there which expose patterns, so take for example these.",
                    "label": 0
                },
                {
                    "sent": "These news issued by companies about stock developments and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So whenever you see something like that, there's some text which follows strict strict patterns or less strict but mostly obvious patterns you can.",
                    "label": 0
                },
                {
                    "sent": "You can apply the approach there.",
                    "label": 0
                },
                {
                    "sent": "This is what I assume.",
                    "label": 0
                },
                {
                    "sent": "So as soon as you identify the entities and as you identify there is a certain pattern in there, then you probably can apply the approach.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you had some problems, country specific information for the.",
                    "label": 0
                },
                {
                    "sent": "Geographic location for example and Germany have lender and French of Department is kind of things.",
                    "label": 0
                },
                {
                    "sent": "Oh as long as it's reflected in DB pedia, we can find this information and actually these things also country specific information.",
                    "label": 0
                },
                {
                    "sent": "There are properties in the PDF that reflect that.",
                    "label": 0
                },
                {
                    "sent": "So there are properties for departments in France and so on and so we can also discover these relations and then you only discover them there.",
                    "label": 0
                },
                {
                    "sent": "But sometimes you get some noise of course, because then the extractor things that they should also be departments in the US and then extract some noisy stuff.",
                    "label": 0
                },
                {
                    "sent": "So this is rather a problem with false positives than in the end.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your talk.",
                    "label": 0
                },
                {
                    "sent": "What are the inputs of your classifier?",
                    "label": 0
                },
                {
                    "sent": "The inputs are the abstracts.",
                    "label": 0
                },
                {
                    "sent": "On these features we extracted from the abstract, so there was this.",
                    "label": 0
                },
                {
                    "sent": "There was this list of features.",
                    "label": 0
                },
                {
                    "sent": "It's basically a fairly fairly simple list, so we look at what are the candidates or candidates are.",
                    "label": 0
                },
                {
                    "sent": "Everything that has a matching type.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're looking for a birthplace then that's all the places we look at.",
                    "label": 0
                },
                {
                    "sent": "All the entities, regardless of their type.",
                    "label": 0
                },
                {
                    "sent": "And then we look at where are they positioned in the sentence in the abstract and where is the where is the sentence position in the abstract?",
                    "label": 0
                },
                {
                    "sent": "Is there a link back from the?",
                    "label": 0
                },
                {
                    "sent": "The linked entity to the one we have at hand, and then we look at the vector of the instances types and this is the features that the classifier gets.",
                    "label": 0
                },
                {
                    "sent": "So you say you you build models which are language specific.",
                    "label": 0
                },
                {
                    "sent": "Have you looked at how much the patterns overlap between languages and how much they differ?",
                    "label": 0
                },
                {
                    "sent": "But also if you are using other sources, can you have you look at whether the patterns are Wikipedia specific or if they actually come back in other sources of writing?",
                    "label": 0
                },
                {
                    "sent": "OK, we didn't do the 2nd, so we didn't look beyond Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But we in fact look at the similarity of patterns, and we found that for some relations there are there are patterns which are universal.",
                    "label": 0
                },
                {
                    "sent": "So for example, if musicians have genre and every genre in the abstract, follow musician is is a general of that musician.",
                    "label": 0
                },
                {
                    "sent": "That's a universal pattern that occurs in every language and for other other relations, the patterns in which people describe this in different languages differ.",
                    "label": 0
                },
                {
                    "sent": "But there are some relations for which you also can find universal patterns and what will be interesting if we find such a universal pattern is down to apply.",
                    "label": 0
                },
                {
                    "sent": "It also to other languages.",
                    "label": 0
                },
                {
                    "sent": "In which we couldn't find a pattern in the first place.",
                    "label": 0
                },
                {
                    "sent": "Very quick technical question, what do you exactly mean by an abstract?",
                    "label": 0
                },
                {
                    "sent": "Is it the first prog paragraph, barriers of language or does it change it according to the different languages?",
                    "label": 0
                },
                {
                    "sent": "It's the text that stands before the 1st headline comes, so in Wikipedia, when the first headline starts, then the abstract nouns.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}