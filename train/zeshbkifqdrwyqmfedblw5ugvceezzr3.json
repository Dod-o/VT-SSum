{
    "id": "zeshbkifqdrwyqmfedblw5ugvceezzr3",
    "title": "Predictive Representations for Policy Gradient in POMDPs",
    "info": {
        "author": [
            "Abdeslam Boularias, DAMAS Laboratory, Universit\u00e9 Laval"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/icml09_boularias_prfp/",
    "segmentation": [
        [
            "Hi everybody, so we will be talking about predictive representations for policy gradient in palm.",
            "DP's this is joint work with brain shape.",
            "There are my advisor and we are from level University.",
            "So many real world problems can be costed only as."
        ],
        [
            "Palm DPS and as we know, learning an optimal policy for partially observable environments is still considered as one of the most difficult challenge in refreshment learning as possible solution to this problem."
        ],
        [
            "We propose gradient descent algorithm for Palm GPS, where the policy is represented as a piercer, a predictive state representation.",
            "We also provide theoretical comparison between the value function of the policy represented by PSR and finite state Comptroller.",
            "Finite state controllers are the most used model for representing policies in Palm DP's and finally we provide numerical comparison between peers and finite state controllers."
        ],
        [
            "So apart important, deep is the state of the system is high, is hidden and at given time T this system is in state St and the agent executive action 80.",
            "The system transition to next state S T + 1 and send an observation of the plus one to the agent.",
            "The agent updates its belief state by using the previous belief state.",
            "BT the executive Action 80 and the reserved observation from the environment.",
            "So the whole sequence of actions and observations.",
            "Is what we call history.",
            "Each history is equivalent to a belief state.",
            "So belief state which is distribution probability distribution over the states of the system is sufficient.",
            "Statistics of the history and alternative model for Palm DPS is what we call."
        ],
        [
            "State representations, which were proposed by City, certainly one and seen 2001.",
            "So here I will be talking about the PSR's predictive state representations using core history.",
            "Usually or in literature, refined pairs with core tests.",
            "So in this talk will be using piercers with core histories, but they are equivalent so at given time T, assume that you are interested in calculating the probability of some future trajectory and the current history.",
            "HT is for example this sequence.",
            "So the probability of this sequence given the current history.",
            "Can be written as linear combination of the probability of the same sequence given different core histories.",
            "So there are some specific histories that will be used at each time to calculate the probability of any trajectory.",
            "So this is the basic idea.",
            "The probability of this what we call a test test Q for example can be written as Alpha T. The probability of the same test if it started after history at 1 Accord history plus beta T the probability of the same test.",
            "If it started after another career history at 2, so in this formula we can see that the constant parameters are probability of the test after different core histories.",
            "This doesn't change if we for any Time Team, so these are the parameters of our model.",
            "However, the parameters Alpha and beta depends on time, so they are our belief States and they are updated after each action observation.",
            "Since we are interested in model free approach where we don't use a model for the environment, we will be using a model to represent the policy and one of the most used models for representing policies in."
        ],
        [
            "DPS is what we call finite state controllers.",
            "So finite state controller is graphical model with internal state G0G1G2.",
            "So these states are the model states of the agent that don't correspond to the state of environment by the correspond to decision states where we should do some actions.",
            "So given an internal state for example G0, the agent with sample in action according to some distribution executives action and then sample reserve an observation from the environment and then sample the next state.",
            "So learning sequence will correspond to something like this.",
            "We have G0, we executed is zero.",
            "We received one.",
            "Then we sample Jezero etc.",
            "What we can see from here is that by using by sampling internal state will be missing a lot of information about the other states, because here we could also sample G1 or G2.",
            "So if we want to calculate or to update the parameters of the internal state G1 by using only these three steps, the gradient or the value for the.",
            "Radiant of the value function with respect to the parameters of G1 will be 0 because it didn't appear in this sequence.",
            "That's why Aberdeen and Baxter 2002 proposed blueknight set controllers with internal belief state.",
            "So we start by an initial belief state.",
            "We sampled action according to this distribution with seven observation, and then we update this belief state.",
            "So we will be maintaining a probability distribution over the state of the of the controller.",
            "So even the controller is completely observable.",
            "They showed that it's better to consider it as partial observable and to use a internal belief state.",
            "So in this case we can see that the probability of G1 is 02 and there is zero 24.",
            "So even with.",
            "We didn't sample G1 here by using this method, we will be able to update the parameter of G1, so it's like we are sharing the gradient among the states.",
            "However, this doesn't come without price because the value function will depend on the.",
            "This probabilities.",
            "This internal belief States and these probabilities are actually calculated from previous probabilities.",
            "Each belief state is product of previous belief state, which means that the value function may may be more complex."
        ],
        [
            "That's why we propose just to remove the internal state and to use a model to represent policies which is at least as powerful as finite state controller and which is based only on what we observe, which are the core histories.",
            "So this is."
        ],
        [
            "What we call a predictive representation of policy of policies so appears our policy.",
            "NPR policies.",
            "We redefined test as observation, action, observation, action, couples.",
            "So if we want to keep all the framework of passers which are used to represent the dynamic of an environment and use it to represent the policy, we have to switch between actions and observations.",
            "So in this case the probability of tests is the probability of observing actions given history and given.",
            "The future observation.",
            "So we are asking the question what will the agent do if you will observe if you will observe these and this and this and this observation given Theta and Theta, are the parameters of the policy."
        ],
        [
            "So as what as what you showed for piercers for representing dynamics of an environment, the probability of a sequence can be written as a linear combination of the probabilities of the same sequence after different core histories.",
            "So for example, if you want just to know at a given moment, we should want to know the priority of executive.",
            "Some action given history HD.",
            "Given the last observation.",
            "Oh, and given a parameter Theta.",
            "So this is what should any stochastic policy.",
            "Do, it should be able to calculate this distribution at each moment we calculated by using this linear combination.",
            "So here we have the probability of the same action after each different core history.",
            "So this is the parameter of the model multiplied by the belief state for each core history.",
            "So the belief state here is the weight of the core history H in the current history and the current time T is not probability.",
            "These are real valued parameters and we can show that they are subject to some constraints.",
            "Among them, they should always sum to one.",
            "Other other concerns.",
            "So what we should remember is that the parameters of our model at this moment are the probabilities of the actions after each history and the belief state is just the weight of the each core history in the current history."
        ],
        [
            "So how do we update this belief state?",
            "We derive the rule from by using Bayes rule.",
            "What we should see here is that you calculate the next beliefs that BT plus one after observed after observing or and executing A is what we need is the previous belief state.",
            "We need the distribution over actions for each core history and we need the this parameter so this parameter what it means.",
            "It means the weight of the core history extension so for.",
            "Each core history we want to know the belief state after one step extension.",
            "So intuitively this corresponds to the transition function in finite state controller or in palm DP, but it's not exactly transition function."
        ],
        [
            "So we will be using this parametric policies to calculate to calculate gradient and improve their parameters, and in order to converge to an optimal or local optimal policy.",
            "So whatever the policy, reconsider if we consider finite Aurizon will find the gradient of value of the policy can be written as.",
            "This is given by this equation, so it's discounted some of the product of two para meters of two factors.",
            "The blue one is what belongs to the environment is the expected reward after history HD multiplied by the probability of the observations contained in this historic history given the actions.",
            "So we don't see the parameter Theta here.",
            "This is an independent of the policy.",
            "We can learn it by using."
        ],
        [
            "For example, look up table.",
            "That's what we did in our experiments.",
            "So for each history will be calculating the estimated probability of the observations given the actions and we also estimate the expected reward.",
            "But if we have definite orason, we can for example use a model based approach and learn model which project dynamics of the environment.",
            "So what is interesting for us?",
            "Is this part the red part which corresponds to which belongs to the policy so.",
            "So this is the gradient of the probabilities of probability of actions given observations.",
            "So this is this reflects the dynamics of the agent.",
            "However, the agent is executing the."
        ],
        [
            "Actions, so the probability of the actions given the actions contained in history given the observations contained in history depends on the model that we use for policy.",
            "So if we use for example a finite state controller, this probability can be written as a product of an initial belief state over the state of the finite set controller multiplied by different transition matrices.",
            "So this is like any Markov chain and each transition matrix contains.",
            "Two entries, the product of two OS.",
            "Each entry contains the product of two parameters, so the first parameter is transition from state G to state G prime and second one is the probability of executing it in state G prime.",
            "So if the finite state controller is completely connected, and."
        ],
        [
            "No one of this parameter is 0, so in general that that that's how we initialize the finest controller.",
            "We can use sparse controllers, but in this case we need to increase the number of internal states.",
            "If we don't want to limit ourselves to limit to special class of controllers.",
            "So this is a polynomial of degree 2T because we have three steps and at each step we have a product of two parameters.",
            "So the value function of finite state controller is polynomial.",
            "T. Unless wonderful parameters is 0.",
            "No, if we use the final."
        ],
        [
            "Addictives later presentation so we can show that the probability of the actions given the observations can be right and almost as the same thing As for finite state controller.",
            "So it's a product of the initial belief state which applied by different transition matrices.",
            "However, each mattress has different signification, so the entrance of a matrix is the product of two or factors.",
            "The first one is the probability of executive action given a core history.",
            "So this corresponds to this term here this term alright, and the second one is the transition from core history to to the extension of this core history.",
            "So it corresponds to this there, so we can see that at most the degree of this.",
            "A polynomial is 2 T, but."
        ],
        [
            "We can show that if.",
            "And this is the main important point in the show.",
            "The advantage of passers over finite state controller if the history start with a core history.",
            "So our history is is for the first ice steps, accord history, then we have the remaining of the history and this happens often.",
            "Almost mostly Historicists will start with the core history and then the models start to mix and start to use the weights of different core histories.",
            "So in this case, the probability this probability can be written as the product of the probabilities of only the actions of executing the action.",
            "There is no transition parameters which are used to calculate the probability of a core history.",
            "That's why we have here a polynomial of degree I, which is just the length of the core history instead of two.",
            "I because we have just one parameter per staff per step and the remaining is the same As for finite state controllers.",
            "We have two T -- I or twice the size of the remaining of the history.",
            "So the whole thing will be two T -- I.",
            "So we saved I degrees in the polynomial of the value function.",
            "What does this mean?",
            "This means that maybe the value function of the finance, the pairs, our policy is easier to optimize, contains less local optimum.",
            "This is an example."
        ],
        [
            "This is a toy example just to show difference between the two functions, so we have the agent which wich start at state at first set and then it goes up and the right and receives a reward of one or up and left and there is reception reward of half and you have two observations, red one and a blue one.",
            "The system is deterministic but it's partially observable.",
            "Of course the optimal policies can be reactive but.",
            "We assume here that we have controller for the state and we try to see if we transform it directly to a PSR.",
            "What will be the difference?",
            "This controller contains 3 step instead G1.",
            "We assume that we already know that we should execute up, which alright use the less parameters in order to promote the value function will use just two parameters, so I assume that we already learned this, but we are not sure if we should go left or right.",
            "So from G1 we go to G2 with probability one and then in G2 executive left with probability Theta two.",
            "So that's why we have this term value function.",
            "It's healthy to 1 multiplied by.",
            "Two then we can go to G to G2 with probability 1 -- 3 to one and then G2.",
            "We execute the action right with probability 1 -- 2 to two.",
            "I use just two parameters in order to be able to present value function.",
            "So this is what you have a polynomial of degree T2 becausw.",
            "Just to calculate this probability even we know that we should go up just to calculate this probability, we need to use two parameters we have because we have transition and this is because we don't observe the internal state during our.",
            "Learning they are not part of our learning, so we should always calculate this transition probabilities.",
            "However, if we take it and transform it directly to predictive state representation and there is an algorithm which can, which is the algorithm proposed by certainly fancy for Palm DP, SPSS.",
            "It can be used to take the next step controller and transform it in linear time to PSR.",
            "In this case, we will find that we have just one core history.",
            "It's zero, we find that since we have just one core history and we said that the belief state, which is the weight of the core history, should always sum to one.",
            "So the belief state in this case is constant.",
            "It's always what because we have just one entry, which means that if we want to if we want to calculate the belief set up after one step extension, there will be no variable parameter.",
            "Then after we execute up so up we execute it with probability one.",
            "We can go left with probability to one prime or right with probability to two prime.",
            "So these are the new para meters.",
            "So the value function will be a healthy on prime plus the two prime.",
            "So it's just the probability of going left and we go, we gain, half underwrite, and we gain one.",
            "This is a polynomial of degree one."
        ],
        [
            "In this case, we found that this value function of the finite state controller because of that, has a degree of two.",
            "This doesn't mean that always it has two local optimal 2.",
            "Two points where the gradient is 0, but in this case we found that for four we have local Optima of 1 hardware.",
            "So if we start initializing our parameters here and we use the steepest gradient approach, we found our self help.",
            "Of course, in this work we use the the just the simplest gradient method because we want just to compare the structures of the value functions.",
            "We can use the more sophisticated, more sophisticated method for calculating the gradient.",
            "Now the value function of the PSR is just a plant, it's polynomial of degree one, and it has just one local optimum global Optima.",
            "Here we should notice that the contrary to this function, which is where the parameters are just between zero and one, they are not disjoint event.",
            "In this case, since Theta one parameter to prime corresponds to the probability of going left and right, so there are disjoint events, they should always less than one, which means that we should eliminate half of this simplex.",
            "So we have some constraints on these parameters.",
            "And if we eliminate this, we can see that we have the same optimal One South wherever we start, our model will always end up to this global optimum.",
            "So we did some empirical comparisons between the by using the same."
        ],
        [
            "Audient method.",
            "But just using different models of the policy, so we find that the.",
            "The green curve correspond to the expected reward will average reward their learning step of the finite state controller.",
            "The red one corresponds to the expected reward of the predictive state representations of the policy, and we see in this problem and this one we have convergence to the optimal solution.",
            "In this one, the finite state controller is tracking in local optimum.",
            "However, for problems where the number of actions and observations is higher in this case, we have more histories the.",
            "The PSR policy find it to discover the right core histories, so this is a point that I did not talk in this presentation.",
            "How we Discover Card histories.",
            "We use some heretics are methods in literature to find the core histories.",
            "So it seems that in this case it was difficult, and I think that this is the most difficult part in learning pieces.",
            "How to learn this structure?",
            "However, for finite state controller the states were specified in advance, so we tried different for different configurations for number of States and we choose the best one.",
            "So there is little advantage for emphasis over passers in this experimentations."
        ],
        [
            "So we showed that piercers can be used as model to represent policies as they were used as model to represent the dynamics of the environment and we can transfer all the advantages that we have for representing the environments to this case where we represent the policies.",
            "The internal states of PSR based on history or on tests or on actions and observations.",
            "We don't use hidden States and the degree of the value function of PSR policy has a smaller smaller than the corresponding value function of the finite state Comptroller.",
            "However, there are three problems.",
            "It's unclear how PSR policies will perform in infinite orison problems.",
            "This was just for financial reason because the value function becomes not.",
            "A polynomial becomes something.",
            "Close to polynomial.",
            "The other thing is discover of Nucor histories.",
            "We used some very sticks.",
            "About the predictability of the history, the entropy of its distribution over the action, the actions and the corrections that we made to project the para meters into the simplex of valid parameters.",
            "This holistic's can be used on Decatur if test if core history is independent or it's depends on the previous call history.",
            "So this point should be improved and the final point is the belief state of PSR are not very stable.",
            "So after a given number of updating.",
            "They start divergent.",
            "I'm making wrong predictions, so these three problems are problems of piercers.",
            "In general they are not.",
            "Just for this approach.",
            "So as future work, in addition to solving these three points, we propose the mainly to study performances of peers policies by using the natural gradient or other upwards of gradients that capture the geometry constructor of the value function instead of just simple gradient.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everybody, so we will be talking about predictive representations for policy gradient in palm.",
                    "label": 1
                },
                {
                    "sent": "DP's this is joint work with brain shape.",
                    "label": 0
                },
                {
                    "sent": "There are my advisor and we are from level University.",
                    "label": 0
                },
                {
                    "sent": "So many real world problems can be costed only as.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Palm DPS and as we know, learning an optimal policy for partially observable environments is still considered as one of the most difficult challenge in refreshment learning as possible solution to this problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We propose gradient descent algorithm for Palm GPS, where the policy is represented as a piercer, a predictive state representation.",
                    "label": 1
                },
                {
                    "sent": "We also provide theoretical comparison between the value function of the policy represented by PSR and finite state Comptroller.",
                    "label": 1
                },
                {
                    "sent": "Finite state controllers are the most used model for representing policies in Palm DP's and finally we provide numerical comparison between peers and finite state controllers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So apart important, deep is the state of the system is high, is hidden and at given time T this system is in state St and the agent executive action 80.",
                    "label": 0
                },
                {
                    "sent": "The system transition to next state S T + 1 and send an observation of the plus one to the agent.",
                    "label": 0
                },
                {
                    "sent": "The agent updates its belief state by using the previous belief state.",
                    "label": 0
                },
                {
                    "sent": "BT the executive Action 80 and the reserved observation from the environment.",
                    "label": 0
                },
                {
                    "sent": "So the whole sequence of actions and observations.",
                    "label": 1
                },
                {
                    "sent": "Is what we call history.",
                    "label": 1
                },
                {
                    "sent": "Each history is equivalent to a belief state.",
                    "label": 0
                },
                {
                    "sent": "So belief state which is distribution probability distribution over the states of the system is sufficient.",
                    "label": 0
                },
                {
                    "sent": "Statistics of the history and alternative model for Palm DPS is what we call.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State representations, which were proposed by City, certainly one and seen 2001.",
                    "label": 0
                },
                {
                    "sent": "So here I will be talking about the PSR's predictive state representations using core history.",
                    "label": 1
                },
                {
                    "sent": "Usually or in literature, refined pairs with core tests.",
                    "label": 0
                },
                {
                    "sent": "So in this talk will be using piercers with core histories, but they are equivalent so at given time T, assume that you are interested in calculating the probability of some future trajectory and the current history.",
                    "label": 1
                },
                {
                    "sent": "HT is for example this sequence.",
                    "label": 0
                },
                {
                    "sent": "So the probability of this sequence given the current history.",
                    "label": 0
                },
                {
                    "sent": "Can be written as linear combination of the probability of the same sequence given different core histories.",
                    "label": 0
                },
                {
                    "sent": "So there are some specific histories that will be used at each time to calculate the probability of any trajectory.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "The probability of this what we call a test test Q for example can be written as Alpha T. The probability of the same test if it started after history at 1 Accord history plus beta T the probability of the same test.",
                    "label": 0
                },
                {
                    "sent": "If it started after another career history at 2, so in this formula we can see that the constant parameters are probability of the test after different core histories.",
                    "label": 0
                },
                {
                    "sent": "This doesn't change if we for any Time Team, so these are the parameters of our model.",
                    "label": 0
                },
                {
                    "sent": "However, the parameters Alpha and beta depends on time, so they are our belief States and they are updated after each action observation.",
                    "label": 0
                },
                {
                    "sent": "Since we are interested in model free approach where we don't use a model for the environment, we will be using a model to represent the policy and one of the most used models for representing policies in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "DPS is what we call finite state controllers.",
                    "label": 1
                },
                {
                    "sent": "So finite state controller is graphical model with internal state G0G1G2.",
                    "label": 1
                },
                {
                    "sent": "So these states are the model states of the agent that don't correspond to the state of environment by the correspond to decision states where we should do some actions.",
                    "label": 0
                },
                {
                    "sent": "So given an internal state for example G0, the agent with sample in action according to some distribution executives action and then sample reserve an observation from the environment and then sample the next state.",
                    "label": 0
                },
                {
                    "sent": "So learning sequence will correspond to something like this.",
                    "label": 0
                },
                {
                    "sent": "We have G0, we executed is zero.",
                    "label": 0
                },
                {
                    "sent": "We received one.",
                    "label": 0
                },
                {
                    "sent": "Then we sample Jezero etc.",
                    "label": 0
                },
                {
                    "sent": "What we can see from here is that by using by sampling internal state will be missing a lot of information about the other states, because here we could also sample G1 or G2.",
                    "label": 0
                },
                {
                    "sent": "So if we want to calculate or to update the parameters of the internal state G1 by using only these three steps, the gradient or the value for the.",
                    "label": 0
                },
                {
                    "sent": "Radiant of the value function with respect to the parameters of G1 will be 0 because it didn't appear in this sequence.",
                    "label": 0
                },
                {
                    "sent": "That's why Aberdeen and Baxter 2002 proposed blueknight set controllers with internal belief state.",
                    "label": 1
                },
                {
                    "sent": "So we start by an initial belief state.",
                    "label": 0
                },
                {
                    "sent": "We sampled action according to this distribution with seven observation, and then we update this belief state.",
                    "label": 0
                },
                {
                    "sent": "So we will be maintaining a probability distribution over the state of the of the controller.",
                    "label": 0
                },
                {
                    "sent": "So even the controller is completely observable.",
                    "label": 0
                },
                {
                    "sent": "They showed that it's better to consider it as partial observable and to use a internal belief state.",
                    "label": 0
                },
                {
                    "sent": "So in this case we can see that the probability of G1 is 02 and there is zero 24.",
                    "label": 0
                },
                {
                    "sent": "So even with.",
                    "label": 0
                },
                {
                    "sent": "We didn't sample G1 here by using this method, we will be able to update the parameter of G1, so it's like we are sharing the gradient among the states.",
                    "label": 1
                },
                {
                    "sent": "However, this doesn't come without price because the value function will depend on the.",
                    "label": 0
                },
                {
                    "sent": "This probabilities.",
                    "label": 0
                },
                {
                    "sent": "This internal belief States and these probabilities are actually calculated from previous probabilities.",
                    "label": 0
                },
                {
                    "sent": "Each belief state is product of previous belief state, which means that the value function may may be more complex.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's why we propose just to remove the internal state and to use a model to represent policies which is at least as powerful as finite state controller and which is based only on what we observe, which are the core histories.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we call a predictive representation of policy of policies so appears our policy.",
                    "label": 1
                },
                {
                    "sent": "NPR policies.",
                    "label": 0
                },
                {
                    "sent": "We redefined test as observation, action, observation, action, couples.",
                    "label": 0
                },
                {
                    "sent": "So if we want to keep all the framework of passers which are used to represent the dynamic of an environment and use it to represent the policy, we have to switch between actions and observations.",
                    "label": 0
                },
                {
                    "sent": "So in this case the probability of tests is the probability of observing actions given history and given.",
                    "label": 1
                },
                {
                    "sent": "The future observation.",
                    "label": 0
                },
                {
                    "sent": "So we are asking the question what will the agent do if you will observe if you will observe these and this and this and this observation given Theta and Theta, are the parameters of the policy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as what as what you showed for piercers for representing dynamics of an environment, the probability of a sequence can be written as a linear combination of the probabilities of the same sequence after different core histories.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you want just to know at a given moment, we should want to know the priority of executive.",
                    "label": 0
                },
                {
                    "sent": "Some action given history HD.",
                    "label": 0
                },
                {
                    "sent": "Given the last observation.",
                    "label": 0
                },
                {
                    "sent": "Oh, and given a parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "So this is what should any stochastic policy.",
                    "label": 0
                },
                {
                    "sent": "Do, it should be able to calculate this distribution at each moment we calculated by using this linear combination.",
                    "label": 0
                },
                {
                    "sent": "So here we have the probability of the same action after each different core history.",
                    "label": 0
                },
                {
                    "sent": "So this is the parameter of the model multiplied by the belief state for each core history.",
                    "label": 0
                },
                {
                    "sent": "So the belief state here is the weight of the core history H in the current history and the current time T is not probability.",
                    "label": 0
                },
                {
                    "sent": "These are real valued parameters and we can show that they are subject to some constraints.",
                    "label": 0
                },
                {
                    "sent": "Among them, they should always sum to one.",
                    "label": 0
                },
                {
                    "sent": "Other other concerns.",
                    "label": 0
                },
                {
                    "sent": "So what we should remember is that the parameters of our model at this moment are the probabilities of the actions after each history and the belief state is just the weight of the each core history in the current history.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we update this belief state?",
                    "label": 0
                },
                {
                    "sent": "We derive the rule from by using Bayes rule.",
                    "label": 1
                },
                {
                    "sent": "What we should see here is that you calculate the next beliefs that BT plus one after observed after observing or and executing A is what we need is the previous belief state.",
                    "label": 0
                },
                {
                    "sent": "We need the distribution over actions for each core history and we need the this parameter so this parameter what it means.",
                    "label": 0
                },
                {
                    "sent": "It means the weight of the core history extension so for.",
                    "label": 0
                },
                {
                    "sent": "Each core history we want to know the belief state after one step extension.",
                    "label": 1
                },
                {
                    "sent": "So intuitively this corresponds to the transition function in finite state controller or in palm DP, but it's not exactly transition function.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will be using this parametric policies to calculate to calculate gradient and improve their parameters, and in order to converge to an optimal or local optimal policy.",
                    "label": 0
                },
                {
                    "sent": "So whatever the policy, reconsider if we consider finite Aurizon will find the gradient of value of the policy can be written as.",
                    "label": 1
                },
                {
                    "sent": "This is given by this equation, so it's discounted some of the product of two para meters of two factors.",
                    "label": 0
                },
                {
                    "sent": "The blue one is what belongs to the environment is the expected reward after history HD multiplied by the probability of the observations contained in this historic history given the actions.",
                    "label": 0
                },
                {
                    "sent": "So we don't see the parameter Theta here.",
                    "label": 0
                },
                {
                    "sent": "This is an independent of the policy.",
                    "label": 0
                },
                {
                    "sent": "We can learn it by using.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, look up table.",
                    "label": 0
                },
                {
                    "sent": "That's what we did in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So for each history will be calculating the estimated probability of the observations given the actions and we also estimate the expected reward.",
                    "label": 1
                },
                {
                    "sent": "But if we have definite orason, we can for example use a model based approach and learn model which project dynamics of the environment.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting for us?",
                    "label": 0
                },
                {
                    "sent": "Is this part the red part which corresponds to which belongs to the policy so.",
                    "label": 0
                },
                {
                    "sent": "So this is the gradient of the probabilities of probability of actions given observations.",
                    "label": 1
                },
                {
                    "sent": "So this is this reflects the dynamics of the agent.",
                    "label": 0
                },
                {
                    "sent": "However, the agent is executing the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actions, so the probability of the actions given the actions contained in history given the observations contained in history depends on the model that we use for policy.",
                    "label": 0
                },
                {
                    "sent": "So if we use for example a finite state controller, this probability can be written as a product of an initial belief state over the state of the finite set controller multiplied by different transition matrices.",
                    "label": 1
                },
                {
                    "sent": "So this is like any Markov chain and each transition matrix contains.",
                    "label": 0
                },
                {
                    "sent": "Two entries, the product of two OS.",
                    "label": 0
                },
                {
                    "sent": "Each entry contains the product of two parameters, so the first parameter is transition from state G to state G prime and second one is the probability of executing it in state G prime.",
                    "label": 0
                },
                {
                    "sent": "So if the finite state controller is completely connected, and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No one of this parameter is 0, so in general that that that's how we initialize the finest controller.",
                    "label": 0
                },
                {
                    "sent": "We can use sparse controllers, but in this case we need to increase the number of internal states.",
                    "label": 0
                },
                {
                    "sent": "If we don't want to limit ourselves to limit to special class of controllers.",
                    "label": 0
                },
                {
                    "sent": "So this is a polynomial of degree 2T because we have three steps and at each step we have a product of two parameters.",
                    "label": 1
                },
                {
                    "sent": "So the value function of finite state controller is polynomial.",
                    "label": 0
                },
                {
                    "sent": "T. Unless wonderful parameters is 0.",
                    "label": 1
                },
                {
                    "sent": "No, if we use the final.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Addictives later presentation so we can show that the probability of the actions given the observations can be right and almost as the same thing As for finite state controller.",
                    "label": 0
                },
                {
                    "sent": "So it's a product of the initial belief state which applied by different transition matrices.",
                    "label": 0
                },
                {
                    "sent": "However, each mattress has different signification, so the entrance of a matrix is the product of two or factors.",
                    "label": 0
                },
                {
                    "sent": "The first one is the probability of executive action given a core history.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds to this term here this term alright, and the second one is the transition from core history to to the extension of this core history.",
                    "label": 0
                },
                {
                    "sent": "So it corresponds to this there, so we can see that at most the degree of this.",
                    "label": 0
                },
                {
                    "sent": "A polynomial is 2 T, but.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that if.",
                    "label": 0
                },
                {
                    "sent": "And this is the main important point in the show.",
                    "label": 0
                },
                {
                    "sent": "The advantage of passers over finite state controller if the history start with a core history.",
                    "label": 0
                },
                {
                    "sent": "So our history is is for the first ice steps, accord history, then we have the remaining of the history and this happens often.",
                    "label": 0
                },
                {
                    "sent": "Almost mostly Historicists will start with the core history and then the models start to mix and start to use the weights of different core histories.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the probability this probability can be written as the product of the probabilities of only the actions of executing the action.",
                    "label": 0
                },
                {
                    "sent": "There is no transition parameters which are used to calculate the probability of a core history.",
                    "label": 0
                },
                {
                    "sent": "That's why we have here a polynomial of degree I, which is just the length of the core history instead of two.",
                    "label": 0
                },
                {
                    "sent": "I because we have just one parameter per staff per step and the remaining is the same As for finite state controllers.",
                    "label": 0
                },
                {
                    "sent": "We have two T -- I or twice the size of the remaining of the history.",
                    "label": 0
                },
                {
                    "sent": "So the whole thing will be two T -- I.",
                    "label": 0
                },
                {
                    "sent": "So we saved I degrees in the polynomial of the value function.",
                    "label": 0
                },
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that maybe the value function of the finance, the pairs, our policy is easier to optimize, contains less local optimum.",
                    "label": 0
                },
                {
                    "sent": "This is an example.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a toy example just to show difference between the two functions, so we have the agent which wich start at state at first set and then it goes up and the right and receives a reward of one or up and left and there is reception reward of half and you have two observations, red one and a blue one.",
                    "label": 0
                },
                {
                    "sent": "The system is deterministic but it's partially observable.",
                    "label": 0
                },
                {
                    "sent": "Of course the optimal policies can be reactive but.",
                    "label": 0
                },
                {
                    "sent": "We assume here that we have controller for the state and we try to see if we transform it directly to a PSR.",
                    "label": 1
                },
                {
                    "sent": "What will be the difference?",
                    "label": 0
                },
                {
                    "sent": "This controller contains 3 step instead G1.",
                    "label": 0
                },
                {
                    "sent": "We assume that we already know that we should execute up, which alright use the less parameters in order to promote the value function will use just two parameters, so I assume that we already learned this, but we are not sure if we should go left or right.",
                    "label": 0
                },
                {
                    "sent": "So from G1 we go to G2 with probability one and then in G2 executive left with probability Theta two.",
                    "label": 0
                },
                {
                    "sent": "So that's why we have this term value function.",
                    "label": 0
                },
                {
                    "sent": "It's healthy to 1 multiplied by.",
                    "label": 0
                },
                {
                    "sent": "Two then we can go to G to G2 with probability 1 -- 3 to one and then G2.",
                    "label": 1
                },
                {
                    "sent": "We execute the action right with probability 1 -- 2 to two.",
                    "label": 0
                },
                {
                    "sent": "I use just two parameters in order to be able to present value function.",
                    "label": 0
                },
                {
                    "sent": "So this is what you have a polynomial of degree T2 becausw.",
                    "label": 0
                },
                {
                    "sent": "Just to calculate this probability even we know that we should go up just to calculate this probability, we need to use two parameters we have because we have transition and this is because we don't observe the internal state during our.",
                    "label": 0
                },
                {
                    "sent": "Learning they are not part of our learning, so we should always calculate this transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "However, if we take it and transform it directly to predictive state representation and there is an algorithm which can, which is the algorithm proposed by certainly fancy for Palm DP, SPSS.",
                    "label": 0
                },
                {
                    "sent": "It can be used to take the next step controller and transform it in linear time to PSR.",
                    "label": 1
                },
                {
                    "sent": "In this case, we will find that we have just one core history.",
                    "label": 0
                },
                {
                    "sent": "It's zero, we find that since we have just one core history and we said that the belief state, which is the weight of the core history, should always sum to one.",
                    "label": 0
                },
                {
                    "sent": "So the belief state in this case is constant.",
                    "label": 0
                },
                {
                    "sent": "It's always what because we have just one entry, which means that if we want to if we want to calculate the belief set up after one step extension, there will be no variable parameter.",
                    "label": 0
                },
                {
                    "sent": "Then after we execute up so up we execute it with probability one.",
                    "label": 0
                },
                {
                    "sent": "We can go left with probability to one prime or right with probability to two prime.",
                    "label": 0
                },
                {
                    "sent": "So these are the new para meters.",
                    "label": 0
                },
                {
                    "sent": "So the value function will be a healthy on prime plus the two prime.",
                    "label": 0
                },
                {
                    "sent": "So it's just the probability of going left and we go, we gain, half underwrite, and we gain one.",
                    "label": 0
                },
                {
                    "sent": "This is a polynomial of degree one.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we found that this value function of the finite state controller because of that, has a degree of two.",
                    "label": 0
                },
                {
                    "sent": "This doesn't mean that always it has two local optimal 2.",
                    "label": 0
                },
                {
                    "sent": "Two points where the gradient is 0, but in this case we found that for four we have local Optima of 1 hardware.",
                    "label": 0
                },
                {
                    "sent": "So if we start initializing our parameters here and we use the steepest gradient approach, we found our self help.",
                    "label": 0
                },
                {
                    "sent": "Of course, in this work we use the the just the simplest gradient method because we want just to compare the structures of the value functions.",
                    "label": 0
                },
                {
                    "sent": "We can use the more sophisticated, more sophisticated method for calculating the gradient.",
                    "label": 0
                },
                {
                    "sent": "Now the value function of the PSR is just a plant, it's polynomial of degree one, and it has just one local optimum global Optima.",
                    "label": 0
                },
                {
                    "sent": "Here we should notice that the contrary to this function, which is where the parameters are just between zero and one, they are not disjoint event.",
                    "label": 0
                },
                {
                    "sent": "In this case, since Theta one parameter to prime corresponds to the probability of going left and right, so there are disjoint events, they should always less than one, which means that we should eliminate half of this simplex.",
                    "label": 0
                },
                {
                    "sent": "So we have some constraints on these parameters.",
                    "label": 0
                },
                {
                    "sent": "And if we eliminate this, we can see that we have the same optimal One South wherever we start, our model will always end up to this global optimum.",
                    "label": 0
                },
                {
                    "sent": "So we did some empirical comparisons between the by using the same.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Audient method.",
                    "label": 0
                },
                {
                    "sent": "But just using different models of the policy, so we find that the.",
                    "label": 0
                },
                {
                    "sent": "The green curve correspond to the expected reward will average reward their learning step of the finite state controller.",
                    "label": 0
                },
                {
                    "sent": "The red one corresponds to the expected reward of the predictive state representations of the policy, and we see in this problem and this one we have convergence to the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "In this one, the finite state controller is tracking in local optimum.",
                    "label": 0
                },
                {
                    "sent": "However, for problems where the number of actions and observations is higher in this case, we have more histories the.",
                    "label": 0
                },
                {
                    "sent": "The PSR policy find it to discover the right core histories, so this is a point that I did not talk in this presentation.",
                    "label": 0
                },
                {
                    "sent": "How we Discover Card histories.",
                    "label": 0
                },
                {
                    "sent": "We use some heretics are methods in literature to find the core histories.",
                    "label": 0
                },
                {
                    "sent": "So it seems that in this case it was difficult, and I think that this is the most difficult part in learning pieces.",
                    "label": 0
                },
                {
                    "sent": "How to learn this structure?",
                    "label": 0
                },
                {
                    "sent": "However, for finite state controller the states were specified in advance, so we tried different for different configurations for number of States and we choose the best one.",
                    "label": 0
                },
                {
                    "sent": "So there is little advantage for emphasis over passers in this experimentations.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we showed that piercers can be used as model to represent policies as they were used as model to represent the dynamics of the environment and we can transfer all the advantages that we have for representing the environments to this case where we represent the policies.",
                    "label": 0
                },
                {
                    "sent": "The internal states of PSR based on history or on tests or on actions and observations.",
                    "label": 0
                },
                {
                    "sent": "We don't use hidden States and the degree of the value function of PSR policy has a smaller smaller than the corresponding value function of the finite state Comptroller.",
                    "label": 0
                },
                {
                    "sent": "However, there are three problems.",
                    "label": 0
                },
                {
                    "sent": "It's unclear how PSR policies will perform in infinite orison problems.",
                    "label": 0
                },
                {
                    "sent": "This was just for financial reason because the value function becomes not.",
                    "label": 0
                },
                {
                    "sent": "A polynomial becomes something.",
                    "label": 0
                },
                {
                    "sent": "Close to polynomial.",
                    "label": 0
                },
                {
                    "sent": "The other thing is discover of Nucor histories.",
                    "label": 0
                },
                {
                    "sent": "We used some very sticks.",
                    "label": 0
                },
                {
                    "sent": "About the predictability of the history, the entropy of its distribution over the action, the actions and the corrections that we made to project the para meters into the simplex of valid parameters.",
                    "label": 0
                },
                {
                    "sent": "This holistic's can be used on Decatur if test if core history is independent or it's depends on the previous call history.",
                    "label": 0
                },
                {
                    "sent": "So this point should be improved and the final point is the belief state of PSR are not very stable.",
                    "label": 0
                },
                {
                    "sent": "So after a given number of updating.",
                    "label": 0
                },
                {
                    "sent": "They start divergent.",
                    "label": 0
                },
                {
                    "sent": "I'm making wrong predictions, so these three problems are problems of piercers.",
                    "label": 0
                },
                {
                    "sent": "In general they are not.",
                    "label": 0
                },
                {
                    "sent": "Just for this approach.",
                    "label": 0
                },
                {
                    "sent": "So as future work, in addition to solving these three points, we propose the mainly to study performances of peers policies by using the natural gradient or other upwards of gradients that capture the geometry constructor of the value function instead of just simple gradient.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}