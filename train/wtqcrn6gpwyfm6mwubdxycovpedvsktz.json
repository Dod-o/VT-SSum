{
    "id": "wtqcrn6gpwyfm6mwubdxycovpedvsktz",
    "title": "Modeling Human Location Data with Mixtures of Kernel Densities",
    "info": {
        "author": [
            "Moshe Lichman, Department of Computer Science, University of California, Davis"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_lichman_kernel_densities/",
    "segmentation": [
        [
            "My name is Marsha Lakeman and I'm from the Department of Computer Science in University of California, Irvine.",
            "Now today I'm going to talk to you about the work that I've done a lot with my advisor, professor pouring Smith on modeling human location data with mixtures of kernel densities.",
            "Now let's start with look at what human location data looks like."
        ],
        [
            "In the example we see here, we have the map of the area of Laguna Beach.",
            "Can you hear me better now there?",
            "In the area of Laguna Beach, which is located on the coast of Southern California, just a little bit South of Los Angeles, Red points on the map represent human location data, or."
        ],
        [
            "Each point that we will refer to as an event is a specific observation on an individual with location X&Y.",
            "Now the goal of our work is to look at this data and use it to create a continuous space."
        ],
        [
            "Density function now."
        ],
        [
            "Or any event in the space.",
            "This function is the likelihood for it to occur in that specific location."
        ],
        [
            "Now, why would I even want to do that?",
            "Well, the easiest way for me to explain that is show you real life application that benefits from modeling spatial activity.",
            "In the Medical Sciences, such monitor used to analyze the spread of infection diseases.",
            "Now there is no argue about the importance of that.",
            "In fraud detection system, such models are used to detect spatial anomalies in individual behavior.",
            "For example, we can use that to detect credit card thefts in incidents.",
            "In order planning, those model can assist with public transportation planning, congestion control, resource allocation and many more things.",
            "On advertising, well, that's obvious if we know where people are likely to be, we know what to advertise to them in ecology.",
            "Such models are used to analyze the life habitat of species for preservation purposes.",
            "Now obviously there are many more examples, but I'm hoping this was enough to convince you that this is important.",
            "So how can we create that density functi?"
        ],
        [
            "Well, in our work we investigate the use of kernel density estimation for modeling human location data.",
            "This approach is a nonparametric one estimating the density directly from the observed events.",
            "Now to understand that better, let's look at a 1 dimensional simple example."
        ],
        [
            "Well, we have.",
            "Here is a set of observed events on the X axis.",
            "Now the first step in computing the density function."
        ],
        [
            "Is to put an individual density around each one of those observed events that we call the kernel function.",
            "Now each kernel function has a bandwidth parameter that controls the width of the distribution or the variance if you'd like.",
            "Now this method where we give each of each one of the observed events the same kernel function in the same bandwidth is called fixed bandwidth, kernel density estimation or fixed KDE."
        ],
        [
            "Now the next step would be to compute the density function that we can see here in the in the blue curve and now this density function again will give us the likelihood of any event in this space.",
            "Now to do that it's relatively simple simple, we just average over the contribution of all the kernel function to that specific location.",
            "Now because the bandwidth parameter control the width of each one of those kernel function, different bandwidth parameter results in different density function."
        ],
        [
            "In the example that we can see here, the green curve uses a large bandwidth and therefore we getting a very smooth density function that clearly doesn't capture the mode of distribution.",
            "Now, on the other hand, if you're using a small bandwidth like in the red curve, we're getting density function that overfits the data an give density in areas.",
            "There clearly represent the tail of the distribution.",
            "Now because of that, having the same bandwidth to all observed events is a serious limitation.",
            "When we're modeling human location data and to see that let's."
        ],
        [
            "Look at the area of Southern California where we were coming from and let's focus on two specific city, the city of Laguna Beach here at the bottom you can see my laser.",
            "So here on the bottom and this city of Barstow, on the road from Eleta Vegas."
        ],
        [
            "Now zooming in on this tool OK."
        ],
        [
            "Nation reveals that both of them has pretty much the same number of observed events, but there are in completely different scale the area on the right is 3000 times larger than the area on the left.",
            "So on the one hand, what we have is a very dense area where all the observed events are really close to each other and practically on top of each other and on the right we have a sparse area where the distance between two observed event is relatively big."
        ],
        [
            "Now trying to fit a density function with the same bandwidth to the two location at the same time is practically impossible, and let's see why.",
            "Here again, we have the data in the two city.",
            "The city of Laguna Beach on the top and the city of Boston on the bottom.",
            "Trying to fit a density function with small bandwidth gives us this knife.",
            "It that we saw earlier in the dense area where we capturing the shape of the road.",
            "Sorry of the city minimizing the waste of density on the ocean and on the mountains around the city.",
            "However, when using the same bandwidth in the sparse area.",
            "We're completely overfitting the data and getting the same problem we had in the simple example.",
            "Switching to a larger bandwidth value gives us a beautiful fit in in the sparse area where we captured in the shape of the road.",
            "The shape of the city and even the shape of the concentrated events here, which is a military base.",
            "But more important, we are completely smoothing out the areas of the desert but using that bandwidth on the dense area we see that we lose all spatial information and getting something that looks like one Gaussian density components.",
            "So the question is, is there a way to enjoy both worlds and get these two nice images at the same time?",
            "The small bandwidth in dense area and the large bandwidth in the sparse area, and obviously the answer is."
        ],
        [
            "One way of doing that is to use the method that is called adaptive bandwidth, kernel density estimation, or adaptive KD.",
            "In this method, each event has its own bandwidth parameter that will adjust it to it.",
            "One way of adjusting it is to set it to be proportional to the distance between that events and it's case nearest neighbor.",
            "Now.",
            "This way the bandwidth will automatically be small one in dense area and large one in sparse error, which is exactly what we need to get the two images that we wanted."
        ],
        [
            "Now let's look at an example and see how it works here.",
            "The same observed event."
        ],
        [
            "And here are the kernel function.",
            "Now we can see that the events in the dense area have tighter higher kernel function that that event in this barreras parceria they get larger bandwidth and a smoother kernel."
        ],
        [
            "Now the density function is computed in the exact same way and we see that we capture in the mode of the distribution while at the same time smoothing the tail.",
            "Now this is exactly what we need for human location data, but so far this is just hand waving intuition.",
            "Let's back it."
        ],
        [
            "With some empirical results.",
            "To do that, we gathered data from Twitter and use 100,000 geolocated events as train set and 100,000 geolocated events at Test set.",
            "We trained both our models using different bandwidth values in K and compute the average log likelihood of the events in the test set.",
            "Now, in terms of average log likelihood, high results mean better fit, so our intuition tells us that we want the adaptive method to get higher average log likelihood than the fixed method."
        ],
        [
            "First, let's look at the fixed bandwidth methods on the X axis.",
            "Here we see different bandwidth values and the Y axis show us the average log likelihood.",
            "Now, as we can see when going from large again can see my laser.",
            "From going from large values to small ones, we're getting better fit up to a certain point where we starting to overfit the data and their results go down."
        ],
        [
            "Looking at the adaptive method, we see that we're getting can almost consistently higher results, except for the case where K equal to in terms of average log likelihood, which is exactly what our intuition told us.",
            "We're going to get.",
            "This is the result of being able to obviously being able to fit different types of location at the same time.",
            "As we saw earlier in the images."
        ],
        [
            "Now the next step in our work is to fit those models to individual level human location data such as we see here.",
            "Now.",
            "This will allow us to perform both analysis and inference on individual location.",
            "The problem is that it is not as trivial as just applying those model to this data that we have here.",
            "What we can see here a set of observed events in a small area next to the City of Riverside.",
            "Now it's important to understand two things.",
            "One this is not fake data.",
            "This is data that comes from a real individual in the area of South California and 2nd.",
            "This is not a rare case.",
            "This is how the data looks like for most of the individual in the data set.",
            "Now fitting the the model to this data results in the following density function of all we can see here is a heat map on top of the salary California map were red indicates higher likelihood in blue indicates lower likelihood.",
            "Now the problem with this model is that it gives the same likelihood for event in the area of Los Angeles as it gives the area the events in the middle of the desert or in the ocean.",
            "Now this is super counter intuitive, 'cause as we could expect, and we know that LA is a very popular area, so it's more likely for the individual to go there then to the middle of the desert or to the ocean.",
            "So the question that comes into mind is what if there is a way to interpolate between the information that we have about popularities of areas to this density function?",
            "And again, the answer is yes, and this is our proposed."
        ],
        [
            "Kernel densities model in R model.",
            "We're not only looking at the individual density, we're also looking at the population density as a whole.",
            "We then combine the two in a way that manner to achieve that.",
            "Make sure that we see here.",
            "Now again, we see that most of the distribution is concentrated around the area of Riverside, but now the rest of the density is distributed around popular area like the Ellee County.",
            "The area of San Diego and even the road to Vegas, and we are no longer wasting density on the ocean around the desert."
        ],
        [
            "So to generalize, our mixture model is a weighted sum of density components for each component reference scale of activity in human location data, and this is unlike Gaussian mixture model, for example, where each component represent different area in the space.",
            "Now as we sign the example, the first component is usually set to be the individual density and the last one is set to be the population density.",
            "But our model is not limited to only two 2 components.",
            "We actually can introduce any other density component to it between the individual and population.",
            "That represent a scale of activity between the two, like City, region, counties, never workplace.",
            "The training of the mall is done like any other mixture model where we take a data set and split it into a training set and a validation set and we can use that to compute the mixing weight using the EM algorithm.",
            "An optimize over the parameter of the bandwidth an K."
        ],
        [
            "Now let's talk about evaluation a little bit in order to validate our model and compare it to commonly used individual level baselines, we gathered gather human location data from location based social networks from the area of Southern California.",
            "We use."
        ],
        [
            "Two about two approaches to evaluate our model and compare it to those model using log likelihood of test data that we saw earlier and tested for the application of fraud detection.",
            "Now let's."
        ],
        [
            "Begin with the log likelihood test on data that gathered from Twitter.",
            "We use around 2 million geolocated events from the month of July 2013.",
            "As training, set an around 45,000 events from the following months as test set."
        ],
        [
            "We first computed the average log likelihood of our baseline using Gaussian Gaussian mixture model with two component and fixed KDE.",
            "Now on the X axis we can see the different models and the Y axis shows us the average log likelihood.",
            "Now again as before average higher average log likelihood stands for better fit."
        ],
        [
            "We also compared our model to adaptive Katie model that was trained on the entire population data and actually achieved better results in the individual models."
        ],
        [
            "When looking at the results of our model, we can see that we achieved far better results than any other baseline, which is somewhat impressive.",
            "Now, to make sure we didn't just create a very, very good Twitter model, we also prefer."
        ],
        [
            "I'm the same evaluation on data that was gathered from Goala using a similar setup."
        ],
        [
            "Now again are mixed.",
            "Katie model achieved far better results in the old.",
            "The other individual baselines in terms of average log likelihood on test data."
        ],
        [
            "Now another way to validate our model is to test its application in the fraud detection system.",
            "In order to do that, we can evaluate its ability to detect spatial anomalies in individual location behavior."
        ],
        [
            "In order to do that, we took the Twitter data set an we introduce something that is called simulated fraud cases by taking random individuals and replacing their test data with some other random individuals data.",
            "This where I valuation data set has both control group Anna simulated fraud cases.",
            "Now for eval."
        ],
        [
            "Location we rank the individual according to the average negative log likelihood, where higher results mean spatial anomaly.",
            "Now basically what we want to see is that the individual from the simulated fraud cases will be ranked higher than the individual from the control group."
        ],
        [
            "Again, we compared our model with the same baseline Gaussian Gaussian mixture model with two components and fixed KDE.",
            "We computed the average negative log likelihood of five test events and then looked at how many individual from the simulated fraud cases are within the top 20 ranked individual.",
            "This is called precision on 20.",
            "Now the Y axis here shows the precision at 20 across 50 runs.",
            "So basically again we want our model to achieve better results on the Y axis, 'cause that will represent that more individual from the simulated fraud cases were ranked higher than the individual.",
            "The control group and not only."
        ],
        [
            "And we achieved that.",
            "In fact, our mixed Katie model achieved twice as much result as they are their baselines, which actually indicates that our fraud detection system can benefit a lot from using our model."
        ],
        [
            "Now, to conclude, we conducted a systematic study of spatial density estimation for social media data.",
            "We also investigated adaptive bandwidth methods for kernel density estimations and most importantly we introduced a multi level mixtures of density models.",
            "Now where are we going?"
        ],
        [
            "Here there are a couple of possible direction for future work.",
            "The first we can generalize the mixture model by using weighted KDE, where the weights will represent the similarities between the individual which are currently represented by the different components.",
            "And 2nd we can adapt our model to work in an online fashion, updating the model with new data and also learning a decay and function as a function of time and the data itself."
        ],
        [
            "Now I'd like to thank the National Science Foundation and the Xerox Corporation for supporting this project.",
            "Allow me to work on it and also my lab mates for providing me with a moral and intellectual support."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Marsha Lakeman and I'm from the Department of Computer Science in University of California, Irvine.",
                    "label": 1
                },
                {
                    "sent": "Now today I'm going to talk to you about the work that I've done a lot with my advisor, professor pouring Smith on modeling human location data with mixtures of kernel densities.",
                    "label": 0
                },
                {
                    "sent": "Now let's start with look at what human location data looks like.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the example we see here, we have the map of the area of Laguna Beach.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me better now there?",
                    "label": 0
                },
                {
                    "sent": "In the area of Laguna Beach, which is located on the coast of Southern California, just a little bit South of Los Angeles, Red points on the map represent human location data, or.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each point that we will refer to as an event is a specific observation on an individual with location X&Y.",
                    "label": 0
                },
                {
                    "sent": "Now the goal of our work is to look at this data and use it to create a continuous space.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Density function now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or any event in the space.",
                    "label": 0
                },
                {
                    "sent": "This function is the likelihood for it to occur in that specific location.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, why would I even want to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the easiest way for me to explain that is show you real life application that benefits from modeling spatial activity.",
                    "label": 1
                },
                {
                    "sent": "In the Medical Sciences, such monitor used to analyze the spread of infection diseases.",
                    "label": 0
                },
                {
                    "sent": "Now there is no argue about the importance of that.",
                    "label": 0
                },
                {
                    "sent": "In fraud detection system, such models are used to detect spatial anomalies in individual behavior.",
                    "label": 0
                },
                {
                    "sent": "For example, we can use that to detect credit card thefts in incidents.",
                    "label": 0
                },
                {
                    "sent": "In order planning, those model can assist with public transportation planning, congestion control, resource allocation and many more things.",
                    "label": 0
                },
                {
                    "sent": "On advertising, well, that's obvious if we know where people are likely to be, we know what to advertise to them in ecology.",
                    "label": 0
                },
                {
                    "sent": "Such models are used to analyze the life habitat of species for preservation purposes.",
                    "label": 0
                },
                {
                    "sent": "Now obviously there are many more examples, but I'm hoping this was enough to convince you that this is important.",
                    "label": 0
                },
                {
                    "sent": "So how can we create that density functi?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, in our work we investigate the use of kernel density estimation for modeling human location data.",
                    "label": 0
                },
                {
                    "sent": "This approach is a nonparametric one estimating the density directly from the observed events.",
                    "label": 1
                },
                {
                    "sent": "Now to understand that better, let's look at a 1 dimensional simple example.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we have.",
                    "label": 0
                },
                {
                    "sent": "Here is a set of observed events on the X axis.",
                    "label": 1
                },
                {
                    "sent": "Now the first step in computing the density function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to put an individual density around each one of those observed events that we call the kernel function.",
                    "label": 0
                },
                {
                    "sent": "Now each kernel function has a bandwidth parameter that controls the width of the distribution or the variance if you'd like.",
                    "label": 0
                },
                {
                    "sent": "Now this method where we give each of each one of the observed events the same kernel function in the same bandwidth is called fixed bandwidth, kernel density estimation or fixed KDE.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the next step would be to compute the density function that we can see here in the in the blue curve and now this density function again will give us the likelihood of any event in this space.",
                    "label": 0
                },
                {
                    "sent": "Now to do that it's relatively simple simple, we just average over the contribution of all the kernel function to that specific location.",
                    "label": 0
                },
                {
                    "sent": "Now because the bandwidth parameter control the width of each one of those kernel function, different bandwidth parameter results in different density function.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the example that we can see here, the green curve uses a large bandwidth and therefore we getting a very smooth density function that clearly doesn't capture the mode of distribution.",
                    "label": 0
                },
                {
                    "sent": "Now, on the other hand, if you're using a small bandwidth like in the red curve, we're getting density function that overfits the data an give density in areas.",
                    "label": 0
                },
                {
                    "sent": "There clearly represent the tail of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Now because of that, having the same bandwidth to all observed events is a serious limitation.",
                    "label": 0
                },
                {
                    "sent": "When we're modeling human location data and to see that let's.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the area of Southern California where we were coming from and let's focus on two specific city, the city of Laguna Beach here at the bottom you can see my laser.",
                    "label": 0
                },
                {
                    "sent": "So here on the bottom and this city of Barstow, on the road from Eleta Vegas.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now zooming in on this tool OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation reveals that both of them has pretty much the same number of observed events, but there are in completely different scale the area on the right is 3000 times larger than the area on the left.",
                    "label": 0
                },
                {
                    "sent": "So on the one hand, what we have is a very dense area where all the observed events are really close to each other and practically on top of each other and on the right we have a sparse area where the distance between two observed event is relatively big.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now trying to fit a density function with the same bandwidth to the two location at the same time is practically impossible, and let's see why.",
                    "label": 0
                },
                {
                    "sent": "Here again, we have the data in the two city.",
                    "label": 0
                },
                {
                    "sent": "The city of Laguna Beach on the top and the city of Boston on the bottom.",
                    "label": 0
                },
                {
                    "sent": "Trying to fit a density function with small bandwidth gives us this knife.",
                    "label": 0
                },
                {
                    "sent": "It that we saw earlier in the dense area where we capturing the shape of the road.",
                    "label": 0
                },
                {
                    "sent": "Sorry of the city minimizing the waste of density on the ocean and on the mountains around the city.",
                    "label": 0
                },
                {
                    "sent": "However, when using the same bandwidth in the sparse area.",
                    "label": 0
                },
                {
                    "sent": "We're completely overfitting the data and getting the same problem we had in the simple example.",
                    "label": 0
                },
                {
                    "sent": "Switching to a larger bandwidth value gives us a beautiful fit in in the sparse area where we captured in the shape of the road.",
                    "label": 0
                },
                {
                    "sent": "The shape of the city and even the shape of the concentrated events here, which is a military base.",
                    "label": 0
                },
                {
                    "sent": "But more important, we are completely smoothing out the areas of the desert but using that bandwidth on the dense area we see that we lose all spatial information and getting something that looks like one Gaussian density components.",
                    "label": 0
                },
                {
                    "sent": "So the question is, is there a way to enjoy both worlds and get these two nice images at the same time?",
                    "label": 0
                },
                {
                    "sent": "The small bandwidth in dense area and the large bandwidth in the sparse area, and obviously the answer is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One way of doing that is to use the method that is called adaptive bandwidth, kernel density estimation, or adaptive KD.",
                    "label": 0
                },
                {
                    "sent": "In this method, each event has its own bandwidth parameter that will adjust it to it.",
                    "label": 1
                },
                {
                    "sent": "One way of adjusting it is to set it to be proportional to the distance between that events and it's case nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This way the bandwidth will automatically be small one in dense area and large one in sparse error, which is exactly what we need to get the two images that we wanted.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at an example and see how it works here.",
                    "label": 0
                },
                {
                    "sent": "The same observed event.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here are the kernel function.",
                    "label": 0
                },
                {
                    "sent": "Now we can see that the events in the dense area have tighter higher kernel function that that event in this barreras parceria they get larger bandwidth and a smoother kernel.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the density function is computed in the exact same way and we see that we capture in the mode of the distribution while at the same time smoothing the tail.",
                    "label": 0
                },
                {
                    "sent": "Now this is exactly what we need for human location data, but so far this is just hand waving intuition.",
                    "label": 0
                },
                {
                    "sent": "Let's back it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With some empirical results.",
                    "label": 0
                },
                {
                    "sent": "To do that, we gathered data from Twitter and use 100,000 geolocated events as train set and 100,000 geolocated events at Test set.",
                    "label": 0
                },
                {
                    "sent": "We trained both our models using different bandwidth values in K and compute the average log likelihood of the events in the test set.",
                    "label": 1
                },
                {
                    "sent": "Now, in terms of average log likelihood, high results mean better fit, so our intuition tells us that we want the adaptive method to get higher average log likelihood than the fixed method.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, let's look at the fixed bandwidth methods on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Here we see different bandwidth values and the Y axis show us the average log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now, as we can see when going from large again can see my laser.",
                    "label": 0
                },
                {
                    "sent": "From going from large values to small ones, we're getting better fit up to a certain point where we starting to overfit the data and their results go down.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the adaptive method, we see that we're getting can almost consistently higher results, except for the case where K equal to in terms of average log likelihood, which is exactly what our intuition told us.",
                    "label": 0
                },
                {
                    "sent": "We're going to get.",
                    "label": 0
                },
                {
                    "sent": "This is the result of being able to obviously being able to fit different types of location at the same time.",
                    "label": 0
                },
                {
                    "sent": "As we saw earlier in the images.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the next step in our work is to fit those models to individual level human location data such as we see here.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This will allow us to perform both analysis and inference on individual location.",
                    "label": 0
                },
                {
                    "sent": "The problem is that it is not as trivial as just applying those model to this data that we have here.",
                    "label": 0
                },
                {
                    "sent": "What we can see here a set of observed events in a small area next to the City of Riverside.",
                    "label": 0
                },
                {
                    "sent": "Now it's important to understand two things.",
                    "label": 0
                },
                {
                    "sent": "One this is not fake data.",
                    "label": 0
                },
                {
                    "sent": "This is data that comes from a real individual in the area of South California and 2nd.",
                    "label": 0
                },
                {
                    "sent": "This is not a rare case.",
                    "label": 0
                },
                {
                    "sent": "This is how the data looks like for most of the individual in the data set.",
                    "label": 0
                },
                {
                    "sent": "Now fitting the the model to this data results in the following density function of all we can see here is a heat map on top of the salary California map were red indicates higher likelihood in blue indicates lower likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with this model is that it gives the same likelihood for event in the area of Los Angeles as it gives the area the events in the middle of the desert or in the ocean.",
                    "label": 0
                },
                {
                    "sent": "Now this is super counter intuitive, 'cause as we could expect, and we know that LA is a very popular area, so it's more likely for the individual to go there then to the middle of the desert or to the ocean.",
                    "label": 0
                },
                {
                    "sent": "So the question that comes into mind is what if there is a way to interpolate between the information that we have about popularities of areas to this density function?",
                    "label": 0
                },
                {
                    "sent": "And again, the answer is yes, and this is our proposed.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kernel densities model in R model.",
                    "label": 0
                },
                {
                    "sent": "We're not only looking at the individual density, we're also looking at the population density as a whole.",
                    "label": 0
                },
                {
                    "sent": "We then combine the two in a way that manner to achieve that.",
                    "label": 0
                },
                {
                    "sent": "Make sure that we see here.",
                    "label": 0
                },
                {
                    "sent": "Now again, we see that most of the distribution is concentrated around the area of Riverside, but now the rest of the density is distributed around popular area like the Ellee County.",
                    "label": 0
                },
                {
                    "sent": "The area of San Diego and even the road to Vegas, and we are no longer wasting density on the ocean around the desert.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to generalize, our mixture model is a weighted sum of density components for each component reference scale of activity in human location data, and this is unlike Gaussian mixture model, for example, where each component represent different area in the space.",
                    "label": 1
                },
                {
                    "sent": "Now as we sign the example, the first component is usually set to be the individual density and the last one is set to be the population density.",
                    "label": 0
                },
                {
                    "sent": "But our model is not limited to only two 2 components.",
                    "label": 0
                },
                {
                    "sent": "We actually can introduce any other density component to it between the individual and population.",
                    "label": 0
                },
                {
                    "sent": "That represent a scale of activity between the two, like City, region, counties, never workplace.",
                    "label": 0
                },
                {
                    "sent": "The training of the mall is done like any other mixture model where we take a data set and split it into a training set and a validation set and we can use that to compute the mixing weight using the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "An optimize over the parameter of the bandwidth an K.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's talk about evaluation a little bit in order to validate our model and compare it to commonly used individual level baselines, we gathered gather human location data from location based social networks from the area of Southern California.",
                    "label": 0
                },
                {
                    "sent": "We use.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two about two approaches to evaluate our model and compare it to those model using log likelihood of test data that we saw earlier and tested for the application of fraud detection.",
                    "label": 0
                },
                {
                    "sent": "Now let's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Begin with the log likelihood test on data that gathered from Twitter.",
                    "label": 0
                },
                {
                    "sent": "We use around 2 million geolocated events from the month of July 2013.",
                    "label": 1
                },
                {
                    "sent": "As training, set an around 45,000 events from the following months as test set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We first computed the average log likelihood of our baseline using Gaussian Gaussian mixture model with two component and fixed KDE.",
                    "label": 0
                },
                {
                    "sent": "Now on the X axis we can see the different models and the Y axis shows us the average log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Now again as before average higher average log likelihood stands for better fit.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared our model to adaptive Katie model that was trained on the entire population data and actually achieved better results in the individual models.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When looking at the results of our model, we can see that we achieved far better results than any other baseline, which is somewhat impressive.",
                    "label": 0
                },
                {
                    "sent": "Now, to make sure we didn't just create a very, very good Twitter model, we also prefer.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm the same evaluation on data that was gathered from Goala using a similar setup.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now again are mixed.",
                    "label": 0
                },
                {
                    "sent": "Katie model achieved far better results in the old.",
                    "label": 0
                },
                {
                    "sent": "The other individual baselines in terms of average log likelihood on test data.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now another way to validate our model is to test its application in the fraud detection system.",
                    "label": 0
                },
                {
                    "sent": "In order to do that, we can evaluate its ability to detect spatial anomalies in individual location behavior.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to do that, we took the Twitter data set an we introduce something that is called simulated fraud cases by taking random individuals and replacing their test data with some other random individuals data.",
                    "label": 0
                },
                {
                    "sent": "This where I valuation data set has both control group Anna simulated fraud cases.",
                    "label": 0
                },
                {
                    "sent": "Now for eval.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location we rank the individual according to the average negative log likelihood, where higher results mean spatial anomaly.",
                    "label": 0
                },
                {
                    "sent": "Now basically what we want to see is that the individual from the simulated fraud cases will be ranked higher than the individual from the control group.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we compared our model with the same baseline Gaussian Gaussian mixture model with two components and fixed KDE.",
                    "label": 0
                },
                {
                    "sent": "We computed the average negative log likelihood of five test events and then looked at how many individual from the simulated fraud cases are within the top 20 ranked individual.",
                    "label": 0
                },
                {
                    "sent": "This is called precision on 20.",
                    "label": 0
                },
                {
                    "sent": "Now the Y axis here shows the precision at 20 across 50 runs.",
                    "label": 0
                },
                {
                    "sent": "So basically again we want our model to achieve better results on the Y axis, 'cause that will represent that more individual from the simulated fraud cases were ranked higher than the individual.",
                    "label": 0
                },
                {
                    "sent": "The control group and not only.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we achieved that.",
                    "label": 0
                },
                {
                    "sent": "In fact, our mixed Katie model achieved twice as much result as they are their baselines, which actually indicates that our fraud detection system can benefit a lot from using our model.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, to conclude, we conducted a systematic study of spatial density estimation for social media data.",
                    "label": 1
                },
                {
                    "sent": "We also investigated adaptive bandwidth methods for kernel density estimations and most importantly we introduced a multi level mixtures of density models.",
                    "label": 0
                },
                {
                    "sent": "Now where are we going?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here there are a couple of possible direction for future work.",
                    "label": 0
                },
                {
                    "sent": "The first we can generalize the mixture model by using weighted KDE, where the weights will represent the similarities between the individual which are currently represented by the different components.",
                    "label": 1
                },
                {
                    "sent": "And 2nd we can adapt our model to work in an online fashion, updating the model with new data and also learning a decay and function as a function of time and the data itself.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'd like to thank the National Science Foundation and the Xerox Corporation for supporting this project.",
                    "label": 0
                },
                {
                    "sent": "Allow me to work on it and also my lab mates for providing me with a moral and intellectual support.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}