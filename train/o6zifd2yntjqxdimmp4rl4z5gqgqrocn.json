{
    "id": "o6zifd2yntjqxdimmp4rl4z5gqgqrocn",
    "title": "Robust Bounds for Classification via Selective Sampling",
    "info": {
        "author": [
            "Francesco Orabona, Toyota Technological Institute at Chicago"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_orabona_rbcs/",
    "segmentation": [
        [
            "OK, so good morning everybody.",
            "I'm precious, Carbone and this is a joint work with an ecological banking.",
            "Claudio Jen Taylor.",
            "I'll start my talk introducing you."
        ],
        [
            "Problem of active learning and while this is an important problem, So what is active learning active learning algorithms basically are able to select some labels, some simple to label in order to gain a significant boost over batch algorithm and why they are important.",
            "They are important because usually it's quite cheap to gather a lot of unlabeled data.",
            "But on the other end usually it's expensive to label this data.",
            "However, most of the previous works on active learning just focus on the case in which the instances are drawn, IID from a fixed distribution, and if you attended."
        ],
        [
            "I see ML tutorial three days ago.",
            "By the time Langford, this is exactly the question that they posed in."
        ],
        [
            "Unless the slide, is it possible to remove this hypothesis?",
            "It's possible to have active learning in a fully adversarial setting and work that I'll present you.",
            "It's exactly a step in this direction."
        ],
        [
            "So this is the outline of the talk.",
            "I will present you first the problem and the hypothesis that I have to make in order to solve the problem, and then I'll present you a new family of algorithm called BQ that is bound on Via Square.",
            "And Lastly I'll present you some experimental results."
        ],
        [
            "Let's start from the problem definition.",
            "And I'll focus on a particular kind of active learning."
        ],
        [
            "Setting that is the selective sampling scenario.",
            "In this scenario, the learning receives an instance at each time step.",
            "And then it tried to predict it after the prediction, the learner may observe or not label.",
            "It can serve the label only if issuing a query and if it doesn't issue a query, it will the label will remain unknown.",
            "In this setting the aim of the learner is to maximize its performance, but at the same time to minimize the number of queried labels.",
            "And then."
        ],
        [
            "Important hypothesis that I will do for the entire talk or lack of hypothesis that we don't use the IID hypothesis on the on the on the samples."
        ],
        [
            "So most of the previous workers I said before focus just on the case where distances are drawn IID from the fixed distribution and there are just.",
            "There are some exception to important one or the first one is the work by some Bianchi genteel zaniboni that is completely worst case.",
            "However, they were not able to prove any bounce on the label query rate.",
            "Another important work is the quick model that was introduced at the last ICML and won the best paper award, and the goal there.",
            "Is to approximate the biased margin within a given accuracy epsilon, and they assume an arbitrary sequence of instances and they have also a linear stochastic model for the labels and with this hypothesis there they are able to compete again against an adaptive adversarial strategy asking only queries of the order of the D to the third power divided by epsilon to the 4th power, where epsilon is the precision that we asked to the algorithm, and D is the dimensionality of the input space.",
            "Now in our work, we'll consider a setting that is really close to this one.",
            "In particular, we will use."
        ],
        [
            "The same label noise model and all the results proven here will hold for any fixed at individual sequence of samples.",
            "And the label noise model.",
            "Basically we consider each label as the realization of a stochastic variable and the expected value of the stochastic variable is the inner product between an unknown fixed vector view and the instance and given this noise model, we have that it's very simple to see that the bias optimal classifier is given by the sign of the inner product between you and the instance.",
            "Now this is just it's true that.",
            "This is just a linear noise model, but he can make make can be made Eileen or linear through the use of kernel functions."
        ],
        [
            "Now let's move to to the algorithm."
        ],
        [
            "Ola Ola, our algorithm will be based on regularised least square, and in particular a well known variant of the regularize least square in which the current simple is always included in the formula to estimate the hyperplane.",
            "So basically at each time step we will we will use all the instances in which a query was done, plus the current samples.",
            "So all the instances are there in this matrix S and occurrence instances.",
            "EXO exo T. And our algorithm."
        ],
        [
            "It's just a very simple modification of the regularizers square.",
            "And this is our algorithm BBQ.",
            "So at each step step we observe an instance, then we predict.",
            "The margin with the regularizer least square and then we use the sign and then there is the key point.",
            "The most important part of the algorithm that we calculate this quantity error T and we use this for our query condition.",
            "If this quantity is greater than T to the minus K, where K is a parameter, we care we query the label, otherwise we don't wear the label and it will remain unknown.",
            "Now regularize least square.",
            "Kimberly obviously formulated the indoor variables and the same for this for this quantity.",
            "Here are OT.",
            "So it means that the entire algorithm can be used with kernels Italy.",
            "And the space and time complexity to predict and to update its square in the dimensionality of the input space for the primal passion and this square in the number of queried labels for the dual version.",
            "So for this algorithm we are able to prove the follow."
        ],
        [
            "Going to regret bound.",
            "For any K between zero and one after any number of step, any number of T steps, direct threat is less than the minimum of that quantity.",
            "Where we have 2.2 basically two quantities, the first one is epsilon multiplied by T of epsilon, where T of epsilon is the number of samples that have.",
            "The absolute values of the bias margin less than epsilon.",
            "So basically these are the difficult samples and this this this number is completely controlled by the adversary because he chooses which instance to give us the other term instead is basically the regret on these samples.",
            "The one that have the true margin greater than epsilon and that term is always logarithmic in T. And we are also able to prove that the number of queried labels it's always polynomial in T, where the exponent of the polynomial is exactly K and it's linear in D, where this dimensionality of the input space.",
            "Now what does it mean that basically with this algorithm we can choose any query rate.",
            "So for example, if we choose K equal to 0.5, the query rate will be over the order of square root of T and the only dependence of K in the regret will be in that in this term.",
            "Here that is a constant term, it does not depend on T. Another point is that I told you that this can be implemented with kernels and this quantity.",
            "Here D will be substituted by expected quantity.",
            "So the group that bond is not vacuous for fork."
        ],
        [
            "So how does it work?",
            "Let's return to the algorithm.",
            "And of course most important parties is here.",
            "Is this quantity ROT and the query condition, and it's a bit difficult to see it from this strange formula, but multiple eighting it a little bit.",
            "It's possible to see that arities related to the distance of the current samples from all the query the samples until that point.",
            "And why this quantity is important this quantity?"
        ],
        [
            "Is important cause?",
            "We can calculate in relation using Arrow T2 an upper bound on the bias and an upper bound on the variance of the regularizer least square estimate.",
            "So it means that every time Arity is small, the upper bound on the bias and the upper bound on the variance will be small.",
            "This means that the regularise least square estimate is close to the true one.",
            "And this implies that the algorithm can trust itself.",
            "So we can say, OK, I don't need this label because I am quite sure that I can.",
            "I already predict it in the right way.",
            "And an import."
        ],
        [
            "Point here.",
            "That simplifies a lot the proofs is the fact that Arity does not depend on the labels.",
            "So if we go back to the formula.",
            "Sorry.",
            "In narrative, there are just distances.",
            "This is the matrix of the query instances and this is the current simple.",
            "But there are no labels inside it.",
            "And this is similar to the approach used by Straylight Munford, a quick algorithm for regression.",
            "Now."
        ],
        [
            "Most of the technical tech.",
            "Maletis of the proof are due to the fact that in the in the final bound the final.",
            "The following regret depends on the optimal choice of epsilon, but the algorithm doesn't know this optimal choice.",
            "So we quit.",
            "We can somehow simplify the task and we can say, OK, we don't want to minimize regret.",
            "We don't want to have.",
            "Is this as small as possible with the optimal epsilon?",
            "What we want just to approximate the buyer margin bias margin within a given precision epsilon.",
            "In this case epsilon is just a parameter and we can pass it to the algorithm.",
            "In this way we can design an algorithm that.",
            "But it does not query the label, it will always give a prediction that is within an error of epsilon with the the bias margin with high probability and in this case the number of queries will be always logarithmic in time.",
            "And this is our second algorithm that we call."
        ],
        [
            "Parametric DBQ and you see that in this case epsilon is a parameter.",
            "And it will be used in the query condition and basically the algorithm is.",
            "It's the same as before.",
            "There is, we observe distance, we predict it with the regularised least square, and then there are there is the same quantity of before I wrote in.",
            "There are two other quantities that are related to our OT and you can see also from the format that they are quite similar and all of them are used to in the query condition.",
            "And for this algorithm."
        ],
        [
            "We are able to prove that.",
            "For every epsilon and Delta between zero and one with probability 1 minus Delta on all the time, steps were no query is issued.",
            "The prediction the bias the prediction is close to the to the bias margin with the maximum error of epsilon and the number of queries is logarithmic in the time and it depends as D divided by epsilon square.",
            "Again you can substitute D with this particular quantity fuse kernels.",
            "So this is a huge improvement on the bound of quick because before it was.",
            "B to the third power divided by epsilon to the first power.",
            "However, we introduce a logarithmic dependence on T that is not present on quick and we could ask if."
        ],
        [
            "It is possible to improve even more this bound."
        ],
        [
            "And, well, the answer is no, because we proved a lower bound that says that basically our bound is optimal up to logarithmic factors.",
            "In fact, at least Omega D divided by Epsilon square labels queries are needed to learn any target hyperplane with arbitrarily small accuracy and arbitrarily high confidence."
        ],
        [
            "So we can now move to the experiments.",
            "All the preliminary experiments."
        ],
        [
            "Love.",
            "So in the first experiment, that is a synthetic experiment.",
            "We tested our second algorithm, the parametric DBQ, and we generated 10,000 random examples on the unit circle in two dimensions.",
            "And the labels were generated according to our noise model and with the random hyperplane with the unit unit norm and then we run the our algorithm with a different setting of epsilon.",
            "This year and with the just one setting of Delta that it was 0.1 and.",
            "We have plotted the maximum error in blue.",
            "All all over the samples in which we didn't query the label.",
            "That is the steps in which the algorithm saying OK, I know it.",
            "I don't know.",
            "I don't need the label and the maximum error for every setting of epsilon.",
            "It's always less than the theoretical one.",
            "So for example, for Epsilon 0.4, the error is less than zero point 4.",
            "You can see that it is here.",
            "At the same time, we have blocked the the number of queried labels.",
            "In green, here is a fraction of the number of symbols and you can see that this follows the the low predicted by the theory that it's it's more or less one over epsilon square.",
            "Now, the important point that I want to underline here is that.",
            "We all know that the usually the bounds are are pretty loose, so in this case the bus would have predicted that all over the 10,000 samples, all of them must be queried.",
            "And this is not the case in practice.",
            "So for example, with the epsilon equal to 0.3, we are acquiring just 10% of the 10,000 samples.",
            "So."
        ],
        [
            "Let's move now to the preliminary real world experiments.",
            "We tested our algorithm, two different data sets.",
            "The first one is LC-1 and the second one is adult.",
            "Unless you want to use the linear kernel and or not.",
            "But we do use the occasion kernels and we we changed.",
            "Our parameter in order to have a different fraction of a queried labels.",
            "And then we plot the fractional query labels versus the F measure at the end of the training.",
            "And you can see, and we compared our algorithm parametric BQ.",
            "That is this line here with the two other baseline.",
            "The first one is is the 2nd order label efficient?",
            "By Chester Bianca Tool and the other one is just a random baseline that is querying simples just at random with a fixed probability.",
            "And, well, the results are not exactly clear because on we can we can say for sure that our algorithm is better than the random strategy of just sampling at random.",
            "But on the on the Earth we won.",
            "And the 2nd order level efficient is better while on adult their performance basically are close one to the other one and we still don't know if this is due to the use of kernels or to the fact that the database are really different one from the other.",
            "Adult is it's a dense feature as dense feature will ever see one is really sparse, so.",
            "In the future, we want to really understand what we want to focus on the on the real performance of the of the.",
            "Algorithm."
        ],
        [
            "So let's summarize.",
            "We've introduced a new family of online algorithm, the BBQ family for selecting simple under oblivious adversarial environments.",
            "And for this algorithm is possible to go from fully supervised to fully unsupervised learning.",
            "Because you have parameter and you can choose your query rate and parametric because the second one is designed to work in a week and quick framework with an improved bounds on the number of square it labels as work in progress we want to.",
            "Extend the algorithm to work with an adaptive adversary, and we want to improve the bound on the number of queried labels to remove the logarithmic dependence on time in a way to match our lower bound.",
            "So that's all, thanks."
        ],
        [
            "Present."
        ],
        [
            "Your setting is potentially very sorry, right?",
            "And this isn't true for the query is deterministic, yes.",
            "In the paper, well, I didn't present it, but in the paper there is also a stochastic version of the algorithm with a similar bound, however, also in that case we are.",
            "Well yeah, it's only for the first algorithm.",
            "The stochastic variable we didn't try for the second one.",
            "Probably it would help, yes.",
            "Jack well, I think the experiments suggest us that probably.",
            "The noise model could be a weak point.",
            "I don't know from from the theoretical point of view, from from the bounds.",
            "If we can change the hypothesis in this way, I know that there is a work of of Littman that extended the quick framework to allow a little bit of of noise in the in the in the processes of the label, so maybe we can.",
            "We can see it that work as possible."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "I'm precious, Carbone and this is a joint work with an ecological banking.",
                    "label": 0
                },
                {
                    "sent": "Claudio Jen Taylor.",
                    "label": 0
                },
                {
                    "sent": "I'll start my talk introducing you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem of active learning and while this is an important problem, So what is active learning active learning algorithms basically are able to select some labels, some simple to label in order to gain a significant boost over batch algorithm and why they are important.",
                    "label": 0
                },
                {
                    "sent": "They are important because usually it's quite cheap to gather a lot of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "But on the other end usually it's expensive to label this data.",
                    "label": 0
                },
                {
                    "sent": "However, most of the previous works on active learning just focus on the case in which the instances are drawn, IID from a fixed distribution, and if you attended.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I see ML tutorial three days ago.",
                    "label": 0
                },
                {
                    "sent": "By the time Langford, this is exactly the question that they posed in.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unless the slide, is it possible to remove this hypothesis?",
                    "label": 0
                },
                {
                    "sent": "It's possible to have active learning in a fully adversarial setting and work that I'll present you.",
                    "label": 0
                },
                {
                    "sent": "It's exactly a step in this direction.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will present you first the problem and the hypothesis that I have to make in order to solve the problem, and then I'll present you a new family of algorithm called BQ that is bound on Via Square.",
                    "label": 0
                },
                {
                    "sent": "And Lastly I'll present you some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start from the problem definition.",
                    "label": 0
                },
                {
                    "sent": "And I'll focus on a particular kind of active learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting that is the selective sampling scenario.",
                    "label": 1
                },
                {
                    "sent": "In this scenario, the learning receives an instance at each time step.",
                    "label": 1
                },
                {
                    "sent": "And then it tried to predict it after the prediction, the learner may observe or not label.",
                    "label": 1
                },
                {
                    "sent": "It can serve the label only if issuing a query and if it doesn't issue a query, it will the label will remain unknown.",
                    "label": 0
                },
                {
                    "sent": "In this setting the aim of the learner is to maximize its performance, but at the same time to minimize the number of queried labels.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Important hypothesis that I will do for the entire talk or lack of hypothesis that we don't use the IID hypothesis on the on the on the samples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So most of the previous workers I said before focus just on the case where distances are drawn IID from the fixed distribution and there are just.",
                    "label": 0
                },
                {
                    "sent": "There are some exception to important one or the first one is the work by some Bianchi genteel zaniboni that is completely worst case.",
                    "label": 0
                },
                {
                    "sent": "However, they were not able to prove any bounce on the label query rate.",
                    "label": 0
                },
                {
                    "sent": "Another important work is the quick model that was introduced at the last ICML and won the best paper award, and the goal there.",
                    "label": 0
                },
                {
                    "sent": "Is to approximate the biased margin within a given accuracy epsilon, and they assume an arbitrary sequence of instances and they have also a linear stochastic model for the labels and with this hypothesis there they are able to compete again against an adaptive adversarial strategy asking only queries of the order of the D to the third power divided by epsilon to the 4th power, where epsilon is the precision that we asked to the algorithm, and D is the dimensionality of the input space.",
                    "label": 0
                },
                {
                    "sent": "Now in our work, we'll consider a setting that is really close to this one.",
                    "label": 0
                },
                {
                    "sent": "In particular, we will use.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same label noise model and all the results proven here will hold for any fixed at individual sequence of samples.",
                    "label": 0
                },
                {
                    "sent": "And the label noise model.",
                    "label": 0
                },
                {
                    "sent": "Basically we consider each label as the realization of a stochastic variable and the expected value of the stochastic variable is the inner product between an unknown fixed vector view and the instance and given this noise model, we have that it's very simple to see that the bias optimal classifier is given by the sign of the inner product between you and the instance.",
                    "label": 0
                },
                {
                    "sent": "Now this is just it's true that.",
                    "label": 0
                },
                {
                    "sent": "This is just a linear noise model, but he can make make can be made Eileen or linear through the use of kernel functions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's move to to the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ola Ola, our algorithm will be based on regularised least square, and in particular a well known variant of the regularize least square in which the current simple is always included in the formula to estimate the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So basically at each time step we will we will use all the instances in which a query was done, plus the current samples.",
                    "label": 0
                },
                {
                    "sent": "So all the instances are there in this matrix S and occurrence instances.",
                    "label": 0
                },
                {
                    "sent": "EXO exo T. And our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's just a very simple modification of the regularizers square.",
                    "label": 0
                },
                {
                    "sent": "And this is our algorithm BBQ.",
                    "label": 0
                },
                {
                    "sent": "So at each step step we observe an instance, then we predict.",
                    "label": 0
                },
                {
                    "sent": "The margin with the regularizer least square and then we use the sign and then there is the key point.",
                    "label": 1
                },
                {
                    "sent": "The most important part of the algorithm that we calculate this quantity error T and we use this for our query condition.",
                    "label": 0
                },
                {
                    "sent": "If this quantity is greater than T to the minus K, where K is a parameter, we care we query the label, otherwise we don't wear the label and it will remain unknown.",
                    "label": 0
                },
                {
                    "sent": "Now regularize least square.",
                    "label": 0
                },
                {
                    "sent": "Kimberly obviously formulated the indoor variables and the same for this for this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here are OT.",
                    "label": 1
                },
                {
                    "sent": "So it means that the entire algorithm can be used with kernels Italy.",
                    "label": 1
                },
                {
                    "sent": "And the space and time complexity to predict and to update its square in the dimensionality of the input space for the primal passion and this square in the number of queried labels for the dual version.",
                    "label": 0
                },
                {
                    "sent": "So for this algorithm we are able to prove the follow.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to regret bound.",
                    "label": 0
                },
                {
                    "sent": "For any K between zero and one after any number of step, any number of T steps, direct threat is less than the minimum of that quantity.",
                    "label": 0
                },
                {
                    "sent": "Where we have 2.2 basically two quantities, the first one is epsilon multiplied by T of epsilon, where T of epsilon is the number of samples that have.",
                    "label": 0
                },
                {
                    "sent": "The absolute values of the bias margin less than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So basically these are the difficult samples and this this this number is completely controlled by the adversary because he chooses which instance to give us the other term instead is basically the regret on these samples.",
                    "label": 0
                },
                {
                    "sent": "The one that have the true margin greater than epsilon and that term is always logarithmic in T. And we are also able to prove that the number of queried labels it's always polynomial in T, where the exponent of the polynomial is exactly K and it's linear in D, where this dimensionality of the input space.",
                    "label": 0
                },
                {
                    "sent": "Now what does it mean that basically with this algorithm we can choose any query rate.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we choose K equal to 0.5, the query rate will be over the order of square root of T and the only dependence of K in the regret will be in that in this term.",
                    "label": 0
                },
                {
                    "sent": "Here that is a constant term, it does not depend on T. Another point is that I told you that this can be implemented with kernels and this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here D will be substituted by expected quantity.",
                    "label": 0
                },
                {
                    "sent": "So the group that bond is not vacuous for fork.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "Let's return to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And of course most important parties is here.",
                    "label": 0
                },
                {
                    "sent": "Is this quantity ROT and the query condition, and it's a bit difficult to see it from this strange formula, but multiple eighting it a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's possible to see that arities related to the distance of the current samples from all the query the samples until that point.",
                    "label": 0
                },
                {
                    "sent": "And why this quantity is important this quantity?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is important cause?",
                    "label": 0
                },
                {
                    "sent": "We can calculate in relation using Arrow T2 an upper bound on the bias and an upper bound on the variance of the regularizer least square estimate.",
                    "label": 1
                },
                {
                    "sent": "So it means that every time Arity is small, the upper bound on the bias and the upper bound on the variance will be small.",
                    "label": 0
                },
                {
                    "sent": "This means that the regularise least square estimate is close to the true one.",
                    "label": 0
                },
                {
                    "sent": "And this implies that the algorithm can trust itself.",
                    "label": 0
                },
                {
                    "sent": "So we can say, OK, I don't need this label because I am quite sure that I can.",
                    "label": 0
                },
                {
                    "sent": "I already predict it in the right way.",
                    "label": 0
                },
                {
                    "sent": "And an import.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point here.",
                    "label": 0
                },
                {
                    "sent": "That simplifies a lot the proofs is the fact that Arity does not depend on the labels.",
                    "label": 1
                },
                {
                    "sent": "So if we go back to the formula.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "In narrative, there are just distances.",
                    "label": 1
                },
                {
                    "sent": "This is the matrix of the query instances and this is the current simple.",
                    "label": 0
                },
                {
                    "sent": "But there are no labels inside it.",
                    "label": 0
                },
                {
                    "sent": "And this is similar to the approach used by Straylight Munford, a quick algorithm for regression.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Most of the technical tech.",
                    "label": 1
                },
                {
                    "sent": "Maletis of the proof are due to the fact that in the in the final bound the final.",
                    "label": 1
                },
                {
                    "sent": "The following regret depends on the optimal choice of epsilon, but the algorithm doesn't know this optimal choice.",
                    "label": 0
                },
                {
                    "sent": "So we quit.",
                    "label": 0
                },
                {
                    "sent": "We can somehow simplify the task and we can say, OK, we don't want to minimize regret.",
                    "label": 1
                },
                {
                    "sent": "We don't want to have.",
                    "label": 0
                },
                {
                    "sent": "Is this as small as possible with the optimal epsilon?",
                    "label": 0
                },
                {
                    "sent": "What we want just to approximate the buyer margin bias margin within a given precision epsilon.",
                    "label": 0
                },
                {
                    "sent": "In this case epsilon is just a parameter and we can pass it to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "In this way we can design an algorithm that.",
                    "label": 0
                },
                {
                    "sent": "But it does not query the label, it will always give a prediction that is within an error of epsilon with the the bias margin with high probability and in this case the number of queries will be always logarithmic in time.",
                    "label": 0
                },
                {
                    "sent": "And this is our second algorithm that we call.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parametric DBQ and you see that in this case epsilon is a parameter.",
                    "label": 0
                },
                {
                    "sent": "And it will be used in the query condition and basically the algorithm is.",
                    "label": 0
                },
                {
                    "sent": "It's the same as before.",
                    "label": 0
                },
                {
                    "sent": "There is, we observe distance, we predict it with the regularised least square, and then there are there is the same quantity of before I wrote in.",
                    "label": 0
                },
                {
                    "sent": "There are two other quantities that are related to our OT and you can see also from the format that they are quite similar and all of them are used to in the query condition.",
                    "label": 0
                },
                {
                    "sent": "And for this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are able to prove that.",
                    "label": 0
                },
                {
                    "sent": "For every epsilon and Delta between zero and one with probability 1 minus Delta on all the time, steps were no query is issued.",
                    "label": 0
                },
                {
                    "sent": "The prediction the bias the prediction is close to the to the bias margin with the maximum error of epsilon and the number of queries is logarithmic in the time and it depends as D divided by epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Again you can substitute D with this particular quantity fuse kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is a huge improvement on the bound of quick because before it was.",
                    "label": 0
                },
                {
                    "sent": "B to the third power divided by epsilon to the first power.",
                    "label": 0
                },
                {
                    "sent": "However, we introduce a logarithmic dependence on T that is not present on quick and we could ask if.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is possible to improve even more this bound.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And, well, the answer is no, because we proved a lower bound that says that basically our bound is optimal up to logarithmic factors.",
                    "label": 0
                },
                {
                    "sent": "In fact, at least Omega D divided by Epsilon square labels queries are needed to learn any target hyperplane with arbitrarily small accuracy and arbitrarily high confidence.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can now move to the experiments.",
                    "label": 0
                },
                {
                    "sent": "All the preliminary experiments.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "So in the first experiment, that is a synthetic experiment.",
                    "label": 0
                },
                {
                    "sent": "We tested our second algorithm, the parametric DBQ, and we generated 10,000 random examples on the unit circle in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And the labels were generated according to our noise model and with the random hyperplane with the unit unit norm and then we run the our algorithm with a different setting of epsilon.",
                    "label": 0
                },
                {
                    "sent": "This year and with the just one setting of Delta that it was 0.1 and.",
                    "label": 0
                },
                {
                    "sent": "We have plotted the maximum error in blue.",
                    "label": 0
                },
                {
                    "sent": "All all over the samples in which we didn't query the label.",
                    "label": 0
                },
                {
                    "sent": "That is the steps in which the algorithm saying OK, I know it.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't need the label and the maximum error for every setting of epsilon.",
                    "label": 0
                },
                {
                    "sent": "It's always less than the theoretical one.",
                    "label": 0
                },
                {
                    "sent": "So for example, for Epsilon 0.4, the error is less than zero point 4.",
                    "label": 0
                },
                {
                    "sent": "You can see that it is here.",
                    "label": 0
                },
                {
                    "sent": "At the same time, we have blocked the the number of queried labels.",
                    "label": 0
                },
                {
                    "sent": "In green, here is a fraction of the number of symbols and you can see that this follows the the low predicted by the theory that it's it's more or less one over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Now, the important point that I want to underline here is that.",
                    "label": 0
                },
                {
                    "sent": "We all know that the usually the bounds are are pretty loose, so in this case the bus would have predicted that all over the 10,000 samples, all of them must be queried.",
                    "label": 0
                },
                {
                    "sent": "And this is not the case in practice.",
                    "label": 0
                },
                {
                    "sent": "So for example, with the epsilon equal to 0.3, we are acquiring just 10% of the 10,000 samples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's move now to the preliminary real world experiments.",
                    "label": 0
                },
                {
                    "sent": "We tested our algorithm, two different data sets.",
                    "label": 1
                },
                {
                    "sent": "The first one is LC-1 and the second one is adult.",
                    "label": 0
                },
                {
                    "sent": "Unless you want to use the linear kernel and or not.",
                    "label": 0
                },
                {
                    "sent": "But we do use the occasion kernels and we we changed.",
                    "label": 0
                },
                {
                    "sent": "Our parameter in order to have a different fraction of a queried labels.",
                    "label": 1
                },
                {
                    "sent": "And then we plot the fractional query labels versus the F measure at the end of the training.",
                    "label": 0
                },
                {
                    "sent": "And you can see, and we compared our algorithm parametric BQ.",
                    "label": 0
                },
                {
                    "sent": "That is this line here with the two other baseline.",
                    "label": 0
                },
                {
                    "sent": "The first one is is the 2nd order label efficient?",
                    "label": 0
                },
                {
                    "sent": "By Chester Bianca Tool and the other one is just a random baseline that is querying simples just at random with a fixed probability.",
                    "label": 0
                },
                {
                    "sent": "And, well, the results are not exactly clear because on we can we can say for sure that our algorithm is better than the random strategy of just sampling at random.",
                    "label": 0
                },
                {
                    "sent": "But on the on the Earth we won.",
                    "label": 0
                },
                {
                    "sent": "And the 2nd order level efficient is better while on adult their performance basically are close one to the other one and we still don't know if this is due to the use of kernels or to the fact that the database are really different one from the other.",
                    "label": 0
                },
                {
                    "sent": "Adult is it's a dense feature as dense feature will ever see one is really sparse, so.",
                    "label": 0
                },
                {
                    "sent": "In the future, we want to really understand what we want to focus on the on the real performance of the of the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's summarize.",
                    "label": 0
                },
                {
                    "sent": "We've introduced a new family of online algorithm, the BBQ family for selecting simple under oblivious adversarial environments.",
                    "label": 0
                },
                {
                    "sent": "And for this algorithm is possible to go from fully supervised to fully unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Because you have parameter and you can choose your query rate and parametric because the second one is designed to work in a week and quick framework with an improved bounds on the number of square it labels as work in progress we want to.",
                    "label": 0
                },
                {
                    "sent": "Extend the algorithm to work with an adaptive adversary, and we want to improve the bound on the number of queried labels to remove the logarithmic dependence on time in a way to match our lower bound.",
                    "label": 0
                },
                {
                    "sent": "So that's all, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Present.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your setting is potentially very sorry, right?",
                    "label": 0
                },
                {
                    "sent": "And this isn't true for the query is deterministic, yes.",
                    "label": 0
                },
                {
                    "sent": "In the paper, well, I didn't present it, but in the paper there is also a stochastic version of the algorithm with a similar bound, however, also in that case we are.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, it's only for the first algorithm.",
                    "label": 0
                },
                {
                    "sent": "The stochastic variable we didn't try for the second one.",
                    "label": 0
                },
                {
                    "sent": "Probably it would help, yes.",
                    "label": 0
                },
                {
                    "sent": "Jack well, I think the experiments suggest us that probably.",
                    "label": 0
                },
                {
                    "sent": "The noise model could be a weak point.",
                    "label": 0
                },
                {
                    "sent": "I don't know from from the theoretical point of view, from from the bounds.",
                    "label": 0
                },
                {
                    "sent": "If we can change the hypothesis in this way, I know that there is a work of of Littman that extended the quick framework to allow a little bit of of noise in the in the in the processes of the label, so maybe we can.",
                    "label": 0
                },
                {
                    "sent": "We can see it that work as possible.",
                    "label": 0
                }
            ]
        }
    }
}