{
    "id": "hytqps4vohk4p4svprcia7ycyafatf33",
    "title": "From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices",
    "info": {
        "author": [
            "Mathieu Salzmann, NICTA, Australia's ICT Research Centre of Excellence"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2014_salzmann_dimensionality_reduction/",
    "segmentation": [
        [
            "So surprisingly, this talk is not going to be about deep learning where quite shallow people.",
            "So this talk is about dimensionality reduction for symmetric positive definite matrices so."
        ],
        [
            "Of all, why do?"
        ],
        [
            "Care about SPD bases and so SPD?",
            "Matrices appear in several computer vision problems, and here in particular we focus on the problem of visual recognition, and for this task region covariance descriptors are proven to be quite powerful representations, so reaching covariance descriptors are computed by extracting features at different locations in an image and then compute the covariance matrix of these features, and so if you have enough observations, then the resulting descriptor is an SPD matrix."
        ],
        [
            "So."
        ],
        [
            "If you want to make these descriptors more discriminative than you typically want to use high dimensional features in the 1st place.",
            "Now, unfortunately, if you have high dimensional features then you have very large descriptors and these descriptors can be very hard to handle.",
            "So."
        ],
        [
            "In this work, we propose to reduce the dimensionality of very large SPD matrices, and more precisely, given a set of images that belong to different classes, we represent each image with."
        ],
        [
            "SPD Ella High dimensional SPD matrix, where here the color indicates the class and we learn a mapping from."
        ],
        [
            "High dimensional matrices to lower dimensional SPD matrices such that the distance between these matrices is a better representative of the class label."
        ],
        [
            "One difficulty that arises when handling SPD matrices is that they form a remaining manifold and therefore they have a nonlinear structure.",
            "So this means that the Euclidean."
        ],
        [
            "Distance is not a very accurate measure to compare two matrices instead."
        ],
        [
            "One would like to rely on the geodesic distance, which is the length of the shortest curves that joins two matrices.",
            "So several methods have."
        ],
        [
            "Proposed to account for the non Euclidean geometry of SPD matrices and these methods typically follow two different approaches.",
            "Some of them rely on tension space approximation.",
            "So the approximate the manifold by Euclidean spaces either locally or globally and others map embed the SPD matrices to higher dimensional Hilbert space, where Euclidean geometry can apply 9."
        ],
        [
            "Cases."
        ],
        [
            "The problem is that the computational cost of these methods increases with the dimensionality of the SPD matrices, and so typically in practice people will use low dimensional descriptors that are potentially less discriminative.",
            "So instead here we propose to reduce the dimensionality of large SPD matrices while preserving or even increasing their discriminative power."
        ],
        [
            "Of course there's been other works on dimensionality reduction of SPD matrices.",
            "However, they typically adopt the following strategy so even high dimensional SPD matrices.",
            "Well, first map these matrices to a high dimensional Euclidean space, and then from these Euclidean Euclidean space that reduces the dimensionality to another Euclidean space.",
            "So this is for instance in the case of principle geodesic analysis, which is the generalization of PCA 2 remaining manifolds and PCM PGA first Maps of the matrices to the tangent space around the car show mean of the data.",
            "And so it's also the case of covariance, discriminative learning, which also approximates the manifold with its tangent space and then find the discriminative Euclidean subspace of the resulting Euclidean space."
        ],
        [
            "So here, in contrast, we propose to map high dimensional SPD matrices to a lower dimensional SPD manifold where the distance better encodes the notion of class labels and so are finite representation.",
            "Still are SPD matrices, so they still benefit from the same useful properties, and they can be used in conjunction with existing manifold based techniques.",
            "Now, before going into the details of our method, I will first review some concepts of the remaining geometry of SPD matrices."
        ],
        [
            "Would be useful for derivations, so first we'll rely on the notion of distance between two SPD matrices.",
            "An in practice will consider two different metrics on the manifold.",
            "The first metric is the affine invariant remaining metric that was introduced by panic, and colleagues in 2006, and that reduces geodesics on the manifold.",
            "The second metric is the Stein metric, and it has the advantage over the IRM of being faster to compute.",
            "Now in the paper we also show that these two metrics are related in the sense that the length of any given curve is the same under both metrics up to a scale factor.",
            "So in the remainder of this talk I will use the notation Delta to indicate the distance between two SPD matrices and this Delta can be obtained with either of these two metrics.",
            "Now these two metrics are."
        ],
        [
            "So have another interesting property which is that they are invariant with fine transformation.",
            "So if we consider two SPD matrices X&Y, and if we multiply these two."
        ],
        [
            "Matrices from left and right by a square invertible matrix M that is M in the general linear group then."
        ],
        [
            "The distance between the two resulting matrices is the same as the distance between the original matrices X&Y and so this is true for both metrics and will use this property in formulation."
        ],
        [
            "OK, so now let's go into the details of our approach.",
            "So recall that our goal is to map a high dimensional SPD matrix X through a lower dimensional 1F of X, so he."
        ],
        [
            "Here we parameterized this mapping with a rectangular matrix W, and we obtain the low dimensional SPD matrix by multiplying X from left and right with W as illustrated below.",
            "And so if."
        ],
        [
            "W has full rank, then F of X is also a symmetric positive definite matrix.",
            "So that would be fine mapping, but."
        ],
        [
            "Now we want also to define a cost function to learn the parameters of this mapping.",
            "So here we rely on the following intuition.",
            "We would like the low dimensional SPD manifold to have a distance that reflects the notion of affinity between the high dimensional SPD matrices.",
            "And so."
        ],
        [
            "For each pair of SPD matrices XI and XJ will ride cost gij, which is a product between an affinity measure AIJ and the distance between the two matrices in the low dimensional manifold.",
            "So this means that if we have a high affinity AIJ, then would like to have a small distance after mapping the matrices to the low dimensional manifold and contrastive hello or a negative affinity AIJ, then we're happy to have a high distance between the two matrices after mapping."
        ],
        [
            "So in practice will use the class labels to define the notion of affinity, and we'll set AIJ to one if XJ is a nearest neighbor of XI from the same class.",
            "So that means that our cost function will try to favor pulling together 2 samples that are of the same class.",
            "Any?"
        ],
        [
            "Interest will set AIJ to minus one if XJ is the nearest neighbor of XI from a different class.",
            "So that means that we will try.",
            "Our cost function will try to separate two matrices that are of different classes."
        ],
        [
            "And finally, the affinity will be set to 0 if XJ is not a nearest neighbor of Excitel.",
            "So now we."
        ],
        [
            "Find the cost function, but this is not enough because as said before, to have valid SPD manifold we also need W to have full rank.",
            "So here what we do is we enforce orthogonality constraints on W. Now this may seem like a tool restrictive assumption because in general not every full rank matrix is an orthogonal matrix, however."
        ],
        [
            "Ensure that these constraints entails no loss of generality and the reason is that any full rank matrix dubled can be decomposed into a product of an orthogonal matrix W and a matrix M in the general linear group.",
            "And so because."
        ],
        [
            "Those of you find invariants property about metrics.",
            "Then we'll have the cost function.",
            "Gij of WTO is exactly the same as the cost function JIJFW, so we can only enforce orthogonality constraints and not lose anything."
        ],
        [
            "So that we have a cost function and we have constraints on W so we can formulate our learning problem and so given a set of high dimensional SPD matrices, learning is expressed as minimizing the sum of all pairwise cost functions such that the resulting matrix W is orthogonal.",
            "And because of these orthogonality constraints, this can be a problem in optimization problem on either a Stifel or a Grassmann manifold, and the difference is that for the Grassmann manifold 2 matrices that are the same up to a rotation actually correspond to exactly the same point on a manifold.",
            "So to study whether we want to use a Stiefel Grassmann manifold optimization approach, we need to study the effect of rotations on our cost function."
        ],
        [
            "So if we take any rotation matrix are are you finding variance property of the metrics also yield that the cost function JIGW is exactly the same as the cost function of the rotated matrix W?",
            "So it means that we can make use."
        ],
        [
            "Of optimization methods on the Grassmann manifold and, in practice will exploit conjugate gradient descent methods on the manifold.",
            "OK, so now we have an approach that can map high dimensional SPD matrices to a lower dimensional SVD manifold now."
        ],
        [
            "On this low dimensional manifold we can still learn different classifiers to do visual recognition and so in our experiments we tried two different such classifiers.",
            "First one is a nearest neighbor classifier on the low dimensional manifold, and this classifier evidences the benefits of dimensionality reduction more clearly uterine simplicity.",
            "It also can exploit both the fine environment remaining metric and this time metric and the second classifier that we use is the RSR classifier.",
            "That was introduced by massage at ECB two years ago, and that classify explores the notion of sparse coding on the manifold.",
            "But it was designed for this time metrics or the results reported for this classifier will only rely on this time metric."
        ],
        [
            "So in our experiments we evaluated following methods first of all, with high dimensional SPD matrices, we evaluated the two nearest neighbor classifiers.",
            "We also evaluated the RSR classifier as well as the CDF method of Wang and colleagues.",
            "And then of course, after our dimensionality reduction, we evaluated the two nearest neighbor classifiers well and also the RSR classifier.",
            "So."
        ],
        [
            "As a first experiment, we tried our method on for the task on material recognition and to this, and we use the UIUC material data set that was introduced last year at CPR.",
            "This data set contains 18 classes and we using 10 different partition between training and test data.",
            "And the input to our algorithms are high dimensional matrices of size 155 by 155 that were computed as region covariance descriptors of SIFT features and color features.",
            "And the parameters of our approach were obtained by cross validation and these parameters are the dimensionality of the low dimensional manifold, the number of neighbors of the same class when building the affinity matrix, the number of neighbors of a different class when building the affinity matrix."
        ],
        [
            "So here are the results.",
            "So the first baseline is the was the result reporting in the paper that introduced the data set and it was 43.5% accuracy in terms of recognition.",
            "And then we evaluated CDL an RSR on our high dimensional SPD matrices.",
            "They both achieve about 52% accuracy and finally the nearest neighbor classifiers on high dimensional matrices only achieve about 35% accuracy.",
            "Now what is interesting is that."
        ],
        [
            "After reducing the dimensionality, if we apply the nearest neighbor classifiers then we increase the accuracy by more than 20%, so reach 58%, which means that we even outperformed the CDL and RSR classifiers that were computed on high dimensional matrices and so finally if we."
        ],
        [
            "We evaluate the RSI classifier on the low dimensional manifold.",
            "We reached the top accuracy of 66.6%."
        ],
        [
            "The second experiment we performed action recognition from motion capture data, and for this we use the HTML5 datasets.",
            "This data set contains 14 different actions an we use two subjects to train our method an three subjects to test it, and in this case there are no images.",
            "There are three joint location of human skeletons and so the inputs high dimensional SPD matrices are 93 by 93 dimensional and they are covariance descriptors of the 31, three joint location of the human skeleton.",
            "And the parameters over approach were again obtained by cross validation."
        ],
        [
            "So again, the results on these datasets here.",
            "CDL perform 79.8% accuracy, which is slightly higher than the RSR glass fire, and this is both in both cases.",
            "In high dimensional matrices the nearest neighbor classifiers both form that 60% accuracy and now again if we use."
        ],
        [
            "Nearest neighbor classifier.",
            "After dimensionality reduction, we increase the performance by several percentage.",
            "Now in this case, unfortunately we don't reach the performance of CDL and RSR, however, if we."
        ],
        [
            "Use RSR after dimensionality reduction.",
            "We reach the best accuracy of 81.9%."
        ],
        [
            "So to conclude, we have introduced an approach to mapping high dimensional SPD matrices to lower dimensional and more discriminative ones, and this yields a practical way to handle large SPD matrices with existing recognition algorithms, and so in the future we plan to extend a framework to using different cost functions and for instance to end of the unsupervised scenario or the semi supervised scenario an.",
            "We'd also like to study if the concept of reducing the dimensionality from one manifold to another manifold could apply to a different type of imagine manifolds."
        ],
        [
            "Thank you for your attention.",
            "So could you explain why you got better results after you produce the sparse smaller matrix well?",
            "So the intuition would be that because we also encouraging the low dimensional representation to be more discriminative than we already better reflect the notion of classes, so would we be the task would be easier for the classifier.",
            "Right?",
            "Here hello.",
            "Yes, so so.",
            "In effect you're building a new descriptor, right?",
            "So by by mapping to this new space.",
            "And so I'm wondering whether there is any intuitions that you have about the new descriptor.",
            "Yes, so so you can think of the new we discussing this in the paper, but we can think of the new descriptor in the case of covariance matrices.",
            "You can think of.",
            "The new descriptor has also been a covariance matrix in lower dimensional features.",
            "And so ultimately, yeah, this is.",
            "This is really the intuition behind this is that we will try to learn.",
            "You can think of it as having lower dimensional features that are going to be more discriminative when you consider the covariance matrices.",
            "So in relation to the previous question, maybe you're you get better results because you're mapping you take into account the distance between the classes.",
            "So if you try to do your mapping from one dimension to the same dimension.",
            "And see if that improves.",
            "So we did not try this, but so the baseline CDL this CDL baseline also tries to separate the classes in a related matter as well.",
            "We're doing so.",
            "In a sense, I think there's more to it than just that.",
            "I wonder how we did was bring mutations is the rows and columns of matrices are in correspondence are invariant to ruin computations?",
            "Yeah, I think you can show that it's also invariant to row and column permutation, and so, yeah, that's why it's only talked about rotation, but it's also true for permutation over one column.",
            "No more questions.",
            "If not, then let's thank our speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So surprisingly, this talk is not going to be about deep learning where quite shallow people.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about dimensionality reduction for symmetric positive definite matrices so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of all, why do?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Care about SPD bases and so SPD?",
                    "label": 0
                },
                {
                    "sent": "Matrices appear in several computer vision problems, and here in particular we focus on the problem of visual recognition, and for this task region covariance descriptors are proven to be quite powerful representations, so reaching covariance descriptors are computed by extracting features at different locations in an image and then compute the covariance matrix of these features, and so if you have enough observations, then the resulting descriptor is an SPD matrix.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want to make these descriptors more discriminative than you typically want to use high dimensional features in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "Now, unfortunately, if you have high dimensional features then you have very large descriptors and these descriptors can be very hard to handle.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this work, we propose to reduce the dimensionality of very large SPD matrices, and more precisely, given a set of images that belong to different classes, we represent each image with.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SPD Ella High dimensional SPD matrix, where here the color indicates the class and we learn a mapping from.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "High dimensional matrices to lower dimensional SPD matrices such that the distance between these matrices is a better representative of the class label.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One difficulty that arises when handling SPD matrices is that they form a remaining manifold and therefore they have a nonlinear structure.",
                    "label": 0
                },
                {
                    "sent": "So this means that the Euclidean.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distance is not a very accurate measure to compare two matrices instead.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One would like to rely on the geodesic distance, which is the length of the shortest curves that joins two matrices.",
                    "label": 0
                },
                {
                    "sent": "So several methods have.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proposed to account for the non Euclidean geometry of SPD matrices and these methods typically follow two different approaches.",
                    "label": 1
                },
                {
                    "sent": "Some of them rely on tension space approximation.",
                    "label": 0
                },
                {
                    "sent": "So the approximate the manifold by Euclidean spaces either locally or globally and others map embed the SPD matrices to higher dimensional Hilbert space, where Euclidean geometry can apply 9.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cases.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that the computational cost of these methods increases with the dimensionality of the SPD matrices, and so typically in practice people will use low dimensional descriptors that are potentially less discriminative.",
                    "label": 0
                },
                {
                    "sent": "So instead here we propose to reduce the dimensionality of large SPD matrices while preserving or even increasing their discriminative power.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course there's been other works on dimensionality reduction of SPD matrices.",
                    "label": 1
                },
                {
                    "sent": "However, they typically adopt the following strategy so even high dimensional SPD matrices.",
                    "label": 0
                },
                {
                    "sent": "Well, first map these matrices to a high dimensional Euclidean space, and then from these Euclidean Euclidean space that reduces the dimensionality to another Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So this is for instance in the case of principle geodesic analysis, which is the generalization of PCA 2 remaining manifolds and PCM PGA first Maps of the matrices to the tangent space around the car show mean of the data.",
                    "label": 0
                },
                {
                    "sent": "And so it's also the case of covariance, discriminative learning, which also approximates the manifold with its tangent space and then find the discriminative Euclidean subspace of the resulting Euclidean space.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, in contrast, we propose to map high dimensional SPD matrices to a lower dimensional SPD manifold where the distance better encodes the notion of class labels and so are finite representation.",
                    "label": 0
                },
                {
                    "sent": "Still are SPD matrices, so they still benefit from the same useful properties, and they can be used in conjunction with existing manifold based techniques.",
                    "label": 1
                },
                {
                    "sent": "Now, before going into the details of our method, I will first review some concepts of the remaining geometry of SPD matrices.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Would be useful for derivations, so first we'll rely on the notion of distance between two SPD matrices.",
                    "label": 0
                },
                {
                    "sent": "An in practice will consider two different metrics on the manifold.",
                    "label": 1
                },
                {
                    "sent": "The first metric is the affine invariant remaining metric that was introduced by panic, and colleagues in 2006, and that reduces geodesics on the manifold.",
                    "label": 1
                },
                {
                    "sent": "The second metric is the Stein metric, and it has the advantage over the IRM of being faster to compute.",
                    "label": 0
                },
                {
                    "sent": "Now in the paper we also show that these two metrics are related in the sense that the length of any given curve is the same under both metrics up to a scale factor.",
                    "label": 0
                },
                {
                    "sent": "So in the remainder of this talk I will use the notation Delta to indicate the distance between two SPD matrices and this Delta can be obtained with either of these two metrics.",
                    "label": 0
                },
                {
                    "sent": "Now these two metrics are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So have another interesting property which is that they are invariant with fine transformation.",
                    "label": 0
                },
                {
                    "sent": "So if we consider two SPD matrices X&Y, and if we multiply these two.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrices from left and right by a square invertible matrix M that is M in the general linear group then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distance between the two resulting matrices is the same as the distance between the original matrices X&Y and so this is true for both metrics and will use this property in formulation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's go into the details of our approach.",
                    "label": 0
                },
                {
                    "sent": "So recall that our goal is to map a high dimensional SPD matrix X through a lower dimensional 1F of X, so he.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we parameterized this mapping with a rectangular matrix W, and we obtain the low dimensional SPD matrix by multiplying X from left and right with W as illustrated below.",
                    "label": 0
                },
                {
                    "sent": "And so if.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "W has full rank, then F of X is also a symmetric positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "So that would be fine mapping, but.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we want also to define a cost function to learn the parameters of this mapping.",
                    "label": 0
                },
                {
                    "sent": "So here we rely on the following intuition.",
                    "label": 0
                },
                {
                    "sent": "We would like the low dimensional SPD manifold to have a distance that reflects the notion of affinity between the high dimensional SPD matrices.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each pair of SPD matrices XI and XJ will ride cost gij, which is a product between an affinity measure AIJ and the distance between the two matrices in the low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "So this means that if we have a high affinity AIJ, then would like to have a small distance after mapping the matrices to the low dimensional manifold and contrastive hello or a negative affinity AIJ, then we're happy to have a high distance between the two matrices after mapping.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in practice will use the class labels to define the notion of affinity, and we'll set AIJ to one if XJ is a nearest neighbor of XI from the same class.",
                    "label": 1
                },
                {
                    "sent": "So that means that our cost function will try to favor pulling together 2 samples that are of the same class.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interest will set AIJ to minus one if XJ is the nearest neighbor of XI from a different class.",
                    "label": 1
                },
                {
                    "sent": "So that means that we will try.",
                    "label": 0
                },
                {
                    "sent": "Our cost function will try to separate two matrices that are of different classes.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, the affinity will be set to 0 if XJ is not a nearest neighbor of Excitel.",
                    "label": 0
                },
                {
                    "sent": "So now we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the cost function, but this is not enough because as said before, to have valid SPD manifold we also need W to have full rank.",
                    "label": 0
                },
                {
                    "sent": "So here what we do is we enforce orthogonality constraints on W. Now this may seem like a tool restrictive assumption because in general not every full rank matrix is an orthogonal matrix, however.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ensure that these constraints entails no loss of generality and the reason is that any full rank matrix dubled can be decomposed into a product of an orthogonal matrix W and a matrix M in the general linear group.",
                    "label": 0
                },
                {
                    "sent": "And so because.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those of you find invariants property about metrics.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have the cost function.",
                    "label": 0
                },
                {
                    "sent": "Gij of WTO is exactly the same as the cost function JIJFW, so we can only enforce orthogonality constraints and not lose anything.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that we have a cost function and we have constraints on W so we can formulate our learning problem and so given a set of high dimensional SPD matrices, learning is expressed as minimizing the sum of all pairwise cost functions such that the resulting matrix W is orthogonal.",
                    "label": 0
                },
                {
                    "sent": "And because of these orthogonality constraints, this can be a problem in optimization problem on either a Stifel or a Grassmann manifold, and the difference is that for the Grassmann manifold 2 matrices that are the same up to a rotation actually correspond to exactly the same point on a manifold.",
                    "label": 1
                },
                {
                    "sent": "So to study whether we want to use a Stiefel Grassmann manifold optimization approach, we need to study the effect of rotations on our cost function.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take any rotation matrix are are you finding variance property of the metrics also yield that the cost function JIGW is exactly the same as the cost function of the rotated matrix W?",
                    "label": 0
                },
                {
                    "sent": "So it means that we can make use.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of optimization methods on the Grassmann manifold and, in practice will exploit conjugate gradient descent methods on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have an approach that can map high dimensional SPD matrices to a lower dimensional SVD manifold now.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this low dimensional manifold we can still learn different classifiers to do visual recognition and so in our experiments we tried two different such classifiers.",
                    "label": 0
                },
                {
                    "sent": "First one is a nearest neighbor classifier on the low dimensional manifold, and this classifier evidences the benefits of dimensionality reduction more clearly uterine simplicity.",
                    "label": 1
                },
                {
                    "sent": "It also can exploit both the fine environment remaining metric and this time metric and the second classifier that we use is the RSR classifier.",
                    "label": 1
                },
                {
                    "sent": "That was introduced by massage at ECB two years ago, and that classify explores the notion of sparse coding on the manifold.",
                    "label": 0
                },
                {
                    "sent": "But it was designed for this time metrics or the results reported for this classifier will only rely on this time metric.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our experiments we evaluated following methods first of all, with high dimensional SPD matrices, we evaluated the two nearest neighbor classifiers.",
                    "label": 0
                },
                {
                    "sent": "We also evaluated the RSR classifier as well as the CDF method of Wang and colleagues.",
                    "label": 0
                },
                {
                    "sent": "And then of course, after our dimensionality reduction, we evaluated the two nearest neighbor classifiers well and also the RSR classifier.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a first experiment, we tried our method on for the task on material recognition and to this, and we use the UIUC material data set that was introduced last year at CPR.",
                    "label": 0
                },
                {
                    "sent": "This data set contains 18 classes and we using 10 different partition between training and test data.",
                    "label": 1
                },
                {
                    "sent": "And the input to our algorithms are high dimensional matrices of size 155 by 155 that were computed as region covariance descriptors of SIFT features and color features.",
                    "label": 0
                },
                {
                    "sent": "And the parameters of our approach were obtained by cross validation and these parameters are the dimensionality of the low dimensional manifold, the number of neighbors of the same class when building the affinity matrix, the number of neighbors of a different class when building the affinity matrix.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the first baseline is the was the result reporting in the paper that introduced the data set and it was 43.5% accuracy in terms of recognition.",
                    "label": 0
                },
                {
                    "sent": "And then we evaluated CDL an RSR on our high dimensional SPD matrices.",
                    "label": 0
                },
                {
                    "sent": "They both achieve about 52% accuracy and finally the nearest neighbor classifiers on high dimensional matrices only achieve about 35% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Now what is interesting is that.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After reducing the dimensionality, if we apply the nearest neighbor classifiers then we increase the accuracy by more than 20%, so reach 58%, which means that we even outperformed the CDL and RSR classifiers that were computed on high dimensional matrices and so finally if we.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We evaluate the RSI classifier on the low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "We reached the top accuracy of 66.6%.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second experiment we performed action recognition from motion capture data, and for this we use the HTML5 datasets.",
                    "label": 1
                },
                {
                    "sent": "This data set contains 14 different actions an we use two subjects to train our method an three subjects to test it, and in this case there are no images.",
                    "label": 1
                },
                {
                    "sent": "There are three joint location of human skeletons and so the inputs high dimensional SPD matrices are 93 by 93 dimensional and they are covariance descriptors of the 31, three joint location of the human skeleton.",
                    "label": 0
                },
                {
                    "sent": "And the parameters over approach were again obtained by cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, the results on these datasets here.",
                    "label": 0
                },
                {
                    "sent": "CDL perform 79.8% accuracy, which is slightly higher than the RSR glass fire, and this is both in both cases.",
                    "label": 0
                },
                {
                    "sent": "In high dimensional matrices the nearest neighbor classifiers both form that 60% accuracy and now again if we use.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "After dimensionality reduction, we increase the performance by several percentage.",
                    "label": 0
                },
                {
                    "sent": "Now in this case, unfortunately we don't reach the performance of CDL and RSR, however, if we.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use RSR after dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "We reach the best accuracy of 81.9%.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, we have introduced an approach to mapping high dimensional SPD matrices to lower dimensional and more discriminative ones, and this yields a practical way to handle large SPD matrices with existing recognition algorithms, and so in the future we plan to extend a framework to using different cost functions and for instance to end of the unsupervised scenario or the semi supervised scenario an.",
                    "label": 0
                },
                {
                    "sent": "We'd also like to study if the concept of reducing the dimensionality from one manifold to another manifold could apply to a different type of imagine manifolds.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So could you explain why you got better results after you produce the sparse smaller matrix well?",
                    "label": 0
                },
                {
                    "sent": "So the intuition would be that because we also encouraging the low dimensional representation to be more discriminative than we already better reflect the notion of classes, so would we be the task would be easier for the classifier.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Here hello.",
                    "label": 0
                },
                {
                    "sent": "Yes, so so.",
                    "label": 0
                },
                {
                    "sent": "In effect you're building a new descriptor, right?",
                    "label": 0
                },
                {
                    "sent": "So by by mapping to this new space.",
                    "label": 0
                },
                {
                    "sent": "And so I'm wondering whether there is any intuitions that you have about the new descriptor.",
                    "label": 0
                },
                {
                    "sent": "Yes, so so you can think of the new we discussing this in the paper, but we can think of the new descriptor in the case of covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "You can think of.",
                    "label": 0
                },
                {
                    "sent": "The new descriptor has also been a covariance matrix in lower dimensional features.",
                    "label": 0
                },
                {
                    "sent": "And so ultimately, yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "This is really the intuition behind this is that we will try to learn.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as having lower dimensional features that are going to be more discriminative when you consider the covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "So in relation to the previous question, maybe you're you get better results because you're mapping you take into account the distance between the classes.",
                    "label": 0
                },
                {
                    "sent": "So if you try to do your mapping from one dimension to the same dimension.",
                    "label": 0
                },
                {
                    "sent": "And see if that improves.",
                    "label": 0
                },
                {
                    "sent": "So we did not try this, but so the baseline CDL this CDL baseline also tries to separate the classes in a related matter as well.",
                    "label": 0
                },
                {
                    "sent": "We're doing so.",
                    "label": 0
                },
                {
                    "sent": "In a sense, I think there's more to it than just that.",
                    "label": 0
                },
                {
                    "sent": "I wonder how we did was bring mutations is the rows and columns of matrices are in correspondence are invariant to ruin computations?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think you can show that it's also invariant to row and column permutation, and so, yeah, that's why it's only talked about rotation, but it's also true for permutation over one column.",
                    "label": 0
                },
                {
                    "sent": "No more questions.",
                    "label": 0
                },
                {
                    "sent": "If not, then let's thank our speaker again.",
                    "label": 0
                }
            ]
        }
    }
}