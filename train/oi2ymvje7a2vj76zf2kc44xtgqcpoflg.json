{
    "id": "oi2ymvje7a2vj76zf2kc44xtgqcpoflg",
    "title": "Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations",
    "info": {
        "author": [
            "Mingyuan Zhou, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/nips09_zhou_npbd/",
    "segmentation": [
        [
            "Good morning, welcome presenting.",
            "I'm presenting the lone pair match comparison dictionary for sparse major positions.",
            "This is joint work with how to intern John Pesni, Lula.",
            "And the mad why the loans carrying in the Department see at Duke University and Gamma Sapiro diplomacy at University."
        ],
        [
            "Minnesota.",
            "First, I will give an introduction.",
            "Then I will talk about dictionary thing.",
            "Then I will present our model and briefly discuss the influence.",
            "Then I will show very encouraging result of our model as applied to image denoising, inpainting under compressive sensing.",
            "Finally, I would."
        ],
        [
            "The conclusion.",
            "Sparse representation has drawn considerable attentions risk University years due to the fact that encourages simple model just over training can be avoided.",
            "Furthermore, this dictionary element and sparse coefficients.",
            "Generally can't be may be easily interpreted in terms of biological or physical minutes, and we have recently been applied to image noising in painting and compressed sensing, with when counting results in which of the shelf dictionary or basis like DCT, DfT or with less commonly used by the recent research, shows that if the train and often complete over complete dictionary on the on the match to the signal of test, we may get improved performance and back."
        ],
        [
            "Interpretations.",
            "So sparse coding is trying to solve the objective function that, given a dictionary D and observation X, we're trying to find the op trying to solve the object function that would minimize the LSL of the sparse coefficient subject to that the L2 error of the observation and the reputation is smaller than constant.",
            "It's well known that exact solution of this problem is NP hard problems, so many approximate solutions have been proposed which can be generally divided into two categories.",
            "One category is the greedy algorithms, in which document pursuit is a representative on the algae.",
            "Is that convex relation approaches in which lost lawsuit or Bascom presence has been widely used?",
            "It has been shown that it is responsive representations and a proper dictionary can be used to carry covers.",
            "Original data based on noisy or incomplete observations or compressed."
        ],
        [
            "Measurements.",
            "So what kind of dictionary coming?",
            "Consider the property dictionary based on our past experience, this DfT, DCT and wavelets this off shelf basis and dictionary can be fairly well used to sparsely represent image Ng, acoustic and audio signals, and they had a simple and fast computations.",
            "However recently set show if we use dictionaries, adapt to the signal and test we may get improved performance and better interpretation."
        ],
        [
            "It's.",
            "So how do we do the diction?",
            "Learning the general approach is trying to solve the global object function that find a proper dictionary, and this pass coefficient that would would minimize the reconstruction error subject to for each observation the sparsity level smaller than certain given level.",
            "And the company has two stages.",
            "The first stage is the sparse coding stage in which we fixed the dictionary and we're trying to minimize.",
            "We're trying to solve this object function in which we easiest soon that we know the lowest varience or assume we know this parsing node.",
            "Then we can use those documents and pursue the other Mercer to solve these problems and the loss runs or special level is used as the stopping criteria.",
            "And the second stage is when trying to update the dictionary in the one way is that we fix this passcodes and use the ordinary least solution to find the find the dictionary which is known as the method of two more direction MoD and the other way we can use the case we D which I proved to give better interpret, better performance and convergence which instead of fixing the sparse codes we are fixing the sparsity pattern.",
            "And using the rank one approximation to simultaneously update the dictionary and the sparse prevent."
        ],
        [
            "Missions.",
            "But this dictionary training algorithms has some restrictions.",
            "The first obvious restriction is that they need to assume the laws were Ryan.",
            "So special level is known then then it can be used as a stopping criteria and other restrictions at the size of a dictionary needs to be set up prior and the service traction is that only point estimate can be provided, which means they cannot tell us what's the confidence region of the estimation.",
            "So how to relax these restrictions here we introduce.",
            "A non parametric basin dictionary approach is basically this is a fact analysis model and that affected loading can be construed as a dictionary here and the factor score can be considered the coefficient here.",
            "So instead of directly enforcing that this coefficient had certain sparsity level, assume we know the loss variance in the data we promote, we introducing sparsity promoting priors on this acting on this factor score which is the coefficient coefficient here.",
            "Also we present.",
            "We presented dictionary size to be large unless the data itself team for what's the proper prediction size."
        ],
        [
            "This given data.",
            "So the life of reputation is a proposition XI is equal to tzi plus Lewiston.",
            "You wish D is the dictionary which has KK dictionary elements and each dictionary elements has the same dimension as the observation XI.",
            "And we impose that observation XI can be expressed as a linear combination of a small subset of the dictionary element and plus last turn.",
            "So we hope that the representation ZI.",
            "Would be sparse.",
            "Here we use the beta process formulation to impose these two up to favor this sparsity.",
            "Like we draw the dictionary element from national, which is the basis based measurement can which can be normal distribution and we are socially each dictionary element with a pikey indicating the probability for this dictionary to be selected in this data and we let Pi case draw from beta.",
            "So we would probably draw our coefficient, CIK which the case element of the eye.",
            "IID from blue deep IK.",
            "So by construction, our presentation would be sparse.",
            "However, assume that we're dropping ballooning, which means they have banned.",
            "Binary coefficient is highly restrictive, so we end up with a another representation that we draw a pseudo ways associated with the eye from the normal.",
            "So our actual is actual weights is element wise product between the binary indicators, EI and the students I we and we end up with a Model XI is equal to DW plus Lewiston.",
            "These dictionary WI is the coefficient which has real value but also imposed the many of its coefficients can be exactly 0.",
            "Based on this representation, we can build higher."
        ],
        [
            "Model on it here.",
            "I'm not going to discuss the detail of this Harold model, but I want to want to mention that we only put long informative prior on this on this parameters and we can get we can let the data yourself to infer what's the property sparsity level and allows once in the data and in case that the case we only observe the subset of the data which means such as an image, we only observe subset of pixels.",
            "And all only have a projected compressed measurement.",
            "Then instead of directly observed data we operate observing Y equal to Sigma XI where Sigma is somebody matrix.",
            "So we end up with a full likelihood model."
        ],
        [
            "The first time, here is the likelihood and all the others are the priors.",
            "We cannot directly calculate the posterior of each parameter, so we can use better.",
            "We can use Gibbs sampling or variational Bayes, while the inference method to calculate the conditional posterior each parameter and.",
            "And the detail I'm not going to discuss the detail which can be found in our post assertion, so want to mention is that I will give some information, will have very fast convergence and has no computational complexity.",
            "And although the dictionary with Bestbet process priors."
        ],
        [
            "Has different perspective compared to the memo.",
            "DMC Sweeney dictionary methods.",
            "However, if we carefully look at this updating equations, we can find the actually closely related.",
            "However, they have apparent difference in the level exploiting previously obtained information and we can show that our dictionary method has the highest level exploiting previously obtained information.",
            "And it's also natural to use our model to do sequence."
        ],
        [
            "Learning since we we we are in the loan payments basing framework.",
            "So instead of directing calculating the posterior dictionary conditional on the whole data set, we can partition the whole data set into J partitions and we first calculate the posterior dictionary on the first data partition D1 and then use this posterior as a prior to calculating the posterior dictionary based on the first date partition D1, the second data passing the two.",
            "I'm going to do it sequentially until we take it into.",
            "All the data into consideration.",
            "So in summary of the noise variance, our model, the lawyers, Verizon."
        ],
        [
            "New lower do not know and additional size automatically inferred the training data required in our model which we can directly build a dictionary and reconstruction data based on noisy and incomplete observations and our each sample's icon has its own unique sparse representation which renders much more flexibility than enforcing the same special level for all the reputations and a single model here can be applied for no matter grayscale, RGB or hyperspectral image in the northern painting.",
            "First I will show that image denoising result and here the."
        ],
        [
            "We can show that if the assumed last level and the true Laslo is the same, then case will be performed similar to our algorithms.",
            "However, if the soon the last level is higher than the true North level lower than the true noise level case, we would tend to either over smoothing image or not given enough noise reduction.",
            "It's also interesting to look at the dictionary in the right column we can see the highest loss level, the smaller size the dictionary would be inferred, which makes sense because the highest level.",
            "There's less information we can extract from larger image.",
            "And here we give the inpainting results on RGB image and."
        ],
        [
            "First of all, we have randomly take 80% pixel of this original image and based on this corrupted image only we reconstruct the image.",
            "Is this and we will take 50% pixel off and reconstruct the image and original mission looks almost the same.",
            "And here is another example for our Jimmy RGB image inpainting we have 80%."
        ],
        [
            "Pixel missing and this is our restore image.",
            "This is original image and this is cropped image and we can see the conversion rate fast this 1.6.",
            "Nation.",
            "And we can still do texture removal listener warning image this revision image.",
            "This is Reese."
        ],
        [
            "Art image.",
            "And what's more interesting to we can is our model applied to hyperspectral image."
        ],
        [
            "We have a hyperspectral airborne images here that is half the size of 150 by 150 with 210 bands which can be considered the data cube, 3 dimensional data, cute and we randomly take 95% of the data in the data Cooper out which means we only observed 5% of the data cube and we this.",
            "This original image understand one is our construct image.",
            "Believe it or not, it looks almost the same on my and we also applied to a larger scale problem.",
            "That we have a hyperspectral image in with about 800."
        ],
        [
            "500 and with 106 hyperspectral bands and we randomly take 98% pixel out of the whole data cube and miss.",
            "We only observed 2% data pixels so.",
            "We look at this image as a baseline.",
            "Having different tell which is original and which is reconstructed image.",
            "Actually the left one.",
            "Is an original and randomize restart in this is Ben 50 and this special 90.",
            "And also we are planning model for campus sensing.",
            "We come with lots of different algorithms and."
        ],
        [
            "We can show that our algorithm has the state lab results.",
            "I'm not going to discuss the detail, we can talk about more in this post assertion.",
            "So the conclusion is that our long parametric basing diction learning we propose here can be do."
        ],
        [
            "Acne apart to grayscale RGB and hyperspectral image denoising, inpainting, and compressed sensing and with very encouraging results.",
            "And we can automatically infer the dictionary size and the lowest level law and the sparsity level.",
            "And we can build a dictionary and reconstructing original data based directly based on the data in the test, which can be noisy, which can be has missing pixels, or which can be the project environment.",
            "So it's promising to consider this model as a general approach to recover the original data for its redundant, noisy and incomplete observations.",
            "So thank you for attention."
        ],
        [
            "Hello.",
            "So your beta Bernoulli process model is essentially identical to an Indian buffet process.",
            "Yes, shown by Thibaut and Jordan as well.",
            "And the model itself seems to be as far as I can tell, essentially identical to the Bayesian sparse nonparametric factor analysis of Knowles and myself, which I notice you've cited here.",
            "So any comment?",
            "Yeah, the beta process is closely related to the Indian buffet process, and this as we know.",
            "The beta process leaning buffet processes like you have infinite number of dishes and every customer can take a limited number of dishes and beta post tends to be a special case.",
            "If we integrate the Pi K in the beta person out when we can get the Indian buffet process.",
            "And also here this is a using effective lesson model for addiction training and we use bus priors on this factor score, so actually this can be considered factors model with sparsity priors on the factor score.",
            "Sing could you tell me something about the variance of the estimates of the dictionary size that you get the distribution over possible dictionary sizes you mean the dictionary size, the worst addiction size you find estimates.",
            "Yeah, dictionary sizes, so how?",
            "How tightly constrained is the dictionary size?",
            "Once you fitted the model?",
            "That's good question.",
            "At single addiction size in most need based on the.",
            "Boost based on the date you have.",
            "We build the dictionary on the date.",
            "So based on my observation the if I do it with different initialization and additional size is almost the same for different different different different realizations.",
            "So they were right.",
            "Sizing is tight, is very small.",
            "So your question is that why don't we use other inference method?",
            "Alba.",
            "My sister.",
            "More.",
            "OK Jenny, you like talking, which means you comparing the based method and the conventional we optimization Mercer, right?",
            "So we.",
            "Here we actually.",
            "In this case, we downloaded and using optimization method for putting the L1 constraint.",
            "And here we show that if we do it in a basic way we can relax some constraints we have on it, which like automatically infers lowest level and sparsity level.",
            "But if you put in L1 constraint to do automatic optimization you have to carefully set the you have to set the regularization parameter.",
            "And you have to do cross validation to tell what's the appropriate.",
            "Related impairment, however you do here, we're doing the base and we can show that we only putting a line from the priors and we can let the data yourself to infer what's the property level wanted by the data itself.",
            "Speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning, welcome presenting.",
                    "label": 0
                },
                {
                    "sent": "I'm presenting the lone pair match comparison dictionary for sparse major positions.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with how to intern John Pesni, Lula.",
                    "label": 0
                },
                {
                    "sent": "And the mad why the loans carrying in the Department see at Duke University and Gamma Sapiro diplomacy at University.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Minnesota.",
                    "label": 0
                },
                {
                    "sent": "First, I will give an introduction.",
                    "label": 0
                },
                {
                    "sent": "Then I will talk about dictionary thing.",
                    "label": 0
                },
                {
                    "sent": "Then I will present our model and briefly discuss the influence.",
                    "label": 1
                },
                {
                    "sent": "Then I will show very encouraging result of our model as applied to image denoising, inpainting under compressive sensing.",
                    "label": 0
                },
                {
                    "sent": "Finally, I would.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The conclusion.",
                    "label": 0
                },
                {
                    "sent": "Sparse representation has drawn considerable attentions risk University years due to the fact that encourages simple model just over training can be avoided.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, this dictionary element and sparse coefficients.",
                    "label": 1
                },
                {
                    "sent": "Generally can't be may be easily interpreted in terms of biological or physical minutes, and we have recently been applied to image noising in painting and compressed sensing, with when counting results in which of the shelf dictionary or basis like DCT, DfT or with less commonly used by the recent research, shows that if the train and often complete over complete dictionary on the on the match to the signal of test, we may get improved performance and back.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interpretations.",
                    "label": 0
                },
                {
                    "sent": "So sparse coding is trying to solve the objective function that, given a dictionary D and observation X, we're trying to find the op trying to solve the object function that would minimize the LSL of the sparse coefficient subject to that the L2 error of the observation and the reputation is smaller than constant.",
                    "label": 0
                },
                {
                    "sent": "It's well known that exact solution of this problem is NP hard problems, so many approximate solutions have been proposed which can be generally divided into two categories.",
                    "label": 1
                },
                {
                    "sent": "One category is the greedy algorithms, in which document pursuit is a representative on the algae.",
                    "label": 0
                },
                {
                    "sent": "Is that convex relation approaches in which lost lawsuit or Bascom presence has been widely used?",
                    "label": 0
                },
                {
                    "sent": "It has been shown that it is responsive representations and a proper dictionary can be used to carry covers.",
                    "label": 0
                },
                {
                    "sent": "Original data based on noisy or incomplete observations or compressed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Measurements.",
                    "label": 0
                },
                {
                    "sent": "So what kind of dictionary coming?",
                    "label": 0
                },
                {
                    "sent": "Consider the property dictionary based on our past experience, this DfT, DCT and wavelets this off shelf basis and dictionary can be fairly well used to sparsely represent image Ng, acoustic and audio signals, and they had a simple and fast computations.",
                    "label": 0
                },
                {
                    "sent": "However recently set show if we use dictionaries, adapt to the signal and test we may get improved performance and better interpretation.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So how do we do the diction?",
                    "label": 0
                },
                {
                    "sent": "Learning the general approach is trying to solve the global object function that find a proper dictionary, and this pass coefficient that would would minimize the reconstruction error subject to for each observation the sparsity level smaller than certain given level.",
                    "label": 0
                },
                {
                    "sent": "And the company has two stages.",
                    "label": 0
                },
                {
                    "sent": "The first stage is the sparse coding stage in which we fixed the dictionary and we're trying to minimize.",
                    "label": 1
                },
                {
                    "sent": "We're trying to solve this object function in which we easiest soon that we know the lowest varience or assume we know this parsing node.",
                    "label": 0
                },
                {
                    "sent": "Then we can use those documents and pursue the other Mercer to solve these problems and the loss runs or special level is used as the stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "And the second stage is when trying to update the dictionary in the one way is that we fix this passcodes and use the ordinary least solution to find the find the dictionary which is known as the method of two more direction MoD and the other way we can use the case we D which I proved to give better interpret, better performance and convergence which instead of fixing the sparse codes we are fixing the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And using the rank one approximation to simultaneously update the dictionary and the sparse prevent.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Missions.",
                    "label": 0
                },
                {
                    "sent": "But this dictionary training algorithms has some restrictions.",
                    "label": 0
                },
                {
                    "sent": "The first obvious restriction is that they need to assume the laws were Ryan.",
                    "label": 0
                },
                {
                    "sent": "So special level is known then then it can be used as a stopping criteria and other restrictions at the size of a dictionary needs to be set up prior and the service traction is that only point estimate can be provided, which means they cannot tell us what's the confidence region of the estimation.",
                    "label": 1
                },
                {
                    "sent": "So how to relax these restrictions here we introduce.",
                    "label": 1
                },
                {
                    "sent": "A non parametric basin dictionary approach is basically this is a fact analysis model and that affected loading can be construed as a dictionary here and the factor score can be considered the coefficient here.",
                    "label": 0
                },
                {
                    "sent": "So instead of directly enforcing that this coefficient had certain sparsity level, assume we know the loss variance in the data we promote, we introducing sparsity promoting priors on this acting on this factor score which is the coefficient coefficient here.",
                    "label": 0
                },
                {
                    "sent": "Also we present.",
                    "label": 1
                },
                {
                    "sent": "We presented dictionary size to be large unless the data itself team for what's the proper prediction size.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This given data.",
                    "label": 0
                },
                {
                    "sent": "So the life of reputation is a proposition XI is equal to tzi plus Lewiston.",
                    "label": 0
                },
                {
                    "sent": "You wish D is the dictionary which has KK dictionary elements and each dictionary elements has the same dimension as the observation XI.",
                    "label": 0
                },
                {
                    "sent": "And we impose that observation XI can be expressed as a linear combination of a small subset of the dictionary element and plus last turn.",
                    "label": 0
                },
                {
                    "sent": "So we hope that the representation ZI.",
                    "label": 0
                },
                {
                    "sent": "Would be sparse.",
                    "label": 0
                },
                {
                    "sent": "Here we use the beta process formulation to impose these two up to favor this sparsity.",
                    "label": 1
                },
                {
                    "sent": "Like we draw the dictionary element from national, which is the basis based measurement can which can be normal distribution and we are socially each dictionary element with a pikey indicating the probability for this dictionary to be selected in this data and we let Pi case draw from beta.",
                    "label": 0
                },
                {
                    "sent": "So we would probably draw our coefficient, CIK which the case element of the eye.",
                    "label": 0
                },
                {
                    "sent": "IID from blue deep IK.",
                    "label": 0
                },
                {
                    "sent": "So by construction, our presentation would be sparse.",
                    "label": 0
                },
                {
                    "sent": "However, assume that we're dropping ballooning, which means they have banned.",
                    "label": 0
                },
                {
                    "sent": "Binary coefficient is highly restrictive, so we end up with a another representation that we draw a pseudo ways associated with the eye from the normal.",
                    "label": 0
                },
                {
                    "sent": "So our actual is actual weights is element wise product between the binary indicators, EI and the students I we and we end up with a Model XI is equal to DW plus Lewiston.",
                    "label": 0
                },
                {
                    "sent": "These dictionary WI is the coefficient which has real value but also imposed the many of its coefficients can be exactly 0.",
                    "label": 0
                },
                {
                    "sent": "Based on this representation, we can build higher.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model on it here.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to discuss the detail of this Harold model, but I want to want to mention that we only put long informative prior on this on this parameters and we can get we can let the data yourself to infer what's the property sparsity level and allows once in the data and in case that the case we only observe the subset of the data which means such as an image, we only observe subset of pixels.",
                    "label": 0
                },
                {
                    "sent": "And all only have a projected compressed measurement.",
                    "label": 0
                },
                {
                    "sent": "Then instead of directly observed data we operate observing Y equal to Sigma XI where Sigma is somebody matrix.",
                    "label": 1
                },
                {
                    "sent": "So we end up with a full likelihood model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first time, here is the likelihood and all the others are the priors.",
                    "label": 0
                },
                {
                    "sent": "We cannot directly calculate the posterior of each parameter, so we can use better.",
                    "label": 0
                },
                {
                    "sent": "We can use Gibbs sampling or variational Bayes, while the inference method to calculate the conditional posterior each parameter and.",
                    "label": 1
                },
                {
                    "sent": "And the detail I'm not going to discuss the detail which can be found in our post assertion, so want to mention is that I will give some information, will have very fast convergence and has no computational complexity.",
                    "label": 0
                },
                {
                    "sent": "And although the dictionary with Bestbet process priors.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has different perspective compared to the memo.",
                    "label": 0
                },
                {
                    "sent": "DMC Sweeney dictionary methods.",
                    "label": 0
                },
                {
                    "sent": "However, if we carefully look at this updating equations, we can find the actually closely related.",
                    "label": 0
                },
                {
                    "sent": "However, they have apparent difference in the level exploiting previously obtained information and we can show that our dictionary method has the highest level exploiting previously obtained information.",
                    "label": 1
                },
                {
                    "sent": "And it's also natural to use our model to do sequence.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning since we we we are in the loan payments basing framework.",
                    "label": 0
                },
                {
                    "sent": "So instead of directing calculating the posterior dictionary conditional on the whole data set, we can partition the whole data set into J partitions and we first calculate the posterior dictionary on the first data partition D1 and then use this posterior as a prior to calculating the posterior dictionary based on the first date partition D1, the second data passing the two.",
                    "label": 1
                },
                {
                    "sent": "I'm going to do it sequentially until we take it into.",
                    "label": 0
                },
                {
                    "sent": "All the data into consideration.",
                    "label": 0
                },
                {
                    "sent": "So in summary of the noise variance, our model, the lawyers, Verizon.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New lower do not know and additional size automatically inferred the training data required in our model which we can directly build a dictionary and reconstruction data based on noisy and incomplete observations and our each sample's icon has its own unique sparse representation which renders much more flexibility than enforcing the same special level for all the reputations and a single model here can be applied for no matter grayscale, RGB or hyperspectral image in the northern painting.",
                    "label": 0
                },
                {
                    "sent": "First I will show that image denoising result and here the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that if the assumed last level and the true Laslo is the same, then case will be performed similar to our algorithms.",
                    "label": 0
                },
                {
                    "sent": "However, if the soon the last level is higher than the true North level lower than the true noise level case, we would tend to either over smoothing image or not given enough noise reduction.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting to look at the dictionary in the right column we can see the highest loss level, the smaller size the dictionary would be inferred, which makes sense because the highest level.",
                    "label": 0
                },
                {
                    "sent": "There's less information we can extract from larger image.",
                    "label": 0
                },
                {
                    "sent": "And here we give the inpainting results on RGB image and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, we have randomly take 80% pixel of this original image and based on this corrupted image only we reconstruct the image.",
                    "label": 1
                },
                {
                    "sent": "Is this and we will take 50% pixel off and reconstruct the image and original mission looks almost the same.",
                    "label": 1
                },
                {
                    "sent": "And here is another example for our Jimmy RGB image inpainting we have 80%.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pixel missing and this is our restore image.",
                    "label": 0
                },
                {
                    "sent": "This is original image and this is cropped image and we can see the conversion rate fast this 1.6.",
                    "label": 1
                },
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "And we can still do texture removal listener warning image this revision image.",
                    "label": 0
                },
                {
                    "sent": "This is Reese.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Art image.",
                    "label": 0
                },
                {
                    "sent": "And what's more interesting to we can is our model applied to hyperspectral image.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a hyperspectral airborne images here that is half the size of 150 by 150 with 210 bands which can be considered the data cube, 3 dimensional data, cute and we randomly take 95% of the data in the data Cooper out which means we only observed 5% of the data cube and we this.",
                    "label": 0
                },
                {
                    "sent": "This original image understand one is our construct image.",
                    "label": 1
                },
                {
                    "sent": "Believe it or not, it looks almost the same on my and we also applied to a larger scale problem.",
                    "label": 1
                },
                {
                    "sent": "That we have a hyperspectral image in with about 800.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "500 and with 106 hyperspectral bands and we randomly take 98% pixel out of the whole data cube and miss.",
                    "label": 0
                },
                {
                    "sent": "We only observed 2% data pixels so.",
                    "label": 0
                },
                {
                    "sent": "We look at this image as a baseline.",
                    "label": 0
                },
                {
                    "sent": "Having different tell which is original and which is reconstructed image.",
                    "label": 0
                },
                {
                    "sent": "Actually the left one.",
                    "label": 0
                },
                {
                    "sent": "Is an original and randomize restart in this is Ben 50 and this special 90.",
                    "label": 0
                },
                {
                    "sent": "And also we are planning model for campus sensing.",
                    "label": 0
                },
                {
                    "sent": "We come with lots of different algorithms and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can show that our algorithm has the state lab results.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to discuss the detail, we can talk about more in this post assertion.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is that our long parametric basing diction learning we propose here can be do.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acne apart to grayscale RGB and hyperspectral image denoising, inpainting, and compressed sensing and with very encouraging results.",
                    "label": 1
                },
                {
                    "sent": "And we can automatically infer the dictionary size and the lowest level law and the sparsity level.",
                    "label": 0
                },
                {
                    "sent": "And we can build a dictionary and reconstructing original data based directly based on the data in the test, which can be noisy, which can be has missing pixels, or which can be the project environment.",
                    "label": 0
                },
                {
                    "sent": "So it's promising to consider this model as a general approach to recover the original data for its redundant, noisy and incomplete observations.",
                    "label": 0
                },
                {
                    "sent": "So thank you for attention.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "So your beta Bernoulli process model is essentially identical to an Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "Yes, shown by Thibaut and Jordan as well.",
                    "label": 0
                },
                {
                    "sent": "And the model itself seems to be as far as I can tell, essentially identical to the Bayesian sparse nonparametric factor analysis of Knowles and myself, which I notice you've cited here.",
                    "label": 0
                },
                {
                    "sent": "So any comment?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the beta process is closely related to the Indian buffet process, and this as we know.",
                    "label": 0
                },
                {
                    "sent": "The beta process leaning buffet processes like you have infinite number of dishes and every customer can take a limited number of dishes and beta post tends to be a special case.",
                    "label": 0
                },
                {
                    "sent": "If we integrate the Pi K in the beta person out when we can get the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "And also here this is a using effective lesson model for addiction training and we use bus priors on this factor score, so actually this can be considered factors model with sparsity priors on the factor score.",
                    "label": 0
                },
                {
                    "sent": "Sing could you tell me something about the variance of the estimates of the dictionary size that you get the distribution over possible dictionary sizes you mean the dictionary size, the worst addiction size you find estimates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, dictionary sizes, so how?",
                    "label": 0
                },
                {
                    "sent": "How tightly constrained is the dictionary size?",
                    "label": 0
                },
                {
                    "sent": "Once you fitted the model?",
                    "label": 0
                },
                {
                    "sent": "That's good question.",
                    "label": 0
                },
                {
                    "sent": "At single addiction size in most need based on the.",
                    "label": 0
                },
                {
                    "sent": "Boost based on the date you have.",
                    "label": 0
                },
                {
                    "sent": "We build the dictionary on the date.",
                    "label": 0
                },
                {
                    "sent": "So based on my observation the if I do it with different initialization and additional size is almost the same for different different different different realizations.",
                    "label": 0
                },
                {
                    "sent": "So they were right.",
                    "label": 0
                },
                {
                    "sent": "Sizing is tight, is very small.",
                    "label": 0
                },
                {
                    "sent": "So your question is that why don't we use other inference method?",
                    "label": 0
                },
                {
                    "sent": "Alba.",
                    "label": 0
                },
                {
                    "sent": "My sister.",
                    "label": 0
                },
                {
                    "sent": "More.",
                    "label": 0
                },
                {
                    "sent": "OK Jenny, you like talking, which means you comparing the based method and the conventional we optimization Mercer, right?",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Here we actually.",
                    "label": 0
                },
                {
                    "sent": "In this case, we downloaded and using optimization method for putting the L1 constraint.",
                    "label": 0
                },
                {
                    "sent": "And here we show that if we do it in a basic way we can relax some constraints we have on it, which like automatically infers lowest level and sparsity level.",
                    "label": 0
                },
                {
                    "sent": "But if you put in L1 constraint to do automatic optimization you have to carefully set the you have to set the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "And you have to do cross validation to tell what's the appropriate.",
                    "label": 0
                },
                {
                    "sent": "Related impairment, however you do here, we're doing the base and we can show that we only putting a line from the priors and we can let the data yourself to infer what's the property level wanted by the data itself.",
                    "label": 0
                },
                {
                    "sent": "Speaker.",
                    "label": 0
                }
            ]
        }
    }
}