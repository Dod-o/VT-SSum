{
    "id": "q3ofg4su7jsejvxvhr3rnkr5hwu5kgfd",
    "title": "Combined Regression and Ranking",
    "info": {
        "author": [
            "D. Sculley, Research at Google, Google, Inc."
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/kdd2010_sculley_crr/",
    "segmentation": [
        [
            "OK, so I'm sorry that there was so much suspense attached to various laptops and issues, but this slides are really great.",
            "You're going to love them, so I hope that it was worth the wait.",
            "So I'm talking bout combined regression and ranking.",
            "My name is descale them from Google Petsburgh nice sunny place."
        ],
        [
            "I would like to start off with the basic claim.",
            "The claim is that many data mining applications require models that give us both good regression performance and good ranking performance.",
            "We want the values to be right and we want the ordering to be right."
        ],
        [
            "Here's one such application predicting star rating.",
            "So users rate, say, the Kodak easyshare three stars four stars, five stars.",
            "We want to get a prediction for a new user that gives them the right number of stars, but also respects their internal ordering preferences as well."
        ],
        [
            "Another important application areas click prediction in sponsored search.",
            "It's very helpful to have extremely accurate predictions on the exact click through rate for a given ad, but we also need to have the right relative ordering among the ads because of the second price auction mechanisms."
        ],
        [
            "So we say, OK, fine, there's situations where we might need to do well on both metrics, but maybe some existing methods can do fine."
        ],
        [
            "And unfortunately this isn't the case.",
            "Standard methods can fail badly.",
            "For example, if we use a rank based model, this can do arbitrarily badly at regressions through normal transformations.",
            "But even if we have, if we have a perfect regressor, it will give us a perfect ranking.",
            "But if we have even a very good regressor.",
            "This can have a bad ranking performance because small perturbations in the relative values can have a lot of pairwise preference errors."
        ],
        [
            "So our approach and a little bit of novelty here is that we say if we care about both sets of metrics, maybe we should optimize them both together in the same optimization function and our main goal here is to try and get best of both performance.",
            "That is, we'd like to predict values as well as ranking only method, and we'd like to get the ordering right as well as a ranking only method.",
            "So we want to as well as regression only and as well as ranking only with just one model.",
            "Secondary goals that we have some reason to believe that we may be able to improve our regression performance through adding these ranking constraints.",
            "And we'll talk a little bit about that later.",
            "So go ahead and build this up in pieces."
        ],
        [
            "So supervised regression has been studied ever since the first single celled datamining Organism, Sir surfaced, so I won't."
        ],
        [
            "Little here, but some important family of supervised regression is this.",
            "In general it is.",
            "So is this idea here where we are minimizing loss over data set and we have a regularization term and this."
        ],
        [
            "Loss function can be any sort of regularization or any sort of regression loss, like a squared loss logistic loss for predicting probabilities and the regularization term we have here happens to be L2 norm regularization, but it could be any other normal regularization term."
        ],
        [
            "So skipping blithely passed into supervised ranking, this is a newer field.",
            "And here we're trying to learn a model that gives us the right relative ordering on our unseen data.",
            "So some well known methods are rank SVM.",
            "Is voted Perceptron invariant some boosting variance listwise approaches?"
        ],
        [
            "One particular family that we're looking at here is very similar to the supervised regression family that we looked at previously."
        ],
        [
            "But we've got this P family this way.",
            "Instead of looking over the entire data set D, we look over the set of candidate pairs, so these are pairs that are drawn from our data set, and each of our examples in the candidate pair has a different rank value and its can parable.",
            "So does this mean incomparable?"
        ],
        [
            "Or maybe in if we were doing rank optimization over search results.",
            "Maybe all of these candidate pairs each item in the pair is drawn from the same query Shard, so these are results from the same query."
        ],
        [
            "Now there's a little bit of a problem here, which is that this size P, because we're looking at all pairs in the data set, this could be quadratic in the size of our original data, so that this P could get big, but we're going to ignore that difficulty for now."
        ],
        [
            "So now we move on to our novelty of doing this optimization jointly."
        ],
        [
            "Not surprisingly, we have this joint optimization function."
        ],
        [
            "That has a regression term.",
            "It has a ranking term and a regularization term, and there's this tradeoff parameter Alpha, that says how much ranking versus how much regression are we.",
            "Are we going to do?"
        ],
        [
            "So the good news is that this is so.",
            "We're using convex loss functions.",
            "This is still a convex optimization problem, so we can solve it efficiently."
        ],
        [
            "Hello, what about this size of P?",
            "This was quadratic in."
        ],
        [
            "Size of D. So if we're going to use some sort of scalable method for doing optimization, we need to be able to do efficient sampling from this set.",
            "Pete, instead of looking at all in squared pairs, we want to be able to pull samples out efficiently.",
            "This can be done for data that fits in memory.",
            "It can be done by indexing the data, and if you want details you can stop by the poster tonight and when data is too large to index, it's not a problem.",
            "We can do a rejection sampling approach so we can sample efficiently from P. We can pull out one candidate pair at a time.",
            "Without all of computational cost."
        ],
        [
            "And this allows us to put together a CRR algorithm combined regression ranking.",
            "We repeat this set number of times where we flip a biased coin.",
            "If it comes up on the regression side, we take one regression step using stochastic gradient descent or Pegasus step.",
            "Something along these lines.",
            "If it comes up on the ranking step, we pull out one candidate pair using our efficient sampling methods and then we do again a stochastic gradient descent update or.",
            "Microsoft update something along these lines.",
            "So we repeat until we hit some stopping criterion.",
            "In practice, this stopping criterion is go for a couple CPU seconds, but there are more sophisticated stopping criterion you can use as well."
        ],
        [
            "So this is highly scalable like other stochastic gradient descent algorithms, it's fast for large data set.",
            "For example, where we can train on a million training examples in less than three CPU seconds on a normal laptop."
        ],
        [
            "We can handle nonlinear models in interest of time.",
            "I'll skip that, but if you're interested in talking more about that, please come by the poster session."
        ],
        [
            "So let's dive into some experiments and see if this combined optimization actually gets us anything so."
        ],
        [
            "Our first data set that we looked at was the MCV one data set, which has thanks 780,000 training examples, 23,000 test examples and what's interesting about this data set is that it has a number of different tasks, so it's sort of per topic identification, some of which are extreme minority class distributions.",
            "So something like .02% in topic and we used logistic loss as our loss function in the various.",
            "Optimization methods."
        ],
        [
            "So our comparison methods we have our CR here is in blue.",
            "We use the same loss function for regression only in the same loss function for ranking only using the different training regimes that we looked at earlier.",
            "And so here on the vertical axis, we're looking at AUC loss.",
            "So here lower is better, but it's a rank based metric.",
            "We're trying to get as low as possible and here are results across different class distributions.",
            "So here on the left we've got extreme minority classes and on the right we've got fairly balanced datasets.",
            "And we're seeing a general trend.",
            "Which is that the ranking only and the combined method both do about the same on this rank based loss function.",
            "But the regression only method does significantly worse, especially in the extreme minority class data.",
            "This is a log rhythmic scale on the vertical axis, so.",
            "You might say final.",
            "We just always use ranking only."
        ],
        [
            "But then if we look at a regression based metric, mean squared errors now on the vertical axis, we can see that now the ranking only method that did so well on the rank based losses now doing terribly at the regression based metric.",
            "But the combined method is doing as well as the regression only metric, and in fact it's doing a little bit better.",
            "Again, it's a log rhythmic scale is doing a little bit better.",
            "This statistically significantly better across this range of extreme minority class distributions.",
            "So."
        ],
        [
            "Why would this be true?",
            "How is it possible that adding in this ranking information gives us actually better regression estimates at the extreme minority class distributions?",
            "Well, you could think that these rank based constraints are informative.",
            "So imagine you two biased coins.",
            "Once it comes up heads with probability .0, two another one comes up probability of .03.",
            "If you have some third coin that you want to estimate its probability of coming up heads, it's really helpful to know that it's in between those first 2 coins.",
            "So these rank based constraints can be extremely informative.",
            "For extreme minority class data."
        ],
        [
            "Moving on to looking at areas where we want to say, you know, mimic the predicting the number of stars.",
            "So we looked at the liter benchmark datasets for learning to rank.",
            "These tasks have multiple relevance levels of 1, two, or three stars we can think.",
            "And here we're using a squared loss function because we're trying to actually predict the ordinal values of 0 one or two with our regression methods."
        ],
        [
            "So the same kind of pattern here are rank based performance metrics and ranking only ANSI are doing about the same.",
            "The differences are not statistically significant.",
            "An regression only is trailing quite a ways behind on these rank based performance metrics."
        ],
        [
            "On the regression based performance metrics, rank only is really suffering and the combined method is doing now almost as well as the regression, only it's not doing exactly as well as regression only.",
            "I think that this is because the class balance is around 30% of sort of relevant versus not relevant as opposed to one of these extreme minority class situations."
        ],
        [
            "One final set of experiments is click prediction, so looking at that trying to estimate clickthrough rates on sponsored search ads, this is from a private data set of several million ads.",
            "Drone from internal data.",
            "It's very high dimensional feature space, and again we're using logistic loss for trying to estimate these probabilities."
        ],
        [
            "So here are the actual numbers.",
            "I'm sorry for putting a chart of numbers up here for you, but at least I'm not reading them off to you.",
            "What we see is that for mean squared error regression only and the combined method are doing exactly the same.",
            "An for AUC loss.",
            "So again trying to get as low as possible in this rank based method ranking only and the combined method are doing equally well.",
            "So really are getting best of both perform."
        ],
        [
            "Here, and in particular, you could think of regression.",
            "Only someone from deep from Yahoo.",
            "Mentioned that logistic regression is considered sort of state of the art for TTR prediction.",
            "We're getting a point 8% improvement on AUC loss over this logistic regression, and you might say well point 8%.",
            "That's not really that big, but it is statistically significant because it's a very large data set and at the scale that search engines work at, even very small.",
            "Relative improvements like this can have a large absolute improvement."
        ],
        [
            "So we had this tradeoff parameter that says OK, how much?"
        ],
        [
            "Ranking versus how much regression are we doing?",
            "And it wouldn't be much fun if we introduce a new method, but then you have to spend all your time tuning Alpha so we did a little bit of analysis to see how sensitive we were to various values of."
        ],
        [
            "So here is for one particular task from the MCV.",
            "One data we've got AUC loss in red.",
            "We've got mean squared error in Green Ann from left to right.",
            "The different values of Alpha's over here on the right.",
            "We've got regression only and over here on the left we've got ranking only, and we can see that there's really a good range of intermediate values of Alpha that will give us much better results than if we were using only regression are only ranking.",
            "So this seems to be pretty pretty robust across a range of.",
            "The values."
        ],
        [
            "So to wrap up, we do find that this combined ranking regression gives us best of both performance.",
            "The fairly simplistic method that uses pairwise method for rank based components which makes it scalable and robust.",
            "But we think that this is probably with any luck, the sort of tip of the iceberg for looking at ways to combine rank loss and in regression loss, so it may be possible to look at joint optimizations that optimize mean average precision or normalized discounted cumulative gain explicitly so."
        ],
        [
            "With that, I'll take any questions I would like to note that there is open source code that you can download and play with.",
            "Thank you very much.",
            "So happy.",
            "Text here.",
            "So I wasn't sure how your approach is different than ordinal regression.",
            "While the rolling off 2 samples are also taken into account and the regression algorithm, so it's a combination of ranking plus regression, yes, so that's interesting.",
            "We should probably talk a bit more offline.",
            "Please come to session.",
            "Send the speaker off."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm sorry that there was so much suspense attached to various laptops and issues, but this slides are really great.",
                    "label": 0
                },
                {
                    "sent": "You're going to love them, so I hope that it was worth the wait.",
                    "label": 0
                },
                {
                    "sent": "So I'm talking bout combined regression and ranking.",
                    "label": 1
                },
                {
                    "sent": "My name is descale them from Google Petsburgh nice sunny place.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would like to start off with the basic claim.",
                    "label": 0
                },
                {
                    "sent": "The claim is that many data mining applications require models that give us both good regression performance and good ranking performance.",
                    "label": 1
                },
                {
                    "sent": "We want the values to be right and we want the ordering to be right.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's one such application predicting star rating.",
                    "label": 0
                },
                {
                    "sent": "So users rate, say, the Kodak easyshare three stars four stars, five stars.",
                    "label": 0
                },
                {
                    "sent": "We want to get a prediction for a new user that gives them the right number of stars, but also respects their internal ordering preferences as well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important application areas click prediction in sponsored search.",
                    "label": 0
                },
                {
                    "sent": "It's very helpful to have extremely accurate predictions on the exact click through rate for a given ad, but we also need to have the right relative ordering among the ads because of the second price auction mechanisms.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we say, OK, fine, there's situations where we might need to do well on both metrics, but maybe some existing methods can do fine.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And unfortunately this isn't the case.",
                    "label": 0
                },
                {
                    "sent": "Standard methods can fail badly.",
                    "label": 1
                },
                {
                    "sent": "For example, if we use a rank based model, this can do arbitrarily badly at regressions through normal transformations.",
                    "label": 0
                },
                {
                    "sent": "But even if we have, if we have a perfect regressor, it will give us a perfect ranking.",
                    "label": 0
                },
                {
                    "sent": "But if we have even a very good regressor.",
                    "label": 0
                },
                {
                    "sent": "This can have a bad ranking performance because small perturbations in the relative values can have a lot of pairwise preference errors.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach and a little bit of novelty here is that we say if we care about both sets of metrics, maybe we should optimize them both together in the same optimization function and our main goal here is to try and get best of both performance.",
                    "label": 1
                },
                {
                    "sent": "That is, we'd like to predict values as well as ranking only method, and we'd like to get the ordering right as well as a ranking only method.",
                    "label": 0
                },
                {
                    "sent": "So we want to as well as regression only and as well as ranking only with just one model.",
                    "label": 0
                },
                {
                    "sent": "Secondary goals that we have some reason to believe that we may be able to improve our regression performance through adding these ranking constraints.",
                    "label": 0
                },
                {
                    "sent": "And we'll talk a little bit about that later.",
                    "label": 1
                },
                {
                    "sent": "So go ahead and build this up in pieces.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So supervised regression has been studied ever since the first single celled datamining Organism, Sir surfaced, so I won't.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little here, but some important family of supervised regression is this.",
                    "label": 0
                },
                {
                    "sent": "In general it is.",
                    "label": 0
                },
                {
                    "sent": "So is this idea here where we are minimizing loss over data set and we have a regularization term and this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loss function can be any sort of regularization or any sort of regression loss, like a squared loss logistic loss for predicting probabilities and the regularization term we have here happens to be L2 norm regularization, but it could be any other normal regularization term.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So skipping blithely passed into supervised ranking, this is a newer field.",
                    "label": 0
                },
                {
                    "sent": "And here we're trying to learn a model that gives us the right relative ordering on our unseen data.",
                    "label": 1
                },
                {
                    "sent": "So some well known methods are rank SVM.",
                    "label": 0
                },
                {
                    "sent": "Is voted Perceptron invariant some boosting variance listwise approaches?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One particular family that we're looking at here is very similar to the supervised regression family that we looked at previously.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we've got this P family this way.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking over the entire data set D, we look over the set of candidate pairs, so these are pairs that are drawn from our data set, and each of our examples in the candidate pair has a different rank value and its can parable.",
                    "label": 0
                },
                {
                    "sent": "So does this mean incomparable?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or maybe in if we were doing rank optimization over search results.",
                    "label": 0
                },
                {
                    "sent": "Maybe all of these candidate pairs each item in the pair is drawn from the same query Shard, so these are results from the same query.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's a little bit of a problem here, which is that this size P, because we're looking at all pairs in the data set, this could be quadratic in the size of our original data, so that this P could get big, but we're going to ignore that difficulty for now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we move on to our novelty of doing this optimization jointly.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not surprisingly, we have this joint optimization function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That has a regression term.",
                    "label": 0
                },
                {
                    "sent": "It has a ranking term and a regularization term, and there's this tradeoff parameter Alpha, that says how much ranking versus how much regression are we.",
                    "label": 0
                },
                {
                    "sent": "Are we going to do?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the good news is that this is so.",
                    "label": 0
                },
                {
                    "sent": "We're using convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "This is still a convex optimization problem, so we can solve it efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, what about this size of P?",
                    "label": 0
                },
                {
                    "sent": "This was quadratic in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Size of D. So if we're going to use some sort of scalable method for doing optimization, we need to be able to do efficient sampling from this set.",
                    "label": 0
                },
                {
                    "sent": "Pete, instead of looking at all in squared pairs, we want to be able to pull samples out efficiently.",
                    "label": 0
                },
                {
                    "sent": "This can be done for data that fits in memory.",
                    "label": 0
                },
                {
                    "sent": "It can be done by indexing the data, and if you want details you can stop by the poster tonight and when data is too large to index, it's not a problem.",
                    "label": 1
                },
                {
                    "sent": "We can do a rejection sampling approach so we can sample efficiently from P. We can pull out one candidate pair at a time.",
                    "label": 0
                },
                {
                    "sent": "Without all of computational cost.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this allows us to put together a CRR algorithm combined regression ranking.",
                    "label": 0
                },
                {
                    "sent": "We repeat this set number of times where we flip a biased coin.",
                    "label": 0
                },
                {
                    "sent": "If it comes up on the regression side, we take one regression step using stochastic gradient descent or Pegasus step.",
                    "label": 0
                },
                {
                    "sent": "Something along these lines.",
                    "label": 0
                },
                {
                    "sent": "If it comes up on the ranking step, we pull out one candidate pair using our efficient sampling methods and then we do again a stochastic gradient descent update or.",
                    "label": 0
                },
                {
                    "sent": "Microsoft update something along these lines.",
                    "label": 0
                },
                {
                    "sent": "So we repeat until we hit some stopping criterion.",
                    "label": 0
                },
                {
                    "sent": "In practice, this stopping criterion is go for a couple CPU seconds, but there are more sophisticated stopping criterion you can use as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is highly scalable like other stochastic gradient descent algorithms, it's fast for large data set.",
                    "label": 0
                },
                {
                    "sent": "For example, where we can train on a million training examples in less than three CPU seconds on a normal laptop.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can handle nonlinear models in interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'll skip that, but if you're interested in talking more about that, please come by the poster session.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's dive into some experiments and see if this combined optimization actually gets us anything so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our first data set that we looked at was the MCV one data set, which has thanks 780,000 training examples, 23,000 test examples and what's interesting about this data set is that it has a number of different tasks, so it's sort of per topic identification, some of which are extreme minority class distributions.",
                    "label": 1
                },
                {
                    "sent": "So something like .02% in topic and we used logistic loss as our loss function in the various.",
                    "label": 0
                },
                {
                    "sent": "Optimization methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our comparison methods we have our CR here is in blue.",
                    "label": 0
                },
                {
                    "sent": "We use the same loss function for regression only in the same loss function for ranking only using the different training regimes that we looked at earlier.",
                    "label": 0
                },
                {
                    "sent": "And so here on the vertical axis, we're looking at AUC loss.",
                    "label": 0
                },
                {
                    "sent": "So here lower is better, but it's a rank based metric.",
                    "label": 0
                },
                {
                    "sent": "We're trying to get as low as possible and here are results across different class distributions.",
                    "label": 0
                },
                {
                    "sent": "So here on the left we've got extreme minority classes and on the right we've got fairly balanced datasets.",
                    "label": 0
                },
                {
                    "sent": "And we're seeing a general trend.",
                    "label": 0
                },
                {
                    "sent": "Which is that the ranking only and the combined method both do about the same on this rank based loss function.",
                    "label": 0
                },
                {
                    "sent": "But the regression only method does significantly worse, especially in the extreme minority class data.",
                    "label": 0
                },
                {
                    "sent": "This is a log rhythmic scale on the vertical axis, so.",
                    "label": 0
                },
                {
                    "sent": "You might say final.",
                    "label": 0
                },
                {
                    "sent": "We just always use ranking only.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then if we look at a regression based metric, mean squared errors now on the vertical axis, we can see that now the ranking only method that did so well on the rank based losses now doing terribly at the regression based metric.",
                    "label": 0
                },
                {
                    "sent": "But the combined method is doing as well as the regression only metric, and in fact it's doing a little bit better.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a log rhythmic scale is doing a little bit better.",
                    "label": 0
                },
                {
                    "sent": "This statistically significantly better across this range of extreme minority class distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why would this be true?",
                    "label": 1
                },
                {
                    "sent": "How is it possible that adding in this ranking information gives us actually better regression estimates at the extreme minority class distributions?",
                    "label": 0
                },
                {
                    "sent": "Well, you could think that these rank based constraints are informative.",
                    "label": 1
                },
                {
                    "sent": "So imagine you two biased coins.",
                    "label": 1
                },
                {
                    "sent": "Once it comes up heads with probability .0, two another one comes up probability of .03.",
                    "label": 1
                },
                {
                    "sent": "If you have some third coin that you want to estimate its probability of coming up heads, it's really helpful to know that it's in between those first 2 coins.",
                    "label": 0
                },
                {
                    "sent": "So these rank based constraints can be extremely informative.",
                    "label": 0
                },
                {
                    "sent": "For extreme minority class data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving on to looking at areas where we want to say, you know, mimic the predicting the number of stars.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the liter benchmark datasets for learning to rank.",
                    "label": 1
                },
                {
                    "sent": "These tasks have multiple relevance levels of 1, two, or three stars we can think.",
                    "label": 0
                },
                {
                    "sent": "And here we're using a squared loss function because we're trying to actually predict the ordinal values of 0 one or two with our regression methods.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the same kind of pattern here are rank based performance metrics and ranking only ANSI are doing about the same.",
                    "label": 0
                },
                {
                    "sent": "The differences are not statistically significant.",
                    "label": 0
                },
                {
                    "sent": "An regression only is trailing quite a ways behind on these rank based performance metrics.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the regression based performance metrics, rank only is really suffering and the combined method is doing now almost as well as the regression, only it's not doing exactly as well as regression only.",
                    "label": 0
                },
                {
                    "sent": "I think that this is because the class balance is around 30% of sort of relevant versus not relevant as opposed to one of these extreme minority class situations.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One final set of experiments is click prediction, so looking at that trying to estimate clickthrough rates on sponsored search ads, this is from a private data set of several million ads.",
                    "label": 1
                },
                {
                    "sent": "Drone from internal data.",
                    "label": 0
                },
                {
                    "sent": "It's very high dimensional feature space, and again we're using logistic loss for trying to estimate these probabilities.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the actual numbers.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry for putting a chart of numbers up here for you, but at least I'm not reading them off to you.",
                    "label": 0
                },
                {
                    "sent": "What we see is that for mean squared error regression only and the combined method are doing exactly the same.",
                    "label": 0
                },
                {
                    "sent": "An for AUC loss.",
                    "label": 0
                },
                {
                    "sent": "So again trying to get as low as possible in this rank based method ranking only and the combined method are doing equally well.",
                    "label": 0
                },
                {
                    "sent": "So really are getting best of both perform.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here, and in particular, you could think of regression.",
                    "label": 0
                },
                {
                    "sent": "Only someone from deep from Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Mentioned that logistic regression is considered sort of state of the art for TTR prediction.",
                    "label": 0
                },
                {
                    "sent": "We're getting a point 8% improvement on AUC loss over this logistic regression, and you might say well point 8%.",
                    "label": 1
                },
                {
                    "sent": "That's not really that big, but it is statistically significant because it's a very large data set and at the scale that search engines work at, even very small.",
                    "label": 0
                },
                {
                    "sent": "Relative improvements like this can have a large absolute improvement.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we had this tradeoff parameter that says OK, how much?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ranking versus how much regression are we doing?",
                    "label": 0
                },
                {
                    "sent": "And it wouldn't be much fun if we introduce a new method, but then you have to spend all your time tuning Alpha so we did a little bit of analysis to see how sensitive we were to various values of.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is for one particular task from the MCV.",
                    "label": 0
                },
                {
                    "sent": "One data we've got AUC loss in red.",
                    "label": 0
                },
                {
                    "sent": "We've got mean squared error in Green Ann from left to right.",
                    "label": 0
                },
                {
                    "sent": "The different values of Alpha's over here on the right.",
                    "label": 0
                },
                {
                    "sent": "We've got regression only and over here on the left we've got ranking only, and we can see that there's really a good range of intermediate values of Alpha that will give us much better results than if we were using only regression are only ranking.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be pretty pretty robust across a range of.",
                    "label": 0
                },
                {
                    "sent": "The values.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to wrap up, we do find that this combined ranking regression gives us best of both performance.",
                    "label": 1
                },
                {
                    "sent": "The fairly simplistic method that uses pairwise method for rank based components which makes it scalable and robust.",
                    "label": 1
                },
                {
                    "sent": "But we think that this is probably with any luck, the sort of tip of the iceberg for looking at ways to combine rank loss and in regression loss, so it may be possible to look at joint optimizations that optimize mean average precision or normalized discounted cumulative gain explicitly so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With that, I'll take any questions I would like to note that there is open source code that you can download and play with.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So happy.",
                    "label": 0
                },
                {
                    "sent": "Text here.",
                    "label": 0
                },
                {
                    "sent": "So I wasn't sure how your approach is different than ordinal regression.",
                    "label": 0
                },
                {
                    "sent": "While the rolling off 2 samples are also taken into account and the regression algorithm, so it's a combination of ranking plus regression, yes, so that's interesting.",
                    "label": 0
                },
                {
                    "sent": "We should probably talk a bit more offline.",
                    "label": 0
                },
                {
                    "sent": "Please come to session.",
                    "label": 0
                },
                {
                    "sent": "Send the speaker off.",
                    "label": 0
                }
            ]
        }
    }
}