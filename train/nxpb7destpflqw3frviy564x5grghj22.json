{
    "id": "nxpb7destpflqw3frviy564x5grghj22",
    "title": "Inferring structure from data",
    "info": {
        "author": [
            "Tom Griffiths, Computational Cognitive Science Lab, Department of Psychology, UC Berkeley"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2010_griffiths_isfd/",
    "segmentation": [
        [
            "Alright, so our plan is we're going to run a little long in this session and then we'll run short and Josh section next.",
            "It will go to the break a little later, but you'll get the break the next session and then we'll have the debate as usual, but I'm going to talk about is going to be semantically related to what Josh will talk about in the next session as well.",
            "We're going to be talking about two different approaches to answering this question of kind of how much structure you can infer from data you observe.",
            "This kind of fundamental statistical question, a question that has a lot to do with soda.",
            "How it is that people are so good at making inductive inferences and about the kinds of representations that they form when they're trying to understand the kinds of processes that going on in the world around them?"
        ],
        [
            "Any on how quickly this goes?",
            "I could give you a little more detail about the study that Nick very quickly went over about discriminative generative models, but we'll see how we go.",
            "So what I'm going to do is return to this kind of list of parallels that we have between human learning and machine learning problems.",
            "On the one side, being the things that we want to understand about human cognition, and on the other side being the things that we kind of know how to do in machine learning, and I'm going to focus on one of these today, and this is going to be nonparametric Bayesian methods as a way of understanding how it is that we can form certain kinds of representations.",
            "So the basic question that this engages is a question that comes up a lot when you're talking about a cognitive."
        ],
        [
            "It's a question of how much structure we should infer from the world around us.",
            "So you can think about lots of learning problems encountering this kind of issue.",
            "If you wanted to, say, learn categories of objects, well, how many of those categories do you need to learn in order to represent the kind of structure that exists in the world around you?",
            "If you're just sort of forming clusters of things that you see, how many clusters should you use in order to represent the amount of structure that's going on in that world?",
            "If you're trying to figure out the properties that objects have, how many features does an object have?",
            "If I have, you know.",
            "Something like this?",
            "This is my object.",
            "Well, you know you could think of many ways to describe this and what's the right way of sort of coming up with a description of the physical properties that this object has is a visual object?",
            "Or say if you're learning a language, one sort of problem, you might have to resolve as an infant is something like figuring out how many separate words there are in a language.",
            "One of the things that we have to do is in very early part of learning languages to figure out, say where the gaps are between words in a sentence because you get, you know, basically a stream of sound, and if you've ever heard an unfamiliar foreign language being spoken, you're not even sure where the words might be.",
            "In that stream of sound, so you have to figure out not just what the gaps are, but what even are the words that people might be using in that language before you can go on and learn things like how it is that those words map onto things in the world, or how it is that those words are combined together to make up sentences.",
            "And likewise there's a similar kind of problem if you're trying to figure out what the grammatical principles are, you still have this sort of kind of difficulty figuring out well how many of these things are there, and what's different, and in what cases can I use this one and this one?",
            "They all have this kind of parallel structure of being uncertainty about just.",
            "How much stuff you should be inferring from the data that you see?",
            "Another sort of way of putting this as a learning problem, framing it perhaps in the language of Bayesian inferences about the hypothesis spaces that people are using when they're solving these problems of forming categories or learning language where the issue is that what we really want to be able to do is to define models which allow us to express rich unbounded hypothesis spaces so rich in the sense that they capture kinds of structures that are in the world, and that you know, give us a way of sort of talking about the source of knowledge that people have an unbounded in the sense that we don't want to make a model which says.",
            "So there are three kinds of categories, or there are four kinds of rules, or there are five kinds of words we want to make models which can figure out how many categories, rules or words they want to use in order to describe the world.",
            "So it's very unsatisfying if I say, here's my model of human cognition.",
            "It only works if there are less than 20 things, right?",
            "Like that.",
            "That's not a very good model of human cognition.",
            "We want to make something which can figure out how much structure there is so."
        ],
        [
            "Nonparametric Bayesian statistics actually provides us with a set of tools that we can use for solving these kinds of problems and the way that this works is to say, if you assume that the world around us contains infinite complexity, then we can assume that through any finite experience we might only observe part of that.",
            "So rather than sort of saying, oh, there's some finite amount of structure in the world and our job is to go out and discover how much there is, so it's not saying, oh, there are just six kinds of categories and seven kinds of features, and you know 8 words in this language, it's saying.",
            "In fact, we're going to commit to the idea that there's potentially infinitely many categories and infinitely many features of an object, and infinitely many words in a language.",
            "It's just that we don't know how many of those we've encountered so far, and this sets up the problem in a slightly different way.",
            "It's not kind of figuring out how much structure there is in the world, but how much structure were justified in believing exists based on our observations under the expectation that there's kind of an infinite amount of stuff out there.",
            "So when I parametric Bayesian statistics the word nonparametric means a lot of different things.",
            "When people use it in statistics.",
            "One of the things that means is basically defining probability distributions that don't assume a particular parametric form, but another thing, it means something like models that become more complex as you get more data.",
            "That increasing complexity with the data that you observe.",
            "And that's kind of the sense that we have here.",
            "This approach allows us to define these models which are kind of arbitrarily complex and then can capture whatever structure actually is present in the data that we see, and the basic strategy is then you know, having to find these infinite hypothesis spaces, we can use the casting process is to define probabilistic distributions over those that we can use as prior distributions in Bayesian inference.",
            "So you already heard a little bit about Gaussian processes from Neal.",
            "So Gaussian processes are an example of a nonparametric Bayesian approach which you can use for making inferences about functions.",
            "I'm going to talk about two other nonparametric Bayesian methods which are based on combinatorial stochastic processes, so they help us to get this issue of being able to define richly structured hypothesis spaces that still have the same property of being unbounded.",
            "So I'm going to talk about models based on the Dirichlet process or the Chinese restaurant process which.",
            "I'll explain their relationship in a moment or the beta process in the Indian buffet process.",
            "So these sound like you know, funny names for things they actually are funny names for things, but what they allow us to do is to define probability distributions over exactly the kinds of objects that we need to make.",
            "The sorts of inferences that I've been talking about.",
            "So they allow us to define distributions that we can use to infer how many clusters there might be, or how many words or how many rules or distributions over combinatorial objects that allow us to make inferences about how many features might be expressed in some data.",
            "So what I'm going to do here is give a relatively nontechnical introduction to these ideas.",
            "There's a lot more technical material behind these things.",
            "If you want to hear more about the technical stuff, you can ask me about it later on.",
            "Maybe I can do a mini tutorial on the beach and we can go into further details, but what I'm going to do is introduce these things and introduce them by thinking about some of the cognitive problems that they actually are useful for examining, and So what I'm going to do in each of these cases is pick one cognitive problem.",
            "And then kind of go through it and show how an idea from nonparametric Bayesian statistics helps us to answer some of the questions that comes up."
        ],
        [
            "So the first of these problems is the problem of categorization.",
            "Says something that you heard lots about already.",
            "So mostly you've heard about this from a discriminative perspective, so I'll talk about it from a generative perspective, which might be useful as we go forward.",
            "But the basic idea is that you might be provided with examples of two different categories.",
            "So here is an example of a dog and a cat, and then you see a novel object and are asked to categorize that object in terms of which of those categories it belongs to.",
            "So what do you guys think?",
            "Yeah, that's pretty good.",
            "You guys have got these categories down.",
            "OK, so the question that psychologists want to ask is how it is that people actually go about representing these kinds of categories.",
            "And you know psychologists and proposed a bunch of theories which are really about the underlying psychological processes that go into forming these sorts of representations.",
            "So in the history of the sort of study of categorization, there have been lots of proposals that people have gone through to focus on a couple which have been sort of a big part of that history."
        ],
        [
            "One of these views is called the prototype view, where the idea is that you observe many different cats and then what you do is you kind of form some abstractions, some kind of idealized cat.",
            "That's your prototypical cat, and the way that you decide whether something belongs to the category is by comparing it to that prototypical example that you have stored in memory, and if you've got something where you want to start, if it's a cat or a dog, you compare it to your typical cat and your product typical dog and assign it to the category which is closest.",
            "Another approach that you could use for learning a category is."
        ],
        [
            "Using an exemplar model, so the idea behind Exemplar models is that what you do is you actually just store every instance of a cat that you've ever seen in memory, and then when you want to decide if something out that knew is a cat, you compare it to all of those instances.",
            "So the way that you would decide if something is a cat or a dog, as you see it, you kind of call up all of the cats and call up all of the dogs that you've seen.",
            "You compare its similarity to all of those examples and add them up to the two different categories, and then you assign it to the category where the sum of the similarity over all of the examples is greatest.",
            "So these are two kind of radically different ways of thinking about how you represent a category.",
            "In one case, you're kind of abstracting this thing away, which is not something that you've ever seen is just your kind of idealization of that.",
            "And in the other case, you're making no abstraction at all.",
            "You just remembering all of the things that you saw, and then using those every time you want to make an inference.",
            "Obviously there are strategies that."
        ],
        [
            "Lie somewhere in between these and people have explored these as well.",
            "So for example, you might decide that there are certain kinds of cats and form sub prototypes for each of those different kinds.",
            "So you might have you know fluffy cats and non fluffy cats.",
            "And you know maybe funny looking cats as your different sort of sub categories of cats and then you sum up the similarity to each of those different prototypes and as a consequence of doing that you would then use that rather than all of the exemplars or a single prototype for representing the category.",
            "So these are psychological theories, their theories about.",
            "You know what are those that are cognitive processes that people go through when they're thinking about, you?",
            "Know, categorizing an object and we can now, maybe, you know, in the spirit of what we're doing here looking at thinking bout connections to machine learning and so on.",
            "Take a step backwards and think about what the ad and allying computational problem is that people might be solving in trying to solve this problem of category."
        ],
        [
            "Session well, one way that we can formulate this computational problem is to think about it as an inductive problem where we have some data.",
            "This stimulus, which is the features of the object that we observe and we have hypothesis about what categories those things might correspond to an as an inductive problem is something that we can solve by using Bayes rule.",
            "We can basically you know if we know what the prior probability of each category is and we know a distribution over stimuli which are associated with each category, then we can use Bayes rule to work backwards and calculate.",
            "The probability that that.",
            "That stimulus belongs to that category.",
            "So it's worth saying this is the generative version of the formulation of the computational problem.",
            "It's the one that says that associated with each category you have an underlying probability distribution, and then you have some expectations about the probabilities of those categories and it's those things that you combine together in order to make this inference, and you know, as you've seen, there's also a discriminative way of formulating this.",
            "I'm going to sort of build off this generative version, but we can talk some more about the connections to discriminative methods in the discussion so.",
            "Assuming this generative view of the computational problem, we've now sort of said OK.",
            "If you know these things, the probability distribution over stimuli given the category in the prior probability of the categories, then you're set.",
            "All you have to do is use Bayes rule.",
            "Well that gives us a new challenge.",
            "We have to figure out what these probabilities are and the hard one is pretty easy to workout.",
            "The prior probability of each category.",
            "You can just kind of add up how often you've seen cats and dogs and so on, and then end up with your distribution.",
            "The hard part is figuring out what the distribution over stimuli for each of those categories is, so this is.",
            "You want some way that you're going to figure out what the distribution over.",
            "You know all kinds of animals might be if the way that you're going to represent your knowledge of what cats are and what dogs are is, say some kind of probability distribution, you need to sort of fix that distribution over animals for each of those categories so."
        ],
        [
            "Statistics this is the problem of density estimation, so we need to estimate this probability distributions.",
            "Statisticians tell us there are two ways to do this.",
            "We can use parametric density estimation or nonparametric density estimation.",
            "Parametric density estimation.",
            "You choose a particular parametric family and then you estimate the parameters of that parameter family as a way of calculating what this probability distribution is.",
            "In nonparametric density estimation, you want to take a strategy which is going to be one which allows you to identify probability distributions that lie outside a particular parametric family.",
            "So the weakness of parametric density estimation is having made a commitment to a particular distributional family.",
            "Then it might be the distribution you want to learn lies outside that family, and you're not going to do a good job of learning it.",
            "So nonparametric methods are designed to give you a way of learning arbitrary distributions, but it typically takes more examples, more data for you, a bit to be able to get a good estimate of what those distributions are.",
            "So what's interesting is that some mathematical psychologists have shown that these two approaches to density estimation, calculating what that probability distribution is.",
            "Actually correspond formally to prototype and exemplar models, so those two strategies that I talked about a psychological models actually translate into two strategies for estimating a probability distribution.",
            "When you're taking this view that what you should be doing is basically applying Bayes rule and just to kind of give you an intuition, I'm going to go through these two examples and you can hopefully see the connection.",
            "It's also interesting to note that there are discriminative analogs, so if you took the discriminative view of the formulation of this computational problem.",
            "There are also discriminative analogs of both prototype and exemplar models, so this is an interesting case where both of these perspectives actually give you a way of explaining these existing psychological models, but from a generative."
        ],
        [
            "Active the connection between the prototype model and parametric density estimation.",
            "Well, if we assume that this probability distribution has a simple form characterized by some parameters, Theta.",
            "Well, say we're going to assume that probability distribution is something which is, say, Gaussian distribution, and these parameters Theta would correspond to say the meaning of that Gaussian.",
            "We have to make some assumptions about the variance, but in the simplest case, we could just assume that we're going to have an equal variance for all of the categories that we're estimating.",
            "Then, if that Theta describes the mean of the Gaussian, that means that as we get further away from the mean, the probability of stimulus under that category is going to decrease.",
            "And if you imagine having two of these distributions and then doing Bayes rule to calculate which distribution you should assign an observation to what's going to matter in that assignment is just your distance from the mean of the category.",
            "So the mean of the category plays the same role as the prototype, right?",
            "I said the prototype model you think about your sort of Canonical ideal members, and then you compare the distance to those ideal members.",
            "Well, if you have a parametric model where probability decreases the distance from some central tendency, Gaussian is 1 example.",
            "There are lots of other distributions like that, then distance is going to be the only thing that matters to categorization, and this makes the same kind of predictions about how it is that you should inform these categories.",
            "So this parametric method actually corresponds quite nicely to the prototype model."
        ],
        [
            "For nonparametric density estimation, standard frequentist method for solving this problem is to use kernel density estimation.",
            "You've heard already about these kinds of methods.",
            "The basic idea is that what we would do if there's some true distribution here in red that we want to estimate the way we would estimate it, is take each of the observations that we've sampled from that distribution and put down a little kernel.",
            "A little probability mass on each of those observations, and then sum those up and the sum over all of those kernels is going to be our approximation to the target distribution.",
            "And as the number of observations we have increases, the quality of that approximation is going to improve, and we can do some clever things about choosing the variance of the kernel appropriately.",
            "And so on.",
            "And those are things that Bernard was telling you about, but the basic idea is you put a little bit of probability over each of your observations, and then some of those up, and then that's going to give you a probability distribution.",
            "Well, the correspondence between this and the exemplar model comes from.",
            "Remember what I said in the example model, the way that you assign something to a category is, you think about all of the examples of that category you've seen in the past and then.",
            "You add up how similar your observation is to each of those.",
            "Well, you can think about that as saying for each of your observations, each of your examples you're going to have a similarity function which is over that exemplar, and then you're going to some the similarity functions, and that's going to be your measure of how well that thing fits with the category.",
            "And that's basically exactly what we're doing in kernel density estimation.",
            "The kernel is like our similarity function.",
            "We're going to Add all of those up, and that's going to act like you know that's going to give us the probability under the category, which is our measure of how well that think that's with the category exactly like in an exemplar model.",
            "So you get this nice formal equivalence everybody clear so far.",
            "Yeah, OK, so one thing that's interesting about this is that there's also a nice sort of probabilistic analogs."
        ],
        [
            "Those strategies that lie somewhere in between.",
            "So if you wanted to take something where you didn't have a single prototype and you didn't have a distribution for each example, you could take something where you have a, say a mixture distribution where you have more than one mixture component per data point.",
            "So the kernel density estimation example is a mixture distribution.",
            "Mixture distribution just means that the way I represent my probability distribution is a mixture of some components, where each of those components as a distribution and is assigned some weight, and those weights them up to one.",
            "In the kernel case you have lots and lots and lots of components, one for every observation that you've got.",
            "In this case we could say.",
            "Well, maybe I'll just use, you know, four or five components, and for each of those components I'll have the analog of a prototype and I'll sum up the probability associated with each of those components.",
            "And that's going to give me my estimate of this target distribution.",
            "So this idea of using a mixture model is interesting.",
            "It's nice, it gives us a way of having something in between exemplar and prototype models, but it also gives us a new problem.",
            "So if you're going to decide to represent your category in terms of.",
            "Some of these subcomponents you can think about them as kind of clusters of things that you could sort of categorized together that are well represented by a single prototype, and so on.",
            "The problem that we run into is the problem of determining how many of those clusters we should use, how many components we should have in our mixture model, how many things we should be clustering together and representing with a single prototype, and so that's exactly this kind of problem that I was talking about.",
            "It's A kind of how much structure problem, right?",
            "It's a problem where we need to workout based on the data that we see, how much we should believe.",
            "How much structure we should believe is expressed in that data?",
            "How many clusters of things do we need to explain what's going on in our data set?",
            "And so this is a problem that now we need to sort of.",
            "Think about how it is that we can define a model that allows us to make inferences about the number of clusters that we need as well.",
            "As you know, being able to sort of tell us then based on those clusters what the probability is of our observed stimuli under the category.",
            "So there's a nice computational model that does this that was proposed by psychology."
        ],
        [
            "Call John Anderson, who's actually made significant contributions to this kind of approach of making rational models of cognition, so he sort of thought about this problem, and he said, look, I'm going to come up with a way of defining a model that doesn't require me to commit to the number of clusters that I'm going to use so Anderson's rational model, he says.",
            "First of all, let's treat category labels just like any other feature of an object.",
            "So rather than saying I'm going to estimate one probability distribution for each category, Now I'm going to say I'm just going to estimate one big probability distribution, and I'm going to add on the category label as an extra feature.",
            "Which appears in that probability distribution and now if I do that then I'm trying to learn a joint distribution on these features including the category label and I'll use a mixture model for that.",
            "So I'm breaking my objects up into clusters just like I do in a mixture model.",
            "But he introduces a sort of clever trick which is to allow the number of clusters that are going to appear in that mixture model to vary, and the basic idea is that each time you see a new object, you compute a prior probability distribution that that object is assigned to a particular cluster.",
            "And in that probability distribution, it's possible for an object to get assigned to an old cluster.",
            "One of the ones that you've already assigned objects too.",
            "But it's also possible to get it assigned to a new cluster.",
            "So the way that he does this is he says will take the probability of choosing a particular cluster to be proportional to the number of things that have been assigned to that cluster in the past.",
            "Assuming it's an old cluster and then with some parameter Alpha, will that will determine the probability that I choose a new cluster.",
            "And So what we get out of this is, you know, say we've already observed.",
            "A bunch of instances of category A, so you could think about these.",
            "As you know, we've observed a bunch of cats.",
            "So say we've seen 9 cats already and we're trying to make a decision as to, I guess these are these are this is not for a single category.",
            "These are examples which are for these things, which could be both cats or dogs, so they shouldn't say Category 8 here.",
            "So 'cause he's trying to define a joint distribution features.",
            "So I've seen things which are cats or dogs, and we assign those things to clusters and you know, we're pretty sure that we've seen you know four things that cluster together here, and two things that cluster together here, and three things that.",
            "Together, here and now, the question is, what am I going to do when I see my 10th observation and the idea is I'm going to say well before I even see it.",
            "I think there's some chance is one of these things I've seen before, and I think it's more likely to be something that I've seen more often in terms of the different clusters, and there's some chance that it's something completely new, right?",
            "It seems kind of reasonable, So what you might notice is that this is actually a way to define a probability distribution over essentially an infinite number of clusters, right?",
            "As we observe more and more data, it becomes possible for us to observe something new every single time.",
            "And while the probability of observing something new is going to decrease because of the way that you know Alpha, here is kind of like the number of counts which we're going to compare against all of the accounts that we have are things that have happened in the past.",
            "So as we get more observations and there's more mass builds up in these things we've seen before, that probability of drawing something new is going to decrease it.",
            "Still, in principle possible at every point to observe something that's completely new, you can kind of think about this like if you were a Explorer and went to a new country.",
            "So Australia, right?",
            "You'd seen some.",
            "A number of different animals before and so you've identified some species or things.",
            "These are the species that you've seen before and you get to Australia and you're like, wow, what's that, right?",
            "Like you suddenly see something like a kangaroo that doesn't fit into any of the categories that you've seen, but having this kind of model which allows you to then form a new cluster, you're able to go.",
            "That's one of those as well, and that's another one of those, and that's another one of those.",
            "Wait a minute, what's that right?",
            "And you see a koala and so on, but at every point it's possible for you to be encountering something that you've never seen before, so this kind of model gives us the capacity to represent an infinite number of clusters, and to always be expecting that it's possible to see something we haven't seen before.",
            "So this is a very elegant way of defining this sort of distribution.",
            "It has some nice properties which I'll go into in a moment.",
            "But what's interesting about this is that this model, which Anderson came up with is actually a model which has been studied in statistics, something that's called a Dirichlet process mixture model.",
            "So this is kind of the jewel of nonparametric Bayesian statistics.",
            "It's a model which allows you to define a distribution which is nonparametric in the sense that it allows us to model.",
            "Obituary, probability densities, and it's something which grows in complexity as we get more data in a way which just uses Bayesian inference, and the idea is that we have sort of being very clever about the way we're defining our prior distribution over these clusters, allowing the complexity of our model to increase with the complexity of our data.",
            "So this way of defining our prior distribution is something that, in the study of combinatorial stochastic."
        ],
        [
            "This is called the Chinese restaurant process, so it has this funny name because the statisticians who came up with it were actually statisticians at Berkeley and spend a lot of time going to Chinese restaurants in San Francisco and they sort of thought that this process might describe.",
            "Basically, you need to make an assumption that you have an infinitely large restaurant with an infinite number of tables, each of which can seat an infinite number of people, and they thought that was best described by these Chinese restaurants.",
            "OK, so the idea is that we have an customers who walk into a restaurant and they're going to choose tables to sit down on with probability, which is proportional to the number of people who are currently seated at that table, and then with some probability, start the next unoccupied table.",
            "What's important about this process from the perspective of actually using these models and thinking about the assumptions that they make, is that out of this we get something which is called an exchangeable probability distribution over seating assignments.",
            "So what this means is that it doesn't matter what order the customers entered the restaurant.",
            "If you've got, you can think about this kind of partitioning the customers up so that they're all sitting at different tables.",
            "It doesn't matter what order the customers enter the restaurant, the probability of getting that partition of customers will remain the same, and it's just a consequence of the way that this probability distribution is defined, and so that's kind of it's useful from the perspective of thinking about defining our models often, we don't want to assert some kind of ordering on data that we see.",
            "We might not even know what order the data we see were generated in, but it also makes it easy to do probabilistic inference in these models, and that's a technical detail that I'm going to skip over.",
            "But the basic idea is that if you ever need to compute a conditional probability of a.",
            "Conditional probability for observation belonging to a cluster.",
            "Then you can do that very easily using this exchangeability assumption.",
            "'cause it means you can always pretend that the customer you're interested in was the last customer who entered the restaurant, and that's useful for doing Gibbs sampling, which I'll talk about tomorrow.",
            "Think about this.",
            "As you know, you take one of the customers away.",
            "He goes to the bathroom.",
            "He comes back and because of exchangeability, if he follows the same process for grabbing a seat, then you know that's the same as if the customers that will come in and that order.",
            "Initially the probability of getting that configuration is appropriate.",
            "So this exchangeability assumption is something which is desirable from a perspective of thinking about making sort of sensible, understandable probabilistic models, but also from the perspective of being able to do probabilistic inference.",
            "Yeah.",
            "That you want the distribution over potentially for number of clusters, which is extendable.",
            "We do somehow automatically arrive at this.",
            "So the question is if you desired exchangeability, would you automatically derive this as the consequence, right?",
            "So at what's interesting is Anderson's choice of this distribution was based on those kinds of considerations, so he wrote down a bunch of axioms that he wanted this distribution to satisfy an, then derive this distribution as the result.",
            "But it's stronger than just exchange ability.",
            "There are many other exchangeable distributions on partitions, and this is just one example.",
            "So you can go to his book and you know he has an appendix.",
            "He goes through what those axioms are.",
            "Any other questions?",
            "OK, alright so."
        ],
        [
            "Sure.",
            "So this translates into what's called the Dirichlet process mixture model.",
            "Dirichlet process is a more complex stochastic process, which is basically a way of defining a distribution over infinite multinomial's.",
            "So multinomial distribution is what you have when you're taking a single choice from a discrete set of options.",
            "When you have a finite set, you can imagine having an infinitely large set of discrete options.",
            "That's essentially what we have when we want to have an infinite mixture model, and then you need to define a way of generating a probability distribution over that infinite set of options.",
            "Well, the Dirichlet process is a way of producing those kinds of distributions, but I think what you for the present purposes I'm not going to talk much about the during the process.",
            "the Chinese restaurant process is what you get when you integrate out that multinomial distribution in the same way that you might integrate out of garishly distribution.",
            "Multinomial distribution with respect to a garishly prior.",
            "So if that made sense to you and you want to learn more, come and talk to me.",
            "If that didn't make sense at all, don't worry 'cause we're going to go on, and you're not going to know it.",
            "So the way that we then define mixture model are Dirichlet process.",
            "Mixture model is that we kind of imagine our Chinese restaurant having some parameters on each of the tables.",
            "So these parameters are going to correspond to the parameters that are associated with those mixture components telling us where that component is located and what its variant size and so on.",
            "The kinds of things that you want to know about a component of a mixture model.",
            "So there's some process from which we've sampled these parameters.",
            "We put down some parameters on every table in our restaurant, and then the customers come into the restaurant.",
            "And our customers are going to be our data points.",
            "They're going to sit down at the table according to the CI P. And then we're going to generate our observations by each customer, sort of spitting out an observation which follows the distribution characterized by the parameters that are sitting on that table.",
            "And this is our whoops generative process, and we can invert this generative process to figure out how many clusters might be expressed in some data that we observe, and we can do that using some of the methods I'll talk about tomorrow.",
            "So the idea is that using a Dirichlet process mixture model.",
            "You can define a mixture model which allows you to have an unbounded number of components, and this in some sense gives you a way of interpolating between something like a prototype model where you've got just one single sort of summary representation for a category, and something like an exemplar model where you have to remember all of the examples every time you want to make an inference.",
            "But it also gives us the basis for coming up with a richer description of what might be going on in cat."
        ],
        [
            "There is a shun, so this idea of density estimation that I've been talking about as a unifying kind of framework for thinking about categorization models.",
            "It's a way that we can think about categorization models characterizing the different options that might be.",
            "You know what's going on inside peoples heads in terms of how they representing categories.",
            "But we could go beyond this to define something which we call a unifying model.",
            "SO1 model, of which all of these other models are special cases.",
            "And in that model, if we can characterize all of these sort of previous proposals, the special cases of that model.",
            "Then it gives us the opportunity to think about what people are doing when they're learning categories, is perhaps adaptively choosing between these different kinds of strategies that they could be using.",
            "So if we really think about what the best way to solve the problem of categorization might be, it's not going to be saying I definitely want to use a prototype model.",
            "I definitely want to use an example model for every single category that I encounter, right?",
            "I said prototypes work well when you are trying to estimate a distribution in that distribution matches the family that you're estimating the distribution from.",
            "That's when parametric density estimation works.",
            "So we've got a small amount of data.",
            "And you think your assumptions about the distribution of the shape of the distribution are satisfied?",
            "That's a good way to go.",
            "Example, our models are flexible.",
            "They can represent any kind of probability distribution using this kind of kernel, like method, but they require lots of data in order to converge to a good representation of that distribution.",
            "So it seems like in general, what you want to be able to do is kind of adaptively select which of these sorts of strategies you're going to use based on the structure that's reflected in the data, and so if we can define a model which has all of these sorts of models are special cases, then we can actually think about.",
            "What might be going on in human categorisation is just adaptively selecting between these different strategies for representing categories, and we can actually do this using the Dirichlet process.",
            "We have to get a little more complicated.",
            "We can think about it in terms of two interacting levels of clusters, and this is something which we get from the hierarchical Dirichlet process, so it's not that much more complicated if."
        ],
        [
            "I kind of understood the Chinese restaurant process.",
            "It's the same kind of idea.",
            "Now what we're going to do is we're going to take a step back.",
            "Remember, Anderson said we're going to think about one big probability distribution, which is going to represent both of those categories, and we're going to think about labels is just being a special kind of features.",
            "Well, one problem with that is that it means that you'd be using exactly the same components and clusters for representing all of the categories that you learn, and that's probably too strong assumption.",
            "You don't necessarily want to think about.",
            "You know your category for cats having in it the same cluster as you know your Category 4.",
            "Chairs or something like that, so it makes a very strong assumption that you're going to use the same basic vocabulary as the building blocks of every single category that you represent, so we can break that assumption by assuming that we're going doing.",
            "We go back and we say we're trying to estimate a probability distribution for each of the categories that we're learning category A and category B and what we're going to do is have a separate Dirichlet process for each of those categories.",
            "So we have this same idea that what you're going to do is going to be forming clusters for each category and those clusters.",
            "There's always the possibility you see something new which is a member of that category that you hadn't seen before.",
            "And that new thing is going to be something which you know is the probability of getting a new thing is parameterized by this parameter Alpha of the Dirichlet process.",
            "But when you choose to generate a new thing the way that you generate a new thing is by going up to this higher level, and there's a second Dirichlet process up here.",
            "So what that means is that now when we decide to make something new IT might be that it's actually not something that's knew across all of the categories that we've ever seen.",
            "It might be something which is shared by some other categories.",
            "Or it could be something which is completely new, so the idea is that say we're going to decide that we're going to generate something that's new.",
            "Well, what we do is we go to this higher level process in this high level process every time we visited this process to what we saw, each of the clusters that we have represented down here is the consequence of a draw from this high level process to the things that are represented up here.",
            "The properties of these clusters which we have down here.",
            "So the idea is that we'd go up.",
            "We take a draw from that high level process, and then it might be that we would choose a cluster which we've chosen before.",
            "Or it could be that we draw a cluster which is completely new and has never been seen before by any of the categories that we use.",
            "So the reason why we need this higher level processes that if we just had a Dirichlet process for each of these two categories, then there's no way in which they would ever interact.",
            "So that means that you know, while you might not necessarily want to have the same clusters in your representation of cats and chairs, you might want to have the same clusters in your representation of, say, cats and Lions or something like that, right?",
            "It might be that some of the same units are useful for representing those categories that are more closely related.",
            "And not useful for representing categories that are more distant from one another.",
            "So you need to have the opportunity to share those clusters, and so this gives us a way of tying together these these clustering processes down here where the way that we generate the sort of a set of clusters which are shared across all categories with some probability, and then for any individual category we sort of sample a subset of those clusters and then use that to represent the distribution that we see in each of those those categories.",
            "So using this framework actually gives us a way of then defining."
        ],
        [
            "A single, unifying rational model that contains all of the examples that I showed you before and you can do this by thinking about the consequences of varying these two parameters.",
            "One parameter which is describing the rate at which we generate new clusters within categories, another which describes the rate at which we generate new clusters across categories.",
            "So I'm just going to show you some sort of examples of what you get by generating things from this kind of process under these different assumptions about the parameters, so I'm going to show you things that are in sort of rectangles corresponding to categories.",
            "The black dots are going to be exemplars, and the.",
            "Unfilled dots are going to be the clusters that we used to represent things within that category.",
            "So as an example, if we take both of these parameters to Infinity, we get back a model that looks like this.",
            "Where say here we have three different categories representation for these three categories has one cluster for each of the examples that we see, and this corresponds to a familiar model.",
            "This is just the exemplar model.",
            "If we so the idea here is that every time you see an observation within a category, you generate something that's completely new.",
            "That's what this Alpha goes to Infinity means, and every time we generate something new.",
            "It's something which has never been seen in any other category.",
            "That's what this gamma goes to Infinity means.",
            "If we set gamma to 0, then we're going to get something where every category is represented by only a. Yeah, Alpha goes to zero and then we get something where every category is represented by only a single cluster.",
            "But if gamma goes to Infinity then those clusters are different across categories and this is the prototype model, so this is a model where now every time we see something here, there's no chance to generate a new cluster for that category, but the clusters that we and every cluster that we generate is going to be something which is new and not shared with any other category.",
            "But there are other models in the spacer Anderson's model.",
            "Here is the one where we assume that for every time we get an observation within a category we generate something which is completely new, so that space.",
            "Likely ignoring the category membership.",
            "Basically it ends up acting.",
            "We need to sort of augment the features of the objects with the category label, but we ignore any information about the way that those things are grouped together, and then we have.",
            "The possibility that we can generate new clusters which can be shared across categories, and so that's the model that Anderson basically proposed, where we've got 11 sort of vocabulary of clusters which are shared across these categories, and there's no sort of specific category specific substructure which respects the structure of those categories, as well as some models we might not believe in.",
            "So this is 1 where it says.",
            "That we never.",
            "We have one cluster per category, but it's possible for that cluster to be shared between multiple categories.",
            "So this would be kind of like if you lived in a world where categories tended to re label things that you learned before, right?",
            "So if there were multiple names for the same category, then this would be the sort of assumption.",
            "But the most interesting models are this one, in which we have every.",
            "Category being represented by.",
            "It's own directly process essentially, so we have a multiple clusters within each category.",
            "But there's no sharing of those clusters, and this one where there are multiple clusters within each category, and there is sharing between those clusters.",
            "And so these two models are sort of the general case where we allow for the possibility of having either multiple clusters within categories or in this.",
            "In this case, multiple clusters within categories and the sharing of the clusters between categories, and we can go on and sort of look at how well those models describe what people are doing.",
            "So remember the basic psychological idea here is that using.",
            "This way of thinking about learning categories makes it possible for people to adaptively select the category structure that they want based on the information which is provided in the data, and so the question is, do people do that?",
            "So do people change the representation that they form for a category between, say, something like a prototype representation and something like an exemplar representation as a consequence of the data that they observe.",
            "And So what we've done is go and look at expense."
        ],
        [
            "Men's wear there's some evidence for this, and this is work with Kevin Kinney, who's a student who's here you can talk about it with Kevin if you want.",
            "So what Kevin did was look at a particular experiment conducted by Smith and Linda.",
            "The idea is that if we take the HTP where the Alpha parameter is allowed to range between zero and Infinity, and say we fix the gamma parameter Infinity so there's no sharing between categories, what it can do is infer a representation that somewhere between exemplars and prototypes.",
            "Provided we're estimating this Alpha parameter from the data.",
            "So the idea is that we take a structure which seems to show some characteristics of both of these things.",
            "So in this experiment, the way that these are often presented in the psychological literature we might have here, these are talking about objects that have 6 features.",
            "Each of those features can have two values, and this category structure corresponds to this set of objects where there's one object which is takes the value 0 on all of those features, one that takes a one in the first position, one that takes one in the second position, and so on and so on and so on.",
            "You can see that a pretty good rule for learning things that belong to Category A is kind of thinking about it in terms of there being a prototypical example, which is 1 where you know it's something like this.",
            "It's having a single value on all of those features of 0, but there's an exception to that rule, so there's one member of Category A which violates that rule.",
            "So this this exception has the other value on all of its features.",
            "Category B has the reverse structure here, mostly ones, and then one exception which has mostly zeros except for one one, and so the question is.",
            "What happens when people learn something like this?",
            "It's not quite consistent with the prototype representation, although prototype representation works pretty well because it's got this exception.",
            "And So what we do is take."
        ],
        [
            "These data and look at what human learning looks like and what you can see in human learning is here.",
            "The white circles at the top show everything that's in white corresponds to the things that belong to category A and this is showing the probability of people are signing something to category A based on the amount of training that they've seen, and So what happens is for these white circles these are the things that follow that prototype rule.",
            "The White triangle is the exception, so this is the thing that violates that rule.",
            "And then down here, this is the things that belong to Category B and follow the prototype rule and then this is the exception.",
            "So these are the things that violate that rule and what you can see is that people crossover in the way that they treat these exception items.",
            "So initially they categorized them in a way which is consistent with the prototype rule, and subsequently they learn that this thing belongs to category A and this thing belongs to category B.",
            "So this pattern of learning is something which basically reflects a shift from using something like a prototype representation to using something which allows for exceptions like an exemplar representation.",
            "So if you just take a prototype model and you give it these data, then it produces a pattern which looks like this where the exceptions get classified together with the things that follow the prototype rule and so this looks right for the early part of training but wrong for the later part of training.",
            "If you take an exemplar model, then you know the way that it can best fit these data is it can sort of perfectly learn the category membership of these things that follow the rule, because it's just remembering examples and so provided it's got previous examples of those things belonging to that category.",
            "It's very easy for it to decide that they belong.",
            "But for these things, which crossover it has to kind of learn an intermediate probability, because it's not able to change its its representation of those.",
            "So what it ends up doing is kind of learning that those things are weakly associated with these categories in order to match the fact that people's probabilities on average are somewhere in between assigning something to category and assigning someone to category B.",
            "So we use the HTTP model that I was talking about.",
            "What it can do is basically change the representation as it gets more data.",
            "So the idea here is that you're seeing more and more examples of those.",
            "Those stimuli which follow that particular category structure, and so as you get as you get more examples, let me just go back.",
            "What you are what you find is that.",
            "You're basically getting more and more evidence that the prototype model is the wrong representation, so the first time you see it, you could just say, oh, that was just a coincidence that was just chance that I happen to see something that didn't follow the prototype rule.",
            "But as you see, more and more examples of that thing that violates the rule, you get stronger and stronger evidence that you need to use a different kind of representation, and so that's what happens.",
            "Is that initially the model clusters that prototype that violating Exemplar together with the other stimuli, and then overtime it differentiates it and gives it its own cluster.",
            "And forms a different representation of the data and so that model ends up giving us the best fit to the human data.",
            "If we compare the log likelihood predicted by the models with the judgments that people make is the only model which produces this crossover effect.",
            "So this."
        ],
        [
            "Kinds of models are also interesting from the perspective of thinking about how it is that people might learn categories in terms of being able to explain how people could get better at learning categories as a consequence of learning categories.",
            "So in the model where we allow both the Alpha and the gamma parameters, then there's an opportunity for people to basically learn a vocabulary which they could use to organize all of the objects in that domain, and then to use to think about learning in future categories.",
            "So, for example, if you would learn the category of cats and as part of that you formed a cluster which contained all tabby cats, right?",
            "Then, knowing that all of these things behave similarly in the way that they're categorized might be useful.",
            "When you go on to learn a category of stripy things, right?",
            "So if you learn the categories stripy things, and I tell you this is a striking thing, then having already formed a cluster which contains all of the tabby cats, you'd be able to generalize pretty easily that all of the tabby cats are stripy things as well, and so using a model which allows you to share those clusters between categories gives you a way of being able to talk about this kind of transfer learning effect, where people might be able to figure out a kind of vocabulary that they can use in learning.",
            "Degrees and Kevin's been investigating that too.",
            "OK, so this is 1 example where you can use Dirichlet processes.",
            "They get to this question."
        ],
        [
            "How much structure you might be able to infer from data?",
            "There are lots of other examples.",
            "The nonparametric block model that I talked about in the causality lecture is basically a Dirichlet process mixture model extended to deal with relations rather than feature based data.",
            "You can think about making models of language where the numbers of words, syntactic classes, or grammar rules is unknown.",
            "The way that this is done and something that makes it very simple is that anytime you have a model which assumes a multinomial distribution, you can replace that multinomial distribution with one of these Chinese restaurant processes.",
            "So that's a very simple way to take a model which is.",
            "One that assumes something is finite to a model which doesn't make that fixed assumption, and that's basically what we did with this mixture model we said normally you have a multinomial distribution over.",
            "You know the number over cluster assignments of objects, where that's a distribution of finite dimension.",
            "Well, now what we can do is replace that with the Chinese restaurant process an in doing that allow this model to learn infinitely many clusters, and this sort of model can be extended in lots of different ways people have looked at lots of extensions, the hierarchical.",
            "During the process I talked about nested models where you basically generate trees by choosing from the Chinese restaurant process and then choosing again.",
            "Dependent garishly prophecies where basically which cluster you get assigned to depends on some other property of the stimulus.",
            "Uh, two parameters, their process which.",
            "Basically, gives you another sort of control over what that distribution of seating at tables looks like.",
            "Distance dependency, which again allows you to specify things in a way where so you might know something already about the relationships between the people who are coming into the restaurant, such that it makes it more likely that certain people will sit at the same table, and so on and so on and so on.",
            "The second example that I'm going to talk about for just showing you case."
        ],
        [
            "Is where these sorts of nonparametric Bayesian models can be used.",
            "Is the example of learning the features of objects so so far I've been talking about categorization and I said, oh, this is how we represent our stimuli.",
            "We've got some features of those stimuli.",
            "You just have to learn a probability distribution.",
            "Well, actually, coming up with what the features of those stimuli are is a nontrivial task in itself.",
            "So if I show you this object here and I ask you what its features are, right, that's not really a well specified question, right?",
            "So you know, you're working for the story.",
            "We tell our participants in our experiment as you're working for a.",
            "The task Force which is analyzing things that are sent back by the Mars Rover.",
            "the Mars Rover is wandered into a cave, and it takes a photograph and of something on the wall, and it sends this back.",
            "So now how should you record this in your book, right?",
            "How are you going to sort of, you know?",
            "What are the parts that you think are the relevant parts for encoding this as as your record of what was sent back by the Mars Rover, other things that you can kind of pull off and describe about this and you might be able to make some guesses based on your sort of general visual experience, but it seems kind of underdetermined.",
            "So the question we've been looking at is what determines the features that we identify, and in particular how the distribution of objects that you observe is relevant to determining this.",
            "These features of these objects and this is work with Joe Osterweil at Berkeley.",
            "So this question might be."
        ],
        [
            "Easier if I show you some other pictures that were brought back by the Mars Rover, right?",
            "So what are the major over?",
            "Did it actually wandered around The Cave and it took photos of these 20 objects here?",
            "I've just sort of organized them a bit, but here's the object that I just showed you.",
            "Now you can think about what features you might identify if you are thinking about recording the things that the merger over we're sending back, right?",
            "And something that might seem like a good strategy to say, you know, it's sort of all of them seem to have this bit on the bottom.",
            "But then there's this bit on the top, and that varies in a few different ways, and so maybe the way to divide these up is to say.",
            "Well, I'll sort of encoded in terms of the variation that I see in these bits on top right, and there's four different bits on top, and so that seems like a pretty good description."
        ],
        [
            "But you can contrast that with, say, a different case where the Mars Rover sends back pictures that look like this.",
            "These are all inscriptions that is found on the walls of this cave, right?",
            "So here's the example that I showed you before and now if these are the things that you see it having sent back and I ask you what the features of the object are, then you might respond differently so you can take a look at these.",
            "And think about what features you might think are relevant to encode what you might notice is that it seems like there's some structures that are repeated across these objects, so like this bit sort of appears in a few different ones.",
            "And then there's this sort of shape here, shows up here as well, and here, and it seems like maybe that's sort of relevant thing to represent, but it's different here and here, and so on.",
            "And if you actually go through and analyze this, what you might think about is there are just each of these things on top is actually made up of several different paths.",
            "And it's those parts that you should be encoding and writing down as describing the features, right?",
            "Does everybody see there are different parts?",
            "Yeah, OK, So what I've done is change the way that you represent the same object by just manipulating the context in which you see that object, right?",
            "So remember, this is exactly the same thing as this, but now what you think about is the features of this object are going to change as a consequence of having seen these different objects that have nothing to do with like I haven't changed what this looks like, but I've changed the surrounding context and that context influences the way that you perceive the features of that object, and we're interested in being able to explain that effect.",
            "Where the context in which you see things changes the way that you perceive them."
        ],
        [
            "So one way of kind of thinking about characterizing this as a mathematical problem is it kind of dimensionality reduction problem, and in particular we analyze this in terms of binary matrix factorization, where the idea is that you could take each of the objects that you saw, and each of those images with a binary image.",
            "Every pixel was either on or off.",
            "You take each image and turn it into one long vector.",
            "Then you stack them up and you get a big matrix where you've got objects as rows and columns.",
            "Pick out the pixels of those images, and then there's some pattern of pixel activations associated with each of those objects.",
            "And so this is a matrix factorization problem, because what we're trying to do is infer a lower dimensional representation for those things when we're trying to learn the features of objects.",
            "What we're trying to do is come up with a more efficient way of describing those objects rather than just referring to the pixels that those objects express, right?",
            "The pixels that appear in those pictures, and so you can kind of think about this as your goal is to kind of come up with a representation where there might be some relatively small number of features where each of these features is either on or off for this object.",
            "So we have a binary matrix here where it has one row for every object.",
            "And one column for every feature and then some information about how it is that those features map you back into that original space.",
            "And we can do this by actually using a noisy or distribution, just like I used last time.",
            "This is the general form of the noisy or distribution, so it says the probability that we have a particular pixel get turned on is 1 -- 1 minus Lambda, which is the sort of strength of these causes.",
            "And then we take the inner product between the features of the object and these vectors that map.",
            "Each of those features back onto the original pixel space.",
            "And then there's some baseline probability that a particular pixel will turn on.",
            "But the basic idea is that if we're interested in predicting whether this particular pixel is on or off, what we do is we look at the row, which tells us which features that object has, and then we go to the column which corresponds to which particular pixel that is in that image, and then we take the inner product between these two things, and as that inner product gets larger, as there are more features turned on, the influence that particular pixel, then it becomes.",
            "More and more probable that particular pixel is turned on in the image, and So what we're really trying to do, then is take this matrix and split it up into these two matrices, one of which tells us about what the features of the objects are and the other, which tells us how to translate those features into that original visual domain."
        ],
        [
            "So one problem that we have is we need to figure out how many features these objects possess.",
            "So what the dimensionality of these matrices are?"
        ],
        [
            "And that's something which we can do again using a nonparametric Bayesian approach.",
            "If we assume the total number of features is unbounded, but only a finite number will be expressed in any finite data set, then we can think about defining a probability distribution over these binary matrices where we want to have a fixed number of rows between unbounded number of columns.",
            "And we can do that using another kind of stochastic process, something that's called the Indian buffet process.",
            "So the Indian buffet process, as you might imagine, is inspired by the Chinese restaurant process uses a slightly different culinary meta."
        ],
        [
            "But the basic idea is sort of similar.",
            "What we have is now an Indian restaurant, and rather than thinking about seating configurations, we think about what kind of food people eat.",
            "So you have to imagine one of those infinitely long Indian buffets, right?",
            "What happens is the first customer walks into the restaurant and then samples are passed on Alpha number of dishes, so they draw a number of dishes from price on distribution.",
            "In retrospect, I regret not having this called this.",
            "The all you can eat seafood process because then we would have an extra pen with the Python distribution, but there you go.",
            "So then the next customer who comes in is going to taste dishes with probability, which is greater for those dishes which have been tasted before.",
            "So for each of those dishes at the first person tasted, there's a 50% chance the second person tastes them, and then they drop acid on Alpha over 2 number of new dishes, and then the next person comes in, and basically the probability that they taste a dish goes up as those dishes have been tasted before and they sample appointment Alpha override new dishes.",
            "Result of this we get a distribution over, you know, patterns of tasting dishes in which the customers are exchangeable, just like in the Chinese restaurant process."
        ],
        [
            "But if you think about kind of making a little matrix and thinking about our roses corresponding to our customers and the columns corresponding to the dishes and then putting one in that matrix every time a customer tastes a dish, then what we're doing is defining a probability distribution over a binary matrix.",
            "And this is a binary matrix where the number of rows is fixed.",
            "We can say how many customers go into the restaurant, the number of columns is unbounded, and as a consequence it gives us exactly the kind of object that we need in order to be able to learn how many features something has.",
            "So we can define a model taking that.",
            "Binary matrix factorization problem using the noisy orders are likelihood.",
            "Using the Indian buffet.",
            "Processes are prior and feeding."
        ],
        [
            "The images that I showed you before and what happens is that it basically picks out features which correspond to those features, which intuitively seems like a good way of representing these different domains.",
            "So if you feed it these images, what it does is infer that you know these different parts on top correspond to features and those are the things that you should encode.",
            "If you feed it these images, and it infers these different paths where now there's a way of kind of building up each of these objects in terms of possessing these different individual features.",
            "What's important about the way that we constructed these is actually that.",
            "Each of these objects corresponds to something which we can generate using these features.",
            "What happens is each of these objects is generated from three of these six features combined together.",
            "If you take these six features and you think about all combinations of three, there are actually 24 of those.",
            "So what I've shown you here is 20 of those 24 possibilities, the frequency of any one of these individual parts is actually the same between these two sets, because these are made up of the same features, and So what we made sure to do is to make sure that you know.",
            "In each case, there are five instances of each of these features because those features appear and sorry, yeah, those features appear in each of these individual stimulate, so I think there's actually 10 instances or well, we can figure it out.",
            "But what we've done is matched the frequencies between these two cases of these individual features, so there's no sort of guide that you could you could.",
            "You could sort of figure out that these are the right features based on, you know they're being higher frequency here and lower frequency here.",
            "The statistical properties are the same, the pixel wise variances are the same.",
            "The only thing that differs between these sets is the amount of correlation that's expressed between these features.",
            "We're here.",
            "The correlation comes out because we've combined these features together in particular ways, and here this represents basically an arbitrary factorial combination of these features.",
            "So what's good about that?",
            "Is it actually means that if you take a generic kind of statistical method for identifying a lower dimensional representation, it actually fails to do so here, and other cognitive models also don't discriminate between these different ways of representing this data, so you actually need to have something like this sort of noisy or binary matrix factorization model to pull out these features.",
            "But it also means that we."
        ],
        [
            "Can test whether people are forming this kind of generalization where we compare the inferences that they make from having seen these correlated images to the inferences that they make from having seen these factorial images and the basic idea is that what we do is train them by exposing them to these and telling them they come from the Mars Rover and then showing them some new pictures and asking them to rate how likely it is that those new pictures are things that the Mars Rover also found in the same cave.",
            "So some of these correspond to things that they've seen in training before, so these are exactly the things that we repeated many times in the correlated case.",
            "They also form a subset of the things that were seen in the factorial case, but we also present them the four factorial combinations of those six features which they never saw before.",
            "So remember I said, there are 24 possible configurations if you take 6 features and you take combinations of three of those, there are 24 possibilities.",
            "So we show people 20 here we hold out four and we show them those four.",
            "And then we also have a control where we just show people shuffled parts.",
            "But what's interesting about this is that we can make a prediction if people have actually infer the appropriate parts from looking at these objects and thus have the feature representation that I showed you is produced by this model.",
            "Then they should generalize to these stimuli as well.",
            "They should say that these are actually fairly likely to be things that the Mars Rover is."
        ],
        [
            "Trying to find and.",
            "So if we look at what they do here, these are the judgments of people make if we show them these factorial cards, then they think that both the things that they saw and the things that they didn't see but are made up of the same parts are likely to be seen in the future.",
            "Whereas if we show them those correlated cards, they're very sure that the things they saw or likely to be seen again.",
            "But there there's a statistically significantly lower generalization to those things that are made up of the same paths, and this is actually what's predicted by our model here.",
            "So this is taking that noisy or.",
            "Infinite matrix factorization model and looking at the predictions it makes and it predicts exactly this pattern of behavior.",
            "So people are basically pulling out those features in a way which depends on the distribution of the."
        ],
        [
            "Objects that they see.",
            "So you can use this sort of strategy of taking distribution over a matrix with a fixed number of rows, an unbounded number of columns.",
            "Basically, in any case where you're interested in defining a sparse latent feature model where you don't know the number of dimensions.",
            "So there are versions of PCA, ICA, collaborative filtering models, and so on that use the ICP as a prior.",
            "You can use it as a prior on the adjacency matrix for a bipartite graph where you have a class of nodes that has unknown sides.",
            "So something like inferring hidden causes where you know that there's some number of observed variables.",
            "You don't know how many UN observed variables influences of those.",
            "As an example of this, there's an interesting link to beta processes.",
            "Remember I said something sort of arcane about the connection between the Chinese restaurant process and there is a process the same arcane connection exists between the beta process and the Indian buffet process, and like the during the process, there are many ways in which you can extend this."
        ],
        [
            "So the basic argument here is that these nonparametric Bayesian models give us a way of answering questions about how much structure to infer, and those questions are kind of ubiquitous when we try and think about the inferences that people make for questions, like how many clusters we can use.",
            "Something like the Chinese restaurant process or the Irish process.",
            "Questions like how many features we can use, something like the Indian buffet process or the beta process, and those things can be extended and combined together in ways that allow us to build up rich representations that are the kinds of things that we want might want people to learn in different domains.",
            "And this kind of you know, ability to combine these models together and also to think about them as the consequence of thinking about sort of particular sorts of combinatorial stochastic processes means that there's a lot of room for people to develop new models that you know in the same way have desirable statistical properties, but allow you to make inferences about domains where you have potentially unbounded."
        ],
        [
            "Instruction so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so our plan is we're going to run a little long in this session and then we'll run short and Josh section next.",
                    "label": 0
                },
                {
                    "sent": "It will go to the break a little later, but you'll get the break the next session and then we'll have the debate as usual, but I'm going to talk about is going to be semantically related to what Josh will talk about in the next session as well.",
                    "label": 0
                },
                {
                    "sent": "We're going to be talking about two different approaches to answering this question of kind of how much structure you can infer from data you observe.",
                    "label": 0
                },
                {
                    "sent": "This kind of fundamental statistical question, a question that has a lot to do with soda.",
                    "label": 0
                },
                {
                    "sent": "How it is that people are so good at making inductive inferences and about the kinds of representations that they form when they're trying to understand the kinds of processes that going on in the world around them?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any on how quickly this goes?",
                    "label": 0
                },
                {
                    "sent": "I could give you a little more detail about the study that Nick very quickly went over about discriminative generative models, but we'll see how we go.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is return to this kind of list of parallels that we have between human learning and machine learning problems.",
                    "label": 1
                },
                {
                    "sent": "On the one side, being the things that we want to understand about human cognition, and on the other side being the things that we kind of know how to do in machine learning, and I'm going to focus on one of these today, and this is going to be nonparametric Bayesian methods as a way of understanding how it is that we can form certain kinds of representations.",
                    "label": 0
                },
                {
                    "sent": "So the basic question that this engages is a question that comes up a lot when you're talking about a cognitive.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a question of how much structure we should infer from the world around us.",
                    "label": 0
                },
                {
                    "sent": "So you can think about lots of learning problems encountering this kind of issue.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to, say, learn categories of objects, well, how many of those categories do you need to learn in order to represent the kind of structure that exists in the world around you?",
                    "label": 0
                },
                {
                    "sent": "If you're just sort of forming clusters of things that you see, how many clusters should you use in order to represent the amount of structure that's going on in that world?",
                    "label": 0
                },
                {
                    "sent": "If you're trying to figure out the properties that objects have, how many features does an object have?",
                    "label": 1
                },
                {
                    "sent": "If I have, you know.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "This is my object.",
                    "label": 0
                },
                {
                    "sent": "Well, you know you could think of many ways to describe this and what's the right way of sort of coming up with a description of the physical properties that this object has is a visual object?",
                    "label": 0
                },
                {
                    "sent": "Or say if you're learning a language, one sort of problem, you might have to resolve as an infant is something like figuring out how many separate words there are in a language.",
                    "label": 0
                },
                {
                    "sent": "One of the things that we have to do is in very early part of learning languages to figure out, say where the gaps are between words in a sentence because you get, you know, basically a stream of sound, and if you've ever heard an unfamiliar foreign language being spoken, you're not even sure where the words might be.",
                    "label": 0
                },
                {
                    "sent": "In that stream of sound, so you have to figure out not just what the gaps are, but what even are the words that people might be using in that language before you can go on and learn things like how it is that those words map onto things in the world, or how it is that those words are combined together to make up sentences.",
                    "label": 0
                },
                {
                    "sent": "And likewise there's a similar kind of problem if you're trying to figure out what the grammatical principles are, you still have this sort of kind of difficulty figuring out well how many of these things are there, and what's different, and in what cases can I use this one and this one?",
                    "label": 0
                },
                {
                    "sent": "They all have this kind of parallel structure of being uncertainty about just.",
                    "label": 0
                },
                {
                    "sent": "How much stuff you should be inferring from the data that you see?",
                    "label": 0
                },
                {
                    "sent": "Another sort of way of putting this as a learning problem, framing it perhaps in the language of Bayesian inferences about the hypothesis spaces that people are using when they're solving these problems of forming categories or learning language where the issue is that what we really want to be able to do is to define models which allow us to express rich unbounded hypothesis spaces so rich in the sense that they capture kinds of structures that are in the world, and that you know, give us a way of sort of talking about the source of knowledge that people have an unbounded in the sense that we don't want to make a model which says.",
                    "label": 0
                },
                {
                    "sent": "So there are three kinds of categories, or there are four kinds of rules, or there are five kinds of words we want to make models which can figure out how many categories, rules or words they want to use in order to describe the world.",
                    "label": 0
                },
                {
                    "sent": "So it's very unsatisfying if I say, here's my model of human cognition.",
                    "label": 0
                },
                {
                    "sent": "It only works if there are less than 20 things, right?",
                    "label": 0
                },
                {
                    "sent": "Like that.",
                    "label": 0
                },
                {
                    "sent": "That's not a very good model of human cognition.",
                    "label": 0
                },
                {
                    "sent": "We want to make something which can figure out how much structure there is so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nonparametric Bayesian statistics actually provides us with a set of tools that we can use for solving these kinds of problems and the way that this works is to say, if you assume that the world around us contains infinite complexity, then we can assume that through any finite experience we might only observe part of that.",
                    "label": 1
                },
                {
                    "sent": "So rather than sort of saying, oh, there's some finite amount of structure in the world and our job is to go out and discover how much there is, so it's not saying, oh, there are just six kinds of categories and seven kinds of features, and you know 8 words in this language, it's saying.",
                    "label": 0
                },
                {
                    "sent": "In fact, we're going to commit to the idea that there's potentially infinitely many categories and infinitely many features of an object, and infinitely many words in a language.",
                    "label": 0
                },
                {
                    "sent": "It's just that we don't know how many of those we've encountered so far, and this sets up the problem in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "It's not kind of figuring out how much structure there is in the world, but how much structure were justified in believing exists based on our observations under the expectation that there's kind of an infinite amount of stuff out there.",
                    "label": 0
                },
                {
                    "sent": "So when I parametric Bayesian statistics the word nonparametric means a lot of different things.",
                    "label": 0
                },
                {
                    "sent": "When people use it in statistics.",
                    "label": 0
                },
                {
                    "sent": "One of the things that means is basically defining probability distributions that don't assume a particular parametric form, but another thing, it means something like models that become more complex as you get more data.",
                    "label": 0
                },
                {
                    "sent": "That increasing complexity with the data that you observe.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of the sense that we have here.",
                    "label": 0
                },
                {
                    "sent": "This approach allows us to define these models which are kind of arbitrarily complex and then can capture whatever structure actually is present in the data that we see, and the basic strategy is then you know, having to find these infinite hypothesis spaces, we can use the casting process is to define probabilistic distributions over those that we can use as prior distributions in Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "So you already heard a little bit about Gaussian processes from Neal.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian processes are an example of a nonparametric Bayesian approach which you can use for making inferences about functions.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about two other nonparametric Bayesian methods which are based on combinatorial stochastic processes, so they help us to get this issue of being able to define richly structured hypothesis spaces that still have the same property of being unbounded.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about models based on the Dirichlet process or the Chinese restaurant process which.",
                    "label": 0
                },
                {
                    "sent": "I'll explain their relationship in a moment or the beta process in the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "So these sound like you know, funny names for things they actually are funny names for things, but what they allow us to do is to define probability distributions over exactly the kinds of objects that we need to make.",
                    "label": 0
                },
                {
                    "sent": "The sorts of inferences that I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "So they allow us to define distributions that we can use to infer how many clusters there might be, or how many words or how many rules or distributions over combinatorial objects that allow us to make inferences about how many features might be expressed in some data.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do here is give a relatively nontechnical introduction to these ideas.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more technical material behind these things.",
                    "label": 0
                },
                {
                    "sent": "If you want to hear more about the technical stuff, you can ask me about it later on.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can do a mini tutorial on the beach and we can go into further details, but what I'm going to do is introduce these things and introduce them by thinking about some of the cognitive problems that they actually are useful for examining, and So what I'm going to do in each of these cases is pick one cognitive problem.",
                    "label": 0
                },
                {
                    "sent": "And then kind of go through it and show how an idea from nonparametric Bayesian statistics helps us to answer some of the questions that comes up.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first of these problems is the problem of categorization.",
                    "label": 0
                },
                {
                    "sent": "Says something that you heard lots about already.",
                    "label": 0
                },
                {
                    "sent": "So mostly you've heard about this from a discriminative perspective, so I'll talk about it from a generative perspective, which might be useful as we go forward.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is that you might be provided with examples of two different categories.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of a dog and a cat, and then you see a novel object and are asked to categorize that object in terms of which of those categories it belongs to.",
                    "label": 0
                },
                {
                    "sent": "So what do you guys think?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "You guys have got these categories down.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question that psychologists want to ask is how it is that people actually go about representing these kinds of categories.",
                    "label": 0
                },
                {
                    "sent": "And you know psychologists and proposed a bunch of theories which are really about the underlying psychological processes that go into forming these sorts of representations.",
                    "label": 0
                },
                {
                    "sent": "So in the history of the sort of study of categorization, there have been lots of proposals that people have gone through to focus on a couple which have been sort of a big part of that history.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of these views is called the prototype view, where the idea is that you observe many different cats and then what you do is you kind of form some abstractions, some kind of idealized cat.",
                    "label": 0
                },
                {
                    "sent": "That's your prototypical cat, and the way that you decide whether something belongs to the category is by comparing it to that prototypical example that you have stored in memory, and if you've got something where you want to start, if it's a cat or a dog, you compare it to your typical cat and your product typical dog and assign it to the category which is closest.",
                    "label": 0
                },
                {
                    "sent": "Another approach that you could use for learning a category is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using an exemplar model, so the idea behind Exemplar models is that what you do is you actually just store every instance of a cat that you've ever seen in memory, and then when you want to decide if something out that knew is a cat, you compare it to all of those instances.",
                    "label": 0
                },
                {
                    "sent": "So the way that you would decide if something is a cat or a dog, as you see it, you kind of call up all of the cats and call up all of the dogs that you've seen.",
                    "label": 0
                },
                {
                    "sent": "You compare its similarity to all of those examples and add them up to the two different categories, and then you assign it to the category where the sum of the similarity over all of the examples is greatest.",
                    "label": 0
                },
                {
                    "sent": "So these are two kind of radically different ways of thinking about how you represent a category.",
                    "label": 0
                },
                {
                    "sent": "In one case, you're kind of abstracting this thing away, which is not something that you've ever seen is just your kind of idealization of that.",
                    "label": 0
                },
                {
                    "sent": "And in the other case, you're making no abstraction at all.",
                    "label": 0
                },
                {
                    "sent": "You just remembering all of the things that you saw, and then using those every time you want to make an inference.",
                    "label": 0
                },
                {
                    "sent": "Obviously there are strategies that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lie somewhere in between these and people have explored these as well.",
                    "label": 0
                },
                {
                    "sent": "So for example, you might decide that there are certain kinds of cats and form sub prototypes for each of those different kinds.",
                    "label": 0
                },
                {
                    "sent": "So you might have you know fluffy cats and non fluffy cats.",
                    "label": 0
                },
                {
                    "sent": "And you know maybe funny looking cats as your different sort of sub categories of cats and then you sum up the similarity to each of those different prototypes and as a consequence of doing that you would then use that rather than all of the exemplars or a single prototype for representing the category.",
                    "label": 0
                },
                {
                    "sent": "So these are psychological theories, their theories about.",
                    "label": 0
                },
                {
                    "sent": "You know what are those that are cognitive processes that people go through when they're thinking about, you?",
                    "label": 0
                },
                {
                    "sent": "Know, categorizing an object and we can now, maybe, you know, in the spirit of what we're doing here looking at thinking bout connections to machine learning and so on.",
                    "label": 0
                },
                {
                    "sent": "Take a step backwards and think about what the ad and allying computational problem is that people might be solving in trying to solve this problem of category.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Session well, one way that we can formulate this computational problem is to think about it as an inductive problem where we have some data.",
                    "label": 1
                },
                {
                    "sent": "This stimulus, which is the features of the object that we observe and we have hypothesis about what categories those things might correspond to an as an inductive problem is something that we can solve by using Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "We can basically you know if we know what the prior probability of each category is and we know a distribution over stimuli which are associated with each category, then we can use Bayes rule to work backwards and calculate.",
                    "label": 0
                },
                {
                    "sent": "The probability that that.",
                    "label": 0
                },
                {
                    "sent": "That stimulus belongs to that category.",
                    "label": 0
                },
                {
                    "sent": "So it's worth saying this is the generative version of the formulation of the computational problem.",
                    "label": 0
                },
                {
                    "sent": "It's the one that says that associated with each category you have an underlying probability distribution, and then you have some expectations about the probabilities of those categories and it's those things that you combine together in order to make this inference, and you know, as you've seen, there's also a discriminative way of formulating this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of build off this generative version, but we can talk some more about the connections to discriminative methods in the discussion so.",
                    "label": 0
                },
                {
                    "sent": "Assuming this generative view of the computational problem, we've now sort of said OK.",
                    "label": 0
                },
                {
                    "sent": "If you know these things, the probability distribution over stimuli given the category in the prior probability of the categories, then you're set.",
                    "label": 1
                },
                {
                    "sent": "All you have to do is use Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Well that gives us a new challenge.",
                    "label": 0
                },
                {
                    "sent": "We have to figure out what these probabilities are and the hard one is pretty easy to workout.",
                    "label": 0
                },
                {
                    "sent": "The prior probability of each category.",
                    "label": 0
                },
                {
                    "sent": "You can just kind of add up how often you've seen cats and dogs and so on, and then end up with your distribution.",
                    "label": 0
                },
                {
                    "sent": "The hard part is figuring out what the distribution over stimuli for each of those categories is, so this is.",
                    "label": 0
                },
                {
                    "sent": "You want some way that you're going to figure out what the distribution over.",
                    "label": 0
                },
                {
                    "sent": "You know all kinds of animals might be if the way that you're going to represent your knowledge of what cats are and what dogs are is, say some kind of probability distribution, you need to sort of fix that distribution over animals for each of those categories so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Statistics this is the problem of density estimation, so we need to estimate this probability distributions.",
                    "label": 1
                },
                {
                    "sent": "Statisticians tell us there are two ways to do this.",
                    "label": 0
                },
                {
                    "sent": "We can use parametric density estimation or nonparametric density estimation.",
                    "label": 0
                },
                {
                    "sent": "Parametric density estimation.",
                    "label": 0
                },
                {
                    "sent": "You choose a particular parametric family and then you estimate the parameters of that parameter family as a way of calculating what this probability distribution is.",
                    "label": 0
                },
                {
                    "sent": "In nonparametric density estimation, you want to take a strategy which is going to be one which allows you to identify probability distributions that lie outside a particular parametric family.",
                    "label": 0
                },
                {
                    "sent": "So the weakness of parametric density estimation is having made a commitment to a particular distributional family.",
                    "label": 0
                },
                {
                    "sent": "Then it might be the distribution you want to learn lies outside that family, and you're not going to do a good job of learning it.",
                    "label": 0
                },
                {
                    "sent": "So nonparametric methods are designed to give you a way of learning arbitrary distributions, but it typically takes more examples, more data for you, a bit to be able to get a good estimate of what those distributions are.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting is that some mathematical psychologists have shown that these two approaches to density estimation, calculating what that probability distribution is.",
                    "label": 1
                },
                {
                    "sent": "Actually correspond formally to prototype and exemplar models, so those two strategies that I talked about a psychological models actually translate into two strategies for estimating a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "When you're taking this view that what you should be doing is basically applying Bayes rule and just to kind of give you an intuition, I'm going to go through these two examples and you can hopefully see the connection.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting to note that there are discriminative analogs, so if you took the discriminative view of the formulation of this computational problem.",
                    "label": 0
                },
                {
                    "sent": "There are also discriminative analogs of both prototype and exemplar models, so this is an interesting case where both of these perspectives actually give you a way of explaining these existing psychological models, but from a generative.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active the connection between the prototype model and parametric density estimation.",
                    "label": 1
                },
                {
                    "sent": "Well, if we assume that this probability distribution has a simple form characterized by some parameters, Theta.",
                    "label": 1
                },
                {
                    "sent": "Well, say we're going to assume that probability distribution is something which is, say, Gaussian distribution, and these parameters Theta would correspond to say the meaning of that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "We have to make some assumptions about the variance, but in the simplest case, we could just assume that we're going to have an equal variance for all of the categories that we're estimating.",
                    "label": 0
                },
                {
                    "sent": "Then, if that Theta describes the mean of the Gaussian, that means that as we get further away from the mean, the probability of stimulus under that category is going to decrease.",
                    "label": 0
                },
                {
                    "sent": "And if you imagine having two of these distributions and then doing Bayes rule to calculate which distribution you should assign an observation to what's going to matter in that assignment is just your distance from the mean of the category.",
                    "label": 0
                },
                {
                    "sent": "So the mean of the category plays the same role as the prototype, right?",
                    "label": 0
                },
                {
                    "sent": "I said the prototype model you think about your sort of Canonical ideal members, and then you compare the distance to those ideal members.",
                    "label": 0
                },
                {
                    "sent": "Well, if you have a parametric model where probability decreases the distance from some central tendency, Gaussian is 1 example.",
                    "label": 0
                },
                {
                    "sent": "There are lots of other distributions like that, then distance is going to be the only thing that matters to categorization, and this makes the same kind of predictions about how it is that you should inform these categories.",
                    "label": 0
                },
                {
                    "sent": "So this parametric method actually corresponds quite nicely to the prototype model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For nonparametric density estimation, standard frequentist method for solving this problem is to use kernel density estimation.",
                    "label": 1
                },
                {
                    "sent": "You've heard already about these kinds of methods.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that what we would do if there's some true distribution here in red that we want to estimate the way we would estimate it, is take each of the observations that we've sampled from that distribution and put down a little kernel.",
                    "label": 0
                },
                {
                    "sent": "A little probability mass on each of those observations, and then sum those up and the sum over all of those kernels is going to be our approximation to the target distribution.",
                    "label": 0
                },
                {
                    "sent": "And as the number of observations we have increases, the quality of that approximation is going to improve, and we can do some clever things about choosing the variance of the kernel appropriately.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 1
                },
                {
                    "sent": "And those are things that Bernard was telling you about, but the basic idea is you put a little bit of probability over each of your observations, and then some of those up, and then that's going to give you a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, the correspondence between this and the exemplar model comes from.",
                    "label": 0
                },
                {
                    "sent": "Remember what I said in the example model, the way that you assign something to a category is, you think about all of the examples of that category you've seen in the past and then.",
                    "label": 0
                },
                {
                    "sent": "You add up how similar your observation is to each of those.",
                    "label": 0
                },
                {
                    "sent": "Well, you can think about that as saying for each of your observations, each of your examples you're going to have a similarity function which is over that exemplar, and then you're going to some the similarity functions, and that's going to be your measure of how well that thing fits with the category.",
                    "label": 0
                },
                {
                    "sent": "And that's basically exactly what we're doing in kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "The kernel is like our similarity function.",
                    "label": 0
                },
                {
                    "sent": "We're going to Add all of those up, and that's going to act like you know that's going to give us the probability under the category, which is our measure of how well that think that's with the category exactly like in an exemplar model.",
                    "label": 0
                },
                {
                    "sent": "So you get this nice formal equivalence everybody clear so far.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so one thing that's interesting about this is that there's also a nice sort of probabilistic analogs.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those strategies that lie somewhere in between.",
                    "label": 1
                },
                {
                    "sent": "So if you wanted to take something where you didn't have a single prototype and you didn't have a distribution for each example, you could take something where you have a, say a mixture distribution where you have more than one mixture component per data point.",
                    "label": 1
                },
                {
                    "sent": "So the kernel density estimation example is a mixture distribution.",
                    "label": 0
                },
                {
                    "sent": "Mixture distribution just means that the way I represent my probability distribution is a mixture of some components, where each of those components as a distribution and is assigned some weight, and those weights them up to one.",
                    "label": 0
                },
                {
                    "sent": "In the kernel case you have lots and lots and lots of components, one for every observation that you've got.",
                    "label": 0
                },
                {
                    "sent": "In this case we could say.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe I'll just use, you know, four or five components, and for each of those components I'll have the analog of a prototype and I'll sum up the probability associated with each of those components.",
                    "label": 0
                },
                {
                    "sent": "And that's going to give me my estimate of this target distribution.",
                    "label": 0
                },
                {
                    "sent": "So this idea of using a mixture model is interesting.",
                    "label": 0
                },
                {
                    "sent": "It's nice, it gives us a way of having something in between exemplar and prototype models, but it also gives us a new problem.",
                    "label": 0
                },
                {
                    "sent": "So if you're going to decide to represent your category in terms of.",
                    "label": 0
                },
                {
                    "sent": "Some of these subcomponents you can think about them as kind of clusters of things that you could sort of categorized together that are well represented by a single prototype, and so on.",
                    "label": 0
                },
                {
                    "sent": "The problem that we run into is the problem of determining how many of those clusters we should use, how many components we should have in our mixture model, how many things we should be clustering together and representing with a single prototype, and so that's exactly this kind of problem that I was talking about.",
                    "label": 0
                },
                {
                    "sent": "It's A kind of how much structure problem, right?",
                    "label": 0
                },
                {
                    "sent": "It's a problem where we need to workout based on the data that we see, how much we should believe.",
                    "label": 0
                },
                {
                    "sent": "How much structure we should believe is expressed in that data?",
                    "label": 0
                },
                {
                    "sent": "How many clusters of things do we need to explain what's going on in our data set?",
                    "label": 0
                },
                {
                    "sent": "And so this is a problem that now we need to sort of.",
                    "label": 0
                },
                {
                    "sent": "Think about how it is that we can define a model that allows us to make inferences about the number of clusters that we need as well.",
                    "label": 0
                },
                {
                    "sent": "As you know, being able to sort of tell us then based on those clusters what the probability is of our observed stimuli under the category.",
                    "label": 0
                },
                {
                    "sent": "So there's a nice computational model that does this that was proposed by psychology.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call John Anderson, who's actually made significant contributions to this kind of approach of making rational models of cognition, so he sort of thought about this problem, and he said, look, I'm going to come up with a way of defining a model that doesn't require me to commit to the number of clusters that I'm going to use so Anderson's rational model, he says.",
                    "label": 0
                },
                {
                    "sent": "First of all, let's treat category labels just like any other feature of an object.",
                    "label": 1
                },
                {
                    "sent": "So rather than saying I'm going to estimate one probability distribution for each category, Now I'm going to say I'm just going to estimate one big probability distribution, and I'm going to add on the category label as an extra feature.",
                    "label": 0
                },
                {
                    "sent": "Which appears in that probability distribution and now if I do that then I'm trying to learn a joint distribution on these features including the category label and I'll use a mixture model for that.",
                    "label": 1
                },
                {
                    "sent": "So I'm breaking my objects up into clusters just like I do in a mixture model.",
                    "label": 0
                },
                {
                    "sent": "But he introduces a sort of clever trick which is to allow the number of clusters that are going to appear in that mixture model to vary, and the basic idea is that each time you see a new object, you compute a prior probability distribution that that object is assigned to a particular cluster.",
                    "label": 0
                },
                {
                    "sent": "And in that probability distribution, it's possible for an object to get assigned to an old cluster.",
                    "label": 0
                },
                {
                    "sent": "One of the ones that you've already assigned objects too.",
                    "label": 0
                },
                {
                    "sent": "But it's also possible to get it assigned to a new cluster.",
                    "label": 0
                },
                {
                    "sent": "So the way that he does this is he says will take the probability of choosing a particular cluster to be proportional to the number of things that have been assigned to that cluster in the past.",
                    "label": 0
                },
                {
                    "sent": "Assuming it's an old cluster and then with some parameter Alpha, will that will determine the probability that I choose a new cluster.",
                    "label": 0
                },
                {
                    "sent": "And So what we get out of this is, you know, say we've already observed.",
                    "label": 0
                },
                {
                    "sent": "A bunch of instances of category A, so you could think about these.",
                    "label": 0
                },
                {
                    "sent": "As you know, we've observed a bunch of cats.",
                    "label": 0
                },
                {
                    "sent": "So say we've seen 9 cats already and we're trying to make a decision as to, I guess these are these are this is not for a single category.",
                    "label": 0
                },
                {
                    "sent": "These are examples which are for these things, which could be both cats or dogs, so they shouldn't say Category 8 here.",
                    "label": 1
                },
                {
                    "sent": "So 'cause he's trying to define a joint distribution features.",
                    "label": 0
                },
                {
                    "sent": "So I've seen things which are cats or dogs, and we assign those things to clusters and you know, we're pretty sure that we've seen you know four things that cluster together here, and two things that cluster together here, and three things that.",
                    "label": 0
                },
                {
                    "sent": "Together, here and now, the question is, what am I going to do when I see my 10th observation and the idea is I'm going to say well before I even see it.",
                    "label": 0
                },
                {
                    "sent": "I think there's some chance is one of these things I've seen before, and I think it's more likely to be something that I've seen more often in terms of the different clusters, and there's some chance that it's something completely new, right?",
                    "label": 0
                },
                {
                    "sent": "It seems kind of reasonable, So what you might notice is that this is actually a way to define a probability distribution over essentially an infinite number of clusters, right?",
                    "label": 0
                },
                {
                    "sent": "As we observe more and more data, it becomes possible for us to observe something new every single time.",
                    "label": 0
                },
                {
                    "sent": "And while the probability of observing something new is going to decrease because of the way that you know Alpha, here is kind of like the number of counts which we're going to compare against all of the accounts that we have are things that have happened in the past.",
                    "label": 0
                },
                {
                    "sent": "So as we get more observations and there's more mass builds up in these things we've seen before, that probability of drawing something new is going to decrease it.",
                    "label": 0
                },
                {
                    "sent": "Still, in principle possible at every point to observe something that's completely new, you can kind of think about this like if you were a Explorer and went to a new country.",
                    "label": 0
                },
                {
                    "sent": "So Australia, right?",
                    "label": 0
                },
                {
                    "sent": "You'd seen some.",
                    "label": 0
                },
                {
                    "sent": "A number of different animals before and so you've identified some species or things.",
                    "label": 0
                },
                {
                    "sent": "These are the species that you've seen before and you get to Australia and you're like, wow, what's that, right?",
                    "label": 0
                },
                {
                    "sent": "Like you suddenly see something like a kangaroo that doesn't fit into any of the categories that you've seen, but having this kind of model which allows you to then form a new cluster, you're able to go.",
                    "label": 0
                },
                {
                    "sent": "That's one of those as well, and that's another one of those, and that's another one of those.",
                    "label": 0
                },
                {
                    "sent": "Wait a minute, what's that right?",
                    "label": 0
                },
                {
                    "sent": "And you see a koala and so on, but at every point it's possible for you to be encountering something that you've never seen before, so this kind of model gives us the capacity to represent an infinite number of clusters, and to always be expecting that it's possible to see something we haven't seen before.",
                    "label": 1
                },
                {
                    "sent": "So this is a very elegant way of defining this sort of distribution.",
                    "label": 0
                },
                {
                    "sent": "It has some nice properties which I'll go into in a moment.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting about this is that this model, which Anderson came up with is actually a model which has been studied in statistics, something that's called a Dirichlet process mixture model.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the jewel of nonparametric Bayesian statistics.",
                    "label": 0
                },
                {
                    "sent": "It's a model which allows you to define a distribution which is nonparametric in the sense that it allows us to model.",
                    "label": 0
                },
                {
                    "sent": "Obituary, probability densities, and it's something which grows in complexity as we get more data in a way which just uses Bayesian inference, and the idea is that we have sort of being very clever about the way we're defining our prior distribution over these clusters, allowing the complexity of our model to increase with the complexity of our data.",
                    "label": 0
                },
                {
                    "sent": "So this way of defining our prior distribution is something that, in the study of combinatorial stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is called the Chinese restaurant process, so it has this funny name because the statisticians who came up with it were actually statisticians at Berkeley and spend a lot of time going to Chinese restaurants in San Francisco and they sort of thought that this process might describe.",
                    "label": 1
                },
                {
                    "sent": "Basically, you need to make an assumption that you have an infinitely large restaurant with an infinite number of tables, each of which can seat an infinite number of people, and they thought that was best described by these Chinese restaurants.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is that we have an customers who walk into a restaurant and they're going to choose tables to sit down on with probability, which is proportional to the number of people who are currently seated at that table, and then with some probability, start the next unoccupied table.",
                    "label": 1
                },
                {
                    "sent": "What's important about this process from the perspective of actually using these models and thinking about the assumptions that they make, is that out of this we get something which is called an exchangeable probability distribution over seating assignments.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that it doesn't matter what order the customers entered the restaurant.",
                    "label": 0
                },
                {
                    "sent": "If you've got, you can think about this kind of partitioning the customers up so that they're all sitting at different tables.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what order the customers enter the restaurant, the probability of getting that partition of customers will remain the same, and it's just a consequence of the way that this probability distribution is defined, and so that's kind of it's useful from the perspective of thinking about defining our models often, we don't want to assert some kind of ordering on data that we see.",
                    "label": 0
                },
                {
                    "sent": "We might not even know what order the data we see were generated in, but it also makes it easy to do probabilistic inference in these models, and that's a technical detail that I'm going to skip over.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is that if you ever need to compute a conditional probability of a.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability for observation belonging to a cluster.",
                    "label": 0
                },
                {
                    "sent": "Then you can do that very easily using this exchangeability assumption.",
                    "label": 0
                },
                {
                    "sent": "'cause it means you can always pretend that the customer you're interested in was the last customer who entered the restaurant, and that's useful for doing Gibbs sampling, which I'll talk about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Think about this.",
                    "label": 0
                },
                {
                    "sent": "As you know, you take one of the customers away.",
                    "label": 0
                },
                {
                    "sent": "He goes to the bathroom.",
                    "label": 0
                },
                {
                    "sent": "He comes back and because of exchangeability, if he follows the same process for grabbing a seat, then you know that's the same as if the customers that will come in and that order.",
                    "label": 0
                },
                {
                    "sent": "Initially the probability of getting that configuration is appropriate.",
                    "label": 0
                },
                {
                    "sent": "So this exchangeability assumption is something which is desirable from a perspective of thinking about making sort of sensible, understandable probabilistic models, but also from the perspective of being able to do probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That you want the distribution over potentially for number of clusters, which is extendable.",
                    "label": 0
                },
                {
                    "sent": "We do somehow automatically arrive at this.",
                    "label": 0
                },
                {
                    "sent": "So the question is if you desired exchangeability, would you automatically derive this as the consequence, right?",
                    "label": 0
                },
                {
                    "sent": "So at what's interesting is Anderson's choice of this distribution was based on those kinds of considerations, so he wrote down a bunch of axioms that he wanted this distribution to satisfy an, then derive this distribution as the result.",
                    "label": 0
                },
                {
                    "sent": "But it's stronger than just exchange ability.",
                    "label": 0
                },
                {
                    "sent": "There are many other exchangeable distributions on partitions, and this is just one example.",
                    "label": 0
                },
                {
                    "sent": "So you can go to his book and you know he has an appendix.",
                    "label": 0
                },
                {
                    "sent": "He goes through what those axioms are.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, alright so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So this translates into what's called the Dirichlet process mixture model.",
                    "label": 1
                },
                {
                    "sent": "Dirichlet process is a more complex stochastic process, which is basically a way of defining a distribution over infinite multinomial's.",
                    "label": 0
                },
                {
                    "sent": "So multinomial distribution is what you have when you're taking a single choice from a discrete set of options.",
                    "label": 0
                },
                {
                    "sent": "When you have a finite set, you can imagine having an infinitely large set of discrete options.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what we have when we want to have an infinite mixture model, and then you need to define a way of generating a probability distribution over that infinite set of options.",
                    "label": 0
                },
                {
                    "sent": "Well, the Dirichlet process is a way of producing those kinds of distributions, but I think what you for the present purposes I'm not going to talk much about the during the process.",
                    "label": 0
                },
                {
                    "sent": "the Chinese restaurant process is what you get when you integrate out that multinomial distribution in the same way that you might integrate out of garishly distribution.",
                    "label": 0
                },
                {
                    "sent": "Multinomial distribution with respect to a garishly prior.",
                    "label": 0
                },
                {
                    "sent": "So if that made sense to you and you want to learn more, come and talk to me.",
                    "label": 0
                },
                {
                    "sent": "If that didn't make sense at all, don't worry 'cause we're going to go on, and you're not going to know it.",
                    "label": 0
                },
                {
                    "sent": "So the way that we then define mixture model are Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "Mixture model is that we kind of imagine our Chinese restaurant having some parameters on each of the tables.",
                    "label": 0
                },
                {
                    "sent": "So these parameters are going to correspond to the parameters that are associated with those mixture components telling us where that component is located and what its variant size and so on.",
                    "label": 0
                },
                {
                    "sent": "The kinds of things that you want to know about a component of a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So there's some process from which we've sampled these parameters.",
                    "label": 0
                },
                {
                    "sent": "We put down some parameters on every table in our restaurant, and then the customers come into the restaurant.",
                    "label": 0
                },
                {
                    "sent": "And our customers are going to be our data points.",
                    "label": 0
                },
                {
                    "sent": "They're going to sit down at the table according to the CI P. And then we're going to generate our observations by each customer, sort of spitting out an observation which follows the distribution characterized by the parameters that are sitting on that table.",
                    "label": 0
                },
                {
                    "sent": "And this is our whoops generative process, and we can invert this generative process to figure out how many clusters might be expressed in some data that we observe, and we can do that using some of the methods I'll talk about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that using a Dirichlet process mixture model.",
                    "label": 0
                },
                {
                    "sent": "You can define a mixture model which allows you to have an unbounded number of components, and this in some sense gives you a way of interpolating between something like a prototype model where you've got just one single sort of summary representation for a category, and something like an exemplar model where you have to remember all of the examples every time you want to make an inference.",
                    "label": 0
                },
                {
                    "sent": "But it also gives us the basis for coming up with a richer description of what might be going on in cat.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a shun, so this idea of density estimation that I've been talking about as a unifying kind of framework for thinking about categorization models.",
                    "label": 0
                },
                {
                    "sent": "It's a way that we can think about categorization models characterizing the different options that might be.",
                    "label": 0
                },
                {
                    "sent": "You know what's going on inside peoples heads in terms of how they representing categories.",
                    "label": 0
                },
                {
                    "sent": "But we could go beyond this to define something which we call a unifying model.",
                    "label": 0
                },
                {
                    "sent": "SO1 model, of which all of these other models are special cases.",
                    "label": 0
                },
                {
                    "sent": "And in that model, if we can characterize all of these sort of previous proposals, the special cases of that model.",
                    "label": 0
                },
                {
                    "sent": "Then it gives us the opportunity to think about what people are doing when they're learning categories, is perhaps adaptively choosing between these different kinds of strategies that they could be using.",
                    "label": 0
                },
                {
                    "sent": "So if we really think about what the best way to solve the problem of categorization might be, it's not going to be saying I definitely want to use a prototype model.",
                    "label": 0
                },
                {
                    "sent": "I definitely want to use an example model for every single category that I encounter, right?",
                    "label": 0
                },
                {
                    "sent": "I said prototypes work well when you are trying to estimate a distribution in that distribution matches the family that you're estimating the distribution from.",
                    "label": 0
                },
                {
                    "sent": "That's when parametric density estimation works.",
                    "label": 0
                },
                {
                    "sent": "So we've got a small amount of data.",
                    "label": 0
                },
                {
                    "sent": "And you think your assumptions about the distribution of the shape of the distribution are satisfied?",
                    "label": 0
                },
                {
                    "sent": "That's a good way to go.",
                    "label": 0
                },
                {
                    "sent": "Example, our models are flexible.",
                    "label": 0
                },
                {
                    "sent": "They can represent any kind of probability distribution using this kind of kernel, like method, but they require lots of data in order to converge to a good representation of that distribution.",
                    "label": 0
                },
                {
                    "sent": "So it seems like in general, what you want to be able to do is kind of adaptively select which of these sorts of strategies you're going to use based on the structure that's reflected in the data, and so if we can define a model which has all of these sorts of models are special cases, then we can actually think about.",
                    "label": 0
                },
                {
                    "sent": "What might be going on in human categorisation is just adaptively selecting between these different strategies for representing categories, and we can actually do this using the Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "We have to get a little more complicated.",
                    "label": 0
                },
                {
                    "sent": "We can think about it in terms of two interacting levels of clusters, and this is something which we get from the hierarchical Dirichlet process, so it's not that much more complicated if.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I kind of understood the Chinese restaurant process.",
                    "label": 0
                },
                {
                    "sent": "It's the same kind of idea.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do is we're going to take a step back.",
                    "label": 0
                },
                {
                    "sent": "Remember, Anderson said we're going to think about one big probability distribution, which is going to represent both of those categories, and we're going to think about labels is just being a special kind of features.",
                    "label": 0
                },
                {
                    "sent": "Well, one problem with that is that it means that you'd be using exactly the same components and clusters for representing all of the categories that you learn, and that's probably too strong assumption.",
                    "label": 0
                },
                {
                    "sent": "You don't necessarily want to think about.",
                    "label": 0
                },
                {
                    "sent": "You know your category for cats having in it the same cluster as you know your Category 4.",
                    "label": 0
                },
                {
                    "sent": "Chairs or something like that, so it makes a very strong assumption that you're going to use the same basic vocabulary as the building blocks of every single category that you represent, so we can break that assumption by assuming that we're going doing.",
                    "label": 0
                },
                {
                    "sent": "We go back and we say we're trying to estimate a probability distribution for each of the categories that we're learning category A and category B and what we're going to do is have a separate Dirichlet process for each of those categories.",
                    "label": 0
                },
                {
                    "sent": "So we have this same idea that what you're going to do is going to be forming clusters for each category and those clusters.",
                    "label": 0
                },
                {
                    "sent": "There's always the possibility you see something new which is a member of that category that you hadn't seen before.",
                    "label": 0
                },
                {
                    "sent": "And that new thing is going to be something which you know is the probability of getting a new thing is parameterized by this parameter Alpha of the Dirichlet process.",
                    "label": 0
                },
                {
                    "sent": "But when you choose to generate a new thing the way that you generate a new thing is by going up to this higher level, and there's a second Dirichlet process up here.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that now when we decide to make something new IT might be that it's actually not something that's knew across all of the categories that we've ever seen.",
                    "label": 0
                },
                {
                    "sent": "It might be something which is shared by some other categories.",
                    "label": 0
                },
                {
                    "sent": "Or it could be something which is completely new, so the idea is that say we're going to decide that we're going to generate something that's new.",
                    "label": 0
                },
                {
                    "sent": "Well, what we do is we go to this higher level process in this high level process every time we visited this process to what we saw, each of the clusters that we have represented down here is the consequence of a draw from this high level process to the things that are represented up here.",
                    "label": 0
                },
                {
                    "sent": "The properties of these clusters which we have down here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we'd go up.",
                    "label": 0
                },
                {
                    "sent": "We take a draw from that high level process, and then it might be that we would choose a cluster which we've chosen before.",
                    "label": 0
                },
                {
                    "sent": "Or it could be that we draw a cluster which is completely new and has never been seen before by any of the categories that we use.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we need this higher level processes that if we just had a Dirichlet process for each of these two categories, then there's no way in which they would ever interact.",
                    "label": 0
                },
                {
                    "sent": "So that means that you know, while you might not necessarily want to have the same clusters in your representation of cats and chairs, you might want to have the same clusters in your representation of, say, cats and Lions or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "It might be that some of the same units are useful for representing those categories that are more closely related.",
                    "label": 0
                },
                {
                    "sent": "And not useful for representing categories that are more distant from one another.",
                    "label": 0
                },
                {
                    "sent": "So you need to have the opportunity to share those clusters, and so this gives us a way of tying together these these clustering processes down here where the way that we generate the sort of a set of clusters which are shared across all categories with some probability, and then for any individual category we sort of sample a subset of those clusters and then use that to represent the distribution that we see in each of those those categories.",
                    "label": 0
                },
                {
                    "sent": "So using this framework actually gives us a way of then defining.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A single, unifying rational model that contains all of the examples that I showed you before and you can do this by thinking about the consequences of varying these two parameters.",
                    "label": 0
                },
                {
                    "sent": "One parameter which is describing the rate at which we generate new clusters within categories, another which describes the rate at which we generate new clusters across categories.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to show you some sort of examples of what you get by generating things from this kind of process under these different assumptions about the parameters, so I'm going to show you things that are in sort of rectangles corresponding to categories.",
                    "label": 0
                },
                {
                    "sent": "The black dots are going to be exemplars, and the.",
                    "label": 0
                },
                {
                    "sent": "Unfilled dots are going to be the clusters that we used to represent things within that category.",
                    "label": 0
                },
                {
                    "sent": "So as an example, if we take both of these parameters to Infinity, we get back a model that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Where say here we have three different categories representation for these three categories has one cluster for each of the examples that we see, and this corresponds to a familiar model.",
                    "label": 0
                },
                {
                    "sent": "This is just the exemplar model.",
                    "label": 0
                },
                {
                    "sent": "If we so the idea here is that every time you see an observation within a category, you generate something that's completely new.",
                    "label": 0
                },
                {
                    "sent": "That's what this Alpha goes to Infinity means, and every time we generate something new.",
                    "label": 0
                },
                {
                    "sent": "It's something which has never been seen in any other category.",
                    "label": 0
                },
                {
                    "sent": "That's what this gamma goes to Infinity means.",
                    "label": 0
                },
                {
                    "sent": "If we set gamma to 0, then we're going to get something where every category is represented by only a. Yeah, Alpha goes to zero and then we get something where every category is represented by only a single cluster.",
                    "label": 0
                },
                {
                    "sent": "But if gamma goes to Infinity then those clusters are different across categories and this is the prototype model, so this is a model where now every time we see something here, there's no chance to generate a new cluster for that category, but the clusters that we and every cluster that we generate is going to be something which is new and not shared with any other category.",
                    "label": 0
                },
                {
                    "sent": "But there are other models in the spacer Anderson's model.",
                    "label": 0
                },
                {
                    "sent": "Here is the one where we assume that for every time we get an observation within a category we generate something which is completely new, so that space.",
                    "label": 0
                },
                {
                    "sent": "Likely ignoring the category membership.",
                    "label": 0
                },
                {
                    "sent": "Basically it ends up acting.",
                    "label": 0
                },
                {
                    "sent": "We need to sort of augment the features of the objects with the category label, but we ignore any information about the way that those things are grouped together, and then we have.",
                    "label": 0
                },
                {
                    "sent": "The possibility that we can generate new clusters which can be shared across categories, and so that's the model that Anderson basically proposed, where we've got 11 sort of vocabulary of clusters which are shared across these categories, and there's no sort of specific category specific substructure which respects the structure of those categories, as well as some models we might not believe in.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 where it says.",
                    "label": 0
                },
                {
                    "sent": "That we never.",
                    "label": 0
                },
                {
                    "sent": "We have one cluster per category, but it's possible for that cluster to be shared between multiple categories.",
                    "label": 0
                },
                {
                    "sent": "So this would be kind of like if you lived in a world where categories tended to re label things that you learned before, right?",
                    "label": 0
                },
                {
                    "sent": "So if there were multiple names for the same category, then this would be the sort of assumption.",
                    "label": 0
                },
                {
                    "sent": "But the most interesting models are this one, in which we have every.",
                    "label": 0
                },
                {
                    "sent": "Category being represented by.",
                    "label": 0
                },
                {
                    "sent": "It's own directly process essentially, so we have a multiple clusters within each category.",
                    "label": 0
                },
                {
                    "sent": "But there's no sharing of those clusters, and this one where there are multiple clusters within each category, and there is sharing between those clusters.",
                    "label": 0
                },
                {
                    "sent": "And so these two models are sort of the general case where we allow for the possibility of having either multiple clusters within categories or in this.",
                    "label": 0
                },
                {
                    "sent": "In this case, multiple clusters within categories and the sharing of the clusters between categories, and we can go on and sort of look at how well those models describe what people are doing.",
                    "label": 0
                },
                {
                    "sent": "So remember the basic psychological idea here is that using.",
                    "label": 0
                },
                {
                    "sent": "This way of thinking about learning categories makes it possible for people to adaptively select the category structure that they want based on the information which is provided in the data, and so the question is, do people do that?",
                    "label": 0
                },
                {
                    "sent": "So do people change the representation that they form for a category between, say, something like a prototype representation and something like an exemplar representation as a consequence of the data that they observe.",
                    "label": 0
                },
                {
                    "sent": "And So what we've done is go and look at expense.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Men's wear there's some evidence for this, and this is work with Kevin Kinney, who's a student who's here you can talk about it with Kevin if you want.",
                    "label": 0
                },
                {
                    "sent": "So what Kevin did was look at a particular experiment conducted by Smith and Linda.",
                    "label": 0
                },
                {
                    "sent": "The idea is that if we take the HTP where the Alpha parameter is allowed to range between zero and Infinity, and say we fix the gamma parameter Infinity so there's no sharing between categories, what it can do is infer a representation that somewhere between exemplars and prototypes.",
                    "label": 1
                },
                {
                    "sent": "Provided we're estimating this Alpha parameter from the data.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that we take a structure which seems to show some characteristics of both of these things.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment, the way that these are often presented in the psychological literature we might have here, these are talking about objects that have 6 features.",
                    "label": 0
                },
                {
                    "sent": "Each of those features can have two values, and this category structure corresponds to this set of objects where there's one object which is takes the value 0 on all of those features, one that takes a one in the first position, one that takes one in the second position, and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "You can see that a pretty good rule for learning things that belong to Category A is kind of thinking about it in terms of there being a prototypical example, which is 1 where you know it's something like this.",
                    "label": 0
                },
                {
                    "sent": "It's having a single value on all of those features of 0, but there's an exception to that rule, so there's one member of Category A which violates that rule.",
                    "label": 1
                },
                {
                    "sent": "So this this exception has the other value on all of its features.",
                    "label": 0
                },
                {
                    "sent": "Category B has the reverse structure here, mostly ones, and then one exception which has mostly zeros except for one one, and so the question is.",
                    "label": 0
                },
                {
                    "sent": "What happens when people learn something like this?",
                    "label": 0
                },
                {
                    "sent": "It's not quite consistent with the prototype representation, although prototype representation works pretty well because it's got this exception.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is take.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These data and look at what human learning looks like and what you can see in human learning is here.",
                    "label": 0
                },
                {
                    "sent": "The white circles at the top show everything that's in white corresponds to the things that belong to category A and this is showing the probability of people are signing something to category A based on the amount of training that they've seen, and So what happens is for these white circles these are the things that follow that prototype rule.",
                    "label": 0
                },
                {
                    "sent": "The White triangle is the exception, so this is the thing that violates that rule.",
                    "label": 0
                },
                {
                    "sent": "And then down here, this is the things that belong to Category B and follow the prototype rule and then this is the exception.",
                    "label": 0
                },
                {
                    "sent": "So these are the things that violate that rule and what you can see is that people crossover in the way that they treat these exception items.",
                    "label": 0
                },
                {
                    "sent": "So initially they categorized them in a way which is consistent with the prototype rule, and subsequently they learn that this thing belongs to category A and this thing belongs to category B.",
                    "label": 0
                },
                {
                    "sent": "So this pattern of learning is something which basically reflects a shift from using something like a prototype representation to using something which allows for exceptions like an exemplar representation.",
                    "label": 0
                },
                {
                    "sent": "So if you just take a prototype model and you give it these data, then it produces a pattern which looks like this where the exceptions get classified together with the things that follow the prototype rule and so this looks right for the early part of training but wrong for the later part of training.",
                    "label": 0
                },
                {
                    "sent": "If you take an exemplar model, then you know the way that it can best fit these data is it can sort of perfectly learn the category membership of these things that follow the rule, because it's just remembering examples and so provided it's got previous examples of those things belonging to that category.",
                    "label": 0
                },
                {
                    "sent": "It's very easy for it to decide that they belong.",
                    "label": 0
                },
                {
                    "sent": "But for these things, which crossover it has to kind of learn an intermediate probability, because it's not able to change its its representation of those.",
                    "label": 0
                },
                {
                    "sent": "So what it ends up doing is kind of learning that those things are weakly associated with these categories in order to match the fact that people's probabilities on average are somewhere in between assigning something to category and assigning someone to category B.",
                    "label": 0
                },
                {
                    "sent": "So we use the HTTP model that I was talking about.",
                    "label": 0
                },
                {
                    "sent": "What it can do is basically change the representation as it gets more data.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that you're seeing more and more examples of those.",
                    "label": 0
                },
                {
                    "sent": "Those stimuli which follow that particular category structure, and so as you get as you get more examples, let me just go back.",
                    "label": 0
                },
                {
                    "sent": "What you are what you find is that.",
                    "label": 0
                },
                {
                    "sent": "You're basically getting more and more evidence that the prototype model is the wrong representation, so the first time you see it, you could just say, oh, that was just a coincidence that was just chance that I happen to see something that didn't follow the prototype rule.",
                    "label": 0
                },
                {
                    "sent": "But as you see, more and more examples of that thing that violates the rule, you get stronger and stronger evidence that you need to use a different kind of representation, and so that's what happens.",
                    "label": 0
                },
                {
                    "sent": "Is that initially the model clusters that prototype that violating Exemplar together with the other stimuli, and then overtime it differentiates it and gives it its own cluster.",
                    "label": 0
                },
                {
                    "sent": "And forms a different representation of the data and so that model ends up giving us the best fit to the human data.",
                    "label": 0
                },
                {
                    "sent": "If we compare the log likelihood predicted by the models with the judgments that people make is the only model which produces this crossover effect.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinds of models are also interesting from the perspective of thinking about how it is that people might learn categories in terms of being able to explain how people could get better at learning categories as a consequence of learning categories.",
                    "label": 0
                },
                {
                    "sent": "So in the model where we allow both the Alpha and the gamma parameters, then there's an opportunity for people to basically learn a vocabulary which they could use to organize all of the objects in that domain, and then to use to think about learning in future categories.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you would learn the category of cats and as part of that you formed a cluster which contained all tabby cats, right?",
                    "label": 0
                },
                {
                    "sent": "Then, knowing that all of these things behave similarly in the way that they're categorized might be useful.",
                    "label": 0
                },
                {
                    "sent": "When you go on to learn a category of stripy things, right?",
                    "label": 0
                },
                {
                    "sent": "So if you learn the categories stripy things, and I tell you this is a striking thing, then having already formed a cluster which contains all of the tabby cats, you'd be able to generalize pretty easily that all of the tabby cats are stripy things as well, and so using a model which allows you to share those clusters between categories gives you a way of being able to talk about this kind of transfer learning effect, where people might be able to figure out a kind of vocabulary that they can use in learning.",
                    "label": 0
                },
                {
                    "sent": "Degrees and Kevin's been investigating that too.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 example where you can use Dirichlet processes.",
                    "label": 0
                },
                {
                    "sent": "They get to this question.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How much structure you might be able to infer from data?",
                    "label": 0
                },
                {
                    "sent": "There are lots of other examples.",
                    "label": 0
                },
                {
                    "sent": "The nonparametric block model that I talked about in the causality lecture is basically a Dirichlet process mixture model extended to deal with relations rather than feature based data.",
                    "label": 0
                },
                {
                    "sent": "You can think about making models of language where the numbers of words, syntactic classes, or grammar rules is unknown.",
                    "label": 1
                },
                {
                    "sent": "The way that this is done and something that makes it very simple is that anytime you have a model which assumes a multinomial distribution, you can replace that multinomial distribution with one of these Chinese restaurant processes.",
                    "label": 0
                },
                {
                    "sent": "So that's a very simple way to take a model which is.",
                    "label": 0
                },
                {
                    "sent": "One that assumes something is finite to a model which doesn't make that fixed assumption, and that's basically what we did with this mixture model we said normally you have a multinomial distribution over.",
                    "label": 0
                },
                {
                    "sent": "You know the number over cluster assignments of objects, where that's a distribution of finite dimension.",
                    "label": 0
                },
                {
                    "sent": "Well, now what we can do is replace that with the Chinese restaurant process an in doing that allow this model to learn infinitely many clusters, and this sort of model can be extended in lots of different ways people have looked at lots of extensions, the hierarchical.",
                    "label": 0
                },
                {
                    "sent": "During the process I talked about nested models where you basically generate trees by choosing from the Chinese restaurant process and then choosing again.",
                    "label": 0
                },
                {
                    "sent": "Dependent garishly prophecies where basically which cluster you get assigned to depends on some other property of the stimulus.",
                    "label": 0
                },
                {
                    "sent": "Uh, two parameters, their process which.",
                    "label": 0
                },
                {
                    "sent": "Basically, gives you another sort of control over what that distribution of seating at tables looks like.",
                    "label": 0
                },
                {
                    "sent": "Distance dependency, which again allows you to specify things in a way where so you might know something already about the relationships between the people who are coming into the restaurant, such that it makes it more likely that certain people will sit at the same table, and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "The second example that I'm going to talk about for just showing you case.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is where these sorts of nonparametric Bayesian models can be used.",
                    "label": 0
                },
                {
                    "sent": "Is the example of learning the features of objects so so far I've been talking about categorization and I said, oh, this is how we represent our stimuli.",
                    "label": 1
                },
                {
                    "sent": "We've got some features of those stimuli.",
                    "label": 0
                },
                {
                    "sent": "You just have to learn a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, coming up with what the features of those stimuli are is a nontrivial task in itself.",
                    "label": 0
                },
                {
                    "sent": "So if I show you this object here and I ask you what its features are, right, that's not really a well specified question, right?",
                    "label": 0
                },
                {
                    "sent": "So you know, you're working for the story.",
                    "label": 0
                },
                {
                    "sent": "We tell our participants in our experiment as you're working for a.",
                    "label": 0
                },
                {
                    "sent": "The task Force which is analyzing things that are sent back by the Mars Rover.",
                    "label": 0
                },
                {
                    "sent": "the Mars Rover is wandered into a cave, and it takes a photograph and of something on the wall, and it sends this back.",
                    "label": 0
                },
                {
                    "sent": "So now how should you record this in your book, right?",
                    "label": 0
                },
                {
                    "sent": "How are you going to sort of, you know?",
                    "label": 0
                },
                {
                    "sent": "What are the parts that you think are the relevant parts for encoding this as as your record of what was sent back by the Mars Rover, other things that you can kind of pull off and describe about this and you might be able to make some guesses based on your sort of general visual experience, but it seems kind of underdetermined.",
                    "label": 1
                },
                {
                    "sent": "So the question we've been looking at is what determines the features that we identify, and in particular how the distribution of objects that you observe is relevant to determining this.",
                    "label": 0
                },
                {
                    "sent": "These features of these objects and this is work with Joe Osterweil at Berkeley.",
                    "label": 0
                },
                {
                    "sent": "So this question might be.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easier if I show you some other pictures that were brought back by the Mars Rover, right?",
                    "label": 0
                },
                {
                    "sent": "So what are the major over?",
                    "label": 0
                },
                {
                    "sent": "Did it actually wandered around The Cave and it took photos of these 20 objects here?",
                    "label": 0
                },
                {
                    "sent": "I've just sort of organized them a bit, but here's the object that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "Now you can think about what features you might identify if you are thinking about recording the things that the merger over we're sending back, right?",
                    "label": 0
                },
                {
                    "sent": "And something that might seem like a good strategy to say, you know, it's sort of all of them seem to have this bit on the bottom.",
                    "label": 0
                },
                {
                    "sent": "But then there's this bit on the top, and that varies in a few different ways, and so maybe the way to divide these up is to say.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll sort of encoded in terms of the variation that I see in these bits on top right, and there's four different bits on top, and so that seems like a pretty good description.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can contrast that with, say, a different case where the Mars Rover sends back pictures that look like this.",
                    "label": 0
                },
                {
                    "sent": "These are all inscriptions that is found on the walls of this cave, right?",
                    "label": 0
                },
                {
                    "sent": "So here's the example that I showed you before and now if these are the things that you see it having sent back and I ask you what the features of the object are, then you might respond differently so you can take a look at these.",
                    "label": 0
                },
                {
                    "sent": "And think about what features you might think are relevant to encode what you might notice is that it seems like there's some structures that are repeated across these objects, so like this bit sort of appears in a few different ones.",
                    "label": 0
                },
                {
                    "sent": "And then there's this sort of shape here, shows up here as well, and here, and it seems like maybe that's sort of relevant thing to represent, but it's different here and here, and so on.",
                    "label": 0
                },
                {
                    "sent": "And if you actually go through and analyze this, what you might think about is there are just each of these things on top is actually made up of several different paths.",
                    "label": 0
                },
                {
                    "sent": "And it's those parts that you should be encoding and writing down as describing the features, right?",
                    "label": 0
                },
                {
                    "sent": "Does everybody see there are different parts?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, So what I've done is change the way that you represent the same object by just manipulating the context in which you see that object, right?",
                    "label": 0
                },
                {
                    "sent": "So remember, this is exactly the same thing as this, but now what you think about is the features of this object are going to change as a consequence of having seen these different objects that have nothing to do with like I haven't changed what this looks like, but I've changed the surrounding context and that context influences the way that you perceive the features of that object, and we're interested in being able to explain that effect.",
                    "label": 0
                },
                {
                    "sent": "Where the context in which you see things changes the way that you perceive them.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way of kind of thinking about characterizing this as a mathematical problem is it kind of dimensionality reduction problem, and in particular we analyze this in terms of binary matrix factorization, where the idea is that you could take each of the objects that you saw, and each of those images with a binary image.",
                    "label": 0
                },
                {
                    "sent": "Every pixel was either on or off.",
                    "label": 0
                },
                {
                    "sent": "You take each image and turn it into one long vector.",
                    "label": 0
                },
                {
                    "sent": "Then you stack them up and you get a big matrix where you've got objects as rows and columns.",
                    "label": 0
                },
                {
                    "sent": "Pick out the pixels of those images, and then there's some pattern of pixel activations associated with each of those objects.",
                    "label": 0
                },
                {
                    "sent": "And so this is a matrix factorization problem, because what we're trying to do is infer a lower dimensional representation for those things when we're trying to learn the features of objects.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is come up with a more efficient way of describing those objects rather than just referring to the pixels that those objects express, right?",
                    "label": 0
                },
                {
                    "sent": "The pixels that appear in those pictures, and so you can kind of think about this as your goal is to kind of come up with a representation where there might be some relatively small number of features where each of these features is either on or off for this object.",
                    "label": 0
                },
                {
                    "sent": "So we have a binary matrix here where it has one row for every object.",
                    "label": 0
                },
                {
                    "sent": "And one column for every feature and then some information about how it is that those features map you back into that original space.",
                    "label": 0
                },
                {
                    "sent": "And we can do this by actually using a noisy or distribution, just like I used last time.",
                    "label": 0
                },
                {
                    "sent": "This is the general form of the noisy or distribution, so it says the probability that we have a particular pixel get turned on is 1 -- 1 minus Lambda, which is the sort of strength of these causes.",
                    "label": 0
                },
                {
                    "sent": "And then we take the inner product between the features of the object and these vectors that map.",
                    "label": 0
                },
                {
                    "sent": "Each of those features back onto the original pixel space.",
                    "label": 0
                },
                {
                    "sent": "And then there's some baseline probability that a particular pixel will turn on.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is that if we're interested in predicting whether this particular pixel is on or off, what we do is we look at the row, which tells us which features that object has, and then we go to the column which corresponds to which particular pixel that is in that image, and then we take the inner product between these two things, and as that inner product gets larger, as there are more features turned on, the influence that particular pixel, then it becomes.",
                    "label": 0
                },
                {
                    "sent": "More and more probable that particular pixel is turned on in the image, and So what we're really trying to do, then is take this matrix and split it up into these two matrices, one of which tells us about what the features of the objects are and the other, which tells us how to translate those features into that original visual domain.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one problem that we have is we need to figure out how many features these objects possess.",
                    "label": 0
                },
                {
                    "sent": "So what the dimensionality of these matrices are?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's something which we can do again using a nonparametric Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "If we assume the total number of features is unbounded, but only a finite number will be expressed in any finite data set, then we can think about defining a probability distribution over these binary matrices where we want to have a fixed number of rows between unbounded number of columns.",
                    "label": 1
                },
                {
                    "sent": "And we can do that using another kind of stochastic process, something that's called the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "So the Indian buffet process, as you might imagine, is inspired by the Chinese restaurant process uses a slightly different culinary meta.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the basic idea is sort of similar.",
                    "label": 0
                },
                {
                    "sent": "What we have is now an Indian restaurant, and rather than thinking about seating configurations, we think about what kind of food people eat.",
                    "label": 0
                },
                {
                    "sent": "So you have to imagine one of those infinitely long Indian buffets, right?",
                    "label": 0
                },
                {
                    "sent": "What happens is the first customer walks into the restaurant and then samples are passed on Alpha number of dishes, so they draw a number of dishes from price on distribution.",
                    "label": 1
                },
                {
                    "sent": "In retrospect, I regret not having this called this.",
                    "label": 0
                },
                {
                    "sent": "The all you can eat seafood process because then we would have an extra pen with the Python distribution, but there you go.",
                    "label": 0
                },
                {
                    "sent": "So then the next customer who comes in is going to taste dishes with probability, which is greater for those dishes which have been tasted before.",
                    "label": 0
                },
                {
                    "sent": "So for each of those dishes at the first person tasted, there's a 50% chance the second person tastes them, and then they drop acid on Alpha over 2 number of new dishes, and then the next person comes in, and basically the probability that they taste a dish goes up as those dishes have been tasted before and they sample appointment Alpha override new dishes.",
                    "label": 1
                },
                {
                    "sent": "Result of this we get a distribution over, you know, patterns of tasting dishes in which the customers are exchangeable, just like in the Chinese restaurant process.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you think about kind of making a little matrix and thinking about our roses corresponding to our customers and the columns corresponding to the dishes and then putting one in that matrix every time a customer tastes a dish, then what we're doing is defining a probability distribution over a binary matrix.",
                    "label": 0
                },
                {
                    "sent": "And this is a binary matrix where the number of rows is fixed.",
                    "label": 0
                },
                {
                    "sent": "We can say how many customers go into the restaurant, the number of columns is unbounded, and as a consequence it gives us exactly the kind of object that we need in order to be able to learn how many features something has.",
                    "label": 0
                },
                {
                    "sent": "So we can define a model taking that.",
                    "label": 0
                },
                {
                    "sent": "Binary matrix factorization problem using the noisy orders are likelihood.",
                    "label": 0
                },
                {
                    "sent": "Using the Indian buffet.",
                    "label": 0
                },
                {
                    "sent": "Processes are prior and feeding.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The images that I showed you before and what happens is that it basically picks out features which correspond to those features, which intuitively seems like a good way of representing these different domains.",
                    "label": 0
                },
                {
                    "sent": "So if you feed it these images, what it does is infer that you know these different parts on top correspond to features and those are the things that you should encode.",
                    "label": 0
                },
                {
                    "sent": "If you feed it these images, and it infers these different paths where now there's a way of kind of building up each of these objects in terms of possessing these different individual features.",
                    "label": 0
                },
                {
                    "sent": "What's important about the way that we constructed these is actually that.",
                    "label": 0
                },
                {
                    "sent": "Each of these objects corresponds to something which we can generate using these features.",
                    "label": 0
                },
                {
                    "sent": "What happens is each of these objects is generated from three of these six features combined together.",
                    "label": 0
                },
                {
                    "sent": "If you take these six features and you think about all combinations of three, there are actually 24 of those.",
                    "label": 0
                },
                {
                    "sent": "So what I've shown you here is 20 of those 24 possibilities, the frequency of any one of these individual parts is actually the same between these two sets, because these are made up of the same features, and So what we made sure to do is to make sure that you know.",
                    "label": 0
                },
                {
                    "sent": "In each case, there are five instances of each of these features because those features appear and sorry, yeah, those features appear in each of these individual stimulate, so I think there's actually 10 instances or well, we can figure it out.",
                    "label": 0
                },
                {
                    "sent": "But what we've done is matched the frequencies between these two cases of these individual features, so there's no sort of guide that you could you could.",
                    "label": 0
                },
                {
                    "sent": "You could sort of figure out that these are the right features based on, you know they're being higher frequency here and lower frequency here.",
                    "label": 0
                },
                {
                    "sent": "The statistical properties are the same, the pixel wise variances are the same.",
                    "label": 0
                },
                {
                    "sent": "The only thing that differs between these sets is the amount of correlation that's expressed between these features.",
                    "label": 0
                },
                {
                    "sent": "We're here.",
                    "label": 0
                },
                {
                    "sent": "The correlation comes out because we've combined these features together in particular ways, and here this represents basically an arbitrary factorial combination of these features.",
                    "label": 0
                },
                {
                    "sent": "So what's good about that?",
                    "label": 0
                },
                {
                    "sent": "Is it actually means that if you take a generic kind of statistical method for identifying a lower dimensional representation, it actually fails to do so here, and other cognitive models also don't discriminate between these different ways of representing this data, so you actually need to have something like this sort of noisy or binary matrix factorization model to pull out these features.",
                    "label": 0
                },
                {
                    "sent": "But it also means that we.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can test whether people are forming this kind of generalization where we compare the inferences that they make from having seen these correlated images to the inferences that they make from having seen these factorial images and the basic idea is that what we do is train them by exposing them to these and telling them they come from the Mars Rover and then showing them some new pictures and asking them to rate how likely it is that those new pictures are things that the Mars Rover also found in the same cave.",
                    "label": 0
                },
                {
                    "sent": "So some of these correspond to things that they've seen in training before, so these are exactly the things that we repeated many times in the correlated case.",
                    "label": 0
                },
                {
                    "sent": "They also form a subset of the things that were seen in the factorial case, but we also present them the four factorial combinations of those six features which they never saw before.",
                    "label": 0
                },
                {
                    "sent": "So remember I said, there are 24 possible configurations if you take 6 features and you take combinations of three of those, there are 24 possibilities.",
                    "label": 0
                },
                {
                    "sent": "So we show people 20 here we hold out four and we show them those four.",
                    "label": 0
                },
                {
                    "sent": "And then we also have a control where we just show people shuffled parts.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting about this is that we can make a prediction if people have actually infer the appropriate parts from looking at these objects and thus have the feature representation that I showed you is produced by this model.",
                    "label": 0
                },
                {
                    "sent": "Then they should generalize to these stimuli as well.",
                    "label": 0
                },
                {
                    "sent": "They should say that these are actually fairly likely to be things that the Mars Rover is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trying to find and.",
                    "label": 0
                },
                {
                    "sent": "So if we look at what they do here, these are the judgments of people make if we show them these factorial cards, then they think that both the things that they saw and the things that they didn't see but are made up of the same parts are likely to be seen in the future.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we show them those correlated cards, they're very sure that the things they saw or likely to be seen again.",
                    "label": 0
                },
                {
                    "sent": "But there there's a statistically significantly lower generalization to those things that are made up of the same paths, and this is actually what's predicted by our model here.",
                    "label": 0
                },
                {
                    "sent": "So this is taking that noisy or.",
                    "label": 0
                },
                {
                    "sent": "Infinite matrix factorization model and looking at the predictions it makes and it predicts exactly this pattern of behavior.",
                    "label": 0
                },
                {
                    "sent": "So people are basically pulling out those features in a way which depends on the distribution of the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Objects that they see.",
                    "label": 0
                },
                {
                    "sent": "So you can use this sort of strategy of taking distribution over a matrix with a fixed number of rows, an unbounded number of columns.",
                    "label": 0
                },
                {
                    "sent": "Basically, in any case where you're interested in defining a sparse latent feature model where you don't know the number of dimensions.",
                    "label": 1
                },
                {
                    "sent": "So there are versions of PCA, ICA, collaborative filtering models, and so on that use the ICP as a prior.",
                    "label": 1
                },
                {
                    "sent": "You can use it as a prior on the adjacency matrix for a bipartite graph where you have a class of nodes that has unknown sides.",
                    "label": 0
                },
                {
                    "sent": "So something like inferring hidden causes where you know that there's some number of observed variables.",
                    "label": 1
                },
                {
                    "sent": "You don't know how many UN observed variables influences of those.",
                    "label": 0
                },
                {
                    "sent": "As an example of this, there's an interesting link to beta processes.",
                    "label": 0
                },
                {
                    "sent": "Remember I said something sort of arcane about the connection between the Chinese restaurant process and there is a process the same arcane connection exists between the beta process and the Indian buffet process, and like the during the process, there are many ways in which you can extend this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic argument here is that these nonparametric Bayesian models give us a way of answering questions about how much structure to infer, and those questions are kind of ubiquitous when we try and think about the inferences that people make for questions, like how many clusters we can use.",
                    "label": 1
                },
                {
                    "sent": "Something like the Chinese restaurant process or the Irish process.",
                    "label": 0
                },
                {
                    "sent": "Questions like how many features we can use, something like the Indian buffet process or the beta process, and those things can be extended and combined together in ways that allow us to build up rich representations that are the kinds of things that we want might want people to learn in different domains.",
                    "label": 0
                },
                {
                    "sent": "And this kind of you know, ability to combine these models together and also to think about them as the consequence of thinking about sort of particular sorts of combinatorial stochastic processes means that there's a lot of room for people to develop new models that you know in the same way have desirable statistical properties, but allow you to make inferences about domains where you have potentially unbounded.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instruction so thank you.",
                    "label": 0
                }
            ]
        }
    }
}