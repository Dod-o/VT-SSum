{
    "id": "7nydzqrfhshjnri7wwsnw5xi4csbpval",
    "title": "Focusing Human Attention on the \"Right\" Visual Data",
    "info": {
        "author": [
            "Kristen Grauman, University of Texas at Austin"
        ],
        "recorded by": [
            "IEEE ICME"
        ],
        "published": "Sept. 18, 2012",
        "recorded": "July 2012",
        "category": [
            "Top->Computer Science",
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Human Computer Interaction"
        ]
    },
    "url": "http://videolectures.net/icme2012_grauman_visual_data/",
    "segmentation": [
        [
            "Hi good afternoon everybody.",
            "I'm really happy to be here and have a chance to talk to you.",
            "All.",
            "I want to thank the organizers for bringing me here.",
            "I've already seen so a number of interesting new applications new to me through your work today and my focus is in computer vision and machine learning or machine learning for applications in computer vision.",
            "And what I want to talk about today specifically is how to bring humans into the loop for certain computer vision tests, and specifically, how do we focus our the humans attention on the right things in visual data.",
            "This work is done together with John Daly, City Judge Anderson Hahn and critique."
        ],
        [
            "As we know, this data really captures very meaningful aspects of our world and has great.",
            "Great potential for helping us with tasks in terms of scientific analysis for discovery, for communication and for entertainment, and this can be from a wide variety of sources like the ones you see depicted here.",
            "We also know that the scale of visual data that we're able to capture very easily today is quite high.",
            "However, the catcher and the storage of this data is one thing.",
            "On the other hand, we need computer vision to allow us to analyze it.",
            "Also at a large scale."
        ],
        [
            "So this indeed is where computer vision is to have its greatest impact, perhaps is how to automate the kind of interpretation or understanding of this visual information.",
            "How to do that in a large scale.",
            "So the kind of things where you would want to automate and standard computer vision task might be things like how do I recognize an object or an activity, an image and video?",
            "How do I do things like track people within a running video?",
            "How do we perform 3D construction?",
            "How do we do image retrieval based on the content of the image is not just the keywords or.",
            "How do I do some form of detection for objects or?",
            "Properties of interest.",
            "So this is just yeah, good illustrative collection of the kind of problems in computer vision that would allow us to bring the human out of the loop, right?",
            "So get the kind of interpretation on a large scale of these big collections of images and video without the human's input.",
            "So we want to go from data vision algorithms and some kind of prediction at the end.",
            "OK, so I'm talking in very broad terms here, but I want to contrast this kind of pipeline that we traditionally take with uh with instead.",
            "Is this summer automatic pipeline?",
            "So now let's think of it not as data algorithm connection, but now data vision algorithm plus human intervention and then output is prediction."
        ],
        [
            "Think about how we semi automate some of these tasks, but the human is brought into the system in a very useful way.",
            "The space key questions is which visual data, meaning which images which video most deserves human attention and this is the focus of my talk today.",
            "And of course this is actually a very broad question itself, so we'll have to think about it in specific contexts of certain tests you need in the context of a task.",
            "What is it that makes the data most important or what makes it most warrant the humans attention?",
            "So let's look."
        ],
        [
            "Now at two different settings and this is the two main parts of the remainder of this talk and the first one will look at how we can sift through all this visual data with our automated techniques to determine which ones should be analyzed by humans, such that I can best teach a system how to detect an object, category and images.",
            "So this is supervised learning about the categories with the human in the loop.",
            "And then the second task setting will look at unsupervised video summarization.",
            "So here, in contrast to the first task, now we're looking at how can I take a very long running video and figure out how to make a short version of that video such that a human could watch it and understand the main gist.",
            "What's common to both of these is the idea of bringing in the human just where necessary.",
            "So in the first one, just where necessary in terms of how to teach the system about these object categories through annotations and in the second one for video summarization, bringing the human judgment necessary to understand what wasn't originally a very long video.",
            "Not much, but will notice here is that there are some challenges that are really shared by both of these distinct applications.",
            "So one thing for both of these tests.",
            "We need to be able to predict what's important.",
            "And at the same time, for both these tests we have.",
            "Very quickly some scaling issues, so we have to be mindful of the computational requirements of our approach because we want these things to be able to cope with very massive collections of data.",
            "OK, so this is our overview."
        ],
        [
            "And we're going to start with this first part on looking at how to do supervised learning about the categories with a human in the loop.",
            "So just to give a little bit of background so they know there is a kind of a diverse collections of expertise in this Community.",
            "So for object recognition technique today generally best performing techniques are based on discriminative learning.",
            "And So what this means is that we have at the on set some kind of training data.",
            "Let's suppose we're learning how to detect cars and images.",
            "So we got images that contain cars.",
            "Perhaps some that don't contain cars and will get a human annotator in the loop here to teach the system in the form of labeled examples.",
            "So someone just say, well, here's these ones.",
            "These instances are cars.",
            "These are not cars and you can imagine learning save some form of discriminative classifier to distinguish between the two such that when you get a new example looking at its image features, you can make a prediction about which class it belongs to.",
            "High in very broad terms, this really is the kind of pipeline that's behind some of the very most successful techniques for object detection object recognition today."
        ],
        [
            "But it shows us then certainly is that the data is key, the kind of data that we used to populate this collection on the left is going to really.",
            "Have a great influence on the kind of models that will get or that the system will learn an impact in literature.",
            "There's even dedicated efforts to do this kind of data set collection, and these these efforts service in terms of test beds, is especially to kind of benchmark different techniques on common datasets.",
            "But Jan data set creation there is work in the community.",
            "Looking at most recently how to get these kind of annotations in this large scale manner, perhaps using crowds so using online services to get perhaps even non experts to label this data for you.",
            "And what's challenging here in what people have shown is how can you package the kind of image annotation test such that nonexperts to do that and that you can do them with good quality control?",
            "And the third line of work, kind of in this general area was to look at active learning techniques.",
            "So to figure out how we could actively request the most useful annotations.",
            "And this is not something I want to spend some time on.",
            "In the last several years and in R and in this first pass setting I want to show you how active learning can be quite beneficial for training object detectors.",
            "OK, So what is active learning in, especially in the context of learning object categories?"
        ],
        [
            "So in contrast to the kind of static learning paradigm where we might have labeled data and build those classifiers for objects, an active learning framework, we can think of it as a more of a continuous process.",
            "Right, so instead of the scan or this.",
            "Fixed set of labeled data, we're going to use our current models at any given point in time to analyze a collection of unlabeled data.",
            "So these are some other images for which we don't yet know the labels.",
            "And use use our certainty about that unlabeled data to actively request new labels from the annotators.",
            "So in this cartoon, suppose they were still learning about cars, and this is an unusual car, let's say under some criterion.",
            "Then we could ask for a label on that guide next.",
            "And use the annotators response to arguments.",
            "The labeled data pool.",
            "And then use them on many labeled data to then refine the current customers so you can see how this is a continuous kind of learning loop.",
            "An active learning paradigm where we can intelligently bring in the human annotator just where we need it."
        ],
        [
            "Not exactly the intent with active learning in general, and in this specific case, and what we'd like to see is that we get better models more cheaply, meaning using the human effort.",
            "Most clever that you're most intelligently if you do that, you would get a learning curve, where as you add more and more applications the accuracy of your predictions shoots up quickly.",
            "OK, so good learning curve is steep one because you learn the most for your money right?",
            "Whereas if you were alone, just passively.",
            "In other words, just stating that other annotations might come your way from your human annotators, you might expect a slower learning curve that looks like this red one.",
            "OK, so that's the goal, right?",
            "This is what we'd like to see happen.",
            "Stuart employ active learning while learning about object categories."
        ],
        [
            "There's a problem though, and then you know, as I said, we've been working in this area for several years in terms of object recognition and interact in an active learning and what we notice.",
            "Is there some some kind of artificial restrictions in in the current paradigms that we use active line for training object detectors?",
            "An offer this is kind of a sandbox learning, So what I mean by that?",
            "Well, there's a few artificial aspects.",
            "One, we generally think of the.",
            "Unlabeled data pool.",
            "As being something we've already gathered.",
            "For example, you might imagine taking a benchmark data set like Caltech 101 or something like this and using that as your only will pull where you simply hold the labels from learning algorithm and then start revealing them as it makes its active choices right.",
            "So in this kind of unfortunately, there's a couple problems, one that datasets relatively small in scale, maybe an order, thousands of examples, and two it's biased or not.",
            "It's already been prepared.",
            "In fact, it's probably won't even been prepared by a vision researcher, and so this is not in the true sense an active learner that's going out on its own to to learn about the world.",
            "Junior images it's very artificially constrained to this kind of sandbox, and the data set that we already knew about, and I see this as a real problem in terms of trying to see whether active learning can have influence in a in a real world setting case.",
            "Let's that's one app that's the first of three aspects that we that we see as a limitation.",
            "Second one is that computational class is getting more than previous techniques for active learning in these kind of settings, and even beyond computer vision settings.",
            "And sometimes this is OK, right?",
            "If I'm really just worried about optimizing the use of my human annotation effort, maybe I don't care about how much computation has to be done in order to come up with these active requests.",
            "But if you think about the real world implications, if I want to run this kind of system and not have my annotators just waiting, I leave while I spend, you know, I had my cluster running to decide which annotation requests should come next, then this is really a problem.",
            "In fact, it might mean that the kind of learning curves I showed you before.",
            "Guess what in practice?",
            "If I don't just account for iterations of active learning, but also the computational time involved in making these requests?",
            "The 3rd and final limitation that we see in this kind of current setting would be that our researcher is generally in the.",
            "So I thought we wanted human loop.",
            "I don't necessarily mean we want a domain expert in the loop, right?",
            "And went where this happens is where you have someone who's kind of fine tuning the annotation tasks as a systems running, usually ourselves or the designer of the learning technique in such a way that the system is not truly autonomous.",
            "OK, so this is the kind of problems we see and I'll show you how we can overcome them.",
            "Our challenge will be to try and create what I call a live active learning system.",
            "OK, So what does it mean to be lying and also act?"
        ],
        [
            "Well, that's why that means we're not going to have a cans benchmark data set as our unlabeled full of data.",
            "In fact, we're gonna have a system that can itself go out and crawl for the potential images that might be of interest.",
            "So that's live in that aspect.",
            "And then we're also going to completely close the system.",
            "I mean, we'll have our hands off and the kind of images that the system decides it wants to learn about will go immediately to crowdsourcing services to get the labels, so it will completely self contained an autonomous system.",
            "That can do this active learning process on data that we have nothing data for which we have nothing to do with this choice.",
            "OK, so this is our goal.",
            "Large life, active learning system and the technical challenge that I wanted to present to you specifically here is how to do this in a large scale.",
            "So how can we scale active learning techniques to allow us to make these kind of selections where unlabeled pool is massive?",
            "Sam orders a million or millions of examples."
        ],
        [
            "OK.",
            "So I can tell you that existing active learning techniques would generally require at least linear time with respect to the number of unlabeled data points in that pool.",
            "If you're considering appraise learning techniques that that scans through to determine which one makes the most seems to have the most benefit for labeling next.",
            "And in fact anymore or even much more expensive, even quadratic or worsen with respect to the data set size.",
            "So let's just look at one specific criterion that that's quite intuitive, so here's an active selection function that's for support vector machine classifier that looks at the point that's nearest to the current hyperplane decision boundary as the one that's going to be most informative to get labeled Ness.",
            "And this is a fairly well known and simple technique, but also quite effective in practice, and has some theoretical underpinnings in terms of quickly reducing the hypothesis space.",
            "So what this criterion does, suppose these black points are unlabeled ones, and then we have some positively labeled points in some negative label points here.",
            "But an are current decision boundary parameterized by this hyperplane W?",
            "Then the best point to label next is the one over all the unlabeled points you.",
            "That minimizes this this function, right?",
            "So it's just saying the one that's nearest to my current decision boundary is the one to get labeled this.",
            "But notice that this indeed requires a linear scan over all points in the only pool, and if I have millions of such points, this is not scalable, 'cause in fact I want to continuously do this for every next request that I want an issue, right?",
            "So let's take this selection criterion and make it scalable to massive collections of data.",
            "To do this, we've introduced a novel hashing technique.",
            "FYI, those points that are nearest to hyperplane surface in sublinear time."
        ],
        [
            "So graphically, it looks like this.",
            "So you've got this large unlabeled data pool.",
            "Then we'll design A hash function H that we can apply to all of those images.",
            "And we'll use it to put all these examples in the hashtable index by these binary hash keys in such a way.",
            "That when I also apply this hash function to the current classifier.",
            "Go guarantee that with high probability.",
            "The classifier itself W is going to match to those examples that are nearest to that current hyperplane.",
            "OK, so I will dare points have been hacked into this table and now we can assure is that as we hash with the with the classifier parameters were going to go directly to those points that are most uncertain.",
            "So why is that a good thing?",
            "Well now I only need to look at those examples.",
            "And compute that selection function and I've got a guarantee on how likely those are to be the ones I would have selected.",
            "Whereas to exhaustively scan through all of the data points.",
            "So look at only those and then I can rate them and determine what my next active request should be.",
            "So this is the kind of high level view of our approach an if you're familiar with hashing techniques in general.",
            "The thing to note here that's distinct.",
            "Instead of doing a similarity search with the hash function, where we would normally think of mapping points to points, here we have a hyperplane to point search.",
            "Problem.",
            "Is my query itself is a hyperplane, and the things I'm retrieving are those points that are nearest to it in any direction.",
            "Let me give you."
        ],
        [
            "A little more intuition about how this could work.",
            "That's basically what we what we want to design, and I'll show you how it can work.",
            "So first quick background on an existing hashing function for locality sensitive hashing, it's quite intuitive, so the probability that a random hyperplane would separate two vectors depends on the angle between them.",
            "So suppose you had two vectors XI, XJ.",
            "If they have a small angle separating them, and then if I come in with some some random hyperplane part, the probability that hyperplane splits XI and XJ.",
            "Is smart.",
            "High contrast effect scientist.",
            "They are separated by a wider angle.",
            "The probability that this random hyperplane splits them is higher.",
            "And that's the basis for simple hash function, where if this hash function parameterized by random random vector R, we could assign a bit according to which side of the hyperplane it falls on.",
            "You can imagine doing this now independently for any new X and what you'll get is that the probability that you collide mean that you get the same bit assigned for one XI and some other XJ is directly related to the angle between upside history.",
            "So this is a known property that's been very useful for locality sensitive hashing.",
            "RLSH an it allows you to do similarity search for or to guarantee that probability of collision is high for points that have small angles between them.",
            "But in fact this is not what we need to solve the problem I just mentioned 'cause this is going to assure that you have high probability of collision for nearly parallel vectors.",
            "Right smaller angle and in fact we need for our cases to have high probability of collision for perpendicular vectors.",
            "So just to kind of illustrate why that's so, if you look at this sketch that."
        ],
        [
            "We had before.",
            "A good point to collide with my hyperplane W is 1 where it's about perpendicular to both the hyperplane normal W and negative value.",
            "To say this is a good point to to get.",
            "Whereas about point to Clyde would be one that's far from the hyperplane, and that would be 1 where both of these angles are not close to 90 degrees.",
            "So what we need is a hash function, then that will guarantee high probability of collision for vectors that are not permitted.",
            "Let me show you, then building an intuition from before how we can do this."
        ],
        [
            "Imagine generating 2 two bit hash function using two random vectors U&V.",
            "OK, the first one will constrain angle between your data plan X and this type of classifier W. The other one will constrain angle between X and negative W. And I will say that collision is likely only if neither of these vectors splits.",
            "So let's look at the two extremes.",
            "So I'm wondering if you have vectors that are nearly nearly parallel, then we want.",
            "These not to collide in our our hashtable, so for the first hyperplane V, let's say it's unlikely to split these guys.",
            "But when we look in the other pair were likely to split with the second hyperplane here.",
            "So these would be likely not to collide in our hash table.",
            "If you look at the other extreme, the one that we do want collisions for.",
            "For perpendicular vectors, the chance of collision, or sorry, the chance of splitting for X&W is lower for the 1st first random hyperplane and also low for the second hyperplane.",
            "So in this case we're unlikely to split, meaning we're likely to fall in the same bucket in hash table.",
            "This is the definition between behind how we can generate the hash functions that are desired, and I'm the kind of nut shell hash."
        ],
        [
            "Option is is written here, so we have a hash function that we call asymmetric, meaning it can be applied either through our data points X or to our classifier parameters.",
            "W will treat them a little bit differently depending on which one it is, and it's all just based on these simple hash functions.",
            "Using the the sign of the inner product between these random hyperplanes, UNB TANAR inputs A&B.",
            "And we've proven the necessary balance to show that it is indeed locality sensitive for the the criterion of interest, which is the amount which the angle between the data point XW deviates from 90 degrees.",
            "OK."
        ],
        [
            "So we have a way to do this caching that I described at the onset and what this means is now we can take our classifier as a ever evolving query right?",
            "And this is what happens in active learning class work keeps evolving as I get new label data, so we can imagine in one iteration perhaps I hash to this first set of points near the current decision boundary.",
            "I get them labeled by a human and now I can refine the classifier hash again with that classifier and get a new set of data points to label and so on.",
            "So what kind of key to this design is that the thing that stationary or data points in the hashtable?",
            "So the one time procedure to get them all hash and the thing that keeps changing is the classifier.",
            "And that's one hash in to then figure out which points we should get labeled next.",
            "OK, so let's look."
        ],
        [
            "Illustrated results so first just a qualitative one.",
            "We've deployed this approach to do some large scale active learning here with million examples from a tiny image data collection, and in fact has not been fully human labeled really due to its scale.",
            "This is great testbed for this active learning approach.",
            "So what I'm showing here at first 9 images that this is someone request labels for when learning either that category airplane or the category automobile, and it's kind of neat to see you know, for example, on the airplane side, these are things that maybe look.",
            "Nearly like airplanes or confusing airplanes that the system is first going to ask a human about an was excited to get this skill of result.",
            "It's really as I said, these are unlabeled data within the tiny image collection, and there's over 1000 categories present within them.",
            "So all those thousands of possible categories it's really singling out.",
            "Those that are most images that are most useful to learn about airplane or automobile.",
            "OK, now looking."
        ],
        [
            "Typically at this result, let's see what we're getting.",
            "So here is a learning curve.",
            "Same format as the kind of curves I showed before.",
            "So as you use more iterations of learning, meaning more labels added, how good is the prediction accuracy?",
            "And here's the passive learning curve.",
            "Things get better as you add more labels, but it's a bit slow in progress.",
            "Now, if you were to do exhaustive active learning using this simple margin criterion, you'll get a curve that looks much better.",
            "And if you were to approximate that exhaustive selection using the approach I just introduced, you'll get a curve that's quite close.",
            "However, you're getting that results are as in blue there with orders of magnitude less computational time.",
            "And that's what's showing these box plots on a large scale.",
            "Now the key thing is if I put both these results together so by account, not just for the human labeling time as it's gone up top here, but also the selection time, meaning how much computation I do to make my choice, then I get learning curves that look like this.",
            "So here's what happened.",
            "Now we're counting for both selection and labeling time.",
            "An interesting Lee and a passive learning.",
            "An exhaustive active learning are basically awash right, so the one is wasting time figuring out which you know.",
            "Past learning is wasting time getting irrelevant examples labeled, whereas my exhaustive active learner is wasting time figuring out which requests to make threats, and we get that kind of the advantage over both if we take our hyperplane hashing approach to do active selection very efficient and it is the key result for this.",
            "For this line of work and really the 1st result of its kind to account for the real human effort as well as the real machine effort to do at large collective learning system.",
            "So now let's"
        ],
        [
            "The time we make a lot of active learning, so I kind of showed you would like to break free from this kind of data set based learning and put active learning out in a real live deployed system.",
            "So to do that we're going to let our system figure out which images might even be relevant so we won't feed them to the system.",
            "System is going to call in this case we use Flickr for potentially relevant images.",
            "So here I say we learn about bicycle gets on Flickr, find some images that may contain bicycle.",
            "And then we'll do some candidate generation to think about where the bicycle might be in each image.",
            "So now we have our unlabeled collection of of potential bicycle windows.",
            "And now consider applying the hashing approach I defined earlier.",
            "Get all your data hash in this table.",
            "Now we can come in with our current classifier for bicycle versus non bicycle an in sublinear time figure out which requests should be made next.",
            "So that needs to be the request that we make, in our case to two on line annotators.",
            "So we're using a service called Mechanical turn.",
            "This allows us to push label request.",
            "Too many annotators in parallel.",
            "We pay them for their work, it's mixing quality, so we use some automatic consensus forming.",
            "We use a mean shift approach to kind of cluster the bounding boxes that they give us in return fully automatic, and then this will all men are labeled data and we can continue this process in a loop like we saw before.",
            "OK, so this is our full live active learning system.",
            "The thing I wanted to do noise here.",
            "Well, there's two parts, so one we had nothing to do with which image is going to the unlabeled pool, right?",
            "And this is in contrast to all the previous results that I was alluding to before.",
            "This is completely autonomous way to deploy this active learning approach and we're doing this on a scale where this unlabeled pool consists of millions of examples.",
            "And doing so is only possible because of the scalability of the approach that we defined.",
            "In fact, to run this in a naive way would have taken 60 hours per iteration.",
            "Have you done?",
            "Had we done that with existing exhaustive search?",
            "But it takes like 10 minutes.",
            "Using your approach I just defined."
        ],
        [
            "OK, so when you do this live active learning when you get here are some some more learning curves.",
            "The format you know you know high and steep early on is the best and I'm showing you is our results in blue doing this live active learning approach from crawled Flickr images and pass it baselines in red and black.",
            "And so these two baseline basically are doing what we would normally do in data set collection today which is keyword search and then manually prune right and the red one is putting them at the image level.",
            "The Black One is putting them at the.",
            "Bounding box window level.",
            "So it was really exciting to me about these results.",
            "Is here with a completely hands off and autonomous system.",
            "We're seeing learning curves that are improved by this large scale active learning approach, and that's improved beyond what I refer to as the status quo collection.",
            "This kind of keyword search and then randomly or passively print out.",
            "Always work and you can see it clear.",
            "Clear failure case for the chair class where the learning curves for the blue are coming up behind that just the keyword search approach.",
            "Trying to get an understanding The thing is and always sticks categories.",
            "The classes are quite difficult.",
            "We pick them intentionally because of their difficulty for existing detectors, but in some sense perhaps chair is one of the most difficult or the most varied among these images."
        ],
        [
            "So what kind of things do we pick here?",
            "Again, it said that the subwindow level within the images 'cause we're not gonna text are here.",
            "When learning about about boats, the system would like to know about the labels for these guys on top first.",
            "Whereas the passive approach might pick these images first in random order and kind of looks.",
            "What's useful to note here is not that these images that the passive learner selects, don't contain both.",
            "However, they seem to contain images of quotes that are either of the kind, not useful for learning the boat detector.",
            "After this passes or you know, correcting an unusual view or both related things."
        ],
        [
            "So final result, I wanted to show you using this kind of large scale active learning system was our biggest challenge to ourselves, which is can we take this approach and actually improve performance on what is a very very well studied object potential benchmark.",
            "And this is the Pascal VLC data set which some of you may be familiar with.",
            "Basically it has images originate Ng from Flickr and this is sort of the benchmark for object detection.",
            "Much effort, dozens of teams compete on this every year to see how well can you do with state of the art detection techniques."
        ],
        [
            "So our role here is to see if all if we took our light active learner and let it try to improve Pascal performance.",
            "How would we do?",
            "And then we looked at the six again, the six most challenging classes in that data set.",
            "We are very encouraged to find out the live active learner getting its own knew crawl data can in fact improve over with what are many well studied and well engineered techniques.",
            "Meaning you know, the best of the state of the art as shown in this table.",
            "And in fact we can only do this because the approach we define is so scalable.",
            "So this is just showing kind of a breakdown at the kind of cost you could expect if you were to use existing strong detectors with straightforward active learning paradigms versus our approach and and what we see is we can do things on order minutes that would have taken days even for existing approaches."
        ],
        [
            "Alright, so so far what I've shown you is how we can let our computer vision system figure out that data that's most important to for a human to look at.",
            "In this case, a human annotator who's trying to train a recognition system.",
            "And.",
            "What's exciting opportunity about these results is that we're trying to break free from purely data set based learning, so get out of that sandbox and let US system learn autonomously enough, at least in a Fuller extent.",
            "And what our results are showing is there is possibility of improving upon our status quo for data set collection using this kind of approach.",
            "On the technical side of main contribution, is this hyperplane hashing idea, which I which I briefly described, and then these do results that are really on a scale we haven't seen before in the active learning literature.",
            "Specifically for computer vision.",
            "And that's with the data pools that are about millions or contain millions of examples."
        ],
        [
            "OK, so now let's look at the other side of the second context that I mentioned at the onset, and we're changing from supervised up to category detector training to unsupervised video summarization.",
            "This is some very recent work that we've been looking at about the past year and I'm very excited about this new area.",
            "So here is the."
        ],
        [
            "Well, I've got a long video, and in fact we're specifically looking at egocentric video.",
            "And like I said check, I don't mean you know self centered.",
            "Well I do, but not in that sense.",
            "I mean self centered where the camera is a wearable camera.",
            "OK, so someone wearing something camera is about at eye level and we're just capturing the camera where we stay.",
            "OK, so you might get something that looks like this video here.",
            "It'll be hours in length easily, and our goal is to summarize it into a storyboard.",
            "So I'd like to take that long video and come up with automatically a storyboard that shows the most important events, objects in people that happen throughout the day.",
            "So this would be great, because now we're going from a very long visual input is something that a human can look at almost in a glance and get the main idea.",
            "So again, another example, we're letting the system figuring out, figure out what should a human look at."
        ],
        [
            "So why would we specifically want egocentric video summarization?",
            "The good thing is wearable cameras have come along way, so in 20 or more years you know it's no longer the specialty of, say, lifelogger, who might wear contraption like this.",
            "Back in the day.",
            "But now we've got these nice lightweight cameras and infect people are wearing in certain settings and I think could could get more value out of if we could provide automatic summarization.",
            "For example, imagine in law enforcement we have police officers who are such cameras with computer vision to do the summarization we could have.",
            "Days worth of activity readily summarize for their later coming, 'cause also applications that I'm interested in an in health care and elder care.",
            "So there are existing studies showing that for someone with memory loss, just looking back at wearable camera capture can help improve recall, and that's even without vision in the loop to do some automated summarization.",
            "And finally, I'm especially interested in this egocentric view, 'cause I think it's most likely to transfer well to save it to to mobile robot applications.",
            "So no longer thinking of just static cameras observing a scene that we want to summarize is more standard and sort of surveillance kind of applications.",
            "But now we have an agent moving around the world and we want to summarize with, but that agencies.",
            "So you can imagine a robot that's exploring on a new territory or new environment and then goes back to the human.",
            "A summary of what it saw.",
            "Alright, so This is why."
        ],
        [
            "I don't want to do this.",
            "Summarization existing techniques would not be well suited for the egocentric view, and that's because generally they need to assume a static camera, which we don't have with this wearable camera, and they're often relying on fairly low level cues.",
            "OK, we'll come back to this and the cubicle imitation really is that they're not concerned about.",
            "What makes the story of the video?",
            "So our goal will be to produce summary that actually pays attention to what are the important objects and people that tell the story of the long running video.",
            "And here it's important, has a very specific meaning.",
            "Cousin, the egocentric view importance, refers to those things with which the camera where has significant interaction."
        ],
        [
            "So our idea is to develop a set of new cues that would possibly reflect importance in the egocentric setting, and in particular we make these.",
            "We would like these cues to be category independent.",
            "By that I mean we want to be able to predict importance with without sensitivity to who the camera where is or what things they are saying.",
            "OK, so it's not that we want to learn that, say, a fire truck is always important, but we want to learn what properties about the fire truck in the image are important.",
            "If in the given context, if they are so."
        ],
        [
            "OK, so this is the basic goal.",
            "Create a summary that pays attention to the important people in objects and let me first give you an outline of the pipeline and I'll fill in a few details and then look at some results.",
            "Is the first thing we'll do in the approaches to learn about what's important, and to do this will need some training data and we'll get we'll do a collection again.",
            "We use some crowdsourcing resources to come up with some annotations about what's important in video.",
            "Then we design the features that might indicate importance and will train an important detector.",
            "So a function that could account for the given region is in the video.",
            "This kind of offline learning then when you get a new video will first partition it into temporal events.",
            "Breach event will discover which things are important and within it.",
            "And finally will generate a storyboard.",
            "A compact story book summary that have slates the main people and objects.",
            "This is the main data flow from top to bottom and I need to tell you a little bit more about each of these steps."
        ],
        [
            "So when we get the data collection we use this camera that's depicted.",
            "Here is the look see camera it captures at the frame rate resolution you see here.",
            "We got about 37 hours of content from initially from about four different camera.",
            "Where's we tried to diversify our camera?",
            "Where's we didn't want them to be?",
            "Just ourselves.",
            "We thought undergraduates, for example, might have more interesting lives than ourselves.",
            "Maybe so.",
            "And so at this point, when we did this study, these are the users we had and we specifically told the camera where is not to do anything special.",
            "We didn't want staged activity, we just wanted to go about their day.",
            "So the kind of data we get back or things like cooking, eating, shopping, working, that kind of activity."
        ],
        [
            "That's our data set and now to do the first stage of just learning about what's important.",
            "We need to ask people to teach the system about about importance and of course important is really context dependent.",
            "So we needed to do is show annotators and accelerated sub clip of video from the training side and then ask them to write down in the first stage.",
            "What are the most important things.",
            "So for this kind of clip they might run it down.",
            "Things like the man in the blue shirt or the yellow notepad on table.",
            "And then as a second task we add."
        ],
        [
            "Our annotators to take those textual descriptions and then annotate within sampled frames where those actual regions are.",
            "So then I give us outlines like you should see here in green.",
            "So why do we divide it out this way?",
            "Well, we learned and developing this that it can bias the results if you just say label things that are important in this video.",
            "For example, you might be inclined to label things that are just easy to outline versus those that are truly important.",
            "So this helps us to to avoid that kind of bias.",
            "As we can get our."
        ],
        [
            "Training data this way and then we want to learn region importance.",
            "So then our task is to develop cues that might suggest important that we can measure from the data.",
            "So first divide the images from our video frames into regions using automated segmentation techniques."
        ],
        [
            "And I will develop these features that might reflect important.",
            "So the first one tries to capture possible physical interaction between the camera where an objects in the world.",
            "So this first Q looks at the distance to the hand.",
            "So if you're looking at this green region here, we want to know then how far is it from the nearest hand.",
            "If there is a hand in view."
        ],
        [
            "Second, egocentric feature is meant to catch her coarsely.",
            "The gaze of the camera.",
            "Where so, of course, keep in mind this camera is rigidly attached to the person's head, and so if we look at the distance from the center of the frame, we're getting approximation of where the person was looking.",
            "And finally we look at an egocentric you that catches frequencies.",
            "We do some matching across a 10 minute window within the video clip an we can automatically then gauge how frequent that object was.",
            "In.",
            "This again, could be accused for important.",
            "In addition to these egocentric use, we develop some object based features.",
            "One we use an object like saliency, using existing techniques from endreson colleagues, and then for the emotion based agency that we developed in our group.",
            "And I'm skipping details on these definition.",
            "Basically, saliency of the region with respect to its surroundings.",
            "And then we also with keep, take into account whether the region overlaps the face regions with the idea that people are generally wanting important players.",
            "In that to be here in the story.",
            "I found the capture.",
            "Some region features things like size, location within the frame and this helps us capture a specific context.",
            "Specific things like you know if I had lunch with someone there generally with certain scale or certain position within the frame.",
            "Hey, I just want to stress that all of these cues are category independent, meaning we're learning properties that we think will be indicative of importance no matter the category of the object."
        ],
        [
            "Yes, we can take these kind of QS.",
            "Employ your favorite learning technique here.",
            "We start with a fairly simple regression pass, so now we're trying to gauge the importance of region R and will extract all those cues.",
            "Letters defined on the previous slide.",
            "These X features for our regions are and.",
            "We will learn a regressor to predict a function to predict how important is given.",
            "The good thing about 9 degrees importance.",
            "Then we can adjust the compactness of the summary accordingly.",
            "I mean food pairwise interaction terms between the features we expect there will be some interesting relationships between them.",
            "For example, maybe a region is only important if it's object like an.",
            "Also a certain point with respect to the center of the frame.",
            "OK, so we're learning.",
            "Then I requested that it will during training use the overlap with the ground truth region as the prediction variable."
        ],
        [
            "OK, so we've got a function that can predict how important our region is, so now we're ready to do a summary.",
            "So we'll take a new video, and 1st, we'll divide it into the temporal events that we see here.",
            "We use a fairly simple but effective approach to do the segmentation.",
            "What I'm showing here is a distance matrix based on color histograms between all your frame.",
            "So frame one through N from one through in and the block structure we see here is showing that there is some color similarity that emerges.",
            "We as you shift from scene to scene and in fact we also put the strong guy here is because we have a waiting according to the temporal distance between the frames as well.",
            "OK, so this allows us to get the main chunks that activity and also allows us to call an object important multiple times if it occurs in different contexts.",
            "For example, if the dog is in the morning, the camera where it plays with the dog in the morning at home and then takes the dog for a walk in the park later, these could emerge as two different important instances within the entire summary.",
            "Hey, once we started our video into events Now what we want to do is say for each event what are the important things that we encountered?"
        ],
        [
            "So here's where we apply our our region inference predictions on all regions, and we might have something like hundreds of regions for every frame, so we'll score them by their imports will also do some automated matching to come up with representative regions for each such discovered object.",
            "And finally we can."
        ],
        [
            "Our summer.",
            "So this is an example summary where we've we've colored the frame groups by events that were detected.",
            "So here we have a 1234 event summary and the frames that are shown are those that contain the most important things.",
            "As our system predicted them to be in each event.",
            "So we've gone from one video to short storyboard summary showing you exactly what the system thinks a human should bother looking at.",
            "Understand the main idea."
        ],
        [
            "So let's look at some of the results.",
            "First, let's look in isolation at how that region importance prediction can do these on in this column on the left, here are the most important regions are method would find in these frames and they again these are the most important regions out of about 500 regions for every frame.",
            "Give you some sense of comparison.",
            "If I then look at very recent work, an object like region detection, Anna even a classic low level saliency method, these are the kind of results that you would get.",
            "So in these good example success cases, we see that we're better able to focus on objects like regions that are also important to the story of the of the video.",
            "Alright, some things, for example like level 70 operators.",
            "Just going to look for more low level Qusayr bright spot that surrounded by dark spots or something like this."
        ],
        [
            "Of course it doesn't always work, and here are some failure cases using our region or important prediction.",
            "Again, ours are on the left and green and what we kind of see happening here.",
            "Some failure cases where very frequent thinkin can emerge is important, even though it's not necessarily so to the story, and we think that we employed some cues that might account for say, the same depth of these features.",
            "This kind of cases might be improved.",
            "OK, and then we find all this.",
            "Certainly you know, looking at all these compared comparing to these kind of low level and high level selling see approaches and we see that indeed by learning about what's important in the context of a story, we can get much stronger and more accurate results.",
            "That's just one piece of it.",
            "The final reporting piece is to look at the kind of summaries that were generated.",
            "So let's look at an example here that's very illustrated on the left.",
            "I'm showing you a 3 hour video.",
            "Not really, will look at a look at exit of course, so here is some some sample from what was a 3 hour video of someone's day.",
            "And you can see kind of.",
            "But the activity might be worse.",
            "We can only see it very slowly and that's the whole point.",
            "So now if you look at the right I'm about to play our storyboard summary just into video form.",
            "So we'll look at the storyboard frame after frame in sequence on the right hand side, and I'm showing you is a simple 12 frame summary of what was originally 3 hours of content, right?",
            "And so your job looking at this summary is to decide what's the story of this person's day.",
            "Let's see what they did.",
            "And the colors are seeing are the events that we detected.",
            "OK, so that was 12 frames, very fast to watch, right?",
            "And I think you got an idea of what happened.",
            "Perhaps there was a drive to the store, some shopping for food.",
            "Looking at the recipe on an iPad, doing some cooking, doing some cleaning, eating and watching TV, something like this, right?",
            "And all this we can get it automatically and you know, it's basically the system allows us to focus on what's really important to the story.",
            "A very good example of what's possible with this kind of approach.",
            "Of course, we want to know what would happen if we did something much simpler than the pipeline that I just defined.",
            "So let's look at a couple baseline techniques below.",
            "Just send a second here.",
            "Now the first baseline will be.",
            "If I were just U uniform keyframe sampling.",
            "So imagine over the course of the three hour video, if I just sampled frames, surely I'll hit some of the things that were primarily present.",
            "And so that's what we'll look at first soon, as this agrees to move forward for me."
        ],
        [
            "OK, so here on the left here the uniform key key frame sampling approach again just a 12 frame summary.",
            "And here's what we would get.",
            "And you'll see that certainly we can.",
            "We can hit on things that show what happened, but you also see a lot of some redundancy and also some irrelevant frames.",
            "There's nothing stopping us from grabbing A-frame or the persons swinging their head from side to side looking around the kitchen, say.",
            "Now on the right, this is a existing summarization technique that would optimize for the diversity between the frames, which is can be a very natural thing to do for a good sign.",
            "So here is a summary, that's.",
            "Maximizing the diversity present within the frames that are selected.",
            "You can kind of see that in the chain of frames that move from darker one sweater ones with different content.",
            "But I hope you see in terms of the summary.",
            "I first showed you using our approach to these kind of baseline techniques is that we're really now better able to focus on the curtains and the eccentric sense things, objects and people in order to make a story out of this video.",
            "And we have quantified this kind of.",
            "Behavior in terms of counting how many important things?"
        ],
        [
            "Do we capture within a summary given land on the skip?",
            "Some of the detail here and basically we can see a strong advantage to using this important prediction to form these summary."
        ],
        [
            "And also we've come back to our cameras to see, you know, perhaps most importantly, how's the perceived quality of these summaries.",
            "So specifically what we did is ask instead of blind test we asked the camera where is to watch the baseline summary Anar summary.",
            "Not saying which is which and then answer these two questions you see here an over all about 6070% of the time.",
            "They're finding our summaries to be more effective."
        ],
        [
            "Other cases, so in this one case there was some consistency among the judges about our approach, working more poorly.",
            "When we looked at it, we realized, well, OK, so the the baseline is doing just as well.",
            "This uniform sampling baseline is doing just as well for this video where the person was on their laptop all day long.",
            "OK, so the person was on her laptop at home.",
            "The person is at laptop in class.",
            "The personal laptop in the lab, right?",
            "So you can do just as well if you sample frames with the laptop across the day.",
            "Yeah, I think for me was this is video captured by a student was very slow to respond to my email so I didn't understand this.",
            "After seeing kind of the story of the day, push amounts to being on the laptop.",
            "But you can see that you know we need some diversity or activity to to have benefit from this approach.",
            "And finally, we're also interested in and not just the temporal context of these stories, but also the spatial contexts.",
            "So in some of our data collection we had our camera.",
            "Where is Carrie GPS receiver so that we could make a storyboard that also shows the locations of the events and so here is our storyboard, but now plotted over the trajectory of the persons day.",
            "So you can imagine making this kind of summary that allows you to glance back at your history, maybe your own or some some social version of such a history to see what were the important things that were encountered.",
            "Throughout the entire day."
        ],
        [
            "OK so I have shown you two ways in which we can learn to focus human attention on the most useful visual data and even by distinct settings.",
            "One for training objects, vectors and two for doing video summarization.",
            "And I think together at least this kind of line of work is quite interesting, because it allows us to rethink purely automating computer vision processing and instead think of semi automating it.",
            "So bringing humans into loop in a way that's most effective, and in doing so, I think we'll discover more and more new applications for large scale visual analysis that can benefit from human intervention.",
            "Not not complete autonomy.",
            "Amazon, I'll conclude, and I'd be glad to take any questions.",
            "Thank you.",
            "When you ask the subject themselves.",
            "Compare.",
            "I was wondering if there are two aspects.",
            "So how much is enough like he's 12 crimes in a 420 or 100?",
            "Person.",
            "Yeah, so the first question in terms of how do you know how many frames are needed to make a good story board is a very important one, and and we've kind of approached the summary, we approach it as if you have a budget, right?",
            "So if your budget is K friends, then what's the best summer you can do?",
            "And in fact I didn't go into these these very until we have one right where we would optimize in terms of capturing the most importance within.",
            "That budget or another word?",
            "Sorry, having a criterion on the level of importance we need to have to make this memory and another version where we actually tried to to optimize towards the budget of frames that you have.",
            "So so in our context, I think it's reasonable to assume that you're given a budget of length.",
            "Another thing you've got this on video.",
            "I'm willing to watch it for this many seconds or this many minutes, so that would be your target for the summary.",
            "Like, but certainly there's more.",
            "There's more that you can think about doing in this regard.",
            "And then the second question about evaluation.",
            "So you're right.",
            "So the study I showed or briefly alluded to here, we ask cameras.",
            "We've also gone to other users as well.",
            "The way we did this is we first had the camera wears, make a textual summary of what they did.",
            "Then we had our other group of users watch the videos, read the textual once, and then make judgments about which was better capturing.",
            "And then kind of more generally, what we're realizing here and what your question points to its right.",
            "It can be very difficult to evaluate something like unsupervised video summarization, and it's very important how the user perceives the result.",
            "So this is one way we felt we could quantify.",
            "Both using the expertise of the camera, where who really saw the full digital content, 'cause we can't make our new users watch while we could, but we can't pay $12.00 a video in order to do this evaluation, so this is the way we've come up with so far to do that.",
            "Do you take it up?",
            "So you said that hashing is fun too.",
            "Switch my cash to the same location.",
            "Yeah, so keep in mind that in this kind of passion framework collision is the goal is to have collision be proportional to similarity distance.",
            "So in General, Kelly sensitive hashing is based on this premise that things that are similar should collide, and that's a good thing for search, because now you go directly to those things that are most similar.",
            "What's the twist here in our setting?",
            "Is that the collision should be happening for not similar points, but points that are close to a hyperplane?",
            "Does that clarify a bit?",
            "Yeah, very interesting.",
            "So I'm wondering about the important issue.",
            "In some ways, what's most important is what's unexpected or uncommon.",
            "And so when you talk about importance, I think that you're automatically assigning a value system and you have subjective POV less important.",
            "So my question is whether you can really do this without any context.",
            "And if you can't, which I think you can't, then what you're doing is actually just maybe summarizing, trying to include as much as we can in that summary that you happily making any important judgments.",
            "Yeah, you do this without context.",
            "So important is a dangerous word because it's very loaded, right?",
            "I hopefully mostly used in quotes for this reason, but but in our setting, what we define importance to be is what is central to the telling of the story of the video, and this is exactly what we instructed.",
            "Our laborers are annotators to keep in mind as they told us what this import.",
            "So if we just said what's important, and that could mean a dozen more things, but ours means what allows you to depict the full story.",
            "And that's also why when we showed our annotators the video, we didn't give them up.",
            "Trying to give them the context of I don't know.",
            "Maybe about 5 minutes of contents to to make their judgment.",
            "Yeah, I think importance and in the most abstract sense we're not going to learn in the way here, but important in terms of category independent cues for the egocentric setting, which also gives us very specific constraints, right?",
            "That allows us to exploit things like centrality in the framework nearest hands.",
            "I think this is possible to capture and learn as the results are showing.",
            "I just want to pull up from the comment around market be done without context.",
            "I think my comment on that Friday to my question is the fact that I need extremely important and visually important.",
            "Adding is made of the different thing and I think very interesting.",
            "If you were in the sense that I think is very much visually version but also think my question was so.",
            "This car fication you got someone else actually entertained another person's life in the sense that what they saw?",
            "When someone else, not the actual person himself.",
            "So I wonder what is the impact of getting someone else to actually summarize innocence when they let someone else is kinda like confusing there, right?",
            "Yes great, great .2.",
            "So you're right, we have people who did not wear the camera as our annotators to judge what was important.",
            "This is intentional in our heart and that we were hoping to learn what I'm referring to is category independent queues.",
            "Meaning I I'd like of summarization technique that is applicable generically to these videos, no matter who the camera where was.",
            "I could think of Alternatively a setting where you actually do want to tailor your summary to the wear himself or herself, where it would be important to learn video secrecies that that have to do with that camera.",
            "Where is life and there I think maybe some of these you could think of this category independent approaches to still a useful precursor to something that's more tailored.",
            "To an individual, but this isn't something that we've explored.",
            "So if there are any other questions, I'd like to the mighty join you.",
            "Thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "I'm really happy to be here and have a chance to talk to you.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "I want to thank the organizers for bringing me here.",
                    "label": 0
                },
                {
                    "sent": "I've already seen so a number of interesting new applications new to me through your work today and my focus is in computer vision and machine learning or machine learning for applications in computer vision.",
                    "label": 0
                },
                {
                    "sent": "And what I want to talk about today specifically is how to bring humans into the loop for certain computer vision tests, and specifically, how do we focus our the humans attention on the right things in visual data.",
                    "label": 1
                },
                {
                    "sent": "This work is done together with John Daly, City Judge Anderson Hahn and critique.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we know, this data really captures very meaningful aspects of our world and has great.",
                    "label": 0
                },
                {
                    "sent": "Great potential for helping us with tasks in terms of scientific analysis for discovery, for communication and for entertainment, and this can be from a wide variety of sources like the ones you see depicted here.",
                    "label": 0
                },
                {
                    "sent": "We also know that the scale of visual data that we're able to capture very easily today is quite high.",
                    "label": 0
                },
                {
                    "sent": "However, the catcher and the storage of this data is one thing.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we need computer vision to allow us to analyze it.",
                    "label": 0
                },
                {
                    "sent": "Also at a large scale.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this indeed is where computer vision is to have its greatest impact, perhaps is how to automate the kind of interpretation or understanding of this visual information.",
                    "label": 0
                },
                {
                    "sent": "How to do that in a large scale.",
                    "label": 0
                },
                {
                    "sent": "So the kind of things where you would want to automate and standard computer vision task might be things like how do I recognize an object or an activity, an image and video?",
                    "label": 0
                },
                {
                    "sent": "How do I do things like track people within a running video?",
                    "label": 0
                },
                {
                    "sent": "How do we perform 3D construction?",
                    "label": 0
                },
                {
                    "sent": "How do we do image retrieval based on the content of the image is not just the keywords or.",
                    "label": 1
                },
                {
                    "sent": "How do I do some form of detection for objects or?",
                    "label": 0
                },
                {
                    "sent": "Properties of interest.",
                    "label": 0
                },
                {
                    "sent": "So this is just yeah, good illustrative collection of the kind of problems in computer vision that would allow us to bring the human out of the loop, right?",
                    "label": 0
                },
                {
                    "sent": "So get the kind of interpretation on a large scale of these big collections of images and video without the human's input.",
                    "label": 0
                },
                {
                    "sent": "So we want to go from data vision algorithms and some kind of prediction at the end.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm talking in very broad terms here, but I want to contrast this kind of pipeline that we traditionally take with uh with instead.",
                    "label": 0
                },
                {
                    "sent": "Is this summer automatic pipeline?",
                    "label": 0
                },
                {
                    "sent": "So now let's think of it not as data algorithm connection, but now data vision algorithm plus human intervention and then output is prediction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about how we semi automate some of these tasks, but the human is brought into the system in a very useful way.",
                    "label": 0
                },
                {
                    "sent": "The space key questions is which visual data, meaning which images which video most deserves human attention and this is the focus of my talk today.",
                    "label": 1
                },
                {
                    "sent": "And of course this is actually a very broad question itself, so we'll have to think about it in specific contexts of certain tests you need in the context of a task.",
                    "label": 0
                },
                {
                    "sent": "What is it that makes the data most important or what makes it most warrant the humans attention?",
                    "label": 0
                },
                {
                    "sent": "So let's look.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now at two different settings and this is the two main parts of the remainder of this talk and the first one will look at how we can sift through all this visual data with our automated techniques to determine which ones should be analyzed by humans, such that I can best teach a system how to detect an object, category and images.",
                    "label": 0
                },
                {
                    "sent": "So this is supervised learning about the categories with the human in the loop.",
                    "label": 1
                },
                {
                    "sent": "And then the second task setting will look at unsupervised video summarization.",
                    "label": 1
                },
                {
                    "sent": "So here, in contrast to the first task, now we're looking at how can I take a very long running video and figure out how to make a short version of that video such that a human could watch it and understand the main gist.",
                    "label": 0
                },
                {
                    "sent": "What's common to both of these is the idea of bringing in the human just where necessary.",
                    "label": 0
                },
                {
                    "sent": "So in the first one, just where necessary in terms of how to teach the system about these object categories through annotations and in the second one for video summarization, bringing the human judgment necessary to understand what wasn't originally a very long video.",
                    "label": 0
                },
                {
                    "sent": "Not much, but will notice here is that there are some challenges that are really shared by both of these distinct applications.",
                    "label": 0
                },
                {
                    "sent": "So one thing for both of these tests.",
                    "label": 0
                },
                {
                    "sent": "We need to be able to predict what's important.",
                    "label": 0
                },
                {
                    "sent": "And at the same time, for both these tests we have.",
                    "label": 0
                },
                {
                    "sent": "Very quickly some scaling issues, so we have to be mindful of the computational requirements of our approach because we want these things to be able to cope with very massive collections of data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our overview.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to start with this first part on looking at how to do supervised learning about the categories with a human in the loop.",
                    "label": 0
                },
                {
                    "sent": "So just to give a little bit of background so they know there is a kind of a diverse collections of expertise in this Community.",
                    "label": 0
                },
                {
                    "sent": "So for object recognition technique today generally best performing techniques are based on discriminative learning.",
                    "label": 1
                },
                {
                    "sent": "And So what this means is that we have at the on set some kind of training data.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose we're learning how to detect cars and images.",
                    "label": 0
                },
                {
                    "sent": "So we got images that contain cars.",
                    "label": 0
                },
                {
                    "sent": "Perhaps some that don't contain cars and will get a human annotator in the loop here to teach the system in the form of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "So someone just say, well, here's these ones.",
                    "label": 0
                },
                {
                    "sent": "These instances are cars.",
                    "label": 0
                },
                {
                    "sent": "These are not cars and you can imagine learning save some form of discriminative classifier to distinguish between the two such that when you get a new example looking at its image features, you can make a prediction about which class it belongs to.",
                    "label": 0
                },
                {
                    "sent": "High in very broad terms, this really is the kind of pipeline that's behind some of the very most successful techniques for object detection object recognition today.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it shows us then certainly is that the data is key, the kind of data that we used to populate this collection on the left is going to really.",
                    "label": 0
                },
                {
                    "sent": "Have a great influence on the kind of models that will get or that the system will learn an impact in literature.",
                    "label": 0
                },
                {
                    "sent": "There's even dedicated efforts to do this kind of data set collection, and these these efforts service in terms of test beds, is especially to kind of benchmark different techniques on common datasets.",
                    "label": 0
                },
                {
                    "sent": "But Jan data set creation there is work in the community.",
                    "label": 0
                },
                {
                    "sent": "Looking at most recently how to get these kind of annotations in this large scale manner, perhaps using crowds so using online services to get perhaps even non experts to label this data for you.",
                    "label": 0
                },
                {
                    "sent": "And what's challenging here in what people have shown is how can you package the kind of image annotation test such that nonexperts to do that and that you can do them with good quality control?",
                    "label": 0
                },
                {
                    "sent": "And the third line of work, kind of in this general area was to look at active learning techniques.",
                    "label": 0
                },
                {
                    "sent": "So to figure out how we could actively request the most useful annotations.",
                    "label": 0
                },
                {
                    "sent": "And this is not something I want to spend some time on.",
                    "label": 0
                },
                {
                    "sent": "In the last several years and in R and in this first pass setting I want to show you how active learning can be quite beneficial for training object detectors.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is active learning in, especially in the context of learning object categories?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in contrast to the kind of static learning paradigm where we might have labeled data and build those classifiers for objects, an active learning framework, we can think of it as a more of a continuous process.",
                    "label": 0
                },
                {
                    "sent": "Right, so instead of the scan or this.",
                    "label": 0
                },
                {
                    "sent": "Fixed set of labeled data, we're going to use our current models at any given point in time to analyze a collection of unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So these are some other images for which we don't yet know the labels.",
                    "label": 0
                },
                {
                    "sent": "And use use our certainty about that unlabeled data to actively request new labels from the annotators.",
                    "label": 0
                },
                {
                    "sent": "So in this cartoon, suppose they were still learning about cars, and this is an unusual car, let's say under some criterion.",
                    "label": 0
                },
                {
                    "sent": "Then we could ask for a label on that guide next.",
                    "label": 0
                },
                {
                    "sent": "And use the annotators response to arguments.",
                    "label": 0
                },
                {
                    "sent": "The labeled data pool.",
                    "label": 0
                },
                {
                    "sent": "And then use them on many labeled data to then refine the current customers so you can see how this is a continuous kind of learning loop.",
                    "label": 1
                },
                {
                    "sent": "An active learning paradigm where we can intelligently bring in the human annotator just where we need it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not exactly the intent with active learning in general, and in this specific case, and what we'd like to see is that we get better models more cheaply, meaning using the human effort.",
                    "label": 1
                },
                {
                    "sent": "Most clever that you're most intelligently if you do that, you would get a learning curve, where as you add more and more applications the accuracy of your predictions shoots up quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so good learning curve is steep one because you learn the most for your money right?",
                    "label": 0
                },
                {
                    "sent": "Whereas if you were alone, just passively.",
                    "label": 0
                },
                {
                    "sent": "In other words, just stating that other annotations might come your way from your human annotators, you might expect a slower learning curve that looks like this red one.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the goal, right?",
                    "label": 0
                },
                {
                    "sent": "This is what we'd like to see happen.",
                    "label": 0
                },
                {
                    "sent": "Stuart employ active learning while learning about object categories.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a problem though, and then you know, as I said, we've been working in this area for several years in terms of object recognition and interact in an active learning and what we notice.",
                    "label": 0
                },
                {
                    "sent": "Is there some some kind of artificial restrictions in in the current paradigms that we use active line for training object detectors?",
                    "label": 0
                },
                {
                    "sent": "An offer this is kind of a sandbox learning, So what I mean by that?",
                    "label": 1
                },
                {
                    "sent": "Well, there's a few artificial aspects.",
                    "label": 0
                },
                {
                    "sent": "One, we generally think of the.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled data pool.",
                    "label": 0
                },
                {
                    "sent": "As being something we've already gathered.",
                    "label": 0
                },
                {
                    "sent": "For example, you might imagine taking a benchmark data set like Caltech 101 or something like this and using that as your only will pull where you simply hold the labels from learning algorithm and then start revealing them as it makes its active choices right.",
                    "label": 0
                },
                {
                    "sent": "So in this kind of unfortunately, there's a couple problems, one that datasets relatively small in scale, maybe an order, thousands of examples, and two it's biased or not.",
                    "label": 0
                },
                {
                    "sent": "It's already been prepared.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's probably won't even been prepared by a vision researcher, and so this is not in the true sense an active learner that's going out on its own to to learn about the world.",
                    "label": 0
                },
                {
                    "sent": "Junior images it's very artificially constrained to this kind of sandbox, and the data set that we already knew about, and I see this as a real problem in terms of trying to see whether active learning can have influence in a in a real world setting case.",
                    "label": 0
                },
                {
                    "sent": "Let's that's one app that's the first of three aspects that we that we see as a limitation.",
                    "label": 0
                },
                {
                    "sent": "Second one is that computational class is getting more than previous techniques for active learning in these kind of settings, and even beyond computer vision settings.",
                    "label": 0
                },
                {
                    "sent": "And sometimes this is OK, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm really just worried about optimizing the use of my human annotation effort, maybe I don't care about how much computation has to be done in order to come up with these active requests.",
                    "label": 0
                },
                {
                    "sent": "But if you think about the real world implications, if I want to run this kind of system and not have my annotators just waiting, I leave while I spend, you know, I had my cluster running to decide which annotation requests should come next, then this is really a problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, it might mean that the kind of learning curves I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Guess what in practice?",
                    "label": 0
                },
                {
                    "sent": "If I don't just account for iterations of active learning, but also the computational time involved in making these requests?",
                    "label": 0
                },
                {
                    "sent": "The 3rd and final limitation that we see in this kind of current setting would be that our researcher is generally in the.",
                    "label": 0
                },
                {
                    "sent": "So I thought we wanted human loop.",
                    "label": 0
                },
                {
                    "sent": "I don't necessarily mean we want a domain expert in the loop, right?",
                    "label": 1
                },
                {
                    "sent": "And went where this happens is where you have someone who's kind of fine tuning the annotation tasks as a systems running, usually ourselves or the designer of the learning technique in such a way that the system is not truly autonomous.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the kind of problems we see and I'll show you how we can overcome them.",
                    "label": 0
                },
                {
                    "sent": "Our challenge will be to try and create what I call a live active learning system.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does it mean to be lying and also act?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, that's why that means we're not going to have a cans benchmark data set as our unlabeled full of data.",
                    "label": 0
                },
                {
                    "sent": "In fact, we're gonna have a system that can itself go out and crawl for the potential images that might be of interest.",
                    "label": 0
                },
                {
                    "sent": "So that's live in that aspect.",
                    "label": 0
                },
                {
                    "sent": "And then we're also going to completely close the system.",
                    "label": 0
                },
                {
                    "sent": "I mean, we'll have our hands off and the kind of images that the system decides it wants to learn about will go immediately to crowdsourcing services to get the labels, so it will completely self contained an autonomous system.",
                    "label": 0
                },
                {
                    "sent": "That can do this active learning process on data that we have nothing data for which we have nothing to do with this choice.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our goal.",
                    "label": 0
                },
                {
                    "sent": "Large life, active learning system and the technical challenge that I wanted to present to you specifically here is how to do this in a large scale.",
                    "label": 1
                },
                {
                    "sent": "So how can we scale active learning techniques to allow us to make these kind of selections where unlabeled pool is massive?",
                    "label": 1
                },
                {
                    "sent": "Sam orders a million or millions of examples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I can tell you that existing active learning techniques would generally require at least linear time with respect to the number of unlabeled data points in that pool.",
                    "label": 0
                },
                {
                    "sent": "If you're considering appraise learning techniques that that scans through to determine which one makes the most seems to have the most benefit for labeling next.",
                    "label": 0
                },
                {
                    "sent": "And in fact anymore or even much more expensive, even quadratic or worsen with respect to the data set size.",
                    "label": 0
                },
                {
                    "sent": "So let's just look at one specific criterion that that's quite intuitive, so here's an active selection function that's for support vector machine classifier that looks at the point that's nearest to the current hyperplane decision boundary as the one that's going to be most informative to get labeled Ness.",
                    "label": 0
                },
                {
                    "sent": "And this is a fairly well known and simple technique, but also quite effective in practice, and has some theoretical underpinnings in terms of quickly reducing the hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "So what this criterion does, suppose these black points are unlabeled ones, and then we have some positively labeled points in some negative label points here.",
                    "label": 0
                },
                {
                    "sent": "But an are current decision boundary parameterized by this hyperplane W?",
                    "label": 1
                },
                {
                    "sent": "Then the best point to label next is the one over all the unlabeled points you.",
                    "label": 0
                },
                {
                    "sent": "That minimizes this this function, right?",
                    "label": 0
                },
                {
                    "sent": "So it's just saying the one that's nearest to my current decision boundary is the one to get labeled this.",
                    "label": 0
                },
                {
                    "sent": "But notice that this indeed requires a linear scan over all points in the only pool, and if I have millions of such points, this is not scalable, 'cause in fact I want to continuously do this for every next request that I want an issue, right?",
                    "label": 0
                },
                {
                    "sent": "So let's take this selection criterion and make it scalable to massive collections of data.",
                    "label": 0
                },
                {
                    "sent": "To do this, we've introduced a novel hashing technique.",
                    "label": 0
                },
                {
                    "sent": "FYI, those points that are nearest to hyperplane surface in sublinear time.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graphically, it looks like this.",
                    "label": 0
                },
                {
                    "sent": "So you've got this large unlabeled data pool.",
                    "label": 1
                },
                {
                    "sent": "Then we'll design A hash function H that we can apply to all of those images.",
                    "label": 1
                },
                {
                    "sent": "And we'll use it to put all these examples in the hashtable index by these binary hash keys in such a way.",
                    "label": 1
                },
                {
                    "sent": "That when I also apply this hash function to the current classifier.",
                    "label": 0
                },
                {
                    "sent": "Go guarantee that with high probability.",
                    "label": 0
                },
                {
                    "sent": "The classifier itself W is going to match to those examples that are nearest to that current hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, so I will dare points have been hacked into this table and now we can assure is that as we hash with the with the classifier parameters were going to go directly to those points that are most uncertain.",
                    "label": 0
                },
                {
                    "sent": "So why is that a good thing?",
                    "label": 0
                },
                {
                    "sent": "Well now I only need to look at those examples.",
                    "label": 0
                },
                {
                    "sent": "And compute that selection function and I've got a guarantee on how likely those are to be the ones I would have selected.",
                    "label": 0
                },
                {
                    "sent": "Whereas to exhaustively scan through all of the data points.",
                    "label": 0
                },
                {
                    "sent": "So look at only those and then I can rate them and determine what my next active request should be.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of high level view of our approach an if you're familiar with hashing techniques in general.",
                    "label": 0
                },
                {
                    "sent": "The thing to note here that's distinct.",
                    "label": 0
                },
                {
                    "sent": "Instead of doing a similarity search with the hash function, where we would normally think of mapping points to points, here we have a hyperplane to point search.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Is my query itself is a hyperplane, and the things I'm retrieving are those points that are nearest to it in any direction.",
                    "label": 0
                },
                {
                    "sent": "Let me give you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little more intuition about how this could work.",
                    "label": 0
                },
                {
                    "sent": "That's basically what we what we want to design, and I'll show you how it can work.",
                    "label": 0
                },
                {
                    "sent": "So first quick background on an existing hashing function for locality sensitive hashing, it's quite intuitive, so the probability that a random hyperplane would separate two vectors depends on the angle between them.",
                    "label": 1
                },
                {
                    "sent": "So suppose you had two vectors XI, XJ.",
                    "label": 0
                },
                {
                    "sent": "If they have a small angle separating them, and then if I come in with some some random hyperplane part, the probability that hyperplane splits XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "Is smart.",
                    "label": 0
                },
                {
                    "sent": "High contrast effect scientist.",
                    "label": 0
                },
                {
                    "sent": "They are separated by a wider angle.",
                    "label": 0
                },
                {
                    "sent": "The probability that this random hyperplane splits them is higher.",
                    "label": 0
                },
                {
                    "sent": "And that's the basis for simple hash function, where if this hash function parameterized by random random vector R, we could assign a bit according to which side of the hyperplane it falls on.",
                    "label": 0
                },
                {
                    "sent": "You can imagine doing this now independently for any new X and what you'll get is that the probability that you collide mean that you get the same bit assigned for one XI and some other XJ is directly related to the angle between upside history.",
                    "label": 0
                },
                {
                    "sent": "So this is a known property that's been very useful for locality sensitive hashing.",
                    "label": 0
                },
                {
                    "sent": "RLSH an it allows you to do similarity search for or to guarantee that probability of collision is high for points that have small angles between them.",
                    "label": 0
                },
                {
                    "sent": "But in fact this is not what we need to solve the problem I just mentioned 'cause this is going to assure that you have high probability of collision for nearly parallel vectors.",
                    "label": 1
                },
                {
                    "sent": "Right smaller angle and in fact we need for our cases to have high probability of collision for perpendicular vectors.",
                    "label": 0
                },
                {
                    "sent": "So just to kind of illustrate why that's so, if you look at this sketch that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had before.",
                    "label": 0
                },
                {
                    "sent": "A good point to collide with my hyperplane W is 1 where it's about perpendicular to both the hyperplane normal W and negative value.",
                    "label": 0
                },
                {
                    "sent": "To say this is a good point to to get.",
                    "label": 0
                },
                {
                    "sent": "Whereas about point to Clyde would be one that's far from the hyperplane, and that would be 1 where both of these angles are not close to 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "So what we need is a hash function, then that will guarantee high probability of collision for vectors that are not permitted.",
                    "label": 0
                },
                {
                    "sent": "Let me show you, then building an intuition from before how we can do this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Imagine generating 2 two bit hash function using two random vectors U&V.",
                    "label": 0
                },
                {
                    "sent": "OK, the first one will constrain angle between your data plan X and this type of classifier W. The other one will constrain angle between X and negative W. And I will say that collision is likely only if neither of these vectors splits.",
                    "label": 1
                },
                {
                    "sent": "So let's look at the two extremes.",
                    "label": 0
                },
                {
                    "sent": "So I'm wondering if you have vectors that are nearly nearly parallel, then we want.",
                    "label": 0
                },
                {
                    "sent": "These not to collide in our our hashtable, so for the first hyperplane V, let's say it's unlikely to split these guys.",
                    "label": 0
                },
                {
                    "sent": "But when we look in the other pair were likely to split with the second hyperplane here.",
                    "label": 0
                },
                {
                    "sent": "So these would be likely not to collide in our hash table.",
                    "label": 0
                },
                {
                    "sent": "If you look at the other extreme, the one that we do want collisions for.",
                    "label": 0
                },
                {
                    "sent": "For perpendicular vectors, the chance of collision, or sorry, the chance of splitting for X&W is lower for the 1st first random hyperplane and also low for the second hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're unlikely to split, meaning we're likely to fall in the same bucket in hash table.",
                    "label": 0
                },
                {
                    "sent": "This is the definition between behind how we can generate the hash functions that are desired, and I'm the kind of nut shell hash.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Option is is written here, so we have a hash function that we call asymmetric, meaning it can be applied either through our data points X or to our classifier parameters.",
                    "label": 0
                },
                {
                    "sent": "W will treat them a little bit differently depending on which one it is, and it's all just based on these simple hash functions.",
                    "label": 0
                },
                {
                    "sent": "Using the the sign of the inner product between these random hyperplanes, UNB TANAR inputs A&B.",
                    "label": 0
                },
                {
                    "sent": "And we've proven the necessary balance to show that it is indeed locality sensitive for the the criterion of interest, which is the amount which the angle between the data point XW deviates from 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a way to do this caching that I described at the onset and what this means is now we can take our classifier as a ever evolving query right?",
                    "label": 0
                },
                {
                    "sent": "And this is what happens in active learning class work keeps evolving as I get new label data, so we can imagine in one iteration perhaps I hash to this first set of points near the current decision boundary.",
                    "label": 0
                },
                {
                    "sent": "I get them labeled by a human and now I can refine the classifier hash again with that classifier and get a new set of data points to label and so on.",
                    "label": 0
                },
                {
                    "sent": "So what kind of key to this design is that the thing that stationary or data points in the hashtable?",
                    "label": 0
                },
                {
                    "sent": "So the one time procedure to get them all hash and the thing that keeps changing is the classifier.",
                    "label": 0
                },
                {
                    "sent": "And that's one hash in to then figure out which points we should get labeled next.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Illustrated results so first just a qualitative one.",
                    "label": 0
                },
                {
                    "sent": "We've deployed this approach to do some large scale active learning here with million examples from a tiny image data collection, and in fact has not been fully human labeled really due to its scale.",
                    "label": 0
                },
                {
                    "sent": "This is great testbed for this active learning approach.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here at first 9 images that this is someone request labels for when learning either that category airplane or the category automobile, and it's kind of neat to see you know, for example, on the airplane side, these are things that maybe look.",
                    "label": 0
                },
                {
                    "sent": "Nearly like airplanes or confusing airplanes that the system is first going to ask a human about an was excited to get this skill of result.",
                    "label": 0
                },
                {
                    "sent": "It's really as I said, these are unlabeled data within the tiny image collection, and there's over 1000 categories present within them.",
                    "label": 0
                },
                {
                    "sent": "So all those thousands of possible categories it's really singling out.",
                    "label": 0
                },
                {
                    "sent": "Those that are most images that are most useful to learn about airplane or automobile.",
                    "label": 0
                },
                {
                    "sent": "OK, now looking.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Typically at this result, let's see what we're getting.",
                    "label": 0
                },
                {
                    "sent": "So here is a learning curve.",
                    "label": 0
                },
                {
                    "sent": "Same format as the kind of curves I showed before.",
                    "label": 0
                },
                {
                    "sent": "So as you use more iterations of learning, meaning more labels added, how good is the prediction accuracy?",
                    "label": 0
                },
                {
                    "sent": "And here's the passive learning curve.",
                    "label": 0
                },
                {
                    "sent": "Things get better as you add more labels, but it's a bit slow in progress.",
                    "label": 0
                },
                {
                    "sent": "Now, if you were to do exhaustive active learning using this simple margin criterion, you'll get a curve that looks much better.",
                    "label": 0
                },
                {
                    "sent": "And if you were to approximate that exhaustive selection using the approach I just introduced, you'll get a curve that's quite close.",
                    "label": 0
                },
                {
                    "sent": "However, you're getting that results are as in blue there with orders of magnitude less computational time.",
                    "label": 0
                },
                {
                    "sent": "And that's what's showing these box plots on a large scale.",
                    "label": 0
                },
                {
                    "sent": "Now the key thing is if I put both these results together so by account, not just for the human labeling time as it's gone up top here, but also the selection time, meaning how much computation I do to make my choice, then I get learning curves that look like this.",
                    "label": 0
                },
                {
                    "sent": "So here's what happened.",
                    "label": 0
                },
                {
                    "sent": "Now we're counting for both selection and labeling time.",
                    "label": 1
                },
                {
                    "sent": "An interesting Lee and a passive learning.",
                    "label": 0
                },
                {
                    "sent": "An exhaustive active learning are basically awash right, so the one is wasting time figuring out which you know.",
                    "label": 0
                },
                {
                    "sent": "Past learning is wasting time getting irrelevant examples labeled, whereas my exhaustive active learner is wasting time figuring out which requests to make threats, and we get that kind of the advantage over both if we take our hyperplane hashing approach to do active selection very efficient and it is the key result for this.",
                    "label": 0
                },
                {
                    "sent": "For this line of work and really the 1st result of its kind to account for the real human effort as well as the real machine effort to do at large collective learning system.",
                    "label": 0
                },
                {
                    "sent": "So now let's",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The time we make a lot of active learning, so I kind of showed you would like to break free from this kind of data set based learning and put active learning out in a real live deployed system.",
                    "label": 0
                },
                {
                    "sent": "So to do that we're going to let our system figure out which images might even be relevant so we won't feed them to the system.",
                    "label": 0
                },
                {
                    "sent": "System is going to call in this case we use Flickr for potentially relevant images.",
                    "label": 0
                },
                {
                    "sent": "So here I say we learn about bicycle gets on Flickr, find some images that may contain bicycle.",
                    "label": 0
                },
                {
                    "sent": "And then we'll do some candidate generation to think about where the bicycle might be in each image.",
                    "label": 0
                },
                {
                    "sent": "So now we have our unlabeled collection of of potential bicycle windows.",
                    "label": 0
                },
                {
                    "sent": "And now consider applying the hashing approach I defined earlier.",
                    "label": 0
                },
                {
                    "sent": "Get all your data hash in this table.",
                    "label": 0
                },
                {
                    "sent": "Now we can come in with our current classifier for bicycle versus non bicycle an in sublinear time figure out which requests should be made next.",
                    "label": 0
                },
                {
                    "sent": "So that needs to be the request that we make, in our case to two on line annotators.",
                    "label": 0
                },
                {
                    "sent": "So we're using a service called Mechanical turn.",
                    "label": 0
                },
                {
                    "sent": "This allows us to push label request.",
                    "label": 0
                },
                {
                    "sent": "Too many annotators in parallel.",
                    "label": 0
                },
                {
                    "sent": "We pay them for their work, it's mixing quality, so we use some automatic consensus forming.",
                    "label": 0
                },
                {
                    "sent": "We use a mean shift approach to kind of cluster the bounding boxes that they give us in return fully automatic, and then this will all men are labeled data and we can continue this process in a loop like we saw before.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our full live active learning system.",
                    "label": 1
                },
                {
                    "sent": "The thing I wanted to do noise here.",
                    "label": 0
                },
                {
                    "sent": "Well, there's two parts, so one we had nothing to do with which image is going to the unlabeled pool, right?",
                    "label": 0
                },
                {
                    "sent": "And this is in contrast to all the previous results that I was alluding to before.",
                    "label": 0
                },
                {
                    "sent": "This is completely autonomous way to deploy this active learning approach and we're doing this on a scale where this unlabeled pool consists of millions of examples.",
                    "label": 0
                },
                {
                    "sent": "And doing so is only possible because of the scalability of the approach that we defined.",
                    "label": 1
                },
                {
                    "sent": "In fact, to run this in a naive way would have taken 60 hours per iteration.",
                    "label": 0
                },
                {
                    "sent": "Have you done?",
                    "label": 1
                },
                {
                    "sent": "Had we done that with existing exhaustive search?",
                    "label": 0
                },
                {
                    "sent": "But it takes like 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "Using your approach I just defined.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so when you do this live active learning when you get here are some some more learning curves.",
                    "label": 1
                },
                {
                    "sent": "The format you know you know high and steep early on is the best and I'm showing you is our results in blue doing this live active learning approach from crawled Flickr images and pass it baselines in red and black.",
                    "label": 0
                },
                {
                    "sent": "And so these two baseline basically are doing what we would normally do in data set collection today which is keyword search and then manually prune right and the red one is putting them at the image level.",
                    "label": 0
                },
                {
                    "sent": "The Black One is putting them at the.",
                    "label": 0
                },
                {
                    "sent": "Bounding box window level.",
                    "label": 0
                },
                {
                    "sent": "So it was really exciting to me about these results.",
                    "label": 0
                },
                {
                    "sent": "Is here with a completely hands off and autonomous system.",
                    "label": 0
                },
                {
                    "sent": "We're seeing learning curves that are improved by this large scale active learning approach, and that's improved beyond what I refer to as the status quo collection.",
                    "label": 0
                },
                {
                    "sent": "This kind of keyword search and then randomly or passively print out.",
                    "label": 0
                },
                {
                    "sent": "Always work and you can see it clear.",
                    "label": 0
                },
                {
                    "sent": "Clear failure case for the chair class where the learning curves for the blue are coming up behind that just the keyword search approach.",
                    "label": 0
                },
                {
                    "sent": "Trying to get an understanding The thing is and always sticks categories.",
                    "label": 0
                },
                {
                    "sent": "The classes are quite difficult.",
                    "label": 0
                },
                {
                    "sent": "We pick them intentionally because of their difficulty for existing detectors, but in some sense perhaps chair is one of the most difficult or the most varied among these images.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what kind of things do we pick here?",
                    "label": 0
                },
                {
                    "sent": "Again, it said that the subwindow level within the images 'cause we're not gonna text are here.",
                    "label": 0
                },
                {
                    "sent": "When learning about about boats, the system would like to know about the labels for these guys on top first.",
                    "label": 1
                },
                {
                    "sent": "Whereas the passive approach might pick these images first in random order and kind of looks.",
                    "label": 0
                },
                {
                    "sent": "What's useful to note here is not that these images that the passive learner selects, don't contain both.",
                    "label": 0
                },
                {
                    "sent": "However, they seem to contain images of quotes that are either of the kind, not useful for learning the boat detector.",
                    "label": 0
                },
                {
                    "sent": "After this passes or you know, correcting an unusual view or both related things.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So final result, I wanted to show you using this kind of large scale active learning system was our biggest challenge to ourselves, which is can we take this approach and actually improve performance on what is a very very well studied object potential benchmark.",
                    "label": 0
                },
                {
                    "sent": "And this is the Pascal VLC data set which some of you may be familiar with.",
                    "label": 0
                },
                {
                    "sent": "Basically it has images originate Ng from Flickr and this is sort of the benchmark for object detection.",
                    "label": 1
                },
                {
                    "sent": "Much effort, dozens of teams compete on this every year to see how well can you do with state of the art detection techniques.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our role here is to see if all if we took our light active learner and let it try to improve Pascal performance.",
                    "label": 0
                },
                {
                    "sent": "How would we do?",
                    "label": 0
                },
                {
                    "sent": "And then we looked at the six again, the six most challenging classes in that data set.",
                    "label": 0
                },
                {
                    "sent": "We are very encouraged to find out the live active learner getting its own knew crawl data can in fact improve over with what are many well studied and well engineered techniques.",
                    "label": 0
                },
                {
                    "sent": "Meaning you know, the best of the state of the art as shown in this table.",
                    "label": 0
                },
                {
                    "sent": "And in fact we can only do this because the approach we define is so scalable.",
                    "label": 0
                },
                {
                    "sent": "So this is just showing kind of a breakdown at the kind of cost you could expect if you were to use existing strong detectors with straightforward active learning paradigms versus our approach and and what we see is we can do things on order minutes that would have taken days even for existing approaches.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so so far what I've shown you is how we can let our computer vision system figure out that data that's most important to for a human to look at.",
                    "label": 1
                },
                {
                    "sent": "In this case, a human annotator who's trying to train a recognition system.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What's exciting opportunity about these results is that we're trying to break free from purely data set based learning, so get out of that sandbox and let US system learn autonomously enough, at least in a Fuller extent.",
                    "label": 0
                },
                {
                    "sent": "And what our results are showing is there is possibility of improving upon our status quo for data set collection using this kind of approach.",
                    "label": 1
                },
                {
                    "sent": "On the technical side of main contribution, is this hyperplane hashing idea, which I which I briefly described, and then these do results that are really on a scale we haven't seen before in the active learning literature.",
                    "label": 1
                },
                {
                    "sent": "Specifically for computer vision.",
                    "label": 0
                },
                {
                    "sent": "And that's with the data pools that are about millions or contain millions of examples.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's look at the other side of the second context that I mentioned at the onset, and we're changing from supervised up to category detector training to unsupervised video summarization.",
                    "label": 1
                },
                {
                    "sent": "This is some very recent work that we've been looking at about the past year and I'm very excited about this new area.",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I've got a long video, and in fact we're specifically looking at egocentric video.",
                    "label": 1
                },
                {
                    "sent": "And like I said check, I don't mean you know self centered.",
                    "label": 0
                },
                {
                    "sent": "Well I do, but not in that sense.",
                    "label": 0
                },
                {
                    "sent": "I mean self centered where the camera is a wearable camera.",
                    "label": 1
                },
                {
                    "sent": "OK, so someone wearing something camera is about at eye level and we're just capturing the camera where we stay.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might get something that looks like this video here.",
                    "label": 0
                },
                {
                    "sent": "It'll be hours in length easily, and our goal is to summarize it into a storyboard.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to take that long video and come up with automatically a storyboard that shows the most important events, objects in people that happen throughout the day.",
                    "label": 0
                },
                {
                    "sent": "So this would be great, because now we're going from a very long visual input is something that a human can look at almost in a glance and get the main idea.",
                    "label": 0
                },
                {
                    "sent": "So again, another example, we're letting the system figuring out, figure out what should a human look at.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why would we specifically want egocentric video summarization?",
                    "label": 1
                },
                {
                    "sent": "The good thing is wearable cameras have come along way, so in 20 or more years you know it's no longer the specialty of, say, lifelogger, who might wear contraption like this.",
                    "label": 0
                },
                {
                    "sent": "Back in the day.",
                    "label": 0
                },
                {
                    "sent": "But now we've got these nice lightweight cameras and infect people are wearing in certain settings and I think could could get more value out of if we could provide automatic summarization.",
                    "label": 0
                },
                {
                    "sent": "For example, imagine in law enforcement we have police officers who are such cameras with computer vision to do the summarization we could have.",
                    "label": 0
                },
                {
                    "sent": "Days worth of activity readily summarize for their later coming, 'cause also applications that I'm interested in an in health care and elder care.",
                    "label": 0
                },
                {
                    "sent": "So there are existing studies showing that for someone with memory loss, just looking back at wearable camera capture can help improve recall, and that's even without vision in the loop to do some automated summarization.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm especially interested in this egocentric view, 'cause I think it's most likely to transfer well to save it to to mobile robot applications.",
                    "label": 0
                },
                {
                    "sent": "So no longer thinking of just static cameras observing a scene that we want to summarize is more standard and sort of surveillance kind of applications.",
                    "label": 0
                },
                {
                    "sent": "But now we have an agent moving around the world and we want to summarize with, but that agencies.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine a robot that's exploring on a new territory or new environment and then goes back to the human.",
                    "label": 0
                },
                {
                    "sent": "A summary of what it saw.",
                    "label": 0
                },
                {
                    "sent": "Alright, so This is why.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't want to do this.",
                    "label": 0
                },
                {
                    "sent": "Summarization existing techniques would not be well suited for the egocentric view, and that's because generally they need to assume a static camera, which we don't have with this wearable camera, and they're often relying on fairly low level cues.",
                    "label": 1
                },
                {
                    "sent": "OK, we'll come back to this and the cubicle imitation really is that they're not concerned about.",
                    "label": 0
                },
                {
                    "sent": "What makes the story of the video?",
                    "label": 1
                },
                {
                    "sent": "So our goal will be to produce summary that actually pays attention to what are the important objects and people that tell the story of the long running video.",
                    "label": 0
                },
                {
                    "sent": "And here it's important, has a very specific meaning.",
                    "label": 0
                },
                {
                    "sent": "Cousin, the egocentric view importance, refers to those things with which the camera where has significant interaction.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our idea is to develop a set of new cues that would possibly reflect importance in the egocentric setting, and in particular we make these.",
                    "label": 0
                },
                {
                    "sent": "We would like these cues to be category independent.",
                    "label": 0
                },
                {
                    "sent": "By that I mean we want to be able to predict importance with without sensitivity to who the camera where is or what things they are saying.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not that we want to learn that, say, a fire truck is always important, but we want to learn what properties about the fire truck in the image are important.",
                    "label": 0
                },
                {
                    "sent": "If in the given context, if they are so.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the basic goal.",
                    "label": 0
                },
                {
                    "sent": "Create a summary that pays attention to the important people in objects and let me first give you an outline of the pipeline and I'll fill in a few details and then look at some results.",
                    "label": 0
                },
                {
                    "sent": "Is the first thing we'll do in the approaches to learn about what's important, and to do this will need some training data and we'll get we'll do a collection again.",
                    "label": 0
                },
                {
                    "sent": "We use some crowdsourcing resources to come up with some annotations about what's important in video.",
                    "label": 0
                },
                {
                    "sent": "Then we design the features that might indicate importance and will train an important detector.",
                    "label": 0
                },
                {
                    "sent": "So a function that could account for the given region is in the video.",
                    "label": 0
                },
                {
                    "sent": "This kind of offline learning then when you get a new video will first partition it into temporal events.",
                    "label": 1
                },
                {
                    "sent": "Breach event will discover which things are important and within it.",
                    "label": 0
                },
                {
                    "sent": "And finally will generate a storyboard.",
                    "label": 0
                },
                {
                    "sent": "A compact story book summary that have slates the main people and objects.",
                    "label": 1
                },
                {
                    "sent": "This is the main data flow from top to bottom and I need to tell you a little bit more about each of these steps.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we get the data collection we use this camera that's depicted.",
                    "label": 0
                },
                {
                    "sent": "Here is the look see camera it captures at the frame rate resolution you see here.",
                    "label": 0
                },
                {
                    "sent": "We got about 37 hours of content from initially from about four different camera.",
                    "label": 0
                },
                {
                    "sent": "Where's we tried to diversify our camera?",
                    "label": 0
                },
                {
                    "sent": "Where's we didn't want them to be?",
                    "label": 0
                },
                {
                    "sent": "Just ourselves.",
                    "label": 0
                },
                {
                    "sent": "We thought undergraduates, for example, might have more interesting lives than ourselves.",
                    "label": 0
                },
                {
                    "sent": "Maybe so.",
                    "label": 0
                },
                {
                    "sent": "And so at this point, when we did this study, these are the users we had and we specifically told the camera where is not to do anything special.",
                    "label": 0
                },
                {
                    "sent": "We didn't want staged activity, we just wanted to go about their day.",
                    "label": 0
                },
                {
                    "sent": "So the kind of data we get back or things like cooking, eating, shopping, working, that kind of activity.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's our data set and now to do the first stage of just learning about what's important.",
                    "label": 0
                },
                {
                    "sent": "We need to ask people to teach the system about about importance and of course important is really context dependent.",
                    "label": 0
                },
                {
                    "sent": "So we needed to do is show annotators and accelerated sub clip of video from the training side and then ask them to write down in the first stage.",
                    "label": 0
                },
                {
                    "sent": "What are the most important things.",
                    "label": 0
                },
                {
                    "sent": "So for this kind of clip they might run it down.",
                    "label": 0
                },
                {
                    "sent": "Things like the man in the blue shirt or the yellow notepad on table.",
                    "label": 1
                },
                {
                    "sent": "And then as a second task we add.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our annotators to take those textual descriptions and then annotate within sampled frames where those actual regions are.",
                    "label": 0
                },
                {
                    "sent": "So then I give us outlines like you should see here in green.",
                    "label": 0
                },
                {
                    "sent": "So why do we divide it out this way?",
                    "label": 0
                },
                {
                    "sent": "Well, we learned and developing this that it can bias the results if you just say label things that are important in this video.",
                    "label": 0
                },
                {
                    "sent": "For example, you might be inclined to label things that are just easy to outline versus those that are truly important.",
                    "label": 0
                },
                {
                    "sent": "So this helps us to to avoid that kind of bias.",
                    "label": 0
                },
                {
                    "sent": "As we can get our.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training data this way and then we want to learn region importance.",
                    "label": 1
                },
                {
                    "sent": "So then our task is to develop cues that might suggest important that we can measure from the data.",
                    "label": 0
                },
                {
                    "sent": "So first divide the images from our video frames into regions using automated segmentation techniques.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will develop these features that might reflect important.",
                    "label": 0
                },
                {
                    "sent": "So the first one tries to capture possible physical interaction between the camera where an objects in the world.",
                    "label": 0
                },
                {
                    "sent": "So this first Q looks at the distance to the hand.",
                    "label": 1
                },
                {
                    "sent": "So if you're looking at this green region here, we want to know then how far is it from the nearest hand.",
                    "label": 0
                },
                {
                    "sent": "If there is a hand in view.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, egocentric feature is meant to catch her coarsely.",
                    "label": 0
                },
                {
                    "sent": "The gaze of the camera.",
                    "label": 0
                },
                {
                    "sent": "Where so, of course, keep in mind this camera is rigidly attached to the person's head, and so if we look at the distance from the center of the frame, we're getting approximation of where the person was looking.",
                    "label": 0
                },
                {
                    "sent": "And finally we look at an egocentric you that catches frequencies.",
                    "label": 0
                },
                {
                    "sent": "We do some matching across a 10 minute window within the video clip an we can automatically then gauge how frequent that object was.",
                    "label": 0
                },
                {
                    "sent": "In.",
                    "label": 0
                },
                {
                    "sent": "This again, could be accused for important.",
                    "label": 0
                },
                {
                    "sent": "In addition to these egocentric use, we develop some object based features.",
                    "label": 0
                },
                {
                    "sent": "One we use an object like saliency, using existing techniques from endreson colleagues, and then for the emotion based agency that we developed in our group.",
                    "label": 0
                },
                {
                    "sent": "And I'm skipping details on these definition.",
                    "label": 0
                },
                {
                    "sent": "Basically, saliency of the region with respect to its surroundings.",
                    "label": 0
                },
                {
                    "sent": "And then we also with keep, take into account whether the region overlaps the face regions with the idea that people are generally wanting important players.",
                    "label": 0
                },
                {
                    "sent": "In that to be here in the story.",
                    "label": 0
                },
                {
                    "sent": "I found the capture.",
                    "label": 0
                },
                {
                    "sent": "Some region features things like size, location within the frame and this helps us capture a specific context.",
                    "label": 0
                },
                {
                    "sent": "Specific things like you know if I had lunch with someone there generally with certain scale or certain position within the frame.",
                    "label": 0
                },
                {
                    "sent": "Hey, I just want to stress that all of these cues are category independent, meaning we're learning properties that we think will be indicative of importance no matter the category of the object.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, we can take these kind of QS.",
                    "label": 0
                },
                {
                    "sent": "Employ your favorite learning technique here.",
                    "label": 0
                },
                {
                    "sent": "We start with a fairly simple regression pass, so now we're trying to gauge the importance of region R and will extract all those cues.",
                    "label": 0
                },
                {
                    "sent": "Letters defined on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "These X features for our regions are and.",
                    "label": 1
                },
                {
                    "sent": "We will learn a regressor to predict a function to predict how important is given.",
                    "label": 1
                },
                {
                    "sent": "The good thing about 9 degrees importance.",
                    "label": 0
                },
                {
                    "sent": "Then we can adjust the compactness of the summary accordingly.",
                    "label": 1
                },
                {
                    "sent": "I mean food pairwise interaction terms between the features we expect there will be some interesting relationships between them.",
                    "label": 0
                },
                {
                    "sent": "For example, maybe a region is only important if it's object like an.",
                    "label": 0
                },
                {
                    "sent": "Also a certain point with respect to the center of the frame.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're learning.",
                    "label": 0
                },
                {
                    "sent": "Then I requested that it will during training use the overlap with the ground truth region as the prediction variable.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've got a function that can predict how important our region is, so now we're ready to do a summary.",
                    "label": 0
                },
                {
                    "sent": "So we'll take a new video, and 1st, we'll divide it into the temporal events that we see here.",
                    "label": 0
                },
                {
                    "sent": "We use a fairly simple but effective approach to do the segmentation.",
                    "label": 0
                },
                {
                    "sent": "What I'm showing here is a distance matrix based on color histograms between all your frame.",
                    "label": 0
                },
                {
                    "sent": "So frame one through N from one through in and the block structure we see here is showing that there is some color similarity that emerges.",
                    "label": 0
                },
                {
                    "sent": "We as you shift from scene to scene and in fact we also put the strong guy here is because we have a waiting according to the temporal distance between the frames as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this allows us to get the main chunks that activity and also allows us to call an object important multiple times if it occurs in different contexts.",
                    "label": 0
                },
                {
                    "sent": "For example, if the dog is in the morning, the camera where it plays with the dog in the morning at home and then takes the dog for a walk in the park later, these could emerge as two different important instances within the entire summary.",
                    "label": 0
                },
                {
                    "sent": "Hey, once we started our video into events Now what we want to do is say for each event what are the important things that we encountered?",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's where we apply our our region inference predictions on all regions, and we might have something like hundreds of regions for every frame, so we'll score them by their imports will also do some automated matching to come up with representative regions for each such discovered object.",
                    "label": 0
                },
                {
                    "sent": "And finally we can.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our summer.",
                    "label": 0
                },
                {
                    "sent": "So this is an example summary where we've we've colored the frame groups by events that were detected.",
                    "label": 0
                },
                {
                    "sent": "So here we have a 1234 event summary and the frames that are shown are those that contain the most important things.",
                    "label": 0
                },
                {
                    "sent": "As our system predicted them to be in each event.",
                    "label": 0
                },
                {
                    "sent": "So we've gone from one video to short storyboard summary showing you exactly what the system thinks a human should bother looking at.",
                    "label": 0
                },
                {
                    "sent": "Understand the main idea.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at some of the results.",
                    "label": 0
                },
                {
                    "sent": "First, let's look in isolation at how that region importance prediction can do these on in this column on the left, here are the most important regions are method would find in these frames and they again these are the most important regions out of about 500 regions for every frame.",
                    "label": 0
                },
                {
                    "sent": "Give you some sense of comparison.",
                    "label": 0
                },
                {
                    "sent": "If I then look at very recent work, an object like region detection, Anna even a classic low level saliency method, these are the kind of results that you would get.",
                    "label": 0
                },
                {
                    "sent": "So in these good example success cases, we see that we're better able to focus on objects like regions that are also important to the story of the of the video.",
                    "label": 0
                },
                {
                    "sent": "Alright, some things, for example like level 70 operators.",
                    "label": 0
                },
                {
                    "sent": "Just going to look for more low level Qusayr bright spot that surrounded by dark spots or something like this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course it doesn't always work, and here are some failure cases using our region or important prediction.",
                    "label": 1
                },
                {
                    "sent": "Again, ours are on the left and green and what we kind of see happening here.",
                    "label": 0
                },
                {
                    "sent": "Some failure cases where very frequent thinkin can emerge is important, even though it's not necessarily so to the story, and we think that we employed some cues that might account for say, the same depth of these features.",
                    "label": 0
                },
                {
                    "sent": "This kind of cases might be improved.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we find all this.",
                    "label": 0
                },
                {
                    "sent": "Certainly you know, looking at all these compared comparing to these kind of low level and high level selling see approaches and we see that indeed by learning about what's important in the context of a story, we can get much stronger and more accurate results.",
                    "label": 0
                },
                {
                    "sent": "That's just one piece of it.",
                    "label": 0
                },
                {
                    "sent": "The final reporting piece is to look at the kind of summaries that were generated.",
                    "label": 0
                },
                {
                    "sent": "So let's look at an example here that's very illustrated on the left.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you a 3 hour video.",
                    "label": 0
                },
                {
                    "sent": "Not really, will look at a look at exit of course, so here is some some sample from what was a 3 hour video of someone's day.",
                    "label": 0
                },
                {
                    "sent": "And you can see kind of.",
                    "label": 0
                },
                {
                    "sent": "But the activity might be worse.",
                    "label": 0
                },
                {
                    "sent": "We can only see it very slowly and that's the whole point.",
                    "label": 0
                },
                {
                    "sent": "So now if you look at the right I'm about to play our storyboard summary just into video form.",
                    "label": 0
                },
                {
                    "sent": "So we'll look at the storyboard frame after frame in sequence on the right hand side, and I'm showing you is a simple 12 frame summary of what was originally 3 hours of content, right?",
                    "label": 0
                },
                {
                    "sent": "And so your job looking at this summary is to decide what's the story of this person's day.",
                    "label": 0
                },
                {
                    "sent": "Let's see what they did.",
                    "label": 0
                },
                {
                    "sent": "And the colors are seeing are the events that we detected.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was 12 frames, very fast to watch, right?",
                    "label": 0
                },
                {
                    "sent": "And I think you got an idea of what happened.",
                    "label": 0
                },
                {
                    "sent": "Perhaps there was a drive to the store, some shopping for food.",
                    "label": 0
                },
                {
                    "sent": "Looking at the recipe on an iPad, doing some cooking, doing some cleaning, eating and watching TV, something like this, right?",
                    "label": 0
                },
                {
                    "sent": "And all this we can get it automatically and you know, it's basically the system allows us to focus on what's really important to the story.",
                    "label": 0
                },
                {
                    "sent": "A very good example of what's possible with this kind of approach.",
                    "label": 0
                },
                {
                    "sent": "Of course, we want to know what would happen if we did something much simpler than the pipeline that I just defined.",
                    "label": 0
                },
                {
                    "sent": "So let's look at a couple baseline techniques below.",
                    "label": 0
                },
                {
                    "sent": "Just send a second here.",
                    "label": 0
                },
                {
                    "sent": "Now the first baseline will be.",
                    "label": 0
                },
                {
                    "sent": "If I were just U uniform keyframe sampling.",
                    "label": 0
                },
                {
                    "sent": "So imagine over the course of the three hour video, if I just sampled frames, surely I'll hit some of the things that were primarily present.",
                    "label": 0
                },
                {
                    "sent": "And so that's what we'll look at first soon, as this agrees to move forward for me.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here on the left here the uniform key key frame sampling approach again just a 12 frame summary.",
                    "label": 0
                },
                {
                    "sent": "And here's what we would get.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that certainly we can.",
                    "label": 0
                },
                {
                    "sent": "We can hit on things that show what happened, but you also see a lot of some redundancy and also some irrelevant frames.",
                    "label": 0
                },
                {
                    "sent": "There's nothing stopping us from grabbing A-frame or the persons swinging their head from side to side looking around the kitchen, say.",
                    "label": 0
                },
                {
                    "sent": "Now on the right, this is a existing summarization technique that would optimize for the diversity between the frames, which is can be a very natural thing to do for a good sign.",
                    "label": 0
                },
                {
                    "sent": "So here is a summary, that's.",
                    "label": 0
                },
                {
                    "sent": "Maximizing the diversity present within the frames that are selected.",
                    "label": 0
                },
                {
                    "sent": "You can kind of see that in the chain of frames that move from darker one sweater ones with different content.",
                    "label": 0
                },
                {
                    "sent": "But I hope you see in terms of the summary.",
                    "label": 0
                },
                {
                    "sent": "I first showed you using our approach to these kind of baseline techniques is that we're really now better able to focus on the curtains and the eccentric sense things, objects and people in order to make a story out of this video.",
                    "label": 0
                },
                {
                    "sent": "And we have quantified this kind of.",
                    "label": 0
                },
                {
                    "sent": "Behavior in terms of counting how many important things?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we capture within a summary given land on the skip?",
                    "label": 0
                },
                {
                    "sent": "Some of the detail here and basically we can see a strong advantage to using this important prediction to form these summary.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also we've come back to our cameras to see, you know, perhaps most importantly, how's the perceived quality of these summaries.",
                    "label": 0
                },
                {
                    "sent": "So specifically what we did is ask instead of blind test we asked the camera where is to watch the baseline summary Anar summary.",
                    "label": 0
                },
                {
                    "sent": "Not saying which is which and then answer these two questions you see here an over all about 6070% of the time.",
                    "label": 0
                },
                {
                    "sent": "They're finding our summaries to be more effective.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other cases, so in this one case there was some consistency among the judges about our approach, working more poorly.",
                    "label": 0
                },
                {
                    "sent": "When we looked at it, we realized, well, OK, so the the baseline is doing just as well.",
                    "label": 0
                },
                {
                    "sent": "This uniform sampling baseline is doing just as well for this video where the person was on their laptop all day long.",
                    "label": 0
                },
                {
                    "sent": "OK, so the person was on her laptop at home.",
                    "label": 0
                },
                {
                    "sent": "The person is at laptop in class.",
                    "label": 0
                },
                {
                    "sent": "The personal laptop in the lab, right?",
                    "label": 0
                },
                {
                    "sent": "So you can do just as well if you sample frames with the laptop across the day.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think for me was this is video captured by a student was very slow to respond to my email so I didn't understand this.",
                    "label": 0
                },
                {
                    "sent": "After seeing kind of the story of the day, push amounts to being on the laptop.",
                    "label": 0
                },
                {
                    "sent": "But you can see that you know we need some diversity or activity to to have benefit from this approach.",
                    "label": 0
                },
                {
                    "sent": "And finally, we're also interested in and not just the temporal context of these stories, but also the spatial contexts.",
                    "label": 0
                },
                {
                    "sent": "So in some of our data collection we had our camera.",
                    "label": 0
                },
                {
                    "sent": "Where is Carrie GPS receiver so that we could make a storyboard that also shows the locations of the events and so here is our storyboard, but now plotted over the trajectory of the persons day.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine making this kind of summary that allows you to glance back at your history, maybe your own or some some social version of such a history to see what were the important things that were encountered.",
                    "label": 0
                },
                {
                    "sent": "Throughout the entire day.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I have shown you two ways in which we can learn to focus human attention on the most useful visual data and even by distinct settings.",
                    "label": 1
                },
                {
                    "sent": "One for training objects, vectors and two for doing video summarization.",
                    "label": 0
                },
                {
                    "sent": "And I think together at least this kind of line of work is quite interesting, because it allows us to rethink purely automating computer vision processing and instead think of semi automating it.",
                    "label": 0
                },
                {
                    "sent": "So bringing humans into loop in a way that's most effective, and in doing so, I think we'll discover more and more new applications for large scale visual analysis that can benefit from human intervention.",
                    "label": 0
                },
                {
                    "sent": "Not not complete autonomy.",
                    "label": 0
                },
                {
                    "sent": "Amazon, I'll conclude, and I'd be glad to take any questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "When you ask the subject themselves.",
                    "label": 0
                },
                {
                    "sent": "Compare.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if there are two aspects.",
                    "label": 0
                },
                {
                    "sent": "So how much is enough like he's 12 crimes in a 420 or 100?",
                    "label": 0
                },
                {
                    "sent": "Person.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the first question in terms of how do you know how many frames are needed to make a good story board is a very important one, and and we've kind of approached the summary, we approach it as if you have a budget, right?",
                    "label": 0
                },
                {
                    "sent": "So if your budget is K friends, then what's the best summer you can do?",
                    "label": 0
                },
                {
                    "sent": "And in fact I didn't go into these these very until we have one right where we would optimize in terms of capturing the most importance within.",
                    "label": 0
                },
                {
                    "sent": "That budget or another word?",
                    "label": 0
                },
                {
                    "sent": "Sorry, having a criterion on the level of importance we need to have to make this memory and another version where we actually tried to to optimize towards the budget of frames that you have.",
                    "label": 0
                },
                {
                    "sent": "So so in our context, I think it's reasonable to assume that you're given a budget of length.",
                    "label": 0
                },
                {
                    "sent": "Another thing you've got this on video.",
                    "label": 0
                },
                {
                    "sent": "I'm willing to watch it for this many seconds or this many minutes, so that would be your target for the summary.",
                    "label": 0
                },
                {
                    "sent": "Like, but certainly there's more.",
                    "label": 0
                },
                {
                    "sent": "There's more that you can think about doing in this regard.",
                    "label": 0
                },
                {
                    "sent": "And then the second question about evaluation.",
                    "label": 0
                },
                {
                    "sent": "So you're right.",
                    "label": 0
                },
                {
                    "sent": "So the study I showed or briefly alluded to here, we ask cameras.",
                    "label": 0
                },
                {
                    "sent": "We've also gone to other users as well.",
                    "label": 0
                },
                {
                    "sent": "The way we did this is we first had the camera wears, make a textual summary of what they did.",
                    "label": 0
                },
                {
                    "sent": "Then we had our other group of users watch the videos, read the textual once, and then make judgments about which was better capturing.",
                    "label": 0
                },
                {
                    "sent": "And then kind of more generally, what we're realizing here and what your question points to its right.",
                    "label": 0
                },
                {
                    "sent": "It can be very difficult to evaluate something like unsupervised video summarization, and it's very important how the user perceives the result.",
                    "label": 0
                },
                {
                    "sent": "So this is one way we felt we could quantify.",
                    "label": 0
                },
                {
                    "sent": "Both using the expertise of the camera, where who really saw the full digital content, 'cause we can't make our new users watch while we could, but we can't pay $12.00 a video in order to do this evaluation, so this is the way we've come up with so far to do that.",
                    "label": 0
                },
                {
                    "sent": "Do you take it up?",
                    "label": 0
                },
                {
                    "sent": "So you said that hashing is fun too.",
                    "label": 0
                },
                {
                    "sent": "Switch my cash to the same location.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so keep in mind that in this kind of passion framework collision is the goal is to have collision be proportional to similarity distance.",
                    "label": 0
                },
                {
                    "sent": "So in General, Kelly sensitive hashing is based on this premise that things that are similar should collide, and that's a good thing for search, because now you go directly to those things that are most similar.",
                    "label": 0
                },
                {
                    "sent": "What's the twist here in our setting?",
                    "label": 0
                },
                {
                    "sent": "Is that the collision should be happening for not similar points, but points that are close to a hyperplane?",
                    "label": 0
                },
                {
                    "sent": "Does that clarify a bit?",
                    "label": 0
                },
                {
                    "sent": "Yeah, very interesting.",
                    "label": 0
                },
                {
                    "sent": "So I'm wondering about the important issue.",
                    "label": 0
                },
                {
                    "sent": "In some ways, what's most important is what's unexpected or uncommon.",
                    "label": 0
                },
                {
                    "sent": "And so when you talk about importance, I think that you're automatically assigning a value system and you have subjective POV less important.",
                    "label": 0
                },
                {
                    "sent": "So my question is whether you can really do this without any context.",
                    "label": 0
                },
                {
                    "sent": "And if you can't, which I think you can't, then what you're doing is actually just maybe summarizing, trying to include as much as we can in that summary that you happily making any important judgments.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you do this without context.",
                    "label": 0
                },
                {
                    "sent": "So important is a dangerous word because it's very loaded, right?",
                    "label": 0
                },
                {
                    "sent": "I hopefully mostly used in quotes for this reason, but but in our setting, what we define importance to be is what is central to the telling of the story of the video, and this is exactly what we instructed.",
                    "label": 0
                },
                {
                    "sent": "Our laborers are annotators to keep in mind as they told us what this import.",
                    "label": 0
                },
                {
                    "sent": "So if we just said what's important, and that could mean a dozen more things, but ours means what allows you to depict the full story.",
                    "label": 0
                },
                {
                    "sent": "And that's also why when we showed our annotators the video, we didn't give them up.",
                    "label": 0
                },
                {
                    "sent": "Trying to give them the context of I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe about 5 minutes of contents to to make their judgment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think importance and in the most abstract sense we're not going to learn in the way here, but important in terms of category independent cues for the egocentric setting, which also gives us very specific constraints, right?",
                    "label": 0
                },
                {
                    "sent": "That allows us to exploit things like centrality in the framework nearest hands.",
                    "label": 0
                },
                {
                    "sent": "I think this is possible to capture and learn as the results are showing.",
                    "label": 0
                },
                {
                    "sent": "I just want to pull up from the comment around market be done without context.",
                    "label": 0
                },
                {
                    "sent": "I think my comment on that Friday to my question is the fact that I need extremely important and visually important.",
                    "label": 0
                },
                {
                    "sent": "Adding is made of the different thing and I think very interesting.",
                    "label": 0
                },
                {
                    "sent": "If you were in the sense that I think is very much visually version but also think my question was so.",
                    "label": 0
                },
                {
                    "sent": "This car fication you got someone else actually entertained another person's life in the sense that what they saw?",
                    "label": 0
                },
                {
                    "sent": "When someone else, not the actual person himself.",
                    "label": 0
                },
                {
                    "sent": "So I wonder what is the impact of getting someone else to actually summarize innocence when they let someone else is kinda like confusing there, right?",
                    "label": 0
                },
                {
                    "sent": "Yes great, great .2.",
                    "label": 0
                },
                {
                    "sent": "So you're right, we have people who did not wear the camera as our annotators to judge what was important.",
                    "label": 0
                },
                {
                    "sent": "This is intentional in our heart and that we were hoping to learn what I'm referring to is category independent queues.",
                    "label": 0
                },
                {
                    "sent": "Meaning I I'd like of summarization technique that is applicable generically to these videos, no matter who the camera where was.",
                    "label": 0
                },
                {
                    "sent": "I could think of Alternatively a setting where you actually do want to tailor your summary to the wear himself or herself, where it would be important to learn video secrecies that that have to do with that camera.",
                    "label": 0
                },
                {
                    "sent": "Where is life and there I think maybe some of these you could think of this category independent approaches to still a useful precursor to something that's more tailored.",
                    "label": 0
                },
                {
                    "sent": "To an individual, but this isn't something that we've explored.",
                    "label": 0
                },
                {
                    "sent": "So if there are any other questions, I'd like to the mighty join you.",
                    "label": 0
                },
                {
                    "sent": "Thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}