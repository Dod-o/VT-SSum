{
    "id": "p7lirmmwko2wevo5ntcpovfrcl4tmtzz",
    "title": "Leveraging User Libraries to Bootstrap Collaborative Filtering",
    "info": {
        "author": [
            "Laurent Charlin, Computer Science Department, Princeton University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_charlin_collaborative_filtering/",
    "segmentation": [
        [
            "Thank you, this is joint work with Richard Zemel at the University of Toronto and you go along with Shell at."
        ],
        [
            "University should work.",
            "The motivation here is quite simple.",
            "We as researchers are submerged with new papers.",
            "Conferences like KDD will have hundreds of new papers every year and there are several conferences a year which you may attend or which you may be interested in if you follow stuff online, archive.org will show you new interesting papers on a daily basis and so the question here is, how can we efficiently find all interesting papers that we may care about that we may want to dedicate time to read?",
            "This may not be a big surprise, but I would.",
            "I would say that one solution to this problem is to do recommendations specifically.",
            "In this talk, I'll sort of focus on document recommendations and.",
            "The running example uses US scientists or researchers as users and."
        ],
        [
            "So what we want to recommend this scientific articles.",
            "There are sort of lots of examples or that could be useful.",
            "For example, recommending papers to conference attendees.",
            "And we also have experiments on book on ibook data set.",
            "And you could imagine the same sort of model that I will apply here.",
            "Could be used in other domains such as in music and the main novelty that we're going to look at is we're going to try to leverage the libraries of users where libraries of scientists are basically your previously published papers, right?",
            "So how can we use your previously published papers as a source of week preferences or as implicit ratings indicative of your preferences?",
            "Again, another domains you could look at other things you could look for example that purchased items right?",
            "So just to make this very very clear and can."
        ],
        [
            "Here's sort of data.",
            "We're going to assume so.",
            "Previous model have assumed that you have users, and they've given you ratings for certain items in this paper, will assume that we actually have the content of the items, so we have the words in the documents that we want to recommend, and in addition, and this is the novelty here, we also have the user libraries.",
            "Each for each user we get a set of articles or books that he's published, or he's read in the past, and this is this is what's going to be.",
            "So this is a novelty in this paper."
        ],
        [
            "Some of the desert desert Dorado that we have is that we want to as soon as a user comes into our system we would like to be able to give him really quickly to be able to quickly provide good recommendations to that user.",
            "So that's sort of the we want to sort of solve that cold start case.",
            "But also we want to model more frequent or more heavy users, right?",
            "So as a user gives more user user, it gives more ratings.",
            "We also want to do really well in that case so.",
            "Now let me just take a step back from what we want and just to talk a little bit more about."
        ],
        [
            "Literature in this field, so you've got a great overview.",
            "In the first talk about some of this, I'll just go quickly through this.",
            "So there's this technique which has been very, very popular called collaborative filtering where the simple intuition is that great.",
            "Well, the simple intuition is that users with similar past preferences, or likely to have similar future preferences and one very useful model for this, has been proposed a couple of years ago is called matrix probabilistic matrix factorization.",
            "The way you can understand this if you've never seen this, is that.",
            "You learn a representation of users, so each user gets a representation.",
            "It's a vector in high dimensional space A and each item gets the you do the same thing for items and then you combine both.",
            "For example using a dot product to come up with ratings and you can see right away that the problem is if an user comes into the system and you have no ratings for that user, then there's no way you can learn this a variable, right?",
            "There's no way you can learn his latent preferences and so.",
            "And so you can't predict ratings for the new user on sort of the flip side of."
        ],
        [
            "Metal is what is.",
            "Then you could use site information where I define side information generally as being any information from users, an items excluding preferences, sort of common side information or common covariates that people have been using in the past include user demographics and item content, and of course the advantage of this is that even if the user is not provided you with any ratings, given some of this site information you could do, you could do good.",
            "You could do good recommendations so you could do predictions in cold start regimes using site information.",
            "And of course, the idea is the site information must be weekly or somewhat indicative of a user's preferences, right?",
            "So now."
        ],
        [
            "Onto sort of our model.",
            "So we proposed this graphical model.",
            "I just wanted to show you at least once the model and it's entirely what I'm going to do now is.",
            "I'm going to sort of dig into some two parts of the model to really try to give you intuitions about what's going on.",
            "The first thing I want to do is I want to show this thing, so this is just a representation of the data that we have in the task that we want to solve.",
            "So you could encode all the data that we have in three matrices.",
            "One matrix is a user by item matrix.",
            "Of reading, that's the one on top.",
            "And of course the task is that you want to be able to predict these question marks, right?",
            "So a user has rated some items, not all of 'em, and you would like to be able to recommend or to predict his ratings for items that he's not rated.",
            "Then on the bottom left we have another matrix which is a word by user matrix and basically each column is a user and the column represents how many words or a column represents the profile of the user library, right?",
            "So for example, the first user has in his library has word one, which is anywhere you can think of 1's and then he's got no other words and the second user has word two and word 3, two and four times, and we do exactly.",
            "We take the exactly same encoding.",
            "For the documents right?",
            "And So what we're going to do here is we have three matrices and at a high level.",
            "What we want to do is we want to find a joint factorization of these three matrices and then the idea is in this joint factorization is that the information from the bottom two matrices will be helpful to reconstruct or ratings matrix."
        ],
        [
            "OK, so I thought a high level now as we said this is document recommendations documents are easily and well modeled using topic models and so that's what we're going to do here.",
            "So this is the again, the graphical representation on the left side you have users on the right side.",
            "You have documents on the left side you see that each user's library is represented as a bag of words.",
            "So just you have the set of words in that user library and we did the same thing for documents.",
            "And what we learn here is a joint topic model or a twin topic model where the topics are shared across users and across documents and that allows us to have representations A&S so user and item representations in the same space, and so at a high level.",
            "If you're not exactly sure what topic models are, you can just think of them as a way of going from word space to some latent topic space, and so now what we have is each user is represented in this latent topic space.",
            "As topic proportions, right?",
            "So we would say for example, this user is interested in a math topic and a history topic and a lot less in this other third topic that we found and it is all learned or inferred automatically from data.",
            "OK, so now."
        ],
        [
            "We have these representations.",
            "How can we combine them to predict ratings?",
            "We combine them very easily by just saying we're going to match these representations, so we're going to match the reputation of documents S and we're going to depositions of users a using it again.",
            "A simple dot product.",
            "So we take a dot product between a an S and that yields are and there's a small twist to this, which is we take.",
            "Actually, we allow the model to learn weights on these topics and the idea here is simply that some topics may be more predictive of preferences, so you may want to wait them.",
            "Higher and some topics may be less productive preferences and so you'll want them too.",
            "To wait and lower sorry and so here what you can notice is this.",
            "This is going to be really useful for cold start problems.",
            "Because this really does not require any ratings data at this point, right?",
            "So as long as the user has a library and as long as an item has you have content of items, you can train this model and you can start recommending items to users.",
            "This is good, but as I said in the beginning, we also want to do really well in this other case where users give us lots of preferences and so we want sort of leverage these these preferences."
        ],
        [
            "And so the second part of the model does exactly this.",
            "The second part of the model simply says.",
            "Let's imagine that we have covert.",
            "Let's imagine that we have.",
            "Item features here we're simply going to learn.",
            "It's a linear regression model over these.",
            "These features of items, right?",
            "So what you think of it?",
            "You can think of it this way.",
            "You can think of gamma here.",
            "We have one diamond per user, and you can think of it as.",
            "Gamma will learn weights under topics of documents, right?",
            "So regardless of your user library using only ratings, the user, the model learns that user is really interested in math topics or history topics or some other topics that you may have interest in."
        ],
        [
            "So these are sort of the two parts of our model and we now combine them very easily.",
            "An additive fashion and the idea is that this parameterization allows the model to perform well in both cold start data regimes and warm started our non cold start.",
            "I guess data regimes and the model can also sort of learn to trade off smoothly one for the other across users and even as a user gets more data than model can learn to use one of these terms more than the other.",
            "And I'll show you an experiment where we show that see."
        ],
        [
            "Skinheads relationships to other models specifically?",
            "There are degeneracies of their model which corresponds to other tools which have been proposed in the literature.",
            "I won't go too much into how we do inference and learning, but just at a high level, this is a graphical model we.",
            "The problem is to infer the posteriors in this graphical model.",
            "The real posteriors are intractable, so we do variational inference.",
            "If you look at the model a little bit harder, you can see that it's actually got some non conjugacy, so there's sort of extra levels of approximations here, but in practice we get really good result."
        ],
        [
            "And there is lots of related work in this field.",
            "In general, there's lots of lots of work that tries to use ratings and something else to better predict ratings the most.",
            "The most sort of similar artists are the two at the bottom relational learning and collective matrix factorization, and regression latent factor models.",
            "The main difference between our work and theirs is that since we have word data, we can use a topic model to especially model the words and that actually end up doing a lot better than just.",
            "Join Factorizing 3 matrices."
        ],
        [
            "OK, so let me talk a little bit about experiments in the remaining time."
        ],
        [
            "So we have a few datasets that we've looked at.",
            "These datasets are so we have sort of two.",
            "Two different domains.",
            "We're looking at one of 'em are conference datasets where the users are reviewers and the task is to predict interesting papers to reviewer's, and we have a book data set from Kobo which is sort of an Amazon like company.",
            "Where here the idea is to predict to recommend books to users and I must say here that these are like fairly small datasets.",
            "I would agree with that.",
            "I think one problem is that we've just not been able to find any bigger data set, so if anybody knows of a data set we'd love to try this model and something bigger.",
            "So now let's look at some results before I look at sort of quantitive results."
        ],
        [
            "I show you some qualitative results.",
            "As I said here, we're learning this model and in this model each user is represented by the high dimensional vector A.",
            "What we can do is we can take, we can learn a model, we learn our model on a data set, and each user has a reputation in high dimensional space and we could project this this user.",
            "Into low dimensional space.",
            "Right, and this is basically what we've done here, and so we've projected this high dimensional vector into 2D, and I've sort of went in and hand labeled to different users, and these were reviewer's for the NIPS conference and you see NIPS traditionally been a lot about machine learning, but also about neuroscience, and this is what you recover here, and the model learns other different thing or other interesting things like there's this cluster of deep learning, people core, deep learning people, and then the people that do Bayesian, non parametrics and people that do graphical models sort of intersect."
        ],
        [
            "We also have lots of quantitative results against a bunch of other models I will skip."
        ],
        [
            "Not just for an interest of time."
        ],
        [
            "And I will just finish off.",
            "Basically by showing you this graph, which is saying that this is simulating an user coming into the system and so on.",
            "The left would have highlighted.",
            "Here is when an user comes in, we have no information whatsoever about that user.",
            "We really can't do all that well, but our model which is in blue here.",
            "As you give, as a user provides, is library, so is set of previously published papers or model can really leverage that and do a lot better and then slowly as you then add user preferences sort of are modeled.",
            "Sort of performance is increased slowly and other models which do not have access to user libraries then slowly catch up right.",
            "So we really see some of the advantage of using user libraries in the."
        ],
        [
            "Graph one last word to say that we've deployed the system as a recommendation system for NIPS reviewer's last year."
        ],
        [
            "And I'll finish off by just saying that user site information here.",
            "We've shown that it's been really useful because it can provide quickly good recommendations.",
            "It also does well and sort of more and more frequent or more heavy users, and there's lots of future work that we like to do, both only computational side of things, but also about from modeling more general covariance.",
            "That's it.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, this is joint work with Richard Zemel at the University of Toronto and you go along with Shell at.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "University should work.",
                    "label": 0
                },
                {
                    "sent": "The motivation here is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We as researchers are submerged with new papers.",
                    "label": 1
                },
                {
                    "sent": "Conferences like KDD will have hundreds of new papers every year and there are several conferences a year which you may attend or which you may be interested in if you follow stuff online, archive.org will show you new interesting papers on a daily basis and so the question here is, how can we efficiently find all interesting papers that we may care about that we may want to dedicate time to read?",
                    "label": 0
                },
                {
                    "sent": "This may not be a big surprise, but I would.",
                    "label": 0
                },
                {
                    "sent": "I would say that one solution to this problem is to do recommendations specifically.",
                    "label": 1
                },
                {
                    "sent": "In this talk, I'll sort of focus on document recommendations and.",
                    "label": 0
                },
                {
                    "sent": "The running example uses US scientists or researchers as users and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to recommend this scientific articles.",
                    "label": 0
                },
                {
                    "sent": "There are sort of lots of examples or that could be useful.",
                    "label": 0
                },
                {
                    "sent": "For example, recommending papers to conference attendees.",
                    "label": 1
                },
                {
                    "sent": "And we also have experiments on book on ibook data set.",
                    "label": 0
                },
                {
                    "sent": "And you could imagine the same sort of model that I will apply here.",
                    "label": 0
                },
                {
                    "sent": "Could be used in other domains such as in music and the main novelty that we're going to look at is we're going to try to leverage the libraries of users where libraries of scientists are basically your previously published papers, right?",
                    "label": 0
                },
                {
                    "sent": "So how can we use your previously published papers as a source of week preferences or as implicit ratings indicative of your preferences?",
                    "label": 0
                },
                {
                    "sent": "Again, another domains you could look at other things you could look for example that purchased items right?",
                    "label": 0
                },
                {
                    "sent": "So just to make this very very clear and can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's sort of data.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume so.",
                    "label": 0
                },
                {
                    "sent": "Previous model have assumed that you have users, and they've given you ratings for certain items in this paper, will assume that we actually have the content of the items, so we have the words in the documents that we want to recommend, and in addition, and this is the novelty here, we also have the user libraries.",
                    "label": 0
                },
                {
                    "sent": "Each for each user we get a set of articles or books that he's published, or he's read in the past, and this is this is what's going to be.",
                    "label": 0
                },
                {
                    "sent": "So this is a novelty in this paper.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the desert desert Dorado that we have is that we want to as soon as a user comes into our system we would like to be able to give him really quickly to be able to quickly provide good recommendations to that user.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the we want to sort of solve that cold start case.",
                    "label": 0
                },
                {
                    "sent": "But also we want to model more frequent or more heavy users, right?",
                    "label": 0
                },
                {
                    "sent": "So as a user gives more user user, it gives more ratings.",
                    "label": 0
                },
                {
                    "sent": "We also want to do really well in that case so.",
                    "label": 0
                },
                {
                    "sent": "Now let me just take a step back from what we want and just to talk a little bit more about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Literature in this field, so you've got a great overview.",
                    "label": 0
                },
                {
                    "sent": "In the first talk about some of this, I'll just go quickly through this.",
                    "label": 0
                },
                {
                    "sent": "So there's this technique which has been very, very popular called collaborative filtering where the simple intuition is that great.",
                    "label": 0
                },
                {
                    "sent": "Well, the simple intuition is that users with similar past preferences, or likely to have similar future preferences and one very useful model for this, has been proposed a couple of years ago is called matrix probabilistic matrix factorization.",
                    "label": 1
                },
                {
                    "sent": "The way you can understand this if you've never seen this, is that.",
                    "label": 0
                },
                {
                    "sent": "You learn a representation of users, so each user gets a representation.",
                    "label": 0
                },
                {
                    "sent": "It's a vector in high dimensional space A and each item gets the you do the same thing for items and then you combine both.",
                    "label": 0
                },
                {
                    "sent": "For example using a dot product to come up with ratings and you can see right away that the problem is if an user comes into the system and you have no ratings for that user, then there's no way you can learn this a variable, right?",
                    "label": 0
                },
                {
                    "sent": "There's no way you can learn his latent preferences and so.",
                    "label": 0
                },
                {
                    "sent": "And so you can't predict ratings for the new user on sort of the flip side of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Metal is what is.",
                    "label": 0
                },
                {
                    "sent": "Then you could use site information where I define side information generally as being any information from users, an items excluding preferences, sort of common side information or common covariates that people have been using in the past include user demographics and item content, and of course the advantage of this is that even if the user is not provided you with any ratings, given some of this site information you could do, you could do good.",
                    "label": 0
                },
                {
                    "sent": "You could do good recommendations so you could do predictions in cold start regimes using site information.",
                    "label": 0
                },
                {
                    "sent": "And of course, the idea is the site information must be weekly or somewhat indicative of a user's preferences, right?",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Onto sort of our model.",
                    "label": 0
                },
                {
                    "sent": "So we proposed this graphical model.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to show you at least once the model and it's entirely what I'm going to do now is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of dig into some two parts of the model to really try to give you intuitions about what's going on.",
                    "label": 0
                },
                {
                    "sent": "The first thing I want to do is I want to show this thing, so this is just a representation of the data that we have in the task that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "So you could encode all the data that we have in three matrices.",
                    "label": 0
                },
                {
                    "sent": "One matrix is a user by item matrix.",
                    "label": 0
                },
                {
                    "sent": "Of reading, that's the one on top.",
                    "label": 0
                },
                {
                    "sent": "And of course the task is that you want to be able to predict these question marks, right?",
                    "label": 0
                },
                {
                    "sent": "So a user has rated some items, not all of 'em, and you would like to be able to recommend or to predict his ratings for items that he's not rated.",
                    "label": 0
                },
                {
                    "sent": "Then on the bottom left we have another matrix which is a word by user matrix and basically each column is a user and the column represents how many words or a column represents the profile of the user library, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, the first user has in his library has word one, which is anywhere you can think of 1's and then he's got no other words and the second user has word two and word 3, two and four times, and we do exactly.",
                    "label": 0
                },
                {
                    "sent": "We take the exactly same encoding.",
                    "label": 0
                },
                {
                    "sent": "For the documents right?",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do here is we have three matrices and at a high level.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to find a joint factorization of these three matrices and then the idea is in this joint factorization is that the information from the bottom two matrices will be helpful to reconstruct or ratings matrix.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I thought a high level now as we said this is document recommendations documents are easily and well modeled using topic models and so that's what we're going to do here.",
                    "label": 0
                },
                {
                    "sent": "So this is the again, the graphical representation on the left side you have users on the right side.",
                    "label": 0
                },
                {
                    "sent": "You have documents on the left side you see that each user's library is represented as a bag of words.",
                    "label": 0
                },
                {
                    "sent": "So just you have the set of words in that user library and we did the same thing for documents.",
                    "label": 0
                },
                {
                    "sent": "And what we learn here is a joint topic model or a twin topic model where the topics are shared across users and across documents and that allows us to have representations A&S so user and item representations in the same space, and so at a high level.",
                    "label": 1
                },
                {
                    "sent": "If you're not exactly sure what topic models are, you can just think of them as a way of going from word space to some latent topic space, and so now what we have is each user is represented in this latent topic space.",
                    "label": 0
                },
                {
                    "sent": "As topic proportions, right?",
                    "label": 0
                },
                {
                    "sent": "So we would say for example, this user is interested in a math topic and a history topic and a lot less in this other third topic that we found and it is all learned or inferred automatically from data.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have these representations.",
                    "label": 0
                },
                {
                    "sent": "How can we combine them to predict ratings?",
                    "label": 0
                },
                {
                    "sent": "We combine them very easily by just saying we're going to match these representations, so we're going to match the reputation of documents S and we're going to depositions of users a using it again.",
                    "label": 0
                },
                {
                    "sent": "A simple dot product.",
                    "label": 0
                },
                {
                    "sent": "So we take a dot product between a an S and that yields are and there's a small twist to this, which is we take.",
                    "label": 0
                },
                {
                    "sent": "Actually, we allow the model to learn weights on these topics and the idea here is simply that some topics may be more predictive of preferences, so you may want to wait them.",
                    "label": 0
                },
                {
                    "sent": "Higher and some topics may be less productive preferences and so you'll want them too.",
                    "label": 0
                },
                {
                    "sent": "To wait and lower sorry and so here what you can notice is this.",
                    "label": 0
                },
                {
                    "sent": "This is going to be really useful for cold start problems.",
                    "label": 1
                },
                {
                    "sent": "Because this really does not require any ratings data at this point, right?",
                    "label": 0
                },
                {
                    "sent": "So as long as the user has a library and as long as an item has you have content of items, you can train this model and you can start recommending items to users.",
                    "label": 0
                },
                {
                    "sent": "This is good, but as I said in the beginning, we also want to do really well in this other case where users give us lots of preferences and so we want sort of leverage these these preferences.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the second part of the model does exactly this.",
                    "label": 0
                },
                {
                    "sent": "The second part of the model simply says.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that we have covert.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine that we have.",
                    "label": 0
                },
                {
                    "sent": "Item features here we're simply going to learn.",
                    "label": 0
                },
                {
                    "sent": "It's a linear regression model over these.",
                    "label": 0
                },
                {
                    "sent": "These features of items, right?",
                    "label": 0
                },
                {
                    "sent": "So what you think of it?",
                    "label": 0
                },
                {
                    "sent": "You can think of it this way.",
                    "label": 0
                },
                {
                    "sent": "You can think of gamma here.",
                    "label": 0
                },
                {
                    "sent": "We have one diamond per user, and you can think of it as.",
                    "label": 0
                },
                {
                    "sent": "Gamma will learn weights under topics of documents, right?",
                    "label": 0
                },
                {
                    "sent": "So regardless of your user library using only ratings, the user, the model learns that user is really interested in math topics or history topics or some other topics that you may have interest in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are sort of the two parts of our model and we now combine them very easily.",
                    "label": 0
                },
                {
                    "sent": "An additive fashion and the idea is that this parameterization allows the model to perform well in both cold start data regimes and warm started our non cold start.",
                    "label": 0
                },
                {
                    "sent": "I guess data regimes and the model can also sort of learn to trade off smoothly one for the other across users and even as a user gets more data than model can learn to use one of these terms more than the other.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you an experiment where we show that see.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Skinheads relationships to other models specifically?",
                    "label": 1
                },
                {
                    "sent": "There are degeneracies of their model which corresponds to other tools which have been proposed in the literature.",
                    "label": 0
                },
                {
                    "sent": "I won't go too much into how we do inference and learning, but just at a high level, this is a graphical model we.",
                    "label": 0
                },
                {
                    "sent": "The problem is to infer the posteriors in this graphical model.",
                    "label": 0
                },
                {
                    "sent": "The real posteriors are intractable, so we do variational inference.",
                    "label": 0
                },
                {
                    "sent": "If you look at the model a little bit harder, you can see that it's actually got some non conjugacy, so there's sort of extra levels of approximations here, but in practice we get really good result.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is lots of related work in this field.",
                    "label": 1
                },
                {
                    "sent": "In general, there's lots of lots of work that tries to use ratings and something else to better predict ratings the most.",
                    "label": 0
                },
                {
                    "sent": "The most sort of similar artists are the two at the bottom relational learning and collective matrix factorization, and regression latent factor models.",
                    "label": 1
                },
                {
                    "sent": "The main difference between our work and theirs is that since we have word data, we can use a topic model to especially model the words and that actually end up doing a lot better than just.",
                    "label": 0
                },
                {
                    "sent": "Join Factorizing 3 matrices.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk a little bit about experiments in the remaining time.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have a few datasets that we've looked at.",
                    "label": 0
                },
                {
                    "sent": "These datasets are so we have sort of two.",
                    "label": 0
                },
                {
                    "sent": "Two different domains.",
                    "label": 0
                },
                {
                    "sent": "We're looking at one of 'em are conference datasets where the users are reviewers and the task is to predict interesting papers to reviewer's, and we have a book data set from Kobo which is sort of an Amazon like company.",
                    "label": 1
                },
                {
                    "sent": "Where here the idea is to predict to recommend books to users and I must say here that these are like fairly small datasets.",
                    "label": 0
                },
                {
                    "sent": "I would agree with that.",
                    "label": 0
                },
                {
                    "sent": "I think one problem is that we've just not been able to find any bigger data set, so if anybody knows of a data set we'd love to try this model and something bigger.",
                    "label": 0
                },
                {
                    "sent": "So now let's look at some results before I look at sort of quantitive results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I show you some qualitative results.",
                    "label": 0
                },
                {
                    "sent": "As I said here, we're learning this model and in this model each user is represented by the high dimensional vector A.",
                    "label": 0
                },
                {
                    "sent": "What we can do is we can take, we can learn a model, we learn our model on a data set, and each user has a reputation in high dimensional space and we could project this this user.",
                    "label": 0
                },
                {
                    "sent": "Into low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is basically what we've done here, and so we've projected this high dimensional vector into 2D, and I've sort of went in and hand labeled to different users, and these were reviewer's for the NIPS conference and you see NIPS traditionally been a lot about machine learning, but also about neuroscience, and this is what you recover here, and the model learns other different thing or other interesting things like there's this cluster of deep learning, people core, deep learning people, and then the people that do Bayesian, non parametrics and people that do graphical models sort of intersect.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have lots of quantitative results against a bunch of other models I will skip.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not just for an interest of time.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will just finish off.",
                    "label": 0
                },
                {
                    "sent": "Basically by showing you this graph, which is saying that this is simulating an user coming into the system and so on.",
                    "label": 0
                },
                {
                    "sent": "The left would have highlighted.",
                    "label": 0
                },
                {
                    "sent": "Here is when an user comes in, we have no information whatsoever about that user.",
                    "label": 0
                },
                {
                    "sent": "We really can't do all that well, but our model which is in blue here.",
                    "label": 0
                },
                {
                    "sent": "As you give, as a user provides, is library, so is set of previously published papers or model can really leverage that and do a lot better and then slowly as you then add user preferences sort of are modeled.",
                    "label": 0
                },
                {
                    "sent": "Sort of performance is increased slowly and other models which do not have access to user libraries then slowly catch up right.",
                    "label": 0
                },
                {
                    "sent": "So we really see some of the advantage of using user libraries in the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph one last word to say that we've deployed the system as a recommendation system for NIPS reviewer's last year.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll finish off by just saying that user site information here.",
                    "label": 0
                },
                {
                    "sent": "We've shown that it's been really useful because it can provide quickly good recommendations.",
                    "label": 0
                },
                {
                    "sent": "It also does well and sort of more and more frequent or more heavy users, and there's lots of future work that we like to do, both only computational side of things, but also about from modeling more general covariance.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}