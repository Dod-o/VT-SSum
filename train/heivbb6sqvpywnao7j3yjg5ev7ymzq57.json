{
    "id": "heivbb6sqvpywnao7j3yjg5ev7ymzq57",
    "title": "Semantic Text Processing",
    "info": {
        "author": [
            "Marko Grobelnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute",
            "Bla\u017e Fortuna, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute",
            "Jan Rupnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Feb. 15, 2012",
        "recorded": "January 2012",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/xlike_kickoff2012_grobelnik_fortuna_rupnik_text/",
    "segmentation": [
        [
            "OK, so this will be.",
            "One presentation which is a little bit more like overview and then I'll.",
            "Blush and Jan will give a little bit more detailed slides on two topics, and I will use also search presentation, which is also kind of related, so take these more like us.",
            "An overview of what we can do or what we are already working on and what will.",
            "These topics will certainly progress in during the X like OK."
        ],
        [
            "Just."
        ],
        [
            "This is."
        ],
        [
            "Couple of overview slides, so this is a slide which I prepared for well for one network of excellence, which is kind of maybe interesting away.",
            "How to look at the how for the ones for cursor.",
            "This this was prepared for planet data.",
            "So.",
            "Data processing in general, so the point now here is where do we live in X like so?",
            "So data in general leaving different data modalities from very unstructured ones, like signals that sensor network here.",
            "Then we have multimedia where we already have some structure but lot of mass, audio, video and so on.",
            "Then we add a little bit more order than we get text, then a little bit more we would get networks graph.",
            "Social networks would be here structured.",
            "This would be data basis and so on.",
            "And example of the most structured types of data we can.",
            "Deal with in general are ontologies like linked open data cycle and and many more?",
            "Now these are just data modalities on the other side we want to have some operators on this data, so we want to do with this data.",
            "So what kind of operators do we generally have?",
            "So one is obviously collecting the data.",
            "This is not something which one should ignore.",
            "Collect collecting his importance.",
            "Then preparing would mean preprocessing and so on.",
            "Then we need to represent this data in some kind of form will go a little bit towards this.",
            "Then we try to build the model.",
            "So here machine learning would come into the play.",
            "Then we would do reasoning.",
            "So where we try to connect bits and pieces to produce new insights which are not explicitly present in the in the data itself.",
            "So through reasoning we got this new insights and at the end we visualize things.",
            "So this is kind of space where all these data related technologies usually would leave.",
            "And then there are a couple of other issues which somehow were hard to integrate in such a nice ordered way.",
            "So we need to take care about scalability, the enemy cities or data changing through time.",
            "Context is extremely important, so data data item, whatever.",
            "This is data record is not often interesting by itself, but only when put in the context.",
            "Then it gets relevant data quality, usage and so on.",
            "So these are related issues which.",
            "Which need to be usually addressed.",
            "Now if you take 1 area of science, computer science or information retrieval or machine learning, text mining, semantic web, there are kind of sub cubes of this cube usually and X like actually covers a lot of pretty big chunk of this sub sub cube.",
            "So certainly we deal with.",
            "Text that works structured and ontology.",
            "So this would be this part would be covered here.",
            "Emphasis is a lot on texts and someone ontologies and we generally will try to address most of these, or if not all of these operators on data and certainly will have to deal with scalability.",
            "The enemy cities that are changing through time context obviously and also quality in usage to some degree usage maybe.",
            "A little bit less so.",
            "This is 1 little bit more general slide, which is kind of good, too good, too good to have in mind.",
            "Just where we leave, how to position our work."
        ],
        [
            "General, we have three types of approaches.",
            "When we deal with things, so again this is more philosophical type of slide.",
            "We have top down approaches where we well.",
            "Give hypothesis we provide the model and then we testing.",
            "So this would be this model driven technologies.",
            "Then we have bottom up data driven technologies like machine learning data mining where we have data.",
            "We don't assume much about this data and we extract the models out of them.",
            "So this would be this bottom up and then the search type of technologies appeared recently which is also related to acts like these are collaborative ones.",
            "So crowdsourcing we ask people and you get the answer back so.",
            "This is also something which we we are addressing.",
            "Let's are promising a little bit index like as well.",
            "Depends on the problem and so on.",
            "We will select the right things."
        ],
        [
            "So elephant we saw already.",
            "I want to repeat this."
        ],
        [
            "And before I mentioned representations, so texts in general can be represented in many ways as we discussed.",
            "Spartan Accelerator was saying before so.",
            "You included already several types of how to represent ASCII text, so we transform it in tokenizer, part of speech, Part 3 cents on these are.",
            "These are maybe 3 very relevant representations, but we have more.",
            "Generally, we could split them.",
            "This is not perfect slide, but somehow just understand.",
            "So we have lexical, syntactic and more like semantic representations of Texan at each level.",
            "Generally we we can solve some problems.",
            "Like"
        ],
        [
            "For language identification or copy detection, copy detection, let's say, is identify.",
            "Which student copies homework from which then we would do copy detection, so it's enough to have just text.",
            "This character level representation.",
            "Then we go to words, phrases, part of speech text.",
            "So this is already a little bit of linguistic, still, mostly lexical.",
            "This is discussable, but let's say for this purpose of this slide is could be lexical.",
            "Then we go one level below where we have taxonomies where we order this lexical items in some kind of structures.",
            "And then we come to.",
            "So called syntactic still this is discussable.",
            "Whether this is syntactic or not, but vector space model.",
            "This is the most used far.",
            "The most used technology in text mining information retrieval.",
            "So search, let's say Google or all search engines would operate on this level of representation of text where basically we transform the document in a vector and then we operate with detector and ISIS vector.",
            "And it's nice because all linear algebra and so on is applicable on this level.",
            "Then we have language models for the spam filtering.",
            "Machine translation would operate on this level, then we come to parsing.",
            "So this is what was discussed before.",
            "Cross modality will be a little bit less interesting here where information can be represented in different forms and then we come to.",
            "Semantic representations where one would be collaborative tagging Web 2.0 it's a little bit of this.",
            "Bottom up semantics Elodie, so link data so this is very popular nowadays and then deeper representations like templates.",
            "My mind matter is my stuff from MIT so.",
            "Concept yeah, so this would be typically on this site and on the very bottom is are these deep ontologies like psych, so which we obviously use in this project.",
            "So we will.",
            "Mostly we will operate on many of these representations of text for different purposes.",
            "OK, now let's switch to the actual technology with their sizes providing into the project.",
            "And this is lots of these technologies given already usable and some of the technologies are kind of semi developed are in development and some will be developed during the course of the project so."
        ],
        [
            "So maybe first.",
            "I said one of the operators which we have on the data is is about collecting data.",
            "So nowadays especially we talk about these social news.",
            "Mainstream news media.",
            "Either you buy stuff or get it for free somewhere.",
            "Or you write your own crawler.",
            "So among colors Jsi obviously just to complement all the the whole spectrum of technologies we have.",
            "Also, our own crawler reaches seems too.",
            "Get pretty competitive also with others.",
            "Otherwise we operate also with the data which we buy like Spinner in particular spinners.",
            "One second value based startup which crawls the data and sells the feet.",
            "So we'll be using probably spinner alot so just a couple of information.",
            "A little bit of information."
        ],
        [
            "But our crawler.",
            "Oh, so tries to collect.",
            "Mostly it is mainstream news.",
            "And it's real time, feet and basically.",
            "It operates on."
        ],
        [
            "Many languages, so these are just this is really the slides which is interesting per day we have we are observing like over 100,000 unique data sources.",
            "So which is a lot I would say.",
            "Per day, per day, we get roughly 200,000 articles and it's good coverage of minority languages as well.",
            "So this is something which is in development of works or operates already, but it's not, still not.",
            "It's 80% done and passed.",
            "The archive includes 35 million articles, so which might get usable so.",
            "We perform also language identification and this.",
            "We clean the texts from this from HTML mostly into some kind of clean form so that this can be further used for processing.",
            "So this is part of our technology assets, so we deal also with this commercial fits which are which include also tweets, Facebook blocks and other stuff which is more problematic in terms of processing.",
            "But we have this available.",
            "OK, so much about."
        ],
        [
            "Think so.",
            "This is maybe a couple of graphs where you see how this crawler works.",
            "So this is number of articles per 10 seconds, so this look rhythmic scale to see that peaks and so on.",
            "OK, this is more like dashboard which which shows the I thought that we would be able to demonstrate this, but blush had to leave now.",
            "OK."
        ],
        [
            "Our next tool, which is very relevant and I guess nicely compliments.",
            "UPC technology is tax enrichment, or."
        ],
        [
            "Which is a system called enricher.",
            "So this is something which was developed already in the past projects and we are constantly investing in this so.",
            "What's the story behind this enricher?",
            "So in the last 10 years we created many components, many pieces of technology dealing with text and at the end it was just a bunch of libraries on various ends and at some point we said well.",
            "Now let's consolidate all this technology, have one simple API which is reusable through a web service for anybody, and so this is in richer easily.",
            "This web service which has many components and we are constantly adding.",
            "New ones in terms of link, so I'll demonstrate.",
            "So basically on the input we put ASCII text on the output we get text in different representations, so as a web service we get one huge XML file and I will show Now the.",
            "The.",
            "But version, which is more suitable for human eyes?"
        ],
        [
            "And so these are some of these models which we have.",
            "But let's switch to demo, I hope.",
            "So this is in richer, so we paste some ASCII text insights.",
            "So will just based on the standard.",
            "Slow, but it's OK, it works.",
            "Or let's give a chance this system to the network actually.",
            "So if anybody is doing something intensive, please stop for a SEC.",
            "OK, otherwise let's go to the next presentation, next bit and in the meantime maybe this individual will come."
        ],
        [
            "So summarization this is also one one bit of text processing, which seems to be quite relevant.",
            "So the what is summarization?",
            "How to compress text down to?",
            "A smaller percentage of the data while preserving the information to some degree."
        ],
        [
            "And so this is 1 summarizer which we use which uses.",
            "Parsers are part of the technology, and so we have big text with parse it.",
            "We get the text in this form of a graph which is produced by and richer and then basically we summarize this graph and then we can generate smaller smaller text.",
            "So this bit actually we don't have a, we have it, you know, wait rusike possibly cycle is extremely good.",
            "Language generator.",
            "Yeah, probably.",
            "Among the better ones, right, yeah?"
        ],
        [
            "So what we do?",
            "How we generate this graph?",
            "So if we have it so we perform also this sentence, splitting the usual stuff and offer a solution coreference resolution, we extract triples and we generate the graph and out of this graph.",
            "Then we generate."
        ],
        [
            "Represent the text in this kind of form so one huge graph and then through topological analysis of this graph we extract the most relevant bits.",
            "This was more like an experiment a couple of years ago, and it was interesting from this bit of research that actually from the topology of such a subject predicate object graph you can detect the good summaries pretty well.",
            "So Summarizers operate more like on a sentence level.",
            "They don't go into the structure of the text.",
            "Which OK, we try it and we were pretty competitive and it's domain independent, so it's very domain independent and we hope also language independent, which we didn't try yet.",
            "So you see, for instance, so this is nice example, so it's pretty obvious what's the article about, so the article maybe had like few 10s of articles or a few 10s of sentences.",
            "But after anaphora solution and everything else.",
            "We get a graph like this.",
            "Obviously it's about the Clinton and some of his talks and.",
            "Now if I show the demo.",
            "Just checking this.",
            "I shall show in Richard afterwards, but let me show first, show this them of this summarizer.",
            "So here I want show how we get a graph graph is provided by enricher.",
            "But at the end we get a document like this, so this is.",
            "An article from LA Times, I think.",
            "And transformed into the graph.",
            "And here we already have, so the graph each note in the graph or each edge in the graph has a weight of its importance.",
            "And now if we reduce the threshold of the graph.",
            "Down to so that we extract only one triple out of it so the whole document gets extracted into this one.",
            "Triple President Bush.",
            "Ordered embargo, economic embargo ends on out of this we can generate one sentence summary of the whole document, which pretty nice reflects the what the document about.",
            "But now if we.",
            "So decrease the threshold, then other parts of the document start appearing so powerful Iraqi army seize buildings.",
            "OK.",
            "This was kind of relevant and that's why President Bush or that embargo Bush or deployment of troops.",
            "This Side Story Turkey cut exports to Iraq and at the end the full document appears in front of us.",
            "So we have this flexibility of playing with.",
            "Document, which I guess in some parts of acts like will actually help us a lot.",
            "Now let's check if Enricher came back.",
            "Yeah it did.",
            "So the document which was before in front of us.",
            "Five this will work as well.",
            "So before I said that Enricher creates the semantic graph, we call it semantic graph is.",
            "Kind of semi semantic.",
            "Basically it's a graph of subject, predicate, object triples, which are which use.",
            "So this is the document which we saw before.",
            "So we can operate with this representation of graph as well.",
            "So before I listed all these different representations of text, so this is 1 possible representation of text, not the only one that they said.",
            "So this bag of words or vector space model is far the most used but not.",
            "Just too shallow to extract certain things out of it.",
            "And here now you will see a couple of more things.",
            "So this is 1 possible representation which we deal with.",
            "But also what we do with this text which we put on the input.",
            "So obviously we extract named entities.",
            "This is not a big science, but with this name entities we do also the following.",
            "So each name entity get this December grated into a couple of ontologies.",
            "In this particular case we use.",
            "DB Pedia so effectively.",
            "Wikipedia and.",
            "Open, open Psych and Yahoo I think.",
            "Do we have some more now?",
            "Freebase is also let's say Brazil so Brazil.",
            "If we go to Wikipedia, but we cannot.",
            "We would see that the words Brazil actually means like country, obviously see T Brazil, then plenty of places towns are called Brazil as well.",
            "Plenty of people have second name Brazil.",
            "We have songs, movies, everything is Brazil.",
            "Now which of these Brazil meanings of the word Brazil is the right one?",
            "So in the context of here?",
            "Yeah, it's like.",
            "Here so which Brazil did we mean here so and we perform this immigration.",
            "Now we click this Brazil and we get links to different ontologies, so this would be.",
            "The DPD I guess are.",
            "This is Debbie Pedia.",
            "This is freebase.",
            "This is a New York Times as well.",
            "Entity and Opencyc and couple of others.",
            "So which is always the country.",
            "So now what we achieved here.",
            "So we know which entity we are talking about and we can properly contextualize it.",
            "And let's say having the link into, let's say, psych.",
            "Oh let's see if this will come.",
            "Yeah, so this is like the description of the of Brazil.",
            "And here we see that.",
            "This is actually a country, an independent country, which is further concept versus the largest combo.",
            "And you know way more things now about Brazil as well.",
            "And we get the whole semantic context of such an entity.",
            "The same would be true if.",
            "So if you take somebody which Robbie Keane, I'm sure it's some nice guy, but I don't know about him.",
            "And here we see how OK Robbie Keane this is his free base.",
            "ID where we would get the rest of the content and we would see also that it's person and so on.",
            "So this is this semantic semantic contextualization or an annotation of.",
            "So the page didn't load yet properly I see, right?",
            "So here we would get also a couple of more things about this document.",
            "We would get also some topic categories, keywords which would be attached to this content.",
            "We would in the new version we would get also sentiment.",
            "As part of another project we have so very extract the sentiment sentiment on.",
            "This is for entity on entity level or more like on the document level.",
            "Hi, it's optimized for tweets at the moment because we need this for.",
            "But in general this sentiment sentiment detection will also be part of it.",
            "OK, so.",
            "Let's try to.",
            "See if this book come out.",
            "So we have also, so I said this is web service really, so we generate the same output also in XML and also RDF or.",
            "Ha, here we see.",
            "So the whole document now.",
            "Is described with this keyboard.",
            "So sports, soccer and so on.",
            "And these are categories from, let's say, demos.",
            "This is Open directory project, so this is kind of summary topic topic level summary and at the bottom you will see.",
            "So these are these are models which were triggered how much each of the models took.",
            "These models are now we are adding them so.",
            "But the point is somehow to have this one one big one week back web service with different modules.",
            "So now for instance for X like would be ideal.",
            "Now this operates in English, if we could.",
            "Connected to UPC technology for different languages.",
            "So this framework this would be for instance, from nice nice accomplishment of the work package, which one two I guess.",
            "Or something along those lines.",
            "So we need to talk about this.",
            "OK, let's proceed now.",
            "Oh so we saw.",
            "Now the."
        ],
        [
            "Mom."
        ],
        [
            "Oh"
        ],
        [
            "OK."
        ],
        [
            "Discretion answer."
        ],
        [
            "Ring maybe we can.",
            "We can do afterwards.",
            "Now the next thing is, whatever was also mentioning.",
            "So if you have let's say Chinese document in English document in Slovenian document, can we say if these documents talk about the same stuff or are they about different things?",
            "So this model would go in the categorization are on this level of representation which I mentioned before.",
            "Vector space model.",
            "Oh"
        ],
        [
            "Level of representation, so it's more like shallow, but it can be very scalable and for a couple of applications can be very useful.",
            "A part of this meta net project or an already before part of this.",
            "Project Smart also in this language technology unit.",
            "We develop this technology based on this.",
            "CCA for this kernel Canonical correlation analysis, which produces some intermediate representation of languages where differently where we try to remove.",
            "Specifics of particular language, so we try to generate language independent or language neutral document representation and young will say a few words about this."
        ],
        [
            "So I'll just quickly tell you about the multi view Canonical correlation analysis.",
            "So the method that Mark mentioned."
        ],
        [
            "So the outline just introduction to Canonical correlation analysis and the generalization that can be used for cross lingual information retrieval."
        ],
        [
            "So first is just this vector space model that Marco mentioned.",
            "So this is just one slide that is just showing that what we mean by that.",
            "So we we transform documents into vectors so each each component of a vector corresponds to awards in a fixed vocabulary and the number in for each component is just the frequency of the this particular word in a document, so.",
            "So this means that we discard some information about the structure of the document and we just keep some, let's say which which words are relevant for it, and by using a this engrams we can also keep some structure so."
        ],
        [
            "So and once we have this representation, then we can simply compare documents in a single language by computing cosine similarity between documents.",
            "So this is just an example of this."
        ],
        [
            "OK, so.",
            "What the Canonical correlation analysis is a.",
            "It's a it's.",
            "We can look at it as a method that does dimensionality reduction.",
            "It is similar to a principal component analysis which is widely used for that purpose, and we can think of it as a method is a method that finds similar patterns in two sources of data.",
            "So patterns that contains some mutual information and we need some training examples for this to work.",
            "So it's a statistical approach that finds this constant.",
            "This patterns so examples of pairs of sources in standard Canonical correlation analysis are, let's say, documents in two languages which we just saw, let's say images and text.",
            "So this, and let's say fMRI scans and brain activity data so it can be, uh, it's a widely applicable method."
        ],
        [
            "And just throw it in.",
            "Technically it boils down to solving an eigenvalue problem.",
            "So this is also very convenient because this technology has been very well developed so a lot of work has been done so so this is one thing and what it acts."
        ],
        [
            "Julie, this is, let's say if we have two spaces of X&Y, so this we imagine these are documents, let's say English in German.",
            "Each with their own dimension.",
            "We will end.",
            "This little characters represent.",
            "So let's say point here and .8 there are.",
            "We imagine these are documents that are translations of each other and the point is that we want to find the directions in the first language and direction in second language, so that when we project the documents into this space so we get correlated numbers.",
            "So here we see the first 2 the most important.",
            "The axis this in this space and this one in this space, and we also have a second set of axis which are, let's say, correspond to noise.",
            "So this was.",
            "This is what I meant with the dimensionality reduction.",
            "So these are the pairs of documents and the method automatically finds this directions where the mutual information is."
        ],
        [
            "Then what we did was to try to work on scaling this Canonical correlation to more than two views.",
            "So and it's, uh, what you do is the generalization of that method.",
            "So instead of, instead of finding directions that maximize the correlation, you find directions across all the languages that maximize the sum of all pairwise correlations.",
            "So this means, let's say in text, we're looking for concepts.",
            "In all the languages that mean the same thing, so they will have the same, let's say Co occurrence patterns.",
            "So and once we find one set of this.",
            "Is this vectors are just we think of them as different representations of the same semantic thing.",
            "So language specific representations.",
            "And once we find one set of this directions, we then proceed to finding another one that contains some new information."
        ],
        [
            "And the so we we have a method that scales roughly linearly in the size of the input data.",
            "So it means that it can.",
            "It can scale across a large number of documents.",
            "It's exploit sparsity, and so the only the only thing is that there's a squared dependence on the number of these concepts that we wish to extract."
        ],
        [
            "So this is just an example of what what an output of the Methodist without the weights that are associated with the words.",
            "So the first set, the first set, is a set of concept vectors.",
            "Let's say in English amendment Russia, human rights and in German in German Ruslan to mention rector and so this is just these are concepts that are important for the corpus the multilingual corpus.",
            "And the that are in, let's say, these are all representations that are language independent and the second set of concepts is orthogonal to the first one.",
            "So it contains truly different information.",
            "But it's also important for if we want to represent documents in the corpus."
        ],
        [
            "So we can use this representation for multilingual search.",
            "So let's say we have documents X and in X space and documents in white space, and what this may this concepts that we found that this concepts enable us to map the all the points in X into this in a common representation space and also the documents in why so this mapping is actually just comparing the each document in X with with the.",
            "Concept vectors that the method discovered and also for the wyview to you.",
            "OM app the Y vectors to the corresponding representations.",
            "So then you get something.",
            "So representation in the common space and if you have a query, let's say in the X space, then you also project it and then just perform standard monolingual.",
            "Methods to to do retrieval.",
            "Also."
        ],
        [
            "We did.",
            "Some experiments were based, so they were based on 8 or 10 European languages.",
            "So this is based on your apparel European Parliament corpus.",
            "And we did some stuff.",
            "Standard information retrieval measures like mate retrieval.",
            "And so the query mate retrieval and we."
        ],
        [
            "Compared it to a cross lingual latent semantic analysis, which is a similar method similar in Spirit, except that there you don't.",
            "You don't keep this information which words come to from which language, you just concatenate everything and then perform the standard, let's say singular value decomposition.",
            "And help.",
            "You do it right."
        ],
        [
            "Installation before you do the vectorization Department, practically no.",
            "So we just take the so the data set.",
            "Your apparel has sets of translations so we have.",
            "We keep some for training and some for test.",
            "So for training we find it in the training set.",
            "We find this concept vectors in different representations and then apply them to the test set and then do retrieval.",
            "Monolingual retrieval.",
            "So no translations, so you take the role data.",
            "The only piece of supervision which of having data is the fact that you have aligned documents to say well for for training phase.",
            "So this document is translation of this one.",
            "So this is the only bit of supervision.",
            "On top of playing unstated.",
            "So an alliance aligned corpus is the input to the method.",
            "And in the case of this year Parliament we have, we have this in 20 something languages align set of documents across the.",
            "So how many documents 100 thousands?",
            "So here we.",
            "So here we tested on 8000 and trained on 100,000 documents.",
            "So this is just.",
            "It shows us that taking this information into account so not just concatenating everything it can help and it's also scalable so.",
            "This is the.",
            "Say just few words about this recent experiment.",
            "So recently we've also started working on Wikipedia, so where the the main goal is to try to learn this concept mapping on, let's say, 100 languages which are let's represented well, represented so that you have over 10,000 documents at least.",
            "So in the goal is to now recently we've been training the this of extracting the concept vectors, which then enable us to compare documents from this 100 languages.",
            "So this is something which is upcoming and potentially will have pretty good.",
            "Impact, once it will function."
        ],
        [
            "And blush, maybe you can, so this is another topic which is related to acts like, so the context is the following.",
            "So news are being published.",
            "Round the world.",
            "So if an event happens.",
            "Many new sources talk about this event and but every new source has its own bias.",
            "Now that this is 1 pre experiment in a way how to model the news bias and in X like we will try to go beyond beyond this experiment.",
            "But certainly this was pretty good insight into the problem.",
            "So here is a couple of slides on the experiment.",
            "So news bias.",
            "So here you are.",
            "Christian couple of new sources in an English language, as Marco said, so in the context of weeks like we would like to extend this to foster more new sources and also to cover more languages."
        ],
        [
            "So this is 1 example of news bias, so you can see the articles talking about the same news at the same event, but they're clearly having different opinions about the event that a particle is from CNN and the bottom article is from Aljazeera English."
        ],
        [
            "And this was the datasets that we were using, so we had four new sources, Aljazeera CNN Detroit News and International Herald Tribune.",
            "So two kind of global coverage newspapers, one more local newspaper for US and one more focused on Middle East.",
            "And we were calling for one year or all the news from international section.",
            "And so this is the top part.",
            "Here is the actual draw data.",
            "Then in the second step we found the matching news articles.",
            "So for each part of new sources we identified the news articles which talked about the same events and in this matrix you can see the overlaps and you can see for example, that was expected, CNN and International Herald Tribune would have the highest overlap with pleasure, and Detroit News have the smallest overlap and matching was done based on the.",
            "Similarity of the articles and based on the time I guess I'm entities mentioned and so couple of holistics together and we were more focused on not on recall.",
            "So not trying to identify all the pairs but on the precision and these pairs that are identified were quite accurate.",
            "More than 95% precision.",
            "So if the articles were identified to be about the same event, if are most likely about the same event.",
            "This is quite important 'cause."
        ],
        [
            "In the next step we were we did experiment, so we try to use this data to learn predictive model to tell whether the source of new articles.",
            "So if you get a new article, can we tell whether CNN wrote about it or Aljazeera wrote about it?",
            "Just looking at the words in the article and if this is possible, this would mean that there's some bias in the vocabulary of the new source that reveals the which of the which one it is.",
            "And it turned out.",
            "So.",
            "Doctor number here."
        ],
        [
            "So it turned out there was around 80% accuracy when you try to predict the new source between CNN and Aljazeera.",
            "And for example if you look at the model so.",
            "But you used was a SVM linear model and you can check the words that had the highest rates and the most significant keywords for CNN would be, so they would write about insurgents about terrorists, militants, suicide.",
            "So on various Aljazeera would talk about rebels resistance and fighters, so quite different.",
            "Point of view.",
            "So what we plan to do in it's like, is to use this extended.",
            "So here we were only focused on the vocabulary so quite basic representation.",
            "We also did another set where we're comparing the new sources based on the coverage, but we would like to extend it to include also coverage of entities and some other things that will be extracted from this multilingual new sources and this island of formal knowledge.",
            "OK, so this news bias and this diversity, which happens.",
            "So we have one project on this topic called Render together with calls through.",
            "And lots of these things happened there, but we don't have cross lingual technology there, so there is more like monolingual and extending this.",
            "News bias problem also towards.",
            "Cross lingual would be actually pretty cool, so there's something what Evan was mentioning before, like if.",
            "Go to New York Times cites.",
            "See this completely unbiased, professionally written news from New York Times, which we all know is the ground truth.",
            "And then you will have bottom.",
            "So the evil ones and the thread.",
            "The eager ones, and you would see what other people are actually writing related to the same stuff."
        ],
        [
            "OK, so just to proceed so we have.",
            "So this is 1 demo which actually was prepared a couple of years ago for New York Times, but Evan doesn't even know that.",
            "Which we are showing now all this year.",
            "So it operates on the Reuters corpus, the one which was mentioned by Evan.",
            "And so it's one year of news, 830,000 news stories, and so this is them.",
            "Or how to show the?",
            "Search results in different ways, such as us from news.",
            "So here will make a search for Clinton.",
            "And we get set of results on the left, just second.",
            "First query always takes a little bit more time."
        ],
        [
            "So these are set of new stories from this corpus, which which mentioned cleaned and so there are quite some of them.",
            "So on the right side we have this topic.",
            "This is 1 visualization of the content.",
            "So each cross here is 1 new story and crosses are closer in the picture.",
            "If they talk about the same stuff.",
            "So let's say here we have one cluster.",
            "With this circle, we put the context and this small window shows the keywords, which are the most relevant for this cluster.",
            "So here we see.",
            "Obviously it's Middle East crisis cluster, so if we would go here would be more like election campaign.",
            "This would be budget discussion.",
            "This is more like economic discussions where Clinton was mentioned.",
            "So this is topic view here we have also so."
        ],
        [
            "Should you wear, we extract names of people and we cluster them.",
            "So this is another view on the same set of results here on the left and the third view is."
        ],
        [
            "How the?",
            "How the dynamics of these stories happened?",
            "So you see, for instance, the first story, which is about the election, was very active in the first part, but then disappeared and discussion about the discussion about.",
            "Budget started and so on.",
            "So we've we would put some other let's say Tomba.",
            "Remember Hutama was skier so he was relevant only in the winter received.",
            "So in the winter months and then he disappeared from the news.",
            "So if we would have like influenza.",
            "We would see that there were obviously two relevant epidemics that year, so one here one here.",
            "So this is 1 view.",
            "How so?",
            "This temporal view of the of the of the topics which we can.",
            "See through such a way this is not optimal, was more like optimally done.",
            "It's more like a prototype but but the idea is here somehow too.",
            "So to go in this direction to have this alternative views to the same corpus of the content.",
            "OK, just to continue.",
            "Oh I'm hot.",
            "So these are actually two a couple of pictures from New York Times or."
        ],
        [
            "Aircraft for instance.",
            "This is Pearl Harbor from from the 15 million or big corpus, so Pearl Harbor visualized in that tool.",
            "So we see that practically didn't was almost not mentioned before.",
            "41 When the attack happened, then it was lots of news and these are kind of clusters of topics.",
            "Now the interface should really support also drill down a couple of other things.",
            "But or."
        ],
        [
            "Belgrade, for instance, in the Second World War.",
            "So you see, there was a big hype in New York Times, so mentioning Belgrade a lot.",
            "And then when the actual attack happened, so this will not April 6 then it all disappeared from the news because it was so much other things going on, I guess.",
            "And these discussions here where if you analyze these keywords you would see that it's discussion whether you Slavia back then will join the pact with Nazis or not.",
            "So this is this is another reason."
        ],
        [
            "And and, let's say Normandy you see before Second World War was mentioned as.",
            "About sailing somewhere like tree lights with topics and then disappeared from the news until the text happens.",
            "This is one couple of snapshots."
        ],
        [
            "New York Times corpus I have just won a couple of slides.",
            "And so.",
            "Um?",
            "This is a topic.",
            "Which should be really presented by by by Meteor Trampusch, but he had to leave before.",
            "I will just extract this.",
            "So this is something which is sort of related to to X.",
            "Like a lot we have.",
            "Oh couple of more like preliminary experiments on this.",
            "So let's see if we go to Google News.",
            "If you go to Google News and.",
            "I will just.",
            "Tell you about the intuition.",
            "We got just the fresh Google News we would see.",
            "There are plenty of this clusters of topics.",
            "So OK, the top news at the moment is Wikipedia.",
            "Google protests US anti piracy proposals nice.",
            "And ABC News.",
            "Top Guardian latimes mention Fox News.",
            "Usually some of these more influential news agencies and then it's written here OK. 3000 more.",
            "2400 more sources.",
            "Nice but actually.",
            "Now the intuition is the following.",
            "Current language technology tools as we know they are not perfect so we cannot extract very accurately from one single document.",
            "Everything what we would like to.",
            "But if we have let's say 10s or hundreds or even thousands of articles talking about the same topic, we generally should be able to extract these bits and pieces from this redundancy.",
            "Which appears because of this alternative ways of expressing by different publishers.",
            "So this is the idea and.",
            "Now going back to the slides.",
            "So here we have this big set of articles.",
            "We clustered them into the stories we create.",
            "These graphs which I showed before in richer and then we merge the graphs and at the end these stories pop out in very accurate way.",
            "So here is 1 example.",
            "Of so Media was doing this experiment.",
            "I think there's some numbers.",
            "So this was set of events on.",
            "Uh.",
            "On some suicide bombers.",
            "And so out of multiple articles we extracted this kind of template.",
            "What?",
            "What actually happened?",
            "You see that different documents are reconfirming certain parts of this graph, and some parts are not so frequent.",
            "So and through this multiple views into the onto the same event, basically we can extract.",
            "Tori and well even more so we can also extract the template of the story so.",
            "I'm showing now actually the this other extracting the templates, which is roughly the same.",
            "The same idea.",
            "So so for instance, these are two stories, this time about some.",
            "The.",
            "Again, some killing basically.",
            "So.",
            "So Don Vito was eating an Apple.",
            "He shot down Amelia.",
            "Sony was helping and on Amelia was that an Sony was wearing sunglasses obviously and he was wearing a knife so this is very typical story.",
            "We have another story where police arrested Jill.",
            "Manjil Blast kills Sony.",
            "Sony was death.",
            "Sony saw her gun Jack Carey Brandy and so you see, this is a story which can be extracted from from enricher pretty much in this form.",
            "Now we have two stories of the same kind, although they talk about two different events.",
            "Now, the point is that we would like to match this.",
            "These notes inappropriate way.",
            "Uh.",
            "So at the end, yeah, so at the end, at the end we would have matching like this so gun would match knife.",
            "Sony would match Jack, Don, Vito, Jill.",
            "And on the media, Sony in this case.",
            "And out of this, this type of information we can extract.",
            "Extract story templates.",
            "So now here we have.",
            "The experiment was 7000 articles from Google News, 200 stories and 10 different topics.",
            "So 10 stories for topic roughly.",
            "So this would be a template for suicide bombing, so we always have suicide bomber.",
            "He blew himself.",
            "On some location it's always location.",
            "There always was that one.",
            "I think explosive.",
            "He blew an organization, blew himself as well and he killed people.",
            "So this is a template of these kind of stories and this came out automatically out of the same would be so person.",
            "Sentence for certain number of years pleaded guilty and so I don't know this.",
            "Was this telling really mean?",
            "So this is this is towards the end of X like pipeline.",
            "The idea would be to extract this kind of templates out of the stories and then this will be then sort of imposed on different languages.",
            "Different languages would share this kind of template, they just express themselves in different ways.",
            "So this would be.",
            "One quick.",
            "Explanation of this?",
            "OK, so this is more like sneaking more like preview of the stuff which we are working in the back, but this is far from mature.",
            "And this kind of templates could be obviously not linked also to let's say.",
            "Event or semantic templates in psych or in some other ontologies where we would actually get extra information out of the.",
            "Data OK, so much from.",
            "Mind my sites.",
            "And we can continue with Michael I guess right?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this will be.",
                    "label": 0
                },
                {
                    "sent": "One presentation which is a little bit more like overview and then I'll.",
                    "label": 0
                },
                {
                    "sent": "Blush and Jan will give a little bit more detailed slides on two topics, and I will use also search presentation, which is also kind of related, so take these more like us.",
                    "label": 0
                },
                {
                    "sent": "An overview of what we can do or what we are already working on and what will.",
                    "label": 0
                },
                {
                    "sent": "These topics will certainly progress in during the X like OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple of overview slides, so this is a slide which I prepared for well for one network of excellence, which is kind of maybe interesting away.",
                    "label": 0
                },
                {
                    "sent": "How to look at the how for the ones for cursor.",
                    "label": 0
                },
                {
                    "sent": "This this was prepared for planet data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Data processing in general, so the point now here is where do we live in X like so?",
                    "label": 0
                },
                {
                    "sent": "So data in general leaving different data modalities from very unstructured ones, like signals that sensor network here.",
                    "label": 0
                },
                {
                    "sent": "Then we have multimedia where we already have some structure but lot of mass, audio, video and so on.",
                    "label": 0
                },
                {
                    "sent": "Then we add a little bit more order than we get text, then a little bit more we would get networks graph.",
                    "label": 0
                },
                {
                    "sent": "Social networks would be here structured.",
                    "label": 0
                },
                {
                    "sent": "This would be data basis and so on.",
                    "label": 0
                },
                {
                    "sent": "And example of the most structured types of data we can.",
                    "label": 0
                },
                {
                    "sent": "Deal with in general are ontologies like linked open data cycle and and many more?",
                    "label": 0
                },
                {
                    "sent": "Now these are just data modalities on the other side we want to have some operators on this data, so we want to do with this data.",
                    "label": 0
                },
                {
                    "sent": "So what kind of operators do we generally have?",
                    "label": 0
                },
                {
                    "sent": "So one is obviously collecting the data.",
                    "label": 1
                },
                {
                    "sent": "This is not something which one should ignore.",
                    "label": 0
                },
                {
                    "sent": "Collect collecting his importance.",
                    "label": 0
                },
                {
                    "sent": "Then preparing would mean preprocessing and so on.",
                    "label": 0
                },
                {
                    "sent": "Then we need to represent this data in some kind of form will go a little bit towards this.",
                    "label": 0
                },
                {
                    "sent": "Then we try to build the model.",
                    "label": 0
                },
                {
                    "sent": "So here machine learning would come into the play.",
                    "label": 0
                },
                {
                    "sent": "Then we would do reasoning.",
                    "label": 0
                },
                {
                    "sent": "So where we try to connect bits and pieces to produce new insights which are not explicitly present in the in the data itself.",
                    "label": 0
                },
                {
                    "sent": "So through reasoning we got this new insights and at the end we visualize things.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of space where all these data related technologies usually would leave.",
                    "label": 0
                },
                {
                    "sent": "And then there are a couple of other issues which somehow were hard to integrate in such a nice ordered way.",
                    "label": 0
                },
                {
                    "sent": "So we need to take care about scalability, the enemy cities or data changing through time.",
                    "label": 0
                },
                {
                    "sent": "Context is extremely important, so data data item, whatever.",
                    "label": 0
                },
                {
                    "sent": "This is data record is not often interesting by itself, but only when put in the context.",
                    "label": 1
                },
                {
                    "sent": "Then it gets relevant data quality, usage and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are related issues which.",
                    "label": 0
                },
                {
                    "sent": "Which need to be usually addressed.",
                    "label": 0
                },
                {
                    "sent": "Now if you take 1 area of science, computer science or information retrieval or machine learning, text mining, semantic web, there are kind of sub cubes of this cube usually and X like actually covers a lot of pretty big chunk of this sub sub cube.",
                    "label": 0
                },
                {
                    "sent": "So certainly we deal with.",
                    "label": 0
                },
                {
                    "sent": "Text that works structured and ontology.",
                    "label": 0
                },
                {
                    "sent": "So this would be this part would be covered here.",
                    "label": 0
                },
                {
                    "sent": "Emphasis is a lot on texts and someone ontologies and we generally will try to address most of these, or if not all of these operators on data and certainly will have to deal with scalability.",
                    "label": 0
                },
                {
                    "sent": "The enemy cities that are changing through time context obviously and also quality in usage to some degree usage maybe.",
                    "label": 0
                },
                {
                    "sent": "A little bit less so.",
                    "label": 0
                },
                {
                    "sent": "This is 1 little bit more general slide, which is kind of good, too good, too good to have in mind.",
                    "label": 0
                },
                {
                    "sent": "Just where we leave, how to position our work.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General, we have three types of approaches.",
                    "label": 0
                },
                {
                    "sent": "When we deal with things, so again this is more philosophical type of slide.",
                    "label": 0
                },
                {
                    "sent": "We have top down approaches where we well.",
                    "label": 0
                },
                {
                    "sent": "Give hypothesis we provide the model and then we testing.",
                    "label": 0
                },
                {
                    "sent": "So this would be this model driven technologies.",
                    "label": 1
                },
                {
                    "sent": "Then we have bottom up data driven technologies like machine learning data mining where we have data.",
                    "label": 1
                },
                {
                    "sent": "We don't assume much about this data and we extract the models out of them.",
                    "label": 0
                },
                {
                    "sent": "So this would be this bottom up and then the search type of technologies appeared recently which is also related to acts like these are collaborative ones.",
                    "label": 0
                },
                {
                    "sent": "So crowdsourcing we ask people and you get the answer back so.",
                    "label": 0
                },
                {
                    "sent": "This is also something which we we are addressing.",
                    "label": 0
                },
                {
                    "sent": "Let's are promising a little bit index like as well.",
                    "label": 0
                },
                {
                    "sent": "Depends on the problem and so on.",
                    "label": 0
                },
                {
                    "sent": "We will select the right things.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So elephant we saw already.",
                    "label": 0
                },
                {
                    "sent": "I want to repeat this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And before I mentioned representations, so texts in general can be represented in many ways as we discussed.",
                    "label": 0
                },
                {
                    "sent": "Spartan Accelerator was saying before so.",
                    "label": 0
                },
                {
                    "sent": "You included already several types of how to represent ASCII text, so we transform it in tokenizer, part of speech, Part 3 cents on these are.",
                    "label": 0
                },
                {
                    "sent": "These are maybe 3 very relevant representations, but we have more.",
                    "label": 0
                },
                {
                    "sent": "Generally, we could split them.",
                    "label": 0
                },
                {
                    "sent": "This is not perfect slide, but somehow just understand.",
                    "label": 0
                },
                {
                    "sent": "So we have lexical, syntactic and more like semantic representations of Texan at each level.",
                    "label": 0
                },
                {
                    "sent": "Generally we we can solve some problems.",
                    "label": 0
                },
                {
                    "sent": "Like",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For language identification or copy detection, copy detection, let's say, is identify.",
                    "label": 1
                },
                {
                    "sent": "Which student copies homework from which then we would do copy detection, so it's enough to have just text.",
                    "label": 0
                },
                {
                    "sent": "This character level representation.",
                    "label": 0
                },
                {
                    "sent": "Then we go to words, phrases, part of speech text.",
                    "label": 0
                },
                {
                    "sent": "So this is already a little bit of linguistic, still, mostly lexical.",
                    "label": 0
                },
                {
                    "sent": "This is discussable, but let's say for this purpose of this slide is could be lexical.",
                    "label": 0
                },
                {
                    "sent": "Then we go one level below where we have taxonomies where we order this lexical items in some kind of structures.",
                    "label": 0
                },
                {
                    "sent": "And then we come to.",
                    "label": 0
                },
                {
                    "sent": "So called syntactic still this is discussable.",
                    "label": 0
                },
                {
                    "sent": "Whether this is syntactic or not, but vector space model.",
                    "label": 0
                },
                {
                    "sent": "This is the most used far.",
                    "label": 0
                },
                {
                    "sent": "The most used technology in text mining information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So search, let's say Google or all search engines would operate on this level of representation of text where basically we transform the document in a vector and then we operate with detector and ISIS vector.",
                    "label": 0
                },
                {
                    "sent": "And it's nice because all linear algebra and so on is applicable on this level.",
                    "label": 1
                },
                {
                    "sent": "Then we have language models for the spam filtering.",
                    "label": 1
                },
                {
                    "sent": "Machine translation would operate on this level, then we come to parsing.",
                    "label": 0
                },
                {
                    "sent": "So this is what was discussed before.",
                    "label": 0
                },
                {
                    "sent": "Cross modality will be a little bit less interesting here where information can be represented in different forms and then we come to.",
                    "label": 0
                },
                {
                    "sent": "Semantic representations where one would be collaborative tagging Web 2.0 it's a little bit of this.",
                    "label": 0
                },
                {
                    "sent": "Bottom up semantics Elodie, so link data so this is very popular nowadays and then deeper representations like templates.",
                    "label": 0
                },
                {
                    "sent": "My mind matter is my stuff from MIT so.",
                    "label": 0
                },
                {
                    "sent": "Concept yeah, so this would be typically on this site and on the very bottom is are these deep ontologies like psych, so which we obviously use in this project.",
                    "label": 0
                },
                {
                    "sent": "So we will.",
                    "label": 0
                },
                {
                    "sent": "Mostly we will operate on many of these representations of text for different purposes.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's switch to the actual technology with their sizes providing into the project.",
                    "label": 0
                },
                {
                    "sent": "And this is lots of these technologies given already usable and some of the technologies are kind of semi developed are in development and some will be developed during the course of the project so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe first.",
                    "label": 0
                },
                {
                    "sent": "I said one of the operators which we have on the data is is about collecting data.",
                    "label": 0
                },
                {
                    "sent": "So nowadays especially we talk about these social news.",
                    "label": 0
                },
                {
                    "sent": "Mainstream news media.",
                    "label": 0
                },
                {
                    "sent": "Either you buy stuff or get it for free somewhere.",
                    "label": 0
                },
                {
                    "sent": "Or you write your own crawler.",
                    "label": 0
                },
                {
                    "sent": "So among colors Jsi obviously just to complement all the the whole spectrum of technologies we have.",
                    "label": 0
                },
                {
                    "sent": "Also, our own crawler reaches seems too.",
                    "label": 0
                },
                {
                    "sent": "Get pretty competitive also with others.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we operate also with the data which we buy like Spinner in particular spinners.",
                    "label": 0
                },
                {
                    "sent": "One second value based startup which crawls the data and sells the feet.",
                    "label": 0
                },
                {
                    "sent": "So we'll be using probably spinner alot so just a couple of information.",
                    "label": 0
                },
                {
                    "sent": "A little bit of information.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But our crawler.",
                    "label": 0
                },
                {
                    "sent": "Oh, so tries to collect.",
                    "label": 0
                },
                {
                    "sent": "Mostly it is mainstream news.",
                    "label": 0
                },
                {
                    "sent": "And it's real time, feet and basically.",
                    "label": 0
                },
                {
                    "sent": "It operates on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many languages, so these are just this is really the slides which is interesting per day we have we are observing like over 100,000 unique data sources.",
                    "label": 0
                },
                {
                    "sent": "So which is a lot I would say.",
                    "label": 0
                },
                {
                    "sent": "Per day, per day, we get roughly 200,000 articles and it's good coverage of minority languages as well.",
                    "label": 1
                },
                {
                    "sent": "So this is something which is in development of works or operates already, but it's not, still not.",
                    "label": 0
                },
                {
                    "sent": "It's 80% done and passed.",
                    "label": 0
                },
                {
                    "sent": "The archive includes 35 million articles, so which might get usable so.",
                    "label": 1
                },
                {
                    "sent": "We perform also language identification and this.",
                    "label": 0
                },
                {
                    "sent": "We clean the texts from this from HTML mostly into some kind of clean form so that this can be further used for processing.",
                    "label": 0
                },
                {
                    "sent": "So this is part of our technology assets, so we deal also with this commercial fits which are which include also tweets, Facebook blocks and other stuff which is more problematic in terms of processing.",
                    "label": 0
                },
                {
                    "sent": "But we have this available.",
                    "label": 0
                },
                {
                    "sent": "OK, so much about.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think so.",
                    "label": 0
                },
                {
                    "sent": "This is maybe a couple of graphs where you see how this crawler works.",
                    "label": 0
                },
                {
                    "sent": "So this is number of articles per 10 seconds, so this look rhythmic scale to see that peaks and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, this is more like dashboard which which shows the I thought that we would be able to demonstrate this, but blush had to leave now.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next tool, which is very relevant and I guess nicely compliments.",
                    "label": 0
                },
                {
                    "sent": "UPC technology is tax enrichment, or.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is a system called enricher.",
                    "label": 0
                },
                {
                    "sent": "So this is something which was developed already in the past projects and we are constantly investing in this so.",
                    "label": 0
                },
                {
                    "sent": "What's the story behind this enricher?",
                    "label": 0
                },
                {
                    "sent": "So in the last 10 years we created many components, many pieces of technology dealing with text and at the end it was just a bunch of libraries on various ends and at some point we said well.",
                    "label": 0
                },
                {
                    "sent": "Now let's consolidate all this technology, have one simple API which is reusable through a web service for anybody, and so this is in richer easily.",
                    "label": 0
                },
                {
                    "sent": "This web service which has many components and we are constantly adding.",
                    "label": 0
                },
                {
                    "sent": "New ones in terms of link, so I'll demonstrate.",
                    "label": 0
                },
                {
                    "sent": "So basically on the input we put ASCII text on the output we get text in different representations, so as a web service we get one huge XML file and I will show Now the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "But version, which is more suitable for human eyes?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these are some of these models which we have.",
                    "label": 0
                },
                {
                    "sent": "But let's switch to demo, I hope.",
                    "label": 0
                },
                {
                    "sent": "So this is in richer, so we paste some ASCII text insights.",
                    "label": 0
                },
                {
                    "sent": "So will just based on the standard.",
                    "label": 0
                },
                {
                    "sent": "Slow, but it's OK, it works.",
                    "label": 0
                },
                {
                    "sent": "Or let's give a chance this system to the network actually.",
                    "label": 0
                },
                {
                    "sent": "So if anybody is doing something intensive, please stop for a SEC.",
                    "label": 0
                },
                {
                    "sent": "OK, otherwise let's go to the next presentation, next bit and in the meantime maybe this individual will come.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So summarization this is also one one bit of text processing, which seems to be quite relevant.",
                    "label": 0
                },
                {
                    "sent": "So the what is summarization?",
                    "label": 0
                },
                {
                    "sent": "How to compress text down to?",
                    "label": 0
                },
                {
                    "sent": "A smaller percentage of the data while preserving the information to some degree.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is 1 summarizer which we use which uses.",
                    "label": 0
                },
                {
                    "sent": "Parsers are part of the technology, and so we have big text with parse it.",
                    "label": 0
                },
                {
                    "sent": "We get the text in this form of a graph which is produced by and richer and then basically we summarize this graph and then we can generate smaller smaller text.",
                    "label": 0
                },
                {
                    "sent": "So this bit actually we don't have a, we have it, you know, wait rusike possibly cycle is extremely good.",
                    "label": 0
                },
                {
                    "sent": "Language generator.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably.",
                    "label": 0
                },
                {
                    "sent": "Among the better ones, right, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "How we generate this graph?",
                    "label": 0
                },
                {
                    "sent": "So if we have it so we perform also this sentence, splitting the usual stuff and offer a solution coreference resolution, we extract triples and we generate the graph and out of this graph.",
                    "label": 0
                },
                {
                    "sent": "Then we generate.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Represent the text in this kind of form so one huge graph and then through topological analysis of this graph we extract the most relevant bits.",
                    "label": 0
                },
                {
                    "sent": "This was more like an experiment a couple of years ago, and it was interesting from this bit of research that actually from the topology of such a subject predicate object graph you can detect the good summaries pretty well.",
                    "label": 0
                },
                {
                    "sent": "So Summarizers operate more like on a sentence level.",
                    "label": 0
                },
                {
                    "sent": "They don't go into the structure of the text.",
                    "label": 0
                },
                {
                    "sent": "Which OK, we try it and we were pretty competitive and it's domain independent, so it's very domain independent and we hope also language independent, which we didn't try yet.",
                    "label": 0
                },
                {
                    "sent": "So you see, for instance, so this is nice example, so it's pretty obvious what's the article about, so the article maybe had like few 10s of articles or a few 10s of sentences.",
                    "label": 0
                },
                {
                    "sent": "But after anaphora solution and everything else.",
                    "label": 0
                },
                {
                    "sent": "We get a graph like this.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's about the Clinton and some of his talks and.",
                    "label": 0
                },
                {
                    "sent": "Now if I show the demo.",
                    "label": 0
                },
                {
                    "sent": "Just checking this.",
                    "label": 0
                },
                {
                    "sent": "I shall show in Richard afterwards, but let me show first, show this them of this summarizer.",
                    "label": 0
                },
                {
                    "sent": "So here I want show how we get a graph graph is provided by enricher.",
                    "label": 0
                },
                {
                    "sent": "But at the end we get a document like this, so this is.",
                    "label": 0
                },
                {
                    "sent": "An article from LA Times, I think.",
                    "label": 0
                },
                {
                    "sent": "And transformed into the graph.",
                    "label": 0
                },
                {
                    "sent": "And here we already have, so the graph each note in the graph or each edge in the graph has a weight of its importance.",
                    "label": 0
                },
                {
                    "sent": "And now if we reduce the threshold of the graph.",
                    "label": 0
                },
                {
                    "sent": "Down to so that we extract only one triple out of it so the whole document gets extracted into this one.",
                    "label": 0
                },
                {
                    "sent": "Triple President Bush.",
                    "label": 0
                },
                {
                    "sent": "Ordered embargo, economic embargo ends on out of this we can generate one sentence summary of the whole document, which pretty nice reflects the what the document about.",
                    "label": 0
                },
                {
                    "sent": "But now if we.",
                    "label": 0
                },
                {
                    "sent": "So decrease the threshold, then other parts of the document start appearing so powerful Iraqi army seize buildings.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This was kind of relevant and that's why President Bush or that embargo Bush or deployment of troops.",
                    "label": 0
                },
                {
                    "sent": "This Side Story Turkey cut exports to Iraq and at the end the full document appears in front of us.",
                    "label": 0
                },
                {
                    "sent": "So we have this flexibility of playing with.",
                    "label": 0
                },
                {
                    "sent": "Document, which I guess in some parts of acts like will actually help us a lot.",
                    "label": 0
                },
                {
                    "sent": "Now let's check if Enricher came back.",
                    "label": 0
                },
                {
                    "sent": "Yeah it did.",
                    "label": 0
                },
                {
                    "sent": "So the document which was before in front of us.",
                    "label": 0
                },
                {
                    "sent": "Five this will work as well.",
                    "label": 0
                },
                {
                    "sent": "So before I said that Enricher creates the semantic graph, we call it semantic graph is.",
                    "label": 0
                },
                {
                    "sent": "Kind of semi semantic.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a graph of subject, predicate, object triples, which are which use.",
                    "label": 0
                },
                {
                    "sent": "So this is the document which we saw before.",
                    "label": 0
                },
                {
                    "sent": "So we can operate with this representation of graph as well.",
                    "label": 0
                },
                {
                    "sent": "So before I listed all these different representations of text, so this is 1 possible representation of text, not the only one that they said.",
                    "label": 0
                },
                {
                    "sent": "So this bag of words or vector space model is far the most used but not.",
                    "label": 0
                },
                {
                    "sent": "Just too shallow to extract certain things out of it.",
                    "label": 0
                },
                {
                    "sent": "And here now you will see a couple of more things.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 possible representation which we deal with.",
                    "label": 0
                },
                {
                    "sent": "But also what we do with this text which we put on the input.",
                    "label": 0
                },
                {
                    "sent": "So obviously we extract named entities.",
                    "label": 0
                },
                {
                    "sent": "This is not a big science, but with this name entities we do also the following.",
                    "label": 0
                },
                {
                    "sent": "So each name entity get this December grated into a couple of ontologies.",
                    "label": 0
                },
                {
                    "sent": "In this particular case we use.",
                    "label": 0
                },
                {
                    "sent": "DB Pedia so effectively.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia and.",
                    "label": 0
                },
                {
                    "sent": "Open, open Psych and Yahoo I think.",
                    "label": 0
                },
                {
                    "sent": "Do we have some more now?",
                    "label": 0
                },
                {
                    "sent": "Freebase is also let's say Brazil so Brazil.",
                    "label": 0
                },
                {
                    "sent": "If we go to Wikipedia, but we cannot.",
                    "label": 0
                },
                {
                    "sent": "We would see that the words Brazil actually means like country, obviously see T Brazil, then plenty of places towns are called Brazil as well.",
                    "label": 0
                },
                {
                    "sent": "Plenty of people have second name Brazil.",
                    "label": 0
                },
                {
                    "sent": "We have songs, movies, everything is Brazil.",
                    "label": 0
                },
                {
                    "sent": "Now which of these Brazil meanings of the word Brazil is the right one?",
                    "label": 0
                },
                {
                    "sent": "So in the context of here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's like.",
                    "label": 0
                },
                {
                    "sent": "Here so which Brazil did we mean here so and we perform this immigration.",
                    "label": 0
                },
                {
                    "sent": "Now we click this Brazil and we get links to different ontologies, so this would be.",
                    "label": 0
                },
                {
                    "sent": "The DPD I guess are.",
                    "label": 0
                },
                {
                    "sent": "This is Debbie Pedia.",
                    "label": 0
                },
                {
                    "sent": "This is freebase.",
                    "label": 0
                },
                {
                    "sent": "This is a New York Times as well.",
                    "label": 0
                },
                {
                    "sent": "Entity and Opencyc and couple of others.",
                    "label": 0
                },
                {
                    "sent": "So which is always the country.",
                    "label": 0
                },
                {
                    "sent": "So now what we achieved here.",
                    "label": 0
                },
                {
                    "sent": "So we know which entity we are talking about and we can properly contextualize it.",
                    "label": 0
                },
                {
                    "sent": "And let's say having the link into, let's say, psych.",
                    "label": 0
                },
                {
                    "sent": "Oh let's see if this will come.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is like the description of the of Brazil.",
                    "label": 0
                },
                {
                    "sent": "And here we see that.",
                    "label": 0
                },
                {
                    "sent": "This is actually a country, an independent country, which is further concept versus the largest combo.",
                    "label": 0
                },
                {
                    "sent": "And you know way more things now about Brazil as well.",
                    "label": 0
                },
                {
                    "sent": "And we get the whole semantic context of such an entity.",
                    "label": 0
                },
                {
                    "sent": "The same would be true if.",
                    "label": 0
                },
                {
                    "sent": "So if you take somebody which Robbie Keane, I'm sure it's some nice guy, but I don't know about him.",
                    "label": 0
                },
                {
                    "sent": "And here we see how OK Robbie Keane this is his free base.",
                    "label": 0
                },
                {
                    "sent": "ID where we would get the rest of the content and we would see also that it's person and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is this semantic semantic contextualization or an annotation of.",
                    "label": 0
                },
                {
                    "sent": "So the page didn't load yet properly I see, right?",
                    "label": 0
                },
                {
                    "sent": "So here we would get also a couple of more things about this document.",
                    "label": 0
                },
                {
                    "sent": "We would get also some topic categories, keywords which would be attached to this content.",
                    "label": 0
                },
                {
                    "sent": "We would in the new version we would get also sentiment.",
                    "label": 0
                },
                {
                    "sent": "As part of another project we have so very extract the sentiment sentiment on.",
                    "label": 0
                },
                {
                    "sent": "This is for entity on entity level or more like on the document level.",
                    "label": 0
                },
                {
                    "sent": "Hi, it's optimized for tweets at the moment because we need this for.",
                    "label": 0
                },
                {
                    "sent": "But in general this sentiment sentiment detection will also be part of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's try to.",
                    "label": 0
                },
                {
                    "sent": "See if this book come out.",
                    "label": 0
                },
                {
                    "sent": "So we have also, so I said this is web service really, so we generate the same output also in XML and also RDF or.",
                    "label": 0
                },
                {
                    "sent": "Ha, here we see.",
                    "label": 0
                },
                {
                    "sent": "So the whole document now.",
                    "label": 0
                },
                {
                    "sent": "Is described with this keyboard.",
                    "label": 0
                },
                {
                    "sent": "So sports, soccer and so on.",
                    "label": 0
                },
                {
                    "sent": "And these are categories from, let's say, demos.",
                    "label": 0
                },
                {
                    "sent": "This is Open directory project, so this is kind of summary topic topic level summary and at the bottom you will see.",
                    "label": 0
                },
                {
                    "sent": "So these are these are models which were triggered how much each of the models took.",
                    "label": 0
                },
                {
                    "sent": "These models are now we are adding them so.",
                    "label": 0
                },
                {
                    "sent": "But the point is somehow to have this one one big one week back web service with different modules.",
                    "label": 0
                },
                {
                    "sent": "So now for instance for X like would be ideal.",
                    "label": 0
                },
                {
                    "sent": "Now this operates in English, if we could.",
                    "label": 0
                },
                {
                    "sent": "Connected to UPC technology for different languages.",
                    "label": 0
                },
                {
                    "sent": "So this framework this would be for instance, from nice nice accomplishment of the work package, which one two I guess.",
                    "label": 0
                },
                {
                    "sent": "Or something along those lines.",
                    "label": 0
                },
                {
                    "sent": "So we need to talk about this.",
                    "label": 0
                },
                {
                    "sent": "OK, let's proceed now.",
                    "label": 0
                },
                {
                    "sent": "Oh so we saw.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mom.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discretion answer.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ring maybe we can.",
                    "label": 0
                },
                {
                    "sent": "We can do afterwards.",
                    "label": 0
                },
                {
                    "sent": "Now the next thing is, whatever was also mentioning.",
                    "label": 0
                },
                {
                    "sent": "So if you have let's say Chinese document in English document in Slovenian document, can we say if these documents talk about the same stuff or are they about different things?",
                    "label": 0
                },
                {
                    "sent": "So this model would go in the categorization are on this level of representation which I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Vector space model.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Level of representation, so it's more like shallow, but it can be very scalable and for a couple of applications can be very useful.",
                    "label": 0
                },
                {
                    "sent": "A part of this meta net project or an already before part of this.",
                    "label": 0
                },
                {
                    "sent": "Project Smart also in this language technology unit.",
                    "label": 0
                },
                {
                    "sent": "We develop this technology based on this.",
                    "label": 0
                },
                {
                    "sent": "CCA for this kernel Canonical correlation analysis, which produces some intermediate representation of languages where differently where we try to remove.",
                    "label": 0
                },
                {
                    "sent": "Specifics of particular language, so we try to generate language independent or language neutral document representation and young will say a few words about this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just quickly tell you about the multi view Canonical correlation analysis.",
                    "label": 0
                },
                {
                    "sent": "So the method that Mark mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the outline just introduction to Canonical correlation analysis and the generalization that can be used for cross lingual information retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first is just this vector space model that Marco mentioned.",
                    "label": 0
                },
                {
                    "sent": "So this is just one slide that is just showing that what we mean by that.",
                    "label": 0
                },
                {
                    "sent": "So we we transform documents into vectors so each each component of a vector corresponds to awards in a fixed vocabulary and the number in for each component is just the frequency of the this particular word in a document, so.",
                    "label": 0
                },
                {
                    "sent": "So this means that we discard some information about the structure of the document and we just keep some, let's say which which words are relevant for it, and by using a this engrams we can also keep some structure so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and once we have this representation, then we can simply compare documents in a single language by computing cosine similarity between documents.",
                    "label": 0
                },
                {
                    "sent": "So this is just an example of this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What the Canonical correlation analysis is a.",
                    "label": 0
                },
                {
                    "sent": "It's a it's.",
                    "label": 0
                },
                {
                    "sent": "We can look at it as a method that does dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "It is similar to a principal component analysis which is widely used for that purpose, and we can think of it as a method is a method that finds similar patterns in two sources of data.",
                    "label": 1
                },
                {
                    "sent": "So patterns that contains some mutual information and we need some training examples for this to work.",
                    "label": 0
                },
                {
                    "sent": "So it's a statistical approach that finds this constant.",
                    "label": 1
                },
                {
                    "sent": "This patterns so examples of pairs of sources in standard Canonical correlation analysis are, let's say, documents in two languages which we just saw, let's say images and text.",
                    "label": 1
                },
                {
                    "sent": "So this, and let's say fMRI scans and brain activity data so it can be, uh, it's a widely applicable method.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just throw it in.",
                    "label": 0
                },
                {
                    "sent": "Technically it boils down to solving an eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "So this is also very convenient because this technology has been very well developed so a lot of work has been done so so this is one thing and what it acts.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Julie, this is, let's say if we have two spaces of X&Y, so this we imagine these are documents, let's say English in German.",
                    "label": 0
                },
                {
                    "sent": "Each with their own dimension.",
                    "label": 0
                },
                {
                    "sent": "We will end.",
                    "label": 0
                },
                {
                    "sent": "This little characters represent.",
                    "label": 0
                },
                {
                    "sent": "So let's say point here and .8 there are.",
                    "label": 0
                },
                {
                    "sent": "We imagine these are documents that are translations of each other and the point is that we want to find the directions in the first language and direction in second language, so that when we project the documents into this space so we get correlated numbers.",
                    "label": 0
                },
                {
                    "sent": "So here we see the first 2 the most important.",
                    "label": 0
                },
                {
                    "sent": "The axis this in this space and this one in this space, and we also have a second set of axis which are, let's say, correspond to noise.",
                    "label": 0
                },
                {
                    "sent": "So this was.",
                    "label": 0
                },
                {
                    "sent": "This is what I meant with the dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So these are the pairs of documents and the method automatically finds this directions where the mutual information is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then what we did was to try to work on scaling this Canonical correlation to more than two views.",
                    "label": 1
                },
                {
                    "sent": "So and it's, uh, what you do is the generalization of that method.",
                    "label": 0
                },
                {
                    "sent": "So instead of, instead of finding directions that maximize the correlation, you find directions across all the languages that maximize the sum of all pairwise correlations.",
                    "label": 1
                },
                {
                    "sent": "So this means, let's say in text, we're looking for concepts.",
                    "label": 0
                },
                {
                    "sent": "In all the languages that mean the same thing, so they will have the same, let's say Co occurrence patterns.",
                    "label": 1
                },
                {
                    "sent": "So and once we find one set of this.",
                    "label": 0
                },
                {
                    "sent": "Is this vectors are just we think of them as different representations of the same semantic thing.",
                    "label": 0
                },
                {
                    "sent": "So language specific representations.",
                    "label": 0
                },
                {
                    "sent": "And once we find one set of this directions, we then proceed to finding another one that contains some new information.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the so we we have a method that scales roughly linearly in the size of the input data.",
                    "label": 1
                },
                {
                    "sent": "So it means that it can.",
                    "label": 0
                },
                {
                    "sent": "It can scale across a large number of documents.",
                    "label": 1
                },
                {
                    "sent": "It's exploit sparsity, and so the only the only thing is that there's a squared dependence on the number of these concepts that we wish to extract.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just an example of what what an output of the Methodist without the weights that are associated with the words.",
                    "label": 0
                },
                {
                    "sent": "So the first set, the first set, is a set of concept vectors.",
                    "label": 0
                },
                {
                    "sent": "Let's say in English amendment Russia, human rights and in German in German Ruslan to mention rector and so this is just these are concepts that are important for the corpus the multilingual corpus.",
                    "label": 0
                },
                {
                    "sent": "And the that are in, let's say, these are all representations that are language independent and the second set of concepts is orthogonal to the first one.",
                    "label": 0
                },
                {
                    "sent": "So it contains truly different information.",
                    "label": 0
                },
                {
                    "sent": "But it's also important for if we want to represent documents in the corpus.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can use this representation for multilingual search.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have documents X and in X space and documents in white space, and what this may this concepts that we found that this concepts enable us to map the all the points in X into this in a common representation space and also the documents in why so this mapping is actually just comparing the each document in X with with the.",
                    "label": 0
                },
                {
                    "sent": "Concept vectors that the method discovered and also for the wyview to you.",
                    "label": 0
                },
                {
                    "sent": "OM app the Y vectors to the corresponding representations.",
                    "label": 0
                },
                {
                    "sent": "So then you get something.",
                    "label": 0
                },
                {
                    "sent": "So representation in the common space and if you have a query, let's say in the X space, then you also project it and then just perform standard monolingual.",
                    "label": 0
                },
                {
                    "sent": "Methods to to do retrieval.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did.",
                    "label": 0
                },
                {
                    "sent": "Some experiments were based, so they were based on 8 or 10 European languages.",
                    "label": 0
                },
                {
                    "sent": "So this is based on your apparel European Parliament corpus.",
                    "label": 0
                },
                {
                    "sent": "And we did some stuff.",
                    "label": 0
                },
                {
                    "sent": "Standard information retrieval measures like mate retrieval.",
                    "label": 1
                },
                {
                    "sent": "And so the query mate retrieval and we.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared it to a cross lingual latent semantic analysis, which is a similar method similar in Spirit, except that there you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't keep this information which words come to from which language, you just concatenate everything and then perform the standard, let's say singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "And help.",
                    "label": 0
                },
                {
                    "sent": "You do it right.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Installation before you do the vectorization Department, practically no.",
                    "label": 0
                },
                {
                    "sent": "So we just take the so the data set.",
                    "label": 0
                },
                {
                    "sent": "Your apparel has sets of translations so we have.",
                    "label": 0
                },
                {
                    "sent": "We keep some for training and some for test.",
                    "label": 0
                },
                {
                    "sent": "So for training we find it in the training set.",
                    "label": 0
                },
                {
                    "sent": "We find this concept vectors in different representations and then apply them to the test set and then do retrieval.",
                    "label": 0
                },
                {
                    "sent": "Monolingual retrieval.",
                    "label": 0
                },
                {
                    "sent": "So no translations, so you take the role data.",
                    "label": 0
                },
                {
                    "sent": "The only piece of supervision which of having data is the fact that you have aligned documents to say well for for training phase.",
                    "label": 0
                },
                {
                    "sent": "So this document is translation of this one.",
                    "label": 0
                },
                {
                    "sent": "So this is the only bit of supervision.",
                    "label": 0
                },
                {
                    "sent": "On top of playing unstated.",
                    "label": 0
                },
                {
                    "sent": "So an alliance aligned corpus is the input to the method.",
                    "label": 0
                },
                {
                    "sent": "And in the case of this year Parliament we have, we have this in 20 something languages align set of documents across the.",
                    "label": 0
                },
                {
                    "sent": "So how many documents 100 thousands?",
                    "label": 0
                },
                {
                    "sent": "So here we.",
                    "label": 0
                },
                {
                    "sent": "So here we tested on 8000 and trained on 100,000 documents.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                },
                {
                    "sent": "It shows us that taking this information into account so not just concatenating everything it can help and it's also scalable so.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Say just few words about this recent experiment.",
                    "label": 0
                },
                {
                    "sent": "So recently we've also started working on Wikipedia, so where the the main goal is to try to learn this concept mapping on, let's say, 100 languages which are let's represented well, represented so that you have over 10,000 documents at least.",
                    "label": 0
                },
                {
                    "sent": "So in the goal is to now recently we've been training the this of extracting the concept vectors, which then enable us to compare documents from this 100 languages.",
                    "label": 0
                },
                {
                    "sent": "So this is something which is upcoming and potentially will have pretty good.",
                    "label": 0
                },
                {
                    "sent": "Impact, once it will function.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And blush, maybe you can, so this is another topic which is related to acts like, so the context is the following.",
                    "label": 0
                },
                {
                    "sent": "So news are being published.",
                    "label": 0
                },
                {
                    "sent": "Round the world.",
                    "label": 0
                },
                {
                    "sent": "So if an event happens.",
                    "label": 0
                },
                {
                    "sent": "Many new sources talk about this event and but every new source has its own bias.",
                    "label": 0
                },
                {
                    "sent": "Now that this is 1 pre experiment in a way how to model the news bias and in X like we will try to go beyond beyond this experiment.",
                    "label": 0
                },
                {
                    "sent": "But certainly this was pretty good insight into the problem.",
                    "label": 0
                },
                {
                    "sent": "So here is a couple of slides on the experiment.",
                    "label": 0
                },
                {
                    "sent": "So news bias.",
                    "label": 0
                },
                {
                    "sent": "So here you are.",
                    "label": 0
                },
                {
                    "sent": "Christian couple of new sources in an English language, as Marco said, so in the context of weeks like we would like to extend this to foster more new sources and also to cover more languages.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is 1 example of news bias, so you can see the articles talking about the same news at the same event, but they're clearly having different opinions about the event that a particle is from CNN and the bottom article is from Aljazeera English.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was the datasets that we were using, so we had four new sources, Aljazeera CNN Detroit News and International Herald Tribune.",
                    "label": 0
                },
                {
                    "sent": "So two kind of global coverage newspapers, one more local newspaper for US and one more focused on Middle East.",
                    "label": 0
                },
                {
                    "sent": "And we were calling for one year or all the news from international section.",
                    "label": 0
                },
                {
                    "sent": "And so this is the top part.",
                    "label": 0
                },
                {
                    "sent": "Here is the actual draw data.",
                    "label": 0
                },
                {
                    "sent": "Then in the second step we found the matching news articles.",
                    "label": 0
                },
                {
                    "sent": "So for each part of new sources we identified the news articles which talked about the same events and in this matrix you can see the overlaps and you can see for example, that was expected, CNN and International Herald Tribune would have the highest overlap with pleasure, and Detroit News have the smallest overlap and matching was done based on the.",
                    "label": 0
                },
                {
                    "sent": "Similarity of the articles and based on the time I guess I'm entities mentioned and so couple of holistics together and we were more focused on not on recall.",
                    "label": 0
                },
                {
                    "sent": "So not trying to identify all the pairs but on the precision and these pairs that are identified were quite accurate.",
                    "label": 0
                },
                {
                    "sent": "More than 95% precision.",
                    "label": 0
                },
                {
                    "sent": "So if the articles were identified to be about the same event, if are most likely about the same event.",
                    "label": 0
                },
                {
                    "sent": "This is quite important 'cause.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next step we were we did experiment, so we try to use this data to learn predictive model to tell whether the source of new articles.",
                    "label": 0
                },
                {
                    "sent": "So if you get a new article, can we tell whether CNN wrote about it or Aljazeera wrote about it?",
                    "label": 0
                },
                {
                    "sent": "Just looking at the words in the article and if this is possible, this would mean that there's some bias in the vocabulary of the new source that reveals the which of the which one it is.",
                    "label": 0
                },
                {
                    "sent": "And it turned out.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Doctor number here.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turned out there was around 80% accuracy when you try to predict the new source between CNN and Aljazeera.",
                    "label": 0
                },
                {
                    "sent": "And for example if you look at the model so.",
                    "label": 0
                },
                {
                    "sent": "But you used was a SVM linear model and you can check the words that had the highest rates and the most significant keywords for CNN would be, so they would write about insurgents about terrorists, militants, suicide.",
                    "label": 0
                },
                {
                    "sent": "So on various Aljazeera would talk about rebels resistance and fighters, so quite different.",
                    "label": 0
                },
                {
                    "sent": "Point of view.",
                    "label": 0
                },
                {
                    "sent": "So what we plan to do in it's like, is to use this extended.",
                    "label": 0
                },
                {
                    "sent": "So here we were only focused on the vocabulary so quite basic representation.",
                    "label": 0
                },
                {
                    "sent": "We also did another set where we're comparing the new sources based on the coverage, but we would like to extend it to include also coverage of entities and some other things that will be extracted from this multilingual new sources and this island of formal knowledge.",
                    "label": 0
                },
                {
                    "sent": "OK, so this news bias and this diversity, which happens.",
                    "label": 0
                },
                {
                    "sent": "So we have one project on this topic called Render together with calls through.",
                    "label": 0
                },
                {
                    "sent": "And lots of these things happened there, but we don't have cross lingual technology there, so there is more like monolingual and extending this.",
                    "label": 0
                },
                {
                    "sent": "News bias problem also towards.",
                    "label": 0
                },
                {
                    "sent": "Cross lingual would be actually pretty cool, so there's something what Evan was mentioning before, like if.",
                    "label": 0
                },
                {
                    "sent": "Go to New York Times cites.",
                    "label": 0
                },
                {
                    "sent": "See this completely unbiased, professionally written news from New York Times, which we all know is the ground truth.",
                    "label": 0
                },
                {
                    "sent": "And then you will have bottom.",
                    "label": 0
                },
                {
                    "sent": "So the evil ones and the thread.",
                    "label": 0
                },
                {
                    "sent": "The eager ones, and you would see what other people are actually writing related to the same stuff.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to proceed so we have.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 demo which actually was prepared a couple of years ago for New York Times, but Evan doesn't even know that.",
                    "label": 0
                },
                {
                    "sent": "Which we are showing now all this year.",
                    "label": 0
                },
                {
                    "sent": "So it operates on the Reuters corpus, the one which was mentioned by Evan.",
                    "label": 0
                },
                {
                    "sent": "And so it's one year of news, 830,000 news stories, and so this is them.",
                    "label": 0
                },
                {
                    "sent": "Or how to show the?",
                    "label": 0
                },
                {
                    "sent": "Search results in different ways, such as us from news.",
                    "label": 0
                },
                {
                    "sent": "So here will make a search for Clinton.",
                    "label": 0
                },
                {
                    "sent": "And we get set of results on the left, just second.",
                    "label": 0
                },
                {
                    "sent": "First query always takes a little bit more time.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are set of new stories from this corpus, which which mentioned cleaned and so there are quite some of them.",
                    "label": 0
                },
                {
                    "sent": "So on the right side we have this topic.",
                    "label": 0
                },
                {
                    "sent": "This is 1 visualization of the content.",
                    "label": 0
                },
                {
                    "sent": "So each cross here is 1 new story and crosses are closer in the picture.",
                    "label": 0
                },
                {
                    "sent": "If they talk about the same stuff.",
                    "label": 0
                },
                {
                    "sent": "So let's say here we have one cluster.",
                    "label": 0
                },
                {
                    "sent": "With this circle, we put the context and this small window shows the keywords, which are the most relevant for this cluster.",
                    "label": 0
                },
                {
                    "sent": "So here we see.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's Middle East crisis cluster, so if we would go here would be more like election campaign.",
                    "label": 0
                },
                {
                    "sent": "This would be budget discussion.",
                    "label": 0
                },
                {
                    "sent": "This is more like economic discussions where Clinton was mentioned.",
                    "label": 0
                },
                {
                    "sent": "So this is topic view here we have also so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should you wear, we extract names of people and we cluster them.",
                    "label": 0
                },
                {
                    "sent": "So this is another view on the same set of results here on the left and the third view is.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How the?",
                    "label": 0
                },
                {
                    "sent": "How the dynamics of these stories happened?",
                    "label": 0
                },
                {
                    "sent": "So you see, for instance, the first story, which is about the election, was very active in the first part, but then disappeared and discussion about the discussion about.",
                    "label": 0
                },
                {
                    "sent": "Budget started and so on.",
                    "label": 0
                },
                {
                    "sent": "So we've we would put some other let's say Tomba.",
                    "label": 0
                },
                {
                    "sent": "Remember Hutama was skier so he was relevant only in the winter received.",
                    "label": 0
                },
                {
                    "sent": "So in the winter months and then he disappeared from the news.",
                    "label": 0
                },
                {
                    "sent": "So if we would have like influenza.",
                    "label": 0
                },
                {
                    "sent": "We would see that there were obviously two relevant epidemics that year, so one here one here.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 view.",
                    "label": 0
                },
                {
                    "sent": "How so?",
                    "label": 0
                },
                {
                    "sent": "This temporal view of the of the of the topics which we can.",
                    "label": 0
                },
                {
                    "sent": "See through such a way this is not optimal, was more like optimally done.",
                    "label": 0
                },
                {
                    "sent": "It's more like a prototype but but the idea is here somehow too.",
                    "label": 0
                },
                {
                    "sent": "So to go in this direction to have this alternative views to the same corpus of the content.",
                    "label": 0
                },
                {
                    "sent": "OK, just to continue.",
                    "label": 0
                },
                {
                    "sent": "Oh I'm hot.",
                    "label": 0
                },
                {
                    "sent": "So these are actually two a couple of pictures from New York Times or.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aircraft for instance.",
                    "label": 0
                },
                {
                    "sent": "This is Pearl Harbor from from the 15 million or big corpus, so Pearl Harbor visualized in that tool.",
                    "label": 0
                },
                {
                    "sent": "So we see that practically didn't was almost not mentioned before.",
                    "label": 0
                },
                {
                    "sent": "41 When the attack happened, then it was lots of news and these are kind of clusters of topics.",
                    "label": 0
                },
                {
                    "sent": "Now the interface should really support also drill down a couple of other things.",
                    "label": 0
                },
                {
                    "sent": "But or.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Belgrade, for instance, in the Second World War.",
                    "label": 0
                },
                {
                    "sent": "So you see, there was a big hype in New York Times, so mentioning Belgrade a lot.",
                    "label": 0
                },
                {
                    "sent": "And then when the actual attack happened, so this will not April 6 then it all disappeared from the news because it was so much other things going on, I guess.",
                    "label": 0
                },
                {
                    "sent": "And these discussions here where if you analyze these keywords you would see that it's discussion whether you Slavia back then will join the pact with Nazis or not.",
                    "label": 0
                },
                {
                    "sent": "So this is this is another reason.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and, let's say Normandy you see before Second World War was mentioned as.",
                    "label": 0
                },
                {
                    "sent": "About sailing somewhere like tree lights with topics and then disappeared from the news until the text happens.",
                    "label": 0
                },
                {
                    "sent": "This is one couple of snapshots.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New York Times corpus I have just won a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is a topic.",
                    "label": 0
                },
                {
                    "sent": "Which should be really presented by by by Meteor Trampusch, but he had to leave before.",
                    "label": 0
                },
                {
                    "sent": "I will just extract this.",
                    "label": 0
                },
                {
                    "sent": "So this is something which is sort of related to to X.",
                    "label": 0
                },
                {
                    "sent": "Like a lot we have.",
                    "label": 0
                },
                {
                    "sent": "Oh couple of more like preliminary experiments on this.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we go to Google News.",
                    "label": 0
                },
                {
                    "sent": "If you go to Google News and.",
                    "label": 0
                },
                {
                    "sent": "I will just.",
                    "label": 0
                },
                {
                    "sent": "Tell you about the intuition.",
                    "label": 0
                },
                {
                    "sent": "We got just the fresh Google News we would see.",
                    "label": 0
                },
                {
                    "sent": "There are plenty of this clusters of topics.",
                    "label": 0
                },
                {
                    "sent": "So OK, the top news at the moment is Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Google protests US anti piracy proposals nice.",
                    "label": 0
                },
                {
                    "sent": "And ABC News.",
                    "label": 0
                },
                {
                    "sent": "Top Guardian latimes mention Fox News.",
                    "label": 0
                },
                {
                    "sent": "Usually some of these more influential news agencies and then it's written here OK. 3000 more.",
                    "label": 0
                },
                {
                    "sent": "2400 more sources.",
                    "label": 0
                },
                {
                    "sent": "Nice but actually.",
                    "label": 0
                },
                {
                    "sent": "Now the intuition is the following.",
                    "label": 0
                },
                {
                    "sent": "Current language technology tools as we know they are not perfect so we cannot extract very accurately from one single document.",
                    "label": 0
                },
                {
                    "sent": "Everything what we would like to.",
                    "label": 0
                },
                {
                    "sent": "But if we have let's say 10s or hundreds or even thousands of articles talking about the same topic, we generally should be able to extract these bits and pieces from this redundancy.",
                    "label": 0
                },
                {
                    "sent": "Which appears because of this alternative ways of expressing by different publishers.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea and.",
                    "label": 0
                },
                {
                    "sent": "Now going back to the slides.",
                    "label": 0
                },
                {
                    "sent": "So here we have this big set of articles.",
                    "label": 0
                },
                {
                    "sent": "We clustered them into the stories we create.",
                    "label": 0
                },
                {
                    "sent": "These graphs which I showed before in richer and then we merge the graphs and at the end these stories pop out in very accurate way.",
                    "label": 0
                },
                {
                    "sent": "So here is 1 example.",
                    "label": 0
                },
                {
                    "sent": "Of so Media was doing this experiment.",
                    "label": 0
                },
                {
                    "sent": "I think there's some numbers.",
                    "label": 0
                },
                {
                    "sent": "So this was set of events on.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "On some suicide bombers.",
                    "label": 0
                },
                {
                    "sent": "And so out of multiple articles we extracted this kind of template.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What actually happened?",
                    "label": 0
                },
                {
                    "sent": "You see that different documents are reconfirming certain parts of this graph, and some parts are not so frequent.",
                    "label": 0
                },
                {
                    "sent": "So and through this multiple views into the onto the same event, basically we can extract.",
                    "label": 0
                },
                {
                    "sent": "Tori and well even more so we can also extract the template of the story so.",
                    "label": 0
                },
                {
                    "sent": "I'm showing now actually the this other extracting the templates, which is roughly the same.",
                    "label": 0
                },
                {
                    "sent": "The same idea.",
                    "label": 0
                },
                {
                    "sent": "So so for instance, these are two stories, this time about some.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Again, some killing basically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So Don Vito was eating an Apple.",
                    "label": 0
                },
                {
                    "sent": "He shot down Amelia.",
                    "label": 0
                },
                {
                    "sent": "Sony was helping and on Amelia was that an Sony was wearing sunglasses obviously and he was wearing a knife so this is very typical story.",
                    "label": 0
                },
                {
                    "sent": "We have another story where police arrested Jill.",
                    "label": 0
                },
                {
                    "sent": "Manjil Blast kills Sony.",
                    "label": 0
                },
                {
                    "sent": "Sony was death.",
                    "label": 0
                },
                {
                    "sent": "Sony saw her gun Jack Carey Brandy and so you see, this is a story which can be extracted from from enricher pretty much in this form.",
                    "label": 0
                },
                {
                    "sent": "Now we have two stories of the same kind, although they talk about two different events.",
                    "label": 0
                },
                {
                    "sent": "Now, the point is that we would like to match this.",
                    "label": 0
                },
                {
                    "sent": "These notes inappropriate way.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So at the end, yeah, so at the end, at the end we would have matching like this so gun would match knife.",
                    "label": 0
                },
                {
                    "sent": "Sony would match Jack, Don, Vito, Jill.",
                    "label": 0
                },
                {
                    "sent": "And on the media, Sony in this case.",
                    "label": 0
                },
                {
                    "sent": "And out of this, this type of information we can extract.",
                    "label": 0
                },
                {
                    "sent": "Extract story templates.",
                    "label": 0
                },
                {
                    "sent": "So now here we have.",
                    "label": 0
                },
                {
                    "sent": "The experiment was 7000 articles from Google News, 200 stories and 10 different topics.",
                    "label": 0
                },
                {
                    "sent": "So 10 stories for topic roughly.",
                    "label": 0
                },
                {
                    "sent": "So this would be a template for suicide bombing, so we always have suicide bomber.",
                    "label": 0
                },
                {
                    "sent": "He blew himself.",
                    "label": 0
                },
                {
                    "sent": "On some location it's always location.",
                    "label": 0
                },
                {
                    "sent": "There always was that one.",
                    "label": 0
                },
                {
                    "sent": "I think explosive.",
                    "label": 0
                },
                {
                    "sent": "He blew an organization, blew himself as well and he killed people.",
                    "label": 0
                },
                {
                    "sent": "So this is a template of these kind of stories and this came out automatically out of the same would be so person.",
                    "label": 0
                },
                {
                    "sent": "Sentence for certain number of years pleaded guilty and so I don't know this.",
                    "label": 0
                },
                {
                    "sent": "Was this telling really mean?",
                    "label": 0
                },
                {
                    "sent": "So this is this is towards the end of X like pipeline.",
                    "label": 0
                },
                {
                    "sent": "The idea would be to extract this kind of templates out of the stories and then this will be then sort of imposed on different languages.",
                    "label": 0
                },
                {
                    "sent": "Different languages would share this kind of template, they just express themselves in different ways.",
                    "label": 0
                },
                {
                    "sent": "So this would be.",
                    "label": 0
                },
                {
                    "sent": "One quick.",
                    "label": 0
                },
                {
                    "sent": "Explanation of this?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is more like sneaking more like preview of the stuff which we are working in the back, but this is far from mature.",
                    "label": 0
                },
                {
                    "sent": "And this kind of templates could be obviously not linked also to let's say.",
                    "label": 0
                },
                {
                    "sent": "Event or semantic templates in psych or in some other ontologies where we would actually get extra information out of the.",
                    "label": 0
                },
                {
                    "sent": "Data OK, so much from.",
                    "label": 0
                },
                {
                    "sent": "Mind my sites.",
                    "label": 0
                },
                {
                    "sent": "And we can continue with Michael I guess right?",
                    "label": 0
                }
            ]
        }
    }
}