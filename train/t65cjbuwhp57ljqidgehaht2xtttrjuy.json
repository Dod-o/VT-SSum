{
    "id": "t65cjbuwhp57ljqidgehaht2xtttrjuy",
    "title": "Gradient Descent with Sparsi\ufb01cation: An Iterative Algorithm for Sparse Recovery with Restricted Isometry Property",
    "info": {
        "author": [
            "Rahul Garg, IBM Thomas J. Watson Research Center"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/icml09_garg_gdws/",
    "segmentation": [
        [
            "So finally we are all set.",
            "Sorry for the startup glitches.",
            "In this talk.",
            "We have a pointer.",
            "So in this talk I'll talk about gradient descent with sparsification and iterative algorithm for sparse recovery using restricted isomorphic property.",
            "I'll begin with the."
        ],
        [
            "Translation problem and the role of L1 minimization in finding this postulation followed by the algorithm grades.",
            "The gradient descent with sparsification.",
            "And then why does it converge?",
            "And then I'll."
        ],
        [
            "Proof, do the proof and then experimental results.",
            "OK, so we are given a vector Y in.",
            "In our in an matrix file, the sparse regression problem and an error E and the sparse regression problem is to find.",
            "Vector X, which is closed such that Phi X is closed.",
            "Why an excess?",
            "Smallest number of nonzero components?"
        ],
        [
            "OK.",
            "So for some reason the animation is not working, but let's stick to that.",
            "OK, so let's just plastic every problem.",
            "Need to put in presentation.",
            "Yeah, so I'm seeing that here but not here.",
            "Don't spend some time on that.",
            "You don't have much, so this is an important problem in machine learning.",
            "And here what we have is given a set of observations and the response variable in the objective is to find the smallest number of features that explain the observation and image deconvolution and denoising.",
            "We're again given a measurement of an image convert with transfer system transfer function with some noise.",
            "And what we want to do is to find the best text that matches the set of data.",
            "Similarly, in sparse PCA, we're given a matrix and you want to find the PCA such that each of the components of the decomposition is past and there is recent activity in the field of compressed sensing.",
            "A lot of activity and the main results there.",
            "Being that you can sample a signal at a rate which is less than Nyquist rate, provided that the signal is.",
            "Spas in some domain.",
            "So here we are given a measurement of X using the matrix Phi and the goal is to reconstruct X given YN 5."
        ],
        [
            "So.",
            "There have been a lot of combinatorial algorithms proposed to solve this problem, like matching pursuit, forward selection and all.",
            "But the problem is shown to be NP hard and not only it's NP hard.",
            "It's hard to approximate, so although these algorithms exist, but we don't know whether they actually find the correct solution or not.",
            "So there's been some activity or a lot of activity around."
        ],
        [
            "One minimization and what has been shown is that by solving the following element relaxation to the program, we actually find a good sparse solution.",
            "So what you do is.",
            "Minimize the L1 norm of X subject to the constraint that firefix is close to Y and this has been shown to work well in practice in a wide variety of applications, and in fact this is sort of accepted wisdom that whenever you want to find a sparse solution, you apply L1 penalty to the objective function and the justification of it comes from like.",
            "Geometrical arguments that says that if we have a space which is quadratic or which is convex but kind of smooth at the boundaries such that this equation and we have an L1 objective, so we have an album ball which we shrink, we minimize the L1, so it's very likely that this ellipse will intersect this Squire this time and at the corner points and which happened to be this path.",
            "So that's the."
        ],
        [
            "Geometry, but there have been some recent results that show that in addition to this intuition, in fact, you can actually prove that the elegant optimization finds the correct solution and.",
            "The basic.",
            "Progress comes from the definition of what is called a restricted isometry property.",
            "So given a matrix 5, define the isometry constant Delta is to be the smallest number such that when you transform X using five, than all of this quantity does not change.",
            "Too much is approximately close to the normal eggs for all S sparse vector X.",
            "So if you have a sparse vector X and you apply fire to it, the norm does not change too much, so that's what is called.",
            "The stricted isometry."
        ],
        [
            "Property.",
            "And what we have is the results show that if D2 S is less than sqrt 2 -- 1, then the L1 optimization actually finds the correct solution when equal to 0 and in case is not equal to 0 then it finds very close to the correct optimal solution.",
            "And this property, some people might say that this is very strong, but it turns out that a lot of matrices, in fact almost all of the random matrices with appropriate sizes who satisfy this property, and in fact random Fourier matrix is also satisfy this property.",
            "And deterministic constructions people are coming up with deterministic constructions which satisfy this property, and because of this what we have is universal encoding strategy.",
            "So you have a sparse signal in any domain.",
            "What you can do?",
            "Is sample it using random measurements in using a very small number of random measurements and then you can reconstruct the sparse signal data.",
            "So this has led to a lot of applications, specially in the image reconstruction domain.",
            "But the problem is relevant.",
            "Minimization is that the algorithm is very slow if you want to solve it in general, you need to do an LP, and that's extremely slow.",
            "People have designed some efficient algorithms like laws, but they are still slow and to give a idea of the magnitude, let's say we're talking about MRI imaging and it typical 1 millimeter kind of image is of the size 256 cube, which correspond to 16,000,000 rows in 16,000,000 columns.",
            "And when we're talking about his past solution, we're talking about something like 100,000 to 1,000,000.",
            "Number of sparse components.",
            "So on problems of these sizes, LP and these algorithms just fail.",
            "They cannot really work, and people still adopt combinatorial algorithms that somehow give you some solution, but they're not probably optimum, and there is a recent activity on combinatorial algae."
        ],
        [
            "Terms that actually show optimality in certain conditions, so this is like the summary of the algorithms known so far.",
            "Some of the algorithms.",
            "And on the top we have the L1 optimization algorithms and the time taken is order M plus in cube.",
            "And that basically means that for the images of the size MRI we're talking about 10 to 21 and 14 absolutely not useful.",
            "And there are some algorithms that are iterative.",
            "Some of them take constant number of iterations, some of them take number of iteration equal to the sparsity and still the runtimes here of the same are of the order of 10 report 13.",
            "So in this paper or what I'm going to describe is an algorithm that we call grades an it.",
            "At it's an iterative algorithm.",
            "In every iteration the cost is K, which is the time to carry out matrix vector product, and if you're dealing with MRI type matrices then you don't really have to do that.",
            "You can do it through Fourier transform, so it's not necessarily M * N. It can be much more efficient than that, and the number of iterations that you need is constant.",
            "It only depends on the precision.",
            "It does not depend on the number of non zero variables.",
            "And then it can probably.",
            "Reconstruct or recover the actual solution, provided we have the following conditions on the on the isometry constant, and this condition is very close to the condition that the best condition that we have under which any algorithm is known, which is quite 2 -- 1 which is about .4 and there's about .33.",
            "And the time taking here becomes 10 to the Power 8, which is very much in the domain that we can do it."
        ],
        [
            "So the algorithm is a combinatorial algorithm.",
            "It starts with the.",
            "Initial solution in iteratively refines it using matrix vector product and the number of iterations is constant.",
            "It does not depend on the sparsity, and basically it's a very simple algorithm.",
            "It just moves along the gradient and then it does the sparsification.",
            "And it needs matrices satisfying rapo, and in that case."
        ],
        [
            "It does find the correct solution.",
            "So what what is happened?",
            "Initially we wanted to solve this past regression problem and we had this combinatorial algorithm with no guarantees to converge.",
            "Then we have the NP hardness results that show that you cannot really probably converge.",
            "It's a hard problem.",
            "Then we have L1 minimization as a heuristic which used to work well in practice.",
            "But people didn't know why.",
            "And then the recent sort of result in compressed sensing that actually proved that if the matrix satisfies certain properties then you do converge to the optimum.",
            "And now we're going to a domain where again we have combinatorial algorithms that probably converge to the correct solution with the restricted isometry assumption, and what's happening now is that we're also seeing probably correct combinatorial algorithm which.",
            "Which which actually converge to the solution with assumptions weaker than our IP or more general assumptions.",
            "So we have come full circle starting from the combinatorial."
        ],
        [
            "The returns and back to the combinatorial algorithms, but we've learned a lot in the process, so this algorithm starts with X equal to 0.",
            "It iteratively moves around the gradient and spa specifies the solution, and that's the full algorithm.",
            "So you start with an X.",
            "You compute the residue, which is a difference from Y.",
            "That's the residue.",
            "Another gradient is defined by minus 2 * 5.",
            "Transpose the residue again matrix will product.",
            "And then what you do is you move along the gradient with the weighting factor 2 by gamma.",
            "I'm going to tell what the gamma is in the moment and after this what you do is take the largest as components in the magnitude.",
            "So it's extremely simple algorithm."
        ],
        [
            "And it's guaranteed to converge.",
            "So what we have is.",
            "If D2 X is less than one by three and Y is equal to 5X for a sparse vector X star, then in K iterations the solution.",
            "Is within two D2 X by 1 minus Delta 2 S times K to the power K. So basically at every iteration the solution, the algorithm decreases the error by a constant multiplicative factor, so therefore in constant number of iterations it actually gets very close to the optimum solution."
        ],
        [
            "And we have the result for the noise case also.",
            "And here what we have is it finds a solution which is within a factor D of the optimum where the error is in the factor D and then again converges in small number of iterations.",
            "Constant number of iterations and this is very compareable to the L1 optimization results of Candace in Tau."
        ],
        [
            "So why does it work?",
            "What happens is we define a potential which is y -- 5.",
            "X and.",
            "You can think of it as wise fight and sex stars, so that's a potential.",
            "Now if the vectors X and extras pause, then using our IP there about.",
            "The normal fixed minus X star.",
            "So now if we look at the gradient of five gradient of PSI of X, it is given by this quantity and if you start from X and move along Delta X, the change in the function is given by the gradient times Delta X plus this Hessian term.",
            "The 2nd order term."
        ],
        [
            "And that's going to be critical here.",
            "So now what we want to do is, if given any solution, we want to find a Delta X that minimizes this, and this term is composed of the two terms.",
            "The first term is separable, so it's like each component of GI times the component of exile, so you can optimize this very easily, but this is a non separable term because it has interactions and that's the hard part.",
            "But what our IP gives is decomposes this into.",
            "One plus minus Delta times Delta X and now it becomes separable.",
            "So now if you optimize this you can represent it.",
            "Just in terms of."
        ],
        [
            "The gradients, so essentially.",
            "The change in five if you go to an optimum solution is given by the gradient times the chain optimum change plus one minus Delta.",
            "That's abound and hear the bound because one plus Delta.",
            "If we go along another direction, and insight is that instead of minimizing this, we minimize this an using this.",
            "What you have is basically Delta X is a gradient times one plus Delta and it works.",
            "After thresholding.",
            "I had some nice animation here.",
            "Sorry, you cannot see that, but.",
            "It gives the current solution."
        ],
        [
            "How does it perform well in practice?",
            "We implemented it MATLAB and then we compared a bunch of algorithms."
        ],
        [
            "And this graph shows how the runtime is the function of number of rules.",
            "And this is the L1 based algorithms Lasso.",
            "And wimpy, which is jacks, one component at a time.",
            "These are the newer algorithms and this is the grades.",
            "In fact, great experiment parameterized by the parameter gamma and what we have is the result for gamma equal to four by three.",
            "But in practice what we found is that if you have if you move slowly along the gradient, then you actually have better positive property, although the convergence is bit slower.",
            "So here the runtime is almost constant with respect.",
            "Number of rows whereas all the other algorithms are in time increases and this difference actually becomes more and more as you want to bigger and bigger."
        ],
        [
            "Metric sizes problem sizes.",
            "Similarly with number of columns, the trend here is linear.",
            "Here it is super linear, and here again it's linear and you see an order of magnitude difference in the runtimes here.",
            "What's positive?"
        ],
        [
            "Again, the great algorithm has a flat curve.",
            "It does not depend on the sparsity, whereas these algorithms depend on the sparsity, and for these algorithms, actually in theory the runtime should be independent of this positive.",
            "But what turns out is that the number of iterations they need to converge actually increases as you have more number of components.",
            "As this positives increased, so you see this kind of a behavior in practice, whereas the grades actually.",
            "Is pretty flat.",
            "In terms of recovery properties, what?"
        ],
        [
            "We have is in fact last year the best guarantees and this my last match and great actually does better than Lasso.",
            "And here's the comparison with some other algorithms.",
            "And it turns out that the L1 implementations are numerically unstable, and that's why you really don't get the good solutions here.",
            "Turn to conclude."
        ],
        [
            "Sparse regression is an important problem.",
            "It's very much applicable in compressed sensing as well as machine learning, feature selection and lot of other areas, and what we have is a fast algorithm that probably works under the restricted isometry property assumptions and it needs a constant number of iterations independent of the sparsity, and it seems to work well in practice also.",
            "Stop your questions."
        ],
        [
            "You have questions, yes.",
            "What is the difference between new organisms and the iterative artificial human?",
            "Said yeah, so it's pretty much like the same question.",
            "So the question is, what is the difference between this algorithm and the iterative hard thresholding algorithm?",
            "The iterative hard thresholding is kind of a special case of this algorithm.",
            "Here the algorithm is parameterized by the parameter gamma.",
            "How fast you move along the gradient an.",
            "Is there so?",
            "Then it's identical.",
            "It's the same algorithm, except that we have a proof that it actually converges when the gamma is full by three.",
            "Put it in there.",
            "Is the automatic, yeah?",
            "So the algorithm in that case algorithm is the same, but the analysis shows.",
            "One more question is eventually.",
            "It.",
            "Typical testing results within C and see where he is doing similar things sampling.",
            "He won't explicitly stating things O and it may be that this result we, well, exactly you will copy so well.",
            "Interesting look at what is the casting version of your own.",
            "You sometimes we have some medical bills.",
            "Yeah, I mean we have not tried the stochastic version here, so yeah, so good, good comment.",
            "OK, we don't have more time for next time the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally we are all set.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the startup glitches.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "We have a pointer.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll talk about gradient descent with sparsification and iterative algorithm for sparse recovery using restricted isomorphic property.",
                    "label": 1
                },
                {
                    "sent": "I'll begin with the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Translation problem and the role of L1 minimization in finding this postulation followed by the algorithm grades.",
                    "label": 1
                },
                {
                    "sent": "The gradient descent with sparsification.",
                    "label": 0
                },
                {
                    "sent": "And then why does it converge?",
                    "label": 0
                },
                {
                    "sent": "And then I'll.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proof, do the proof and then experimental results.",
                    "label": 0
                },
                {
                    "sent": "OK, so we are given a vector Y in.",
                    "label": 1
                },
                {
                    "sent": "In our in an matrix file, the sparse regression problem and an error E and the sparse regression problem is to find.",
                    "label": 0
                },
                {
                    "sent": "Vector X, which is closed such that Phi X is closed.",
                    "label": 0
                },
                {
                    "sent": "Why an excess?",
                    "label": 0
                },
                {
                    "sent": "Smallest number of nonzero components?",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So for some reason the animation is not working, but let's stick to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just plastic every problem.",
                    "label": 0
                },
                {
                    "sent": "Need to put in presentation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm seeing that here but not here.",
                    "label": 0
                },
                {
                    "sent": "Don't spend some time on that.",
                    "label": 0
                },
                {
                    "sent": "You don't have much, so this is an important problem in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And here what we have is given a set of observations and the response variable in the objective is to find the smallest number of features that explain the observation and image deconvolution and denoising.",
                    "label": 1
                },
                {
                    "sent": "We're again given a measurement of an image convert with transfer system transfer function with some noise.",
                    "label": 0
                },
                {
                    "sent": "And what we want to do is to find the best text that matches the set of data.",
                    "label": 0
                },
                {
                    "sent": "Similarly, in sparse PCA, we're given a matrix and you want to find the PCA such that each of the components of the decomposition is past and there is recent activity in the field of compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "A lot of activity and the main results there.",
                    "label": 0
                },
                {
                    "sent": "Being that you can sample a signal at a rate which is less than Nyquist rate, provided that the signal is.",
                    "label": 0
                },
                {
                    "sent": "Spas in some domain.",
                    "label": 1
                },
                {
                    "sent": "So here we are given a measurement of X using the matrix Phi and the goal is to reconstruct X given YN 5.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There have been a lot of combinatorial algorithms proposed to solve this problem, like matching pursuit, forward selection and all.",
                    "label": 1
                },
                {
                    "sent": "But the problem is shown to be NP hard and not only it's NP hard.",
                    "label": 1
                },
                {
                    "sent": "It's hard to approximate, so although these algorithms exist, but we don't know whether they actually find the correct solution or not.",
                    "label": 0
                },
                {
                    "sent": "So there's been some activity or a lot of activity around.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One minimization and what has been shown is that by solving the following element relaxation to the program, we actually find a good sparse solution.",
                    "label": 1
                },
                {
                    "sent": "So what you do is.",
                    "label": 0
                },
                {
                    "sent": "Minimize the L1 norm of X subject to the constraint that firefix is close to Y and this has been shown to work well in practice in a wide variety of applications, and in fact this is sort of accepted wisdom that whenever you want to find a sparse solution, you apply L1 penalty to the objective function and the justification of it comes from like.",
                    "label": 0
                },
                {
                    "sent": "Geometrical arguments that says that if we have a space which is quadratic or which is convex but kind of smooth at the boundaries such that this equation and we have an L1 objective, so we have an album ball which we shrink, we minimize the L1, so it's very likely that this ellipse will intersect this Squire this time and at the corner points and which happened to be this path.",
                    "label": 0
                },
                {
                    "sent": "So that's the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Geometry, but there have been some recent results that show that in addition to this intuition, in fact, you can actually prove that the elegant optimization finds the correct solution and.",
                    "label": 0
                },
                {
                    "sent": "The basic.",
                    "label": 0
                },
                {
                    "sent": "Progress comes from the definition of what is called a restricted isometry property.",
                    "label": 1
                },
                {
                    "sent": "So given a matrix 5, define the isometry constant Delta is to be the smallest number such that when you transform X using five, than all of this quantity does not change.",
                    "label": 0
                },
                {
                    "sent": "Too much is approximately close to the normal eggs for all S sparse vector X.",
                    "label": 1
                },
                {
                    "sent": "So if you have a sparse vector X and you apply fire to it, the norm does not change too much, so that's what is called.",
                    "label": 0
                },
                {
                    "sent": "The stricted isometry.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Property.",
                    "label": 0
                },
                {
                    "sent": "And what we have is the results show that if D2 S is less than sqrt 2 -- 1, then the L1 optimization actually finds the correct solution when equal to 0 and in case is not equal to 0 then it finds very close to the correct optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And this property, some people might say that this is very strong, but it turns out that a lot of matrices, in fact almost all of the random matrices with appropriate sizes who satisfy this property, and in fact random Fourier matrix is also satisfy this property.",
                    "label": 0
                },
                {
                    "sent": "And deterministic constructions people are coming up with deterministic constructions which satisfy this property, and because of this what we have is universal encoding strategy.",
                    "label": 0
                },
                {
                    "sent": "So you have a sparse signal in any domain.",
                    "label": 0
                },
                {
                    "sent": "What you can do?",
                    "label": 0
                },
                {
                    "sent": "Is sample it using random measurements in using a very small number of random measurements and then you can reconstruct the sparse signal data.",
                    "label": 0
                },
                {
                    "sent": "So this has led to a lot of applications, specially in the image reconstruction domain.",
                    "label": 0
                },
                {
                    "sent": "But the problem is relevant.",
                    "label": 0
                },
                {
                    "sent": "Minimization is that the algorithm is very slow if you want to solve it in general, you need to do an LP, and that's extremely slow.",
                    "label": 0
                },
                {
                    "sent": "People have designed some efficient algorithms like laws, but they are still slow and to give a idea of the magnitude, let's say we're talking about MRI imaging and it typical 1 millimeter kind of image is of the size 256 cube, which correspond to 16,000,000 rows in 16,000,000 columns.",
                    "label": 1
                },
                {
                    "sent": "And when we're talking about his past solution, we're talking about something like 100,000 to 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "Number of sparse components.",
                    "label": 0
                },
                {
                    "sent": "So on problems of these sizes, LP and these algorithms just fail.",
                    "label": 0
                },
                {
                    "sent": "They cannot really work, and people still adopt combinatorial algorithms that somehow give you some solution, but they're not probably optimum, and there is a recent activity on combinatorial algae.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Terms that actually show optimality in certain conditions, so this is like the summary of the algorithms known so far.",
                    "label": 0
                },
                {
                    "sent": "Some of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "And on the top we have the L1 optimization algorithms and the time taken is order M plus in cube.",
                    "label": 0
                },
                {
                    "sent": "And that basically means that for the images of the size MRI we're talking about 10 to 21 and 14 absolutely not useful.",
                    "label": 0
                },
                {
                    "sent": "And there are some algorithms that are iterative.",
                    "label": 0
                },
                {
                    "sent": "Some of them take constant number of iterations, some of them take number of iteration equal to the sparsity and still the runtimes here of the same are of the order of 10 report 13.",
                    "label": 1
                },
                {
                    "sent": "So in this paper or what I'm going to describe is an algorithm that we call grades an it.",
                    "label": 0
                },
                {
                    "sent": "At it's an iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "In every iteration the cost is K, which is the time to carry out matrix vector product, and if you're dealing with MRI type matrices then you don't really have to do that.",
                    "label": 1
                },
                {
                    "sent": "You can do it through Fourier transform, so it's not necessarily M * N. It can be much more efficient than that, and the number of iterations that you need is constant.",
                    "label": 0
                },
                {
                    "sent": "It only depends on the precision.",
                    "label": 0
                },
                {
                    "sent": "It does not depend on the number of non zero variables.",
                    "label": 0
                },
                {
                    "sent": "And then it can probably.",
                    "label": 0
                },
                {
                    "sent": "Reconstruct or recover the actual solution, provided we have the following conditions on the on the isometry constant, and this condition is very close to the condition that the best condition that we have under which any algorithm is known, which is quite 2 -- 1 which is about .4 and there's about .33.",
                    "label": 0
                },
                {
                    "sent": "And the time taking here becomes 10 to the Power 8, which is very much in the domain that we can do it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the algorithm is a combinatorial algorithm.",
                    "label": 1
                },
                {
                    "sent": "It starts with the.",
                    "label": 1
                },
                {
                    "sent": "Initial solution in iteratively refines it using matrix vector product and the number of iterations is constant.",
                    "label": 0
                },
                {
                    "sent": "It does not depend on the sparsity, and basically it's a very simple algorithm.",
                    "label": 1
                },
                {
                    "sent": "It just moves along the gradient and then it does the sparsification.",
                    "label": 1
                },
                {
                    "sent": "And it needs matrices satisfying rapo, and in that case.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It does find the correct solution.",
                    "label": 0
                },
                {
                    "sent": "So what what is happened?",
                    "label": 0
                },
                {
                    "sent": "Initially we wanted to solve this past regression problem and we had this combinatorial algorithm with no guarantees to converge.",
                    "label": 0
                },
                {
                    "sent": "Then we have the NP hardness results that show that you cannot really probably converge.",
                    "label": 1
                },
                {
                    "sent": "It's a hard problem.",
                    "label": 0
                },
                {
                    "sent": "Then we have L1 minimization as a heuristic which used to work well in practice.",
                    "label": 0
                },
                {
                    "sent": "But people didn't know why.",
                    "label": 0
                },
                {
                    "sent": "And then the recent sort of result in compressed sensing that actually proved that if the matrix satisfies certain properties then you do converge to the optimum.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to a domain where again we have combinatorial algorithms that probably converge to the correct solution with the restricted isometry assumption, and what's happening now is that we're also seeing probably correct combinatorial algorithm which.",
                    "label": 0
                },
                {
                    "sent": "Which which actually converge to the solution with assumptions weaker than our IP or more general assumptions.",
                    "label": 0
                },
                {
                    "sent": "So we have come full circle starting from the combinatorial.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The returns and back to the combinatorial algorithms, but we've learned a lot in the process, so this algorithm starts with X equal to 0.",
                    "label": 1
                },
                {
                    "sent": "It iteratively moves around the gradient and spa specifies the solution, and that's the full algorithm.",
                    "label": 1
                },
                {
                    "sent": "So you start with an X.",
                    "label": 0
                },
                {
                    "sent": "You compute the residue, which is a difference from Y.",
                    "label": 0
                },
                {
                    "sent": "That's the residue.",
                    "label": 0
                },
                {
                    "sent": "Another gradient is defined by minus 2 * 5.",
                    "label": 0
                },
                {
                    "sent": "Transpose the residue again matrix will product.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is you move along the gradient with the weighting factor 2 by gamma.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell what the gamma is in the moment and after this what you do is take the largest as components in the magnitude.",
                    "label": 0
                },
                {
                    "sent": "So it's extremely simple algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's guaranteed to converge.",
                    "label": 0
                },
                {
                    "sent": "So what we have is.",
                    "label": 0
                },
                {
                    "sent": "If D2 X is less than one by three and Y is equal to 5X for a sparse vector X star, then in K iterations the solution.",
                    "label": 1
                },
                {
                    "sent": "Is within two D2 X by 1 minus Delta 2 S times K to the power K. So basically at every iteration the solution, the algorithm decreases the error by a constant multiplicative factor, so therefore in constant number of iterations it actually gets very close to the optimum solution.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have the result for the noise case also.",
                    "label": 0
                },
                {
                    "sent": "And here what we have is it finds a solution which is within a factor D of the optimum where the error is in the factor D and then again converges in small number of iterations.",
                    "label": 0
                },
                {
                    "sent": "Constant number of iterations and this is very compareable to the L1 optimization results of Candace in Tau.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why does it work?",
                    "label": 0
                },
                {
                    "sent": "What happens is we define a potential which is y -- 5.",
                    "label": 0
                },
                {
                    "sent": "X and.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as wise fight and sex stars, so that's a potential.",
                    "label": 0
                },
                {
                    "sent": "Now if the vectors X and extras pause, then using our IP there about.",
                    "label": 0
                },
                {
                    "sent": "The normal fixed minus X star.",
                    "label": 0
                },
                {
                    "sent": "So now if we look at the gradient of five gradient of PSI of X, it is given by this quantity and if you start from X and move along Delta X, the change in the function is given by the gradient times Delta X plus this Hessian term.",
                    "label": 0
                },
                {
                    "sent": "The 2nd order term.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's going to be critical here.",
                    "label": 0
                },
                {
                    "sent": "So now what we want to do is, if given any solution, we want to find a Delta X that minimizes this, and this term is composed of the two terms.",
                    "label": 1
                },
                {
                    "sent": "The first term is separable, so it's like each component of GI times the component of exile, so you can optimize this very easily, but this is a non separable term because it has interactions and that's the hard part.",
                    "label": 0
                },
                {
                    "sent": "But what our IP gives is decomposes this into.",
                    "label": 0
                },
                {
                    "sent": "One plus minus Delta times Delta X and now it becomes separable.",
                    "label": 0
                },
                {
                    "sent": "So now if you optimize this you can represent it.",
                    "label": 0
                },
                {
                    "sent": "Just in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradients, so essentially.",
                    "label": 0
                },
                {
                    "sent": "The change in five if you go to an optimum solution is given by the gradient times the chain optimum change plus one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "That's abound and hear the bound because one plus Delta.",
                    "label": 0
                },
                {
                    "sent": "If we go along another direction, and insight is that instead of minimizing this, we minimize this an using this.",
                    "label": 0
                },
                {
                    "sent": "What you have is basically Delta X is a gradient times one plus Delta and it works.",
                    "label": 0
                },
                {
                    "sent": "After thresholding.",
                    "label": 0
                },
                {
                    "sent": "I had some nice animation here.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you cannot see that, but.",
                    "label": 0
                },
                {
                    "sent": "It gives the current solution.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does it perform well in practice?",
                    "label": 0
                },
                {
                    "sent": "We implemented it MATLAB and then we compared a bunch of algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this graph shows how the runtime is the function of number of rules.",
                    "label": 0
                },
                {
                    "sent": "And this is the L1 based algorithms Lasso.",
                    "label": 0
                },
                {
                    "sent": "And wimpy, which is jacks, one component at a time.",
                    "label": 0
                },
                {
                    "sent": "These are the newer algorithms and this is the grades.",
                    "label": 0
                },
                {
                    "sent": "In fact, great experiment parameterized by the parameter gamma and what we have is the result for gamma equal to four by three.",
                    "label": 0
                },
                {
                    "sent": "But in practice what we found is that if you have if you move slowly along the gradient, then you actually have better positive property, although the convergence is bit slower.",
                    "label": 0
                },
                {
                    "sent": "So here the runtime is almost constant with respect.",
                    "label": 0
                },
                {
                    "sent": "Number of rows whereas all the other algorithms are in time increases and this difference actually becomes more and more as you want to bigger and bigger.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metric sizes problem sizes.",
                    "label": 0
                },
                {
                    "sent": "Similarly with number of columns, the trend here is linear.",
                    "label": 1
                },
                {
                    "sent": "Here it is super linear, and here again it's linear and you see an order of magnitude difference in the runtimes here.",
                    "label": 0
                },
                {
                    "sent": "What's positive?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, the great algorithm has a flat curve.",
                    "label": 0
                },
                {
                    "sent": "It does not depend on the sparsity, whereas these algorithms depend on the sparsity, and for these algorithms, actually in theory the runtime should be independent of this positive.",
                    "label": 0
                },
                {
                    "sent": "But what turns out is that the number of iterations they need to converge actually increases as you have more number of components.",
                    "label": 1
                },
                {
                    "sent": "As this positives increased, so you see this kind of a behavior in practice, whereas the grades actually.",
                    "label": 0
                },
                {
                    "sent": "Is pretty flat.",
                    "label": 0
                },
                {
                    "sent": "In terms of recovery properties, what?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have is in fact last year the best guarantees and this my last match and great actually does better than Lasso.",
                    "label": 0
                },
                {
                    "sent": "And here's the comparison with some other algorithms.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the L1 implementations are numerically unstable, and that's why you really don't get the good solutions here.",
                    "label": 0
                },
                {
                    "sent": "Turn to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparse regression is an important problem.",
                    "label": 0
                },
                {
                    "sent": "It's very much applicable in compressed sensing as well as machine learning, feature selection and lot of other areas, and what we have is a fast algorithm that probably works under the restricted isometry property assumptions and it needs a constant number of iterations independent of the sparsity, and it seems to work well in practice also.",
                    "label": 0
                },
                {
                    "sent": "Stop your questions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have questions, yes.",
                    "label": 0
                },
                {
                    "sent": "What is the difference between new organisms and the iterative artificial human?",
                    "label": 0
                },
                {
                    "sent": "Said yeah, so it's pretty much like the same question.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what is the difference between this algorithm and the iterative hard thresholding algorithm?",
                    "label": 0
                },
                {
                    "sent": "The iterative hard thresholding is kind of a special case of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here the algorithm is parameterized by the parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "How fast you move along the gradient an.",
                    "label": 0
                },
                {
                    "sent": "Is there so?",
                    "label": 0
                },
                {
                    "sent": "Then it's identical.",
                    "label": 0
                },
                {
                    "sent": "It's the same algorithm, except that we have a proof that it actually converges when the gamma is full by three.",
                    "label": 0
                },
                {
                    "sent": "Put it in there.",
                    "label": 0
                },
                {
                    "sent": "Is the automatic, yeah?",
                    "label": 0
                },
                {
                    "sent": "So the algorithm in that case algorithm is the same, but the analysis shows.",
                    "label": 0
                },
                {
                    "sent": "One more question is eventually.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Typical testing results within C and see where he is doing similar things sampling.",
                    "label": 0
                },
                {
                    "sent": "He won't explicitly stating things O and it may be that this result we, well, exactly you will copy so well.",
                    "label": 0
                },
                {
                    "sent": "Interesting look at what is the casting version of your own.",
                    "label": 0
                },
                {
                    "sent": "You sometimes we have some medical bills.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean we have not tried the stochastic version here, so yeah, so good, good comment.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't have more time for next time the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}