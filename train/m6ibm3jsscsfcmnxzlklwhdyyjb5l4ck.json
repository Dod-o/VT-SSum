{
    "id": "m6ibm3jsscsfcmnxzlklwhdyyjb5l4ck",
    "title": "Data-Driven Scene Understanding from 3D Models",
    "info": {
        "author": [
            "Scott Satkin, The Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Robotics"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_satkin_scene_understanding/",
    "segmentation": [
        [
            "Over the past decade, researchers have demonstrated the effectiveness of data driven approach."
        ],
        [
            "As for complex computer vision tasks, it is now possible to imagine input image without."
        ],
        [
            "Dozens or even a millions of images from a large data set to transfer information from one image to another.",
            "Recently a new source of data has emerged on the Internet 3D models using sites like Google 3D Warehouse.",
            "We can perform queries for scenes such as bedrooms or living rooms and download thousands of 3D models.",
            "Most models in 3D warehouse also contain accurate object labels and dimension."
        ],
        [
            "The robotics vision and graphics community have recently begun leveraging this data for various problems such as point cloud analysis, characterizing Object Co occurrences in estimating affordances."
        ],
        [
            "But what if we could match three models with images?",
            "This data could be used."
        ],
        [
            "Assist various vision tasks such as object recognition and scene understanding.",
            "Our work is one of the first combined this geometric prior with image features in a framework capable of producing detailed 3D models from an in."
        ],
        [
            "Traditional scene understanding approaches are trained on 2D data."
        ],
        [
            "Resulting in simple bounding boxes or keyboard detections.",
            "Here we are not interested in simply determining locations of objects."
        ],
        [
            "Our goal is to automatically create detailed 3D models from a single image.",
            "Humans are capable of taking a single image in estimating the geometry seen by relating objects in an image to objects we have seen before.",
            "We can easily get a sense of the locations and orientations of furniture and even estimate the free space in the environment.",
            "All this is possible despite the fact that this is Anil Post problem."
        ],
        [
            "There exists an infinite number of 3D geometries which had textured correctly and viewed from just the right angle will project to the exact same image."
        ],
        [
            "So how can we overcome this problem?",
            "The major question we addressed in this paper is how can we match a 2D image with a 3D model?",
            "This issue is fundamentally complicated by the fact that we need to compare two objects of a completely different nature.",
            "An array."
        ],
        [
            "With pixels on one hand and a set of services with little or no appearance information on."
        ],
        [
            "The other.",
            "Of course, this is not an entirely new problem.",
            "The idea of relating 3D models to two B projections was actually one of the foundations of earlier vision approaches.",
            "However, these systems were primarily rule based looking at relatively simple scenarios.",
            "Here we are tackling the more general case and utilizing vast repository's of 3D data which require novel vision and learning approaches."
        ],
        [
            "We"
        ],
        [
            "We addressed this problem in three stages.",
            "First, we determine how to align each 30 model with an input image.",
            "Next, we compute a set of features which relate how similar each model is with the image.",
            "And Lastly we use these features to rank each hypothesized geometry and determine the best matching 3D model for the input image."
        ],
        [
            "Model alignment means that 3D object."
        ],
        [
            "Are properly scaled and oriented relative to the image."
        ],
        [
            "It's not enough."
        ],
        [
            "Object to simply be positioned correct."
        ],
        [
            "The object has to follow the rules of perspective.",
            "We'll need to estimate both the intrinsic and extrinsic camera parameters to properly align 3D models we build upon previous works in estimating the intrinsic and extrinsic parameters, which I will now briefly summarize."
        ],
        [
            "To begin the alignment process, we begin by estimating the vanishing points in the scene using the algorithm of Lee at all.",
            "This algorithm begins by extracting edges and clustering."
        ],
        [
            "And into 3 directions."
        ],
        [
            "Under Manhattan world assumption, we can use these vanishing points and orthogonality constraint to compute the intrinsic camera calibration matrix.",
            "The vanishing points are also used to compute the rotation of the camera relative to the walls and floors in the image."
        ],
        [
            "Next, we would like to detur."
        ],
        [
            "The location of the walls and floors in the image."
        ],
        [
            "We use the room layout algorithm."
        ],
        [
            "Which of her do at all which begins by estimating the locations of each wall and floor using him at all is geometric context algorithm."
        ],
        [
            "Their approach then searches over different room layout hypothesis."
        ],
        [
            "To find the one which best matches the predicted wall and floor locations."
        ],
        [
            "Now that we know where the walls and floors are, we can use the corner of the room as an origin for the scene location of the camera relative to the walls and the floor is computed assuming that the camera is a fixed height of five feet from the ground.",
            "We have now estimated both intrinsic and extrinsic parameters of the camera which captured this image."
        ],
        [
            "Using this information we can now render objects from the same viewpoint that the image was captured we can take."
        ],
        [
            "Furniture arrangements from Google Warehouse and drop them right into the image, aligning the walls and floors of the image with the walls and the floors from the 3D model.",
            "Although this is the incorrect arrangement of furniture, you can see that the objects correctly align with the walls and floor in the image and the perspective effects are correctly accounted for.",
            "We down."
        ],
        [
            "Loaded approximately"
        ],
        [
            "2000 models"
        ],
        [
            "Bedrooms and living rooms from 3D warehouse."
        ],
        [
            "He"
        ],
        [
            "Have these furniture rain."
        ],
        [
            "Is a separate geometry hypoth?"
        ],
        [
            "This."
        ],
        [
            "We iterate through each."
        ],
        [
            "These hypothesis and search the one that best matches the image."
        ],
        [
            "We also eval."
        ],
        [
            "Weight each model."
        ],
        [
            "Eight different view."
        ],
        [
            "It's."
        ],
        [
            "Responding to four."
        ],
        [
            "90 degree rotations as well as."
        ],
        [
            "A mirror image of the model.",
            "This is analogous to training a classifier with images that were horizontally flipped."
        ],
        [
            "Once we can align our 3D models with an input image, we still need a mechanism for comparing each model with the."
        ],
        [
            "Edge.",
            "We propose a series of image based descriptors and their 3D counterparts.",
            "We use our renderer which can insert objects into a scene to produce synthetic image descriptors for each 3D model and compare these to traditional image based descriptors.",
            "Here we are not creating photo realistic renderings, so we're only interested in simply rendering descriptors."
        ],
        [
            "The first similarity feature focuses on locations of objects in a scene.",
            "We use the pre trained geometric context model to estimate the likelihood that each pixel in an image contains a foreground object."
        ],
        [
            "Then for each 3D model, we render a simple object mask by coloring each Polygon of the 3D model black over a white background.",
            "The normalized dot product between the predicted object locations and the rendered object masks indicates how well the model matches our image.",
            "This similarity measure is the first feature we used to compare 3D models with an input image."
        ],
        [
            "Our next similarity feature focuses on the orientations of each Patch in an image.",
            "We use the plain sweeping algorithm of Lee at all to predict the orientations of each pixel in an input image.",
            "For each 3D model, we render a surface normal image by simply setting the red, green and blue color channels of each Polygon to the XY and Z components of the Polygon surface normal.",
            "The normalized dot product of these two descriptors quantifies their similarity.",
            "We use this value as a feature when scoring each 3D model."
        ],
        [
            "Our next similarity feature."
        ],
        [
            "We also combine the object mass."
        ],
        [
            "With the."
        ],
        [
            "Surface normal descriptors to create a highly informative hybrid feature.",
            "For this feature, we multiply the object Mask agreement score with the Surface Normal agreement score for each pixel.",
            "This combined score aims to count how many pixels in the image satisfied two constraints.",
            "Firstly, objects in the renderings should appear at the same locations as objects in the image.",
            "Secondly, the surface normals of these objects at these locations should agree with the predicted surface normals."
        ],
        [
            "Our last similarity feature focuses on edge locations.",
            "We extract edges from an image using the global PB algorithm."
        ],
        [
            "These edges."
        ],
        [
            "These are compared to canny edges which are extracted from rendered surface.",
            "Normal images of each scene hypothesis."
        ],
        [
            "Pairs of edge images extracted from each input image and each rendering are compared using a symmetric chamfer distance.",
            "We first compute a distance transform."
        ],
        [
            "And of the boundaries."
        ],
        [
            "And then see how well the edges from the 3D model align with the edges."
        ],
        [
            "Of the input image."
        ],
        [
            "Here you can see edges which align well, are colored red and edges which align poorly, are colored blue.",
            "The total."
        ],
        [
            "Distance is computed by summing the distance of each edge in one image to the distance of the nearest edge in the other image.",
            "Naturally, outlier edges are going to dominate this edge distance score."
        ],
        [
            "To reduce the influence of."
        ],
        [
            "Outlier edges, we truncate individual edge distance penalties.",
            "Intuitively, distances that are computed with smaller values of Tau encourage fine grained matching of edges, while distances computed with larger thresholds aim to penalize larger errors.",
            "Each of these distances treated as a different similarity feature for a total of 4 features."
        ],
        [
            "In total we have 7 similarity features which are treated independently."
        ],
        [
            "For each model we download from Pretty Warehouse, we align it using the estimated camera parameters and compute similarity features represented by the vector X.",
            "This process is repeat."
        ],
        [
            "Thousands of times to compute similarity features for each 3D model we download from Google Warehouse."
        ],
        [
            "A linear combination of these features is used to score how well each 3D model matches the input image.",
            "In order to learn a proper weighting, these features will need some training data."
        ],
        [
            "If we had ground truth surface normals for an image, we could go through 3D warehouse, align each model, and compare the masked surface normals to determine the best matching scene from the data set."
        ],
        [
            "We could also determine which 3D models do not match the image well."
        ],
        [
            "In fact, for."
        ],
        [
            "Given image, we can compute a distance Delta indicating how much better one model matches than another model does."
        ],
        [
            "Howl."
        ],
        [
            "Or we don't have a ground truth data for this task.",
            "There does not yet exist a data set of images and their corresponding detailed 3D models."
        ],
        [
            "Thus we."
        ],
        [
            "We did our own data set.",
            "We had a team of annotators create detailed 3D models using Google SketchUp.",
            "SketchUp allows you to take labeled lines to indicate vanishing points, set the locations of walls and floors and insert objects from a repository to handcraft detailed 3D models which nicely aligned with an input image."
        ],
        [
            "We've created a set of annotated geometry's for 500 randomly selected bedroom and living room images from the Suns scene, understanding data set.",
            "For each image.",
            "We have detailed 3D models, object labels as well as ground truth intrinsic and extrinsic camera parameters.",
            "This data set will be made publicly available for researchers to download starting next."
        ],
        [
            "Now that we have this data, we can use it to train a support vector.",
            "Rank are as follows.",
            "Given an image and it's manually annotated geometry, we can retrieve the top 3D models from Google Warehouse which most closely matched the manually annotated geometry using a service normal comparison.",
            "We can also retrieve 3D models which poorly matched the annotated geometry."
        ],
        [
            "If we select one good 3D model and one bad 3D model, we know that the one on the left should rank higher than the one on the right."
        ],
        [
            "Mathematically, this simply translates to W transpose X for the first model being higher than W, transpose X for the second model."
        ],
        [
            "We include the margin Delta, which indicated how much better the first model matched the image than the second model did."
        ],
        [
            "This this constraint is incorporated into a standard optimization to solve for the weight vector which best ranks pairs of 3D models for each training image."
        ],
        [
            "The slack variable is also included in the support vector ranking formulation."
        ],
        [
            "This equation is optimized using stochastic subgradient methods.",
            "In each iteration, we randomly selected training image.",
            "I and a pair of 3D models J&K.",
            "If the current weight vector causes the pair of 3D models to be incorrectly ranked or if their distance difference in scores is less than the margin Delta, we compute a subgradient and update the weight vector.",
            "This process is repeated until convergence."
        ],
        [
            "Now, given our learned weight vector, we can score every hypothesis from 3D warehouse and find the top ranking 3D models.",
            "Here you can see that these models not only contain a couch in the correct position, they also tend to match the style of the furniture."
        ],
        [
            "Our data driven framework also provides a mechanism for transferring rich information from these models back to the input image, such as object labels, their segmentations, surface, normals, and even death."
        ],
        [
            "Let's now review some results of our algorithm.",
            "Here you can see an image from the Sun's data set of."
        ],
        [
            "Living room.",
            "And here's the automatically matched 3D model aligned to the input image in the upper right.",
            "We showed transferred segmentation masks color coded by object categories.",
            "Here, couches are labeled yellow and tables are labeled red.",
            "We also show the predicted surface normals and depths below."
        ],
        [
            "Here's another example of the living room scene where we correctly predicted the locations and orientations of couches and seats."
        ],
        [
            "This example shows the bedroom scene with nicely matched and align night stands and in bed."
        ],
        [
            "Many images in our data set include her."
        ],
        [
            "The included objects, such as this night stand here a traditional bottom up appearance based object detector would typically fail to detect this instance."
        ],
        [
            "However, our holistic scene matching approach is capable of finding these challenging objects as shown."
        ],
        [
            "Here."
        ],
        [
            "Here."
        ],
        [
            "Two additional examples of our algorithm detecting heavily occluded objects."
        ],
        [
            "Because our approach is 3D data, we are inherently invariant to rotation.",
            "That means we're able to recover the labels, locations, and orientations of objects even from obscur viewpoints.",
            "For example, this image was captured from behind the couch."
        ],
        [
            "However, we are still able to correctly identify this unique layout.",
            "Recovering the position and orientation of both couches and the coffee table.",
            "This is because, although the viewpoint is unusual, the arrangement of furniture is fairly common.",
            "Two couches at right angles to each other, with a coffee table in the center."
        ],
        [
            "Of course, this is computer vision doesn't always work.",
            "A fundamental issue with our approach is our reliance on auto calibration and room layout estimation.",
            "If this stage of the pipeline fails, we may incorrectly estimate the arrangements of objects in the scene.",
            "Here is an example error due to incorrect vanishing point estimation which has the complete failure of our system."
        ],
        [
            "In this stage in this situation, it was the model alignment stage of the pipeline that failed."
        ],
        [
            "However, if we used annotated camera parameters.",
            "It's possible that we could use this information for alignment and go through with the rest of the process of computing similarity features, ranking each hypothesis and generating a good arrangement of furnitures."
        ],
        [
            "Here's an example of this.",
            "For this image had Dowdall's room layout algorithm failed to correctly estimate the locations of walls and floor.",
            "This causes our geometry matching process to fail to find a good furniture arrangement."
        ],
        [
            "However, if we had used the groundtruth camera parameters from the annotation process and use this information for aligning, we would have retrieved an excellent match for the furniture arrangement."
        ],
        [
            "Let's now go into a quantitative evaluation.",
            "One way to quantify the quality of our 3D models is to measure how accurately we predict the orientation of surface is in the image.",
            "We score 3D hypothesis by taking the dot product of the ground truth surface normals from the annotated models and the orientation of pixels of are rendered hypothesis normalized by the number of pixels in the image."
        ],
        [
            "This score represents the percentage of pixels for which we have correctly identified the orientations.",
            "Since the majority of pixels in most scenes correspond to walls or floor which are not informative to the task of quantifying the object geometries, we also report surface normal scores for only those pixels which belong to objects.",
            "Did a couple the effects of incorrect room layout estimation from our primary goal of determining arrangements of objects.",
            "We are reporting results using annotated room layouts.",
            "And camera parameters as well as fully automatic results incorporating the room layout approach of a Dow at all.",
            "For sale"
        ],
        [
            "Instead of images in our data set, we had two annotators create 3D models.",
            "By comparing these different versions of the same geometry as, we can evaluate the subjectivity of this task and."
        ],
        [
            "Annotation process, so here you can see human performance compared to the annotated layout just using the ground truth camera parameters as well as a fully automatic system for estimating the geometry's."
        ],
        [
            "We also have evaluated our algorithm on for the free space of a broom because we have full 3D models of each room and known camera parameters, we can rectify the scene and view it from above.",
            "Here you can see an automatically generated overhead architectural floor plan.",
            "You can see that we have good estimates of the free space between objects such as the couch in the coffee table in the image on the bottom."
        ],
        [
            "We compare estimated object locations with ground truth object locations for each square inch of the floor that we predict to be occupied.",
            "We compare it to the ground truth occupancy to report precision and recall.",
            "We also show the F measure which incorporates both precision and recall into one metric as a baseline.",
            "For comparison, we run the pre trained geometry estimation algorithm of Gupta ET al, whose performance is indicated on the right.",
            "This metric is similar to the one presented at Cdr this year by Haddow at all in her free space estimation work.",
            "Our paper.",
            "It also includes this evaluation metric."
        ],
        [
            "We can also perform a value metric evaluation."
        ],
        [
            "In our overhead views here we show the Heights of objects taken directly from warehouse data."
        ],
        [
            "Using this information, we can create a voxelized representation of our scenes and compute how precisely we estimate which voxels are occupied."
        ],
        [
            "In conclusion, I presented a Gmail learning framework capable of lining 3D models to an input image, computing features which compare these models to the image and a mechanism for selecting which model best matches the input image, allowing us to transfer rich information from these models back to."
        ],
        [
            "Input image.",
            "Thank you for your time and please check out our data set we created for this paper which will be online starting next week."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the past decade, researchers have demonstrated the effectiveness of data driven approach.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As for complex computer vision tasks, it is now possible to imagine input image without.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dozens or even a millions of images from a large data set to transfer information from one image to another.",
                    "label": 0
                },
                {
                    "sent": "Recently a new source of data has emerged on the Internet 3D models using sites like Google 3D Warehouse.",
                    "label": 0
                },
                {
                    "sent": "We can perform queries for scenes such as bedrooms or living rooms and download thousands of 3D models.",
                    "label": 0
                },
                {
                    "sent": "Most models in 3D warehouse also contain accurate object labels and dimension.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The robotics vision and graphics community have recently begun leveraging this data for various problems such as point cloud analysis, characterizing Object Co occurrences in estimating affordances.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what if we could match three models with images?",
                    "label": 0
                },
                {
                    "sent": "This data could be used.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assist various vision tasks such as object recognition and scene understanding.",
                    "label": 0
                },
                {
                    "sent": "Our work is one of the first combined this geometric prior with image features in a framework capable of producing detailed 3D models from an in.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Traditional scene understanding approaches are trained on 2D data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Resulting in simple bounding boxes or keyboard detections.",
                    "label": 0
                },
                {
                    "sent": "Here we are not interested in simply determining locations of objects.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our goal is to automatically create detailed 3D models from a single image.",
                    "label": 0
                },
                {
                    "sent": "Humans are capable of taking a single image in estimating the geometry seen by relating objects in an image to objects we have seen before.",
                    "label": 0
                },
                {
                    "sent": "We can easily get a sense of the locations and orientations of furniture and even estimate the free space in the environment.",
                    "label": 0
                },
                {
                    "sent": "All this is possible despite the fact that this is Anil Post problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There exists an infinite number of 3D geometries which had textured correctly and viewed from just the right angle will project to the exact same image.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can we overcome this problem?",
                    "label": 0
                },
                {
                    "sent": "The major question we addressed in this paper is how can we match a 2D image with a 3D model?",
                    "label": 0
                },
                {
                    "sent": "This issue is fundamentally complicated by the fact that we need to compare two objects of a completely different nature.",
                    "label": 0
                },
                {
                    "sent": "An array.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With pixels on one hand and a set of services with little or no appearance information on.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is not an entirely new problem.",
                    "label": 0
                },
                {
                    "sent": "The idea of relating 3D models to two B projections was actually one of the foundations of earlier vision approaches.",
                    "label": 0
                },
                {
                    "sent": "However, these systems were primarily rule based looking at relatively simple scenarios.",
                    "label": 0
                },
                {
                    "sent": "Here we are tackling the more general case and utilizing vast repository's of 3D data which require novel vision and learning approaches.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We addressed this problem in three stages.",
                    "label": 0
                },
                {
                    "sent": "First, we determine how to align each 30 model with an input image.",
                    "label": 0
                },
                {
                    "sent": "Next, we compute a set of features which relate how similar each model is with the image.",
                    "label": 0
                },
                {
                    "sent": "And Lastly we use these features to rank each hypothesized geometry and determine the best matching 3D model for the input image.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model alignment means that 3D object.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are properly scaled and oriented relative to the image.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not enough.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Object to simply be positioned correct.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The object has to follow the rules of perspective.",
                    "label": 0
                },
                {
                    "sent": "We'll need to estimate both the intrinsic and extrinsic camera parameters to properly align 3D models we build upon previous works in estimating the intrinsic and extrinsic parameters, which I will now briefly summarize.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To begin the alignment process, we begin by estimating the vanishing points in the scene using the algorithm of Lee at all.",
                    "label": 0
                },
                {
                    "sent": "This algorithm begins by extracting edges and clustering.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And into 3 directions.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under Manhattan world assumption, we can use these vanishing points and orthogonality constraint to compute the intrinsic camera calibration matrix.",
                    "label": 0
                },
                {
                    "sent": "The vanishing points are also used to compute the rotation of the camera relative to the walls and floors in the image.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, we would like to detur.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The location of the walls and floors in the image.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use the room layout algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which of her do at all which begins by estimating the locations of each wall and floor using him at all is geometric context algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their approach then searches over different room layout hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To find the one which best matches the predicted wall and floor locations.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we know where the walls and floors are, we can use the corner of the room as an origin for the scene location of the camera relative to the walls and the floor is computed assuming that the camera is a fixed height of five feet from the ground.",
                    "label": 0
                },
                {
                    "sent": "We have now estimated both intrinsic and extrinsic parameters of the camera which captured this image.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this information we can now render objects from the same viewpoint that the image was captured we can take.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Furniture arrangements from Google Warehouse and drop them right into the image, aligning the walls and floors of the image with the walls and the floors from the 3D model.",
                    "label": 0
                },
                {
                    "sent": "Although this is the incorrect arrangement of furniture, you can see that the objects correctly align with the walls and floor in the image and the perspective effects are correctly accounted for.",
                    "label": 0
                },
                {
                    "sent": "We down.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loaded approximately",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2000 models",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bedrooms and living rooms from 3D warehouse.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have these furniture rain.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a separate geometry hypoth?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We iterate through each.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These hypothesis and search the one that best matches the image.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also eval.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weight each model.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eight different view.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Responding to four.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "90 degree rotations as well as.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A mirror image of the model.",
                    "label": 0
                },
                {
                    "sent": "This is analogous to training a classifier with images that were horizontally flipped.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we can align our 3D models with an input image, we still need a mechanism for comparing each model with the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edge.",
                    "label": 0
                },
                {
                    "sent": "We propose a series of image based descriptors and their 3D counterparts.",
                    "label": 0
                },
                {
                    "sent": "We use our renderer which can insert objects into a scene to produce synthetic image descriptors for each 3D model and compare these to traditional image based descriptors.",
                    "label": 0
                },
                {
                    "sent": "Here we are not creating photo realistic renderings, so we're only interested in simply rendering descriptors.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first similarity feature focuses on locations of objects in a scene.",
                    "label": 0
                },
                {
                    "sent": "We use the pre trained geometric context model to estimate the likelihood that each pixel in an image contains a foreground object.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then for each 3D model, we render a simple object mask by coloring each Polygon of the 3D model black over a white background.",
                    "label": 0
                },
                {
                    "sent": "The normalized dot product between the predicted object locations and the rendered object masks indicates how well the model matches our image.",
                    "label": 0
                },
                {
                    "sent": "This similarity measure is the first feature we used to compare 3D models with an input image.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our next similarity feature focuses on the orientations of each Patch in an image.",
                    "label": 0
                },
                {
                    "sent": "We use the plain sweeping algorithm of Lee at all to predict the orientations of each pixel in an input image.",
                    "label": 1
                },
                {
                    "sent": "For each 3D model, we render a surface normal image by simply setting the red, green and blue color channels of each Polygon to the XY and Z components of the Polygon surface normal.",
                    "label": 0
                },
                {
                    "sent": "The normalized dot product of these two descriptors quantifies their similarity.",
                    "label": 0
                },
                {
                    "sent": "We use this value as a feature when scoring each 3D model.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next similarity feature.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also combine the object mass.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Surface normal descriptors to create a highly informative hybrid feature.",
                    "label": 0
                },
                {
                    "sent": "For this feature, we multiply the object Mask agreement score with the Surface Normal agreement score for each pixel.",
                    "label": 0
                },
                {
                    "sent": "This combined score aims to count how many pixels in the image satisfied two constraints.",
                    "label": 0
                },
                {
                    "sent": "Firstly, objects in the renderings should appear at the same locations as objects in the image.",
                    "label": 0
                },
                {
                    "sent": "Secondly, the surface normals of these objects at these locations should agree with the predicted surface normals.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our last similarity feature focuses on edge locations.",
                    "label": 0
                },
                {
                    "sent": "We extract edges from an image using the global PB algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These edges.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are compared to canny edges which are extracted from rendered surface.",
                    "label": 0
                },
                {
                    "sent": "Normal images of each scene hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pairs of edge images extracted from each input image and each rendering are compared using a symmetric chamfer distance.",
                    "label": 0
                },
                {
                    "sent": "We first compute a distance transform.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of the boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then see how well the edges from the 3D model align with the edges.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the input image.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you can see edges which align well, are colored red and edges which align poorly, are colored blue.",
                    "label": 0
                },
                {
                    "sent": "The total.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distance is computed by summing the distance of each edge in one image to the distance of the nearest edge in the other image.",
                    "label": 0
                },
                {
                    "sent": "Naturally, outlier edges are going to dominate this edge distance score.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To reduce the influence of.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outlier edges, we truncate individual edge distance penalties.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, distances that are computed with smaller values of Tau encourage fine grained matching of edges, while distances computed with larger thresholds aim to penalize larger errors.",
                    "label": 0
                },
                {
                    "sent": "Each of these distances treated as a different similarity feature for a total of 4 features.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In total we have 7 similarity features which are treated independently.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each model we download from Pretty Warehouse, we align it using the estimated camera parameters and compute similarity features represented by the vector X.",
                    "label": 0
                },
                {
                    "sent": "This process is repeat.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thousands of times to compute similarity features for each 3D model we download from Google Warehouse.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A linear combination of these features is used to score how well each 3D model matches the input image.",
                    "label": 0
                },
                {
                    "sent": "In order to learn a proper weighting, these features will need some training data.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we had ground truth surface normals for an image, we could go through 3D warehouse, align each model, and compare the masked surface normals to determine the best matching scene from the data set.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We could also determine which 3D models do not match the image well.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, for.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given image, we can compute a distance Delta indicating how much better one model matches than another model does.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Howl.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or we don't have a ground truth data for this task.",
                    "label": 0
                },
                {
                    "sent": "There does not yet exist a data set of images and their corresponding detailed 3D models.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thus we.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did our own data set.",
                    "label": 0
                },
                {
                    "sent": "We had a team of annotators create detailed 3D models using Google SketchUp.",
                    "label": 0
                },
                {
                    "sent": "SketchUp allows you to take labeled lines to indicate vanishing points, set the locations of walls and floors and insert objects from a repository to handcraft detailed 3D models which nicely aligned with an input image.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've created a set of annotated geometry's for 500 randomly selected bedroom and living room images from the Suns scene, understanding data set.",
                    "label": 0
                },
                {
                    "sent": "For each image.",
                    "label": 0
                },
                {
                    "sent": "We have detailed 3D models, object labels as well as ground truth intrinsic and extrinsic camera parameters.",
                    "label": 0
                },
                {
                    "sent": "This data set will be made publicly available for researchers to download starting next.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that we have this data, we can use it to train a support vector.",
                    "label": 0
                },
                {
                    "sent": "Rank are as follows.",
                    "label": 0
                },
                {
                    "sent": "Given an image and it's manually annotated geometry, we can retrieve the top 3D models from Google Warehouse which most closely matched the manually annotated geometry using a service normal comparison.",
                    "label": 0
                },
                {
                    "sent": "We can also retrieve 3D models which poorly matched the annotated geometry.",
                    "label": 1
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we select one good 3D model and one bad 3D model, we know that the one on the left should rank higher than the one on the right.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mathematically, this simply translates to W transpose X for the first model being higher than W, transpose X for the second model.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We include the margin Delta, which indicated how much better the first model matched the image than the second model did.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this constraint is incorporated into a standard optimization to solve for the weight vector which best ranks pairs of 3D models for each training image.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The slack variable is also included in the support vector ranking formulation.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This equation is optimized using stochastic subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "In each iteration, we randomly selected training image.",
                    "label": 0
                },
                {
                    "sent": "I and a pair of 3D models J&K.",
                    "label": 0
                },
                {
                    "sent": "If the current weight vector causes the pair of 3D models to be incorrectly ranked or if their distance difference in scores is less than the margin Delta, we compute a subgradient and update the weight vector.",
                    "label": 0
                },
                {
                    "sent": "This process is repeated until convergence.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, given our learned weight vector, we can score every hypothesis from 3D warehouse and find the top ranking 3D models.",
                    "label": 0
                },
                {
                    "sent": "Here you can see that these models not only contain a couch in the correct position, they also tend to match the style of the furniture.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our data driven framework also provides a mechanism for transferring rich information from these models back to the input image, such as object labels, their segmentations, surface, normals, and even death.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now review some results of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here you can see an image from the Sun's data set of.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Living room.",
                    "label": 0
                },
                {
                    "sent": "And here's the automatically matched 3D model aligned to the input image in the upper right.",
                    "label": 0
                },
                {
                    "sent": "We showed transferred segmentation masks color coded by object categories.",
                    "label": 0
                },
                {
                    "sent": "Here, couches are labeled yellow and tables are labeled red.",
                    "label": 0
                },
                {
                    "sent": "We also show the predicted surface normals and depths below.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example of the living room scene where we correctly predicted the locations and orientations of couches and seats.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This example shows the bedroom scene with nicely matched and align night stands and in bed.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many images in our data set include her.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The included objects, such as this night stand here a traditional bottom up appearance based object detector would typically fail to detect this instance.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, our holistic scene matching approach is capable of finding these challenging objects as shown.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two additional examples of our algorithm detecting heavily occluded objects.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because our approach is 3D data, we are inherently invariant to rotation.",
                    "label": 0
                },
                {
                    "sent": "That means we're able to recover the labels, locations, and orientations of objects even from obscur viewpoints.",
                    "label": 0
                },
                {
                    "sent": "For example, this image was captured from behind the couch.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, we are still able to correctly identify this unique layout.",
                    "label": 0
                },
                {
                    "sent": "Recovering the position and orientation of both couches and the coffee table.",
                    "label": 0
                },
                {
                    "sent": "This is because, although the viewpoint is unusual, the arrangement of furniture is fairly common.",
                    "label": 0
                },
                {
                    "sent": "Two couches at right angles to each other, with a coffee table in the center.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, this is computer vision doesn't always work.",
                    "label": 0
                },
                {
                    "sent": "A fundamental issue with our approach is our reliance on auto calibration and room layout estimation.",
                    "label": 0
                },
                {
                    "sent": "If this stage of the pipeline fails, we may incorrectly estimate the arrangements of objects in the scene.",
                    "label": 0
                },
                {
                    "sent": "Here is an example error due to incorrect vanishing point estimation which has the complete failure of our system.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this stage in this situation, it was the model alignment stage of the pipeline that failed.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, if we used annotated camera parameters.",
                    "label": 0
                },
                {
                    "sent": "It's possible that we could use this information for alignment and go through with the rest of the process of computing similarity features, ranking each hypothesis and generating a good arrangement of furnitures.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of this.",
                    "label": 0
                },
                {
                    "sent": "For this image had Dowdall's room layout algorithm failed to correctly estimate the locations of walls and floor.",
                    "label": 0
                },
                {
                    "sent": "This causes our geometry matching process to fail to find a good furniture arrangement.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, if we had used the groundtruth camera parameters from the annotation process and use this information for aligning, we would have retrieved an excellent match for the furniture arrangement.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's now go into a quantitative evaluation.",
                    "label": 0
                },
                {
                    "sent": "One way to quantify the quality of our 3D models is to measure how accurately we predict the orientation of surface is in the image.",
                    "label": 0
                },
                {
                    "sent": "We score 3D hypothesis by taking the dot product of the ground truth surface normals from the annotated models and the orientation of pixels of are rendered hypothesis normalized by the number of pixels in the image.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This score represents the percentage of pixels for which we have correctly identified the orientations.",
                    "label": 0
                },
                {
                    "sent": "Since the majority of pixels in most scenes correspond to walls or floor which are not informative to the task of quantifying the object geometries, we also report surface normal scores for only those pixels which belong to objects.",
                    "label": 0
                },
                {
                    "sent": "Did a couple the effects of incorrect room layout estimation from our primary goal of determining arrangements of objects.",
                    "label": 0
                },
                {
                    "sent": "We are reporting results using annotated room layouts.",
                    "label": 0
                },
                {
                    "sent": "And camera parameters as well as fully automatic results incorporating the room layout approach of a Dow at all.",
                    "label": 0
                },
                {
                    "sent": "For sale",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of images in our data set, we had two annotators create 3D models.",
                    "label": 0
                },
                {
                    "sent": "By comparing these different versions of the same geometry as, we can evaluate the subjectivity of this task and.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotation process, so here you can see human performance compared to the annotated layout just using the ground truth camera parameters as well as a fully automatic system for estimating the geometry's.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have evaluated our algorithm on for the free space of a broom because we have full 3D models of each room and known camera parameters, we can rectify the scene and view it from above.",
                    "label": 0
                },
                {
                    "sent": "Here you can see an automatically generated overhead architectural floor plan.",
                    "label": 0
                },
                {
                    "sent": "You can see that we have good estimates of the free space between objects such as the couch in the coffee table in the image on the bottom.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compare estimated object locations with ground truth object locations for each square inch of the floor that we predict to be occupied.",
                    "label": 0
                },
                {
                    "sent": "We compare it to the ground truth occupancy to report precision and recall.",
                    "label": 0
                },
                {
                    "sent": "We also show the F measure which incorporates both precision and recall into one metric as a baseline.",
                    "label": 0
                },
                {
                    "sent": "For comparison, we run the pre trained geometry estimation algorithm of Gupta ET al, whose performance is indicated on the right.",
                    "label": 0
                },
                {
                    "sent": "This metric is similar to the one presented at Cdr this year by Haddow at all in her free space estimation work.",
                    "label": 0
                },
                {
                    "sent": "Our paper.",
                    "label": 0
                },
                {
                    "sent": "It also includes this evaluation metric.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also perform a value metric evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our overhead views here we show the Heights of objects taken directly from warehouse data.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this information, we can create a voxelized representation of our scenes and compute how precisely we estimate which voxels are occupied.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion, I presented a Gmail learning framework capable of lining 3D models to an input image, computing features which compare these models to the image and a mechanism for selecting which model best matches the input image, allowing us to transfer rich information from these models back to.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Input image.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your time and please check out our data set we created for this paper which will be online starting next week.",
                    "label": 0
                }
            ]
        }
    }
}