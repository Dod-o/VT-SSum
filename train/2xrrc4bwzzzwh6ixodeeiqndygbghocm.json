{
    "id": "2xrrc4bwzzzwh6ixodeeiqndygbghocm",
    "title": "Building Sparse Support Vector Machines for Multi-Instance Classification",
    "info": {
        "author": [
            "Zhouyu Fu, Monash University"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Classification"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_fu_building/",
    "segmentation": [
        [
            "Hi everyone, my name is Joey who I'm from Monash University in Australia and the work I'm presenting here is about building sparse support vector machines for multi instance classification and this is joint work with grudging Luke, limiting attention junk."
        ],
        [
            "I.",
            "The outline of my talk is as follows.",
            "1st, I'll cover the background briefly, introducing multi instance classification and sparse support vector machines, and then I go into the detail of sparse classified design for multi instance classification.",
            "Firstly present a labeling formulation which is simple formulation for multi instance classification which enjoys quite nice theoretical properties.",
            "And then I'll detail the sparse semi algorithm and experimental results will be presented, including both synthetic data and real real data.",
            "Finally, concluding remarks will be given."
        ],
        [
            "So the problem definition for multi instance classification and my classification is different from standard single instance classification in the sense that in single instance really just given features an labels and the purpose is to learn a feature map to learn a function mapping called classification classifier classification rule from the input feature vector to the output label.",
            "However, in multi instance classification.",
            "We are dealing with collection of feature vectors called bags.",
            "So each back basically verified number of feature vectors and only back level labels are given.",
            "Here we have the maltings assumption in the sense that a positive bag contains at least positive instance.",
            "Negative back contains negative instances only and purpose is to learn a classifier from bags to the output labels."
        ],
        [
            "To give you a better idea on multi SIM classification, here's a simple example with four bags.",
            "Positive instances are in circles and squares showed negative instances.",
            "I want to talk about positive instances and really talking about the true positive instances.",
            "Cousin positive bag you can see that.",
            "A positive feedback can contain negative instances.",
            "But the negative bags only contain negative instances, so it doesn't matter if you're positive that contains negative instances, as long as it contains at least one positive instances.",
            "Then the question arises here.",
            "Why do we need this classification scheme?",
            "What's the usefulness of this?"
        ],
        [
            "Classification.",
            "There are a couple of applications of multi instance classification.",
            "One is the drug activity prediction.",
            "This is actually one of the first applications for multi instance classification introduced by Dietrich in 1998.",
            "I think that's the first application of multi instance class paging.",
            "So injectivity prediction.",
            "Each track has different.",
            "I mean the molecules for each track has different deformations and only maybe few of these deformations are active.",
            "Others may be inactive, so you need to find out.",
            "Which definition is active?",
            "So in this case this is a multi multi instance classification problem, an for image classification.",
            "The purpose is to assign a label to the whole image.",
            "Of course we can extract a feature vector for each image based on color texture features, but.",
            "We are only interested in the foreground regions because only the foreground regions carry important information about same category.",
            "So better way to model this problem is probably to do image segmentation and then extract feature vectors from the regions.",
            "And try to formulate as a multi instance classification problem because for the target category it contains foreground which carries discriminate information for that category and for negative ones.",
            "If I'm trying to classify buildings then this one contains building information.",
            "But all of these doesn't contain any building information.",
            "So this is another example of multi instance classification."
        ],
        [
            "OK. And focusing on we are focusing on support vector machine classifiers for multi instance classification in this work because SVM is clearly currently the state of art in my classification.",
            "I reckon people should be quite familiar with them so I just skip the introduction of SVM.",
            "We're looking at here.",
            "Actually the prediction problem of kernel SVM.",
            "As we all know that kernel SVM basically a linear generalized linear model.",
            "In order to do prediction you have to evaluate.",
            "Your kernel values for all the 20 instances with non zero coefficients.",
            "So these are called support vectors.",
            "So the number of support vectors further depends on the training data size.",
            "If you have a very large data set and most likely you end up with a large number of support vectors.",
            "Also, it depends on the complexity of the problem.",
            "If the decision boundary is quite complicated, then you probably have a larger number of support vectors, so it's highly derivable if we have a support vector machine classifier with a few support vectors.",
            "When the prediction speed is a major concern."
        ],
        [
            "An why sparsity is important for multi instance classification.",
            "Of course, if it's important for standard one then it should be important for multi instant classification and I would argue that it's actually more important here becauses in multi instance classification we easily encounter with larger number of feature vectors.",
            "Even the problem might be small with maybe a couple of 100 bags.",
            "Each bag may contain 10,000 tens or hundreds of instances.",
            "So you actually have larger scale problems in multi instance learning.",
            "Also, in multi instance classification back level prediction usually involves going through all the instances.",
            "And if you do quite slow prediction for the instances, then you probably won't get fast speed at the back level.",
            "So the challenge here is really computer sparse as spam classifier for MI classification comprising fewer number of VMS without compromising on performance.",
            "So both these directions have been explored, but to the best of our knowledge, sparse as I'm learning for multi instance classification has never been started before."
        ],
        [
            "We start with the formulation.",
            "It's very natural from the MI assumption.",
            "This is just a mathematical form of my assumption.",
            "It leads to this constraint.",
            "Basically, take the maximum of your instance predictions as your back predictions.",
            "This agrees very well with multi instance assumption.",
            "The problem that this Max constraint is really nasty here.",
            "It leads to very difficult optimization problem with non differentiable nonconvex optimization target and constraint.",
            "And learning sparse and my classifier would further complicate the issue here.",
            "'cause we need to enforce additional constraint on the sparsity of F. So we really need a simpler method that can do well.",
            "So we propose to use label mean.",
            "So here instead of taking maximum, we take."
        ],
        [
            "Average.",
            "So the pros and cons of.",
            "Label mean formulation.",
            "It's very simple an it's with guarantee of global optimality.",
            "However, it's a violation of my assumption.",
            "Can you see that?",
            "So this is the wrong model.",
            "But I."
        ],
        [
            "Argue not necessarily the case."
        ],
        [
            "This is shown by connecting it with an existing method called MI kernel by Gartner.",
            "So here are the key results.",
            "The first lemma, second, the main theorem from the paper so.",
            "I'm just going to give some intuition."
        ],
        [
            "Sing a color.",
            "Skip this one so.",
            "The intuition is our if instances are linearly separable, then we can construct our back level classifier using the label mean.",
            "And separable by a polynomial kernel.",
            "This is from the previous theorem.",
            "So if instance is separable by Gaussian kernel, then we can separate the bags using different Gaussian kernel with slightly larger bandwidth.",
            "Using the same label mean formulation.",
            "Note here a very important observation here is that instant prediction function has to be a kernel classifier.",
            "Because from the first point, even if it's linearly separable at instance level, it's not necessarily linearly separable.",
            "If you can short linear kernel at the kernel level at the back level, so linear classifier is unlikely to do good job for back discrimination, so that's why we're concerned.",
            "Actually, to develop this sparse at my algorithm be cause for multi instance learning most useful.",
            "The most useful classifier would probably be kernel SVM, and we use Gaussian kernel in our work, but the algorithm developed here is quite general, so it doesn't have to be Gaussian."
        ],
        [
            "So.",
            "As you can see, if we do this with label mean, we still have a quite complicated prediction function.",
            "It really depends on the coefficients and the number of instances with non 0."
        ],
        [
            "Revisions and if we expand it.",
            "It all comes down to reducing those support vectors."
        ],
        [
            "So.",
            "We have the sparse at my problem defined here.",
            "The objective is to find discriminant function classifier with small number of support vectors for MI classification.",
            "There are a couple of alternatives, can use random subsampling.",
            "Just choosing that randomly from training set.",
            "Or you can approximate learned classifier by minimizing the reconstruction error using the reduced set method.",
            "There's also this miles method by enforcing sparsity on the coefficients using L1 norm that Sinon trick."
        ],
        [
            "So the proposed approach.",
            "We tried to formula as a single optimization problem which jointly learns the classifier, weights and support vectors.",
            "We use square hinge loss here because we need to have differentiable.",
            "Objective function the whole objective function is not convex.",
            "But it is convex given that which is the concatenation of the support vectors."
        ],
        [
            "And we can reformulate the problem just by minimizing.",
            "The target function.",
            "And express it in terms of just support vectors.",
            "So what's special about G here?",
            "It just depends on the solution of another optimization problem.",
            "So does this mean that it's more difficult to solve G?"
        ],
        [
            "Again, it's not now."
        ],
        [
            "Sara Lee so we have this theorem from Bonance.",
            "Actually.",
            "It shows that if both conditions satisfied, then we can."
        ],
        [
            "Have a differentiable G, so in this case we can compute derivative of G for each zed.",
            "As it does not depend on beta and B."
        ],
        [
            "So we have the algorithm here.",
            "It's basically gradient descent, 'cause we can compute gradient of the suspects that easily.",
            "And we use a very simple line search here a more."
        ],
        [
            "Involved linesearch technique, like Wolf condition, can be used, but yeah.",
            "Would add extra computational cost.",
            "I would like to make some remarks.",
            "So.",
            "The same optimizing stretch has been adopting solving existing problems like simple MCL and SCAF.",
            "Actually ours is most related to SK away with two main differences as KLA.",
            "Cannot be used to solve MI classification problem directly, 'cause it need my formulation with unique optimal solution an it performs optimization and updates of fees in the dual formulation hours used optimization in the primal which is much more efficient."
        ],
        [
            "Training.",
            "So for the multiclass Spark semi we can use combination classifier.",
            "Fusion scheme like the one versus sore, but in this case we have to learn their support vectors in a joint."
        ],
        [
            "In the middle of the process examples.",
            "In background, the ring are the negative instances initially.",
            "We just use one support vector.",
            "Of course the best one would be here.",
            "So we try to start our problem with very poor initialization."
        ],
        [
            "And.",
            "So just.",
            "First iteration, it moves here, so improves a lot."
        ],
        [
            "Anne.",
            "It kind of stabilize.",
            "Ask iteration 10."
        ],
        [
            "So if we look at cost value.",
            "It keeps decreasing."
        ],
        [
            "And this is a three class problem, and the optimal would be around these two corners."
        ],
        [
            "Anne."
        ],
        [
            "First iteration eventually."
        ],
        [
            "And the."
        ],
        [
            "Change of function values.",
            "So we have 5 datasets for.",
            "Testing our algorithm."
        ],
        [
            "From the real data Ann, this is the table for performance comparison.",
            "We can see that within each group.",
            "We compare with others at the other two simple alternatives except for the two datasets.",
            "Seems our method not.",
            "Compare quite well with others other.",
            "Datasets we show consistent better results than these other two methods.",
            "Actually, our result can be compatible to my kernel, which is one of the best method for multi instance classification."
        ],
        [
            "These are the convergence results for the core eternity assets with number of iterations, function value decreases and accuracy.",
            "Gradually in."
        ],
        [
            "This is."
        ],
        [
            "Is another result.",
            "So the conclusions we propose a sparse SVM classifier for my classification, and we strong optimization of fezan classifier weights.",
            "With controlled sparsity.",
            "We can do efficient learning the primal formulation.",
            "Also, it's got Comperable performance, but much more efficient in testing 'cause it's got much less number of support vectors."
        ],
        [
            "So in future we plan to explore several directions.",
            "We can.",
            "Extend this for non ideal identical independent identically distributed instance distributions.",
            "We can also incorporate with alternate convex and my classifiers.",
            "There's one method in NIPS 2010.",
            "It has proposed a convex formulation for MI and with quite nice yard property, but a bit too complicated to be applied here.",
            "Also, we can explore the selection of SVS.",
            "Currently, yes we are chosen unbounded, so the data points could be here and it may end up that you eventually as we could could not be in your bunch of data points, so it would be nice if we have some strategy to choose the SV more wisely.",
            "This is actually the problem that caused.",
            "On the performance of the mask two data set.",
            "I.",
            "Of course the method, although our purposes develop method for multi instance learning, the proposed method can also be used for standard learning from the primal formulation.",
            "It should be much more faster than existing methods.",
            "Doing the job from the two forms thank you very much.",
            "Do you have any questions?",
            "So questions comments?",
            "Well, I guess everything is clear, so I just want to comment if I make so there's actually one other area where multi instance problems are very important and that's in fact in inductive logic programming there is this idea of.",
            "Called property propositional isation, and there you essentially convert your instances to bags so so this one other sort of place where this could apply the public.",
            "OK, thank you, thank you everybody that concludes the session.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, my name is Joey who I'm from Monash University in Australia and the work I'm presenting here is about building sparse support vector machines for multi instance classification and this is joint work with grudging Luke, limiting attention junk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The outline of my talk is as follows.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll cover the background briefly, introducing multi instance classification and sparse support vector machines, and then I go into the detail of sparse classified design for multi instance classification.",
                    "label": 1
                },
                {
                    "sent": "Firstly present a labeling formulation which is simple formulation for multi instance classification which enjoys quite nice theoretical properties.",
                    "label": 1
                },
                {
                    "sent": "And then I'll detail the sparse semi algorithm and experimental results will be presented, including both synthetic data and real real data.",
                    "label": 0
                },
                {
                    "sent": "Finally, concluding remarks will be given.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem definition for multi instance classification and my classification is different from standard single instance classification in the sense that in single instance really just given features an labels and the purpose is to learn a feature map to learn a function mapping called classification classifier classification rule from the input feature vector to the output label.",
                    "label": 0
                },
                {
                    "sent": "However, in multi instance classification.",
                    "label": 0
                },
                {
                    "sent": "We are dealing with collection of feature vectors called bags.",
                    "label": 0
                },
                {
                    "sent": "So each back basically verified number of feature vectors and only back level labels are given.",
                    "label": 0
                },
                {
                    "sent": "Here we have the maltings assumption in the sense that a positive bag contains at least positive instance.",
                    "label": 1
                },
                {
                    "sent": "Negative back contains negative instances only and purpose is to learn a classifier from bags to the output labels.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give you a better idea on multi SIM classification, here's a simple example with four bags.",
                    "label": 0
                },
                {
                    "sent": "Positive instances are in circles and squares showed negative instances.",
                    "label": 0
                },
                {
                    "sent": "I want to talk about positive instances and really talking about the true positive instances.",
                    "label": 0
                },
                {
                    "sent": "Cousin positive bag you can see that.",
                    "label": 0
                },
                {
                    "sent": "A positive feedback can contain negative instances.",
                    "label": 0
                },
                {
                    "sent": "But the negative bags only contain negative instances, so it doesn't matter if you're positive that contains negative instances, as long as it contains at least one positive instances.",
                    "label": 0
                },
                {
                    "sent": "Then the question arises here.",
                    "label": 0
                },
                {
                    "sent": "Why do we need this classification scheme?",
                    "label": 0
                },
                {
                    "sent": "What's the usefulness of this?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of applications of multi instance classification.",
                    "label": 1
                },
                {
                    "sent": "One is the drug activity prediction.",
                    "label": 1
                },
                {
                    "sent": "This is actually one of the first applications for multi instance classification introduced by Dietrich in 1998.",
                    "label": 0
                },
                {
                    "sent": "I think that's the first application of multi instance class paging.",
                    "label": 0
                },
                {
                    "sent": "So injectivity prediction.",
                    "label": 0
                },
                {
                    "sent": "Each track has different.",
                    "label": 0
                },
                {
                    "sent": "I mean the molecules for each track has different deformations and only maybe few of these deformations are active.",
                    "label": 0
                },
                {
                    "sent": "Others may be inactive, so you need to find out.",
                    "label": 0
                },
                {
                    "sent": "Which definition is active?",
                    "label": 0
                },
                {
                    "sent": "So in this case this is a multi multi instance classification problem, an for image classification.",
                    "label": 0
                },
                {
                    "sent": "The purpose is to assign a label to the whole image.",
                    "label": 0
                },
                {
                    "sent": "Of course we can extract a feature vector for each image based on color texture features, but.",
                    "label": 0
                },
                {
                    "sent": "We are only interested in the foreground regions because only the foreground regions carry important information about same category.",
                    "label": 0
                },
                {
                    "sent": "So better way to model this problem is probably to do image segmentation and then extract feature vectors from the regions.",
                    "label": 0
                },
                {
                    "sent": "And try to formulate as a multi instance classification problem because for the target category it contains foreground which carries discriminate information for that category and for negative ones.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to classify buildings then this one contains building information.",
                    "label": 0
                },
                {
                    "sent": "But all of these doesn't contain any building information.",
                    "label": 0
                },
                {
                    "sent": "So this is another example of multi instance classification.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And focusing on we are focusing on support vector machine classifiers for multi instance classification in this work because SVM is clearly currently the state of art in my classification.",
                    "label": 0
                },
                {
                    "sent": "I reckon people should be quite familiar with them so I just skip the introduction of SVM.",
                    "label": 0
                },
                {
                    "sent": "We're looking at here.",
                    "label": 0
                },
                {
                    "sent": "Actually the prediction problem of kernel SVM.",
                    "label": 0
                },
                {
                    "sent": "As we all know that kernel SVM basically a linear generalized linear model.",
                    "label": 0
                },
                {
                    "sent": "In order to do prediction you have to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Your kernel values for all the 20 instances with non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "So these are called support vectors.",
                    "label": 0
                },
                {
                    "sent": "So the number of support vectors further depends on the training data size.",
                    "label": 1
                },
                {
                    "sent": "If you have a very large data set and most likely you end up with a large number of support vectors.",
                    "label": 1
                },
                {
                    "sent": "Also, it depends on the complexity of the problem.",
                    "label": 0
                },
                {
                    "sent": "If the decision boundary is quite complicated, then you probably have a larger number of support vectors, so it's highly derivable if we have a support vector machine classifier with a few support vectors.",
                    "label": 1
                },
                {
                    "sent": "When the prediction speed is a major concern.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An why sparsity is important for multi instance classification.",
                    "label": 1
                },
                {
                    "sent": "Of course, if it's important for standard one then it should be important for multi instant classification and I would argue that it's actually more important here becauses in multi instance classification we easily encounter with larger number of feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Even the problem might be small with maybe a couple of 100 bags.",
                    "label": 0
                },
                {
                    "sent": "Each bag may contain 10,000 tens or hundreds of instances.",
                    "label": 0
                },
                {
                    "sent": "So you actually have larger scale problems in multi instance learning.",
                    "label": 0
                },
                {
                    "sent": "Also, in multi instance classification back level prediction usually involves going through all the instances.",
                    "label": 1
                },
                {
                    "sent": "And if you do quite slow prediction for the instances, then you probably won't get fast speed at the back level.",
                    "label": 0
                },
                {
                    "sent": "So the challenge here is really computer sparse as spam classifier for MI classification comprising fewer number of VMS without compromising on performance.",
                    "label": 1
                },
                {
                    "sent": "So both these directions have been explored, but to the best of our knowledge, sparse as I'm learning for multi instance classification has never been started before.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We start with the formulation.",
                    "label": 0
                },
                {
                    "sent": "It's very natural from the MI assumption.",
                    "label": 1
                },
                {
                    "sent": "This is just a mathematical form of my assumption.",
                    "label": 0
                },
                {
                    "sent": "It leads to this constraint.",
                    "label": 0
                },
                {
                    "sent": "Basically, take the maximum of your instance predictions as your back predictions.",
                    "label": 0
                },
                {
                    "sent": "This agrees very well with multi instance assumption.",
                    "label": 0
                },
                {
                    "sent": "The problem that this Max constraint is really nasty here.",
                    "label": 0
                },
                {
                    "sent": "It leads to very difficult optimization problem with non differentiable nonconvex optimization target and constraint.",
                    "label": 1
                },
                {
                    "sent": "And learning sparse and my classifier would further complicate the issue here.",
                    "label": 1
                },
                {
                    "sent": "'cause we need to enforce additional constraint on the sparsity of F. So we really need a simpler method that can do well.",
                    "label": 0
                },
                {
                    "sent": "So we propose to use label mean.",
                    "label": 0
                },
                {
                    "sent": "So here instead of taking maximum, we take.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Average.",
                    "label": 0
                },
                {
                    "sent": "So the pros and cons of.",
                    "label": 1
                },
                {
                    "sent": "Label mean formulation.",
                    "label": 0
                },
                {
                    "sent": "It's very simple an it's with guarantee of global optimality.",
                    "label": 1
                },
                {
                    "sent": "However, it's a violation of my assumption.",
                    "label": 0
                },
                {
                    "sent": "Can you see that?",
                    "label": 1
                },
                {
                    "sent": "So this is the wrong model.",
                    "label": 0
                },
                {
                    "sent": "But I.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Argue not necessarily the case.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is shown by connecting it with an existing method called MI kernel by Gartner.",
                    "label": 0
                },
                {
                    "sent": "So here are the key results.",
                    "label": 0
                },
                {
                    "sent": "The first lemma, second, the main theorem from the paper so.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to give some intuition.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sing a color.",
                    "label": 0
                },
                {
                    "sent": "Skip this one so.",
                    "label": 0
                },
                {
                    "sent": "The intuition is our if instances are linearly separable, then we can construct our back level classifier using the label mean.",
                    "label": 1
                },
                {
                    "sent": "And separable by a polynomial kernel.",
                    "label": 1
                },
                {
                    "sent": "This is from the previous theorem.",
                    "label": 1
                },
                {
                    "sent": "So if instance is separable by Gaussian kernel, then we can separate the bags using different Gaussian kernel with slightly larger bandwidth.",
                    "label": 1
                },
                {
                    "sent": "Using the same label mean formulation.",
                    "label": 1
                },
                {
                    "sent": "Note here a very important observation here is that instant prediction function has to be a kernel classifier.",
                    "label": 0
                },
                {
                    "sent": "Because from the first point, even if it's linearly separable at instance level, it's not necessarily linearly separable.",
                    "label": 0
                },
                {
                    "sent": "If you can short linear kernel at the kernel level at the back level, so linear classifier is unlikely to do good job for back discrimination, so that's why we're concerned.",
                    "label": 0
                },
                {
                    "sent": "Actually, to develop this sparse at my algorithm be cause for multi instance learning most useful.",
                    "label": 0
                },
                {
                    "sent": "The most useful classifier would probably be kernel SVM, and we use Gaussian kernel in our work, but the algorithm developed here is quite general, so it doesn't have to be Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As you can see, if we do this with label mean, we still have a quite complicated prediction function.",
                    "label": 0
                },
                {
                    "sent": "It really depends on the coefficients and the number of instances with non 0.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Revisions and if we expand it.",
                    "label": 0
                },
                {
                    "sent": "It all comes down to reducing those support vectors.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have the sparse at my problem defined here.",
                    "label": 0
                },
                {
                    "sent": "The objective is to find discriminant function classifier with small number of support vectors for MI classification.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of alternatives, can use random subsampling.",
                    "label": 0
                },
                {
                    "sent": "Just choosing that randomly from training set.",
                    "label": 1
                },
                {
                    "sent": "Or you can approximate learned classifier by minimizing the reconstruction error using the reduced set method.",
                    "label": 0
                },
                {
                    "sent": "There's also this miles method by enforcing sparsity on the coefficients using L1 norm that Sinon trick.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "We tried to formula as a single optimization problem which jointly learns the classifier, weights and support vectors.",
                    "label": 1
                },
                {
                    "sent": "We use square hinge loss here because we need to have differentiable.",
                    "label": 0
                },
                {
                    "sent": "Objective function the whole objective function is not convex.",
                    "label": 0
                },
                {
                    "sent": "But it is convex given that which is the concatenation of the support vectors.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can reformulate the problem just by minimizing.",
                    "label": 0
                },
                {
                    "sent": "The target function.",
                    "label": 0
                },
                {
                    "sent": "And express it in terms of just support vectors.",
                    "label": 0
                },
                {
                    "sent": "So what's special about G here?",
                    "label": 1
                },
                {
                    "sent": "It just depends on the solution of another optimization problem.",
                    "label": 1
                },
                {
                    "sent": "So does this mean that it's more difficult to solve G?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, it's not now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sara Lee so we have this theorem from Bonance.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "It shows that if both conditions satisfied, then we can.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a differentiable G, so in this case we can compute derivative of G for each zed.",
                    "label": 0
                },
                {
                    "sent": "As it does not depend on beta and B.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the algorithm here.",
                    "label": 0
                },
                {
                    "sent": "It's basically gradient descent, 'cause we can compute gradient of the suspects that easily.",
                    "label": 0
                },
                {
                    "sent": "And we use a very simple line search here a more.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Involved linesearch technique, like Wolf condition, can be used, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Would add extra computational cost.",
                    "label": 0
                },
                {
                    "sent": "I would like to make some remarks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The same optimizing stretch has been adopting solving existing problems like simple MCL and SCAF.",
                    "label": 0
                },
                {
                    "sent": "Actually ours is most related to SK away with two main differences as KLA.",
                    "label": 1
                },
                {
                    "sent": "Cannot be used to solve MI classification problem directly, 'cause it need my formulation with unique optimal solution an it performs optimization and updates of fees in the dual formulation hours used optimization in the primal which is much more efficient.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "So for the multiclass Spark semi we can use combination classifier.",
                    "label": 0
                },
                {
                    "sent": "Fusion scheme like the one versus sore, but in this case we have to learn their support vectors in a joint.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the middle of the process examples.",
                    "label": 0
                },
                {
                    "sent": "In background, the ring are the negative instances initially.",
                    "label": 0
                },
                {
                    "sent": "We just use one support vector.",
                    "label": 0
                },
                {
                    "sent": "Of course the best one would be here.",
                    "label": 0
                },
                {
                    "sent": "So we try to start our problem with very poor initialization.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                },
                {
                    "sent": "First iteration, it moves here, so improves a lot.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It kind of stabilize.",
                    "label": 0
                },
                {
                    "sent": "Ask iteration 10.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we look at cost value.",
                    "label": 0
                },
                {
                    "sent": "It keeps decreasing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a three class problem, and the optimal would be around these two corners.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First iteration eventually.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change of function values.",
                    "label": 0
                },
                {
                    "sent": "So we have 5 datasets for.",
                    "label": 0
                },
                {
                    "sent": "Testing our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the real data Ann, this is the table for performance comparison.",
                    "label": 0
                },
                {
                    "sent": "We can see that within each group.",
                    "label": 0
                },
                {
                    "sent": "We compare with others at the other two simple alternatives except for the two datasets.",
                    "label": 0
                },
                {
                    "sent": "Seems our method not.",
                    "label": 0
                },
                {
                    "sent": "Compare quite well with others other.",
                    "label": 0
                },
                {
                    "sent": "Datasets we show consistent better results than these other two methods.",
                    "label": 0
                },
                {
                    "sent": "Actually, our result can be compatible to my kernel, which is one of the best method for multi instance classification.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the convergence results for the core eternity assets with number of iterations, function value decreases and accuracy.",
                    "label": 0
                },
                {
                    "sent": "Gradually in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is another result.",
                    "label": 0
                },
                {
                    "sent": "So the conclusions we propose a sparse SVM classifier for my classification, and we strong optimization of fezan classifier weights.",
                    "label": 1
                },
                {
                    "sent": "With controlled sparsity.",
                    "label": 0
                },
                {
                    "sent": "We can do efficient learning the primal formulation.",
                    "label": 0
                },
                {
                    "sent": "Also, it's got Comperable performance, but much more efficient in testing 'cause it's got much less number of support vectors.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in future we plan to explore several directions.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Extend this for non ideal identical independent identically distributed instance distributions.",
                    "label": 1
                },
                {
                    "sent": "We can also incorporate with alternate convex and my classifiers.",
                    "label": 0
                },
                {
                    "sent": "There's one method in NIPS 2010.",
                    "label": 0
                },
                {
                    "sent": "It has proposed a convex formulation for MI and with quite nice yard property, but a bit too complicated to be applied here.",
                    "label": 0
                },
                {
                    "sent": "Also, we can explore the selection of SVS.",
                    "label": 0
                },
                {
                    "sent": "Currently, yes we are chosen unbounded, so the data points could be here and it may end up that you eventually as we could could not be in your bunch of data points, so it would be nice if we have some strategy to choose the SV more wisely.",
                    "label": 0
                },
                {
                    "sent": "This is actually the problem that caused.",
                    "label": 0
                },
                {
                    "sent": "On the performance of the mask two data set.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Of course the method, although our purposes develop method for multi instance learning, the proposed method can also be used for standard learning from the primal formulation.",
                    "label": 1
                },
                {
                    "sent": "It should be much more faster than existing methods.",
                    "label": 0
                },
                {
                    "sent": "Doing the job from the two forms thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "So questions comments?",
                    "label": 0
                },
                {
                    "sent": "Well, I guess everything is clear, so I just want to comment if I make so there's actually one other area where multi instance problems are very important and that's in fact in inductive logic programming there is this idea of.",
                    "label": 0
                },
                {
                    "sent": "Called property propositional isation, and there you essentially convert your instances to bags so so this one other sort of place where this could apply the public.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, thank you everybody that concludes the session.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}