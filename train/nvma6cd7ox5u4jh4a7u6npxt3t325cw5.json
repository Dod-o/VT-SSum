{
    "id": "nvma6cd7ox5u4jh4a7u6npxt3t325cw5",
    "title": "Selecting Diverse Features via Spectral Regularization",
    "info": {
        "author": [
            "Abhimanyu Das, Microsoft Research"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/machine_das_spectral/",
    "segmentation": [
        [
            "So I'm going to talk about the diverse feature selection problem here.",
            "This is joint work with an urban task, obtains every command.",
            "So."
        ],
        [
            "The problem is as follows.",
            "You're given a set of feature vectors and a target vector, and you want to select a small subset of features that are diverse and that can predict the target well.",
            "So what do I mean by diversity?",
            "Essentially I want to capture some notion of how close to orthogonal is your set of features that you select.",
            "And in this paper we deal with linear predictions.",
            "So the prediction objective is to maximize the squared multiple correlations R ^2.",
            "So why is diversity important?",
            "There are several reasons here.",
            "Are two of these.",
            "First, your model is more interpretable if it has diverse features, and Secondly the model parameters are more robust to noise in the input data."
        ],
        [
            "So let me let's say this problem with a simple example in the three dimensional subspace.",
            "Suppose you want to predict this target vector T that lies.",
            "Along the diagonal of the XY plane.",
            "And to help you do that, you're given 4 feature vectors.",
            "We run through V4V1 lies along the X axis.",
            "Virtue lies along the Y axis.",
            "We tree has a large component along the diagonal of the XY plane, but it also has a small Z component with value epsilon.",
            "And we follow the same as we tree, except that it has a Z component of minus epsilon and the goal is to predict T using at most two of these features.",
            "So the question is, which two features will you choose?",
            "So you can clearly see from this picture that in this case, both V1 and V2 and V3 and V4 are equally good in terms of predicting the target.",
            "However, if you want to select diverse features, then you're better off choosing V1 and V2 instead, because their orthogonal.",
            "On the other hand, existing feature selection schemes like greedy or L1, will end up selecting V3 and V4 instead.",
            "So these schemes do not really address feature diversity."
        ],
        [
            "So we formalize this problem in an optimization framework as follows.",
            "You're given the target vector T and the set U of feature vectors, and you want to find the subset South of at most K features that maximize the following.",
            "The AH square of the feature set, which is the prediction quality plus a constant times a regularization function F of S&FS is what we call a diversity promoting regularizer.",
            "It's a class of spectral set functions that measure the diversity of your feature set.",
            "And by spectral sight functions I mean functions of the eigenvalues of your feature covariance matrix.",
            "For example, the entropy regularizer is one such function, it's it's just the sum of the log of eigenvalues of the features that you select.",
            "And we show that we can actually cast this diverse feature selection problem as an approximate non monotone submodular optimization problem.",
            "And while this problem is NP hard, we design a simple algorithm based on a mixture of greedy and local search ideas.",
            "That gives us good approximation guarantees, and if you want to learn more about the algorithm or this work in general, please stop by our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about the diverse feature selection problem here.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with an urban task, obtains every command.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is as follows.",
                    "label": 0
                },
                {
                    "sent": "You're given a set of feature vectors and a target vector, and you want to select a small subset of features that are diverse and that can predict the target well.",
                    "label": 1
                },
                {
                    "sent": "So what do I mean by diversity?",
                    "label": 1
                },
                {
                    "sent": "Essentially I want to capture some notion of how close to orthogonal is your set of features that you select.",
                    "label": 0
                },
                {
                    "sent": "And in this paper we deal with linear predictions.",
                    "label": 1
                },
                {
                    "sent": "So the prediction objective is to maximize the squared multiple correlations R ^2.",
                    "label": 0
                },
                {
                    "sent": "So why is diversity important?",
                    "label": 0
                },
                {
                    "sent": "There are several reasons here.",
                    "label": 0
                },
                {
                    "sent": "Are two of these.",
                    "label": 0
                },
                {
                    "sent": "First, your model is more interpretable if it has diverse features, and Secondly the model parameters are more robust to noise in the input data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me let's say this problem with a simple example in the three dimensional subspace.",
                    "label": 0
                },
                {
                    "sent": "Suppose you want to predict this target vector T that lies.",
                    "label": 0
                },
                {
                    "sent": "Along the diagonal of the XY plane.",
                    "label": 0
                },
                {
                    "sent": "And to help you do that, you're given 4 feature vectors.",
                    "label": 0
                },
                {
                    "sent": "We run through V4V1 lies along the X axis.",
                    "label": 0
                },
                {
                    "sent": "Virtue lies along the Y axis.",
                    "label": 0
                },
                {
                    "sent": "We tree has a large component along the diagonal of the XY plane, but it also has a small Z component with value epsilon.",
                    "label": 0
                },
                {
                    "sent": "And we follow the same as we tree, except that it has a Z component of minus epsilon and the goal is to predict T using at most two of these features.",
                    "label": 1
                },
                {
                    "sent": "So the question is, which two features will you choose?",
                    "label": 0
                },
                {
                    "sent": "So you can clearly see from this picture that in this case, both V1 and V2 and V3 and V4 are equally good in terms of predicting the target.",
                    "label": 0
                },
                {
                    "sent": "However, if you want to select diverse features, then you're better off choosing V1 and V2 instead, because their orthogonal.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, existing feature selection schemes like greedy or L1, will end up selecting V3 and V4 instead.",
                    "label": 0
                },
                {
                    "sent": "So these schemes do not really address feature diversity.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we formalize this problem in an optimization framework as follows.",
                    "label": 0
                },
                {
                    "sent": "You're given the target vector T and the set U of feature vectors, and you want to find the subset South of at most K features that maximize the following.",
                    "label": 1
                },
                {
                    "sent": "The AH square of the feature set, which is the prediction quality plus a constant times a regularization function F of S&FS is what we call a diversity promoting regularizer.",
                    "label": 0
                },
                {
                    "sent": "It's a class of spectral set functions that measure the diversity of your feature set.",
                    "label": 1
                },
                {
                    "sent": "And by spectral sight functions I mean functions of the eigenvalues of your feature covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "For example, the entropy regularizer is one such function, it's it's just the sum of the log of eigenvalues of the features that you select.",
                    "label": 1
                },
                {
                    "sent": "And we show that we can actually cast this diverse feature selection problem as an approximate non monotone submodular optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And while this problem is NP hard, we design a simple algorithm based on a mixture of greedy and local search ideas.",
                    "label": 0
                },
                {
                    "sent": "That gives us good approximation guarantees, and if you want to learn more about the algorithm or this work in general, please stop by our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}