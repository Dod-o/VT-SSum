{
    "id": "yivfc7dt6olsojwdessfb5ygmfbc43g2",
    "title": "Similarity Word-Sequence Kernels for Sentence Clustering",
    "info": {
        "author": [
            "Jesus Andres-Ferrer, Technical University of Valencia (UPV)"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_andres_ferrer_swsk/",
    "segmentation": [
        [
            "My name is Cassandra's for Aaron from the from the University of Valencia, and I have developed this this world with her massages.",
            "That's going to give a dog after this one an Francisco and what we have done is to use some kernels like like similarity measures for sentence."
        ],
        [
            "Clustering, so they talk the index of the talk.",
            "Something like that we're going to throw an introduction and then we are too.",
            "We are going to review the seeming clustering algorithm.",
            "And afterwards we're going to review the war sequence kernels in which we have a proposal.",
            "Modifications of this standard.",
            "Word sequence kernels that were presented by consider after we're going to stand those corners to the building while.",
            "Today we went to the worst sequence kernels which are going to be used in building while synthesis corpora and we're going to see some experiments in conclusion."
        ],
        [
            "Suggestion.",
            "So as you know, this classification is problem in which we have unknown number of classes an for a given document or text.",
            "We want to classify it in this set.",
            "We are dealing with a similar problem here, which is discussed in which we don't know the set of classes in which we have to classify the documents.",
            "But even more we modify the document and we constrain it just to be made up of one single sentence that we what we call sentence clustering.",
            "And since we we plan to use that for statistical machine translation in which you have.",
            "Sentence aligned corpora form from 2 languages on the source language and the target language.",
            "We stand in there for billing while sentence clustering, so the motivation of this work.",
            "It's mainly two points.",
            "One is to make a domain adaptation of the statistical models.",
            "So if we fly and find some clustering from the sentence, you can try a specific model for each of those clusters and then with this information you you when you are given a new sentence.",
            "You can determine this domain from the clusters and also since the domains or the cluster are is more than the original training data.",
            "If you training algorithm is not linearly in time, you will get some spy.",
            "Speed up.",
            "For training those algorithms.",
            "So we know the clustering problem is an NP hard problem.",
            "On most of the algorithms, not all, but most of the algorithms use distance for doing this clustering.",
            "A very famous on will spread algorithm is the C means, which is a greedy algorithm for finding the clusters."
        ],
        [
            "This seems algorithms.",
            "It was in the previous slide has component is that it's not able to.",
            "Is not able to.",
            "Two clusters to properly find clusters in data set which is not linearly separable.",
            "So for for Checkel for tackling this problem.",
            "Market allow me presented a modified version in which you use kernel.",
            "So the UI.",
            "Map the input data into a high dimensional space and then you try to find the clusters in this in this space so that with the hope that you will be able to.",
            "Did you find the suitable clusters in this higher domain?",
            "But usually they use like like distance.",
            "What we we proposed in this work is just.",
            "Use the kernels as a similarity between the object, not like a distance in the higher space domain.",
            "So it is important to remember that the kernels are symmetric, which is one of the requirements for the similarity."
        ],
        [
            "This seems I clustering algorithm.",
            "As you may recall.",
            "Even a set of of samples we are, we want to find the cluster for these samples and we usually pose the problem something like that in which we have an index variable for each sample.",
            "And then we want to minimize the distance between each sample to the centroid of the class to which it belongs to.",
            "So that's essentially this equation.",
            "This is very common where the MC is the centroid of each cluster, ansehen, and set subnet C. Is 1 index variable one if this number belongs to the cluster C and zero if it does not?",
            "So the distance that usually used in this algorithm is the two norm, so it can be another one.",
            "And one important property is that you have to use a similar metric.",
            "At least show that this algorithm converges."
        ],
        [
            "So the extension that was proposed by Margella meme.",
            "What does is that just?",
            "We simply modify the distance, but the distance in the higher domain of the kernel.",
            "And then with us with that, we are able to apply the Siemens algorithm.",
            "So.",
            "Here we are going to review the properties of metric, so there were some talks that we did yesterday so.",
            "A metric has free for properties and depending on which properties do we have with or function without distance, we can call the metric semi metric or similar metric.",
            "So a cinematic is a function that is symmetric and is positive.",
            "So if you look at the kernels we only need the current positive in order to have a similar metric.",
            "OK, a cinematic is a similar metric that also has this identity of interesting levels.",
            "That means that if an object has a distance zero another object, then the object is the same OK. Ann for venema.",
            "Did we also need a jungle inequality?",
            "OK, is also well known in mathematics that.",
            "I wish that you have distance.",
            "You can define a similarity using this distance."
        ],
        [
            "So our proposal.",
            "Is just too.",
            "Erase or to change this in the in the Siemens algorithm in the question to the distance.",
            "For a similarity measure and instead of minimizing maximizing this."
        ],
        [
            "Clarity.",
            "For the night we change the deep for this South, and we can.",
            "We can use a kernel for computing this dissimilarity, as long as the kernel.",
            "Has some of the properties that are similarity needs."
        ],
        [
            "So that was regarding to the Siemens algorithm, and now we're going to.",
            "To see so many some kernels that we have used for the sending clustering problem.",
            "So we have a base of work in the work of cancer in which the authors proposed awards, sequence kernels, or sequence kernels is a mapping.",
            "Is well, this is another product in a higher space domain which is compute like a mapping and this mapping what they might be notice given an value N which is going to be the engram value.",
            "What the kernel does, which is the question 6 confused and some of the product of the number of time we see each ngram.",
            "In each of the sentence.",
            "So essentially we have two sentences and we're computing bigrams.",
            "We go through all the BI grams of the first sentence and try to find this engram in the second sentence.",
            "So we multiply the number of times each engram occurs in the Champions.",
            "That has a problem from our point of view.",
            "Just because of the application of of this kernel that we will see in an example.",
            "In a common slides.",
            "And is that if we have seen in one sentence by ground twice, and we see this, this this this bigram, once in the last sentence, then we are going to actually connect true, no one, so that might give some point, for instance, for the single dose that occur in a sentence, for instance the articles.",
            "They were D. For instance.",
            "You have seen the word in one sentence and then you see another sentence just once the D. Then you are going to have a very high similarity.",
            "OK, this is going to be more clear in an example that we're going to see in some slides.",
            "So."
        ],
        [
            "To try to solve this problem, what we have done is we have a clip.",
            "The word sequence kernel.",
            "We have previously proposal, so we count the bigrams, but we just allow them to come one or zero.",
            "So if you have seen it has been seen three times, we just call it like 1.",
            "So this is actually similarity measure, so it's positive an is symmetric."
        ],
        [
            "And then we have fashion normalized as kernels.",
            "We have no other scanners so that they were between zero and one because if not the we have a problem with different sentence lengths.",
            "Because if you have a sentence of five and you compare that with an election file, you are going to have at most a similarity of five.",
            "But you have a sentence of 10 and then you compare that with sentence of length 10.",
            "You're going to have a similarity of 10, which is not good for comparing different lengths.",
            "Because it seems that this this sentence is more similarity to another 10% of four to five words sentences."
        ],
        [
            "Finally, as we know form from the natural Language processing task, the engrams arbores purse of a exponential decaying.",
            "Occurrence probability depending on the size of the engram.",
            "So in order to to overcome this problem, we have proposed this.",
            "Some word sequence kernels that initial into into account the sum of all the.",
            "The current occurrence that we have previously defined.",
            "For increasing or Ingram orders, so this K. By N is this is the sum of all the previous KN.",
            "This gives some smoothing to the computing."
        ],
        [
            "The Colonel.",
            "So why it's important to define this this different word sequence kernels and why we didn't just use the Concetta version?",
            "If you look at this example, we have four strings.",
            "OK, so since we are using using sentences for us, the symbols of this string in all tasks are going to be words not not characters on anything.",
            "So if you look at the one S2S3 and S4 for many of you, if they were words S one is more similar to S2 and two S3 than two S4.",
            "OK, because there's only one one.",
            "One word of difference.",
            "But if you use the well, obviously we are assuming here 11 state distance.",
            "From the sentence level."
        ],
        [
            "And if we analyze every show that by Graham will give us, give us with the catheter originally or sequence, we can see that the similarity between S1 and S2 is 2 with S1S3 is 1 and with this one too is S4 is 4.",
            "That means that this one is more similar to this four unto itself.",
            "And therefore sentences is not good.",
            "OK, so if we apply."
        ],
        [
            "The eclipse version this summer one is because we have cleared the codes of engrams.",
            "So the kernel corn is 113 and three, so we have in some way we have fixed the problem that we have in the previous kernel that as sentence which is not the same sentence can have a higher similarity.",
            "To his sentence two to itself.",
            "So in this case, what we have is that at most dissenters will have the same similarity that we solve, But yet this is a similar metric if we."
        ],
        [
            "You supply the normalized version.",
            "Then we fixed all those problems and what we have is that a sentence can only be equal only half a similarity of 1 if it's the itself.",
            "So that's more similar.",
            "That's what we wanted because it's very similar to the Levenshtein distance.",
            "OK, I thought this is just symmetric because it does not verify the edge angle inequality."
        ],
        [
            "So if your input instead of 1 sentence.",
            "So instead of trying to cluster one sentence an another sentence or such, a set of sentences from one language, we have to document a line, a sentence level and we have to find clusters inside this align document.",
            "Each awful elements of the kernel will be made up of two sentences, one from the English, for instance another from Spanish, and they will be.",
            "They will be the translation from 1 phone.",
            "One each to the other.",
            "So in order to do that, we have extended the word sequence kernels today William Barr case, in which.",
            "Each of the parameters of the kernel is made up of two of two sentence.",
            "The shows on the target sentence.",
            "We have compute that using the sum roll.",
            "So we just sum the kernel of the source part and then the kernel of the tire target part of the.",
            "Of the building was synthesis so he can extend all the kernels that we were sequence kernel that we have proposed for the monolingual case to the building.",
            "Worked with this simple rule.",
            "Obviously, in the future we could add some weights to the kernels, but for the moment we have.",
            "We haven't tried that."
        ],
        [
            "So.",
            "Now we are going to see some experiments to check how these kernels and this clustering algorithm works in practice.",
            "We have used two corpora.",
            "One is more corpora, which is the BTC which is a basic travel special corpus.",
            "From the IW St. Conference or task and we have.",
            "Yes it would have used the Chinese to English direction.",
            "So we have documents on Chinese.",
            "We have documents on English and we have demaline at sentence level.",
            "So we have we want to find clusters in this doc in those documents as sensors level the number of centers of this of this training corpora.",
            "Is that 20,000 words which is not very very huge.",
            "And you can see that the perplexity computes with the trigram language model.",
            "E 24 which that?",
            "That means that if we are given a prefix of three or or 20 words in average, given the information of the language model, we have to decide among 24 words.",
            "In order to predict the following word OK. And we have also used the Euro bar Division Three, which is a very well known tax for statistical machine translation, but we have limited the length of the sentence.",
            "2:20 at most.",
            "That's because of computational problems.",
            "You know that you compute the clusters on those things, so you can see both have similar perplexities.",
            "But the second one is larger than the first one.",
            "So in order to compute the kernels, we have filter out all the single tones that were occurring, just one in the corporate because they do not affect the computation of the kernels.",
            "Since you need it to occur twice at least in order to find a colon anagram.",
            "And we have also filter office towards, so they must frequent words because they are mainly they occur on all these sentences and they are mainly noise."
        ],
        [
            "So one problem is how do we measure this?",
            "This clustering algorithm?",
            "Obviously we are using the same inside with him and theoretically.",
            "Is a greedy algorithm and we're going to get a drop in the in the distance or in the similarity.",
            "Because it's a function that we're optimizing, so we decide to find some true alternative measures which are not depending on the on the on the on the list and only similarity, but are well, well measure.",
            "For measuring the complexity of the of the cluster you have.",
            "So in fact cluster is done properly, you will have a lower perplexity in this in this cluster, so we started with a public city of 24, for instance for the BTC.",
            "Corpora and so if we divide that into clusters and the cluster, half proper sentences, sentences that are related one to each other, we're going to be able to build the N gram language model in those clusters, and we're going to be able to drop this perplexity.",
            "Another measure that we could use is the edit distance.",
            "In practice, both both both distance are equivalent, so we have used in the in the in the cluster perplexity because it's faster to compute.",
            "But they are.",
            "They are very very very similar in preview in.",
            "Previous work we have we have developed.",
            "So the intercluster perplexity is the geometric average of the perplexity of in each cluster, compute with an N gram language model.",
            "Compute in this cluster."
        ],
        [
            "So in this plot is one of the results we have used these small corporate EBT I see and we have used the Chinese and English reaction we have.",
            "We here compare two kernels.",
            "We compare the decay so the clip kernel that Canada clip the Conan Grams to one.",
            "So if an agent has of course.",
            "Twice it comes just one and then we multiply them and the billing war version and we have a subplot or random clustering.",
            "So you see that we have the average perplexity in the cluster perplexity.",
            "And we have the number of cluster you can see that if you don't use.",
            "Our our clustering algorithm and you just use the random splits of the of the training of the training corpora.",
            "Then you don't get an improvement and you can also see that if you have an old data on your clusters, because remember that this corporate had 2020 thousand sentences.",
            "So in this case we only have 1000 sentences on average per cluster, so you have enough sentences in your clusters.",
            "You can use the billing information.",
            "To try to improve your clustering.",
            "Also, this is the by grammar, the bigram, a version of the kernels.",
            "So the next."
        ],
        [
            "Say we have a similar plot, but this time we have.",
            "We have plot all the.",
            "The same kernel for four different engrams grace in order of engrams.",
            "So we can see we have dinner.",
            "A result which is similar to that obtained.",
            "By the work of Cansada, in which the best performing.",
            "Colonel is the bigram.",
            "OK.",
            "So, so the results are not totally statistically significant, they are pretty stable and this is a way on.",
            "The bigram is always better."
        ],
        [
            "So what I would allocate corporate like the euro part?",
            "OK, Europe has over 100 more.",
            "It's like 2 orders of magnitude larger than the previous one, and we can see that if we compare the same billing, why cluster the same that we compare in the?",
            "In the by BTS EC task.",
            "Within a similar plot.",
            "So it essentially means that that it scales behavior scales very well."
        ],
        [
            "And again, if we compare the different order of the engrams, we see that the bigram is again the better one, so there is not a statistically significant.",
            "The bigram always keys the best result.",
            "So."
        ],
        [
            "YN grams there's one question that we ask yourself.",
            "You have to remember that we are using sentences.",
            "So if you.",
            "Obviously by Graham has more structural information.",
            "A unigram trigram has more useful information than a bigram, and so on.",
            "But the problem is that the single tones on the double tones statistics for bigrams are quite low.",
            "For the bigram are still quite low, but if you go to the trigrams you see that almost 91% of the of the trigrams, or just one or two times in the in a sentence.",
            "So that means that you only have over 9% or 5% of the of the bigrams, or the OR the trigrams or programs, or Kareem in more than two sentences.",
            "So essentially so they have more complex, more complex information.",
            "And they are.",
            "They are.",
            "The numbers are very, very low, just so we don't have a car purpose.",
            "Larger and not to take advantage of these engrams?",
            "That's probably because we are we're constraining ourselves to sentences if we were using documents, maybe the thing would be different."
        ],
        [
            "And the conclusions.",
            "OK.",
            "So we have proposed similarity based clustering algorithm based on this Siemens Siemens.",
            "Sorry.",
            "We have proposed several families of cluster for these tax base in a work or work in the cancer network.",
            "We have studied those those those kernels to the building work case.",
            "And we have to serve that the best performing kernels in practice are the bigrams, and especially specifically the billing world, bigram kernels.",
            "In practice that I have not given you diploid because you have limited time and a limited space in the in the paper.",
            "The clip version and the version behave the same, because in practice there is no difference at the sentence level.",
            "Once you have raised the non stop words.",
            "OK so in practice the both kernels are the same.",
            "But theoretically, for instance, the clip Colonel is a semi semi minority and the other one is not so.",
            "We have proved that the wealth while we have shown that the intercluster perplexity.",
            "Does not provide insight into why the same.",
            "The optimal number of cluster, because it's always decreasing, so we do not know when to end the the plot of the number of clusters.",
            "One interesting thing that we want to do for the future is to see the relationship that the kernels have with the gentleman approach in which they use the kernel, like a distant measure and not like a similarity measure.",
            "And while there are some extensions that can be made on this."
        ],
        [
            "And that's all you have any question.",
            "Very much."
        ],
        [
            "Are there any questions or comments?",
            "Use of additional external data in order to have more trigrams, yeah.",
            "The clustering problem independently on the date yet, but the problem is more than the data is is that the constraint of the sentence OK?",
            "Because you have a lot of data, but all your sentences has half a length of 10 works, then the probability of this trigram to occur in the other sentence, which also has a length of 12 or 10 is very slow.",
            "So the problem is not not for the training data, but for the constraint of the document.",
            "Maybe as a sort of standard question.",
            "Perhaps other people hate asking before so you use K means clustering, which is a very yeah.",
            "And in our in our community is would there be other alternatives that were interesting to consider?",
            "Yeah, the problem is that I have not commented and have not gone into the data we have.",
            "We have some computational issues with this kernel in four sentences because we have.",
            "3300 thousand sentences and these are lower of data.",
            "In order to compute distance matrix, so you can apply kernel algorithm.",
            "I sorry clustering algorithm, as long as you have not to compute this distance matrix that we what we went through.",
            "This means because you can do that.",
            "OK, thank you so I don't see any further questions or let's thank the speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Cassandra's for Aaron from the from the University of Valencia, and I have developed this this world with her massages.",
                    "label": 0
                },
                {
                    "sent": "That's going to give a dog after this one an Francisco and what we have done is to use some kernels like like similarity measures for sentence.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering, so they talk the index of the talk.",
                    "label": 0
                },
                {
                    "sent": "Something like that we're going to throw an introduction and then we are too.",
                    "label": 0
                },
                {
                    "sent": "We are going to review the seeming clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "And afterwards we're going to review the war sequence kernels in which we have a proposal.",
                    "label": 0
                },
                {
                    "sent": "Modifications of this standard.",
                    "label": 0
                },
                {
                    "sent": "Word sequence kernels that were presented by consider after we're going to stand those corners to the building while.",
                    "label": 0
                },
                {
                    "sent": "Today we went to the worst sequence kernels which are going to be used in building while synthesis corpora and we're going to see some experiments in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suggestion.",
                    "label": 0
                },
                {
                    "sent": "So as you know, this classification is problem in which we have unknown number of classes an for a given document or text.",
                    "label": 1
                },
                {
                    "sent": "We want to classify it in this set.",
                    "label": 1
                },
                {
                    "sent": "We are dealing with a similar problem here, which is discussed in which we don't know the set of classes in which we have to classify the documents.",
                    "label": 0
                },
                {
                    "sent": "But even more we modify the document and we constrain it just to be made up of one single sentence that we what we call sentence clustering.",
                    "label": 0
                },
                {
                    "sent": "And since we we plan to use that for statistical machine translation in which you have.",
                    "label": 1
                },
                {
                    "sent": "Sentence aligned corpora form from 2 languages on the source language and the target language.",
                    "label": 0
                },
                {
                    "sent": "We stand in there for billing while sentence clustering, so the motivation of this work.",
                    "label": 1
                },
                {
                    "sent": "It's mainly two points.",
                    "label": 0
                },
                {
                    "sent": "One is to make a domain adaptation of the statistical models.",
                    "label": 0
                },
                {
                    "sent": "So if we fly and find some clustering from the sentence, you can try a specific model for each of those clusters and then with this information you you when you are given a new sentence.",
                    "label": 1
                },
                {
                    "sent": "You can determine this domain from the clusters and also since the domains or the cluster are is more than the original training data.",
                    "label": 0
                },
                {
                    "sent": "If you training algorithm is not linearly in time, you will get some spy.",
                    "label": 0
                },
                {
                    "sent": "Speed up.",
                    "label": 0
                },
                {
                    "sent": "For training those algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we know the clustering problem is an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "On most of the algorithms, not all, but most of the algorithms use distance for doing this clustering.",
                    "label": 0
                },
                {
                    "sent": "A very famous on will spread algorithm is the C means, which is a greedy algorithm for finding the clusters.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This seems algorithms.",
                    "label": 0
                },
                {
                    "sent": "It was in the previous slide has component is that it's not able to.",
                    "label": 0
                },
                {
                    "sent": "Is not able to.",
                    "label": 0
                },
                {
                    "sent": "Two clusters to properly find clusters in data set which is not linearly separable.",
                    "label": 0
                },
                {
                    "sent": "So for for Checkel for tackling this problem.",
                    "label": 0
                },
                {
                    "sent": "Market allow me presented a modified version in which you use kernel.",
                    "label": 0
                },
                {
                    "sent": "So the UI.",
                    "label": 0
                },
                {
                    "sent": "Map the input data into a high dimensional space and then you try to find the clusters in this in this space so that with the hope that you will be able to.",
                    "label": 1
                },
                {
                    "sent": "Did you find the suitable clusters in this higher domain?",
                    "label": 0
                },
                {
                    "sent": "But usually they use like like distance.",
                    "label": 0
                },
                {
                    "sent": "What we we proposed in this work is just.",
                    "label": 0
                },
                {
                    "sent": "Use the kernels as a similarity between the object, not like a distance in the higher space domain.",
                    "label": 0
                },
                {
                    "sent": "So it is important to remember that the kernels are symmetric, which is one of the requirements for the similarity.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This seems I clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you may recall.",
                    "label": 0
                },
                {
                    "sent": "Even a set of of samples we are, we want to find the cluster for these samples and we usually pose the problem something like that in which we have an index variable for each sample.",
                    "label": 1
                },
                {
                    "sent": "And then we want to minimize the distance between each sample to the centroid of the class to which it belongs to.",
                    "label": 0
                },
                {
                    "sent": "So that's essentially this equation.",
                    "label": 0
                },
                {
                    "sent": "This is very common where the MC is the centroid of each cluster, ansehen, and set subnet C. Is 1 index variable one if this number belongs to the cluster C and zero if it does not?",
                    "label": 0
                },
                {
                    "sent": "So the distance that usually used in this algorithm is the two norm, so it can be another one.",
                    "label": 0
                },
                {
                    "sent": "And one important property is that you have to use a similar metric.",
                    "label": 0
                },
                {
                    "sent": "At least show that this algorithm converges.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the extension that was proposed by Margella meme.",
                    "label": 0
                },
                {
                    "sent": "What does is that just?",
                    "label": 0
                },
                {
                    "sent": "We simply modify the distance, but the distance in the higher domain of the kernel.",
                    "label": 0
                },
                {
                    "sent": "And then with us with that, we are able to apply the Siemens algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we are going to review the properties of metric, so there were some talks that we did yesterday so.",
                    "label": 0
                },
                {
                    "sent": "A metric has free for properties and depending on which properties do we have with or function without distance, we can call the metric semi metric or similar metric.",
                    "label": 0
                },
                {
                    "sent": "So a cinematic is a function that is symmetric and is positive.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the kernels we only need the current positive in order to have a similar metric.",
                    "label": 0
                },
                {
                    "sent": "OK, a cinematic is a similar metric that also has this identity of interesting levels.",
                    "label": 0
                },
                {
                    "sent": "That means that if an object has a distance zero another object, then the object is the same OK. Ann for venema.",
                    "label": 0
                },
                {
                    "sent": "Did we also need a jungle inequality?",
                    "label": 0
                },
                {
                    "sent": "OK, is also well known in mathematics that.",
                    "label": 0
                },
                {
                    "sent": "I wish that you have distance.",
                    "label": 0
                },
                {
                    "sent": "You can define a similarity using this distance.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our proposal.",
                    "label": 0
                },
                {
                    "sent": "Is just too.",
                    "label": 0
                },
                {
                    "sent": "Erase or to change this in the in the Siemens algorithm in the question to the distance.",
                    "label": 0
                },
                {
                    "sent": "For a similarity measure and instead of minimizing maximizing this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clarity.",
                    "label": 0
                },
                {
                    "sent": "For the night we change the deep for this South, and we can.",
                    "label": 0
                },
                {
                    "sent": "We can use a kernel for computing this dissimilarity, as long as the kernel.",
                    "label": 0
                },
                {
                    "sent": "Has some of the properties that are similarity needs.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was regarding to the Siemens algorithm, and now we're going to.",
                    "label": 0
                },
                {
                    "sent": "To see so many some kernels that we have used for the sending clustering problem.",
                    "label": 0
                },
                {
                    "sent": "So we have a base of work in the work of cancer in which the authors proposed awards, sequence kernels, or sequence kernels is a mapping.",
                    "label": 1
                },
                {
                    "sent": "Is well, this is another product in a higher space domain which is compute like a mapping and this mapping what they might be notice given an value N which is going to be the engram value.",
                    "label": 0
                },
                {
                    "sent": "What the kernel does, which is the question 6 confused and some of the product of the number of time we see each ngram.",
                    "label": 1
                },
                {
                    "sent": "In each of the sentence.",
                    "label": 0
                },
                {
                    "sent": "So essentially we have two sentences and we're computing bigrams.",
                    "label": 0
                },
                {
                    "sent": "We go through all the BI grams of the first sentence and try to find this engram in the second sentence.",
                    "label": 0
                },
                {
                    "sent": "So we multiply the number of times each engram occurs in the Champions.",
                    "label": 0
                },
                {
                    "sent": "That has a problem from our point of view.",
                    "label": 0
                },
                {
                    "sent": "Just because of the application of of this kernel that we will see in an example.",
                    "label": 0
                },
                {
                    "sent": "In a common slides.",
                    "label": 0
                },
                {
                    "sent": "And is that if we have seen in one sentence by ground twice, and we see this, this this this bigram, once in the last sentence, then we are going to actually connect true, no one, so that might give some point, for instance, for the single dose that occur in a sentence, for instance the articles.",
                    "label": 0
                },
                {
                    "sent": "They were D. For instance.",
                    "label": 1
                },
                {
                    "sent": "You have seen the word in one sentence and then you see another sentence just once the D. Then you are going to have a very high similarity.",
                    "label": 0
                },
                {
                    "sent": "OK, this is going to be more clear in an example that we're going to see in some slides.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To try to solve this problem, what we have done is we have a clip.",
                    "label": 0
                },
                {
                    "sent": "The word sequence kernel.",
                    "label": 0
                },
                {
                    "sent": "We have previously proposal, so we count the bigrams, but we just allow them to come one or zero.",
                    "label": 0
                },
                {
                    "sent": "So if you have seen it has been seen three times, we just call it like 1.",
                    "label": 0
                },
                {
                    "sent": "So this is actually similarity measure, so it's positive an is symmetric.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have fashion normalized as kernels.",
                    "label": 0
                },
                {
                    "sent": "We have no other scanners so that they were between zero and one because if not the we have a problem with different sentence lengths.",
                    "label": 0
                },
                {
                    "sent": "Because if you have a sentence of five and you compare that with an election file, you are going to have at most a similarity of five.",
                    "label": 0
                },
                {
                    "sent": "But you have a sentence of 10 and then you compare that with sentence of length 10.",
                    "label": 0
                },
                {
                    "sent": "You're going to have a similarity of 10, which is not good for comparing different lengths.",
                    "label": 0
                },
                {
                    "sent": "Because it seems that this this sentence is more similarity to another 10% of four to five words sentences.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, as we know form from the natural Language processing task, the engrams arbores purse of a exponential decaying.",
                    "label": 0
                },
                {
                    "sent": "Occurrence probability depending on the size of the engram.",
                    "label": 0
                },
                {
                    "sent": "So in order to to overcome this problem, we have proposed this.",
                    "label": 0
                },
                {
                    "sent": "Some word sequence kernels that initial into into account the sum of all the.",
                    "label": 0
                },
                {
                    "sent": "The current occurrence that we have previously defined.",
                    "label": 0
                },
                {
                    "sent": "For increasing or Ingram orders, so this K. By N is this is the sum of all the previous KN.",
                    "label": 0
                },
                {
                    "sent": "This gives some smoothing to the computing.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Colonel.",
                    "label": 0
                },
                {
                    "sent": "So why it's important to define this this different word sequence kernels and why we didn't just use the Concetta version?",
                    "label": 0
                },
                {
                    "sent": "If you look at this example, we have four strings.",
                    "label": 0
                },
                {
                    "sent": "OK, so since we are using using sentences for us, the symbols of this string in all tasks are going to be words not not characters on anything.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the one S2S3 and S4 for many of you, if they were words S one is more similar to S2 and two S3 than two S4.",
                    "label": 1
                },
                {
                    "sent": "OK, because there's only one one.",
                    "label": 0
                },
                {
                    "sent": "One word of difference.",
                    "label": 0
                },
                {
                    "sent": "But if you use the well, obviously we are assuming here 11 state distance.",
                    "label": 0
                },
                {
                    "sent": "From the sentence level.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we analyze every show that by Graham will give us, give us with the catheter originally or sequence, we can see that the similarity between S1 and S2 is 2 with S1S3 is 1 and with this one too is S4 is 4.",
                    "label": 0
                },
                {
                    "sent": "That means that this one is more similar to this four unto itself.",
                    "label": 0
                },
                {
                    "sent": "And therefore sentences is not good.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we apply.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The eclipse version this summer one is because we have cleared the codes of engrams.",
                    "label": 0
                },
                {
                    "sent": "So the kernel corn is 113 and three, so we have in some way we have fixed the problem that we have in the previous kernel that as sentence which is not the same sentence can have a higher similarity.",
                    "label": 0
                },
                {
                    "sent": "To his sentence two to itself.",
                    "label": 0
                },
                {
                    "sent": "So in this case, what we have is that at most dissenters will have the same similarity that we solve, But yet this is a similar metric if we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You supply the normalized version.",
                    "label": 0
                },
                {
                    "sent": "Then we fixed all those problems and what we have is that a sentence can only be equal only half a similarity of 1 if it's the itself.",
                    "label": 0
                },
                {
                    "sent": "So that's more similar.",
                    "label": 0
                },
                {
                    "sent": "That's what we wanted because it's very similar to the Levenshtein distance.",
                    "label": 0
                },
                {
                    "sent": "OK, I thought this is just symmetric because it does not verify the edge angle inequality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if your input instead of 1 sentence.",
                    "label": 0
                },
                {
                    "sent": "So instead of trying to cluster one sentence an another sentence or such, a set of sentences from one language, we have to document a line, a sentence level and we have to find clusters inside this align document.",
                    "label": 0
                },
                {
                    "sent": "Each awful elements of the kernel will be made up of two sentences, one from the English, for instance another from Spanish, and they will be.",
                    "label": 0
                },
                {
                    "sent": "They will be the translation from 1 phone.",
                    "label": 0
                },
                {
                    "sent": "One each to the other.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that, we have extended the word sequence kernels today William Barr case, in which.",
                    "label": 0
                },
                {
                    "sent": "Each of the parameters of the kernel is made up of two of two sentence.",
                    "label": 0
                },
                {
                    "sent": "The shows on the target sentence.",
                    "label": 0
                },
                {
                    "sent": "We have compute that using the sum roll.",
                    "label": 0
                },
                {
                    "sent": "So we just sum the kernel of the source part and then the kernel of the tire target part of the.",
                    "label": 0
                },
                {
                    "sent": "Of the building was synthesis so he can extend all the kernels that we were sequence kernel that we have proposed for the monolingual case to the building.",
                    "label": 0
                },
                {
                    "sent": "Worked with this simple rule.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in the future we could add some weights to the kernels, but for the moment we have.",
                    "label": 0
                },
                {
                    "sent": "We haven't tried that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we are going to see some experiments to check how these kernels and this clustering algorithm works in practice.",
                    "label": 0
                },
                {
                    "sent": "We have used two corpora.",
                    "label": 0
                },
                {
                    "sent": "One is more corpora, which is the BTC which is a basic travel special corpus.",
                    "label": 0
                },
                {
                    "sent": "From the IW St. Conference or task and we have.",
                    "label": 0
                },
                {
                    "sent": "Yes it would have used the Chinese to English direction.",
                    "label": 0
                },
                {
                    "sent": "So we have documents on Chinese.",
                    "label": 0
                },
                {
                    "sent": "We have documents on English and we have demaline at sentence level.",
                    "label": 0
                },
                {
                    "sent": "So we have we want to find clusters in this doc in those documents as sensors level the number of centers of this of this training corpora.",
                    "label": 0
                },
                {
                    "sent": "Is that 20,000 words which is not very very huge.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the perplexity computes with the trigram language model.",
                    "label": 0
                },
                {
                    "sent": "E 24 which that?",
                    "label": 0
                },
                {
                    "sent": "That means that if we are given a prefix of three or or 20 words in average, given the information of the language model, we have to decide among 24 words.",
                    "label": 0
                },
                {
                    "sent": "In order to predict the following word OK. And we have also used the Euro bar Division Three, which is a very well known tax for statistical machine translation, but we have limited the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "2:20 at most.",
                    "label": 0
                },
                {
                    "sent": "That's because of computational problems.",
                    "label": 0
                },
                {
                    "sent": "You know that you compute the clusters on those things, so you can see both have similar perplexities.",
                    "label": 0
                },
                {
                    "sent": "But the second one is larger than the first one.",
                    "label": 0
                },
                {
                    "sent": "So in order to compute the kernels, we have filter out all the single tones that were occurring, just one in the corporate because they do not affect the computation of the kernels.",
                    "label": 0
                },
                {
                    "sent": "Since you need it to occur twice at least in order to find a colon anagram.",
                    "label": 0
                },
                {
                    "sent": "And we have also filter office towards, so they must frequent words because they are mainly they occur on all these sentences and they are mainly noise.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one problem is how do we measure this?",
                    "label": 0
                },
                {
                    "sent": "This clustering algorithm?",
                    "label": 0
                },
                {
                    "sent": "Obviously we are using the same inside with him and theoretically.",
                    "label": 0
                },
                {
                    "sent": "Is a greedy algorithm and we're going to get a drop in the in the distance or in the similarity.",
                    "label": 0
                },
                {
                    "sent": "Because it's a function that we're optimizing, so we decide to find some true alternative measures which are not depending on the on the on the on the list and only similarity, but are well, well measure.",
                    "label": 0
                },
                {
                    "sent": "For measuring the complexity of the of the cluster you have.",
                    "label": 0
                },
                {
                    "sent": "So in fact cluster is done properly, you will have a lower perplexity in this in this cluster, so we started with a public city of 24, for instance for the BTC.",
                    "label": 0
                },
                {
                    "sent": "Corpora and so if we divide that into clusters and the cluster, half proper sentences, sentences that are related one to each other, we're going to be able to build the N gram language model in those clusters, and we're going to be able to drop this perplexity.",
                    "label": 0
                },
                {
                    "sent": "Another measure that we could use is the edit distance.",
                    "label": 0
                },
                {
                    "sent": "In practice, both both both distance are equivalent, so we have used in the in the in the cluster perplexity because it's faster to compute.",
                    "label": 1
                },
                {
                    "sent": "But they are.",
                    "label": 0
                },
                {
                    "sent": "They are very very very similar in preview in.",
                    "label": 0
                },
                {
                    "sent": "Previous work we have we have developed.",
                    "label": 0
                },
                {
                    "sent": "So the intercluster perplexity is the geometric average of the perplexity of in each cluster, compute with an N gram language model.",
                    "label": 1
                },
                {
                    "sent": "Compute in this cluster.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this plot is one of the results we have used these small corporate EBT I see and we have used the Chinese and English reaction we have.",
                    "label": 0
                },
                {
                    "sent": "We here compare two kernels.",
                    "label": 0
                },
                {
                    "sent": "We compare the decay so the clip kernel that Canada clip the Conan Grams to one.",
                    "label": 0
                },
                {
                    "sent": "So if an agent has of course.",
                    "label": 0
                },
                {
                    "sent": "Twice it comes just one and then we multiply them and the billing war version and we have a subplot or random clustering.",
                    "label": 0
                },
                {
                    "sent": "So you see that we have the average perplexity in the cluster perplexity.",
                    "label": 0
                },
                {
                    "sent": "And we have the number of cluster you can see that if you don't use.",
                    "label": 0
                },
                {
                    "sent": "Our our clustering algorithm and you just use the random splits of the of the training of the training corpora.",
                    "label": 0
                },
                {
                    "sent": "Then you don't get an improvement and you can also see that if you have an old data on your clusters, because remember that this corporate had 2020 thousand sentences.",
                    "label": 0
                },
                {
                    "sent": "So in this case we only have 1000 sentences on average per cluster, so you have enough sentences in your clusters.",
                    "label": 0
                },
                {
                    "sent": "You can use the billing information.",
                    "label": 0
                },
                {
                    "sent": "To try to improve your clustering.",
                    "label": 0
                },
                {
                    "sent": "Also, this is the by grammar, the bigram, a version of the kernels.",
                    "label": 0
                },
                {
                    "sent": "So the next.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say we have a similar plot, but this time we have.",
                    "label": 0
                },
                {
                    "sent": "We have plot all the.",
                    "label": 0
                },
                {
                    "sent": "The same kernel for four different engrams grace in order of engrams.",
                    "label": 0
                },
                {
                    "sent": "So we can see we have dinner.",
                    "label": 0
                },
                {
                    "sent": "A result which is similar to that obtained.",
                    "label": 0
                },
                {
                    "sent": "By the work of Cansada, in which the best performing.",
                    "label": 0
                },
                {
                    "sent": "Colonel is the bigram.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, so the results are not totally statistically significant, they are pretty stable and this is a way on.",
                    "label": 0
                },
                {
                    "sent": "The bigram is always better.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I would allocate corporate like the euro part?",
                    "label": 0
                },
                {
                    "sent": "OK, Europe has over 100 more.",
                    "label": 0
                },
                {
                    "sent": "It's like 2 orders of magnitude larger than the previous one, and we can see that if we compare the same billing, why cluster the same that we compare in the?",
                    "label": 0
                },
                {
                    "sent": "In the by BTS EC task.",
                    "label": 0
                },
                {
                    "sent": "Within a similar plot.",
                    "label": 0
                },
                {
                    "sent": "So it essentially means that that it scales behavior scales very well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, if we compare the different order of the engrams, we see that the bigram is again the better one, so there is not a statistically significant.",
                    "label": 0
                },
                {
                    "sent": "The bigram always keys the best result.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "YN grams there's one question that we ask yourself.",
                    "label": 0
                },
                {
                    "sent": "You have to remember that we are using sentences.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                },
                {
                    "sent": "Obviously by Graham has more structural information.",
                    "label": 1
                },
                {
                    "sent": "A unigram trigram has more useful information than a bigram, and so on.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that the single tones on the double tones statistics for bigrams are quite low.",
                    "label": 0
                },
                {
                    "sent": "For the bigram are still quite low, but if you go to the trigrams you see that almost 91% of the of the trigrams, or just one or two times in the in a sentence.",
                    "label": 0
                },
                {
                    "sent": "So that means that you only have over 9% or 5% of the of the bigrams, or the OR the trigrams or programs, or Kareem in more than two sentences.",
                    "label": 0
                },
                {
                    "sent": "So essentially so they have more complex, more complex information.",
                    "label": 0
                },
                {
                    "sent": "And they are.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "The numbers are very, very low, just so we don't have a car purpose.",
                    "label": 0
                },
                {
                    "sent": "Larger and not to take advantage of these engrams?",
                    "label": 0
                },
                {
                    "sent": "That's probably because we are we're constraining ourselves to sentences if we were using documents, maybe the thing would be different.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the conclusions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have proposed similarity based clustering algorithm based on this Siemens Siemens.",
                    "label": 1
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "We have proposed several families of cluster for these tax base in a work or work in the cancer network.",
                    "label": 1
                },
                {
                    "sent": "We have studied those those those kernels to the building work case.",
                    "label": 1
                },
                {
                    "sent": "And we have to serve that the best performing kernels in practice are the bigrams, and especially specifically the billing world, bigram kernels.",
                    "label": 0
                },
                {
                    "sent": "In practice that I have not given you diploid because you have limited time and a limited space in the in the paper.",
                    "label": 0
                },
                {
                    "sent": "The clip version and the version behave the same, because in practice there is no difference at the sentence level.",
                    "label": 0
                },
                {
                    "sent": "Once you have raised the non stop words.",
                    "label": 0
                },
                {
                    "sent": "OK so in practice the both kernels are the same.",
                    "label": 0
                },
                {
                    "sent": "But theoretically, for instance, the clip Colonel is a semi semi minority and the other one is not so.",
                    "label": 1
                },
                {
                    "sent": "We have proved that the wealth while we have shown that the intercluster perplexity.",
                    "label": 1
                },
                {
                    "sent": "Does not provide insight into why the same.",
                    "label": 1
                },
                {
                    "sent": "The optimal number of cluster, because it's always decreasing, so we do not know when to end the the plot of the number of clusters.",
                    "label": 1
                },
                {
                    "sent": "One interesting thing that we want to do for the future is to see the relationship that the kernels have with the gentleman approach in which they use the kernel, like a distant measure and not like a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "And while there are some extensions that can be made on this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's all you have any question.",
                    "label": 0
                },
                {
                    "sent": "Very much.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are there any questions or comments?",
                    "label": 0
                },
                {
                    "sent": "Use of additional external data in order to have more trigrams, yeah.",
                    "label": 0
                },
                {
                    "sent": "The clustering problem independently on the date yet, but the problem is more than the data is is that the constraint of the sentence OK?",
                    "label": 0
                },
                {
                    "sent": "Because you have a lot of data, but all your sentences has half a length of 10 works, then the probability of this trigram to occur in the other sentence, which also has a length of 12 or 10 is very slow.",
                    "label": 0
                },
                {
                    "sent": "So the problem is not not for the training data, but for the constraint of the document.",
                    "label": 0
                },
                {
                    "sent": "Maybe as a sort of standard question.",
                    "label": 0
                },
                {
                    "sent": "Perhaps other people hate asking before so you use K means clustering, which is a very yeah.",
                    "label": 0
                },
                {
                    "sent": "And in our in our community is would there be other alternatives that were interesting to consider?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the problem is that I have not commented and have not gone into the data we have.",
                    "label": 0
                },
                {
                    "sent": "We have some computational issues with this kernel in four sentences because we have.",
                    "label": 0
                },
                {
                    "sent": "3300 thousand sentences and these are lower of data.",
                    "label": 0
                },
                {
                    "sent": "In order to compute distance matrix, so you can apply kernel algorithm.",
                    "label": 0
                },
                {
                    "sent": "I sorry clustering algorithm, as long as you have not to compute this distance matrix that we what we went through.",
                    "label": 0
                },
                {
                    "sent": "This means because you can do that.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you so I don't see any further questions or let's thank the speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}