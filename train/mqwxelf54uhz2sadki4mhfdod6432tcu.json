{
    "id": "mqwxelf54uhz2sadki4mhfdod6432tcu",
    "title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning",
    "info": {
        "author": [
            "Ronan Collobert, NEC Laboratories America, Inc."
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Neural Networks"
        ]
    },
    "url": "http://videolectures.net/icml08_collobert_uanl/",
    "segmentation": [
        [
            "Right?",
            "For some strange reasons, mayor.",
            "My computer crashed when I plugged it.",
            "So.",
            "In this talk, we are like interested in.",
            "In presenting a general architecture for natural language processing task which involve tagging and this is John Walk with my office mate Jason Western at."
        ],
        [
            "She lives.",
            "So natural natural language processing tagging means dealing with text and.",
            "We are.",
            "We basically want to present general non network architecture.",
            "Well, you give at the beginning some sentence, straight text.",
            "We don't want to add any prior features, so you give some blah blah blah sentence and it outputs at the end some tags.",
            "So it's end to end.",
            "And so basically each word in the sentence is going to be.",
            "Do you have a pointer?",
            "Is going to be.",
            "To be embedded.",
            "Nothing.",
            "To be embedded in some.",
            "Yes.",
            "Thank you Sir.",
            "Do we so fast that the taxes, each problem that sometimes it's going?",
            "No.",
            "OK, each word in the sentence is going to be embedded like some feature space.",
            "Then each feature vectors is going to be like combine to obtain some local features.",
            "Local features are going to be combined to obtain some global features.",
            "And finally we output some tags.",
            "Everything is strainer in my back propagation stochastic gradient end to end system.",
            "So we will show that this architecture, which is kind of generic kind of unify all kind of natural language processing task for tagging."
        ],
        [
            "And the fact that we can unify this task owns us to train everything together.",
            "And by sharing, for example, D embedding part of the network, we can actually gain performance on all tasks at the same time."
        ],
        [
            "So the natural language processing task I'm interested in are like part of speech tagging here.",
            "Which is about trying to find a syntactic world in a sentence, like if.",
            "Is a lot of urban elements on then we are interested also in chunking, so finding syntactic constituent in the sentence like verb phrase, noun phrases and so on.",
            "We are also interested in something more semantic named entity recognition finding if award is a person or company name or location and so on.",
            "And then there is this semantical labeling task which we consider as the most important task, yeah.",
            "Which is about extracting semantic words for words in a sentence so.",
            "If you consider, for example, this sentence John 8, the airport in the garden while you consider first verb with respect, you want to get the tagging.",
            "So for example about 8 here and you want to know where is acting on this verb.",
            "Who is the actor.",
            "So here is John on what it's acting the upper when we're done soon.",
            "So it's important to note that it's a tagging which depends on the verb chosen before ending the sentence.",
            "All those tasks come with some labeled data from the community and we used here the worst regional data which is about like 1,000,000."
        ],
        [
            "Love tag world.",
            "Of course this task are have been around like since a while so many people tackle the problem already quite well and the usual approach is first to choose some good and make features for your task.",
            "Once you're extracted, this handmade features from your sentence, you just have to feed it to some classical classifier.",
            "For example, a good support vector machine.",
            "For the task a bit more complicated like."
        ],
        [
            "Well, semantically living.",
            "You actually usually have to cascade features.",
            "So for example you can extract part of speech once you have Spartan speech you like constructive bar stream.",
            "Once you have the past three, you extract some ham officials from this party and you finally feel this on my features to your void classifier.",
            "Well, we think this approach works well.",
            "It's good, but it's a bit questionable from a machine learning POV.",
            "Because first of all, there's this cascade, so if an error occurs in a classifier at the beginning of this cascade, there always going to propagate all along the cascade.",
            "Also, you have to extract handmade features for the task you're interested in, so each time you have a new task, you have to extract some new features and to find the right features for this task.",
            "So instead we are interested in trying to find complete general end to end system which would learn this feature completely implicitly and try to see if with no particular Prairie in the features we can gain the same kind of thing."
        ],
        [
            "Comments.",
            "So we considered here non network approach.",
            "Well, we input at the beginning.",
            "For example window of text or wonder what you are interested in.",
            "So for example, here's what set.",
            "You you first map each word which is actually just an index.",
            "Basically, in addition, you first map this world into some embedded in space by applying basically just a look up table.",
            "So the world is going to map.",
            "Each word is going to be back to feature vector or for size that you decided before hand.",
            "Then we concatenate this vector and you can feed it through a classifier classical classifier.",
            "I mean classical non networks layer basically.",
            "While this approach works very well actually for a simple task like butter, speech tagging or named entity organization, but as soon as you deal with semantic role labeling while you have to classify, I mean to Tiger one with respect to a verb in the sentence.",
            "So basically you want to see all the sentences at the same time, so you can generalize."
        ],
        [
            "This is our approach by using basically what we call a convolutional neural network.",
            "So it's extremely similar this time you consider all the sentence.",
            "You have to specify also the verb with respect to which one you want to classify.",
            "So here we like indicate as input feature the position of each one with respect to the verb of interest.",
            "Then you map each word again into some embedding space.",
            "You also have the positions.",
            "And after what you are going to combine, basically you are going to apply."
        ],
        [
            "The window approach for all windows in the sentence.",
            "So you have this feature vector wants you consider for example a window of size 3.",
            "You procrastinate this vector, you apply another inspector.",
            "Operation on this Coca technically vector you obtain a new feature right now.",
            "Then you shift it takes it as three.",
            "Next one you do the same thing, and so on.",
            "So it's going to version.",
            "It has been used with success in image and speech in the past and we will show that it's also."
        ],
        [
            "Can be useful in text.",
            "So once you have this correlation layer, you basically have extracted like some local features for each word in.",
            "I mean all each word in a sentence, but the number of local features vector depends on the number of words in the sentence.",
            "You still have to get rid of this time dimension to be able to fit it to a normal non network layers."
        ],
        [
            "So for that purpose we like applying what we call on maximal time here.",
            "So."
        ],
        [
            "For.",
            "Each.",
            "Feature so here we have like for example a represented the local local feature for sentence.",
            "Yesterday after blah blah blah.",
            "So.",
            "These are all the feature local global feature vectors for the words along the sentence and in Word you can see basically the Max.",
            "I mean the feature which has been chosen by the network.",
            "For each picture, I mean, each one is basically offshore, so it shows which feature has been chosen along time and you can see that basically it shows.",
            "There is that too close or like a cluster around the one of interest in green here and there is a verb of interest."
        ],
        [
            "Yeah, and if I move, So what if enjoys the cluster basically moves.",
            "So once we obtain like this fixed size feature vector, you can just feed it to classical layers and obtain your tags.",
            "Everything is trained by replicating back publication using stochastic gradient descent.",
            "And that's it.",
            "There is no particular trick."
        ],
        [
            "And as I said before, as its general, we can apply this idea of multi task learning and training everything together by sharing part of the network here with shared only look up table layer.",
            "So this embedding space of the world.",
            "So which is worse?",
            "Learning is also not that new.",
            "I mean it has been used quite a lot in machine learning.",
            "You can find a good overview enriched carbon or thesis if you want."
        ],
        [
            "For networks.",
            "But still.",
            "There's a little problem here, it's.",
            "Even if the databases seems quite big 1,000,000 for it it's not that bad.",
            "It's actually kind of small in the case of natural language processing, because many words appear, you know just few times.",
            "If I considered like the dictionary, I considered basically 15% of the most common word appear 90% of the time.",
            "So many words are seen like just few times.",
            "So if you saw something like sometimes like the cat sat on the mat in the training set.",
            "It might work very well because catches like very common.",
            "But then if you see this other sometimes where I know you say fill in or fill in is really less common.",
            "Well, it might think it's a plus one very well, but not the second one.",
            "So to deal with this problem, what we would like basically is try to force in the embedding space the word cut to be close to the word feline.",
            "So for that we consider an additional multi task.",
            "Taskings are rigid, asking approach.",
            "We we took Walnut and we tried to pull together awards linked into on it and to push about all other words in one net.",
            "What's actually we can go or like a step further, because it's kind of frustrating that we have all these data.",
            "Lying around, you know we have the web with stun, basically infinite."
        ],
        [
            "Amount of text.",
            "Anne.",
            "If we could find a task to extract automatically automatically this I mean the structure in Atlanta.",
            "This text, while it would be great for our system, so we consider language model which basically is able to answer if sometimes is actually English or not.",
            "So if you have a system which works really well or not, I mean of course the NPC team behind it has to extract the syntax that's to extract correctly the grammar it has to understand the semantics.",
            "So we consider basically Wikipedia, which is already like 600 more than 600 million of ones.",
            "And we try to to train another network able to discriminate between.",
            "Sometimes of text in Wikipedia from sometimes of texts in Wikipedia, where the middle one has been replaced by some random world.",
            "And what we want if F is our non network output in like a score for window particular window, we want to the score for the true window of Wikipedia to be larger with a certain margin one here then all other window where the middle word has been replaced by a random one.",
            "Once again, this has been trained like during weeks and because it's like 600 million of words.",
            "But after a few weeks of."
        ],
        [
            "664 twenty we were interested, quite amazed at what the network would find.",
            "So here this is like a few words I picked randomly in the dictionary.",
            "The number under underworld is basically the frequency ranking of the world.",
            "So if the the number is small it means the what is refrigerant and you can see on Dennis the 10 closest World India building space.",
            "According to the Euclidean distance.",
            "So so Interestingly, or we can see it seems to really catch something semantic and even further."
        ],
        [
            "Real world.",
            "So we apply all these ideas to our semantic whole labeling approach which were a semantical labeling task which was most interesting one for us.",
            "And here it's over consumption of test errors, who's on iteration?",
            "So there's a training set.",
            "To show us that actually it converges very quickly and for two different feature world sites in their building, so 15 and 100.",
            "So if you consider, for example, the one on one in black hair on top, you have the error rate from the semantical labeling an underneath.",
            "As you can see, this is kind of clustered here when we train semantic role labeling with auto speech, drinking bottle, speech and chunking and things like that.",
            "You gain a bit of improvement when you use the word net approach.",
            "When you multitask with the word net approach.",
            "And you gain a huge improvement if you use the language model alone.",
            "And in the end we can obtain state of the art performance and while being extremely fast like a fraction of 2nd for big sentence of Wall Street."
        ],
        [
            "Alarm.",
            "We also looked at other task in in.",
            "In that we were interested in and we compare here on the task alone and the task train in multitasking mode.",
            "You can see that for part of speech we gain a little bit, but not much, which is not surprising because part of speech is very easy task.",
            "For chunking we gain quite a bit and we gave actually like close to where the state of the art it's actually evaluated on.",
            "Uh, the Coronal 2000 task an if on top of that we add additional features we actually can easily beat the state of the art on this task in."
        ],
        [
            "Fun school.",
            "To summarize this talk I presented you like General Neural Network architecture, which can be applied to natural language processing task which involves tagging.",
            "It does so interesting advantages it's general.",
            "Still obtain state of the art performance and there is for that.",
            "No need for hand design features.",
            "When you can exploit John training and train over massive, unlabeled data as I should, I say so.",
            "It's also extremely fast.",
            "Of course there's a big inconvenient.",
            "It's a known network, which is a powerful tool, so it's kind of hard to wonder.",
            "But we have already like some early impacts.",
            "As it's very easy to you know, apply to some other task or some other languages were already training with success on Japanese.",
            "And then we also got for free.",
            "Basically a semantic search system working well on Wikipedia because of the speed.",
            "So that's it.",
            "Question.",
            "So I have a couple questions about the evaluation, so on the semantic role labeling.",
            "Evaluation you're using kind of a non standard metric.",
            "Historical.",
            "When we started we were not aware of that and.",
            "Run it with the standard method.",
            "Well, basically now we have an intern which just started and we have another one.",
            "We did the chunking so that's why the shrinking is here, but the semantic role labeling is coming sooner so also on this site.",
            "So all the joint tasks you have probably named entity recognition is has the highest impact so far.",
            "You know, outside of the research community already, OK, you know, so I didn't see the improvements.",
            "Yeah, this thing is for my monthly organization.",
            "At the time we started that we did not have any good layer labeled data set.",
            "So what we did is we took some standard named entity organizer or we applied it on Wall Street Journal.",
            "And we use.",
            "This lowers, but we couldn't show results because it seems pretty good.",
            "But I mean what can I say?",
            "It's maybe like learning zero.",
            "Also the other one.",
            "But we showed that actually.",
            "Interestingly, it still helps when you train it together.",
            "Those on task.",
            "Yep.",
            "As far as I understand, when you convert a sentence representation at some point to do a Max operation, India.",
            "You tried some as well, yeah, but then you.",
            "I mean basically yeah, try it works worse and it's not surprising.",
            "Yeah, much worse actually.",
            "So surprising because I mean you really want to get the interesting features for.",
            "For the water antecedent attack, unusually interesting feature around the world and not.",
            "You know, if you sum, you basically smooth completely of the features and you lose a lot of information.",
            "Yeah indeed.",
            "Or you know, it's so hot warm these days deep.",
            "Well, I mean it's you know of course, every coalitional network is deep because there is always a lot of layers, so you can call it deep into like I don't care personally always said actually no networking is talking and it's written deep with.",
            "They actually had no particular problems paying this buyback, something no.",
            "Or I mean it's not that deep in then you know it's like 6 layers so so I mean it's OK.",
            "So I didn't use any trick, just don't know by partially fixed learning rate works very well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "For some strange reasons, mayor.",
                    "label": 0
                },
                {
                    "sent": "My computer crashed when I plugged it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we are like interested in.",
                    "label": 0
                },
                {
                    "sent": "In presenting a general architecture for natural language processing task which involve tagging and this is John Walk with my office mate Jason Western at.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She lives.",
                    "label": 0
                },
                {
                    "sent": "So natural natural language processing tagging means dealing with text and.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "We basically want to present general non network architecture.",
                    "label": 0
                },
                {
                    "sent": "Well, you give at the beginning some sentence, straight text.",
                    "label": 0
                },
                {
                    "sent": "We don't want to add any prior features, so you give some blah blah blah sentence and it outputs at the end some tags.",
                    "label": 0
                },
                {
                    "sent": "So it's end to end.",
                    "label": 0
                },
                {
                    "sent": "And so basically each word in the sentence is going to be.",
                    "label": 0
                },
                {
                    "sent": "Do you have a pointer?",
                    "label": 0
                },
                {
                    "sent": "Is going to be.",
                    "label": 0
                },
                {
                    "sent": "To be embedded.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "To be embedded in some.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Thank you Sir.",
                    "label": 0
                },
                {
                    "sent": "Do we so fast that the taxes, each problem that sometimes it's going?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, each word in the sentence is going to be embedded like some feature space.",
                    "label": 0
                },
                {
                    "sent": "Then each feature vectors is going to be like combine to obtain some local features.",
                    "label": 0
                },
                {
                    "sent": "Local features are going to be combined to obtain some global features.",
                    "label": 1
                },
                {
                    "sent": "And finally we output some tags.",
                    "label": 0
                },
                {
                    "sent": "Everything is strainer in my back propagation stochastic gradient end to end system.",
                    "label": 0
                },
                {
                    "sent": "So we will show that this architecture, which is kind of generic kind of unify all kind of natural language processing task for tagging.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the fact that we can unify this task owns us to train everything together.",
                    "label": 0
                },
                {
                    "sent": "And by sharing, for example, D embedding part of the network, we can actually gain performance on all tasks at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the natural language processing task I'm interested in are like part of speech tagging here.",
                    "label": 0
                },
                {
                    "sent": "Which is about trying to find a syntactic world in a sentence, like if.",
                    "label": 0
                },
                {
                    "sent": "Is a lot of urban elements on then we are interested also in chunking, so finding syntactic constituent in the sentence like verb phrase, noun phrases and so on.",
                    "label": 1
                },
                {
                    "sent": "We are also interested in something more semantic named entity recognition finding if award is a person or company name or location and so on.",
                    "label": 0
                },
                {
                    "sent": "And then there is this semantical labeling task which we consider as the most important task, yeah.",
                    "label": 0
                },
                {
                    "sent": "Which is about extracting semantic words for words in a sentence so.",
                    "label": 0
                },
                {
                    "sent": "If you consider, for example, this sentence John 8, the airport in the garden while you consider first verb with respect, you want to get the tagging.",
                    "label": 0
                },
                {
                    "sent": "So for example about 8 here and you want to know where is acting on this verb.",
                    "label": 0
                },
                {
                    "sent": "Who is the actor.",
                    "label": 0
                },
                {
                    "sent": "So here is John on what it's acting the upper when we're done soon.",
                    "label": 0
                },
                {
                    "sent": "So it's important to note that it's a tagging which depends on the verb chosen before ending the sentence.",
                    "label": 1
                },
                {
                    "sent": "All those tasks come with some labeled data from the community and we used here the worst regional data which is about like 1,000,000.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love tag world.",
                    "label": 0
                },
                {
                    "sent": "Of course this task are have been around like since a while so many people tackle the problem already quite well and the usual approach is first to choose some good and make features for your task.",
                    "label": 0
                },
                {
                    "sent": "Once you're extracted, this handmade features from your sentence, you just have to feed it to some classical classifier.",
                    "label": 0
                },
                {
                    "sent": "For example, a good support vector machine.",
                    "label": 0
                },
                {
                    "sent": "For the task a bit more complicated like.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, semantically living.",
                    "label": 0
                },
                {
                    "sent": "You actually usually have to cascade features.",
                    "label": 1
                },
                {
                    "sent": "So for example you can extract part of speech once you have Spartan speech you like constructive bar stream.",
                    "label": 0
                },
                {
                    "sent": "Once you have the past three, you extract some ham officials from this party and you finally feel this on my features to your void classifier.",
                    "label": 0
                },
                {
                    "sent": "Well, we think this approach works well.",
                    "label": 0
                },
                {
                    "sent": "It's good, but it's a bit questionable from a machine learning POV.",
                    "label": 0
                },
                {
                    "sent": "Because first of all, there's this cascade, so if an error occurs in a classifier at the beginning of this cascade, there always going to propagate all along the cascade.",
                    "label": 0
                },
                {
                    "sent": "Also, you have to extract handmade features for the task you're interested in, so each time you have a new task, you have to extract some new features and to find the right features for this task.",
                    "label": 1
                },
                {
                    "sent": "So instead we are interested in trying to find complete general end to end system which would learn this feature completely implicitly and try to see if with no particular Prairie in the features we can gain the same kind of thing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comments.",
                    "label": 0
                },
                {
                    "sent": "So we considered here non network approach.",
                    "label": 0
                },
                {
                    "sent": "Well, we input at the beginning.",
                    "label": 0
                },
                {
                    "sent": "For example window of text or wonder what you are interested in.",
                    "label": 0
                },
                {
                    "sent": "So for example, here's what set.",
                    "label": 0
                },
                {
                    "sent": "You you first map each word which is actually just an index.",
                    "label": 0
                },
                {
                    "sent": "Basically, in addition, you first map this world into some embedded in space by applying basically just a look up table.",
                    "label": 0
                },
                {
                    "sent": "So the world is going to map.",
                    "label": 0
                },
                {
                    "sent": "Each word is going to be back to feature vector or for size that you decided before hand.",
                    "label": 0
                },
                {
                    "sent": "Then we concatenate this vector and you can feed it through a classifier classical classifier.",
                    "label": 0
                },
                {
                    "sent": "I mean classical non networks layer basically.",
                    "label": 0
                },
                {
                    "sent": "While this approach works very well actually for a simple task like butter, speech tagging or named entity organization, but as soon as you deal with semantic role labeling while you have to classify, I mean to Tiger one with respect to a verb in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to see all the sentences at the same time, so you can generalize.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is our approach by using basically what we call a convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "So it's extremely similar this time you consider all the sentence.",
                    "label": 0
                },
                {
                    "sent": "You have to specify also the verb with respect to which one you want to classify.",
                    "label": 0
                },
                {
                    "sent": "So here we like indicate as input feature the position of each one with respect to the verb of interest.",
                    "label": 0
                },
                {
                    "sent": "Then you map each word again into some embedding space.",
                    "label": 0
                },
                {
                    "sent": "You also have the positions.",
                    "label": 0
                },
                {
                    "sent": "And after what you are going to combine, basically you are going to apply.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The window approach for all windows in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So you have this feature vector wants you consider for example a window of size 3.",
                    "label": 0
                },
                {
                    "sent": "You procrastinate this vector, you apply another inspector.",
                    "label": 0
                },
                {
                    "sent": "Operation on this Coca technically vector you obtain a new feature right now.",
                    "label": 0
                },
                {
                    "sent": "Then you shift it takes it as three.",
                    "label": 0
                },
                {
                    "sent": "Next one you do the same thing, and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's going to version.",
                    "label": 0
                },
                {
                    "sent": "It has been used with success in image and speech in the past and we will show that it's also.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be useful in text.",
                    "label": 0
                },
                {
                    "sent": "So once you have this correlation layer, you basically have extracted like some local features for each word in.",
                    "label": 0
                },
                {
                    "sent": "I mean all each word in a sentence, but the number of local features vector depends on the number of words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "You still have to get rid of this time dimension to be able to fit it to a normal non network layers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that purpose we like applying what we call on maximal time here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Each.",
                    "label": 0
                },
                {
                    "sent": "Feature so here we have like for example a represented the local local feature for sentence.",
                    "label": 0
                },
                {
                    "sent": "Yesterday after blah blah blah.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "These are all the feature local global feature vectors for the words along the sentence and in Word you can see basically the Max.",
                    "label": 0
                },
                {
                    "sent": "I mean the feature which has been chosen by the network.",
                    "label": 0
                },
                {
                    "sent": "For each picture, I mean, each one is basically offshore, so it shows which feature has been chosen along time and you can see that basically it shows.",
                    "label": 0
                },
                {
                    "sent": "There is that too close or like a cluster around the one of interest in green here and there is a verb of interest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and if I move, So what if enjoys the cluster basically moves.",
                    "label": 0
                },
                {
                    "sent": "So once we obtain like this fixed size feature vector, you can just feed it to classical layers and obtain your tags.",
                    "label": 0
                },
                {
                    "sent": "Everything is trained by replicating back publication using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "There is no particular trick.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I said before, as its general, we can apply this idea of multi task learning and training everything together by sharing part of the network here with shared only look up table layer.",
                    "label": 0
                },
                {
                    "sent": "So this embedding space of the world.",
                    "label": 0
                },
                {
                    "sent": "So which is worse?",
                    "label": 0
                },
                {
                    "sent": "Learning is also not that new.",
                    "label": 0
                },
                {
                    "sent": "I mean it has been used quite a lot in machine learning.",
                    "label": 0
                },
                {
                    "sent": "You can find a good overview enriched carbon or thesis if you want.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For networks.",
                    "label": 0
                },
                {
                    "sent": "But still.",
                    "label": 0
                },
                {
                    "sent": "There's a little problem here, it's.",
                    "label": 0
                },
                {
                    "sent": "Even if the databases seems quite big 1,000,000 for it it's not that bad.",
                    "label": 0
                },
                {
                    "sent": "It's actually kind of small in the case of natural language processing, because many words appear, you know just few times.",
                    "label": 0
                },
                {
                    "sent": "If I considered like the dictionary, I considered basically 15% of the most common word appear 90% of the time.",
                    "label": 0
                },
                {
                    "sent": "So many words are seen like just few times.",
                    "label": 1
                },
                {
                    "sent": "So if you saw something like sometimes like the cat sat on the mat in the training set.",
                    "label": 1
                },
                {
                    "sent": "It might work very well because catches like very common.",
                    "label": 0
                },
                {
                    "sent": "But then if you see this other sometimes where I know you say fill in or fill in is really less common.",
                    "label": 0
                },
                {
                    "sent": "Well, it might think it's a plus one very well, but not the second one.",
                    "label": 0
                },
                {
                    "sent": "So to deal with this problem, what we would like basically is try to force in the embedding space the word cut to be close to the word feline.",
                    "label": 0
                },
                {
                    "sent": "So for that we consider an additional multi task.",
                    "label": 0
                },
                {
                    "sent": "Taskings are rigid, asking approach.",
                    "label": 0
                },
                {
                    "sent": "We we took Walnut and we tried to pull together awards linked into on it and to push about all other words in one net.",
                    "label": 0
                },
                {
                    "sent": "What's actually we can go or like a step further, because it's kind of frustrating that we have all these data.",
                    "label": 0
                },
                {
                    "sent": "Lying around, you know we have the web with stun, basically infinite.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Amount of text.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we could find a task to extract automatically automatically this I mean the structure in Atlanta.",
                    "label": 0
                },
                {
                    "sent": "This text, while it would be great for our system, so we consider language model which basically is able to answer if sometimes is actually English or not.",
                    "label": 1
                },
                {
                    "sent": "So if you have a system which works really well or not, I mean of course the NPC team behind it has to extract the syntax that's to extract correctly the grammar it has to understand the semantics.",
                    "label": 0
                },
                {
                    "sent": "So we consider basically Wikipedia, which is already like 600 more than 600 million of ones.",
                    "label": 0
                },
                {
                    "sent": "And we try to to train another network able to discriminate between.",
                    "label": 0
                },
                {
                    "sent": "Sometimes of text in Wikipedia from sometimes of texts in Wikipedia, where the middle one has been replaced by some random world.",
                    "label": 0
                },
                {
                    "sent": "And what we want if F is our non network output in like a score for window particular window, we want to the score for the true window of Wikipedia to be larger with a certain margin one here then all other window where the middle word has been replaced by a random one.",
                    "label": 0
                },
                {
                    "sent": "Once again, this has been trained like during weeks and because it's like 600 million of words.",
                    "label": 0
                },
                {
                    "sent": "But after a few weeks of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "664 twenty we were interested, quite amazed at what the network would find.",
                    "label": 0
                },
                {
                    "sent": "So here this is like a few words I picked randomly in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "The number under underworld is basically the frequency ranking of the world.",
                    "label": 0
                },
                {
                    "sent": "So if the the number is small it means the what is refrigerant and you can see on Dennis the 10 closest World India building space.",
                    "label": 0
                },
                {
                    "sent": "According to the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "So so Interestingly, or we can see it seems to really catch something semantic and even further.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real world.",
                    "label": 0
                },
                {
                    "sent": "So we apply all these ideas to our semantic whole labeling approach which were a semantical labeling task which was most interesting one for us.",
                    "label": 0
                },
                {
                    "sent": "And here it's over consumption of test errors, who's on iteration?",
                    "label": 0
                },
                {
                    "sent": "So there's a training set.",
                    "label": 0
                },
                {
                    "sent": "To show us that actually it converges very quickly and for two different feature world sites in their building, so 15 and 100.",
                    "label": 0
                },
                {
                    "sent": "So if you consider, for example, the one on one in black hair on top, you have the error rate from the semantical labeling an underneath.",
                    "label": 0
                },
                {
                    "sent": "As you can see, this is kind of clustered here when we train semantic role labeling with auto speech, drinking bottle, speech and chunking and things like that.",
                    "label": 0
                },
                {
                    "sent": "You gain a bit of improvement when you use the word net approach.",
                    "label": 0
                },
                {
                    "sent": "When you multitask with the word net approach.",
                    "label": 0
                },
                {
                    "sent": "And you gain a huge improvement if you use the language model alone.",
                    "label": 0
                },
                {
                    "sent": "And in the end we can obtain state of the art performance and while being extremely fast like a fraction of 2nd for big sentence of Wall Street.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alarm.",
                    "label": 0
                },
                {
                    "sent": "We also looked at other task in in.",
                    "label": 0
                },
                {
                    "sent": "In that we were interested in and we compare here on the task alone and the task train in multitasking mode.",
                    "label": 0
                },
                {
                    "sent": "You can see that for part of speech we gain a little bit, but not much, which is not surprising because part of speech is very easy task.",
                    "label": 0
                },
                {
                    "sent": "For chunking we gain quite a bit and we gave actually like close to where the state of the art it's actually evaluated on.",
                    "label": 0
                },
                {
                    "sent": "Uh, the Coronal 2000 task an if on top of that we add additional features we actually can easily beat the state of the art on this task in.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fun school.",
                    "label": 0
                },
                {
                    "sent": "To summarize this talk I presented you like General Neural Network architecture, which can be applied to natural language processing task which involves tagging.",
                    "label": 0
                },
                {
                    "sent": "It does so interesting advantages it's general.",
                    "label": 0
                },
                {
                    "sent": "Still obtain state of the art performance and there is for that.",
                    "label": 0
                },
                {
                    "sent": "No need for hand design features.",
                    "label": 0
                },
                {
                    "sent": "When you can exploit John training and train over massive, unlabeled data as I should, I say so.",
                    "label": 1
                },
                {
                    "sent": "It's also extremely fast.",
                    "label": 0
                },
                {
                    "sent": "Of course there's a big inconvenient.",
                    "label": 1
                },
                {
                    "sent": "It's a known network, which is a powerful tool, so it's kind of hard to wonder.",
                    "label": 1
                },
                {
                    "sent": "But we have already like some early impacts.",
                    "label": 0
                },
                {
                    "sent": "As it's very easy to you know, apply to some other task or some other languages were already training with success on Japanese.",
                    "label": 0
                },
                {
                    "sent": "And then we also got for free.",
                    "label": 0
                },
                {
                    "sent": "Basically a semantic search system working well on Wikipedia because of the speed.",
                    "label": 1
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So I have a couple questions about the evaluation, so on the semantic role labeling.",
                    "label": 0
                },
                {
                    "sent": "Evaluation you're using kind of a non standard metric.",
                    "label": 0
                },
                {
                    "sent": "Historical.",
                    "label": 0
                },
                {
                    "sent": "When we started we were not aware of that and.",
                    "label": 0
                },
                {
                    "sent": "Run it with the standard method.",
                    "label": 0
                },
                {
                    "sent": "Well, basically now we have an intern which just started and we have another one.",
                    "label": 0
                },
                {
                    "sent": "We did the chunking so that's why the shrinking is here, but the semantic role labeling is coming sooner so also on this site.",
                    "label": 0
                },
                {
                    "sent": "So all the joint tasks you have probably named entity recognition is has the highest impact so far.",
                    "label": 0
                },
                {
                    "sent": "You know, outside of the research community already, OK, you know, so I didn't see the improvements.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this thing is for my monthly organization.",
                    "label": 0
                },
                {
                    "sent": "At the time we started that we did not have any good layer labeled data set.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we took some standard named entity organizer or we applied it on Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "And we use.",
                    "label": 0
                },
                {
                    "sent": "This lowers, but we couldn't show results because it seems pretty good.",
                    "label": 0
                },
                {
                    "sent": "But I mean what can I say?",
                    "label": 0
                },
                {
                    "sent": "It's maybe like learning zero.",
                    "label": 0
                },
                {
                    "sent": "Also the other one.",
                    "label": 0
                },
                {
                    "sent": "But we showed that actually.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, it still helps when you train it together.",
                    "label": 0
                },
                {
                    "sent": "Those on task.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "As far as I understand, when you convert a sentence representation at some point to do a Max operation, India.",
                    "label": 0
                },
                {
                    "sent": "You tried some as well, yeah, but then you.",
                    "label": 0
                },
                {
                    "sent": "I mean basically yeah, try it works worse and it's not surprising.",
                    "label": 0
                },
                {
                    "sent": "Yeah, much worse actually.",
                    "label": 0
                },
                {
                    "sent": "So surprising because I mean you really want to get the interesting features for.",
                    "label": 0
                },
                {
                    "sent": "For the water antecedent attack, unusually interesting feature around the world and not.",
                    "label": 0
                },
                {
                    "sent": "You know, if you sum, you basically smooth completely of the features and you lose a lot of information.",
                    "label": 0
                },
                {
                    "sent": "Yeah indeed.",
                    "label": 0
                },
                {
                    "sent": "Or you know, it's so hot warm these days deep.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean it's you know of course, every coalitional network is deep because there is always a lot of layers, so you can call it deep into like I don't care personally always said actually no networking is talking and it's written deep with.",
                    "label": 0
                },
                {
                    "sent": "They actually had no particular problems paying this buyback, something no.",
                    "label": 0
                },
                {
                    "sent": "Or I mean it's not that deep in then you know it's like 6 layers so so I mean it's OK.",
                    "label": 0
                },
                {
                    "sent": "So I didn't use any trick, just don't know by partially fixed learning rate works very well.",
                    "label": 0
                }
            ]
        }
    }
}