{
    "id": "yff564bw3lk3rzhlsujxfrjyvumjzzvd",
    "title": "Analysis of Clustering Procedures",
    "info": {
        "author": [
            "Sanjoy Dasgupta, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/mlss09us_dasgupta_acp/",
    "segmentation": [
        [
            "We have a tutorial today.",
            "As usual, it's going to go from 8:30 to 12:00 with some breaks in between.",
            "And today we'll have the analysis of clustering procedures by Sanjoy Dasgupta.",
            "OK, so so clustering is one of the most basic statistical primitives.",
            "It's something that's.",
            "Used all the time and used everywhere, but it's also something that has traditionally suffered from a kind of murkiness, almost to the point of ill defined itness.",
            "Over over the years, however, various people have tried to to put clustering on a somewhat firmer footing by coming up with specific goals for for a clustering procedure and trying to analyze algorithms with regard to these specific goals.",
            "And So what I'll do is to just give a very brief introduction to this specific area.",
            "And as you'll see, the picture is still very far from being complete.",
            "There's a lot of things that really still need to be worked out.",
            "Feel free to stop me at anytime if there are any questions about about anything at all."
        ],
        [
            "OK.",
            "So.",
            "Just in terms of the sort of problems I'll be talking about, we're talking about two different types of clustering procedure.",
            "The first is what's often called flat clustering.",
            "OK, so the input in this case to the clustering algorithm is a data set.",
            "And in addition.",
            "The algorithm is given some number, it's still how many clusters are desired.",
            "OK, so in this case it's given these data.",
            "It's still there.",
            "Two clusters are needed, and then it returns something like the thing on the right.",
            "Many algorithms of this form in addition to actually returning a partition of the data in addition to returning the two clusters, also return representatives for each of the clusters.",
            "OK, which are very frequently just the means of the clusters.",
            "The second type of procedure I'll be talking about is a hierarchical procedure, and here the input is is just the data set.",
            "OK, so.",
            "The number of clusters doesn't need to be specified, and the output is a tree that contains K clusterings for all K OK that contains clusterings at all levels of granularity, and this is attractive because a lot of the time you have no idea how many clusters to expect, OK?",
            "In this picture over here, I just shown 2 levels of granularity.",
            "I've shown a case where you would have two clusters.",
            "OK, these two clusters and also a case where you would have three clusters, but in general the tree would contain.",
            "All levels of granularity would have one cluster to cluster three clusters and so on.",
            "OK, so this is called a hierarchical clustering and these are the two types of procedures that I'll talk about.",
            "OK."
        ],
        [
            "The second degree of freedom is the manner in which the data is presented.",
            "And.",
            "So sort of the most sort of the most common form of input is just is to assume that the data is just presented as a batch, just given a whole bunch of data points, and you get to work on them.",
            "And in particular, this assumes that the data actually fits into memory which.",
            "You know which is inconvenient sometimes?",
            "OK, so that's the batch model, and that's the setting for most of the algorithms.",
            "Sometimes the data set is just really too large to fit into memory, and these sort of settings have have motivated a screaming model.",
            "Where?",
            "Well, where the data are just sort of whizzing by, so the problem is exactly the same one you have a specific finite job to do.",
            "But you can't keep it all in memory and so you just assume that you see these points one at a time.",
            "You do a pass through the points and if some points are interesting too, you can keep them in memory, but the amount of memory you have is little over Ben.",
            "OK, for instance, it might be square root of N, so you can only keep a few of the points in memory.",
            "And there's also a potential to do more than one pass, so maybe you'll go through the data once and then you might want to go through it a second time.",
            "OK, and so the complexity measure here includes the number of times you have to pass through the data.",
            "The third model is actually quite different from the 1st two.",
            "It's a model where there isn't a finite job to be done, but rather there's an endless infinite stream of data.",
            "OK, so there's no point at which you you know, throw up your hands and you say that I'm done.",
            "This is the final clustering.",
            "It's endless.",
            "OK, it's.",
            "This is when you're creating a system that is continuously adapting to new data, and at any given point you have to maintain a clustering that is good for all the data you've seen so far, and the distribution might be shifting in an arbitrary way.",
            "OK, so that's an online model, and so these are the three sort of models that we look at.",
            "Now, in terms of analyzing these."
        ],
        [
            "Here's what are the sort of questions that that one asks.",
            "The first is how good is the procedure, and this really depends on its intended use.",
            "There are many ways in which clustering algorithms get used.",
            "But I'll just talk about two very broad types of uses, each of which motivates different types of performance guarantees for different types of analysis.",
            "The first use is what can be described as quantization.",
            "This is what happens, for instance when you are doing you know audio coding or something like this.",
            "So the situation here is that you have a very large set, perhaps an infinite set, and you want to just summarize it by a few representatives.",
            "OK, and these representatives are going to be the cluster centers, so just to draw a picture you know you have some sort of set.",
            "OK. And.",
            "And you want to.",
            "OK. OK, you just want to find a certain bunch of representatives.",
            "It will discretize this set or quantize it.",
            "And the goal here is just to minimize the overall distortion.",
            "The loss induced in going from this large that perhaps a continuous set to a finite number of representatives.",
            "OK, so the loss in this case might simply be the distance the typical distance of a point from its representative.",
            "So.",
            "So this is an important use of clustering and it's a use in which there's typically a fairly precisely defined cost function that is to be optimized.",
            "Cost function captures the amount of distortion, so you want to find a clustering, a bunch of centers that minimizes the distortion that's induced.",
            "The second use of clustering.",
            "I'll be talking about the secondary.",
            "Broad use is just finding meaningful structuring data, and this is a.",
            "And this is this is a much hazier sort of question.",
            "It's formulated in many different ways.",
            "In some cases one wants to say that if there is a clear grouping in the data, or if there's one group in a data set that seems to stand out from the rest, then we want to be able to pick out that group.",
            "But it's but there, of course many ways to define what it means for one group to be standing out from a data set.",
            "Another aspect of this of this type of clustering is that one is typically interested in statistical guarantees.",
            "So in this case, for instance, you can think of the data as being drawn IID from some underlying distribution, and the hope is that whatever clustering you obtain on the finite data set is in some way indicative of this underlying distribution.",
            "It tells you something important or interesting about the distribution.",
            "So this is a setting where generically you have a certain bunch of groups and you hope to be able to detect them OK. OK, and so so these are really two very different uses of clustering, and they'll motivate different types of analysis.",
            "The second question that will sometimes want to ask is simply, how fast is the algorithm and this is because some of the algorithms that are commonly used.",
            "Oh, it's actually not at all clear how long they take OK, and so this is an interesting question."
        ],
        [
            "OK, so here's the outline of the things I'll cover.",
            "I'll basically go through three parts and then we can take a break between between parts.",
            "The first is, the first is very much in the quantization setting where you have a precisely defined cost function for clustering.",
            "It turns out that just about any cost function that people use is intractable to optimize exactly 'cause it's not about NP, hard problem, and so so one question one can ask is if they're good approximation algorithms, and so that's what I'll talk about in the first part look at.",
            "Look at sort of perhaps the two most common or important cost functions, and then we'll look at algorithms for for dealing with these in the flat.",
            "For flat clustering, hierarchical clustering and also for batch streaming and online models.",
            "OK.",
            "It turns out, however, that most of the algorithms will talk about in the first part are not algorithms that people typically use.",
            "These are not the ones that are that are, you know, better popular.",
            "So in the second part we will do is.",
            "We look at some of the some of the most commonly used clustering algorithms and just try to understand what can be said about them and what sort of you know what sort of guarantees one can hope for.",
            "What are the right questions to be asking about these?",
            "And in the third part will finally get to the statistical theory of clustering.",
            "How does?",
            "How does the clustering of a finite data set relate to the underlying distribution?",
            "What's known over here?",
            "OK. OK, so yeah once again feel free to stop me if you have any questions at all."
        ],
        [
            "I will be doing just a few proofs.",
            "One thing I should say is."
        ],
        [
            "That this is, you know, this just an introduction to the sort of the theoretical terrain around clustering, and I've left out a lot of topics.",
            "OK, in particular, I've left out anything that is going to be covered by other talks in this workshop.",
            "For instance, I'm not going to be talking about spectral methods, which are which are, you know, which you know beautiful and you know have have been developed quite a bit recently.",
            "The other thing I'm leaving out is is that a lot of algorithms that have.",
            "Very good associated guarantees, but which are impractical for for one reason or the other.",
            "For instance, maybe they take more than quadratic time, and those are also, you know.",
            "I'm also leaving those out."
        ],
        [
            "OK, so I'll start by talking about approximation algorithm for clustering."
        ],
        [
            "OK, so here's the setting, OK, just to be concrete.",
            "Let's assume that we have vector data.",
            "OK, so you have a data set that lies in some Euclidean space Rd, and let's start by looking at flat clustering.",
            "So this is a situation where you're told how many clusters you want.",
            "OK, so you want K clusters.",
            "And now the output is going to be some bunch of centers, some bunch of representatives for the data, and we want to minimize the amount by which the data has been distorted.",
            "Want to minimize the overall distortion OK?",
            "Now in this setting, the distortion on a particular point is pretty easy to characterize.",
            "It's simply the distance from the point to its nearest representative.",
            "OK, so over here X is a particular data point, and C is the set of representatives.",
            "The set of centers.",
            "The distortion on X is simply the distance from X to those centers.",
            "The distance from X to its closest representative.",
            "Oh, the part where variability comes in.",
            "Is fair is when you decide how to characterize the overall distortion, so we have the distortion on a single point and there are two very popular ways to characterize the overall distortion.",
            "One of them is the maximum distortion.",
            "The distortion induced on the very worst point, and this is called the case center cost function.",
            "OK, so for instance in this data set over here, let's say that the blue points are the X, the data points and the red points are the representatives that we've chosen.",
            "So in this case the blue points with the data set we ask we will ask for three clusters and we ended up choosing these three centers.",
            "The distortion is that of the worst point, namely this distance over here.",
            "OK, so for the K center cost function, this is the distortion.",
            "So it's a worst case type of distortion.",
            "And this is this turns out to be very useful because it's it's tantamount to coming up with an epsilon cover of a data set or covering the data set by a bunch of balls of fixed radius, which is a which is very useful basic primitive.",
            "The second measure of distortion is.",
            "Is instead of focusing on the very worst point to look at the typical distortion, and this is something that would be more important in something like a coding application.",
            "OK, we don't care about what the worst were.",
            "The very worst point looks like, but on average, how much do you lose and so you can look at it as an average or just a sum, just the sum of all these distances.",
            "Now it turns out that for various reasons, basically statistical reasons.",
            "It's convenient to put a square over here.",
            "OK, so instead of looking at the average distance to look at the average distance squared and this is nice for many reasons.",
            "For instance, that implies that the correct representative for a set of points is simply the mean of those points.",
            "OK, which is something very natural and nice.",
            "OK.",
            "So these are two models we'll be looking at.",
            "The second one is called K means.",
            "OK, so any any questions about these?",
            "Yeah.",
            "Right, so yeah, so let's look at a situation where one of the clusters had a whole bunch of points, right?",
            "And let me just draw that on the right over here.",
            "So we just have a huge number of points over here.",
            "Now these points are all going to be summarized by this one representative.",
            "And so you know, it's an important representative.",
            "But the average still captures the typical distortion you know.",
            "So even though the clusters have different numbers of points, the average distance from a point to its closest representative.",
            "Captures the distortion on a typical point on a point drawn at random, so.",
            "Just to clarify, the averages per point, the average over points rather than over clusters.",
            "So it's not that you know this cluster has this radius that clusters that radius Plus says this radius.",
            "Let's average those three radii, it's the average over points.",
            "So pick a point at random.",
            "What is the expected distance to its representative?",
            "So that's the measure.",
            "OK, OK so these are the two measures of distortion that we'll be looking at, and there's been a lot of work on both of them, and so I'll I'll just describe some of this."
        ],
        [
            "OK, so the first one will look at is the is the notion of maximum distortion so."
        ],
        [
            "This measure over here OK where we want to find a bunch of representatives that are close to every single point.",
            "OK, we we cannot ignore any any given point."
        ],
        [
            "And there's a very nice algorithm for doing this, which was given in 1985 by Teofilo Gonzalez and it's called furthest first traversal.",
            "And it's something that's very widely used, and so it's because it's so.",
            "It's so simple and clean.",
            "OK, so here's how the algorithm works.",
            "You have a data set, you're told how many clusters you want.",
            "You told how many centers you want.",
            "Here's how you pick the centers.",
            "Start by picking any of the data points as the very first center.",
            "It doesn't matter which one.",
            "Then at the second center, pick the data point furthest away from it.",
            "As the third center picked the data point furthest away from those two, and so on, and in this way you pick K centers.",
            "OK, so let's see how it would work on this data set.",
            "So we have these these nine points over here.",
            "Sorry, 10 points over here for our first center.",
            "We just pick any of the data points.",
            "OK, let's say I pick this one.",
            "At this point, the cost of the clustering.",
            "If I would stop over here is the maximum possible distortion.",
            "Which is pretty much the distance to this point.",
            "That is the maximum distortion.",
            "That point is the current bottleneck.",
            "It's the point that currently defines the distortion and therefore it's going to be my natural choice for the second center.",
            "OK, that's where I put the second one.",
            "Now suppose we just had these two simple OK, so let's say we want four centers, right?",
            "So that's my second center.",
            "Now if I just had these two centers, the maximum distortion would be maybe this point, or maybe that one up there.",
            "This one is not too bad because it's fairly close to the two.",
            "This one is pretty close to the one.",
            "All of these are fine.",
            "This one is looking pretty bad and the one up there is looking pretty bad.",
            "OK, so.",
            "Actually let me.",
            "So it turns out the next one.",
            "I guess it's going to be the one up there.",
            "OK, that's the one that's furthest away from the two I have.",
            "And now the point that's looking worst is this one, and so that's going to be the 4th center.",
            "And that's it, that's it.",
            "That's the algorithm, OK, these are the four clusters centers.",
            "Now at this point, what is the cost or the distortion of the clustering?",
            "It's simply the worst case distance, which is which probably this one over here.",
            "So the cost is that particular distance are that's the radius of the clustering.",
            "So we found four centers and everything is within distance are of those four centers.",
            "OK, so any questions about the algorithm?",
            "OK, so this is an algorithm that's very simple and convenient.",
            "The question is, what can one possibly say about something like this?",
            "Is it in what sense is this algorithm good algorithm?",
            "Is it after all we had a very specific cost function that we wanted to optimize?",
            "We want to minimize the distortion.",
            "Are these really the four?",
            "Is this really the optimal placement of four centers?",
            "Not necessarily, but it isn't too bad.",
            "OK, and here's what you can show.",
            "You can show that the cost of these four centers is at most twice whatever the optimal cost is.",
            "OK, so you're never worse than a factor two of optimal, and so let me explain why that is the case.",
            "And here's the here's the proof over here.",
            "OK, so in this procedure we end up constructing some set of centers.",
            "See OK. We speak any point.",
            "The one furthest away from it.",
            "The first one furthest away from that and in this way we get K centers and those centers are what I'm calling C over here OK?",
            "Once you have those centers, which in this example is just the points 1234, let's look at the data point that's furthest away from those centers.",
            "In this case, it's this point over here.",
            "It's that data point that defines the cost of the clustering.",
            "OK, so this is point X over here.",
            "That data point defines the cost of the clustering.",
            "The cost of the clustering is simply the distance from that point to the cluster centers.",
            "In this case, it's R. So the cost of this clustering is R. No, look at the cluster centers and look at this additional point X.",
            "So instead of K points we now have K plus one points.",
            "The K cluster centers as well as the additional point X.",
            "The thing about these points is that they're really all pretty far away from each other.",
            "They're all a distance, at least are from each other, so you have K plus one points that are all distance are from each other.",
            "Now whatever the optimal clustering is, it has only K clusters.",
            "And we have K plus one points that are distance are from each other.",
            "So two of these points must be in the same optimal cluster.",
            "So you have some optimal cluster that contains 2 points that are distance are from each other.",
            "Therefore that optimal cluster must have got a radius of at least R / 2.",
            "OK.",
            "So the optimal distortion is at least R / 2.",
            "This time here.",
            "I could, I'm not.",
            "I'm not sure exactly what the what the epsilon sampling thing is.",
            "Excellent.",
            "Up to epsilon radius.",
            "Yeah, so this is.",
            "This is a very basic procedure and it turns out that it's used in a vast array of different contexts.",
            "So yeah, sometimes one stops when one gets to a certain number of centers.",
            "Sometimes one stops when one gets to a certain radius, because in that case the goal is to is to cover.",
            "The point is to have a distortion of at most epsilon, and so that lets you construct an epsilon net of the data points.",
            "OK, so so any questions so far about this.",
            "OK so this is very useful procedure and it has this nice guarantee even if it doesn't find the very best placement of the case centers.",
            "It's quick, it's really easy and it finds some placement of the case centers.",
            "That is, it is at most of factor 2 away from optimal OK.",
            "So let me just talk briefly about this factor of two so you know people who are in computer science are very used to approximation algorithms.",
            "You know you have some algorithms that have a factor of 1 plus epsilon.",
            "You have some sort of constant factor approximation factor of two factor of 10 factor of 20.",
            "Then you have some algorithms that have a factor like log in.",
            "What do these factors mean?",
            "And specifically, in a clustering context, what does it mean that you have a factor 2 approximation?",
            "Is that something that you should be feeling pleased about, or something that is really not that good?",
            "OK, how do we interpret a factor 2?",
            "So let me show you a picture over here."
        ],
        [
            "Suppose that the data set looks like these spheres over here.",
            "OK, so the data set consists of these three blobs.",
            "It has three very clearly defined clusters in it.",
            "And let's say your clustering procedure instead of identifying the three clusters, ends up putting one center over here, one center over here and one center up here.",
            "What is the cost of this?",
            "How far is it from optimal?",
            "Will be optimal?",
            "Radius is something like this OK and so the and the cost of this clustering is something like this.",
            "So this is an example of a clustering that's probably a factor 5 away from optimal.",
            "The radius is 5 times as large as it should be, or maybe 10 times as large as it should be.",
            "So what factor 2 guarantee tells you is that you'll never end up in this situation.",
            "OK, it prevents major screwups.",
            "That's that's what it tells you.",
            "OK, now this might not seem like much.",
            "You know.",
            "You know, it's ridiculous to have a clustering like this, you know, but it turns out that the procedures that are commonly used things like the K means algorithm.",
            "Very easily fall into local Optima of exactly this type.",
            "This is a very typical sort of local optimum into which into which one of the sort of popular clustering heuristics commonly folds OK, and it doesn't matter how far away these things are from each other.",
            "It's one of this is one of the classic types of local Optima.",
            "More than N squared, I didn't want more than N ^2.",
            "Yeah.",
            "Yeah, it would be interesting to find ways to speed this up.",
            "It appears that the dimension plays into it as well, proportional to the dimension."
        ],
        [
            "OK, So what is known in terms of the complexity of this problem?",
            "So we've seen that we can come up with a factor 2 approximation, but.",
            "But perhaps we can do better.",
            "We can so factor 2 is a reasonable sort of guarantee.",
            "Is it possible to do better?",
            "OK, so.",
            "So the first thing is that although we've been talking about Euclidean space, it turns out that exactly the same thing works in any metric space.",
            "The only thing we needed was the triangle inequality.",
            "OK, so.",
            "OK, so that's again in fact it to what about lower bounds?",
            "Can one come up with a factor better than two?",
            "So this was considered in the late 80s, and so these two results were shown.",
            "We've shown that if you're talking about points in a general metric space, you can't do better than a factor of two.",
            "You can't even do you know a 1.99.",
            "OK.",
            "In Euclidean space, what was shown is that you can't do better than a factor of 1.82.",
            "So this is what's known in terms of lower bounds, so some so a couple of open problems here are first of all, to close the gap in the Euclidean case, in the sense that is the most interesting case, and it would be nice to resolve the gap.",
            "This is probably not something that's you know particularly important in practice, but it's kind of an annoying hold that that in the period list, that would be nice to be very nice to resolve.",
            "And yeah.",
            "Yeah, so if you write, so that's a great question.",
            "So the question is, what happens if the data set contains outliers, OK?",
            "And for this particular cost, functions outliers are very important.",
            "OK, because the goal of this cost function is to be close to all data points, your centers must be close to all the points including outliers.",
            "So if there's some point, if you have a million points that are really close together and you have one point that you know outer Infinity, you must have a center.",
            "At that point, there's no two ways about it and this procedure that I talked about Further's first traversal will certainly put one of the centers out there.",
            "The next sentence I mean.",
            "Yes.",
            "Are the distances that computed from all the points considered so far right?",
            "So let me just draw a picture.",
            "OK, so suppose why data looks like this?",
            "OK, so we start by picking any old center.",
            "Let's say we pick one at random, which means it probably won't be this one out here.",
            "OK, so let's say we pick this one, the next one we'll take is going to be the one furthest away from that.",
            "Which is this?",
            "At this point, the distances are computed to the closest center.",
            "OK, so basically everything's distance is computed to this one because point #2 is not close to anything, and so the next one that will be chosen is this.",
            "Cancel, yeah, so outliers are something that can't be ignored in this function return.",
            "Approximation.",
            "I'm not sure.",
            "I'm not sure.",
            "I mean, certainly one can if 1 four allowed logarithmically many then it would become a set cover type problem.",
            "But I don't know about the intermediate.",
            "But there might be some work on it in by Feder and Greene.",
            "OK, and the second the second problem is that so this particular procedure really does turn out to be very useful and very widely used.",
            "This furthers first traversal, but there's certain things about it.",
            "They definitely seem a little odd.",
            "For instance, the fact that the very first point you pick can be anything, it doesn't matter.",
            "So we really interesting if there were other procedures that were also good in practice.",
            "You know to have something else to do with this problem.",
            "To do for this prob."
        ],
        [
            "OK, so now let's move onto the K means cost function and just as a reminder, the way this differs from from the current cost function is that.",
            "When you have a bunch of coins and you choose representatives, the cost is the sum or average of all distances.",
            "OK, it doesn't matter very much whether we choose the sum or the average, since they differ from each other just by the number of points by the buyer fixed by a fixed ratio.",
            "OK, so this is the K means cost function.",
            "Now it turns out that this is in spirit quite different from from the case enter cause function and the outliers really give some insight into that.",
            "OK, so if there's one point that's really far away.",
            "In the case center cost function, you really must pick that point because we want to be close to everything in the K means cost function.",
            "You can kind of sacrifice that point since it really receives away just of one over, and so it's a point that you're allowed to kind of give up on if everything else is really close together, and so a furthest first traversal will not work.",
            "In this case, it's just not a good idea since it basically in a case with outliers it will exactly pick out the outliers, but it turns out.",
            "Did a stochastic version of.",
            "It works just fine.",
            "OK, so here's the version.",
            "It's very close to a further first reversal.",
            "What you do first is that the first center you pick is a random datapoint.",
            "Now, in the previous case, it was an arbitrary data point.",
            "There's a big difference between the two.",
            "Here.",
            "It's important that you pick one of them at random, OK?",
            "Once you do that."
        ],
        [
            "OK, so let's say you have these data points.",
            "OK, you pick one of them at random.",
            "Let's say you pick this one.",
            "That's your first center.",
            "After this, instead of picking the point that's furthest away from it.",
            "You again pick the second center at random, but weighted by the squared distance from this one.",
            "OK, so for the choice of 2nd point you could easily pick this one, but that's a low probability event.",
            "You're more likely to pick something that's further away, something like this, or like this.",
            "So like this.",
            "OK, so let's say just for the sake of argument that the second one we pick is this one.",
            "The third center we pick is again going to be chosen randomly.",
            "But weighted by disk by distances from the first 2 centers.",
            "OK, so for instance, this has this point has got a low probability of being chosen.",
            "Whereas this point out here has a pretty high probability of being chosen.",
            "OK, and so let's say this is the one we actually pick.",
            "And so on.",
            "OK, so this is a stochastic version of the furthest first reversal.",
            "This is what's needed to address the slightly the actually rather very very different cost function, and now what can you?",
            "What can you prove for this?",
            "Well, this is a randomized procedure, so you run it 3 different times, you're going to get a different answer in each case, so you can talk, however, about the expected cost of the answer and the cost.",
            "Now is the K means cost the average distortion?",
            "Rather than the worst distortion and what you can show is that this expected distort this expected cost is at most log K times optimal.",
            "OK, I've said order log K because there's a constant in front of it.",
            "The constant is not terribly large.",
            "It's single digits, but it's there.",
            "It would, but you can think of it in this way.",
            "Suppose you have a million data points and one of them is an outlier.",
            "OK, so now it picks the first point at random.",
            "The outlier definitely has a much higher chance of being picked than any of the other 1,000,000 -- 2 points, but there's 1,000,000 -- 2 of them.",
            "All with a relatively small probability, but because there's so many of them, there's a much higher chance that you'll end up with one of those as your second point.",
            "OK anymore questions about this.",
            "OK so this is.",
            "This is a very nice procedure.",
            "An actually it turns out that a lot of a lot of sort of empirical testing has been done of this and it really works quite well OK. Now, so the disturbing thing over here, however, is that the factor depends on K. OK, the the the quality of the approximation depends on K and it's not just because it's a loose bound, it really does depend on K. It depends logarithmically on K and so one might ask can you do a constant factor?",
            "Can you come up with a factor that doesn't depend on on K?",
            "Because sometimes we want K to be large.",
            "And it turns out the answer is yes, and I'll give you an example of such a procedure.",
            "This is the local search algorithm which was developed in 2003 by.",
            "By Kanungo and David Mount and Angela.",
            "You and some other people.",
            "And as a local search procedure, what it does is.",
            "You know, let's say you want K centers.",
            "It ends up putting the case centers wherever and then repeatedly trying to tweak them to see if there's.",
            "If there's some simple way to update those centers, which improves the cost.",
            "So for instance.",
            "We start with any way to choose the centers.",
            "So let's say we want three centers.",
            "And let's say we pick one here.",
            "You know what?",
            "Let's say we pick one here, one here and one here.",
            "Let's say those are our three centers.",
            "OK, so that's how you.",
            "That's how you start off.",
            "It doesn't matter how you start off.",
            "What you do next is to try and swap one of the existing centers with one of the data points and see if that helps things.",
            "OK, so you say, well, let me just pick any of these centers like let's say I pick this one, would it help to swap it with one of the data points?",
            "Like say this one OK and you try all possibilities, every current center, every data point you try the swap, and if there's any swap that improves things then you do this one.",
            "OK.",
            "So, for instance, let's say you decide to swap this with this one because it helps.",
            "It really does help.",
            "And so that's what you do.",
            "Now you see, well, is there any other swap I could do?",
            "Well maybe it would help to swap this with that.",
            "Then you do that and so on until you can no longer do until no swap helps OK?",
            "And in this case you can show that the cost is at most 50 times the optimal cost.",
            "OK so I just put this up as an example of a local search procedure and also as an example of a constant factor approximation.",
            "This is not something that's a terribly efficient algorithm, but but it's one of these really cool cases where you can actually prove something about a Hill climbing or local search procedure, which is which is in general very difficult to do OK.",
            "The other thing is that you know the fact seems cosmetically a little bit better 'cause it's a constant rather than log K, But you know log K can be is is actually less than 50 on this key is very large, so KK would have to be more than two to the 50.",
            "OK. Intuition about money.",
            "It could be 49, it could be 49.",
            "Basically, in these analysis, in this particular analysis, there wasn't.",
            "There wasn't a huge effort made to get the constant down.",
            "The goal was mostly to get a constant factor approximation.",
            "I don't think that you know what ends up happening is that one keeps using the triangle inequality over and over again, and you know and so these factors of 2.",
            "Tend to multiply, but it may well be that you know for the same algorithm one can prove a better factor.",
            "Examples.",
            "Yeah, the so the lower bound is something I believe it's something like 10.",
            "Selfie.",
            "The optimal cost is.",
            "Is the average distortion or induced by the optimal placement of centers?",
            "So in this case, so for instance, in this particular procedure always returns centers that happen taliah data points, but that's clearly suboptimal.",
            "In Euclidean space, the centers are never going to be on data points.",
            "Probably the optimal centers would be something like this.",
            "A point over here a point over here and.",
            "And maybe a point over here or something like that.",
            "The cost, the cost of these will be the sum of distances to these centers, and so the optimal cost is is the minimum cost over overall placements of of sentence.",
            "So the optimal cost is some refers.",
            "You know it is a minimum taken over positionings of centers that don't have to coincide with data points that could be anywhere in space.",
            "OK, and that immediately induces a factor to sort of, since we're dealing with.",
            "Since our algorithm is only putting centers at data points, it immediately loses a factor of 2.",
            "Because of that.",
            "OK."
        ],
        [
            "OK, so again, you know we can ask whether it's possible to do better.",
            "What what is known in terms of?",
            "You know we have a factor 50 approximation.",
            "We have a factor log K approximation.",
            "Why can't we just solve this problem exactly?",
            "Why do we even need an approximation at all?",
            "What's known in terms of upper and lower bounds?",
            "OK, so can we solve K means exactly?",
            "So one thing that's known is that you can solve it exactly if you have time end to the KD, but that's that's the astronomical.",
            "You know, unless the dimension is 1 and K is like two.",
            "OK, so in most cases that's astronomical and this is an exhaustive procedure that basically tries all possible clusterings.",
            "What about lower bounds?",
            "So the following lower bounds are known for the case where.",
            "So K means is known to be NP hard when even if you have just K = 2.",
            "Even if you're looking for just two centers, it's known to be NP hard.",
            "And.",
            "And you know, one way to think about this is that just recorded the centers don't have to be a data points.",
            "OK, if you were looking for just two centers and they were a data points, well then you just have N squared possibilities to try out.",
            "But the centers could be anywhere in space and it turns out that therefore, even for the case where you're looking for just two centers, if the dimension is allowed to be arbitrarily high, this problem is NP hard.",
            "Likewise, the problem is also NP hard if the dimension is 2, it's NP hard even in the plane.",
            "So some open problems here are to come up with some good hardness of approximation result and to come up with better approximation algorithms.",
            "OK, so any questions about this stuff?",
            "Yeah.",
            "Just pick.",
            "Um?",
            "I will.",
            "So, so I'll tell you the way that in which this would typically end up happening.",
            "So clearly in real life one definitely wants to to pick centers that are not data points."
        ],
        [
            "This particular algorithm over here gives you something that's fairly good.",
            "It's a log K approximation and this can be then used to initialize something like the K means algorithm.",
            "OK, so K means is a cost function, but there's also an algorithm by the same name.",
            "Which is a very popular, perhaps the most popular clustering algorithm out there.",
            "But it's a local search procedure and it's not necessarily.",
            "You know, it doesn't necessarily give a very good answer, but if you initialize it well, it is guaranteed to never be worse than what you initialized it with.",
            "So you can be certain that it's never going to be worse than a log key approximation, and it will round things out nicely.",
            "You know it will.",
            "It will end up with centers that are not data points and so that would be, you know if one has to sort of.",
            "If you know, come up with, you know one reasonable way of solving this problem that will be it to initialize like this and then to solve."
        ],
        [
            "But there are other ways to do it as well."
        ],
        [
            "OK. OK, so so far we've talked about flat clustering.",
            "So now let's move on to hierarchical clustering.",
            "So what are these things?",
            "Hierarchical clustering is just a recursive partitioning of a data set.",
            "So let's say you have five data points.",
            "You start with everything in one big cluster.",
            "At the top of the tree.",
            "And then you split it into two.",
            "OK, so you branch out into two clusters and then you split one of those and then you split another one and then you split another one.",
            "And often these things are drawn as a dendrogram.",
            "The diagram on the right.",
            "OK, so at the top you have a one clustering.",
            "And then if you cut the dendrogram at any point, like let's say you cut it here, you get 2 branches, so you get one cluster containing the points 123 and another cluster containing the points 45.",
            "If you cut it over here, you get 3 clusters.",
            "This one, this one and this one.",
            "If you cut it here, you get 4 clusters, and if you cut it here, you get 5 clusters.",
            "OK, so a dendrogram is a very convenient representation of these sort of clusterings depending on where you cut it, you get a K clustering for any K. OK, so this is a hierarchical clustering and these things are really very popular.",
            "And for at least three reasons, the first is that you don't need to specify a number of clusters in many situations.",
            "You know you're doing some sort of exploratory analysis of the data set.",
            "You have a new data set that you've been given.",
            "It's in high dimension.",
            "You can't visualize it.",
            "You don't know the first thing about it and you want to get some sense of whether it contains any groups that are significant or that you should be aware of.",
            "You have no idea what number of clusters to pick, and so hierarchical clustering is something that's reasonable to do.",
            "You don't have to specify how many clusters you want.",
            "It might also be the case that the data has different clusterings at different levels of granularity, and so it's nice to have a single picture that captures all of those structures.",
            "You know it might be at a certain level of granularity at at a certain scale, there is a very nice clustering, but then at Louis scale there's another nice clustering, and so on.",
            "And you want to be able to capture all of these in a single picture.",
            "Finally, there another reason why this is popular is that it turns out there are some really simple greedy.",
            "Turistic for constructing these things that are in very wide use in our part of you know all statistical software packages and so it's very easy to use.",
            "This is a very popular kind of enterprise.",
            "So.",
            "What can we do to what can we say about these things?",
            "You know, you give it a data set.",
            "It spits out a tree.",
            "What can you say about the tree?",
            "In what sense is this tree good?",
            "What sort of performance guarantee can one possibly give for hierarchical clustering?",
            "And once more, I'll talk about this in the quantization setting, where we have a sort of a cost function in mind, we want to minimize distortion."
        ],
        [
            "So before getting into details, I just want to point out a basic difficulty with hierarchical clustering.",
            "Or there's a there's a certain sort of basic existence problem, and this is the following.",
            "OK, so look at this data set over here.",
            "It's got 12 points.",
            "Suppose I ask you give Me 2 clusters.",
            "OK, by any of the measures we've been seeing so far, this sort of radius based measures.",
            "This is the two clusters you cook up.",
            "OK, suppose I ask you give me 3 clusters.",
            "These are the three clusters you would cook up, so this points to a basic difficulty in hierarchical clustering.",
            "The two clustering, the optimal two clustering and the optimal three clustering are not hierarchically compatible with each other.",
            "One of them is not nested inside the other.",
            "And this seems like bad news because we want to come up with a tree that contains K clustering for all K is going to have to give us a two clustering.",
            "It's going to have to give us a three clustering and therefore it has to compromise on at least one of these.",
            "So the main sort of one of the first questions one would want to ask is how bad is this tradeoff?",
            "OK, the suspicion is it the case that.",
            "Dead by imposing a hierarchical structure, we are just dooming ourselves to intermediate clusterings of very low quality is.",
            "That is that a possibility, because that would be very bad news.",
            "OK, and so and so this is the question that we look at.",
            "We've already seen that in general, there is no way to construct a hierarchical clustering that is optimal at every level.",
            "This is nothing to do with NP hardness or anything like that.",
            "It's simply a structural issue.",
            "There simply does not exist a hierarchical clustering that is going to be optimal at every level, so we have to talk about approximate approximate optimality.",
            "How much do you lose by enforcing a hierarchical structure on top of these intermediate clusterings?",
            "OK, is it really bad?",
            "So we'll start by looking at the case center cost function.",
            "OK, here we want a tree where for each K if you cut the hierarchy at that point and you look at the K clusters you get it's not too far from the optimal K clustering from the optimal K center solution.",
            "We've seen that in the case of flat case center you can get a factor of 2.",
            "In the hierarchical case, it's not going to be that good, and what we can do is a factor eight.",
            "OK, so let me show you how to do that.",
            "Actually turns out there many ways to."
        ],
        [
            "This, but I'll just give you a particular example of a way to do it.",
            "There are many ways to actually do hierarchical case center.",
            "So.",
            "So this this particular solution to hierarchical clustering uses something called a cover tree, and this is an idea that's really been around for awhile, but.",
            "But recently you know some of the people have looked at incarnations of IT Crowd Gamer and Lee.",
            "Bagels I'm Earl Langford and kakade.",
            "A couple trees, a spatial data structure and it works on data points in any metric space.",
            "They don't have to be in Euclidean space.",
            "So let me tell you what these things are, because it's a pretty cool data structure, but it takes a little while to just kind of get used to it.",
            "OK, so let's say you have a bunch of data points like.",
            "Let's say you have these five points, we're going to organize them into a tree with the following properties.",
            "The tree is divided into levels into distinct levels, and each level has got a number associated with it.",
            "The numbers are consecutive.",
            "But the top level is not necessarily #0.",
            "The top level could be #3.",
            "If it is, if the top level is number 3, the next one is 4 and the next one is 5 in the next one is 6.",
            "The top level could be number minus 90, in which case the next one would be minus 89 -- 88 and so on.",
            "OK, so the numbers these level numbers go from minus Infinity to plus Infinity, but they're consecutive.",
            "OK, so these are the properties that the tree has.",
            "Suppose each node of the tree is associated with the data point.",
            "For instance, this node is associated with Point X1.",
            "If a node is associated with the point one of its children must also be associated with that point.",
            "And so on.",
            "So if you have an X one here, one of its children must be X one and one of these things.",
            "Children must be X one and one of these things.",
            "Children must be X1 and in this sense it's an infinite tree because you can imagine at this point, although we've captured all five data points, you can imagine these just along just these long straight paths going down to Infinity.",
            "OK, so that's the overall structure of the tree.",
            "The tree now has got a certain property.",
            "Any node.",
            "Is within a predictable distance from its parent.",
            "OK, so if you have a node that's at level J + 1, its distance from its parent is at most 1 / 2 over to the J.",
            "So for instance, this distance from X3 to X4.",
            "Is at most one.",
            "This distance from X1 to X5 is at most two.",
            "This distance is at most 1/2.",
            "So that's one guarantee that you'll give it as you go down the tree.",
            "The lengths of these distances are bounded.",
            "The other thing you're told?",
            "He said within a level.",
            "Within level J, distances are at least this nodes, or at least distance 1 / 2 to the J from each other.",
            "So for instance, these points.",
            "The distance between X1 and X2 because it's at level 0, is at least one.",
            "And the distance between X. X5 and X4 is at least 1/2.",
            "And this distance is.",
            "At least 1/4.",
            "OK, so you basically get a bunch of data points and you build a tree on them which has this property.",
            "It's somehow.",
            "Infinite tree, but you don't need to keep all of it around you.",
            "There's a.",
            "There's a way to just store it so that you keep only one copy of each node so that the storage is only order in.",
            "But it's a useful data structure because it somehow captures the geometry of the points.",
            "OK, so one thing at this point it's not even clear how you would build something like this, or why.",
            "Can you always make a data structure with these properties?",
            "So that's one question.",
            "In the second is why is it useful to even organize points in this way?",
            "OK, yeah.",
            "Sorry.",
            "Oh no, it's not unique.",
            "There are many ways to.",
            "There are many.",
            "In general there will be many different trees on the same set of points that have that have these properties.",
            "So the tree is not something that is optimized, it's simply something that happens to satisfy a few properties, and in general the many trees that will.",
            "OK, yeah.",
            "Yeah, it's only conceptually infinite.",
            "In this case, you never need more than N levels and yeah.",
            "OK, so first of all, how would you even build something like this?",
            "That's actually quite simple.",
            "Here's what you do.",
            "You can build it online.",
            "Each time you get a new data point X OK, so you can imagine the points arriving in an online fashion.",
            "First X1 arrives, then X2 arrives.",
            "The next 3 arrives and each point in each time a new data point arrives, you simply add it to the tree.",
            "How do you add it to the tree?",
            "Well, you look at the largest level such that the point is within 1 / 2 to the J of some node at that level, and then make it a child there.",
            "OK, so let's just do an example 'cause that's a little confusing.",
            "Let's say that.",
            "We already have a tree on these five points, and now let's say that a new point shows up X6.",
            "OK, so where is XI?",
            "Don't know we could put it over here, say.",
            "OK. And now we want to know how to incorporate X6 into the tree.",
            "Well, we go to this level.",
            "Does it lie within distance two of X1?",
            "Yes it does.",
            "Now we go down to this level.",
            "Does it lie within distance?",
            "One of one of these nodes?",
            "Yes it does.",
            "It looks like it lies within distance one of X2.",
            "Then we go down to this level.",
            "Does it lie within distance 1/2 of one of these nodes?",
            "No, it's more than distance 1/2 from all of those.",
            "We go down to this level.",
            "Does it lie within distance 12:45 of these nodes?",
            "No, it doesn't.",
            "So this was the last level at which it lay within the requisite distance, and so we make it a child of that.",
            "OK. And then of course this continues on down to Infinity.",
            "So that will become apparent very soon.",
            "The factor of two has actually been.",
            "The maximum distance between 2 points and then clusters.",
            "So at the mentality increases made so sorry in this specific example that happens to hold, but the tree itself does not tell us that.",
            "The tree only tells us that the maximum distance between any two points is 4 because everything is with the distance from X, one is at most 2 + 1 + 1/2 and so on since most full.",
            "OK, so kind of a weird data structure, but it is something that's easy to build, yeah?",
            "Um?",
            "So.",
            "Yeah, that's a good point.",
            "Well, what happens is the first point.",
            "Can actually go anywhere, but when you start with two points then it tells you where these things go.",
            "Yoko.",
            "Far away.",
            "Then it goes above.",
            "OK, so the mysterious thing over here is the factor of two, and it doesn't have to be exactly a factor of two, but let me.",
            "It could be some other factor, but let me show you how this plays."
        ],
        [
            "Into a hierarchical clustering guarantee or or why this addresses the hierarchical clustering problem?",
            "OK, so you have N data points.",
            "You build a cover tree on them and now suppose you want suppose you want to K clustering.",
            "You want to partition into K clusters.",
            "Here's what you do.",
            "You go to the lowest level that contains.",
            "At most two points, and so you would return this single center.",
            "Let's say you want to.",
            "For clustering.",
            "You return this and so on.",
            "OK, so when you want to K clustering?",
            "You return an entire level, and you simply return the lowest level that is legal.",
            "The lowest level that has got at most K points.",
            "And it turns out that this is at most a factor 8 of optimal.",
            "So let's quickly see why this is the case.",
            "It's basically because of a.",
            "So let's just go through the argument over here, OK?",
            "So, so we've asked for a K clustering.",
            "And what did we do?",
            "We just looked at the lowest level that was valid, the lowest level with at most K nodes.",
            "The cake was to just return the root because.",
            "Yeah, because that's within a factor eight of the best two centers.",
            "OK, so you just returned the lowest level that that you can.",
            "So, so here's the argument.",
            "Suppose you end up returning level J.",
            "In other words, J is the lowest level that contains at most K points.",
            "Why is that within a factor rate of optimal?",
            "Well?",
            "What we do know is that if it's level J, all of its children are within distance 1, / 2 to the J of it.",
            "OK, so if it's this level, all of its children are distance one from it.",
            "OK, so all of its children are distance 1 / 2 to the J from it.",
            "All of its grandchildren are an additional distance of at most 1 / 2 to the J plus one and all of its great grandchildren are an additional distance of 1 / 2 to the J + 2 and it's going down geometrically.",
            "So geometric series and so the distance, the cost of the clustering is at most 1 / 2 to the J -- 1 here.",
            "Just adding up this geometric series.",
            "OK, so each point is within distance at most 1 / 2 to the J + 1 / 2 to the J + 1 + 1 over to the J + 2 + 1 over to the J + 3.",
            "So we just using the triangle inequality over here OK, and so the everything the cost of the clustering.",
            "The case centered distortion is at most this much OK, so that's the actual cost of the clustering we return.",
            "But in order to show that it's a factor 8 of optimal, we have to show that any clustering.",
            "Is going to have, at least is going to have a cost that is at least 1/8 of that OK?",
            "And why is that the case?",
            "The reason is that.",
            "So we returned level J. J was the lowest level with at most K nodes.",
            "That means if you go down one more level you got K plus one nodes.",
            "At least.",
            "OK, so if you go down to level J + 1, you have at least K plus one nodes.",
            "Moreover, by construction, those nodes are pretty far away from each other because they are in a cover tree.",
            "We know there at least distance 1 / 2 to the J plus one from each other and therefore we have at least K plus one points which are at least distance 1 / 2 to the J plus one from each other.",
            "And again we use the same kind of argument is with K center.",
            "If you have K plus one points that are at least this distance away from each other.",
            "Any optimal clustering must have got two of those points in the same cluster.",
            "By pigeonhole principle.",
            "'cause you got K plus one point so far away from each other, no matter how you carve them up into clusters, two of those points are going to be in the same cluster.",
            "OK, and therefore the radius of the cluster is going to be at least half the distance between those two points.",
            "OK, and so that's how we get the factor of eight over there.",
            "OK, so.",
            "So this is how we do a hierarchical case antenna.",
            "Actually.",
            "Just do you know they actually many ways to do hierarchical Kate Center.",
            "Henry.",
            "Then you're so right, so this.",
            "Yeah, there's just a single tree that contains all the points and you can build it online and in the order in which the points arrive, but it is true that the there are cases in which you don't want to store all N data points.",
            "You know, sometimes you want to do hierarchical clustering.",
            "And you have a million data points which are never going to ask for K = 1,000,000.",
            "You're going to ask for K at most.",
            "I don't know 100 and thousand, and so you don't want to keep all million things in the tree.",
            "If that's the case, you don't need to keep around the whole tree.",
            "You can basically just keep.",
            "Just keep it up to the up to the last level that has more than K means.",
            "Stopping it does an so that it depends on the order in which the points were presented.",
            "If you build it in this particular way."
        ],
        [
            "So here are some open problems in hierarchical clustering.",
            "So we've seen a factor 8 approximation.",
            "Can we do better?",
            "As I said, there are many ways to do hierarchical K center, and they all give you a factor eight.",
            "OK, so I know of at least three or four different algorithms.",
            "There's a randomized variant of this that gives you a factor of 5.4.",
            "But in terms of deterministic versions, they all seem to give a factor of 8.",
            "Is there something better?",
            "OK now in terms of lower bounds that we might prove there are at least two sources of a lower bound.",
            "One is a short one is a pure NP hardness thing.",
            "We already know that case center.",
            "You cannot solve better than a factor of two, so you're certainly not going to be able to come up with a hierarchical clustering that has everything better than a factor of 2.",
            "OK, since that's a sub problem.",
            "But there's another sort of lower bound, which is the structural incompatibility, and it's not clear exactly what that lower bound is.",
            "Is it a factor of two?",
            "Is that you know this thing where the best two clustering and the best three clustering are not compatible with each other?",
            "There's some kind of lower bound that comes out of that.",
            "There's nothing to do with computational complexity, just a structural issue.",
            "What is that lower bound?",
            "Is it a factor of two, and can you?",
            "Can you team that up with the computational complexity lower bound to get some bigger lower bound, like maybe a factor of four or something like that?",
            "Not known, so that would be nice to get to have some progress on that.",
            "And the second thing is, hierarchical K means.",
            "Is there a good way to do this that would be that would be really interesting?",
            "OK, so so maybe we could just take a little break.",
            "Now I don't know like 10 minutes or so and then we come back and do some stuff."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a tutorial today.",
                    "label": 0
                },
                {
                    "sent": "As usual, it's going to go from 8:30 to 12:00 with some breaks in between.",
                    "label": 0
                },
                {
                    "sent": "And today we'll have the analysis of clustering procedures by Sanjoy Dasgupta.",
                    "label": 1
                },
                {
                    "sent": "OK, so so clustering is one of the most basic statistical primitives.",
                    "label": 0
                },
                {
                    "sent": "It's something that's.",
                    "label": 0
                },
                {
                    "sent": "Used all the time and used everywhere, but it's also something that has traditionally suffered from a kind of murkiness, almost to the point of ill defined itness.",
                    "label": 0
                },
                {
                    "sent": "Over over the years, however, various people have tried to to put clustering on a somewhat firmer footing by coming up with specific goals for for a clustering procedure and trying to analyze algorithms with regard to these specific goals.",
                    "label": 0
                },
                {
                    "sent": "And So what I'll do is to just give a very brief introduction to this specific area.",
                    "label": 0
                },
                {
                    "sent": "And as you'll see, the picture is still very far from being complete.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of things that really still need to be worked out.",
                    "label": 0
                },
                {
                    "sent": "Feel free to stop me at anytime if there are any questions about about anything at all.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just in terms of the sort of problems I'll be talking about, we're talking about two different types of clustering procedure.",
                    "label": 0
                },
                {
                    "sent": "The first is what's often called flat clustering.",
                    "label": 1
                },
                {
                    "sent": "OK, so the input in this case to the clustering algorithm is a data set.",
                    "label": 0
                },
                {
                    "sent": "And in addition.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is given some number, it's still how many clusters are desired.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case it's given these data.",
                    "label": 0
                },
                {
                    "sent": "It's still there.",
                    "label": 0
                },
                {
                    "sent": "Two clusters are needed, and then it returns something like the thing on the right.",
                    "label": 0
                },
                {
                    "sent": "Many algorithms of this form in addition to actually returning a partition of the data in addition to returning the two clusters, also return representatives for each of the clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, which are very frequently just the means of the clusters.",
                    "label": 0
                },
                {
                    "sent": "The second type of procedure I'll be talking about is a hierarchical procedure, and here the input is is just the data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The number of clusters doesn't need to be specified, and the output is a tree that contains K clusterings for all K OK that contains clusterings at all levels of granularity, and this is attractive because a lot of the time you have no idea how many clusters to expect, OK?",
                    "label": 0
                },
                {
                    "sent": "In this picture over here, I just shown 2 levels of granularity.",
                    "label": 0
                },
                {
                    "sent": "I've shown a case where you would have two clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, these two clusters and also a case where you would have three clusters, but in general the tree would contain.",
                    "label": 0
                },
                {
                    "sent": "All levels of granularity would have one cluster to cluster three clusters and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is called a hierarchical clustering and these are the two types of procedures that I'll talk about.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second degree of freedom is the manner in which the data is presented.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So sort of the most sort of the most common form of input is just is to assume that the data is just presented as a batch, just given a whole bunch of data points, and you get to work on them.",
                    "label": 0
                },
                {
                    "sent": "And in particular, this assumes that the data actually fits into memory which.",
                    "label": 0
                },
                {
                    "sent": "You know which is inconvenient sometimes?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the batch model, and that's the setting for most of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the data set is just really too large to fit into memory, and these sort of settings have have motivated a screaming model.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "Well, where the data are just sort of whizzing by, so the problem is exactly the same one you have a specific finite job to do.",
                    "label": 0
                },
                {
                    "sent": "But you can't keep it all in memory and so you just assume that you see these points one at a time.",
                    "label": 1
                },
                {
                    "sent": "You do a pass through the points and if some points are interesting too, you can keep them in memory, but the amount of memory you have is little over Ben.",
                    "label": 0
                },
                {
                    "sent": "OK, for instance, it might be square root of N, so you can only keep a few of the points in memory.",
                    "label": 0
                },
                {
                    "sent": "And there's also a potential to do more than one pass, so maybe you'll go through the data once and then you might want to go through it a second time.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the complexity measure here includes the number of times you have to pass through the data.",
                    "label": 0
                },
                {
                    "sent": "The third model is actually quite different from the 1st two.",
                    "label": 1
                },
                {
                    "sent": "It's a model where there isn't a finite job to be done, but rather there's an endless infinite stream of data.",
                    "label": 1
                },
                {
                    "sent": "OK, so there's no point at which you you know, throw up your hands and you say that I'm done.",
                    "label": 0
                },
                {
                    "sent": "This is the final clustering.",
                    "label": 0
                },
                {
                    "sent": "It's endless.",
                    "label": 0
                },
                {
                    "sent": "OK, it's.",
                    "label": 0
                },
                {
                    "sent": "This is when you're creating a system that is continuously adapting to new data, and at any given point you have to maintain a clustering that is good for all the data you've seen so far, and the distribution might be shifting in an arbitrary way.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's an online model, and so these are the three sort of models that we look at.",
                    "label": 0
                },
                {
                    "sent": "Now, in terms of analyzing these.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's what are the sort of questions that that one asks.",
                    "label": 0
                },
                {
                    "sent": "The first is how good is the procedure, and this really depends on its intended use.",
                    "label": 1
                },
                {
                    "sent": "There are many ways in which clustering algorithms get used.",
                    "label": 0
                },
                {
                    "sent": "But I'll just talk about two very broad types of uses, each of which motivates different types of performance guarantees for different types of analysis.",
                    "label": 0
                },
                {
                    "sent": "The first use is what can be described as quantization.",
                    "label": 0
                },
                {
                    "sent": "This is what happens, for instance when you are doing you know audio coding or something like this.",
                    "label": 1
                },
                {
                    "sent": "So the situation here is that you have a very large set, perhaps an infinite set, and you want to just summarize it by a few representatives.",
                    "label": 0
                },
                {
                    "sent": "OK, and these representatives are going to be the cluster centers, so just to draw a picture you know you have some sort of set.",
                    "label": 0
                },
                {
                    "sent": "OK. And.",
                    "label": 0
                },
                {
                    "sent": "And you want to.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, you just want to find a certain bunch of representatives.",
                    "label": 0
                },
                {
                    "sent": "It will discretize this set or quantize it.",
                    "label": 0
                },
                {
                    "sent": "And the goal here is just to minimize the overall distortion.",
                    "label": 0
                },
                {
                    "sent": "The loss induced in going from this large that perhaps a continuous set to a finite number of representatives.",
                    "label": 0
                },
                {
                    "sent": "OK, so the loss in this case might simply be the distance the typical distance of a point from its representative.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "So this is an important use of clustering and it's a use in which there's typically a fairly precisely defined cost function that is to be optimized.",
                    "label": 0
                },
                {
                    "sent": "Cost function captures the amount of distortion, so you want to find a clustering, a bunch of centers that minimizes the distortion that's induced.",
                    "label": 0
                },
                {
                    "sent": "The second use of clustering.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about the secondary.",
                    "label": 0
                },
                {
                    "sent": "Broad use is just finding meaningful structuring data, and this is a.",
                    "label": 0
                },
                {
                    "sent": "And this is this is a much hazier sort of question.",
                    "label": 0
                },
                {
                    "sent": "It's formulated in many different ways.",
                    "label": 0
                },
                {
                    "sent": "In some cases one wants to say that if there is a clear grouping in the data, or if there's one group in a data set that seems to stand out from the rest, then we want to be able to pick out that group.",
                    "label": 0
                },
                {
                    "sent": "But it's but there, of course many ways to define what it means for one group to be standing out from a data set.",
                    "label": 0
                },
                {
                    "sent": "Another aspect of this of this type of clustering is that one is typically interested in statistical guarantees.",
                    "label": 0
                },
                {
                    "sent": "So in this case, for instance, you can think of the data as being drawn IID from some underlying distribution, and the hope is that whatever clustering you obtain on the finite data set is in some way indicative of this underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "It tells you something important or interesting about the distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is a setting where generically you have a certain bunch of groups and you hope to be able to detect them OK. OK, and so so these are really two very different uses of clustering, and they'll motivate different types of analysis.",
                    "label": 0
                },
                {
                    "sent": "The second question that will sometimes want to ask is simply, how fast is the algorithm and this is because some of the algorithms that are commonly used.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's actually not at all clear how long they take OK, and so this is an interesting question.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the outline of the things I'll cover.",
                    "label": 1
                },
                {
                    "sent": "I'll basically go through three parts and then we can take a break between between parts.",
                    "label": 1
                },
                {
                    "sent": "The first is, the first is very much in the quantization setting where you have a precisely defined cost function for clustering.",
                    "label": 1
                },
                {
                    "sent": "It turns out that just about any cost function that people use is intractable to optimize exactly 'cause it's not about NP, hard problem, and so so one question one can ask is if they're good approximation algorithms, and so that's what I'll talk about in the first part look at.",
                    "label": 0
                },
                {
                    "sent": "Look at sort of perhaps the two most common or important cost functions, and then we'll look at algorithms for for dealing with these in the flat.",
                    "label": 0
                },
                {
                    "sent": "For flat clustering, hierarchical clustering and also for batch streaming and online models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It turns out, however, that most of the algorithms will talk about in the first part are not algorithms that people typically use.",
                    "label": 0
                },
                {
                    "sent": "These are not the ones that are that are, you know, better popular.",
                    "label": 0
                },
                {
                    "sent": "So in the second part we will do is.",
                    "label": 0
                },
                {
                    "sent": "We look at some of the some of the most commonly used clustering algorithms and just try to understand what can be said about them and what sort of you know what sort of guarantees one can hope for.",
                    "label": 0
                },
                {
                    "sent": "What are the right questions to be asking about these?",
                    "label": 1
                },
                {
                    "sent": "And in the third part will finally get to the statistical theory of clustering.",
                    "label": 0
                },
                {
                    "sent": "How does?",
                    "label": 0
                },
                {
                    "sent": "How does the clustering of a finite data set relate to the underlying distribution?",
                    "label": 0
                },
                {
                    "sent": "What's known over here?",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so yeah once again feel free to stop me if you have any questions at all.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will be doing just a few proofs.",
                    "label": 0
                },
                {
                    "sent": "One thing I should say is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That this is, you know, this just an introduction to the sort of the theoretical terrain around clustering, and I've left out a lot of topics.",
                    "label": 0
                },
                {
                    "sent": "OK, in particular, I've left out anything that is going to be covered by other talks in this workshop.",
                    "label": 0
                },
                {
                    "sent": "For instance, I'm not going to be talking about spectral methods, which are which are, you know, which you know beautiful and you know have have been developed quite a bit recently.",
                    "label": 0
                },
                {
                    "sent": "The other thing I'm leaving out is is that a lot of algorithms that have.",
                    "label": 0
                },
                {
                    "sent": "Very good associated guarantees, but which are impractical for for one reason or the other.",
                    "label": 0
                },
                {
                    "sent": "For instance, maybe they take more than quadratic time, and those are also, you know.",
                    "label": 0
                },
                {
                    "sent": "I'm also leaving those out.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll start by talking about approximation algorithm for clustering.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the setting, OK, just to be concrete.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we have vector data.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have a data set that lies in some Euclidean space Rd, and let's start by looking at flat clustering.",
                    "label": 0
                },
                {
                    "sent": "So this is a situation where you're told how many clusters you want.",
                    "label": 0
                },
                {
                    "sent": "OK, so you want K clusters.",
                    "label": 0
                },
                {
                    "sent": "And now the output is going to be some bunch of centers, some bunch of representatives for the data, and we want to minimize the amount by which the data has been distorted.",
                    "label": 0
                },
                {
                    "sent": "Want to minimize the overall distortion OK?",
                    "label": 1
                },
                {
                    "sent": "Now in this setting, the distortion on a particular point is pretty easy to characterize.",
                    "label": 1
                },
                {
                    "sent": "It's simply the distance from the point to its nearest representative.",
                    "label": 0
                },
                {
                    "sent": "OK, so over here X is a particular data point, and C is the set of representatives.",
                    "label": 1
                },
                {
                    "sent": "The set of centers.",
                    "label": 0
                },
                {
                    "sent": "The distortion on X is simply the distance from X to those centers.",
                    "label": 0
                },
                {
                    "sent": "The distance from X to its closest representative.",
                    "label": 0
                },
                {
                    "sent": "Oh, the part where variability comes in.",
                    "label": 0
                },
                {
                    "sent": "Is fair is when you decide how to characterize the overall distortion, so we have the distortion on a single point and there are two very popular ways to characterize the overall distortion.",
                    "label": 0
                },
                {
                    "sent": "One of them is the maximum distortion.",
                    "label": 0
                },
                {
                    "sent": "The distortion induced on the very worst point, and this is called the case center cost function.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance in this data set over here, let's say that the blue points are the X, the data points and the red points are the representatives that we've chosen.",
                    "label": 0
                },
                {
                    "sent": "So in this case the blue points with the data set we ask we will ask for three clusters and we ended up choosing these three centers.",
                    "label": 0
                },
                {
                    "sent": "The distortion is that of the worst point, namely this distance over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the K center cost function, this is the distortion.",
                    "label": 0
                },
                {
                    "sent": "So it's a worst case type of distortion.",
                    "label": 0
                },
                {
                    "sent": "And this is this turns out to be very useful because it's it's tantamount to coming up with an epsilon cover of a data set or covering the data set by a bunch of balls of fixed radius, which is a which is very useful basic primitive.",
                    "label": 0
                },
                {
                    "sent": "The second measure of distortion is.",
                    "label": 0
                },
                {
                    "sent": "Is instead of focusing on the very worst point to look at the typical distortion, and this is something that would be more important in something like a coding application.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't care about what the worst were.",
                    "label": 0
                },
                {
                    "sent": "The very worst point looks like, but on average, how much do you lose and so you can look at it as an average or just a sum, just the sum of all these distances.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that for various reasons, basically statistical reasons.",
                    "label": 0
                },
                {
                    "sent": "It's convenient to put a square over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so instead of looking at the average distance to look at the average distance squared and this is nice for many reasons.",
                    "label": 0
                },
                {
                    "sent": "For instance, that implies that the correct representative for a set of points is simply the mean of those points.",
                    "label": 0
                },
                {
                    "sent": "OK, which is something very natural and nice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are two models we'll be looking at.",
                    "label": 0
                },
                {
                    "sent": "The second one is called K means.",
                    "label": 0
                },
                {
                    "sent": "OK, so any any questions about these?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so yeah, so let's look at a situation where one of the clusters had a whole bunch of points, right?",
                    "label": 0
                },
                {
                    "sent": "And let me just draw that on the right over here.",
                    "label": 0
                },
                {
                    "sent": "So we just have a huge number of points over here.",
                    "label": 0
                },
                {
                    "sent": "Now these points are all going to be summarized by this one representative.",
                    "label": 0
                },
                {
                    "sent": "And so you know, it's an important representative.",
                    "label": 0
                },
                {
                    "sent": "But the average still captures the typical distortion you know.",
                    "label": 0
                },
                {
                    "sent": "So even though the clusters have different numbers of points, the average distance from a point to its closest representative.",
                    "label": 0
                },
                {
                    "sent": "Captures the distortion on a typical point on a point drawn at random, so.",
                    "label": 0
                },
                {
                    "sent": "Just to clarify, the averages per point, the average over points rather than over clusters.",
                    "label": 0
                },
                {
                    "sent": "So it's not that you know this cluster has this radius that clusters that radius Plus says this radius.",
                    "label": 0
                },
                {
                    "sent": "Let's average those three radii, it's the average over points.",
                    "label": 0
                },
                {
                    "sent": "So pick a point at random.",
                    "label": 0
                },
                {
                    "sent": "What is the expected distance to its representative?",
                    "label": 0
                },
                {
                    "sent": "So that's the measure.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so these are the two measures of distortion that we'll be looking at, and there's been a lot of work on both of them, and so I'll I'll just describe some of this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first one will look at is the is the notion of maximum distortion so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This measure over here OK where we want to find a bunch of representatives that are close to every single point.",
                    "label": 0
                },
                {
                    "sent": "OK, we we cannot ignore any any given point.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a very nice algorithm for doing this, which was given in 1985 by Teofilo Gonzalez and it's called furthest first traversal.",
                    "label": 0
                },
                {
                    "sent": "And it's something that's very widely used, and so it's because it's so.",
                    "label": 0
                },
                {
                    "sent": "It's so simple and clean.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's how the algorithm works.",
                    "label": 0
                },
                {
                    "sent": "You have a data set, you're told how many clusters you want.",
                    "label": 0
                },
                {
                    "sent": "You told how many centers you want.",
                    "label": 0
                },
                {
                    "sent": "Here's how you pick the centers.",
                    "label": 0
                },
                {
                    "sent": "Start by picking any of the data points as the very first center.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which one.",
                    "label": 0
                },
                {
                    "sent": "Then at the second center, pick the data point furthest away from it.",
                    "label": 0
                },
                {
                    "sent": "As the third center picked the data point furthest away from those two, and so on, and in this way you pick K centers.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how it would work on this data set.",
                    "label": 0
                },
                {
                    "sent": "So we have these these nine points over here.",
                    "label": 0
                },
                {
                    "sent": "Sorry, 10 points over here for our first center.",
                    "label": 0
                },
                {
                    "sent": "We just pick any of the data points.",
                    "label": 0
                },
                {
                    "sent": "OK, let's say I pick this one.",
                    "label": 0
                },
                {
                    "sent": "At this point, the cost of the clustering.",
                    "label": 0
                },
                {
                    "sent": "If I would stop over here is the maximum possible distortion.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty much the distance to this point.",
                    "label": 0
                },
                {
                    "sent": "That is the maximum distortion.",
                    "label": 0
                },
                {
                    "sent": "That point is the current bottleneck.",
                    "label": 0
                },
                {
                    "sent": "It's the point that currently defines the distortion and therefore it's going to be my natural choice for the second center.",
                    "label": 0
                },
                {
                    "sent": "OK, that's where I put the second one.",
                    "label": 0
                },
                {
                    "sent": "Now suppose we just had these two simple OK, so let's say we want four centers, right?",
                    "label": 0
                },
                {
                    "sent": "So that's my second center.",
                    "label": 0
                },
                {
                    "sent": "Now if I just had these two centers, the maximum distortion would be maybe this point, or maybe that one up there.",
                    "label": 0
                },
                {
                    "sent": "This one is not too bad because it's fairly close to the two.",
                    "label": 0
                },
                {
                    "sent": "This one is pretty close to the one.",
                    "label": 0
                },
                {
                    "sent": "All of these are fine.",
                    "label": 0
                },
                {
                    "sent": "This one is looking pretty bad and the one up there is looking pretty bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Actually let me.",
                    "label": 0
                },
                {
                    "sent": "So it turns out the next one.",
                    "label": 0
                },
                {
                    "sent": "I guess it's going to be the one up there.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the one that's furthest away from the two I have.",
                    "label": 0
                },
                {
                    "sent": "And now the point that's looking worst is this one, and so that's going to be the 4th center.",
                    "label": 0
                },
                {
                    "sent": "And that's it, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's the algorithm, OK, these are the four clusters centers.",
                    "label": 0
                },
                {
                    "sent": "Now at this point, what is the cost or the distortion of the clustering?",
                    "label": 0
                },
                {
                    "sent": "It's simply the worst case distance, which is which probably this one over here.",
                    "label": 0
                },
                {
                    "sent": "So the cost is that particular distance are that's the radius of the clustering.",
                    "label": 0
                },
                {
                    "sent": "So we found four centers and everything is within distance are of those four centers.",
                    "label": 0
                },
                {
                    "sent": "OK, so any questions about the algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an algorithm that's very simple and convenient.",
                    "label": 0
                },
                {
                    "sent": "The question is, what can one possibly say about something like this?",
                    "label": 0
                },
                {
                    "sent": "Is it in what sense is this algorithm good algorithm?",
                    "label": 0
                },
                {
                    "sent": "Is it after all we had a very specific cost function that we wanted to optimize?",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the distortion.",
                    "label": 0
                },
                {
                    "sent": "Are these really the four?",
                    "label": 0
                },
                {
                    "sent": "Is this really the optimal placement of four centers?",
                    "label": 0
                },
                {
                    "sent": "Not necessarily, but it isn't too bad.",
                    "label": 0
                },
                {
                    "sent": "OK, and here's what you can show.",
                    "label": 0
                },
                {
                    "sent": "You can show that the cost of these four centers is at most twice whatever the optimal cost is.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're never worse than a factor two of optimal, and so let me explain why that is the case.",
                    "label": 0
                },
                {
                    "sent": "And here's the here's the proof over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this procedure we end up constructing some set of centers.",
                    "label": 0
                },
                {
                    "sent": "See OK. We speak any point.",
                    "label": 0
                },
                {
                    "sent": "The one furthest away from it.",
                    "label": 0
                },
                {
                    "sent": "The first one furthest away from that and in this way we get K centers and those centers are what I'm calling C over here OK?",
                    "label": 0
                },
                {
                    "sent": "Once you have those centers, which in this example is just the points 1234, let's look at the data point that's furthest away from those centers.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's this point over here.",
                    "label": 0
                },
                {
                    "sent": "It's that data point that defines the cost of the clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is point X over here.",
                    "label": 0
                },
                {
                    "sent": "That data point defines the cost of the clustering.",
                    "label": 0
                },
                {
                    "sent": "The cost of the clustering is simply the distance from that point to the cluster centers.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's R. So the cost of this clustering is R. No, look at the cluster centers and look at this additional point X.",
                    "label": 0
                },
                {
                    "sent": "So instead of K points we now have K plus one points.",
                    "label": 0
                },
                {
                    "sent": "The K cluster centers as well as the additional point X.",
                    "label": 0
                },
                {
                    "sent": "The thing about these points is that they're really all pretty far away from each other.",
                    "label": 0
                },
                {
                    "sent": "They're all a distance, at least are from each other, so you have K plus one points that are all distance are from each other.",
                    "label": 0
                },
                {
                    "sent": "Now whatever the optimal clustering is, it has only K clusters.",
                    "label": 0
                },
                {
                    "sent": "And we have K plus one points that are distance are from each other.",
                    "label": 1
                },
                {
                    "sent": "So two of these points must be in the same optimal cluster.",
                    "label": 1
                },
                {
                    "sent": "So you have some optimal cluster that contains 2 points that are distance are from each other.",
                    "label": 0
                },
                {
                    "sent": "Therefore that optimal cluster must have got a radius of at least R / 2.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the optimal distortion is at least R / 2.",
                    "label": 0
                },
                {
                    "sent": "This time here.",
                    "label": 0
                },
                {
                    "sent": "I could, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure exactly what the what the epsilon sampling thing is.",
                    "label": 0
                },
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "Up to epsilon radius.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is a very basic procedure and it turns out that it's used in a vast array of different contexts.",
                    "label": 0
                },
                {
                    "sent": "So yeah, sometimes one stops when one gets to a certain number of centers.",
                    "label": 0
                },
                {
                    "sent": "Sometimes one stops when one gets to a certain radius, because in that case the goal is to is to cover.",
                    "label": 0
                },
                {
                    "sent": "The point is to have a distortion of at most epsilon, and so that lets you construct an epsilon net of the data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so so any questions so far about this.",
                    "label": 0
                },
                {
                    "sent": "OK so this is very useful procedure and it has this nice guarantee even if it doesn't find the very best placement of the case centers.",
                    "label": 0
                },
                {
                    "sent": "It's quick, it's really easy and it finds some placement of the case centers.",
                    "label": 0
                },
                {
                    "sent": "That is, it is at most of factor 2 away from optimal OK.",
                    "label": 0
                },
                {
                    "sent": "So let me just talk briefly about this factor of two so you know people who are in computer science are very used to approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "You know you have some algorithms that have a factor of 1 plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "You have some sort of constant factor approximation factor of two factor of 10 factor of 20.",
                    "label": 0
                },
                {
                    "sent": "Then you have some algorithms that have a factor like log in.",
                    "label": 0
                },
                {
                    "sent": "What do these factors mean?",
                    "label": 0
                },
                {
                    "sent": "And specifically, in a clustering context, what does it mean that you have a factor 2 approximation?",
                    "label": 0
                },
                {
                    "sent": "Is that something that you should be feeling pleased about, or something that is really not that good?",
                    "label": 0
                },
                {
                    "sent": "OK, how do we interpret a factor 2?",
                    "label": 0
                },
                {
                    "sent": "So let me show you a picture over here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose that the data set looks like these spheres over here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the data set consists of these three blobs.",
                    "label": 0
                },
                {
                    "sent": "It has three very clearly defined clusters in it.",
                    "label": 0
                },
                {
                    "sent": "And let's say your clustering procedure instead of identifying the three clusters, ends up putting one center over here, one center over here and one center up here.",
                    "label": 0
                },
                {
                    "sent": "What is the cost of this?",
                    "label": 0
                },
                {
                    "sent": "How far is it from optimal?",
                    "label": 0
                },
                {
                    "sent": "Will be optimal?",
                    "label": 0
                },
                {
                    "sent": "Radius is something like this OK and so the and the cost of this clustering is something like this.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of a clustering that's probably a factor 5 away from optimal.",
                    "label": 0
                },
                {
                    "sent": "The radius is 5 times as large as it should be, or maybe 10 times as large as it should be.",
                    "label": 0
                },
                {
                    "sent": "So what factor 2 guarantee tells you is that you'll never end up in this situation.",
                    "label": 0
                },
                {
                    "sent": "OK, it prevents major screwups.",
                    "label": 0
                },
                {
                    "sent": "That's that's what it tells you.",
                    "label": 0
                },
                {
                    "sent": "OK, now this might not seem like much.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "You know, it's ridiculous to have a clustering like this, you know, but it turns out that the procedures that are commonly used things like the K means algorithm.",
                    "label": 0
                },
                {
                    "sent": "Very easily fall into local Optima of exactly this type.",
                    "label": 0
                },
                {
                    "sent": "This is a very typical sort of local optimum into which into which one of the sort of popular clustering heuristics commonly folds OK, and it doesn't matter how far away these things are from each other.",
                    "label": 0
                },
                {
                    "sent": "It's one of this is one of the classic types of local Optima.",
                    "label": 0
                },
                {
                    "sent": "More than N squared, I didn't want more than N ^2.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it would be interesting to find ways to speed this up.",
                    "label": 0
                },
                {
                    "sent": "It appears that the dimension plays into it as well, proportional to the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is known in terms of the complexity of this problem?",
                    "label": 0
                },
                {
                    "sent": "So we've seen that we can come up with a factor 2 approximation, but.",
                    "label": 0
                },
                {
                    "sent": "But perhaps we can do better.",
                    "label": 0
                },
                {
                    "sent": "We can so factor 2 is a reasonable sort of guarantee.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to do better?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is that although we've been talking about Euclidean space, it turns out that exactly the same thing works in any metric space.",
                    "label": 1
                },
                {
                    "sent": "The only thing we needed was the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's again in fact it to what about lower bounds?",
                    "label": 0
                },
                {
                    "sent": "Can one come up with a factor better than two?",
                    "label": 0
                },
                {
                    "sent": "So this was considered in the late 80s, and so these two results were shown.",
                    "label": 1
                },
                {
                    "sent": "We've shown that if you're talking about points in a general metric space, you can't do better than a factor of two.",
                    "label": 0
                },
                {
                    "sent": "You can't even do you know a 1.99.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In Euclidean space, what was shown is that you can't do better than a factor of 1.82.",
                    "label": 1
                },
                {
                    "sent": "So this is what's known in terms of lower bounds, so some so a couple of open problems here are first of all, to close the gap in the Euclidean case, in the sense that is the most interesting case, and it would be nice to resolve the gap.",
                    "label": 1
                },
                {
                    "sent": "This is probably not something that's you know particularly important in practice, but it's kind of an annoying hold that that in the period list, that would be nice to be very nice to resolve.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you write, so that's a great question.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what happens if the data set contains outliers, OK?",
                    "label": 0
                },
                {
                    "sent": "And for this particular cost, functions outliers are very important.",
                    "label": 0
                },
                {
                    "sent": "OK, because the goal of this cost function is to be close to all data points, your centers must be close to all the points including outliers.",
                    "label": 0
                },
                {
                    "sent": "So if there's some point, if you have a million points that are really close together and you have one point that you know outer Infinity, you must have a center.",
                    "label": 0
                },
                {
                    "sent": "At that point, there's no two ways about it and this procedure that I talked about Further's first traversal will certainly put one of the centers out there.",
                    "label": 0
                },
                {
                    "sent": "The next sentence I mean.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Are the distances that computed from all the points considered so far right?",
                    "label": 0
                },
                {
                    "sent": "So let me just draw a picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose why data looks like this?",
                    "label": 0
                },
                {
                    "sent": "OK, so we start by picking any old center.",
                    "label": 0
                },
                {
                    "sent": "Let's say we pick one at random, which means it probably won't be this one out here.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say we pick this one, the next one we'll take is going to be the one furthest away from that.",
                    "label": 0
                },
                {
                    "sent": "Which is this?",
                    "label": 0
                },
                {
                    "sent": "At this point, the distances are computed to the closest center.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically everything's distance is computed to this one because point #2 is not close to anything, and so the next one that will be chosen is this.",
                    "label": 0
                },
                {
                    "sent": "Cancel, yeah, so outliers are something that can't be ignored in this function return.",
                    "label": 0
                },
                {
                    "sent": "Approximation.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 1
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I mean, certainly one can if 1 four allowed logarithmically many then it would become a set cover type problem.",
                    "label": 0
                },
                {
                    "sent": "But I don't know about the intermediate.",
                    "label": 0
                },
                {
                    "sent": "But there might be some work on it in by Feder and Greene.",
                    "label": 0
                },
                {
                    "sent": "OK, and the second the second problem is that so this particular procedure really does turn out to be very useful and very widely used.",
                    "label": 0
                },
                {
                    "sent": "This furthers first traversal, but there's certain things about it.",
                    "label": 0
                },
                {
                    "sent": "They definitely seem a little odd.",
                    "label": 0
                },
                {
                    "sent": "For instance, the fact that the very first point you pick can be anything, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So we really interesting if there were other procedures that were also good in practice.",
                    "label": 0
                },
                {
                    "sent": "You know to have something else to do with this problem.",
                    "label": 0
                },
                {
                    "sent": "To do for this prob.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's move onto the K means cost function and just as a reminder, the way this differs from from the current cost function is that.",
                    "label": 0
                },
                {
                    "sent": "When you have a bunch of coins and you choose representatives, the cost is the sum or average of all distances.",
                    "label": 0
                },
                {
                    "sent": "OK, it doesn't matter very much whether we choose the sum or the average, since they differ from each other just by the number of points by the buyer fixed by a fixed ratio.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the K means cost function.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that this is in spirit quite different from from the case enter cause function and the outliers really give some insight into that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if there's one point that's really far away.",
                    "label": 0
                },
                {
                    "sent": "In the case center cost function, you really must pick that point because we want to be close to everything in the K means cost function.",
                    "label": 0
                },
                {
                    "sent": "You can kind of sacrifice that point since it really receives away just of one over, and so it's a point that you're allowed to kind of give up on if everything else is really close together, and so a furthest first traversal will not work.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's just not a good idea since it basically in a case with outliers it will exactly pick out the outliers, but it turns out.",
                    "label": 0
                },
                {
                    "sent": "Did a stochastic version of.",
                    "label": 1
                },
                {
                    "sent": "It works just fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the version.",
                    "label": 0
                },
                {
                    "sent": "It's very close to a further first reversal.",
                    "label": 0
                },
                {
                    "sent": "What you do first is that the first center you pick is a random datapoint.",
                    "label": 0
                },
                {
                    "sent": "Now, in the previous case, it was an arbitrary data point.",
                    "label": 0
                },
                {
                    "sent": "There's a big difference between the two.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 1
                },
                {
                    "sent": "It's important that you pick one of them at random, OK?",
                    "label": 0
                },
                {
                    "sent": "Once you do that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's say you have these data points.",
                    "label": 0
                },
                {
                    "sent": "OK, you pick one of them at random.",
                    "label": 0
                },
                {
                    "sent": "Let's say you pick this one.",
                    "label": 0
                },
                {
                    "sent": "That's your first center.",
                    "label": 0
                },
                {
                    "sent": "After this, instead of picking the point that's furthest away from it.",
                    "label": 0
                },
                {
                    "sent": "You again pick the second center at random, but weighted by the squared distance from this one.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the choice of 2nd point you could easily pick this one, but that's a low probability event.",
                    "label": 0
                },
                {
                    "sent": "You're more likely to pick something that's further away, something like this, or like this.",
                    "label": 0
                },
                {
                    "sent": "So like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say just for the sake of argument that the second one we pick is this one.",
                    "label": 0
                },
                {
                    "sent": "The third center we pick is again going to be chosen randomly.",
                    "label": 0
                },
                {
                    "sent": "But weighted by disk by distances from the first 2 centers.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance, this has this point has got a low probability of being chosen.",
                    "label": 0
                },
                {
                    "sent": "Whereas this point out here has a pretty high probability of being chosen.",
                    "label": 0
                },
                {
                    "sent": "OK, and so let's say this is the one we actually pick.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a stochastic version of the furthest first reversal.",
                    "label": 0
                },
                {
                    "sent": "This is what's needed to address the slightly the actually rather very very different cost function, and now what can you?",
                    "label": 0
                },
                {
                    "sent": "What can you prove for this?",
                    "label": 0
                },
                {
                    "sent": "Well, this is a randomized procedure, so you run it 3 different times, you're going to get a different answer in each case, so you can talk, however, about the expected cost of the answer and the cost.",
                    "label": 0
                },
                {
                    "sent": "Now is the K means cost the average distortion?",
                    "label": 0
                },
                {
                    "sent": "Rather than the worst distortion and what you can show is that this expected distort this expected cost is at most log K times optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, I've said order log K because there's a constant in front of it.",
                    "label": 0
                },
                {
                    "sent": "The constant is not terribly large.",
                    "label": 0
                },
                {
                    "sent": "It's single digits, but it's there.",
                    "label": 0
                },
                {
                    "sent": "It would, but you can think of it in this way.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a million data points and one of them is an outlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so now it picks the first point at random.",
                    "label": 0
                },
                {
                    "sent": "The outlier definitely has a much higher chance of being picked than any of the other 1,000,000 -- 2 points, but there's 1,000,000 -- 2 of them.",
                    "label": 0
                },
                {
                    "sent": "All with a relatively small probability, but because there's so many of them, there's a much higher chance that you'll end up with one of those as your second point.",
                    "label": 0
                },
                {
                    "sent": "OK anymore questions about this.",
                    "label": 0
                },
                {
                    "sent": "OK so this is.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice procedure.",
                    "label": 0
                },
                {
                    "sent": "An actually it turns out that a lot of a lot of sort of empirical testing has been done of this and it really works quite well OK. Now, so the disturbing thing over here, however, is that the factor depends on K. OK, the the the quality of the approximation depends on K and it's not just because it's a loose bound, it really does depend on K. It depends logarithmically on K and so one might ask can you do a constant factor?",
                    "label": 0
                },
                {
                    "sent": "Can you come up with a factor that doesn't depend on on K?",
                    "label": 0
                },
                {
                    "sent": "Because sometimes we want K to be large.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the answer is yes, and I'll give you an example of such a procedure.",
                    "label": 0
                },
                {
                    "sent": "This is the local search algorithm which was developed in 2003 by.",
                    "label": 0
                },
                {
                    "sent": "By Kanungo and David Mount and Angela.",
                    "label": 0
                },
                {
                    "sent": "You and some other people.",
                    "label": 0
                },
                {
                    "sent": "And as a local search procedure, what it does is.",
                    "label": 0
                },
                {
                    "sent": "You know, let's say you want K centers.",
                    "label": 0
                },
                {
                    "sent": "It ends up putting the case centers wherever and then repeatedly trying to tweak them to see if there's.",
                    "label": 0
                },
                {
                    "sent": "If there's some simple way to update those centers, which improves the cost.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "We start with any way to choose the centers.",
                    "label": 0
                },
                {
                    "sent": "So let's say we want three centers.",
                    "label": 0
                },
                {
                    "sent": "And let's say we pick one here.",
                    "label": 0
                },
                {
                    "sent": "You know what?",
                    "label": 0
                },
                {
                    "sent": "Let's say we pick one here, one here and one here.",
                    "label": 0
                },
                {
                    "sent": "Let's say those are our three centers.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how you.",
                    "label": 0
                },
                {
                    "sent": "That's how you start off.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how you start off.",
                    "label": 0
                },
                {
                    "sent": "What you do next is to try and swap one of the existing centers with one of the data points and see if that helps things.",
                    "label": 0
                },
                {
                    "sent": "OK, so you say, well, let me just pick any of these centers like let's say I pick this one, would it help to swap it with one of the data points?",
                    "label": 0
                },
                {
                    "sent": "Like say this one OK and you try all possibilities, every current center, every data point you try the swap, and if there's any swap that improves things then you do this one.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, let's say you decide to swap this with this one because it helps.",
                    "label": 0
                },
                {
                    "sent": "It really does help.",
                    "label": 0
                },
                {
                    "sent": "And so that's what you do.",
                    "label": 0
                },
                {
                    "sent": "Now you see, well, is there any other swap I could do?",
                    "label": 0
                },
                {
                    "sent": "Well maybe it would help to swap this with that.",
                    "label": 0
                },
                {
                    "sent": "Then you do that and so on until you can no longer do until no swap helps OK?",
                    "label": 0
                },
                {
                    "sent": "And in this case you can show that the cost is at most 50 times the optimal cost.",
                    "label": 0
                },
                {
                    "sent": "OK so I just put this up as an example of a local search procedure and also as an example of a constant factor approximation.",
                    "label": 0
                },
                {
                    "sent": "This is not something that's a terribly efficient algorithm, but but it's one of these really cool cases where you can actually prove something about a Hill climbing or local search procedure, which is which is in general very difficult to do OK.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that you know the fact seems cosmetically a little bit better 'cause it's a constant rather than log K, But you know log K can be is is actually less than 50 on this key is very large, so KK would have to be more than two to the 50.",
                    "label": 0
                },
                {
                    "sent": "OK. Intuition about money.",
                    "label": 0
                },
                {
                    "sent": "It could be 49, it could be 49.",
                    "label": 0
                },
                {
                    "sent": "Basically, in these analysis, in this particular analysis, there wasn't.",
                    "label": 0
                },
                {
                    "sent": "There wasn't a huge effort made to get the constant down.",
                    "label": 0
                },
                {
                    "sent": "The goal was mostly to get a constant factor approximation.",
                    "label": 0
                },
                {
                    "sent": "I don't think that you know what ends up happening is that one keeps using the triangle inequality over and over again, and you know and so these factors of 2.",
                    "label": 0
                },
                {
                    "sent": "Tend to multiply, but it may well be that you know for the same algorithm one can prove a better factor.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the so the lower bound is something I believe it's something like 10.",
                    "label": 0
                },
                {
                    "sent": "Selfie.",
                    "label": 0
                },
                {
                    "sent": "The optimal cost is.",
                    "label": 0
                },
                {
                    "sent": "Is the average distortion or induced by the optimal placement of centers?",
                    "label": 0
                },
                {
                    "sent": "So in this case, so for instance, in this particular procedure always returns centers that happen taliah data points, but that's clearly suboptimal.",
                    "label": 0
                },
                {
                    "sent": "In Euclidean space, the centers are never going to be on data points.",
                    "label": 0
                },
                {
                    "sent": "Probably the optimal centers would be something like this.",
                    "label": 0
                },
                {
                    "sent": "A point over here a point over here and.",
                    "label": 0
                },
                {
                    "sent": "And maybe a point over here or something like that.",
                    "label": 0
                },
                {
                    "sent": "The cost, the cost of these will be the sum of distances to these centers, and so the optimal cost is is the minimum cost over overall placements of of sentence.",
                    "label": 0
                },
                {
                    "sent": "So the optimal cost is some refers.",
                    "label": 0
                },
                {
                    "sent": "You know it is a minimum taken over positionings of centers that don't have to coincide with data points that could be anywhere in space.",
                    "label": 0
                },
                {
                    "sent": "OK, and that immediately induces a factor to sort of, since we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "Since our algorithm is only putting centers at data points, it immediately loses a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "Because of that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so again, you know we can ask whether it's possible to do better.",
                    "label": 0
                },
                {
                    "sent": "What what is known in terms of?",
                    "label": 0
                },
                {
                    "sent": "You know we have a factor 50 approximation.",
                    "label": 0
                },
                {
                    "sent": "We have a factor log K approximation.",
                    "label": 0
                },
                {
                    "sent": "Why can't we just solve this problem exactly?",
                    "label": 0
                },
                {
                    "sent": "Why do we even need an approximation at all?",
                    "label": 0
                },
                {
                    "sent": "What's known in terms of upper and lower bounds?",
                    "label": 0
                },
                {
                    "sent": "OK, so can we solve K means exactly?",
                    "label": 0
                },
                {
                    "sent": "So one thing that's known is that you can solve it exactly if you have time end to the KD, but that's that's the astronomical.",
                    "label": 0
                },
                {
                    "sent": "You know, unless the dimension is 1 and K is like two.",
                    "label": 0
                },
                {
                    "sent": "OK, so in most cases that's astronomical and this is an exhaustive procedure that basically tries all possible clusterings.",
                    "label": 0
                },
                {
                    "sent": "What about lower bounds?",
                    "label": 0
                },
                {
                    "sent": "So the following lower bounds are known for the case where.",
                    "label": 1
                },
                {
                    "sent": "So K means is known to be NP hard when even if you have just K = 2.",
                    "label": 0
                },
                {
                    "sent": "Even if you're looking for just two centers, it's known to be NP hard.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And you know, one way to think about this is that just recorded the centers don't have to be a data points.",
                    "label": 0
                },
                {
                    "sent": "OK, if you were looking for just two centers and they were a data points, well then you just have N squared possibilities to try out.",
                    "label": 0
                },
                {
                    "sent": "But the centers could be anywhere in space and it turns out that therefore, even for the case where you're looking for just two centers, if the dimension is allowed to be arbitrarily high, this problem is NP hard.",
                    "label": 0
                },
                {
                    "sent": "Likewise, the problem is also NP hard if the dimension is 2, it's NP hard even in the plane.",
                    "label": 0
                },
                {
                    "sent": "So some open problems here are to come up with some good hardness of approximation result and to come up with better approximation algorithms.",
                    "label": 1
                },
                {
                    "sent": "OK, so any questions about this stuff?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Just pick.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "So, so I'll tell you the way that in which this would typically end up happening.",
                    "label": 0
                },
                {
                    "sent": "So clearly in real life one definitely wants to to pick centers that are not data points.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This particular algorithm over here gives you something that's fairly good.",
                    "label": 0
                },
                {
                    "sent": "It's a log K approximation and this can be then used to initialize something like the K means algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so K means is a cost function, but there's also an algorithm by the same name.",
                    "label": 0
                },
                {
                    "sent": "Which is a very popular, perhaps the most popular clustering algorithm out there.",
                    "label": 0
                },
                {
                    "sent": "But it's a local search procedure and it's not necessarily.",
                    "label": 0
                },
                {
                    "sent": "You know, it doesn't necessarily give a very good answer, but if you initialize it well, it is guaranteed to never be worse than what you initialized it with.",
                    "label": 0
                },
                {
                    "sent": "So you can be certain that it's never going to be worse than a log key approximation, and it will round things out nicely.",
                    "label": 0
                },
                {
                    "sent": "You know it will.",
                    "label": 0
                },
                {
                    "sent": "It will end up with centers that are not data points and so that would be, you know if one has to sort of.",
                    "label": 0
                },
                {
                    "sent": "If you know, come up with, you know one reasonable way of solving this problem that will be it to initialize like this and then to solve.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are other ways to do it as well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, so so far we've talked about flat clustering.",
                    "label": 0
                },
                {
                    "sent": "So now let's move on to hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "So what are these things?",
                    "label": 0
                },
                {
                    "sent": "Hierarchical clustering is just a recursive partitioning of a data set.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have five data points.",
                    "label": 0
                },
                {
                    "sent": "You start with everything in one big cluster.",
                    "label": 0
                },
                {
                    "sent": "At the top of the tree.",
                    "label": 1
                },
                {
                    "sent": "And then you split it into two.",
                    "label": 0
                },
                {
                    "sent": "OK, so you branch out into two clusters and then you split one of those and then you split another one and then you split another one.",
                    "label": 0
                },
                {
                    "sent": "And often these things are drawn as a dendrogram.",
                    "label": 0
                },
                {
                    "sent": "The diagram on the right.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the top you have a one clustering.",
                    "label": 0
                },
                {
                    "sent": "And then if you cut the dendrogram at any point, like let's say you cut it here, you get 2 branches, so you get one cluster containing the points 123 and another cluster containing the points 45.",
                    "label": 0
                },
                {
                    "sent": "If you cut it over here, you get 3 clusters.",
                    "label": 0
                },
                {
                    "sent": "This one, this one and this one.",
                    "label": 0
                },
                {
                    "sent": "If you cut it here, you get 4 clusters, and if you cut it here, you get 5 clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so a dendrogram is a very convenient representation of these sort of clusterings depending on where you cut it, you get a K clustering for any K. OK, so this is a hierarchical clustering and these things are really very popular.",
                    "label": 0
                },
                {
                    "sent": "And for at least three reasons, the first is that you don't need to specify a number of clusters in many situations.",
                    "label": 1
                },
                {
                    "sent": "You know you're doing some sort of exploratory analysis of the data set.",
                    "label": 0
                },
                {
                    "sent": "You have a new data set that you've been given.",
                    "label": 0
                },
                {
                    "sent": "It's in high dimension.",
                    "label": 0
                },
                {
                    "sent": "You can't visualize it.",
                    "label": 0
                },
                {
                    "sent": "You don't know the first thing about it and you want to get some sense of whether it contains any groups that are significant or that you should be aware of.",
                    "label": 0
                },
                {
                    "sent": "You have no idea what number of clusters to pick, and so hierarchical clustering is something that's reasonable to do.",
                    "label": 0
                },
                {
                    "sent": "You don't have to specify how many clusters you want.",
                    "label": 0
                },
                {
                    "sent": "It might also be the case that the data has different clusterings at different levels of granularity, and so it's nice to have a single picture that captures all of those structures.",
                    "label": 0
                },
                {
                    "sent": "You know it might be at a certain level of granularity at at a certain scale, there is a very nice clustering, but then at Louis scale there's another nice clustering, and so on.",
                    "label": 0
                },
                {
                    "sent": "And you want to be able to capture all of these in a single picture.",
                    "label": 0
                },
                {
                    "sent": "Finally, there another reason why this is popular is that it turns out there are some really simple greedy.",
                    "label": 0
                },
                {
                    "sent": "Turistic for constructing these things that are in very wide use in our part of you know all statistical software packages and so it's very easy to use.",
                    "label": 0
                },
                {
                    "sent": "This is a very popular kind of enterprise.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What can we do to what can we say about these things?",
                    "label": 0
                },
                {
                    "sent": "You know, you give it a data set.",
                    "label": 0
                },
                {
                    "sent": "It spits out a tree.",
                    "label": 0
                },
                {
                    "sent": "What can you say about the tree?",
                    "label": 0
                },
                {
                    "sent": "In what sense is this tree good?",
                    "label": 0
                },
                {
                    "sent": "What sort of performance guarantee can one possibly give for hierarchical clustering?",
                    "label": 0
                },
                {
                    "sent": "And once more, I'll talk about this in the quantization setting, where we have a sort of a cost function in mind, we want to minimize distortion.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before getting into details, I just want to point out a basic difficulty with hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "Or there's a there's a certain sort of basic existence problem, and this is the following.",
                    "label": 1
                },
                {
                    "sent": "OK, so look at this data set over here.",
                    "label": 0
                },
                {
                    "sent": "It's got 12 points.",
                    "label": 0
                },
                {
                    "sent": "Suppose I ask you give Me 2 clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, by any of the measures we've been seeing so far, this sort of radius based measures.",
                    "label": 0
                },
                {
                    "sent": "This is the two clusters you cook up.",
                    "label": 0
                },
                {
                    "sent": "OK, suppose I ask you give me 3 clusters.",
                    "label": 0
                },
                {
                    "sent": "These are the three clusters you would cook up, so this points to a basic difficulty in hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "The two clustering, the optimal two clustering and the optimal three clustering are not hierarchically compatible with each other.",
                    "label": 0
                },
                {
                    "sent": "One of them is not nested inside the other.",
                    "label": 0
                },
                {
                    "sent": "And this seems like bad news because we want to come up with a tree that contains K clustering for all K is going to have to give us a two clustering.",
                    "label": 0
                },
                {
                    "sent": "It's going to have to give us a three clustering and therefore it has to compromise on at least one of these.",
                    "label": 0
                },
                {
                    "sent": "So the main sort of one of the first questions one would want to ask is how bad is this tradeoff?",
                    "label": 0
                },
                {
                    "sent": "OK, the suspicion is it the case that.",
                    "label": 0
                },
                {
                    "sent": "Dead by imposing a hierarchical structure, we are just dooming ourselves to intermediate clusterings of very low quality is.",
                    "label": 0
                },
                {
                    "sent": "That is that a possibility, because that would be very bad news.",
                    "label": 0
                },
                {
                    "sent": "OK, and so and so this is the question that we look at.",
                    "label": 0
                },
                {
                    "sent": "We've already seen that in general, there is no way to construct a hierarchical clustering that is optimal at every level.",
                    "label": 0
                },
                {
                    "sent": "This is nothing to do with NP hardness or anything like that.",
                    "label": 0
                },
                {
                    "sent": "It's simply a structural issue.",
                    "label": 0
                },
                {
                    "sent": "There simply does not exist a hierarchical clustering that is going to be optimal at every level, so we have to talk about approximate approximate optimality.",
                    "label": 1
                },
                {
                    "sent": "How much do you lose by enforcing a hierarchical structure on top of these intermediate clusterings?",
                    "label": 0
                },
                {
                    "sent": "OK, is it really bad?",
                    "label": 0
                },
                {
                    "sent": "So we'll start by looking at the case center cost function.",
                    "label": 0
                },
                {
                    "sent": "OK, here we want a tree where for each K if you cut the hierarchy at that point and you look at the K clusters you get it's not too far from the optimal K clustering from the optimal K center solution.",
                    "label": 0
                },
                {
                    "sent": "We've seen that in the case of flat case center you can get a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "In the hierarchical case, it's not going to be that good, and what we can do is a factor eight.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me show you how to do that.",
                    "label": 0
                },
                {
                    "sent": "Actually turns out there many ways to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This, but I'll just give you a particular example of a way to do it.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to actually do hierarchical case center.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this this particular solution to hierarchical clustering uses something called a cover tree, and this is an idea that's really been around for awhile, but.",
                    "label": 0
                },
                {
                    "sent": "But recently you know some of the people have looked at incarnations of IT Crowd Gamer and Lee.",
                    "label": 0
                },
                {
                    "sent": "Bagels I'm Earl Langford and kakade.",
                    "label": 0
                },
                {
                    "sent": "A couple trees, a spatial data structure and it works on data points in any metric space.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you what these things are, because it's a pretty cool data structure, but it takes a little while to just kind of get used to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say you have a bunch of data points like.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have these five points, we're going to organize them into a tree with the following properties.",
                    "label": 0
                },
                {
                    "sent": "The tree is divided into levels into distinct levels, and each level has got a number associated with it.",
                    "label": 0
                },
                {
                    "sent": "The numbers are consecutive.",
                    "label": 0
                },
                {
                    "sent": "But the top level is not necessarily #0.",
                    "label": 0
                },
                {
                    "sent": "The top level could be #3.",
                    "label": 0
                },
                {
                    "sent": "If it is, if the top level is number 3, the next one is 4 and the next one is 5 in the next one is 6.",
                    "label": 0
                },
                {
                    "sent": "The top level could be number minus 90, in which case the next one would be minus 89 -- 88 and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so the numbers these level numbers go from minus Infinity to plus Infinity, but they're consecutive.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the properties that the tree has.",
                    "label": 0
                },
                {
                    "sent": "Suppose each node of the tree is associated with the data point.",
                    "label": 1
                },
                {
                    "sent": "For instance, this node is associated with Point X1.",
                    "label": 1
                },
                {
                    "sent": "If a node is associated with the point one of its children must also be associated with that point.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So if you have an X one here, one of its children must be X one and one of these things.",
                    "label": 0
                },
                {
                    "sent": "Children must be X one and one of these things.",
                    "label": 0
                },
                {
                    "sent": "Children must be X1 and in this sense it's an infinite tree because you can imagine at this point, although we've captured all five data points, you can imagine these just along just these long straight paths going down to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the overall structure of the tree.",
                    "label": 0
                },
                {
                    "sent": "The tree now has got a certain property.",
                    "label": 0
                },
                {
                    "sent": "Any node.",
                    "label": 0
                },
                {
                    "sent": "Is within a predictable distance from its parent.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have a node that's at level J + 1, its distance from its parent is at most 1 / 2 over to the J.",
                    "label": 1
                },
                {
                    "sent": "So for instance, this distance from X3 to X4.",
                    "label": 0
                },
                {
                    "sent": "Is at most one.",
                    "label": 0
                },
                {
                    "sent": "This distance from X1 to X5 is at most two.",
                    "label": 0
                },
                {
                    "sent": "This distance is at most 1/2.",
                    "label": 0
                },
                {
                    "sent": "So that's one guarantee that you'll give it as you go down the tree.",
                    "label": 0
                },
                {
                    "sent": "The lengths of these distances are bounded.",
                    "label": 0
                },
                {
                    "sent": "The other thing you're told?",
                    "label": 0
                },
                {
                    "sent": "He said within a level.",
                    "label": 1
                },
                {
                    "sent": "Within level J, distances are at least this nodes, or at least distance 1 / 2 to the J from each other.",
                    "label": 0
                },
                {
                    "sent": "So for instance, these points.",
                    "label": 0
                },
                {
                    "sent": "The distance between X1 and X2 because it's at level 0, is at least one.",
                    "label": 0
                },
                {
                    "sent": "And the distance between X. X5 and X4 is at least 1/2.",
                    "label": 0
                },
                {
                    "sent": "And this distance is.",
                    "label": 0
                },
                {
                    "sent": "At least 1/4.",
                    "label": 0
                },
                {
                    "sent": "OK, so you basically get a bunch of data points and you build a tree on them which has this property.",
                    "label": 0
                },
                {
                    "sent": "It's somehow.",
                    "label": 0
                },
                {
                    "sent": "Infinite tree, but you don't need to keep all of it around you.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's a way to just store it so that you keep only one copy of each node so that the storage is only order in.",
                    "label": 0
                },
                {
                    "sent": "But it's a useful data structure because it somehow captures the geometry of the points.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing at this point it's not even clear how you would build something like this, or why.",
                    "label": 0
                },
                {
                    "sent": "Can you always make a data structure with these properties?",
                    "label": 0
                },
                {
                    "sent": "So that's one question.",
                    "label": 0
                },
                {
                    "sent": "In the second is why is it useful to even organize points in this way?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh no, it's not unique.",
                    "label": 0
                },
                {
                    "sent": "There are many ways to.",
                    "label": 0
                },
                {
                    "sent": "There are many.",
                    "label": 0
                },
                {
                    "sent": "In general there will be many different trees on the same set of points that have that have these properties.",
                    "label": 0
                },
                {
                    "sent": "So the tree is not something that is optimized, it's simply something that happens to satisfy a few properties, and in general the many trees that will.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's only conceptually infinite.",
                    "label": 0
                },
                {
                    "sent": "In this case, you never need more than N levels and yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, how would you even build something like this?",
                    "label": 0
                },
                {
                    "sent": "That's actually quite simple.",
                    "label": 0
                },
                {
                    "sent": "Here's what you do.",
                    "label": 0
                },
                {
                    "sent": "You can build it online.",
                    "label": 0
                },
                {
                    "sent": "Each time you get a new data point X OK, so you can imagine the points arriving in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "First X1 arrives, then X2 arrives.",
                    "label": 1
                },
                {
                    "sent": "The next 3 arrives and each point in each time a new data point arrives, you simply add it to the tree.",
                    "label": 0
                },
                {
                    "sent": "How do you add it to the tree?",
                    "label": 0
                },
                {
                    "sent": "Well, you look at the largest level such that the point is within 1 / 2 to the J of some node at that level, and then make it a child there.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's just do an example 'cause that's a little confusing.",
                    "label": 0
                },
                {
                    "sent": "Let's say that.",
                    "label": 0
                },
                {
                    "sent": "We already have a tree on these five points, and now let's say that a new point shows up X6.",
                    "label": 0
                },
                {
                    "sent": "OK, so where is XI?",
                    "label": 0
                },
                {
                    "sent": "Don't know we could put it over here, say.",
                    "label": 0
                },
                {
                    "sent": "OK. And now we want to know how to incorporate X6 into the tree.",
                    "label": 0
                },
                {
                    "sent": "Well, we go to this level.",
                    "label": 0
                },
                {
                    "sent": "Does it lie within distance two of X1?",
                    "label": 0
                },
                {
                    "sent": "Yes it does.",
                    "label": 0
                },
                {
                    "sent": "Now we go down to this level.",
                    "label": 0
                },
                {
                    "sent": "Does it lie within distance?",
                    "label": 0
                },
                {
                    "sent": "One of one of these nodes?",
                    "label": 0
                },
                {
                    "sent": "Yes it does.",
                    "label": 0
                },
                {
                    "sent": "It looks like it lies within distance one of X2.",
                    "label": 1
                },
                {
                    "sent": "Then we go down to this level.",
                    "label": 0
                },
                {
                    "sent": "Does it lie within distance 1/2 of one of these nodes?",
                    "label": 0
                },
                {
                    "sent": "No, it's more than distance 1/2 from all of those.",
                    "label": 0
                },
                {
                    "sent": "We go down to this level.",
                    "label": 0
                },
                {
                    "sent": "Does it lie within distance 12:45 of these nodes?",
                    "label": 0
                },
                {
                    "sent": "No, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So this was the last level at which it lay within the requisite distance, and so we make it a child of that.",
                    "label": 0
                },
                {
                    "sent": "OK. And then of course this continues on down to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So that will become apparent very soon.",
                    "label": 0
                },
                {
                    "sent": "The factor of two has actually been.",
                    "label": 0
                },
                {
                    "sent": "The maximum distance between 2 points and then clusters.",
                    "label": 0
                },
                {
                    "sent": "So at the mentality increases made so sorry in this specific example that happens to hold, but the tree itself does not tell us that.",
                    "label": 0
                },
                {
                    "sent": "The tree only tells us that the maximum distance between any two points is 4 because everything is with the distance from X, one is at most 2 + 1 + 1/2 and so on since most full.",
                    "label": 0
                },
                {
                    "sent": "OK, so kind of a weird data structure, but it is something that's easy to build, yeah?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Well, what happens is the first point.",
                    "label": 0
                },
                {
                    "sent": "Can actually go anywhere, but when you start with two points then it tells you where these things go.",
                    "label": 0
                },
                {
                    "sent": "Yoko.",
                    "label": 0
                },
                {
                    "sent": "Far away.",
                    "label": 0
                },
                {
                    "sent": "Then it goes above.",
                    "label": 0
                },
                {
                    "sent": "OK, so the mysterious thing over here is the factor of two, and it doesn't have to be exactly a factor of two, but let me.",
                    "label": 0
                },
                {
                    "sent": "It could be some other factor, but let me show you how this plays.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into a hierarchical clustering guarantee or or why this addresses the hierarchical clustering problem?",
                    "label": 0
                },
                {
                    "sent": "OK, so you have N data points.",
                    "label": 0
                },
                {
                    "sent": "You build a cover tree on them and now suppose you want suppose you want to K clustering.",
                    "label": 1
                },
                {
                    "sent": "You want to partition into K clusters.",
                    "label": 0
                },
                {
                    "sent": "Here's what you do.",
                    "label": 0
                },
                {
                    "sent": "You go to the lowest level that contains.",
                    "label": 0
                },
                {
                    "sent": "At most two points, and so you would return this single center.",
                    "label": 0
                },
                {
                    "sent": "Let's say you want to.",
                    "label": 0
                },
                {
                    "sent": "For clustering.",
                    "label": 0
                },
                {
                    "sent": "You return this and so on.",
                    "label": 1
                },
                {
                    "sent": "OK, so when you want to K clustering?",
                    "label": 0
                },
                {
                    "sent": "You return an entire level, and you simply return the lowest level that is legal.",
                    "label": 0
                },
                {
                    "sent": "The lowest level that has got at most K points.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this is at most a factor 8 of optimal.",
                    "label": 0
                },
                {
                    "sent": "So let's quickly see why this is the case.",
                    "label": 0
                },
                {
                    "sent": "It's basically because of a.",
                    "label": 0
                },
                {
                    "sent": "So let's just go through the argument over here, OK?",
                    "label": 0
                },
                {
                    "sent": "So, so we've asked for a K clustering.",
                    "label": 0
                },
                {
                    "sent": "And what did we do?",
                    "label": 0
                },
                {
                    "sent": "We just looked at the lowest level that was valid, the lowest level with at most K nodes.",
                    "label": 0
                },
                {
                    "sent": "The cake was to just return the root because.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because that's within a factor eight of the best two centers.",
                    "label": 0
                },
                {
                    "sent": "OK, so you just returned the lowest level that that you can.",
                    "label": 0
                },
                {
                    "sent": "So, so here's the argument.",
                    "label": 0
                },
                {
                    "sent": "Suppose you end up returning level J.",
                    "label": 0
                },
                {
                    "sent": "In other words, J is the lowest level that contains at most K points.",
                    "label": 0
                },
                {
                    "sent": "Why is that within a factor rate of optimal?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "What we do know is that if it's level J, all of its children are within distance 1, / 2 to the J of it.",
                    "label": 1
                },
                {
                    "sent": "OK, so if it's this level, all of its children are distance one from it.",
                    "label": 0
                },
                {
                    "sent": "OK, so all of its children are distance 1 / 2 to the J from it.",
                    "label": 0
                },
                {
                    "sent": "All of its grandchildren are an additional distance of at most 1 / 2 to the J plus one and all of its great grandchildren are an additional distance of 1 / 2 to the J + 2 and it's going down geometrically.",
                    "label": 0
                },
                {
                    "sent": "So geometric series and so the distance, the cost of the clustering is at most 1 / 2 to the J -- 1 here.",
                    "label": 0
                },
                {
                    "sent": "Just adding up this geometric series.",
                    "label": 0
                },
                {
                    "sent": "OK, so each point is within distance at most 1 / 2 to the J + 1 / 2 to the J + 1 + 1 over to the J + 2 + 1 over to the J + 3.",
                    "label": 0
                },
                {
                    "sent": "So we just using the triangle inequality over here OK, and so the everything the cost of the clustering.",
                    "label": 0
                },
                {
                    "sent": "The case centered distortion is at most this much OK, so that's the actual cost of the clustering we return.",
                    "label": 0
                },
                {
                    "sent": "But in order to show that it's a factor 8 of optimal, we have to show that any clustering.",
                    "label": 0
                },
                {
                    "sent": "Is going to have, at least is going to have a cost that is at least 1/8 of that OK?",
                    "label": 0
                },
                {
                    "sent": "And why is that the case?",
                    "label": 0
                },
                {
                    "sent": "The reason is that.",
                    "label": 0
                },
                {
                    "sent": "So we returned level J. J was the lowest level with at most K nodes.",
                    "label": 1
                },
                {
                    "sent": "That means if you go down one more level you got K plus one nodes.",
                    "label": 0
                },
                {
                    "sent": "At least.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you go down to level J + 1, you have at least K plus one nodes.",
                    "label": 1
                },
                {
                    "sent": "Moreover, by construction, those nodes are pretty far away from each other because they are in a cover tree.",
                    "label": 0
                },
                {
                    "sent": "We know there at least distance 1 / 2 to the J plus one from each other and therefore we have at least K plus one points which are at least distance 1 / 2 to the J plus one from each other.",
                    "label": 0
                },
                {
                    "sent": "And again we use the same kind of argument is with K center.",
                    "label": 0
                },
                {
                    "sent": "If you have K plus one points that are at least this distance away from each other.",
                    "label": 0
                },
                {
                    "sent": "Any optimal clustering must have got two of those points in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "By pigeonhole principle.",
                    "label": 0
                },
                {
                    "sent": "'cause you got K plus one point so far away from each other, no matter how you carve them up into clusters, two of those points are going to be in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "OK, and therefore the radius of the cluster is going to be at least half the distance between those two points.",
                    "label": 0
                },
                {
                    "sent": "OK, and so that's how we get the factor of eight over there.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this is how we do a hierarchical case antenna.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Just do you know they actually many ways to do hierarchical Kate Center.",
                    "label": 0
                },
                {
                    "sent": "Henry.",
                    "label": 0
                },
                {
                    "sent": "Then you're so right, so this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's just a single tree that contains all the points and you can build it online and in the order in which the points arrive, but it is true that the there are cases in which you don't want to store all N data points.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes you want to do hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "And you have a million data points which are never going to ask for K = 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "You're going to ask for K at most.",
                    "label": 0
                },
                {
                    "sent": "I don't know 100 and thousand, and so you don't want to keep all million things in the tree.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, you don't need to keep around the whole tree.",
                    "label": 0
                },
                {
                    "sent": "You can basically just keep.",
                    "label": 0
                },
                {
                    "sent": "Just keep it up to the up to the last level that has more than K means.",
                    "label": 0
                },
                {
                    "sent": "Stopping it does an so that it depends on the order in which the points were presented.",
                    "label": 0
                },
                {
                    "sent": "If you build it in this particular way.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some open problems in hierarchical clustering.",
                    "label": 1
                },
                {
                    "sent": "So we've seen a factor 8 approximation.",
                    "label": 1
                },
                {
                    "sent": "Can we do better?",
                    "label": 0
                },
                {
                    "sent": "As I said, there are many ways to do hierarchical K center, and they all give you a factor eight.",
                    "label": 0
                },
                {
                    "sent": "OK, so I know of at least three or four different algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's a randomized variant of this that gives you a factor of 5.4.",
                    "label": 0
                },
                {
                    "sent": "But in terms of deterministic versions, they all seem to give a factor of 8.",
                    "label": 1
                },
                {
                    "sent": "Is there something better?",
                    "label": 0
                },
                {
                    "sent": "OK now in terms of lower bounds that we might prove there are at least two sources of a lower bound.",
                    "label": 1
                },
                {
                    "sent": "One is a short one is a pure NP hardness thing.",
                    "label": 0
                },
                {
                    "sent": "We already know that case center.",
                    "label": 1
                },
                {
                    "sent": "You cannot solve better than a factor of two, so you're certainly not going to be able to come up with a hierarchical clustering that has everything better than a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "OK, since that's a sub problem.",
                    "label": 0
                },
                {
                    "sent": "But there's another sort of lower bound, which is the structural incompatibility, and it's not clear exactly what that lower bound is.",
                    "label": 0
                },
                {
                    "sent": "Is it a factor of two?",
                    "label": 0
                },
                {
                    "sent": "Is that you know this thing where the best two clustering and the best three clustering are not compatible with each other?",
                    "label": 0
                },
                {
                    "sent": "There's some kind of lower bound that comes out of that.",
                    "label": 0
                },
                {
                    "sent": "There's nothing to do with computational complexity, just a structural issue.",
                    "label": 0
                },
                {
                    "sent": "What is that lower bound?",
                    "label": 0
                },
                {
                    "sent": "Is it a factor of two, and can you?",
                    "label": 0
                },
                {
                    "sent": "Can you team that up with the computational complexity lower bound to get some bigger lower bound, like maybe a factor of four or something like that?",
                    "label": 0
                },
                {
                    "sent": "Not known, so that would be nice to get to have some progress on that.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is, hierarchical K means.",
                    "label": 0
                },
                {
                    "sent": "Is there a good way to do this that would be that would be really interesting?",
                    "label": 0
                },
                {
                    "sent": "OK, so so maybe we could just take a little break.",
                    "label": 0
                },
                {
                    "sent": "Now I don't know like 10 minutes or so and then we come back and do some stuff.",
                    "label": 0
                }
            ]
        }
    }
}