{
    "id": "wkorfeu3zlqpxqfzionkhp352t2zj4n3",
    "title": "A Dual Coordinate Descent Method for Large-scale Linear SVM",
    "info": {
        "author": [
            "Kai-Wei Chang, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_chang_dcd/",
    "segmentation": [
        [
            "Good afternoon, highway down and combination of University in today.",
            "Would like to introduce a deal called decent measure for the large scale linear SVN.",
            "And I joined, well, we've she's a share and she's adding and two.",
            "Researchers who cercyon sandora."
        ],
        [
            "And here is today's online.",
            "I will introduce linear SVM and then discussing our dual coordinate, dissent, Mesa and discussing some implementation issue and show some comparison in conclusion."
        ],
        [
            "So the power factor machine has been one of promise in classification tool and usually SVM map data into a high dimensional space and solve the do about kernel trip.",
            "However, if aerating Colonel cost a lot of time so SVN is very high.",
            "To a large problem.",
            "But in some situations, such as in a document classification, we use bag of words model.",
            "So the number of feature is already very large, thus with and without the nonlinear mapping give a similar performance.",
            "So this gives us a chance to solve them or larger data, but SBN."
        ],
        [
            "And this situation has been discussed by several paper.",
            "For example, these two paper.",
            "Then these two.",
            "Then our paper discuss to use modify noodle measure for the L2 SVM.",
            "And these papers discuss to use a cutting plane measure for the L1 SDN and also this paper discuss user bundle Mesa and fill the cutting plane measure as a spatial cast.",
            "Also on the last year of ACNL this paper proposed a measure called Texas and he is using stochastic gradient descent measure.",
            "Also about two has another implementation about the stochastic gradient descent.",
            "And there is another paper user exponential gradient descent.",
            "And professor, we propose primal code and Decent Mesa folder L2 SVM."
        ],
        [
            "And here is the formula we want to discuss about and given the training data Y and Zhao and the number of feature is N and the number of instances L we consider these two formula.",
            "The first one use L1 loss function and we rather use the square root function.",
            "And here the variable is W and a number of fireball here is equal to a number of features and hear the sea is a penalty parimeter to balance the generalization term and the loss function.",
            "But instead you solve this problem for primal formula.",
            "We solve the dual problem."
        ],
        [
            "The dual problem showing here the fireball is Alpha and the number of our variable here is equal to a number of instances.",
            "Q Bar Here is a square matrix with a dimension L * L and L is the number of instances.",
            "The hell that E is a factor of 1, so this term is the same as to some mention all the all the element of the Alpha.",
            "And, uh, and uh and a dual problem is a quadratic problem with constraint off.",
            "I should be bounded by the needle in EU, and here we use technical user trick to combine the L1 and L2 SVM do.",
            "That is, we let the Q bar as a similar metric similarity metric.",
            "You pass a diagonal matrix D and assimilate similarity metric Q is equal to yyz and the inner product of X Ray.",
            "And if it is in a nonlinear situation.",
            "Yes, well Colonel.",
            "And here if we are solving L1 SVN then you upload you here should be.",
            "And the idea here is little.",
            "So the cube is equal to Q and this one will be L1 SVN do.",
            "But if we let you here as Infinity, that is, we remove the upper bound and to let the diagonal matrix D each element as 1 / 2 C. Then this one will be L2 SVM do."
        ],
        [
            "So now we're talking about."
        ],
        [
            "Dual coordinate descent.",
            "Adeel coordinate dissent is very simple.",
            "That is, we solve one variable at a time and fix the rest.",
            "And at this time we sequentially solve one variable subproblem until after getting the optimal and these techniques.",
            "As this technique is a classic, it's optimized optimization techniques and has been.",
            "Discussed being a lot of paper and we can transfer it to 1950 sample if we constrain are now considered.",
            "Also, the dual coding design measure for SVM is already be, well, star."
        ],
        [
            "Did.",
            "For simple in the 1999 years paper to discuss you use according Addison Mazer bollocks VM.",
            "But unless time, they don't focus on the linear SVM with a large number of feature.",
            "And on the last year of ACNL there's a paper use a mexicola rank and this is for the multiplies SVM with cur node but they didn't focus on the linear SVM as well.",
            "And there are others paper to discuss this situation.",
            "And here we are going to show that with a good design and implementation according to Dismays, or can be very suitable for the large linear SVM problem."
        ],
        [
            "Here is a procedure.",
            "At this time we are going to solve one variable subproblem, and if we were also in solving the problem then we use to indicate factory I.",
            "That is only a F element is 1 and the US is 0 and we use it the indicator vector to update it.",
            "I element of the Alpha and to find the minimum of these sound problem.",
            "At least our problem is the is a quadratic problem and we've only one variable, so the optimal D can be easy by that.",
            "If we all the constraint, but if we feel constrained off, I should be bounded by ONU so that if the updated Alpha is exist bound, we need to push them back to the bound.",
            "So the updating rule is showing that like that.",
            "And here you may see that the Qi it can be precomputed and stored it so the main cost of this updating rule is only the gradient IF Alpha."
        ],
        [
            "Anne.",
            "If we directly to up to calculate gradient IF Alpha, this may be caused a lot of time since we need a whole roll of the Q matrix, but too if there are one element of the Q matrix, you need to involve the inner part of SISA.",
            "And to do this inner product need time.",
            "Big ON&N is now is the number of features.",
            "So if we need the whole load of the queue, we need to do that at all times, so it needs bigger Ln.",
            "So if we directly calculate it need bigger L at total.",
            "But in the linear case, that can be different.",
            "Since we can use, we can use a W trick that that we let the W as a.",
            "As a as a as a formula and then we can calculate the gradient by this rule.",
            "And here if you if you submit Subs, substitute the W into this W then this formula is the same as the above.",
            "And to calculate this for this rule is quite easy because it's only involved to do this inner product of the WT Annex.",
            "So we can calculate it by this rule and it's only need bigger North, which is reduced a lot from the Big Island to the bigger.",
            "So the rest problem is, how can we update the W?"
        ],
        [
            "To update the W is quite easy because the W is only change if the Alpha is change since the essay and why they here is a constant.",
            "So so if the other is changed then we change your W code in Italy and it's only need to go through the OH element of the W so it's only cause a bigger than two.",
            "So so only need a big owns a bigger in time to solve our problem which produce one professor.",
            "Big OLN?",
            "And if it's in a sparse situation, is the same in the sparse situation, the bigger Ellen is?",
            "Is some element is missing, so we only store the number of non linear vector number of non zero elements and then and then we can reduce the time for each subproblem from the number of non 0 to a number of non zero divide 2 L."
        ],
        [
            "So here is our algorithm.",
            "That is, given an initial W initial Alpha and corresponding W, we consider two layer of the iteration in each other iteration, we loop until the office skating optimal and into each inner iteration we solve the L1 variable subproblem.",
            "So for solving the ones our problem, we firstly calculate its gradient by the W trick and then we update the Alpha.",
            "And then we update the W and so on and loop until the L star problem is solved and to check the Alpha is optimal.",
            "If not, we do until that."
        ],
        [
            "And above procedure convergence can be getting by the existing result in this paper.",
            "And following we are showing that we were careful implementation.",
            "We can greatly improve the training time of the above procedure."
        ],
        [
            "So firstly, we are discussed a shrinking technique.",
            "A shooting technique is that if the our eyes is already on the bond and updating the Russian is elected, the minus gradient is also taller bound, so Alpha may be forced to stay on the bonds and we may guess that the offer will stay on the bank until the end of the optimization procedure.",
            "So.",
            "We made remove such of I.",
            "To get a small, smaller subproblem, most smaller problem.",
            "But sometimes we may guess wrong that the US is in about now, but they may remove may move out of bounds.",
            "So after the smaller problem optimization problem is solved, we need to push all the shin compatible of our back and to see the optimal W of the smaller problem is the same as the optimal solution for the whole problem.",
            "And to check that we need a whole gradient.",
            "And since updating all grade whole gradient is time consuming, so in a shrinking procedure we may not maintain a gradient of loads of I is shrinking.",
            "So in the if we are in a nonlinear SVM case plus if we say we need bigger L into Calculator, one element of the gradient if need be going North.",
            "So now we need to calculate all the files over shrinking authorize gradient.",
            "So it's up to bigger L ^2 N. But the linear situation we have the W, you may see that even some of our is shrinking.",
            "We still have.",
            "We still can update the W by that, so we W still available.",
            "So probably we say that using the W2 Calculator 1, one of the gradient is only need bigger North, so to calculate the whole gradient is only need bigger.",
            "But actually we don't need this bigger since we are doing a situation updating so we can calculate the gradient I when we update in the Alpha, so we don't need any extra.",
            "Is try ever to do the shrinking and it's showing that the shrinking technique is more suitable for our linear case than nonlinear case."
        ],
        [
            "The other implementation issue is that Professor we say we sequentially update the subproblem.",
            "However, we can.",
            "We can update the subproblem at any random order.",
            "MP FACILELY results showed that if we if we use any random order at each outer iteration, then the operation, then another position may be faster to converge.",
            "That is, we at each outer iteration we random shuffle one 2L index set and to have a permutation set by 1 to the py L and so far the random order.",
            "And after this iteration we do that again to random all the sub problem and then solve the subproblem.",
            "And so on.",
            "And it's showing that this is very efficient, effective in the practice.",
            "Another is to use online setting since updating one of our is only needed.",
            "Only need 11 instance so we can at each time we pick out an Alpha and updated it at once and such setting is related to several papers like this."
        ],
        [
            "One shot."
        ],
        [
            "Finally we are showing some compare comparison comparison result.",
            "For L1 SVN we compare our measure without thinking, and we flushing technique.",
            "And these two is also all use the random order at each outer iteration.",
            "And we compared to the Pegasus, which we use stochastic gradient, dissent, Mesa and also compared to SVM curve which uses cutting plane measure.",
            "Info Lt SVN we compare our measure without thinking and we thinking with primer coding, design Mesa and with another new domains are.",
            "Chung.",
            "And all the other implementation here use double decision for floating point.",
            "And if you use a single decision that may be faster that maybe reduce the time, but the precision may be lost."
        ],
        [
            "A Hill showing the relative function value difference.",
            "With the optimal along time.",
            "And the left two figure is showing the L1 SPM result and the right will figure is showing L2 SVM result.",
            "And we we show the two they'll say, hear, hear the news 20 has 1 million of feature and a great number of instance, and RCP one has 1/2 million of instant with a great number of features.",
            "And the green line here is the SVM per the red line.",
            "Here is up access and blue line is our measure without shrinking, and purple line is are made with shrinking as shown line.",
            "Although the it's been proven, practices are very efficient to solve the problem, but almost is more faster.",
            "And if you use are shrinking then it may be even faster.",
            "And for the L2 SVM case, we shall new domains are in the green line and the.",
            "The primary coordinate descent method in the red line and blue line is our method, and the purple line is always a function key with drinking.",
            "And is showing like if for the LVL 2 SVN situation and one situation is the same that we are all efficient.",
            "And here the blue line is showing 1% of the relative difference and we here we are wondering the time for each solver to reach this 1% line.",
            "So here is."
        ],
        [
            "The result we compare it in the sixth data set an hour method is constantly faster to reach the 1% of 1% line of the area.",
            "Anne.",
            "We are discussing.",
            "We need to discuss in the relationship between the testing accuracy and the training time."
        ],
        [
            "So we here we showing the time to reach at the time to reach that.",
            "Are we showing a testing accuracy?",
            "Difference with the final testing accuracy.",
            "A long time here.",
            "And all measure is a blue line.",
            "And Texas is a red line and the green line is it being purf.",
            "And since our measure with and without thinking give a similar line in there so we only showing one line.",
            "Anne.",
            "Is the line since, since even for the data we've helped millions of help, meaning of instance or measure can be reached.",
            "Final accuracy in few seconds or hear the line here, the xpi zone is for the cycle.",
            "And for L2 SV and the situation is similar.",
            "Our measure is most more stable and fast to reach.",
            "The final conference converge accuracy.",
            "And the red line is the primary coordinate is amazing, and the green line is a chunk.",
            "These are new domains in the blue line is all measure.",
            "So."
        ],
        [
            "Finally, we give some conclusion and discussion here."
        ],
        [
            "Audio coordinate dissent makes can be very, very efficient if a number of data in a number of features or both large.",
            "And it is specially useful for the document classification.",
            "Another experiment showed that even with the help medium of data we can solve, we can get the final accuracy in few seconds.",
            "But our measure has some limitation.",
            "That is, if a number of feature is small, although penalty penalty parameter C is large or make some maybe suffer from latency iteration.",
            "But for them, if the number of feature is small, we shall not solve the dual problem as we solve which will solve the problem in state since the variable of the primal problem is the same as the number of features.",
            "And for the large penalty parameter, C is usually not encounter in the document data, since theoretically showing that with a with a great certain level of C that testing accuracy is the same.",
            "So we need to use such large penalty of these."
        ],
        [
            "And all measures are implemented in a package Lib linear and can be downloaded from this website.",
            "And all the thoughts for reproduce our experiment result.",
            "It can be downloaded from the website too.",
            "Yeah, Nancy, thank you.",
            "Questions.",
            "One question, so you go to the dual and derive the update rules.",
            "Therefore individual samples, and then because everything is linear, it collapses back and you can represent basically everything was a W vector.",
            "So can you give some intuition what your updates that you perform by reasoning in the dual to what they correspond to in the primal?",
            "Because we still keep this W vector.",
            "So how do you move?",
            "Basically in this primal W vector when you perform your dual updates you mean because we updated for dual?",
            "So what is relationship?",
            "To apply more with that.",
            "As I understand, if I understood correctly, you still keep only the W as a representation right?",
            "In each iteration you keep you summarize the current iterate by W vector, yes?",
            "So what does it correspond to in the primal?",
            "I mean yeah."
        ],
        [
            "So."
        ],
        [
            "Here W is showing that like this Ann we updated offer the WS updated corresponding and this W is equal to its optimal situation.",
            "This W is the primal primal optimal value.",
            "If the Alpha is optimal, right?",
            "So is our updated.",
            "Since the W we updated Alpha and we corresponded to update your W. Is somehow to that?",
            "We updated W for one instant of XI.",
            "An updated Alpha corresponding.",
            "This.",
            "Is that make sense?",
            "So, so one experience we've had with these dual coordinate descent methods is on the 1st iteration.",
            "They can be quite slow in comparison to Pegasus because you are having to do a lot of work in getting good initial dual variables.",
            "So is wondering how has your experience been the same?",
            "Have you looked at a comparison to pick a sauce after just one iteration over the data?",
            "Well here we use up access is online stating of the pack."
        ],
        [
            "Less is Packers us at each time picks us to pick out one instance, an updated W and hear our measure is need to go through.",
            "So here the one point.",
            "So here we will show in a time with the relative value difference.",
            "So it is a time so even practice is very fast in 111 iteration.",
            "But for a fair comparison he should do.",
            "Integration and we only show in a time here.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, highway down and combination of University in today.",
                    "label": 0
                },
                {
                    "sent": "Would like to introduce a deal called decent measure for the large scale linear SVN.",
                    "label": 0
                },
                {
                    "sent": "And I joined, well, we've she's a share and she's adding and two.",
                    "label": 0
                },
                {
                    "sent": "Researchers who cercyon sandora.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is today's online.",
                    "label": 0
                },
                {
                    "sent": "I will introduce linear SVM and then discussing our dual coordinate, dissent, Mesa and discussing some implementation issue and show some comparison in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the power factor machine has been one of promise in classification tool and usually SVM map data into a high dimensional space and solve the do about kernel trip.",
                    "label": 1
                },
                {
                    "sent": "However, if aerating Colonel cost a lot of time so SVN is very high.",
                    "label": 0
                },
                {
                    "sent": "To a large problem.",
                    "label": 0
                },
                {
                    "sent": "But in some situations, such as in a document classification, we use bag of words model.",
                    "label": 1
                },
                {
                    "sent": "So the number of feature is already very large, thus with and without the nonlinear mapping give a similar performance.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a chance to solve them or larger data, but SBN.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this situation has been discussed by several paper.",
                    "label": 0
                },
                {
                    "sent": "For example, these two paper.",
                    "label": 0
                },
                {
                    "sent": "Then these two.",
                    "label": 0
                },
                {
                    "sent": "Then our paper discuss to use modify noodle measure for the L2 SVM.",
                    "label": 0
                },
                {
                    "sent": "And these papers discuss to use a cutting plane measure for the L1 SDN and also this paper discuss user bundle Mesa and fill the cutting plane measure as a spatial cast.",
                    "label": 0
                },
                {
                    "sent": "Also on the last year of ACNL this paper proposed a measure called Texas and he is using stochastic gradient descent measure.",
                    "label": 0
                },
                {
                    "sent": "Also about two has another implementation about the stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And there is another paper user exponential gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And professor, we propose primal code and Decent Mesa folder L2 SVM.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the formula we want to discuss about and given the training data Y and Zhao and the number of feature is N and the number of instances L we consider these two formula.",
                    "label": 0
                },
                {
                    "sent": "The first one use L1 loss function and we rather use the square root function.",
                    "label": 0
                },
                {
                    "sent": "And here the variable is W and a number of fireball here is equal to a number of features and hear the sea is a penalty parimeter to balance the generalization term and the loss function.",
                    "label": 0
                },
                {
                    "sent": "But instead you solve this problem for primal formula.",
                    "label": 0
                },
                {
                    "sent": "We solve the dual problem.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The dual problem showing here the fireball is Alpha and the number of our variable here is equal to a number of instances.",
                    "label": 0
                },
                {
                    "sent": "Q Bar Here is a square matrix with a dimension L * L and L is the number of instances.",
                    "label": 0
                },
                {
                    "sent": "The hell that E is a factor of 1, so this term is the same as to some mention all the all the element of the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And, uh, and uh and a dual problem is a quadratic problem with constraint off.",
                    "label": 0
                },
                {
                    "sent": "I should be bounded by the needle in EU, and here we use technical user trick to combine the L1 and L2 SVM do.",
                    "label": 0
                },
                {
                    "sent": "That is, we let the Q bar as a similar metric similarity metric.",
                    "label": 0
                },
                {
                    "sent": "You pass a diagonal matrix D and assimilate similarity metric Q is equal to yyz and the inner product of X Ray.",
                    "label": 0
                },
                {
                    "sent": "And if it is in a nonlinear situation.",
                    "label": 0
                },
                {
                    "sent": "Yes, well Colonel.",
                    "label": 0
                },
                {
                    "sent": "And here if we are solving L1 SVN then you upload you here should be.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is little.",
                    "label": 0
                },
                {
                    "sent": "So the cube is equal to Q and this one will be L1 SVN do.",
                    "label": 0
                },
                {
                    "sent": "But if we let you here as Infinity, that is, we remove the upper bound and to let the diagonal matrix D each element as 1 / 2 C. Then this one will be L2 SVM do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're talking about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dual coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "Adeel coordinate dissent is very simple.",
                    "label": 1
                },
                {
                    "sent": "That is, we solve one variable at a time and fix the rest.",
                    "label": 1
                },
                {
                    "sent": "And at this time we sequentially solve one variable subproblem until after getting the optimal and these techniques.",
                    "label": 0
                },
                {
                    "sent": "As this technique is a classic, it's optimized optimization techniques and has been.",
                    "label": 0
                },
                {
                    "sent": "Discussed being a lot of paper and we can transfer it to 1950 sample if we constrain are now considered.",
                    "label": 0
                },
                {
                    "sent": "Also, the dual coding design measure for SVM is already be, well, star.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did.",
                    "label": 0
                },
                {
                    "sent": "For simple in the 1999 years paper to discuss you use according Addison Mazer bollocks VM.",
                    "label": 0
                },
                {
                    "sent": "But unless time, they don't focus on the linear SVM with a large number of feature.",
                    "label": 1
                },
                {
                    "sent": "And on the last year of ACNL there's a paper use a mexicola rank and this is for the multiplies SVM with cur node but they didn't focus on the linear SVM as well.",
                    "label": 0
                },
                {
                    "sent": "And there are others paper to discuss this situation.",
                    "label": 1
                },
                {
                    "sent": "And here we are going to show that with a good design and implementation according to Dismays, or can be very suitable for the large linear SVM problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a procedure.",
                    "label": 0
                },
                {
                    "sent": "At this time we are going to solve one variable subproblem, and if we were also in solving the problem then we use to indicate factory I.",
                    "label": 0
                },
                {
                    "sent": "That is only a F element is 1 and the US is 0 and we use it the indicator vector to update it.",
                    "label": 0
                },
                {
                    "sent": "I element of the Alpha and to find the minimum of these sound problem.",
                    "label": 0
                },
                {
                    "sent": "At least our problem is the is a quadratic problem and we've only one variable, so the optimal D can be easy by that.",
                    "label": 0
                },
                {
                    "sent": "If we all the constraint, but if we feel constrained off, I should be bounded by ONU so that if the updated Alpha is exist bound, we need to push them back to the bound.",
                    "label": 0
                },
                {
                    "sent": "So the updating rule is showing that like that.",
                    "label": 0
                },
                {
                    "sent": "And here you may see that the Qi it can be precomputed and stored it so the main cost of this updating rule is only the gradient IF Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we directly to up to calculate gradient IF Alpha, this may be caused a lot of time since we need a whole roll of the Q matrix, but too if there are one element of the Q matrix, you need to involve the inner part of SISA.",
                    "label": 0
                },
                {
                    "sent": "And to do this inner product need time.",
                    "label": 0
                },
                {
                    "sent": "Big ON&N is now is the number of features.",
                    "label": 0
                },
                {
                    "sent": "So if we need the whole load of the queue, we need to do that at all times, so it needs bigger Ln.",
                    "label": 0
                },
                {
                    "sent": "So if we directly calculate it need bigger L at total.",
                    "label": 0
                },
                {
                    "sent": "But in the linear case, that can be different.",
                    "label": 0
                },
                {
                    "sent": "Since we can use, we can use a W trick that that we let the W as a.",
                    "label": 0
                },
                {
                    "sent": "As a as a as a formula and then we can calculate the gradient by this rule.",
                    "label": 0
                },
                {
                    "sent": "And here if you if you submit Subs, substitute the W into this W then this formula is the same as the above.",
                    "label": 0
                },
                {
                    "sent": "And to calculate this for this rule is quite easy because it's only involved to do this inner product of the WT Annex.",
                    "label": 0
                },
                {
                    "sent": "So we can calculate it by this rule and it's only need bigger North, which is reduced a lot from the Big Island to the bigger.",
                    "label": 0
                },
                {
                    "sent": "So the rest problem is, how can we update the W?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To update the W is quite easy because the W is only change if the Alpha is change since the essay and why they here is a constant.",
                    "label": 0
                },
                {
                    "sent": "So so if the other is changed then we change your W code in Italy and it's only need to go through the OH element of the W so it's only cause a bigger than two.",
                    "label": 0
                },
                {
                    "sent": "So so only need a big owns a bigger in time to solve our problem which produce one professor.",
                    "label": 0
                },
                {
                    "sent": "Big OLN?",
                    "label": 0
                },
                {
                    "sent": "And if it's in a sparse situation, is the same in the sparse situation, the bigger Ellen is?",
                    "label": 0
                },
                {
                    "sent": "Is some element is missing, so we only store the number of non linear vector number of non zero elements and then and then we can reduce the time for each subproblem from the number of non 0 to a number of non zero divide 2 L.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is, given an initial W initial Alpha and corresponding W, we consider two layer of the iteration in each other iteration, we loop until the office skating optimal and into each inner iteration we solve the L1 variable subproblem.",
                    "label": 0
                },
                {
                    "sent": "So for solving the ones our problem, we firstly calculate its gradient by the W trick and then we update the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And then we update the W and so on and loop until the L star problem is solved and to check the Alpha is optimal.",
                    "label": 0
                },
                {
                    "sent": "If not, we do until that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And above procedure convergence can be getting by the existing result in this paper.",
                    "label": 0
                },
                {
                    "sent": "And following we are showing that we were careful implementation.",
                    "label": 0
                },
                {
                    "sent": "We can greatly improve the training time of the above procedure.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So firstly, we are discussed a shrinking technique.",
                    "label": 0
                },
                {
                    "sent": "A shooting technique is that if the our eyes is already on the bond and updating the Russian is elected, the minus gradient is also taller bound, so Alpha may be forced to stay on the bonds and we may guess that the offer will stay on the bank until the end of the optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We made remove such of I.",
                    "label": 0
                },
                {
                    "sent": "To get a small, smaller subproblem, most smaller problem.",
                    "label": 0
                },
                {
                    "sent": "But sometimes we may guess wrong that the US is in about now, but they may remove may move out of bounds.",
                    "label": 0
                },
                {
                    "sent": "So after the smaller problem optimization problem is solved, we need to push all the shin compatible of our back and to see the optimal W of the smaller problem is the same as the optimal solution for the whole problem.",
                    "label": 1
                },
                {
                    "sent": "And to check that we need a whole gradient.",
                    "label": 0
                },
                {
                    "sent": "And since updating all grade whole gradient is time consuming, so in a shrinking procedure we may not maintain a gradient of loads of I is shrinking.",
                    "label": 0
                },
                {
                    "sent": "So in the if we are in a nonlinear SVM case plus if we say we need bigger L into Calculator, one element of the gradient if need be going North.",
                    "label": 0
                },
                {
                    "sent": "So now we need to calculate all the files over shrinking authorize gradient.",
                    "label": 0
                },
                {
                    "sent": "So it's up to bigger L ^2 N. But the linear situation we have the W, you may see that even some of our is shrinking.",
                    "label": 0
                },
                {
                    "sent": "We still have.",
                    "label": 0
                },
                {
                    "sent": "We still can update the W by that, so we W still available.",
                    "label": 0
                },
                {
                    "sent": "So probably we say that using the W2 Calculator 1, one of the gradient is only need bigger North, so to calculate the whole gradient is only need bigger.",
                    "label": 0
                },
                {
                    "sent": "But actually we don't need this bigger since we are doing a situation updating so we can calculate the gradient I when we update in the Alpha, so we don't need any extra.",
                    "label": 1
                },
                {
                    "sent": "Is try ever to do the shrinking and it's showing that the shrinking technique is more suitable for our linear case than nonlinear case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other implementation issue is that Professor we say we sequentially update the subproblem.",
                    "label": 0
                },
                {
                    "sent": "However, we can.",
                    "label": 0
                },
                {
                    "sent": "We can update the subproblem at any random order.",
                    "label": 0
                },
                {
                    "sent": "MP FACILELY results showed that if we if we use any random order at each outer iteration, then the operation, then another position may be faster to converge.",
                    "label": 1
                },
                {
                    "sent": "That is, we at each outer iteration we random shuffle one 2L index set and to have a permutation set by 1 to the py L and so far the random order.",
                    "label": 0
                },
                {
                    "sent": "And after this iteration we do that again to random all the sub problem and then solve the subproblem.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 1
                },
                {
                    "sent": "And it's showing that this is very efficient, effective in the practice.",
                    "label": 0
                },
                {
                    "sent": "Another is to use online setting since updating one of our is only needed.",
                    "label": 0
                },
                {
                    "sent": "Only need 11 instance so we can at each time we pick out an Alpha and updated it at once and such setting is related to several papers like this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One shot.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally we are showing some compare comparison comparison result.",
                    "label": 0
                },
                {
                    "sent": "For L1 SVN we compare our measure without thinking, and we flushing technique.",
                    "label": 0
                },
                {
                    "sent": "And these two is also all use the random order at each outer iteration.",
                    "label": 0
                },
                {
                    "sent": "And we compared to the Pegasus, which we use stochastic gradient, dissent, Mesa and also compared to SVM curve which uses cutting plane measure.",
                    "label": 1
                },
                {
                    "sent": "Info Lt SVN we compare our measure without thinking and we thinking with primer coding, design Mesa and with another new domains are.",
                    "label": 0
                },
                {
                    "sent": "Chung.",
                    "label": 0
                },
                {
                    "sent": "And all the other implementation here use double decision for floating point.",
                    "label": 0
                },
                {
                    "sent": "And if you use a single decision that may be faster that maybe reduce the time, but the precision may be lost.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A Hill showing the relative function value difference.",
                    "label": 0
                },
                {
                    "sent": "With the optimal along time.",
                    "label": 0
                },
                {
                    "sent": "And the left two figure is showing the L1 SPM result and the right will figure is showing L2 SVM result.",
                    "label": 0
                },
                {
                    "sent": "And we we show the two they'll say, hear, hear the news 20 has 1 million of feature and a great number of instance, and RCP one has 1/2 million of instant with a great number of features.",
                    "label": 0
                },
                {
                    "sent": "And the green line here is the SVM per the red line.",
                    "label": 0
                },
                {
                    "sent": "Here is up access and blue line is our measure without shrinking, and purple line is are made with shrinking as shown line.",
                    "label": 0
                },
                {
                    "sent": "Although the it's been proven, practices are very efficient to solve the problem, but almost is more faster.",
                    "label": 0
                },
                {
                    "sent": "And if you use are shrinking then it may be even faster.",
                    "label": 0
                },
                {
                    "sent": "And for the L2 SVM case, we shall new domains are in the green line and the.",
                    "label": 0
                },
                {
                    "sent": "The primary coordinate descent method in the red line and blue line is our method, and the purple line is always a function key with drinking.",
                    "label": 0
                },
                {
                    "sent": "And is showing like if for the LVL 2 SVN situation and one situation is the same that we are all efficient.",
                    "label": 0
                },
                {
                    "sent": "And here the blue line is showing 1% of the relative difference and we here we are wondering the time for each solver to reach this 1% line.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The result we compare it in the sixth data set an hour method is constantly faster to reach the 1% of 1% line of the area.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We are discussing.",
                    "label": 0
                },
                {
                    "sent": "We need to discuss in the relationship between the testing accuracy and the training time.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we here we showing the time to reach at the time to reach that.",
                    "label": 0
                },
                {
                    "sent": "Are we showing a testing accuracy?",
                    "label": 1
                },
                {
                    "sent": "Difference with the final testing accuracy.",
                    "label": 0
                },
                {
                    "sent": "A long time here.",
                    "label": 0
                },
                {
                    "sent": "And all measure is a blue line.",
                    "label": 0
                },
                {
                    "sent": "And Texas is a red line and the green line is it being purf.",
                    "label": 0
                },
                {
                    "sent": "And since our measure with and without thinking give a similar line in there so we only showing one line.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Is the line since, since even for the data we've helped millions of help, meaning of instance or measure can be reached.",
                    "label": 0
                },
                {
                    "sent": "Final accuracy in few seconds or hear the line here, the xpi zone is for the cycle.",
                    "label": 0
                },
                {
                    "sent": "And for L2 SV and the situation is similar.",
                    "label": 0
                },
                {
                    "sent": "Our measure is most more stable and fast to reach.",
                    "label": 0
                },
                {
                    "sent": "The final conference converge accuracy.",
                    "label": 0
                },
                {
                    "sent": "And the red line is the primary coordinate is amazing, and the green line is a chunk.",
                    "label": 0
                },
                {
                    "sent": "These are new domains in the blue line is all measure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we give some conclusion and discussion here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Audio coordinate dissent makes can be very, very efficient if a number of data in a number of features or both large.",
                    "label": 1
                },
                {
                    "sent": "And it is specially useful for the document classification.",
                    "label": 1
                },
                {
                    "sent": "Another experiment showed that even with the help medium of data we can solve, we can get the final accuracy in few seconds.",
                    "label": 0
                },
                {
                    "sent": "But our measure has some limitation.",
                    "label": 0
                },
                {
                    "sent": "That is, if a number of feature is small, although penalty penalty parameter C is large or make some maybe suffer from latency iteration.",
                    "label": 0
                },
                {
                    "sent": "But for them, if the number of feature is small, we shall not solve the dual problem as we solve which will solve the problem in state since the variable of the primal problem is the same as the number of features.",
                    "label": 0
                },
                {
                    "sent": "And for the large penalty parameter, C is usually not encounter in the document data, since theoretically showing that with a with a great certain level of C that testing accuracy is the same.",
                    "label": 1
                },
                {
                    "sent": "So we need to use such large penalty of these.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And all measures are implemented in a package Lib linear and can be downloaded from this website.",
                    "label": 0
                },
                {
                    "sent": "And all the thoughts for reproduce our experiment result.",
                    "label": 0
                },
                {
                    "sent": "It can be downloaded from the website too.",
                    "label": 0
                },
                {
                    "sent": "Yeah, Nancy, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "One question, so you go to the dual and derive the update rules.",
                    "label": 0
                },
                {
                    "sent": "Therefore individual samples, and then because everything is linear, it collapses back and you can represent basically everything was a W vector.",
                    "label": 0
                },
                {
                    "sent": "So can you give some intuition what your updates that you perform by reasoning in the dual to what they correspond to in the primal?",
                    "label": 0
                },
                {
                    "sent": "Because we still keep this W vector.",
                    "label": 0
                },
                {
                    "sent": "So how do you move?",
                    "label": 0
                },
                {
                    "sent": "Basically in this primal W vector when you perform your dual updates you mean because we updated for dual?",
                    "label": 0
                },
                {
                    "sent": "So what is relationship?",
                    "label": 0
                },
                {
                    "sent": "To apply more with that.",
                    "label": 0
                },
                {
                    "sent": "As I understand, if I understood correctly, you still keep only the W as a representation right?",
                    "label": 0
                },
                {
                    "sent": "In each iteration you keep you summarize the current iterate by W vector, yes?",
                    "label": 0
                },
                {
                    "sent": "So what does it correspond to in the primal?",
                    "label": 1
                },
                {
                    "sent": "I mean yeah.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here W is showing that like this Ann we updated offer the WS updated corresponding and this W is equal to its optimal situation.",
                    "label": 0
                },
                {
                    "sent": "This W is the primal primal optimal value.",
                    "label": 0
                },
                {
                    "sent": "If the Alpha is optimal, right?",
                    "label": 0
                },
                {
                    "sent": "So is our updated.",
                    "label": 0
                },
                {
                    "sent": "Since the W we updated Alpha and we corresponded to update your W. Is somehow to that?",
                    "label": 0
                },
                {
                    "sent": "We updated W for one instant of XI.",
                    "label": 0
                },
                {
                    "sent": "An updated Alpha corresponding.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Is that make sense?",
                    "label": 0
                },
                {
                    "sent": "So, so one experience we've had with these dual coordinate descent methods is on the 1st iteration.",
                    "label": 0
                },
                {
                    "sent": "They can be quite slow in comparison to Pegasus because you are having to do a lot of work in getting good initial dual variables.",
                    "label": 0
                },
                {
                    "sent": "So is wondering how has your experience been the same?",
                    "label": 0
                },
                {
                    "sent": "Have you looked at a comparison to pick a sauce after just one iteration over the data?",
                    "label": 0
                },
                {
                    "sent": "Well here we use up access is online stating of the pack.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Less is Packers us at each time picks us to pick out one instance, an updated W and hear our measure is need to go through.",
                    "label": 0
                },
                {
                    "sent": "So here the one point.",
                    "label": 0
                },
                {
                    "sent": "So here we will show in a time with the relative value difference.",
                    "label": 0
                },
                {
                    "sent": "So it is a time so even practice is very fast in 111 iteration.",
                    "label": 0
                },
                {
                    "sent": "But for a fair comparison he should do.",
                    "label": 0
                },
                {
                    "sent": "Integration and we only show in a time here.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}