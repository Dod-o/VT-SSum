{
    "id": "kqizwgbfm2ify2mrif6ezneb7zf3w5am",
    "title": "Optimal aggregation of affine estimators",
    "info": {
        "author": [
            "Joseph Salmon, Denis Diderot University - Paris 7"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/colt2011_salmon_optimal/",
    "segmentation": [
        [
            "So."
        ],
        [
            "Let's introduce and motivate what I'm going to talk about in this talk.",
            "A lot of the results are going to present our theoretical and are dealing with our regular inequalities, so we can think about it in several cases, such as.",
            "I did mention sparsity scenario.",
            "And we also deal with adaptation results in the regression model.",
            "A little like the result developing the edges or something.",
            "So you can think for application as several different fields such as image processing that I've been working on for awhile, genetics, inverse problems or so.",
            "And one of the message here you can get is why aggregating well because aggregating and mixing estimators can be.",
            "Better in a lot of ways than just selecting only one estimators in."
        ],
        [
            "Your family would see that in, you know.",
            "So let's make sure that again, the first type of.",
            "Estimate as you would like to aggregate or select in some sense could be for instance convolution operator, so they are quite firm, useful in signal processing.",
            "So imagine you get away here that you observe a noisy version and you would like to find a convolution of these signals that would be more regular and that will be close to the original unknown signals.",
            "So you got a family and usually you have bandwidth that you need to choose.",
            "And finding the good one or.",
            "And like combining several bandwidth could be hard or like difficult.",
            "So that would be one kind of setting we are going to consider."
        ],
        [
            "Another one would be something not really directly related to that, but we have already heard about it this morning with build store.",
            "Can also yesterday with Daniels talk it's about dictionary learning and the community of image processing.",
            "It's really something that is providing the best version nowadays for problems such as in painting, denoising, texture synthesis or something.",
            "So the idea is just like you want to build small patches so there are just more images.",
            "Inside your big image, then you want to find a dictionary or a union of dictionary.",
            "So the idea would be to combine those elementary elements in a clever way so that you can approximate your signal."
        ],
        [
            "So we got several solutions to do that to classical.",
            "One could be penalization methods.",
            "So here your estimator is going to be combinations like linear combinations of several features in RM, and you would like to recover.",
            "The closest estimation in with respect to the.",
            "Unknown.",
            "So one possibility it is to just optimized.",
            "A compromise between the data fitting and a penalized an opinion is action that you can also consider as a regularization term.",
            "So the usual is just the square square risk here and for the pain until you got a lot of choices that are more less common.",
            "For multiview, like the oldest, one would be the Ridge regression or the chicken off penalty depending on your community and it's just to penalize the L2.",
            "Normal of the coefficient of your dictionary.",
            "Another solution would be to use up signal.",
            "That would be like quite difficult to solve, but that would be a good idea in practice.",
            "If you get sparsity assumption or scenario and a proxy to that could be to use just now.",
            "There's something that is really common law, so meaning the L1 regularization, and in most of the one in part of those cases we can get some Oracle inequalities and I could just take a polar format.",
            "From that you get beat up.",
            "Here in all this setting will be some smoothing parameter.",
            "And you can also consider like Banerji that combine all those different.",
            "LOLP norms and I'm presenting can also think about.",
            "Mixture will block story of this ones, but in the end usually what you get after optimizing that is just along the Hut that is just the best one you get.",
            "So in the end you just get one."
        ],
        [
            "So here we're going to present results on aggregation, so we try to combine the two settings with convolution settings and the dictionary learning.",
            "As far as I could, so in the first setting.",
            "We got these metrics of convolution and the estimator are linear in the data.",
            "N it exists like prison.",
            "Improved by long and barren in 2006, but for a small.",
            "Type of family office, tomatoes and they are just projection matrices.",
            "Another independent connected work that provides sharper vehicle inequality is the one by WNT back off in a series of pepper.",
            "And the gadget in the setting where you combine features are basically independent.",
            "Of your observed signal.",
            "So what we try to do here is just to try to combine all of that together and we'll see as far as you can go."
        ],
        [
            "So here's another well, considering the why are the noisy observation?",
            "If I is a true underlying signal?",
            "And you got a noisy version, so the noise is Atheros elastic, meaning that she got a standard deviation that could be different Sigma I and it's a epsilon I is going to be the IAG Gaussian random variables.",
            "So you can think about the FII as an observation on the grid on a fixed grid.",
            "And if you prefer, you can also go back to the easiest case of former sadistic regression where just Sigma 2.",
            "Is well.",
            "Diagonal matrix of variances is just longer times identity where Sigma two is just the variance.",
            "So we're going to use the performance of our procedures with respect to the.",
            "And to risk.",
            "And just kind of settings was could be also interesting for solving inverse problems, so it was.",
            "A motivation for the beginning of the work with we cannot go as far as we wanted.",
            "So."
        ],
        [
            "It's just a motivation letter, so here's the setting of aggregation.",
            "Oracle inequality so.",
            "For you, after hearing a lot about bandit and regrets, you're going to see a lot of steps that are close to that.",
            "So the idea is to have at a big family of estimators, F capital, Lambda, so it is indexed by a capital Lambda inside RM.",
            "So M is going to be the number of features if you want that she can use.",
            "When it's fine it or it could be a set that is really bigger than that.",
            "So now is your regular quality tells you that you can build well you would like to prove.",
            "Sorry, you will have to build an aggregation procedures that you can control its risk, so this is the left and term by the sum of two term one that is here.",
            "The risk of the Oracle, so this would be the best possible and achievable risk if you knew in advance the real signal.",
            "Of course you don't know the real signal, so the name Oracle refers to this absence of knowledge.",
            "And you got two other terms, so you got a price to pay for not knowing in advance this this F. So this price to pay should go to zero of course, but it depends on the complexity of the family you're using.",
            "So the capital Lambda M of the noise of the intensity of the noise.",
            "And then the last term is see of any it's against and that might depend on N depending on the kind of results Canongate.",
            "It should be like above 1 and we would like to get a record inequality with CN equals one that we could sharp.",
            "So in this setting one Mark is at F aggregate, might not be inside the original family and the second one is that with this kind of inequalities you get also optimality sinus lower bounds for a lot of common sets will see the other little letter and most of them were approved by the backup in 2003.",
            "Eunuch."
        ],
        [
            "Paper.",
            "So here is how we define our estimator is going to be.",
            "First we need to.",
            "A distribution over our Lambda set.",
            "So first you gotta prior \u03c0 and then you want to modify your prior thanks to the data feeling that you can get.",
            "So you must have at M and then by your squeezed estimates like we call it here.",
            "Eric Lambda.",
            "And you change the probability that you get at the beginning supply here by multiplying it by the exponential form.",
            "That is here.",
            "Ambetter will be just.",
            "A trade off.",
            "For regularization or it is usually referred to as the temperature.",
            "So now would you define this probability an?",
            "I think this probability on the set of your Lambda.",
            "You can just recover information for.",
            "Creating an estimator.",
            "The simplest way is just to take the posterior expectation, but you can imagine other procedures doing the same.",
            "We are considering this one in the following, so if there goes to zero, you just recover the best element with respect to this measure of risk and if the charges to Infinity you just recover.",
            "An originalist in matters like you could get just using the prior as your knowledge on this side.",
            "So in fact the data doesn't modify your estimators, so a common tool that we need in this context is such time lemma.",
            "So both work that we that we use for our demonstration rely on this one.",
            "So if you want to generalize it, you need in some sense to get a generalization."
        ],
        [
            "Sistema but I won't go deep in deeper into these datums.",
            "So another point of view of you are that might be also interesting that it's a penalty point of view, but on a bigger set so you can just enlarge that you're considering using for instance, probability over Lambda.",
            "Then you can define a penalty estimators in the following way.",
            "So this is the other edge with respect to this distribution where this distribution is an optimized solution of the compromise of a tradeoff between a data fitting.",
            "Overall the distribution P. In a bit magic term.",
            "Over all the others."
        ],
        [
            "It depends on the distribution you're using, so in the very case where you're using the callback number divergents to control this penalty.",
            "You will recover exactly the exponential weight with the prior Pi if you control the distance to between P and Pi with the Carol divergent that I recorded here and again here, the better parameter is exactly the one that was on the previous slide, the one that you were using here."
        ],
        [
            "So now we need.",
            "We cannot prove a general Oracle inequality for the whole kind of estimators, so we just focused on simple simple case.",
            "It's going to be fine.",
            "Structure, so just we can write F at Lambda equals L onda Y plus belanda, where alumna is matrix belong.",
            "There is a deterministic vector and they're all independent of Y and possibly Lambda could be non countable.",
            "So simple case is the one of dictionary learning would be with a Lambda equals 0.",
            "So you can think about that having some feature vectors, F1, FM and then you can try to get a regular inequality over those different family.",
            "Either the family itself, a feature convex or linear or espers combinations."
        ],
        [
            "No, if you remember you get and also the case where the Lambda equals 0, meaning that we got linear transform.",
            "So our work extends the work by long environment that we're also like.",
            "Pursued by IKEA.",
            "Lunacy and legal entity back off over altogether projectors over subspaces of RM.",
            "Another point of view could be to use diagonal."
        ],
        [
            "This is so either you recover projectors or you can define shrinking estimators.",
            "So we're going to see an error that we can use, for instance, Pinsker filters having this formula.",
            "And that rely on two parameters, so that you are going to get coefficient that decreases from one to zero at a given red that is controlled by those two coefficient."
        ],
        [
            "So now we need 2 assumptions or two conditions to prove our theorem surface.",
            "One is at matrices or optical projection and at the bottom there are all in the new space of a Lambda."
        ],
        [
            "That is basically the kind of setting regarding Barron and the second one is that you need to get matrices that are symmetric positive, semi definite, that all commutes altogether in the family and the commutes also with the covariance of the noise metrics and in the end the vectors belong.",
            "There should be in the new space of all other element prime."
        ],
        [
            "So with that we can get done that we call like Pack or ESC pack Bayesian burn.",
            "And so under this to condition for any prior parties that you can choose, we get the following record inequality.",
            "So you control the risk of the procedure by a compromise over all the distribution of the integrated risk over the distribution you choosing.",
            "Plus a penalty term that depends on the product you get an multiply by bit over in.",
            "So for that you need better to be big enough and depending on the condition you're assuming you need either that it's bigger than the maximum of the variances or that is bigger than item maximum variances.",
            "So maybe this one is a little complex, but you can think about this theorem as something that could provide you some corollary's.",
            "So one of them I won't show this one because Sebastian should you watch."
        ],
        [
            "For the L1 sparsity scenario, with the one concern, but here, let's get to the simple case where you just find it family.",
            "So Lambda is just one 2M and you use by as a uniform distribution over this set.",
            "So you just recover here and Oracle inequality that is you controlling the performance of the over by the best risk over your family plus a logarithmic term in the number of filament you got in your family multiplied by Beethoven.",
            "Still, with the same constraints on video.",
            "So indications were Belinda equals zero is just extend the resolution of lung environment.",
            "And what is good to know is that in the case of homicide astex this term is exactly the price that is that you need to pay.",
            "In fact it is optimal.",
            "So you can get better bounded something a constant time this logarithmic term over N."
        ],
        [
            "So now the last part of the talk is going to be an application of this formula.",
            "It's going to be for adaptation, so imagine now we are in the almost sadistic setting and we consider the discrete free coefficient of our signal.",
            "Let's assume that the signal is inside Sobolev bowls for this type of.",
            "Well, the the geometry of the signal is inside the circle of balls, so I denote them by Alpha and R where R is the radius and Alpha controls the decay of the coefficient.",
            "And so a reminder serum by being scared from the edges, where in fact the minimax rates over this ellipsoid.",
            "Is given by linear estimators, so you get just instead of considering all the estimator you can just consider a linear one.",
            "And Moreover, you can even consider only shrinker shrinkers estimators of the biggest type.",
            "So this is all this one meaning here that you need to transform your signal in the.",
            "For your basis you just shrink them with this formula and then you invert it to risk over the signal.",
            "So here is the index Lambda going to be."
        ],
        [
            "Topless store to the Power 2.",
            "And we will choose to aggregate with your procedure represented before those very elements of physical, physical.",
            "And now we only need a prior to get the estimators or the prior.",
            "We're going to use is following you first draw Alpha according to an exponential distribution.",
            "And then knowing this Alpha, you're going to draw W according to this polynomial distribution, where N Sigma is just the ratio between the noise and.",
            "Well, the noise level and the number of observation.",
            "So the theoretical guarantee that we can get in this context is that we can prove that this estimator.",
            "Is adaptive in the exact minimax sense, uncivilized ellipsoid, meaning that you can achieve the performance without noting here.",
            "You don't know to know in advance Ian far in the are so you get this kind of result.",
            "I think that due to the time I won't present deeply in details the practical performances, but we compare our procedure with other well known adaptive methods such as social link by don't know Anna Johnson using short imitation for the threshold as blockstein method.",
            "Bitonic, I 99 and the last one being the empirical risk minimization."
        ],
        [
            "An on pictures you can see like it's going to be OK, we're going to recover.",
            "So here is the noisy.",
            "Sorry it's true.",
            "Underlying signal here is the noisy signal.",
            "Here this is our procedure and we got some peace in arms.",
            "That is pretty reasonable and we compare with the three other Mr Data presented before.",
            "You can see that we compare quite decently with the your that is the last one, but it's normal because it just optimizing over the whole family.",
            "So.",
            "That would be the minimizer of the unbiased risk and the two other one are quite far away from that."
        ],
        [
            "So to conclude this talk, I hope I convinced you that we can get some sharper record inequalities for.",
            "A support of the affinity matters.",
            "We can produce adaptive results with the.",
            "We speak to the signals, misses.",
            "In the case of using Pinsker filters.",
            "N like pretty reasonable experiment performance where provided.",
            "So I'm going worked out to weekend.",
            "The assumptions are still strong in our context and we have used, for instance, symmetric version of the award to do that.",
            "Another extension would be to focus on the type of noise."
        ],
        [
            "Well, in this work we only focus on version, but more can be done in in a few months.",
            "So if you're interested in a longer version of the paper or for the software, you can just go online on our website and you find everything.",
            "Thank you.",
            "So you're looking at a heteroskedastic regression problem.",
            "And if you go back for example to, well, actually from the definition of the estimator, at some point, you know from the paper when you're dealing with the inverse problems, you have to block what's wrong with actually waiting by the Sigma eyes in the definition of the argument.",
            "So it is sorry.",
            "So when you're doing the argument, you say.",
            "In the paper you have problem because you're using the same data for each of your observations, but instead of looking at the convex combinations of the L2 norm, why don't you look at the weighted L2 norm like I mean usually in statistics, when you have heteroskedastic errors for regression, just wait by the by the level of the noise.",
            "I think we're just saying is some kind of already done in the because you need it inside the estimation of the risk.",
            "In fact, you need the estimation of the unbiased estimation of the risk, and you need a sigmai somewhere and I guess are already there.",
            "So for the better you want to be a little different, but.",
            "I guess.",
            "You should well.",
            "It's inside the formula.",
            "In fact, it's just hidden in the year in the air Lambda at.",
            "Any other questions?",
            "Alright, let's thank specific."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's introduce and motivate what I'm going to talk about in this talk.",
                    "label": 0
                },
                {
                    "sent": "A lot of the results are going to present our theoretical and are dealing with our regular inequalities, so we can think about it in several cases, such as.",
                    "label": 0
                },
                {
                    "sent": "I did mention sparsity scenario.",
                    "label": 0
                },
                {
                    "sent": "And we also deal with adaptation results in the regression model.",
                    "label": 1
                },
                {
                    "sent": "A little like the result developing the edges or something.",
                    "label": 1
                },
                {
                    "sent": "So you can think for application as several different fields such as image processing that I've been working on for awhile, genetics, inverse problems or so.",
                    "label": 0
                },
                {
                    "sent": "And one of the message here you can get is why aggregating well because aggregating and mixing estimators can be.",
                    "label": 1
                },
                {
                    "sent": "Better in a lot of ways than just selecting only one estimators in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your family would see that in, you know.",
                    "label": 0
                },
                {
                    "sent": "So let's make sure that again, the first type of.",
                    "label": 0
                },
                {
                    "sent": "Estimate as you would like to aggregate or select in some sense could be for instance convolution operator, so they are quite firm, useful in signal processing.",
                    "label": 0
                },
                {
                    "sent": "So imagine you get away here that you observe a noisy version and you would like to find a convolution of these signals that would be more regular and that will be close to the original unknown signals.",
                    "label": 0
                },
                {
                    "sent": "So you got a family and usually you have bandwidth that you need to choose.",
                    "label": 0
                },
                {
                    "sent": "And finding the good one or.",
                    "label": 0
                },
                {
                    "sent": "And like combining several bandwidth could be hard or like difficult.",
                    "label": 0
                },
                {
                    "sent": "So that would be one kind of setting we are going to consider.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another one would be something not really directly related to that, but we have already heard about it this morning with build store.",
                    "label": 0
                },
                {
                    "sent": "Can also yesterday with Daniels talk it's about dictionary learning and the community of image processing.",
                    "label": 1
                },
                {
                    "sent": "It's really something that is providing the best version nowadays for problems such as in painting, denoising, texture synthesis or something.",
                    "label": 0
                },
                {
                    "sent": "So the idea is just like you want to build small patches so there are just more images.",
                    "label": 0
                },
                {
                    "sent": "Inside your big image, then you want to find a dictionary or a union of dictionary.",
                    "label": 0
                },
                {
                    "sent": "So the idea would be to combine those elementary elements in a clever way so that you can approximate your signal.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we got several solutions to do that to classical.",
                    "label": 0
                },
                {
                    "sent": "One could be penalization methods.",
                    "label": 1
                },
                {
                    "sent": "So here your estimator is going to be combinations like linear combinations of several features in RM, and you would like to recover.",
                    "label": 0
                },
                {
                    "sent": "The closest estimation in with respect to the.",
                    "label": 0
                },
                {
                    "sent": "Unknown.",
                    "label": 0
                },
                {
                    "sent": "So one possibility it is to just optimized.",
                    "label": 0
                },
                {
                    "sent": "A compromise between the data fitting and a penalized an opinion is action that you can also consider as a regularization term.",
                    "label": 0
                },
                {
                    "sent": "So the usual is just the square square risk here and for the pain until you got a lot of choices that are more less common.",
                    "label": 0
                },
                {
                    "sent": "For multiview, like the oldest, one would be the Ridge regression or the chicken off penalty depending on your community and it's just to penalize the L2.",
                    "label": 0
                },
                {
                    "sent": "Normal of the coefficient of your dictionary.",
                    "label": 0
                },
                {
                    "sent": "Another solution would be to use up signal.",
                    "label": 0
                },
                {
                    "sent": "That would be like quite difficult to solve, but that would be a good idea in practice.",
                    "label": 0
                },
                {
                    "sent": "If you get sparsity assumption or scenario and a proxy to that could be to use just now.",
                    "label": 0
                },
                {
                    "sent": "There's something that is really common law, so meaning the L1 regularization, and in most of the one in part of those cases we can get some Oracle inequalities and I could just take a polar format.",
                    "label": 0
                },
                {
                    "sent": "From that you get beat up.",
                    "label": 1
                },
                {
                    "sent": "Here in all this setting will be some smoothing parameter.",
                    "label": 0
                },
                {
                    "sent": "And you can also consider like Banerji that combine all those different.",
                    "label": 0
                },
                {
                    "sent": "LOLP norms and I'm presenting can also think about.",
                    "label": 0
                },
                {
                    "sent": "Mixture will block story of this ones, but in the end usually what you get after optimizing that is just along the Hut that is just the best one you get.",
                    "label": 0
                },
                {
                    "sent": "So in the end you just get one.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we're going to present results on aggregation, so we try to combine the two settings with convolution settings and the dictionary learning.",
                    "label": 0
                },
                {
                    "sent": "As far as I could, so in the first setting.",
                    "label": 0
                },
                {
                    "sent": "We got these metrics of convolution and the estimator are linear in the data.",
                    "label": 0
                },
                {
                    "sent": "N it exists like prison.",
                    "label": 0
                },
                {
                    "sent": "Improved by long and barren in 2006, but for a small.",
                    "label": 0
                },
                {
                    "sent": "Type of family office, tomatoes and they are just projection matrices.",
                    "label": 0
                },
                {
                    "sent": "Another independent connected work that provides sharper vehicle inequality is the one by WNT back off in a series of pepper.",
                    "label": 0
                },
                {
                    "sent": "And the gadget in the setting where you combine features are basically independent.",
                    "label": 0
                },
                {
                    "sent": "Of your observed signal.",
                    "label": 0
                },
                {
                    "sent": "So what we try to do here is just to try to combine all of that together and we'll see as far as you can go.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's another well, considering the why are the noisy observation?",
                    "label": 0
                },
                {
                    "sent": "If I is a true underlying signal?",
                    "label": 0
                },
                {
                    "sent": "And you got a noisy version, so the noise is Atheros elastic, meaning that she got a standard deviation that could be different Sigma I and it's a epsilon I is going to be the IAG Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "So you can think about the FII as an observation on the grid on a fixed grid.",
                    "label": 0
                },
                {
                    "sent": "And if you prefer, you can also go back to the easiest case of former sadistic regression where just Sigma 2.",
                    "label": 0
                },
                {
                    "sent": "Is well.",
                    "label": 0
                },
                {
                    "sent": "Diagonal matrix of variances is just longer times identity where Sigma two is just the variance.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use the performance of our procedures with respect to the.",
                    "label": 0
                },
                {
                    "sent": "And to risk.",
                    "label": 0
                },
                {
                    "sent": "And just kind of settings was could be also interesting for solving inverse problems, so it was.",
                    "label": 0
                },
                {
                    "sent": "A motivation for the beginning of the work with we cannot go as far as we wanted.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's just a motivation letter, so here's the setting of aggregation.",
                    "label": 0
                },
                {
                    "sent": "Oracle inequality so.",
                    "label": 0
                },
                {
                    "sent": "For you, after hearing a lot about bandit and regrets, you're going to see a lot of steps that are close to that.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to have at a big family of estimators, F capital, Lambda, so it is indexed by a capital Lambda inside RM.",
                    "label": 1
                },
                {
                    "sent": "So M is going to be the number of features if you want that she can use.",
                    "label": 0
                },
                {
                    "sent": "When it's fine it or it could be a set that is really bigger than that.",
                    "label": 0
                },
                {
                    "sent": "So now is your regular quality tells you that you can build well you would like to prove.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you will have to build an aggregation procedures that you can control its risk, so this is the left and term by the sum of two term one that is here.",
                    "label": 0
                },
                {
                    "sent": "The risk of the Oracle, so this would be the best possible and achievable risk if you knew in advance the real signal.",
                    "label": 0
                },
                {
                    "sent": "Of course you don't know the real signal, so the name Oracle refers to this absence of knowledge.",
                    "label": 1
                },
                {
                    "sent": "And you got two other terms, so you got a price to pay for not knowing in advance this this F. So this price to pay should go to zero of course, but it depends on the complexity of the family you're using.",
                    "label": 1
                },
                {
                    "sent": "So the capital Lambda M of the noise of the intensity of the noise.",
                    "label": 0
                },
                {
                    "sent": "And then the last term is see of any it's against and that might depend on N depending on the kind of results Canongate.",
                    "label": 0
                },
                {
                    "sent": "It should be like above 1 and we would like to get a record inequality with CN equals one that we could sharp.",
                    "label": 0
                },
                {
                    "sent": "So in this setting one Mark is at F aggregate, might not be inside the original family and the second one is that with this kind of inequalities you get also optimality sinus lower bounds for a lot of common sets will see the other little letter and most of them were approved by the backup in 2003.",
                    "label": 0
                },
                {
                    "sent": "Eunuch.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "So here is how we define our estimator is going to be.",
                    "label": 0
                },
                {
                    "sent": "First we need to.",
                    "label": 0
                },
                {
                    "sent": "A distribution over our Lambda set.",
                    "label": 0
                },
                {
                    "sent": "So first you gotta prior \u03c0 and then you want to modify your prior thanks to the data feeling that you can get.",
                    "label": 0
                },
                {
                    "sent": "So you must have at M and then by your squeezed estimates like we call it here.",
                    "label": 0
                },
                {
                    "sent": "Eric Lambda.",
                    "label": 0
                },
                {
                    "sent": "And you change the probability that you get at the beginning supply here by multiplying it by the exponential form.",
                    "label": 0
                },
                {
                    "sent": "That is here.",
                    "label": 0
                },
                {
                    "sent": "Ambetter will be just.",
                    "label": 0
                },
                {
                    "sent": "A trade off.",
                    "label": 0
                },
                {
                    "sent": "For regularization or it is usually referred to as the temperature.",
                    "label": 0
                },
                {
                    "sent": "So now would you define this probability an?",
                    "label": 0
                },
                {
                    "sent": "I think this probability on the set of your Lambda.",
                    "label": 0
                },
                {
                    "sent": "You can just recover information for.",
                    "label": 0
                },
                {
                    "sent": "Creating an estimator.",
                    "label": 0
                },
                {
                    "sent": "The simplest way is just to take the posterior expectation, but you can imagine other procedures doing the same.",
                    "label": 0
                },
                {
                    "sent": "We are considering this one in the following, so if there goes to zero, you just recover the best element with respect to this measure of risk and if the charges to Infinity you just recover.",
                    "label": 0
                },
                {
                    "sent": "An originalist in matters like you could get just using the prior as your knowledge on this side.",
                    "label": 0
                },
                {
                    "sent": "So in fact the data doesn't modify your estimators, so a common tool that we need in this context is such time lemma.",
                    "label": 0
                },
                {
                    "sent": "So both work that we that we use for our demonstration rely on this one.",
                    "label": 0
                },
                {
                    "sent": "So if you want to generalize it, you need in some sense to get a generalization.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sistema but I won't go deep in deeper into these datums.",
                    "label": 0
                },
                {
                    "sent": "So another point of view of you are that might be also interesting that it's a penalty point of view, but on a bigger set so you can just enlarge that you're considering using for instance, probability over Lambda.",
                    "label": 1
                },
                {
                    "sent": "Then you can define a penalty estimators in the following way.",
                    "label": 0
                },
                {
                    "sent": "So this is the other edge with respect to this distribution where this distribution is an optimized solution of the compromise of a tradeoff between a data fitting.",
                    "label": 0
                },
                {
                    "sent": "Overall the distribution P. In a bit magic term.",
                    "label": 0
                },
                {
                    "sent": "Over all the others.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It depends on the distribution you're using, so in the very case where you're using the callback number divergents to control this penalty.",
                    "label": 0
                },
                {
                    "sent": "You will recover exactly the exponential weight with the prior Pi if you control the distance to between P and Pi with the Carol divergent that I recorded here and again here, the better parameter is exactly the one that was on the previous slide, the one that you were using here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we need.",
                    "label": 0
                },
                {
                    "sent": "We cannot prove a general Oracle inequality for the whole kind of estimators, so we just focused on simple simple case.",
                    "label": 0
                },
                {
                    "sent": "It's going to be fine.",
                    "label": 0
                },
                {
                    "sent": "Structure, so just we can write F at Lambda equals L onda Y plus belanda, where alumna is matrix belong.",
                    "label": 0
                },
                {
                    "sent": "There is a deterministic vector and they're all independent of Y and possibly Lambda could be non countable.",
                    "label": 1
                },
                {
                    "sent": "So simple case is the one of dictionary learning would be with a Lambda equals 0.",
                    "label": 0
                },
                {
                    "sent": "So you can think about that having some feature vectors, F1, FM and then you can try to get a regular inequality over those different family.",
                    "label": 0
                },
                {
                    "sent": "Either the family itself, a feature convex or linear or espers combinations.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, if you remember you get and also the case where the Lambda equals 0, meaning that we got linear transform.",
                    "label": 0
                },
                {
                    "sent": "So our work extends the work by long environment that we're also like.",
                    "label": 0
                },
                {
                    "sent": "Pursued by IKEA.",
                    "label": 0
                },
                {
                    "sent": "Lunacy and legal entity back off over altogether projectors over subspaces of RM.",
                    "label": 1
                },
                {
                    "sent": "Another point of view could be to use diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is so either you recover projectors or you can define shrinking estimators.",
                    "label": 0
                },
                {
                    "sent": "So we're going to see an error that we can use, for instance, Pinsker filters having this formula.",
                    "label": 0
                },
                {
                    "sent": "And that rely on two parameters, so that you are going to get coefficient that decreases from one to zero at a given red that is controlled by those two coefficient.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we need 2 assumptions or two conditions to prove our theorem surface.",
                    "label": 0
                },
                {
                    "sent": "One is at matrices or optical projection and at the bottom there are all in the new space of a Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is basically the kind of setting regarding Barron and the second one is that you need to get matrices that are symmetric positive, semi definite, that all commutes altogether in the family and the commutes also with the covariance of the noise metrics and in the end the vectors belong.",
                    "label": 0
                },
                {
                    "sent": "There should be in the new space of all other element prime.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that we can get done that we call like Pack or ESC pack Bayesian burn.",
                    "label": 0
                },
                {
                    "sent": "And so under this to condition for any prior parties that you can choose, we get the following record inequality.",
                    "label": 0
                },
                {
                    "sent": "So you control the risk of the procedure by a compromise over all the distribution of the integrated risk over the distribution you choosing.",
                    "label": 0
                },
                {
                    "sent": "Plus a penalty term that depends on the product you get an multiply by bit over in.",
                    "label": 0
                },
                {
                    "sent": "So for that you need better to be big enough and depending on the condition you're assuming you need either that it's bigger than the maximum of the variances or that is bigger than item maximum variances.",
                    "label": 0
                },
                {
                    "sent": "So maybe this one is a little complex, but you can think about this theorem as something that could provide you some corollary's.",
                    "label": 0
                },
                {
                    "sent": "So one of them I won't show this one because Sebastian should you watch.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the L1 sparsity scenario, with the one concern, but here, let's get to the simple case where you just find it family.",
                    "label": 0
                },
                {
                    "sent": "So Lambda is just one 2M and you use by as a uniform distribution over this set.",
                    "label": 0
                },
                {
                    "sent": "So you just recover here and Oracle inequality that is you controlling the performance of the over by the best risk over your family plus a logarithmic term in the number of filament you got in your family multiplied by Beethoven.",
                    "label": 0
                },
                {
                    "sent": "Still, with the same constraints on video.",
                    "label": 0
                },
                {
                    "sent": "So indications were Belinda equals zero is just extend the resolution of lung environment.",
                    "label": 0
                },
                {
                    "sent": "And what is good to know is that in the case of homicide astex this term is exactly the price that is that you need to pay.",
                    "label": 0
                },
                {
                    "sent": "In fact it is optimal.",
                    "label": 0
                },
                {
                    "sent": "So you can get better bounded something a constant time this logarithmic term over N.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the last part of the talk is going to be an application of this formula.",
                    "label": 0
                },
                {
                    "sent": "It's going to be for adaptation, so imagine now we are in the almost sadistic setting and we consider the discrete free coefficient of our signal.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that the signal is inside Sobolev bowls for this type of.",
                    "label": 0
                },
                {
                    "sent": "Well, the the geometry of the signal is inside the circle of balls, so I denote them by Alpha and R where R is the radius and Alpha controls the decay of the coefficient.",
                    "label": 0
                },
                {
                    "sent": "And so a reminder serum by being scared from the edges, where in fact the minimax rates over this ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "Is given by linear estimators, so you get just instead of considering all the estimator you can just consider a linear one.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, you can even consider only shrinker shrinkers estimators of the biggest type.",
                    "label": 0
                },
                {
                    "sent": "So this is all this one meaning here that you need to transform your signal in the.",
                    "label": 0
                },
                {
                    "sent": "For your basis you just shrink them with this formula and then you invert it to risk over the signal.",
                    "label": 0
                },
                {
                    "sent": "So here is the index Lambda going to be.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topless store to the Power 2.",
                    "label": 0
                },
                {
                    "sent": "And we will choose to aggregate with your procedure represented before those very elements of physical, physical.",
                    "label": 0
                },
                {
                    "sent": "And now we only need a prior to get the estimators or the prior.",
                    "label": 0
                },
                {
                    "sent": "We're going to use is following you first draw Alpha according to an exponential distribution.",
                    "label": 1
                },
                {
                    "sent": "And then knowing this Alpha, you're going to draw W according to this polynomial distribution, where N Sigma is just the ratio between the noise and.",
                    "label": 0
                },
                {
                    "sent": "Well, the noise level and the number of observation.",
                    "label": 0
                },
                {
                    "sent": "So the theoretical guarantee that we can get in this context is that we can prove that this estimator.",
                    "label": 0
                },
                {
                    "sent": "Is adaptive in the exact minimax sense, uncivilized ellipsoid, meaning that you can achieve the performance without noting here.",
                    "label": 1
                },
                {
                    "sent": "You don't know to know in advance Ian far in the are so you get this kind of result.",
                    "label": 1
                },
                {
                    "sent": "I think that due to the time I won't present deeply in details the practical performances, but we compare our procedure with other well known adaptive methods such as social link by don't know Anna Johnson using short imitation for the threshold as blockstein method.",
                    "label": 0
                },
                {
                    "sent": "Bitonic, I 99 and the last one being the empirical risk minimization.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An on pictures you can see like it's going to be OK, we're going to recover.",
                    "label": 0
                },
                {
                    "sent": "So here is the noisy.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's true.",
                    "label": 0
                },
                {
                    "sent": "Underlying signal here is the noisy signal.",
                    "label": 0
                },
                {
                    "sent": "Here this is our procedure and we got some peace in arms.",
                    "label": 0
                },
                {
                    "sent": "That is pretty reasonable and we compare with the three other Mr Data presented before.",
                    "label": 0
                },
                {
                    "sent": "You can see that we compare quite decently with the your that is the last one, but it's normal because it just optimizing over the whole family.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That would be the minimizer of the unbiased risk and the two other one are quite far away from that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude this talk, I hope I convinced you that we can get some sharper record inequalities for.",
                    "label": 0
                },
                {
                    "sent": "A support of the affinity matters.",
                    "label": 1
                },
                {
                    "sent": "We can produce adaptive results with the.",
                    "label": 1
                },
                {
                    "sent": "We speak to the signals, misses.",
                    "label": 0
                },
                {
                    "sent": "In the case of using Pinsker filters.",
                    "label": 0
                },
                {
                    "sent": "N like pretty reasonable experiment performance where provided.",
                    "label": 0
                },
                {
                    "sent": "So I'm going worked out to weekend.",
                    "label": 1
                },
                {
                    "sent": "The assumptions are still strong in our context and we have used, for instance, symmetric version of the award to do that.",
                    "label": 1
                },
                {
                    "sent": "Another extension would be to focus on the type of noise.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, in this work we only focus on version, but more can be done in in a few months.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in a longer version of the paper or for the software, you can just go online on our website and you find everything.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So you're looking at a heteroskedastic regression problem.",
                    "label": 0
                },
                {
                    "sent": "And if you go back for example to, well, actually from the definition of the estimator, at some point, you know from the paper when you're dealing with the inverse problems, you have to block what's wrong with actually waiting by the Sigma eyes in the definition of the argument.",
                    "label": 0
                },
                {
                    "sent": "So it is sorry.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing the argument, you say.",
                    "label": 0
                },
                {
                    "sent": "In the paper you have problem because you're using the same data for each of your observations, but instead of looking at the convex combinations of the L2 norm, why don't you look at the weighted L2 norm like I mean usually in statistics, when you have heteroskedastic errors for regression, just wait by the by the level of the noise.",
                    "label": 1
                },
                {
                    "sent": "I think we're just saying is some kind of already done in the because you need it inside the estimation of the risk.",
                    "label": 0
                },
                {
                    "sent": "In fact, you need the estimation of the unbiased estimation of the risk, and you need a sigmai somewhere and I guess are already there.",
                    "label": 0
                },
                {
                    "sent": "So for the better you want to be a little different, but.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "You should well.",
                    "label": 0
                },
                {
                    "sent": "It's inside the formula.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's just hidden in the year in the air Lambda at.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank specific.",
                    "label": 0
                }
            ]
        }
    }
}