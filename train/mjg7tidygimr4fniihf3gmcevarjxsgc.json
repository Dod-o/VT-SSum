{
    "id": "mjg7tidygimr4fniihf3gmcevarjxsgc",
    "title": "Embedding Mapping Approaches for Tensor Factorization and Knowledge Graph Modelling",
    "info": {
        "author": [
            "Yinchong Yang, Department of Computer Science, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_yang_graph_modelling/",
    "segmentation": [
        [
            "My name is Andrew Yang from the University of Munich and Seaman soggy, and I'm doing my PhD under the supervision of Professor Pocket."
        ],
        [
            "What I'm going to talk about today is the embedding mapping approaches.",
            "I will start my presentation with the recap of the latent embedding models.",
            "Then I will try to explain why we need these so called embedding mapping mechanisms at all.",
            "Then I will dive into some technical details and show you some ways to perform these embedding mappings, and then I will convince you of the functionality of those models with my experimental results."
        ],
        [
            "So first the latent embedding models the starting point of latent embedding models, is at least in the context of semantic web or knowledge.",
            "Graph is always two.",
            "Represent a knowledge graph as a adjacency matrix or adjacency tensor.",
            "Let us denote these 10s or SY and each arbitrary entry in this tensor Y, which is indexed by J&K, corresponds to a triple.",
            "The entity I over relation K to the entity J.",
            "If this triple is observed in my graph than this entry in this tensor is 1, otherwise it is then zero.",
            "So in the context of machine learning we realized that.",
            "The so called feature space of the entities, namely everything that we can observe on this entity, namely the.",
            "Entire vector which indicates all the links from this entity to all other possible entities.",
            "Such a feature space has some very bad characteristics.",
            "First, this is highly dimensional because it has exactly the same number of dimension as the set of all the entities, and second, it is very.",
            "It is highly sparse as we can see on this example, though on the left side we have a really busy graph on the right side.",
            "Side we see the really sparse tensor with very few ones in it.",
            "So to solve this problem this data situation, we need something new, not something new IT is something special which is called the latent embedding models.",
            "So according to our interpretation this these latent building models learn two things.",
            "The first it learns for each entity in my graph, a latent embedding and such a latent embedding is first, it is real instead of binary as we saw in the feature space.",
            "Second, it is the low dimensional instead of high dimensional and the second thing in this model learns is a interaction rule that can combine all those latent embeddings latent vectors together.",
            "To reconstruct this very entry in my tenzer, if we look at on the right side, we have this his car model from our group.",
            "1st, in order to reconstruct to model a an entry IJK we need on the bottom one vector in corresponding to entity I won."
        ],
        [
            "Actor corresponding to entity J and one matrix for the relation type K. And if we do a vector matrix vector product, we can always.",
            "We can always construct this entry in the 10s or igk.",
            "So there are other possibilities to do such to perform such latent bedding learning, such as the Powerhouse Park, PCP, factorization or Tucker factorisation module.",
            "Network and so on and so on, and all of these models have shown very good performances in the tasks such as recommender systems and knowledge graph modeling.",
            "So, so far so good.",
            "Problem is up to now.",
            "We have always assumed that my knowledge graph and my tensor is static.",
            "That is not always the case.",
            "It might happen that there are new entities being observed and being appended to my database and the problem is, how can we derive for those new entities knew latent embeddings and vectors or matrices?",
            "So there are two very.",
            "Naive way to do that.",
            "First we can for."
        ],
        [
            "What about my old factorization model and try a new one and I would say this might be the best solution that we have in terms of the model quality but not in terms of efficiency because it is very expensive to train such a factorization model.",
            "So the second trick would be that we can play some K nearest neighbor games.",
            "I will not go into the details, we can talk about that in our Q&A session if that should.",
            "Interest there anyone so so?",
            "This is the problem and we propose this embedding mapping approach as we call the Emma and such a embedding mapping mechanism can map each new entity and not only knew but also old one from its feature space to the latent embedding space.",
            "Now let us see how that should work."
        ],
        [
            "First, there are some good news.",
            "We realized that there exists a class of factorization machines where we get these embedding mapping mechanisms for free from the factorization model itself.",
            "If we look at the simplest case, the SVD factorization.",
            "We can always, if we write this equals UDV transpose, we have no problem writing EU matrix to the left side and X to the right and.",
            "So then we see this this matrix produced by the DV transpose and pseudoinverse is exactly this embedding mapping matrix that we need because it Maps each role in X to each row in U and this U is could be interpreted as the latent embedding for each entity in and it is then generally assumed that if we have a new, which is exactly this and this.",
            "Relation should also apply, so the same is also for Tucker, but the first we should do right, the Tucker model in matrix form actual skip the details here.",
            "And.",
            "So the bad news is other models such as Crisco or the modular neural network.",
            "These models do not enjoy such advantage.",
            "Unfortunately, it is exactly these more advanced models that show lately better performances in latest studies, so we need something more general and applicable for those models as well.",
            "So the."
        ],
        [
            "First idea that we have is really intuitive.",
            "We do it in two steps.",
            "In the first step, we just train the factorization model just as usual.",
            "For example, we factorize this tensor in one matrix, one tensor and another matrix, so."
        ],
        [
            "In the second step, we fit a linear regression and in this linear regression the independent variables are the feature matrix for each entity and the dependent variable is the latent embedding of this of each entity.",
            "If we have such a linear regression relationship."
        ],
        [
            "Then we can.",
            "Also we can always assume that if there is a new entity being appended to the database, we can apply this linear regression relation to this new feature matrix and have a linear estimate of its latent vector and.",
            "So in the terminology of machine learning we have now 2 cost functions.",
            "The first is the factorization error and then the embedding mapping.",
            "This linear regression error.",
            "So in order to try to reduce the number of cost functions, we come up with another idea which is just.",
            "Slight modification of the first one.",
            "That means we fit exactly the same linear regression."
        ],
        [
            "But instead of fitting this linear regression only one time after the factorization completes, we integrate this linear regression into the factorization itself.",
            "That means after each iteration of the factorization learning, we fit this linear regression and we replace the latent embedding with is at least square estimates and by doing so we actually set the linear regression.",
            "This mapping error 20.",
            "And from the from the in the next iteration, then some other algorithms such as the else and the gradient approaches that we use to train the factorization.",
            "So proceed from the least square estimates.",
            "So let me see OK."
        ],
        [
            "The third approach is a little bit more complicated because it involves a new aspect.",
            "By looking at the factorization machines as neural networks.",
            "In such neural networks we have as input of the neural network, then the latent embeddings, the corresponding to entity IJ, and relation type K and the output of the neural network, which is is a. Scala is exactly, this is IJK.",
            "So if we.",
            "Extend."
        ],
        [
            "This point of view with our embedding mapping mechanism, we realize that it is nothing more complicated than adding one more linear activated layer from from.",
            "From bottom to the neural networks and this time the inputs of the neural network, they are not not the latent embedding feature of the entities, but they are the they are the feature vector of the latent of the entities.",
            "And by doing so, we actually eliminate the embedding mapping because.",
            "Mapping error actually because we do not store those latent vectors anymore explicitly.",
            "But we always encode those latent embeddings as the multiplication of the feature vector of the entities and the mapping matrix M here.",
            "So, as you might also have realized that such a model is also very complicated, it's very expensive to train.",
            "But the good news is always that we can train such a model from end to end."
        ],
        [
            "So now come the experiments.",
            "We actually run three different experiments using our approaches.",
            "First, we test our models on the movie length 100K data set, which we transform into a classical binary user item matrix.",
            "So the task was we would like to make movie recommendations to new users and as well as making user recommendations to new.",
            "Movies so.",
            "OK, and we also assume that such knew users and such new movies there appended into the database with incomplete information.",
            "That's why we do such a massive we mask proportion of from zero to 50% of all the entities that these users and these movies already have.",
            "And 1st we map we map those new users and movies into the latent space and try to make.",
            "Recommendations and see how good they could be so also I shall skip the details by just saying that for GNU user.",
            "For making movie recommendations to users, the post mapping work and the back propagation, they work better than the hunting algorithm and the for making recommendations.",
            "User recommendation to movies, the back propagation and helping algorithm work better than the post mapping.",
            "And we apply it."
        ],
        [
            "Actually, the same idea to attend a case where we sampled different small fractions from the Freebase data set and we try to make new link prediction for new entities and we measure for that.",
            "We also assume that 20% of the links of all the new entities they are unknown and try to reconstruct them.",
            "So we measure the performance is using the RCA UPC and so the first information from these experiments that the trend is the multiple neural network perform better than the race car, which confirms another paper from our group last year and 2nd the hashing algorithm and the back propagation work better than the these are the two more complicated ones.",
            "They work better than the.",
            "The intuitive poster mapping and most encouraging from this experiment is the combination of the multi neural network as factorization model and the back propagation as mapping model.",
            "This combination can give us performances close even to those that we get from retraining the data, retraining the model, sorry."
        ],
        [
            "So the third experiment, with the last two, actually we focused on the prediction quality of the latent embedding models.",
            "But with this experiment we want to have a closer look at the embedding themselves.",
            "In other words, we want to verify the interpretability of those latent spaces, but unfortunately there are not so many datasets that would allow us such luxury.",
            "We only found this one data set, which is also very well.",
            "It is well documented and simple to understand, so the story goes like this.",
            "There are three types of amino acids and five recipes describing how to mix them and this information we store them.",
            "In the matrix Y and then after mixing those mean Oasis.",
            "Then we have actually 5 samples, 5 samples and we measure those five samples using two different ways which will give us a 10s of.",
            "561 and 201 so the other says if we run a rank three CP factorization on this 10s or X then and and getting them three matrices of shape five, 361, three and 201 three, then the first matrix of 5 three would correlate with the recipe matrix columnwise almost perfectly.",
            "Of course, we first we could first verify that that's what the other set is true, and then we see our possibility at last to.",
            "To test if the mapped if the latent embedding that we derive from mapping, are they as good as those that we get from the factorization model so?",
            "What if I say that I have now a new recipe?",
            "This this blue one?",
            "The left what if I have a new recipe and I have a new mixture that I get from from this recipe and then we Max we map.",
            "We map this feature matrix using our embedding mapping functions into the latent space and append these mapped mapped embedding vectors to the old embedding vectors and the problem is.",
            "Is this maybe better use this?",
            "I'm afraid it is not working.",
            "So if we append these mapped vector into the old ones, are this appended?",
            "Is this appended matrix still correlating with the recipe matrix with which is again also appended with my new recipe as well as before?",
            "So the result was very encouraging.",
            "First, we try to leave.",
            "Well, we leave one recipe out to hold out and train the factorization and mapping model with four recipes and samples and we get a person correlation coefficient of oh .99.",
            "And if we leave two recipes out and train the model with three, then we get correlation of oh .991 so.",
            "So far we think with such an experiment, we can claim that the latent embeddings that we get from mapping are as well as those that we get from factorization."
        ],
        [
            "So conclusions and outlooks.",
            "So what this embedding mapping does these approaches give us the possibility to to derive the latent embeddings for new entities being added to the database in real time.",
            "And there are two classes of such technical techniques.",
            "The first other explicit mapping where we define mapping model with the mapping error which we try to optimize during the training and the second class there is the implicit mapping.",
            "For example, we have this new point of view of looking at the factorization and mapping as entire neural network model and the training as a whole.",
            "In this case.",
            "Then we do not define any explicit mapping error cost function and also do not forget that.",
            "For some models that we get, this embedding mapping for free.",
            "So with the experiments we show that we can map those new entities into the latent space and do a very good recommendation and knowledge graph link prediction.",
            "Also with this amino acid data set, we see that the interpretability could also of these embedding vectors could also be verified.",
            "As for future work, as you might have already seen, there are some weak points in our model.",
            "First, we have this very huge bottleneck cause if we want to map the entire feature space into the latent space, we actually need to unfold the 10s or and that means the input of the model shell increase quadratically with the size of the entity set.",
            "And in order to solve that problem, we are right now working on some solutions have some.",
            "Very good first results already and.",
            "The second be to combine the work of this Gardner paper from 2010, which he also did a very similar way of mapping, but what he does is to map the content information into the feature space, but our model is exactly what follows actually from the feature space to the latent space we see a very interesting combination of our work here so."
        ],
        [
            "Some references and thank you."
        ],
        [
            "Very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Andrew Yang from the University of Munich and Seaman soggy, and I'm doing my PhD under the supervision of Professor Pocket.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm going to talk about today is the embedding mapping approaches.",
                    "label": 1
                },
                {
                    "sent": "I will start my presentation with the recap of the latent embedding models.",
                    "label": 0
                },
                {
                    "sent": "Then I will try to explain why we need these so called embedding mapping mechanisms at all.",
                    "label": 0
                },
                {
                    "sent": "Then I will dive into some technical details and show you some ways to perform these embedding mappings, and then I will convince you of the functionality of those models with my experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first the latent embedding models the starting point of latent embedding models, is at least in the context of semantic web or knowledge.",
                    "label": 1
                },
                {
                    "sent": "Graph is always two.",
                    "label": 0
                },
                {
                    "sent": "Represent a knowledge graph as a adjacency matrix or adjacency tensor.",
                    "label": 0
                },
                {
                    "sent": "Let us denote these 10s or SY and each arbitrary entry in this tensor Y, which is indexed by J&K, corresponds to a triple.",
                    "label": 1
                },
                {
                    "sent": "The entity I over relation K to the entity J.",
                    "label": 0
                },
                {
                    "sent": "If this triple is observed in my graph than this entry in this tensor is 1, otherwise it is then zero.",
                    "label": 0
                },
                {
                    "sent": "So in the context of machine learning we realized that.",
                    "label": 0
                },
                {
                    "sent": "The so called feature space of the entities, namely everything that we can observe on this entity, namely the.",
                    "label": 0
                },
                {
                    "sent": "Entire vector which indicates all the links from this entity to all other possible entities.",
                    "label": 0
                },
                {
                    "sent": "Such a feature space has some very bad characteristics.",
                    "label": 0
                },
                {
                    "sent": "First, this is highly dimensional because it has exactly the same number of dimension as the set of all the entities, and second, it is very.",
                    "label": 0
                },
                {
                    "sent": "It is highly sparse as we can see on this example, though on the left side we have a really busy graph on the right side.",
                    "label": 0
                },
                {
                    "sent": "Side we see the really sparse tensor with very few ones in it.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem this data situation, we need something new, not something new IT is something special which is called the latent embedding models.",
                    "label": 0
                },
                {
                    "sent": "So according to our interpretation this these latent building models learn two things.",
                    "label": 0
                },
                {
                    "sent": "The first it learns for each entity in my graph, a latent embedding and such a latent embedding is first, it is real instead of binary as we saw in the feature space.",
                    "label": 0
                },
                {
                    "sent": "Second, it is the low dimensional instead of high dimensional and the second thing in this model learns is a interaction rule that can combine all those latent embeddings latent vectors together.",
                    "label": 0
                },
                {
                    "sent": "To reconstruct this very entry in my tenzer, if we look at on the right side, we have this his car model from our group.",
                    "label": 0
                },
                {
                    "sent": "1st, in order to reconstruct to model a an entry IJK we need on the bottom one vector in corresponding to entity I won.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actor corresponding to entity J and one matrix for the relation type K. And if we do a vector matrix vector product, we can always.",
                    "label": 0
                },
                {
                    "sent": "We can always construct this entry in the 10s or igk.",
                    "label": 0
                },
                {
                    "sent": "So there are other possibilities to do such to perform such latent bedding learning, such as the Powerhouse Park, PCP, factorization or Tucker factorisation module.",
                    "label": 0
                },
                {
                    "sent": "Network and so on and so on, and all of these models have shown very good performances in the tasks such as recommender systems and knowledge graph modeling.",
                    "label": 0
                },
                {
                    "sent": "So, so far so good.",
                    "label": 0
                },
                {
                    "sent": "Problem is up to now.",
                    "label": 0
                },
                {
                    "sent": "We have always assumed that my knowledge graph and my tensor is static.",
                    "label": 0
                },
                {
                    "sent": "That is not always the case.",
                    "label": 0
                },
                {
                    "sent": "It might happen that there are new entities being observed and being appended to my database and the problem is, how can we derive for those new entities knew latent embeddings and vectors or matrices?",
                    "label": 0
                },
                {
                    "sent": "So there are two very.",
                    "label": 0
                },
                {
                    "sent": "Naive way to do that.",
                    "label": 0
                },
                {
                    "sent": "First we can for.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about my old factorization model and try a new one and I would say this might be the best solution that we have in terms of the model quality but not in terms of efficiency because it is very expensive to train such a factorization model.",
                    "label": 0
                },
                {
                    "sent": "So the second trick would be that we can play some K nearest neighbor games.",
                    "label": 0
                },
                {
                    "sent": "I will not go into the details, we can talk about that in our Q&A session if that should.",
                    "label": 0
                },
                {
                    "sent": "Interest there anyone so so?",
                    "label": 0
                },
                {
                    "sent": "This is the problem and we propose this embedding mapping approach as we call the Emma and such a embedding mapping mechanism can map each new entity and not only knew but also old one from its feature space to the latent embedding space.",
                    "label": 1
                },
                {
                    "sent": "Now let us see how that should work.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, there are some good news.",
                    "label": 1
                },
                {
                    "sent": "We realized that there exists a class of factorization machines where we get these embedding mapping mechanisms for free from the factorization model itself.",
                    "label": 0
                },
                {
                    "sent": "If we look at the simplest case, the SVD factorization.",
                    "label": 0
                },
                {
                    "sent": "We can always, if we write this equals UDV transpose, we have no problem writing EU matrix to the left side and X to the right and.",
                    "label": 0
                },
                {
                    "sent": "So then we see this this matrix produced by the DV transpose and pseudoinverse is exactly this embedding mapping matrix that we need because it Maps each role in X to each row in U and this U is could be interpreted as the latent embedding for each entity in and it is then generally assumed that if we have a new, which is exactly this and this.",
                    "label": 0
                },
                {
                    "sent": "Relation should also apply, so the same is also for Tucker, but the first we should do right, the Tucker model in matrix form actual skip the details here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the bad news is other models such as Crisco or the modular neural network.",
                    "label": 1
                },
                {
                    "sent": "These models do not enjoy such advantage.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, it is exactly these more advanced models that show lately better performances in latest studies, so we need something more general and applicable for those models as well.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First idea that we have is really intuitive.",
                    "label": 0
                },
                {
                    "sent": "We do it in two steps.",
                    "label": 0
                },
                {
                    "sent": "In the first step, we just train the factorization model just as usual.",
                    "label": 1
                },
                {
                    "sent": "For example, we factorize this tensor in one matrix, one tensor and another matrix, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the second step, we fit a linear regression and in this linear regression the independent variables are the feature matrix for each entity and the dependent variable is the latent embedding of this of each entity.",
                    "label": 0
                },
                {
                    "sent": "If we have such a linear regression relationship.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "Also we can always assume that if there is a new entity being appended to the database, we can apply this linear regression relation to this new feature matrix and have a linear estimate of its latent vector and.",
                    "label": 1
                },
                {
                    "sent": "So in the terminology of machine learning we have now 2 cost functions.",
                    "label": 1
                },
                {
                    "sent": "The first is the factorization error and then the embedding mapping.",
                    "label": 0
                },
                {
                    "sent": "This linear regression error.",
                    "label": 1
                },
                {
                    "sent": "So in order to try to reduce the number of cost functions, we come up with another idea which is just.",
                    "label": 0
                },
                {
                    "sent": "Slight modification of the first one.",
                    "label": 0
                },
                {
                    "sent": "That means we fit exactly the same linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But instead of fitting this linear regression only one time after the factorization completes, we integrate this linear regression into the factorization itself.",
                    "label": 1
                },
                {
                    "sent": "That means after each iteration of the factorization learning, we fit this linear regression and we replace the latent embedding with is at least square estimates and by doing so we actually set the linear regression.",
                    "label": 1
                },
                {
                    "sent": "This mapping error 20.",
                    "label": 0
                },
                {
                    "sent": "And from the from the in the next iteration, then some other algorithms such as the else and the gradient approaches that we use to train the factorization.",
                    "label": 0
                },
                {
                    "sent": "So proceed from the least square estimates.",
                    "label": 0
                },
                {
                    "sent": "So let me see OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third approach is a little bit more complicated because it involves a new aspect.",
                    "label": 0
                },
                {
                    "sent": "By looking at the factorization machines as neural networks.",
                    "label": 1
                },
                {
                    "sent": "In such neural networks we have as input of the neural network, then the latent embeddings, the corresponding to entity IJ, and relation type K and the output of the neural network, which is is a. Scala is exactly, this is IJK.",
                    "label": 1
                },
                {
                    "sent": "So if we.",
                    "label": 0
                },
                {
                    "sent": "Extend.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This point of view with our embedding mapping mechanism, we realize that it is nothing more complicated than adding one more linear activated layer from from.",
                    "label": 0
                },
                {
                    "sent": "From bottom to the neural networks and this time the inputs of the neural network, they are not not the latent embedding feature of the entities, but they are the they are the feature vector of the latent of the entities.",
                    "label": 0
                },
                {
                    "sent": "And by doing so, we actually eliminate the embedding mapping because.",
                    "label": 0
                },
                {
                    "sent": "Mapping error actually because we do not store those latent vectors anymore explicitly.",
                    "label": 0
                },
                {
                    "sent": "But we always encode those latent embeddings as the multiplication of the feature vector of the entities and the mapping matrix M here.",
                    "label": 0
                },
                {
                    "sent": "So, as you might also have realized that such a model is also very complicated, it's very expensive to train.",
                    "label": 0
                },
                {
                    "sent": "But the good news is always that we can train such a model from end to end.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now come the experiments.",
                    "label": 0
                },
                {
                    "sent": "We actually run three different experiments using our approaches.",
                    "label": 0
                },
                {
                    "sent": "First, we test our models on the movie length 100K data set, which we transform into a classical binary user item matrix.",
                    "label": 0
                },
                {
                    "sent": "So the task was we would like to make movie recommendations to new users and as well as making user recommendations to new.",
                    "label": 1
                },
                {
                    "sent": "Movies so.",
                    "label": 1
                },
                {
                    "sent": "OK, and we also assume that such knew users and such new movies there appended into the database with incomplete information.",
                    "label": 1
                },
                {
                    "sent": "That's why we do such a massive we mask proportion of from zero to 50% of all the entities that these users and these movies already have.",
                    "label": 1
                },
                {
                    "sent": "And 1st we map we map those new users and movies into the latent space and try to make.",
                    "label": 0
                },
                {
                    "sent": "Recommendations and see how good they could be so also I shall skip the details by just saying that for GNU user.",
                    "label": 0
                },
                {
                    "sent": "For making movie recommendations to users, the post mapping work and the back propagation, they work better than the hunting algorithm and the for making recommendations.",
                    "label": 0
                },
                {
                    "sent": "User recommendation to movies, the back propagation and helping algorithm work better than the post mapping.",
                    "label": 0
                },
                {
                    "sent": "And we apply it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, the same idea to attend a case where we sampled different small fractions from the Freebase data set and we try to make new link prediction for new entities and we measure for that.",
                    "label": 1
                },
                {
                    "sent": "We also assume that 20% of the links of all the new entities they are unknown and try to reconstruct them.",
                    "label": 0
                },
                {
                    "sent": "So we measure the performance is using the RCA UPC and so the first information from these experiments that the trend is the multiple neural network perform better than the race car, which confirms another paper from our group last year and 2nd the hashing algorithm and the back propagation work better than the these are the two more complicated ones.",
                    "label": 0
                },
                {
                    "sent": "They work better than the.",
                    "label": 0
                },
                {
                    "sent": "The intuitive poster mapping and most encouraging from this experiment is the combination of the multi neural network as factorization model and the back propagation as mapping model.",
                    "label": 0
                },
                {
                    "sent": "This combination can give us performances close even to those that we get from retraining the data, retraining the model, sorry.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the third experiment, with the last two, actually we focused on the prediction quality of the latent embedding models.",
                    "label": 1
                },
                {
                    "sent": "But with this experiment we want to have a closer look at the embedding themselves.",
                    "label": 0
                },
                {
                    "sent": "In other words, we want to verify the interpretability of those latent spaces, but unfortunately there are not so many datasets that would allow us such luxury.",
                    "label": 0
                },
                {
                    "sent": "We only found this one data set, which is also very well.",
                    "label": 0
                },
                {
                    "sent": "It is well documented and simple to understand, so the story goes like this.",
                    "label": 1
                },
                {
                    "sent": "There are three types of amino acids and five recipes describing how to mix them and this information we store them.",
                    "label": 0
                },
                {
                    "sent": "In the matrix Y and then after mixing those mean Oasis.",
                    "label": 0
                },
                {
                    "sent": "Then we have actually 5 samples, 5 samples and we measure those five samples using two different ways which will give us a 10s of.",
                    "label": 0
                },
                {
                    "sent": "561 and 201 so the other says if we run a rank three CP factorization on this 10s or X then and and getting them three matrices of shape five, 361, three and 201 three, then the first matrix of 5 three would correlate with the recipe matrix columnwise almost perfectly.",
                    "label": 1
                },
                {
                    "sent": "Of course, we first we could first verify that that's what the other set is true, and then we see our possibility at last to.",
                    "label": 0
                },
                {
                    "sent": "To test if the mapped if the latent embedding that we derive from mapping, are they as good as those that we get from the factorization model so?",
                    "label": 0
                },
                {
                    "sent": "What if I say that I have now a new recipe?",
                    "label": 0
                },
                {
                    "sent": "This this blue one?",
                    "label": 1
                },
                {
                    "sent": "The left what if I have a new recipe and I have a new mixture that I get from from this recipe and then we Max we map.",
                    "label": 0
                },
                {
                    "sent": "We map this feature matrix using our embedding mapping functions into the latent space and append these mapped mapped embedding vectors to the old embedding vectors and the problem is.",
                    "label": 0
                },
                {
                    "sent": "Is this maybe better use this?",
                    "label": 0
                },
                {
                    "sent": "I'm afraid it is not working.",
                    "label": 0
                },
                {
                    "sent": "So if we append these mapped vector into the old ones, are this appended?",
                    "label": 0
                },
                {
                    "sent": "Is this appended matrix still correlating with the recipe matrix with which is again also appended with my new recipe as well as before?",
                    "label": 0
                },
                {
                    "sent": "So the result was very encouraging.",
                    "label": 0
                },
                {
                    "sent": "First, we try to leave.",
                    "label": 0
                },
                {
                    "sent": "Well, we leave one recipe out to hold out and train the factorization and mapping model with four recipes and samples and we get a person correlation coefficient of oh .99.",
                    "label": 0
                },
                {
                    "sent": "And if we leave two recipes out and train the model with three, then we get correlation of oh .991 so.",
                    "label": 0
                },
                {
                    "sent": "So far we think with such an experiment, we can claim that the latent embeddings that we get from mapping are as well as those that we get from factorization.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusions and outlooks.",
                    "label": 0
                },
                {
                    "sent": "So what this embedding mapping does these approaches give us the possibility to to derive the latent embeddings for new entities being added to the database in real time.",
                    "label": 1
                },
                {
                    "sent": "And there are two classes of such technical techniques.",
                    "label": 0
                },
                {
                    "sent": "The first other explicit mapping where we define mapping model with the mapping error which we try to optimize during the training and the second class there is the implicit mapping.",
                    "label": 0
                },
                {
                    "sent": "For example, we have this new point of view of looking at the factorization and mapping as entire neural network model and the training as a whole.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "Then we do not define any explicit mapping error cost function and also do not forget that.",
                    "label": 1
                },
                {
                    "sent": "For some models that we get, this embedding mapping for free.",
                    "label": 0
                },
                {
                    "sent": "So with the experiments we show that we can map those new entities into the latent space and do a very good recommendation and knowledge graph link prediction.",
                    "label": 0
                },
                {
                    "sent": "Also with this amino acid data set, we see that the interpretability could also of these embedding vectors could also be verified.",
                    "label": 0
                },
                {
                    "sent": "As for future work, as you might have already seen, there are some weak points in our model.",
                    "label": 0
                },
                {
                    "sent": "First, we have this very huge bottleneck cause if we want to map the entire feature space into the latent space, we actually need to unfold the 10s or and that means the input of the model shell increase quadratically with the size of the entity set.",
                    "label": 0
                },
                {
                    "sent": "And in order to solve that problem, we are right now working on some solutions have some.",
                    "label": 0
                },
                {
                    "sent": "Very good first results already and.",
                    "label": 0
                },
                {
                    "sent": "The second be to combine the work of this Gardner paper from 2010, which he also did a very similar way of mapping, but what he does is to map the content information into the feature space, but our model is exactly what follows actually from the feature space to the latent space we see a very interesting combination of our work here so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some references and thank you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}