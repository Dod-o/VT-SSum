{
    "id": "ssszl4eqac5f5umy4ge736u7a55scsm3",
    "title": "Training SVM with Indefinite Kernels",
    "info": {
        "author": [
            "Jieping Ye, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "Aug. 6, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_ye_tsvm/",
    "segmentation": [
        [
            "Yeah, good morning.",
            "So this work is about training, SVM wanna kernel matrix is non PSD.",
            "This is a joint work with my students generally well both from."
        ],
        [
            "At least on a state.",
            "So the last speaker very good overview of kernel SVN and multiple kernel learning.",
            "So this makes my talk much easier.",
            "So SVN has been applied successfully in many applications and over key element in SVN is the kernel matrix.",
            "And the company is required the kernel matrix is positive semidefinite PSD.",
            "That means in the matrix does not contain any negative eigenvalues, at least zero.",
            "So the PSD property of the kernel matrix ensure the existence of a reproducing kernel Hilbert space and also resulted in the convex quadratic program.",
            "So here why is label and K is kernel matrix sees the parameter?",
            "So if K is a PhD, learning is a convex and we can find global optimal solution using stand techniques such as in."
        ],
        [
            "Yellow point method.",
            "So however many applications we may have this kernel match to be non PSD indefinite.",
            "For example in bioinformatics the similarity matrix based on sequence pairwise alignment like protein sequence alignment may not be PSD.",
            "It may contain negative eigenvalues.",
            "So there have been many prior work on training SVN.",
            "One kernel matrix is not PSD PSD, so there are some work.",
            "Try to modify the SVM formulation.",
            "User general form of recent theory.",
            "And we also many other work which useless original SVM formulation bug may apply spectral transformation.",
            "Put Transformer indefinite kernel matrix to a PSD matrix.",
            "So for example this is the noise is 1 popular approach.",
            "It works by first computing all eigenvalues of kernel matrix and then remove the next eigenvalues, essentially training the next group OK and Flipper is also not approach which change the sign of the negative values to be positive and the shift is we add positive constant over.",
            "Values to make all the invalid nonnegative.",
            "So one limitation of this approach is the learning of the kernel matrix, and SVN is actually coupled."
        ],
        [
            "OK, so you need to work with.",
            "We start a new approach which will learn the kernel matrix and list SVM classifier simultaneously.",
            "So essentially, in this work we show the invariant kernel which is provided right?",
            "So we are given in dental matrix and we assume the indefinite kernel is noisy observation of the true but unknown PSD matrix with complex matrix.",
            "And I will try to learn that is a kernel matrix and least SVM classifier simultaneously and we show this problem can be formulated as these semidefinite quadratic program.",
            "And we propose the algorithm.",
            "It's following the idea of cutting plane to solve this efficiently.",
            "And we conduct this convergence analysis.",
            "At least formulation still computationally expensive to solve, so we propose a polling strategy to improve the efficiency of algorithm an in, and we show the close connection between these formulation list multiple formation and multiple."
        ],
        [
            "Cannelloni.",
            "So first I will give a overview of SVM, so we're given a kernel matrix K and we have.",
            "Why is class label an while capitalizing the diagonal matrix of class label an SVN deal form of 1 SVN we have leased maximization problems.",
            "So here case lock key metric case, the kernel matrix and which is commonly required to be PSD, see the parameter wise label.",
            "So in traditional SVM case to be required to be non PSD, PSD, so in all cases were given a case zero matrix which is non PSD.",
            "And we try to learn.",
            "We try to learn.",
            "You can imagine K. Even though we've seen SVM formulation, so the formula for way uses this one, we try to use the regular version of SVM formulation.",
            "The difference between this and this original SVS, we have leased additional term and the first part is the sense as VM.",
            "So essentially in this formulation we we try to compute a noisy observation of the case yellow which is K case lower our kernel matrix unknown, we try to compute and Ocasio is given.",
            "So we try to find the kernel matrix K which maximize margin SVM.",
            "This part and simultaneously which had minimized.",
            "Creation we decay in our our given kernel matrix case.",
            "Yellow is indefinite.",
            "And the lowest, the permit which controller balance right between SVN this maximization margin and also the least minimisation of difference between at least K and our original."
        ],
        [
            "Quesadilla matrix.",
            "So we try to solve this min Max optimization here.",
            "Always do available is a vector OK is a kernel matrix.",
            "And this problem turns out to be a.",
            "It's a non smooth, nonsmooth than in previous work.",
            "People try to using this smoothing technique, they add some sometime which try to make this optimized, smooth and so the solution may not be.",
            "Actually, it's approximation is not exact.",
            "And he needs a walkway proposed to solve this problem using idea flow from cutting plane.",
            "So we define the objective sour K this part.",
            "And we define this minimize K over this as function this part.",
            "Dusty, so we change the problem 4 to be this one this one.",
            "So maximize Alpha and notify this minimal trustee.",
            "So if the minimisation over K is tear, this means this function as our K is larger or equal to T for any cake because this minimum is is T, so it must be larger or equal to T for any K. So it is where we make a list object function linear simple, but we have this.",
            "We have this complex constraint and the number of consumer K here depends on a number of least possible kernel matrix K is invented.",
            "And we follow idea for chronic pain to solve this problem.",
            "Essentially for initially will give a kernel matrix KOK, so we have only one constraint.",
            "Then we can solve this maximization.",
            "We have only one to three constraint and linear object.",
            "If we can solve this using this stand technique from.",
            "In this case we use the technique flow mosaic.",
            "Now we solve, we get T and Alpha and we try to find a new new matrix K which violates consider most essentially try to minimize this function in terms of OK and we get another constraint K matrix.",
            "Now we have two kernel matrix.",
            "Two constraint.",
            "We solve this problem again and will repeat until convergence."
        ],
        [
            "So we have two step two key step.",
            "The first step is we forgiven collection of kernel matrix.",
            "We call this localizing set.",
            "K what KP we solve.",
            "Lisa quadratic constraint linear program to get T another so we call intermediate solution path.",
            "After we get to know each other, we try to minimize this function.",
            "Essentially, we try to find the key metrics which violate this consumer most.",
            "So computer this we get a new local's instead of K and only piano convergence."
        ],
        [
            "So next we go through this two step in detail.",
            "In the first step.",
            "If T and always given, so we assume T and always given and we try to find the optimal K which valid the consumer most.",
            "And essentially we try to minimize this function as function in terms of cases always given.",
            "And the solution actually from last year by Lausanne his colleague is given by a close form.",
            "Is KCL matrix is given an applause Lisa?",
            "Why is the diagonal matrix, which consist of the class labels and 10s otherwise the solution?",
            "Then we try to truncate, pluses, truncate, essentially remove all the negative values.",
            "Only keep the positive eigenvalues so it's very easy to get and after we get this kernel matrix case star.",
            "Now we track this constraint is a Lt is less than equal to S advertise this new case dot matrix.",
            "If this satisfied then it shows that we already got our global optimal solution and then we're done.",
            "Otherwise if this is valid then we added this case 02 hour set localising set so OK make the set will increase by one then we repeat.",
            "So this survey is quite."
        ],
        [
            "Easy to implement when in the second step for a given set of kernel matrix K for K12 KP we have P constraint and we need to solve T ANOTHER.",
            "So this one is quadratic constraint.",
            "This is quadratic constraint and linear program and we use that software from msek to solve this problem.",
            "So all the key observation here is the complexity of this problem.",
            "Is depends on the number of constraints here.",
            "So P the set of localized inside K if K is large, learning is much more expensive to solve."
        ],
        [
            "So two controller convergence we need about the lower bound of objective and use the gap between these these bonds to control convergence.",
            "So step one, we are given over and T we solve the optimal K matrix by minimizing this object function.",
            "Then we can keep track of all the objects functions so far until iteration I will choose the maximum at least will be lower bound control easily show.",
            "And similarly, given a given is a localized instead K, we can compute optimal Alpha T and using these tears up."
        ],
        [
            "And we can show at least lower bound.",
            "Always increase an upon always decrease and the gap between these two bonds will actually come close to each other and it's guaranteed the algorithm will converge to the optimal solution and we can use the gap between this low on board controller to check the convergence of the algorithm."
        ],
        [
            "So I will skip the details here.",
            "So as I mentioned, one local limitation of this approach is.",
            "Is when we solve this quadratic linear program.",
            "If the number of constraint or the size of the suitcase large length will be expensive to solve.",
            "And the number of a quadratic constraint increased by one at each iteration, because each time we get a new kernel matrix and we add into this localized set, so the size will increase.",
            "So after 200 iterations, little size of the set K is 200 and we have 200 quadratic constraint and this really expensive solve.",
            "It turns out we had easily solved this problem by actually using disappointing.",
            "When we solve this quadratic linear program, it turns out many of the quadratic constraint inactive.",
            "It means the equality is not satisfied.",
            "Then we can effectively prone leads in active constraint."
        ],
        [
            "So assume Lisa Ki is the current localising set an we can decompose K into two parts is Elise is active part means the equality is satisfied and the other one is is the inactive constraint is it's the quality is not satisfied so it's less than this value.",
            "And if we do not apply pulling strategy, the new localized set is just the unit of the previous local set and this new kernel matrix zero.",
            "We add kernel matrix one by one at each iteration.",
            "But if we apply pruning strategy then we can remove all the inactive constraint.",
            "So using this followed, it turns out disappointing strategy will improve cost efficiency, improve efficiency while retaining the convergence.",
            "Obligation.",
            "So we'll show some results."
        ],
        [
            "Nice spot.",
            "So finally we should close connection between this formulation and multiple kernel learning languages.",
            "In 2004 Proposal, the following formulation for learning convex linear combination of a given kernel matrices were given up equal matrices and we try to find a linear combination.",
            "Here CI is coefficient follow for lesco metrics and we can solve this problem efficiently.",
            "Using queues could be or semi infinite program.",
            "So in our case, at each intermediate step we have assumed we have localized set which consists of P kernel matrices which is K12 KP and we have Lisa KO is the indicator matrix.",
            "We can compute the difference between Ki and K0 so it knows us UI.",
            "So that's the deviation between K and our original case zero matrix.",
            "Long we can consider a glass version of this original multiple kernel learning formulation.",
            "The difference here is we have this penalty term.",
            "So we minimize the deviation between RKI and our original matrix and the way salicylates it I and lowest Lisa control parameter.",
            "So these two are very similar and we have at least 10.",
            "So we can actually show that our.",
            "Proposed formation for kernel learning with indefinite kernel is equivalent to 17.",
            "OK, so we can view or QC LP formulation as extension of multiple cleaning with additional regularization.",
            "So Lapete consequence of this fact is really already many, many work on multiple cleaning.",
            "There are many efficient solver for multiple kernel learning.",
            "We can effectively use existing technique for multiple cloning to solve our problem.",
            "So for detailed proving and finding the paper."
        ],
        [
            "This one.",
            "So far empirical study, we generate some synthetic individual metrics and we use the benchmark datasets from UCI and all the parameter determined by 5 code validation and we reported the accuracy will run 10 times for each data set."
        ],
        [
            "So further we look at the convergence property.",
            "So the left graph shows the convergence of algorithm without pulling an with pulling in the right side and red curve denotes the abundance of algorithm.",
            "At least our black line shows the lower bound you can see.",
            "In most cases the outbound increase low by increase and they contact gradually and interesting.",
            "In both case, with pulling out without pulling the convergence behavior valid looks very similar.",
            "So we can see for most technique following cutting plane at the first few iteration, Lisa Gap will reduce dramatically an in after filtration, learning the difference, the gap is very small, so we can potentially apply early stopping criteria stop much earlier before even before."
        ],
        [
            "Virgins."
        ],
        [
            "So in terms of convergence, the pruning strategy will not affect lot."
        ],
        [
            "So key advantage of pruning is the number of Lisa kernel matrix involved will be much smaller, so remember if we if we do not apply pruning strategy.",
            "So here's this graph shows the size of localising.",
            "Set K is the P value OK if we do not apply this list point strategy.",
            "The number of Colonel increase gradually one by one each of the each iteration is by one, so it's at least linear line.",
            "And if we apply pulling strategy, so we expect the number of kernels will be will be much smaller.",
            "Sohail result this red line turns out after until convergence the size of the set will not increase a lot, keep almost constant.",
            "So this makes the second part much more efficient compared to this original formulation.",
            "So you mentioned we have potentially apply early stopping because the gap duality gap will not decrease much after the first few iterations.",
            "So we actually here we check we check the accuracy when the algorithm actually iterates.",
            "So over the first forty iteration on the train left so much change in terms of accuracy here at Y axis is accuracy, XLS, iterations.",
            "After 4050 iterations they're not much change.",
            "So this is consistent with our."
        ],
        [
            "Give us a result.",
            "In terms of performance, we compare with several other approach for SVN, learning with indefinite kernel, denoise, flip, shift and SVN.",
            "He is very essentially you just plug it in different kernel KO into SVM.",
            "An you will find some solution.",
            "Of course it's one kernel is non PSD learn SVN is non convex so you may get local solution.",
            "But anyway can still solve your problem.",
            "You get some solution which may be local solution.",
            "So in most cases, especially one matrix is non PSD PSD which can be measured by the minimum and maximum value.",
            "So the proposed method is much more effective in the extreme case.",
            "And in other cases they are compatible."
        ],
        [
            "To all approach.",
            "So in conclusion, here will propose a submit final formulation for training SVM with non PSD kernel matrix and we propose an algorithm to solve this formulation following the idea from cutting plane and we conduct convergence analysis.",
            "And we follow proposal police strategy to improve efficiency and we show with this pruning strategy we can still prove the convergence of the algorithm and all results show Lisa two strategy with or without pulling strategy.",
            "They still have a similar behavior in terms of convergence.",
            "And finally we show the close connection between the proposed formulation and multiple kernel learning.",
            "So the healer key idea we got from this work is.",
            "Given our current governor given on kernel matrix KO, so the key idea here is where each time we generated kernel matrix candidate kind of metrics and in the end we combine them and we show this world.",
            "This is optimal kind of metrics we can derive from this regular SVM formulation.",
            "That's why there's a close connection between this work and multiple kernel, so we generate candidate kernel and will try to combine its linear combination.",
            "So for future work we are currently employs child employ alternative technique to further improve the efficiency of the algorithm.",
            "For example, we will try to use level method, an approximate method, try to improve the efficiency of algorithm and we all showed up algorithms to real world applications involving this indefinite kernel matrices from the biological domain.",
            "Thank you."
        ],
        [
            "So after training, use again."
        ],
        [
            "For testing drugs.",
            "Yes.",
            "Sarah.",
            "Bugs out.",
            "I mean, if you're getting kernel is very different for the positive definite one that approximates it, so the combination.",
            "Happy about that forms the way, so here is the primary lower lower controller controller deviation between K&KO.",
            "So if so if in clause validation we find out K matrix which is very different from Halo but outperform the least the other case which is lowest case close to 0 right now.",
            "In this case I think our approach will perform better.",
            "So really I think the key here is low parameter right controller.",
            "Deviation and install applications.",
            "Maybe a large K large deviation measure may perform better.",
            "OK, well in many cases actually.",
            "In our experiments this K is reasonably close to our original case.",
            "Yellow matrix solo.",
            "I think it's the parameter.",
            "Did you try to compare also?",
            "So try to decode with the with the with the proxy kernel instead of the original 1 to see if it performs like you mean the test case like.",
            "Yes in the test case we use at least came matrix.",
            "So we learn we learn how long is yellow and then the test we just use chemistries.",
            "USB.",
            "Could you please go back to your jacket for lunch in your."
        ],
        [
            "So basically you are balancing the tradeoff.",
            "Respect yeah, yeah.",
            "Yes, but SVN lesser regularization, so this part is SVM.",
            "Translation look at this deal for MSV, and so there is a regulations in a prime form, but we're working on the deal form.",
            "Individual form you get something like now to transpose number I up.",
            "Which corresponds to weatherization in here stating that, for example, on your site the land reformulation.",
            "Yeah, see, let's see.",
            "This is a deal for not private, but if you go to the.",
            "SVM."
        ],
        [
            "Hit.",
            "Because.",
            "The same thing right here, only difference cases.",
            "Where is a linear combination of Colonel the other part standards in.",
            "Also there, then the Lambda by imposing that one of the metrics is actually the identity matrix and which Lambda.",
            "Which number or C?",
            "Yeah, yeah, that's possible, yeah?",
            "Square cost conscious, so nothing like classical escapes everything.",
            "Yeah, so we did some study actually using two noise VM and one V1 versus the coefficient for identity kernel matrix and sometimes perform well by learning is not guarantee it will perform better than cross validation.",
            "That's my experience.",
            "And again, this formulation, nice technical work from, you know, starting point to get this algorithm work.",
            "But my question is, do you have any motivation behind using the provenience norm as a duration between two matrices when you interpret your problem as you know?",
            "Positive summer definite matrix lost.",
            "Some noise noise is not really something like you know.",
            "You really had a true positive senator that makes someone throws a Gaussian noise over here.",
            "No noise comes from whatever processes related to data acquisition, so just messing around.",
            "You know in this case of matrices looking for something that's closer to completion.",
            "Snowing looks somewhat unmotivated.",
            "Yeah, that's a.",
            "That's a good question, so he La La difference between our kernel matrix case yellow.",
            "Error 10 is not necessarily PSD, or it can be any any error.",
            "Any Lisa matrix right not necessary PSD.",
            "The difference.",
            "Of course you can.",
            "You can use potentially use other normal problems, least computational this issue, right?",
            "So if you if you use this training tool other than one or two Long Island on may be computationally will be more difficult to solve.",
            "Look into it.",
            "The other group of approaches like how strong you know where they take a limited form of investment matrix that Gustavson.",
            "Prime Rib isn't Colonel Brian.",
            "In that case, we have freedom.",
            "You have real structure, you understand it started at least two indefinite instructions.",
            "Yeah.",
            "Yeah, that's a good point.",
            "So as I mentioned, there are two kind of approach.",
            "The first approach correspond to what you just said like this."
        ],
        [
            "His work.",
            "Based on I think different type of liquid space and represented Salem.",
            "Now there's another extreme case which just use original SVM but apply simple transformation.",
            "So I think a little proposed work lies between these two.",
            "OK, so we consider we try to modify something formulation, but we also learn kernel automatically, not independent of SVN like like this.",
            "But this one is independent then classify.",
            "Going up in that question here is if you have the indefinite similarity metric, why insist on using SVM in the 1st place rather than just use other methods based on some letters?",
            "So use this American values, features and.",
            "Use more direct methods.",
            "OK, so if you use the similarity as the feature right now it correspond to one of this approach.",
            "Which kernel is the Aztec South transpose?",
            "The Christmas one piece.",
            "So if you use if you use let's say.",
            "So if you use S similar, this feature similarity is a feature for.",
            "Maybe call the first data point.",
            "Essentially you get a kernel matrix, which as tense as transport, right?",
            "So we know at times as transpose guaranteed to be placed.",
            "Position could be more similar to SVM rather than notarization in this feature space.",
            "Yeah, OK, yeah I think that would be interesting extension yeah.",
            "OK, one last question before moving so which things until this revelation, if understand correctly, is equivalent to the Lucent Leukorrhea?",
            "Yeah yeah.",
            "OK.",
            "Performance of that.",
            "So I mentioned earlier, right?",
            "Yeah, so I mentioned early lease.",
            "Formulation is none smooth actually.",
            "If you consider my SVM maximization so big."
        ],
        [
            "It's actually simpler solution for K, so this one is we have less clustered here, so not smooth.",
            "So in original formula, previous work layer place on the add some smoothing to the objective.",
            "So making the problem easy.",
            "I think differentiable so the solution May is approximation essentially not exact depending on the lesson lesson control parameter.",
            "Yeah, because they had some smoothing time, right?",
            "So will not be exact.",
            "Equivalent to regional formulation.",
            "Now here we show we can actually solve the original formulation exactly.",
            "We don't apply any up.",
            "Hard to get to the optimal solution of approximating optimal solution, which is we're always doing with this formulation, but shrinking the smoothing parameter.",
            "So I mean, if you compare the is.",
            "To me it seems that both methods are optimized.",
            "It's different methods using, yeah.",
            "The next, no I I I single as I mentioned in the paper, I think a little performance should be compatible, just thus always kind of different.",
            "So we should.",
            "Yeah, so the problem in terms of accuracy should be should be compatible.",
            "So that should be should be different.",
            "So the problem is how you implement right?",
            "So the way we implement and they original implementation, I think there will be some some kind of difference.",
            "So we try to get their original code, but we didn't get it.",
            "We know, so that's always an issue.",
            "When you implement someone else algorithm.",
            "Many kilometers and the way we use the solver like, can we make different so?",
            "But I think the interesting here is we should we should connection between this work and multiple kernel learning and give you some insight.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, good morning.",
                    "label": 0
                },
                {
                    "sent": "So this work is about training, SVM wanna kernel matrix is non PSD.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with my students generally well both from.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At least on a state.",
                    "label": 0
                },
                {
                    "sent": "So the last speaker very good overview of kernel SVN and multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "So this makes my talk much easier.",
                    "label": 0
                },
                {
                    "sent": "So SVN has been applied successfully in many applications and over key element in SVN is the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And the company is required the kernel matrix is positive semidefinite PSD.",
                    "label": 0
                },
                {
                    "sent": "That means in the matrix does not contain any negative eigenvalues, at least zero.",
                    "label": 0
                },
                {
                    "sent": "So the PSD property of the kernel matrix ensure the existence of a reproducing kernel Hilbert space and also resulted in the convex quadratic program.",
                    "label": 1
                },
                {
                    "sent": "So here why is label and K is kernel matrix sees the parameter?",
                    "label": 0
                },
                {
                    "sent": "So if K is a PhD, learning is a convex and we can find global optimal solution using stand techniques such as in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yellow point method.",
                    "label": 0
                },
                {
                    "sent": "So however many applications we may have this kernel match to be non PSD indefinite.",
                    "label": 0
                },
                {
                    "sent": "For example in bioinformatics the similarity matrix based on sequence pairwise alignment like protein sequence alignment may not be PSD.",
                    "label": 0
                },
                {
                    "sent": "It may contain negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So there have been many prior work on training SVN.",
                    "label": 0
                },
                {
                    "sent": "One kernel matrix is not PSD PSD, so there are some work.",
                    "label": 0
                },
                {
                    "sent": "Try to modify the SVM formulation.",
                    "label": 0
                },
                {
                    "sent": "User general form of recent theory.",
                    "label": 0
                },
                {
                    "sent": "And we also many other work which useless original SVM formulation bug may apply spectral transformation.",
                    "label": 0
                },
                {
                    "sent": "Put Transformer indefinite kernel matrix to a PSD matrix.",
                    "label": 0
                },
                {
                    "sent": "So for example this is the noise is 1 popular approach.",
                    "label": 0
                },
                {
                    "sent": "It works by first computing all eigenvalues of kernel matrix and then remove the next eigenvalues, essentially training the next group OK and Flipper is also not approach which change the sign of the negative values to be positive and the shift is we add positive constant over.",
                    "label": 0
                },
                {
                    "sent": "Values to make all the invalid nonnegative.",
                    "label": 0
                },
                {
                    "sent": "So one limitation of this approach is the learning of the kernel matrix, and SVN is actually coupled.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you need to work with.",
                    "label": 0
                },
                {
                    "sent": "We start a new approach which will learn the kernel matrix and list SVM classifier simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So essentially, in this work we show the invariant kernel which is provided right?",
                    "label": 0
                },
                {
                    "sent": "So we are given in dental matrix and we assume the indefinite kernel is noisy observation of the true but unknown PSD matrix with complex matrix.",
                    "label": 1
                },
                {
                    "sent": "And I will try to learn that is a kernel matrix and least SVM classifier simultaneously and we show this problem can be formulated as these semidefinite quadratic program.",
                    "label": 0
                },
                {
                    "sent": "And we propose the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's following the idea of cutting plane to solve this efficiently.",
                    "label": 1
                },
                {
                    "sent": "And we conduct this convergence analysis.",
                    "label": 0
                },
                {
                    "sent": "At least formulation still computationally expensive to solve, so we propose a polling strategy to improve the efficiency of algorithm an in, and we show the close connection between these formulation list multiple formation and multiple.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cannelloni.",
                    "label": 0
                },
                {
                    "sent": "So first I will give a overview of SVM, so we're given a kernel matrix K and we have.",
                    "label": 0
                },
                {
                    "sent": "Why is class label an while capitalizing the diagonal matrix of class label an SVN deal form of 1 SVN we have leased maximization problems.",
                    "label": 0
                },
                {
                    "sent": "So here case lock key metric case, the kernel matrix and which is commonly required to be PSD, see the parameter wise label.",
                    "label": 0
                },
                {
                    "sent": "So in traditional SVM case to be required to be non PSD, PSD, so in all cases were given a case zero matrix which is non PSD.",
                    "label": 0
                },
                {
                    "sent": "And we try to learn.",
                    "label": 0
                },
                {
                    "sent": "We try to learn.",
                    "label": 0
                },
                {
                    "sent": "You can imagine K. Even though we've seen SVM formulation, so the formula for way uses this one, we try to use the regular version of SVM formulation.",
                    "label": 0
                },
                {
                    "sent": "The difference between this and this original SVS, we have leased additional term and the first part is the sense as VM.",
                    "label": 0
                },
                {
                    "sent": "So essentially in this formulation we we try to compute a noisy observation of the case yellow which is K case lower our kernel matrix unknown, we try to compute and Ocasio is given.",
                    "label": 1
                },
                {
                    "sent": "So we try to find the kernel matrix K which maximize margin SVM.",
                    "label": 0
                },
                {
                    "sent": "This part and simultaneously which had minimized.",
                    "label": 0
                },
                {
                    "sent": "Creation we decay in our our given kernel matrix case.",
                    "label": 0
                },
                {
                    "sent": "Yellow is indefinite.",
                    "label": 0
                },
                {
                    "sent": "And the lowest, the permit which controller balance right between SVN this maximization margin and also the least minimisation of difference between at least K and our original.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quesadilla matrix.",
                    "label": 0
                },
                {
                    "sent": "So we try to solve this min Max optimization here.",
                    "label": 0
                },
                {
                    "sent": "Always do available is a vector OK is a kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "And this problem turns out to be a.",
                    "label": 0
                },
                {
                    "sent": "It's a non smooth, nonsmooth than in previous work.",
                    "label": 0
                },
                {
                    "sent": "People try to using this smoothing technique, they add some sometime which try to make this optimized, smooth and so the solution may not be.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's approximation is not exact.",
                    "label": 0
                },
                {
                    "sent": "And he needs a walkway proposed to solve this problem using idea flow from cutting plane.",
                    "label": 0
                },
                {
                    "sent": "So we define the objective sour K this part.",
                    "label": 0
                },
                {
                    "sent": "And we define this minimize K over this as function this part.",
                    "label": 0
                },
                {
                    "sent": "Dusty, so we change the problem 4 to be this one this one.",
                    "label": 0
                },
                {
                    "sent": "So maximize Alpha and notify this minimal trustee.",
                    "label": 0
                },
                {
                    "sent": "So if the minimisation over K is tear, this means this function as our K is larger or equal to T for any cake because this minimum is is T, so it must be larger or equal to T for any K. So it is where we make a list object function linear simple, but we have this.",
                    "label": 0
                },
                {
                    "sent": "We have this complex constraint and the number of consumer K here depends on a number of least possible kernel matrix K is invented.",
                    "label": 0
                },
                {
                    "sent": "And we follow idea for chronic pain to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Essentially for initially will give a kernel matrix KOK, so we have only one constraint.",
                    "label": 0
                },
                {
                    "sent": "Then we can solve this maximization.",
                    "label": 0
                },
                {
                    "sent": "We have only one to three constraint and linear object.",
                    "label": 0
                },
                {
                    "sent": "If we can solve this using this stand technique from.",
                    "label": 0
                },
                {
                    "sent": "In this case we use the technique flow mosaic.",
                    "label": 0
                },
                {
                    "sent": "Now we solve, we get T and Alpha and we try to find a new new matrix K which violates consider most essentially try to minimize this function in terms of OK and we get another constraint K matrix.",
                    "label": 0
                },
                {
                    "sent": "Now we have two kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Two constraint.",
                    "label": 0
                },
                {
                    "sent": "We solve this problem again and will repeat until convergence.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have two step two key step.",
                    "label": 0
                },
                {
                    "sent": "The first step is we forgiven collection of kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "We call this localizing set.",
                    "label": 0
                },
                {
                    "sent": "K what KP we solve.",
                    "label": 1
                },
                {
                    "sent": "Lisa quadratic constraint linear program to get T another so we call intermediate solution path.",
                    "label": 0
                },
                {
                    "sent": "After we get to know each other, we try to minimize this function.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we try to find the key metrics which violate this consumer most.",
                    "label": 0
                },
                {
                    "sent": "So computer this we get a new local's instead of K and only piano convergence.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next we go through this two step in detail.",
                    "label": 0
                },
                {
                    "sent": "In the first step.",
                    "label": 0
                },
                {
                    "sent": "If T and always given, so we assume T and always given and we try to find the optimal K which valid the consumer most.",
                    "label": 0
                },
                {
                    "sent": "And essentially we try to minimize this function as function in terms of cases always given.",
                    "label": 0
                },
                {
                    "sent": "And the solution actually from last year by Lausanne his colleague is given by a close form.",
                    "label": 0
                },
                {
                    "sent": "Is KCL matrix is given an applause Lisa?",
                    "label": 0
                },
                {
                    "sent": "Why is the diagonal matrix, which consist of the class labels and 10s otherwise the solution?",
                    "label": 0
                },
                {
                    "sent": "Then we try to truncate, pluses, truncate, essentially remove all the negative values.",
                    "label": 0
                },
                {
                    "sent": "Only keep the positive eigenvalues so it's very easy to get and after we get this kernel matrix case star.",
                    "label": 0
                },
                {
                    "sent": "Now we track this constraint is a Lt is less than equal to S advertise this new case dot matrix.",
                    "label": 0
                },
                {
                    "sent": "If this satisfied then it shows that we already got our global optimal solution and then we're done.",
                    "label": 0
                },
                {
                    "sent": "Otherwise if this is valid then we added this case 02 hour set localising set so OK make the set will increase by one then we repeat.",
                    "label": 0
                },
                {
                    "sent": "So this survey is quite.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easy to implement when in the second step for a given set of kernel matrix K for K12 KP we have P constraint and we need to solve T ANOTHER.",
                    "label": 0
                },
                {
                    "sent": "So this one is quadratic constraint.",
                    "label": 0
                },
                {
                    "sent": "This is quadratic constraint and linear program and we use that software from msek to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So all the key observation here is the complexity of this problem.",
                    "label": 0
                },
                {
                    "sent": "Is depends on the number of constraints here.",
                    "label": 0
                },
                {
                    "sent": "So P the set of localized inside K if K is large, learning is much more expensive to solve.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So two controller convergence we need about the lower bound of objective and use the gap between these these bonds to control convergence.",
                    "label": 0
                },
                {
                    "sent": "So step one, we are given over and T we solve the optimal K matrix by minimizing this object function.",
                    "label": 0
                },
                {
                    "sent": "Then we can keep track of all the objects functions so far until iteration I will choose the maximum at least will be lower bound control easily show.",
                    "label": 0
                },
                {
                    "sent": "And similarly, given a given is a localized instead K, we can compute optimal Alpha T and using these tears up.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can show at least lower bound.",
                    "label": 0
                },
                {
                    "sent": "Always increase an upon always decrease and the gap between these two bonds will actually come close to each other and it's guaranteed the algorithm will converge to the optimal solution and we can use the gap between this low on board controller to check the convergence of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will skip the details here.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, one local limitation of this approach is.",
                    "label": 1
                },
                {
                    "sent": "Is when we solve this quadratic linear program.",
                    "label": 0
                },
                {
                    "sent": "If the number of constraint or the size of the suitcase large length will be expensive to solve.",
                    "label": 1
                },
                {
                    "sent": "And the number of a quadratic constraint increased by one at each iteration, because each time we get a new kernel matrix and we add into this localized set, so the size will increase.",
                    "label": 1
                },
                {
                    "sent": "So after 200 iterations, little size of the set K is 200 and we have 200 quadratic constraint and this really expensive solve.",
                    "label": 0
                },
                {
                    "sent": "It turns out we had easily solved this problem by actually using disappointing.",
                    "label": 0
                },
                {
                    "sent": "When we solve this quadratic linear program, it turns out many of the quadratic constraint inactive.",
                    "label": 0
                },
                {
                    "sent": "It means the equality is not satisfied.",
                    "label": 0
                },
                {
                    "sent": "Then we can effectively prone leads in active constraint.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So assume Lisa Ki is the current localising set an we can decompose K into two parts is Elise is active part means the equality is satisfied and the other one is is the inactive constraint is it's the quality is not satisfied so it's less than this value.",
                    "label": 0
                },
                {
                    "sent": "And if we do not apply pulling strategy, the new localized set is just the unit of the previous local set and this new kernel matrix zero.",
                    "label": 1
                },
                {
                    "sent": "We add kernel matrix one by one at each iteration.",
                    "label": 1
                },
                {
                    "sent": "But if we apply pruning strategy then we can remove all the inactive constraint.",
                    "label": 1
                },
                {
                    "sent": "So using this followed, it turns out disappointing strategy will improve cost efficiency, improve efficiency while retaining the convergence.",
                    "label": 0
                },
                {
                    "sent": "Obligation.",
                    "label": 0
                },
                {
                    "sent": "So we'll show some results.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice spot.",
                    "label": 0
                },
                {
                    "sent": "So finally we should close connection between this formulation and multiple kernel learning languages.",
                    "label": 0
                },
                {
                    "sent": "In 2004 Proposal, the following formulation for learning convex linear combination of a given kernel matrices were given up equal matrices and we try to find a linear combination.",
                    "label": 0
                },
                {
                    "sent": "Here CI is coefficient follow for lesco metrics and we can solve this problem efficiently.",
                    "label": 0
                },
                {
                    "sent": "Using queues could be or semi infinite program.",
                    "label": 0
                },
                {
                    "sent": "So in our case, at each intermediate step we have assumed we have localized set which consists of P kernel matrices which is K12 KP and we have Lisa KO is the indicator matrix.",
                    "label": 0
                },
                {
                    "sent": "We can compute the difference between Ki and K0 so it knows us UI.",
                    "label": 1
                },
                {
                    "sent": "So that's the deviation between K and our original case zero matrix.",
                    "label": 0
                },
                {
                    "sent": "Long we can consider a glass version of this original multiple kernel learning formulation.",
                    "label": 0
                },
                {
                    "sent": "The difference here is we have this penalty term.",
                    "label": 0
                },
                {
                    "sent": "So we minimize the deviation between RKI and our original matrix and the way salicylates it I and lowest Lisa control parameter.",
                    "label": 0
                },
                {
                    "sent": "So these two are very similar and we have at least 10.",
                    "label": 0
                },
                {
                    "sent": "So we can actually show that our.",
                    "label": 0
                },
                {
                    "sent": "Proposed formation for kernel learning with indefinite kernel is equivalent to 17.",
                    "label": 1
                },
                {
                    "sent": "OK, so we can view or QC LP formulation as extension of multiple cleaning with additional regularization.",
                    "label": 0
                },
                {
                    "sent": "So Lapete consequence of this fact is really already many, many work on multiple cleaning.",
                    "label": 0
                },
                {
                    "sent": "There are many efficient solver for multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "We can effectively use existing technique for multiple cloning to solve our problem.",
                    "label": 0
                },
                {
                    "sent": "So for detailed proving and finding the paper.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "So far empirical study, we generate some synthetic individual metrics and we use the benchmark datasets from UCI and all the parameter determined by 5 code validation and we reported the accuracy will run 10 times for each data set.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So further we look at the convergence property.",
                    "label": 0
                },
                {
                    "sent": "So the left graph shows the convergence of algorithm without pulling an with pulling in the right side and red curve denotes the abundance of algorithm.",
                    "label": 1
                },
                {
                    "sent": "At least our black line shows the lower bound you can see.",
                    "label": 0
                },
                {
                    "sent": "In most cases the outbound increase low by increase and they contact gradually and interesting.",
                    "label": 0
                },
                {
                    "sent": "In both case, with pulling out without pulling the convergence behavior valid looks very similar.",
                    "label": 0
                },
                {
                    "sent": "So we can see for most technique following cutting plane at the first few iteration, Lisa Gap will reduce dramatically an in after filtration, learning the difference, the gap is very small, so we can potentially apply early stopping criteria stop much earlier before even before.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Virgins.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of convergence, the pruning strategy will not affect lot.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So key advantage of pruning is the number of Lisa kernel matrix involved will be much smaller, so remember if we if we do not apply pruning strategy.",
                    "label": 0
                },
                {
                    "sent": "So here's this graph shows the size of localising.",
                    "label": 0
                },
                {
                    "sent": "Set K is the P value OK if we do not apply this list point strategy.",
                    "label": 0
                },
                {
                    "sent": "The number of Colonel increase gradually one by one each of the each iteration is by one, so it's at least linear line.",
                    "label": 0
                },
                {
                    "sent": "And if we apply pulling strategy, so we expect the number of kernels will be will be much smaller.",
                    "label": 0
                },
                {
                    "sent": "Sohail result this red line turns out after until convergence the size of the set will not increase a lot, keep almost constant.",
                    "label": 0
                },
                {
                    "sent": "So this makes the second part much more efficient compared to this original formulation.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned we have potentially apply early stopping because the gap duality gap will not decrease much after the first few iterations.",
                    "label": 0
                },
                {
                    "sent": "So we actually here we check we check the accuracy when the algorithm actually iterates.",
                    "label": 0
                },
                {
                    "sent": "So over the first forty iteration on the train left so much change in terms of accuracy here at Y axis is accuracy, XLS, iterations.",
                    "label": 0
                },
                {
                    "sent": "After 4050 iterations they're not much change.",
                    "label": 0
                },
                {
                    "sent": "So this is consistent with our.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give us a result.",
                    "label": 0
                },
                {
                    "sent": "In terms of performance, we compare with several other approach for SVN, learning with indefinite kernel, denoise, flip, shift and SVN.",
                    "label": 1
                },
                {
                    "sent": "He is very essentially you just plug it in different kernel KO into SVM.",
                    "label": 0
                },
                {
                    "sent": "An you will find some solution.",
                    "label": 0
                },
                {
                    "sent": "Of course it's one kernel is non PSD learn SVN is non convex so you may get local solution.",
                    "label": 0
                },
                {
                    "sent": "But anyway can still solve your problem.",
                    "label": 0
                },
                {
                    "sent": "You get some solution which may be local solution.",
                    "label": 0
                },
                {
                    "sent": "So in most cases, especially one matrix is non PSD PSD which can be measured by the minimum and maximum value.",
                    "label": 0
                },
                {
                    "sent": "So the proposed method is much more effective in the extreme case.",
                    "label": 0
                },
                {
                    "sent": "And in other cases they are compatible.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To all approach.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, here will propose a submit final formulation for training SVM with non PSD kernel matrix and we propose an algorithm to solve this formulation following the idea from cutting plane and we conduct convergence analysis.",
                    "label": 1
                },
                {
                    "sent": "And we follow proposal police strategy to improve efficiency and we show with this pruning strategy we can still prove the convergence of the algorithm and all results show Lisa two strategy with or without pulling strategy.",
                    "label": 0
                },
                {
                    "sent": "They still have a similar behavior in terms of convergence.",
                    "label": 0
                },
                {
                    "sent": "And finally we show the close connection between the proposed formulation and multiple kernel learning.",
                    "label": 1
                },
                {
                    "sent": "So the healer key idea we got from this work is.",
                    "label": 0
                },
                {
                    "sent": "Given our current governor given on kernel matrix KO, so the key idea here is where each time we generated kernel matrix candidate kind of metrics and in the end we combine them and we show this world.",
                    "label": 0
                },
                {
                    "sent": "This is optimal kind of metrics we can derive from this regular SVM formulation.",
                    "label": 0
                },
                {
                    "sent": "That's why there's a close connection between this work and multiple kernel, so we generate candidate kernel and will try to combine its linear combination.",
                    "label": 0
                },
                {
                    "sent": "So for future work we are currently employs child employ alternative technique to further improve the efficiency of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "For example, we will try to use level method, an approximate method, try to improve the efficiency of algorithm and we all showed up algorithms to real world applications involving this indefinite kernel matrices from the biological domain.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after training, use again.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For testing drugs.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sarah.",
                    "label": 0
                },
                {
                    "sent": "Bugs out.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you're getting kernel is very different for the positive definite one that approximates it, so the combination.",
                    "label": 0
                },
                {
                    "sent": "Happy about that forms the way, so here is the primary lower lower controller controller deviation between K&KO.",
                    "label": 0
                },
                {
                    "sent": "So if so if in clause validation we find out K matrix which is very different from Halo but outperform the least the other case which is lowest case close to 0 right now.",
                    "label": 0
                },
                {
                    "sent": "In this case I think our approach will perform better.",
                    "label": 0
                },
                {
                    "sent": "So really I think the key here is low parameter right controller.",
                    "label": 0
                },
                {
                    "sent": "Deviation and install applications.",
                    "label": 0
                },
                {
                    "sent": "Maybe a large K large deviation measure may perform better.",
                    "label": 0
                },
                {
                    "sent": "OK, well in many cases actually.",
                    "label": 0
                },
                {
                    "sent": "In our experiments this K is reasonably close to our original case.",
                    "label": 0
                },
                {
                    "sent": "Yellow matrix solo.",
                    "label": 0
                },
                {
                    "sent": "I think it's the parameter.",
                    "label": 0
                },
                {
                    "sent": "Did you try to compare also?",
                    "label": 0
                },
                {
                    "sent": "So try to decode with the with the with the proxy kernel instead of the original 1 to see if it performs like you mean the test case like.",
                    "label": 0
                },
                {
                    "sent": "Yes in the test case we use at least came matrix.",
                    "label": 0
                },
                {
                    "sent": "So we learn we learn how long is yellow and then the test we just use chemistries.",
                    "label": 0
                },
                {
                    "sent": "USB.",
                    "label": 0
                },
                {
                    "sent": "Could you please go back to your jacket for lunch in your.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically you are balancing the tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Respect yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, but SVN lesser regularization, so this part is SVM.",
                    "label": 0
                },
                {
                    "sent": "Translation look at this deal for MSV, and so there is a regulations in a prime form, but we're working on the deal form.",
                    "label": 0
                },
                {
                    "sent": "Individual form you get something like now to transpose number I up.",
                    "label": 0
                },
                {
                    "sent": "Which corresponds to weatherization in here stating that, for example, on your site the land reformulation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, see, let's see.",
                    "label": 0
                },
                {
                    "sent": "This is a deal for not private, but if you go to the.",
                    "label": 0
                },
                {
                    "sent": "SVM.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hit.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "The same thing right here, only difference cases.",
                    "label": 0
                },
                {
                    "sent": "Where is a linear combination of Colonel the other part standards in.",
                    "label": 0
                },
                {
                    "sent": "Also there, then the Lambda by imposing that one of the metrics is actually the identity matrix and which Lambda.",
                    "label": 0
                },
                {
                    "sent": "Which number or C?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's possible, yeah?",
                    "label": 0
                },
                {
                    "sent": "Square cost conscious, so nothing like classical escapes everything.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we did some study actually using two noise VM and one V1 versus the coefficient for identity kernel matrix and sometimes perform well by learning is not guarantee it will perform better than cross validation.",
                    "label": 0
                },
                {
                    "sent": "That's my experience.",
                    "label": 0
                },
                {
                    "sent": "And again, this formulation, nice technical work from, you know, starting point to get this algorithm work.",
                    "label": 0
                },
                {
                    "sent": "But my question is, do you have any motivation behind using the provenience norm as a duration between two matrices when you interpret your problem as you know?",
                    "label": 0
                },
                {
                    "sent": "Positive summer definite matrix lost.",
                    "label": 0
                },
                {
                    "sent": "Some noise noise is not really something like you know.",
                    "label": 0
                },
                {
                    "sent": "You really had a true positive senator that makes someone throws a Gaussian noise over here.",
                    "label": 0
                },
                {
                    "sent": "No noise comes from whatever processes related to data acquisition, so just messing around.",
                    "label": 0
                },
                {
                    "sent": "You know in this case of matrices looking for something that's closer to completion.",
                    "label": 0
                },
                {
                    "sent": "Snowing looks somewhat unmotivated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a.",
                    "label": 0
                },
                {
                    "sent": "That's a good question, so he La La difference between our kernel matrix case yellow.",
                    "label": 0
                },
                {
                    "sent": "Error 10 is not necessarily PSD, or it can be any any error.",
                    "label": 0
                },
                {
                    "sent": "Any Lisa matrix right not necessary PSD.",
                    "label": 0
                },
                {
                    "sent": "The difference.",
                    "label": 0
                },
                {
                    "sent": "Of course you can.",
                    "label": 0
                },
                {
                    "sent": "You can use potentially use other normal problems, least computational this issue, right?",
                    "label": 0
                },
                {
                    "sent": "So if you if you use this training tool other than one or two Long Island on may be computationally will be more difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "Look into it.",
                    "label": 0
                },
                {
                    "sent": "The other group of approaches like how strong you know where they take a limited form of investment matrix that Gustavson.",
                    "label": 0
                },
                {
                    "sent": "Prime Rib isn't Colonel Brian.",
                    "label": 0
                },
                {
                    "sent": "In that case, we have freedom.",
                    "label": 0
                },
                {
                    "sent": "You have real structure, you understand it started at least two indefinite instructions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, there are two kind of approach.",
                    "label": 0
                },
                {
                    "sent": "The first approach correspond to what you just said like this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His work.",
                    "label": 0
                },
                {
                    "sent": "Based on I think different type of liquid space and represented Salem.",
                    "label": 0
                },
                {
                    "sent": "Now there's another extreme case which just use original SVM but apply simple transformation.",
                    "label": 0
                },
                {
                    "sent": "So I think a little proposed work lies between these two.",
                    "label": 0
                },
                {
                    "sent": "OK, so we consider we try to modify something formulation, but we also learn kernel automatically, not independent of SVN like like this.",
                    "label": 0
                },
                {
                    "sent": "But this one is independent then classify.",
                    "label": 0
                },
                {
                    "sent": "Going up in that question here is if you have the indefinite similarity metric, why insist on using SVM in the 1st place rather than just use other methods based on some letters?",
                    "label": 0
                },
                {
                    "sent": "So use this American values, features and.",
                    "label": 0
                },
                {
                    "sent": "Use more direct methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you use the similarity as the feature right now it correspond to one of this approach.",
                    "label": 0
                },
                {
                    "sent": "Which kernel is the Aztec South transpose?",
                    "label": 0
                },
                {
                    "sent": "The Christmas one piece.",
                    "label": 0
                },
                {
                    "sent": "So if you use if you use let's say.",
                    "label": 0
                },
                {
                    "sent": "So if you use S similar, this feature similarity is a feature for.",
                    "label": 0
                },
                {
                    "sent": "Maybe call the first data point.",
                    "label": 0
                },
                {
                    "sent": "Essentially you get a kernel matrix, which as tense as transport, right?",
                    "label": 0
                },
                {
                    "sent": "So we know at times as transpose guaranteed to be placed.",
                    "label": 0
                },
                {
                    "sent": "Position could be more similar to SVM rather than notarization in this feature space.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, yeah I think that would be interesting extension yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, one last question before moving so which things until this revelation, if understand correctly, is equivalent to the Lucent Leukorrhea?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Performance of that.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned earlier, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mentioned early lease.",
                    "label": 0
                },
                {
                    "sent": "Formulation is none smooth actually.",
                    "label": 0
                },
                {
                    "sent": "If you consider my SVM maximization so big.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's actually simpler solution for K, so this one is we have less clustered here, so not smooth.",
                    "label": 0
                },
                {
                    "sent": "So in original formula, previous work layer place on the add some smoothing to the objective.",
                    "label": 0
                },
                {
                    "sent": "So making the problem easy.",
                    "label": 0
                },
                {
                    "sent": "I think differentiable so the solution May is approximation essentially not exact depending on the lesson lesson control parameter.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because they had some smoothing time, right?",
                    "label": 0
                },
                {
                    "sent": "So will not be exact.",
                    "label": 0
                },
                {
                    "sent": "Equivalent to regional formulation.",
                    "label": 0
                },
                {
                    "sent": "Now here we show we can actually solve the original formulation exactly.",
                    "label": 0
                },
                {
                    "sent": "We don't apply any up.",
                    "label": 0
                },
                {
                    "sent": "Hard to get to the optimal solution of approximating optimal solution, which is we're always doing with this formulation, but shrinking the smoothing parameter.",
                    "label": 0
                },
                {
                    "sent": "So I mean, if you compare the is.",
                    "label": 0
                },
                {
                    "sent": "To me it seems that both methods are optimized.",
                    "label": 0
                },
                {
                    "sent": "It's different methods using, yeah.",
                    "label": 0
                },
                {
                    "sent": "The next, no I I I single as I mentioned in the paper, I think a little performance should be compatible, just thus always kind of different.",
                    "label": 0
                },
                {
                    "sent": "So we should.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the problem in terms of accuracy should be should be compatible.",
                    "label": 0
                },
                {
                    "sent": "So that should be should be different.",
                    "label": 0
                },
                {
                    "sent": "So the problem is how you implement right?",
                    "label": 0
                },
                {
                    "sent": "So the way we implement and they original implementation, I think there will be some some kind of difference.",
                    "label": 0
                },
                {
                    "sent": "So we try to get their original code, but we didn't get it.",
                    "label": 0
                },
                {
                    "sent": "We know, so that's always an issue.",
                    "label": 0
                },
                {
                    "sent": "When you implement someone else algorithm.",
                    "label": 0
                },
                {
                    "sent": "Many kilometers and the way we use the solver like, can we make different so?",
                    "label": 0
                },
                {
                    "sent": "But I think the interesting here is we should we should connection between this work and multiple kernel learning and give you some insight.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}