{
    "id": "dypmte476upxqhv6rlgbq2wvgwuxodga",
    "title": "Drifting Games, Boosting and Online Learning",
    "info": {
        "author": [
            "Yoav Freund, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_freund_dgbol/",
    "segmentation": [
        [
            "So let us start now the next talk will be by your front on a linear separation drifting.",
            "Games and posting OK. Alright thank you.",
            "And this is a little bit of a different title and different talk than what I was planning to talk in the title.",
            "There was online learning and just occurred to me after working on this that it's just too much for one talk, so.",
            "The online learning stuff will have to wait for a different opportunity.",
            "OK, So what is this?",
            "OK, let me let me try to use this.",
            "Can you hear me now?",
            "OK. Alright, I never performed with a microphone, so alright, let me see.",
            "OK, so to start motivating this."
        ],
        [
            "Program this talk.",
            "I want to tell you about the problem that I've been working on for a very long time, which is problem with Adaboost.",
            "So probably you heard about Adaboost and many people use it, but many people that actually use it know that there is a very serious problem with it, and that is when you have you add label noise to your data.",
            "OK, so here is an example of a real data set.",
            "The Irvine database we took the later.",
            "The letter database from the Irvine data set and we made it into a binary problem because we want to concentrate on one issue.",
            "So we look at the letters FIJ versus all the other letters.",
            "OK. And if you use Adaboost or logic boost on that problem, you get very very good performance.",
            "OK however, if you add label noise, so you basically take the labels and flip 20% of them then you get horrible performance.",
            "OK, so you added 20% noise, but the noise the error on unseen data now increased not just to 20% but 233 or 31%.",
            "OK so logic boots tend to be a little bit better, but both of them are not so good.",
            "So I've been working on this problem.",
            "Maybe for the last 10 years.",
            "And finally, I think that they have a good solution.",
            "And what is the issue really?",
            "Is the boosting puts too much weight on outliers?",
            "So if you have problems if you have examples that are inherently noisy, so they're on the incorrect side, then boosting will tend to put more and more weight on them and either will just get stuck there or eventually will overfit.",
            "So you need to give up on outliers.",
            "Now that's a very hard issue, yes.",
            "So with bagging, the problem is much much less and and indeed what this is a lot of this work is motivated by is by the cases in which bagging is not so good is much better than boosting.",
            "OK, but he."
        ],
        [
            "Is the new algorithm that I'm going to tell you about robust boost where you with when you add this label noise, you really don't increase the amount of error by that much OK, and that is very compareable to.",
            "Bagging C 4.5 OK so we get the same result yeah.",
            "Yeah I can.",
            "I can work with that.",
            "Do you have another one, OK. OK. OK, so so it's more even impressive if you look at the performance of this generated classifier relative to the original labels.",
            "OK, so with the original labels, if you look at the at the Adaboost or logic boost you have a huge amount of error, but with respect when you use this robust boost algorithm, you get only three point, 7% error.",
            "OK, so you get actually that the algorithm literally corrects most of the mistakes.",
            "OK, so bagging does it too, but but I'm going to tell you how algorithm does it.",
            "OK, so this is the."
        ],
        [
            "Plan of the talk.",
            "I'm going to tell you about the relationship between label noise and the problems with convex loss functions.",
            "OK.",
            "I'll switch.",
            "Hope there won't be feedback from this, I guess disconnect.",
            "If I know how to disconnect.",
            "Yeah.",
            "OK, so you can hear it.",
            "Oh yeah, I guess you can.",
            "OK sorry, OK.",
            "So let me put it closer.",
            "OK you can hear me now yes good OK, very good.",
            "So we're going to start with this then I'm going to tell you about a very old algorithm boost by majority.",
            "And then I'm going to tell you about how I need to go through continuous time analysis to go to this new algorithm, robust boost and I'll show you."
        ],
        [
            "Some experimental results that explain these new things that you've seen.",
            "Just the summary.",
            "OK, so late."
        ],
        [
            "Noise and convex loss functions.",
            "So many algorithms use when they want to do classification.",
            "They use some convex loss function.",
            "That's what they really minimize.",
            "OK, so that includes the Perceptron algorithm, Adaboost logic boost an soft margin, SVM.",
            "OK, they all use some kind of convex loss function when they need to deal with those examples that are inherently incorrect.",
            "OK, so this all works great when the data is linearly separable and that's what we like to concentrate on.",
            "But when you don't have linear separability then you have some real problem, right?",
            "Because you're paying for examples that are incorrect and Moreover you're paying for them more the further they are from the decision boundary, and there's really no reason for that other than you want to work with convex loss functions.",
            "OK, so the problem is that convex functions are very poor approximation for the classification error.",
            "But on the other hand, we have no efficient algorithms for minimizing non convex functions.",
            "So really inherently this is a computational issue, right?",
            "We don't know how to really deal with non convex functions and what I will and basically giving up is a way of being not convex.",
            "So somehow I need to address this."
        ],
        [
            "And this is just to illustrate my point, here is the classification error if we talk about margins, so hopefully everybody knows what I mean by margin.",
            "It's simply the sum of the output of our some classifier before we take a threshold.",
            "OK, so classification rule is just taking the just the step function, which we would like to really minimize.",
            "Then what we really use is if we use perceptron, we use this hedge.",
            "This it's called the.",
            "What is called?",
            "I forget the this this this kind of function if we use support, we soft margin SVM.",
            "We use this function that actually starts to pay, not just when it's zero when it crosses some margin one.",
            "And this is Adaboost, which goes up very, very fast exponentially.",
            "And this is logic boost that goes less fast.",
            "Essentially asymptotically it goes linearly, so it's better in terms of not over emphasizing these examples, but they all.",
            "Overemphasize these examples."
        ],
        [
            "OK, so in fact in 2008 Phil Long in Rochester Video in ICML 2008, they showed that this is inherently a problem, that this is an unresolvable problem.",
            "You cannot use convex loss functions, and this is basically their example.",
            "So this is basically a linearly classifiable data set.",
            "OK, so you have the blue points and you have the red points and you have this separating hyperplane.",
            "In between them.",
            "OK, so it's perfectly separable.",
            "No algorithm will have any problem.",
            "OK, now suppose you add to this some 10% label noise, so each one of these things has now noise associated with it.",
            "OK, so you have some typical examples, 10% of them.",
            "What will happen with the convex loss function?",
            "This is where these different we name these examples different ways.",
            "These are called the large margin ones because they are furthest from this original boundary.",
            "And these are called the pullers, and these are the penalize are now just starting when we add this label noise.",
            "The best classifier we can use is still the same right.",
            "This is the best classifier.",
            "Right, there's no nothing.",
            "Nothing better.",
            "Well, you can play a little bit with it if you don't change any example, but other than that you just want to stay with the same thing.",
            "However, if you use a convex loss function, what you want, you have to get is something like this.",
            "And why is that?",
            "That is because those large margin examples that are very, very far from the origin on them on the.",
            "I hope you can see my laser point on them that you pay a lot for the typical examples because they're very far from the boundary.",
            "So had it not been for this puller out here?",
            "Basically it would make the separator very, very close to here, right?",
            "Because because those examples are pulling very hard, but how will they pull?",
            "Well, there are many many directions to pull, so that's where these pullers break the symmetry.",
            "They cause you to go in a particular direction.",
            "And what is a lot of examples that are right in here very close to the origin.",
            "They're going to get actually.",
            "Classified incorrectly simply because they have very little influence.",
            "There's too close to everything to any boundary, so the amount you pay for them is not that big, and in fact you can prove."
        ],
        [
            "That for any convex loss function, there exists a linearly separable distribution such that when you add independent label noise to it, the linear classifier that minimize the loss function has very poor classification error.",
            "OK, so there's nothing you can do, as long as you were using convex loss functions.",
            "We're going to have this problem, so everything suffers from this other boosts soft margin support vector machines.",
            "Everything OK, alright, so how?"
        ],
        [
            "Am I going to overcome it?",
            "So I'm going to tell you about a very old algorithm actually older than other boosts.",
            "Believe it or not, it's called boost by majority, and through that I'll introduce this notion of drifting games, which is central to the analysis.",
            "OK."
        ],
        [
            "So what is boost by majority?",
            "So we think about boosting now as a game that is going on between a booster and a learner.",
            "OK. And the boosting is restricted to generate extremely simple rules.",
            "Just I mean weighted sum is a simple thing, but this is even simpler.",
            "It's unweighted sum, so each one of the week rules is going to get weight 1.",
            "OK, and Moreover, we're going to decide in advance how many rules we're going to combine.",
            "OK, so that all seems extremely restrictive, but this last restriction is actually what is going to give us the ability to do non convex functions.",
            "OK, I will see why in a minute.",
            "OK so here is how the game proceeds.",
            "It's literally a game in the game theoretical sense.",
            "OK, so the booster assigns weight to the training examples OK?",
            "Then the learner has to choose a rule whose error with respect to the chosen weights is smaller than half minus gamma.",
            "OK, so it has to be better than random guessing by a given amount gamma.",
            "This gamma is set up ahead of time, just like T. OK, then this rule is added to the majority to the majority rule.",
            "OK, and the goal of the booster is to minimize the number of errors at the end when you get all of these T rules, you want to have the minimal number of mistakes.",
            "OK, so it's a very well defined game.",
            "Any questions about it?",
            "OK.",
            "So."
        ],
        [
            "So that's what we.",
            "It's essentially a special case of what we call a drifting game, so a drifting game is this game that in this case happens on the integer line.",
            "Each location is simply this margin, which is the label Y + 1 + 1 or minus one times the sum of the rules that we have up to that point.",
            "OK, so that's basically the majority.",
            "The correct or incorrect nusoor incorrectness of the majority rule at that point.",
            "And then we think about the examples as chips.",
            "OK, that are all initially sitting on the origin.",
            "OK, so chips are examples.",
            "And then I contains the examples for which the difference between the number of correct and incorrect rules is.",
            "I actually I'll use S most of the time for this for this, so there's a typo here.",
            "But basically this bin here initially contains all the bins because initially we don't have a rule, so all of the examples have vote exactly 0.",
            "OK, now I want to do a slight change here, just because it would make everything that follows much easier so."
        ],
        [
            "I want to think about the continuous chip limit.",
            "What does this mean?",
            "Well, you can think about it very simply as the number of training examples going to Infinity.",
            "OK so, but that's not really what the way that I would like you to think about it.",
            "Instead, I would like to think about it as the examples are continuous probability mass OK, and there is a measure of probability measure mu defined on them.",
            "OK, so it's now a continuous mass instead of, so the advantage of that is that you can divide it in any old way.",
            "Unlike discrete chips that you might have problems dividing them in the ways that you want and that gives a lot of that basically makes the game tight.",
            "In a way that you'll see OK.",
            "So."
        ],
        [
            "Now we can think about this game is going on in lattice, right?",
            "So basically all the chips start at zero, then at the first step some of them go to one.",
            "Some of them go to minus one at the next step some of them go to two, some of them go to 0 back to zero and so on.",
            "And let's assume that the total number of iterations we're going to do is odd, OK, and why they want to do that?",
            "Because otherwise at the end we might end up with something that you have exactly the same number of votes, plus in the number of number of votes minus, and then you kind of have to break the tie.",
            "So when you have it odd.",
            "It's much cleaner.",
            "OK, just keep it that that way.",
            "Alright, so here's how the game goes on.",
            "Oak."
        ],
        [
            "So we have this initial configuration.",
            "Everything is in the origin.",
            "Now the boot."
        ],
        [
            "Other assigns weights to these examples.",
            "Actually it assigns density to the bins.",
            "That's the right way to think about it, but you can think about it this way too.",
            "And initially there's only one place where all the examples are, so it just gives them a uniform distribution.",
            "I mean, it doesn't really matter what weight it gives them.",
            "And what the learner now has to do.",
            "It has to choose."
        ],
        [
            "Subset on this, on which it will predict correctly according to the uniform distribution.",
            "So we choose.",
            "Is that OK?",
            "And then we have this move.",
            "OK, so these examples go up in these."
        ],
        [
            "Samples go down.",
            "The incorrect examples go down.",
            "That's clear enough.",
            "OK, now we have some more information.",
            "Now we have some examples in which we made one correct thing and some examples on which we made some incorrect thing.",
            "And now the question of which weights to put on them becomes more interesting.",
            "OK, so let's say that we put on them the weights zero point."
        ],
        [
            "6 and 0.4.",
            "OK, that's actually turns out to be the optimal weights to put on them when this gamma is 0.1.",
            "OK, that's that's the way we're going to put on them.",
            "And now the weak learner has."
        ],
        [
            "To choose subsets of each one, but it actually has to pay more attention to this set, right?",
            "Because I put more weight on it and here it actually can split things so that you have more incorrect and correct because I gave it this freedom.",
            "OK, now this is the move."
        ],
        [
            "OK, and now again we're going to assign them weights now."
        ],
        [
            "This is we're just talking about 3 iterations, so at this point there's it's actually very obvious what to do, and that is those examples that are here.",
            "We don't need to put on them any weight because they are already correct and even however we change, we add the last hypothesis.",
            "They're going to stay correct.",
            "More Interestingly, these examples that are here are given to be incorrect.",
            "Right, they're going to be incorrect no matter what we do, because they can't cross the zero, so these are the examples in which we got things twice correct and will give going to give them 0 weight.",
            "But also these are going to be the ones that we got twice incorrect, and we're going to give them zero wait.",
            "So this is our first example of giving up on examples that are too hard.",
            "Right, something nonconvex is happening here.",
            "OK, and so we put all of the weights on these examples that are in the middle and those now the weak learner has to split into 50."
        ],
        [
            "Now, because we didn't put any weight on these, it can do the worst thing on them.",
            "It can basically predict incorrectly on all of them.",
            "OK, but it doesn't matter.",
            "OK, now we're going to move this way."
        ],
        [
            "OK, and this is basically our final configuration."
        ],
        [
            "And here the majority vote is correct.",
            "All the time and on all of these and on these ones it's incorrect all the time.",
            "OK, so that's basically the end of the game and and the goal of the booster is to is to somehow try to guarantee that this is as big as possible.",
            "OK.",
            "So."
        ],
        [
            "Oh, so to make a Long story short.",
            "It's not very easy to think about what should the booster do, but it's very, very easy to think about what should the weak learner do.",
            "OK, so the weak learner has a very simple optimal strategy.",
            "It chooses half plus gamma of the weight from each bin, always independent of what happened in the past.",
            "OK, now it's easy to see that that is completely insensitive to what the booster does, right?",
            "Because if I give you 1/2 plus gram of each and every bin, then of course I give you half plus gamma of the total weight.",
            "Right, so that's clear.",
            "The interesting thing is that that the that the booster can actually force the weak learner to not do any better than that.",
            "That's really the new stuff.",
            "Well, new as in 1995.",
            "OK, so equivalent to producing too.",
            "This is you can think about it differently.",
            "You can think now because we actually define the measure even though we defined it just formally.",
            "We can think about this as equivalent to a weak learner that produces the correct answer in dependently at random with probability half plus gamma.",
            "That's interesting, right?",
            "Because if I give you T rules, each one of them gives you the correct answer with probability half plus gamma.",
            "Then obviously after I do some small number of iterations, I'm going to get the majority is correct most of the time.",
            "So it's amazing in a sense that this kind of very very stupid weak learner is essentially our worst adversary.",
            "That's something in here.",
            "OK, so OK. How do we analyze this thing formally?",
            "And how do we actually come up with a proof that we have a strategy to force the weak learner not to do any better?"
        ],
        [
            "We define this thing that I called potential.",
            "Essentially it's the same thing as what we called before loss, but I need to give it a new name, otherwise it will get confused.",
            "OK, so the potential of a configuration is the probability of examples.",
            "This new probability that I define.",
            "On which the final majority vote is incorrect.",
            "Given that the configuration after iteration T is this configuration, so we're giving you a configuration and that the remaining steps the learner plays optimally OK, so basically I'm looking at it like this.",
            "If the learner always gives me like things that look independent, well, it doesn't really matter what I do, then it's all fine.",
            "But now suppose the learner did something stupid right.",
            "It actually gave me some bins, it made more correct ones and some bins it did let correct ones.",
            "What should I do I mean?",
            "How should I weigh the example so that it can't do things worse well?",
            "What I'm going to do is I'm going to say OK. At this point it did something really stupid, but from now on it's going to do the smart thing.",
            "OK, because it's not stupid.",
            "So so that's really what configuration measures.",
            "What will I gain at the end if from the following point after this configuration is reached, you'll always the adversary will always play optimally?",
            "So we can think about two particularly interesting ones in terms of this configuration.",
            "First, the original configuration OK, the original configuration, everything is at 0, where at times zero.",
            "OK, so this is the initial potential.",
            "Then we have the training error of the final majority vote is the potential of the final configuration simply because, well, if we say in the remaining steps, well, there are no remaining steps, right?",
            "So the potential at the final step is simply the training error.",
            "So what we're going to have is that the boosting algorithm chooses the weight so that the total potential does not increase.",
            "Ever cannot increase.",
            "The weak learner cannot do anything to increase it.",
            "And what do we have at our hand?",
            "We have the weight, right?",
            "We can put the weight where we want.",
            "So that would basically tell us that the initial potential is bigger, larger, equal to the final training error.",
            "OK, so if we can set the initial potential to something, then it will be the final training error.",
            "Now if you think about it potentially is really related to each bin, right?",
            "So each bin kind of has its history coming forward and that."
        ],
        [
            "What we're going to define now?",
            "It's the potential for a particular time and that particular bin is.",
            "OK, so that's the fraction of the examples in Benesse after iteration T on which the final majority rule will be incorrect, assuming that the adversary plays optimally.",
            "OK, that's kind of like a long thing to try to parse, but in fact it is something very simple.",
            "It is the probability of at most this number of heads when you flip this number, you do this number of coin flips where the probability of the head is half plus gamma.",
            "OK, so this.",
            "Thing if you just go back to the definitions and calculate it for the optimal strategy, then it is simply this equation.",
            "OK, so you can just calculate it in closed form.",
            "OK so this is.",
            "This is what it is.",
            "It's the binomial distribution or binomial distribution is the standard definition like this with these parameters.",
            "OK, and it has an interesting recursion, right?",
            "So this potential potential at been at time T -- 1 it been S is basically the the weighted average of the potentials at the next time step according to the probabilities that you reach them.",
            "OK, so that's what it is and you have these boundary conditions where at the end the potential is simply whether you're correct or incorrect.",
            "And at the beginning it is this binomial right?",
            "If we just plug in 00, all of the bins, all of the chips initially are at zero at time 0.",
            "So that's exactly the potential.",
            "Now what is this?",
            "This is the minimal probability under the binomial distribution of T. / 2 heads.",
            "Inti coin flips where the probability of head is half plus gamma actually should be.",
            "Yeah, less probability of having less than T, then T / 2 heads.",
            "OK, so that's again this vanishingly small probability.",
            "OK, so."
        ],
        [
            "Here is how the.",
            "Proof basically goes now that we have all of these components in place, we basically say OK, FTS is the probability of the examples in bin S after iteration T. So that's like how they are distributed and the total potential.",
            "You can just think about it as the weight as the average over these bin potentials according to the distribution of the bins there.",
            "And now you define this DTS.",
            "That's basically the advantage that the weak learner puts on the examples on that particular bin.",
            "So the measure of examples on which you got things correct minus the measure on what you got things incorrect for a particular bin.",
            "OK, now if you just do a very simple kind of algebra and where what goes where you find that the total potential at iteration T plus one is the total potential iteration T plus this expression.",
            "OK. And this expression turns out to be an OK.",
            "So in this wait here is WTS is the difference between two neighboring potentials at time T + 1.",
            "OK, so it's essentially if you think about the potential.",
            "It's the gradient of the potential.",
            "OK. And what you can immediately see is that if these advantages that you have over the bins weighted according to these weights that I formally defined here.",
            "Are larger than half plus gamma, which is exactly what the weak learner is supposed to give us.",
            "Then the potential AT plus one is smaller equal to the potential AT.",
            "OK, so that basically finishes it because basically it says if we set our weights to be this way and these are explicit formula for the potential, then we're guaranteed that the potential that we have at the beginning is exactly the potential that we have at the end.",
            "So if we start with the potential 0.01 at the beginning.",
            "Then the training error at the end will be 0.01 at most."
        ],
        [
            "OK, so.",
            "This is how this is what boost by majority basically tells you.",
            "It says that if you use these weights, it's kind of a long question, but it just comes out of what I told you before.",
            "It's particular binomial term.",
            "Then it guarantees that the potential never increases, and that guarantees that the initial potential, which is epsilon, which is just this binomial set according to gamma and T, is a bound on the final training error.",
            "OK, and in fact this is the optimal boosting method.",
            "40 iterations when you don't.",
            "When you give equal weights to the to the rules and more intra."
        ],
        [
            "Singly is to look at how do these potential in weight evolve?",
            "So as I said, the potential is this loss function and the weight is the gradient of the loss function.",
            "OK, so this is the weight and this is the potential.",
            "So here's the."
        ],
        [
            "Menschel"
        ],
        [
            "And here is how it is."
        ],
        [
            "Evolves and what you see?"
        ],
        [
            "Is that this?"
        ],
        [
            "Point the potential starts to not be convex.",
            "OK, and accordingly the weight function has a peak.",
            "It actually has a maximum here."
        ],
        [
            "OK, so that's how."
        ],
        [
            "It evolves and what will be familiar is this last step right?",
            "Remember that in the last step we said the only examples we care about are all those that are exactly in the middle.",
            "OK, all of these we gave up in and all of these we gave up on.",
            "OK, so that's basically how."
        ],
        [
            "This new"
        ],
        [
            "Algorithm works."
        ],
        [
            "It's actually not new.",
            "As I said, it's 1995."
        ],
        [
            "So, and this is how it relates to the other boosting algorithms.",
            "OK, so if you have, this is the.",
            "This is the weight and potential for other boosting algorithms, so exponential Adaboost is always confusing because the derivative of an exponent is an exponent, so the potential and the wait look exactly the same, but for logic boots that was suggested by Trevor Hastie Friedman and Tibshirani that that is the equation for the potential.",
            "And this is the equation for the wait.",
            "OK, so it's much better than boosting because what the weight does it basically at some point the weight just get bounded.",
            "You don't get too much weight.",
            "OK, but this this boost by majority does something much more extreme.",
            "It basically gives more and more weight on examples that are hard.",
            "And then it basically gives up just says oh, these examples are too hard.",
            "I'm just going to not predict on them.",
            "Remember that was the problem with the with the Adaboost, it wouldn't give up on noisy examples."
        ],
        [
            "OK, so we have some hope here.",
            "Let me just do a high level summary.",
            "The worst case adversary splits each bin into half minus gamma, incorrect and half plus gamma correct?",
            "That's the worst case adversary, so that is kind of like when you think about nature being the worst case.",
            "It's actually this IID case.",
            "That is very, very strange, but true.",
            "An alternative interpretation is that this is a random walk with IID steps.",
            "OK, you just having a random walk with a slight bias towards going up.",
            "So the algorithm is derived essentially as the optimal response to this simple worst case adversary.",
            "OK, so that's that's kind of high level summary."
        ],
        [
            "OK, now we're going to go to boosting and continuous time.",
            "So really the question is OK.",
            "This is from 1995.",
            "Adaboost was published in 1997.",
            "How come?",
            "Why didn't we use this one from the start?"
        ],
        [
            "Well, boost by majority is not practical.",
            "Why is it not practical?",
            "Because you need to know all of these parameters ahead of time.",
            "You need to know what error you're shooting for.",
            "You need to know how much advantage you need OK, and that's just not realistic.",
            "I mean, even if you could set these, then you can easily calculate the number of boosting iterations that you need behaves like one over the advantage squared times log one over epsilon.",
            "So if your advantage is you assume that you're weak learner advantages 1%, you're committed immediately to doing 10,000 iterations of boosting.",
            "Nobody in practice wants to do that.",
            "Right, and you don't know what the gamma is going to be in advance.",
            "So here is basically why Adaboost was so successful, because Adaboost actually for those that don't know, stands for adaptive boosting.",
            "Not for boosting written order so well.",
            "Some people think that, but no, it's adaptive boosting, and the reason that it is adaptive, it is that it adapts to this sequence of gammas that you get.",
            "OK, you don't need to know anything about those gammas ahead of time, you just get them as you go, and then you use them so you don't need any parameters.",
            "OK, you don't need to know anything in advance, it's a parameter free algorithm and practical algorithms need to be parameter free.",
            "That's what, yes.",
            "We'll get to the margin in a minute.",
            "Haven't talked about margins yet.",
            "OK, so there's no need to set parameters in advance.",
            "It generates a weighted majority rule which is nicer and you can decide when to stop using cross validation.",
            "You don't have to decide ahead of time.",
            "I'm going to run for this number of iterations and remember everything that boost by majority was is that you give up when you're close to the end.",
            "In other boost you don't know when you're close to the end, so of course you never give up.",
            "OK, so the real question was how can we make boost by majority adaptive?",
            "OK, that's really now the next."
        ],
        [
            "OK, so to make other boost boost by majority adaptive like Adaboost, we are going to do an interesting thing which is letting the timestep go to 0.",
            "OK, so this is the number of iterations that we need for getting boost by majority to work.",
            "OK, so suppose that we have it setting for some gamma.",
            "Well if we have a weak learner that would work for one gamma.",
            "It of course works also for gamma smaller right?",
            "So the idea is that we keep epsilon fixed and we let gamma go to 0.",
            "And then we let the number of iterations go to Infinity.",
            "OK, so that's just a complete thought experiment, not something that we're actually going to do, but as a thought experiment, we can do it.",
            "Right, and what will come out of it?",
            "OK, so first of all, just as a technical thing, instead of the iterations being 123 and so on, we're going to quantify the iterations as one over big T2 over big T and so on up to one.",
            "OK, so we're going to put it on the segment 01, and here is the most important thing.",
            "The same week rule is going to be added many times, right?",
            "Because suppose we have gamma that we're asking is 1% and we get a rule.",
            "That is, it's error is 40% OK, so we add it.",
            "We go through the mechanism.",
            "We get the new distribution.",
            "We look at the same rule.",
            "It still has an advantage of nine percent.",
            "We add it again.",
            "Go through this mechanism.",
            "We add it again and so on.",
            "We had the same rule many, many times, one after the other.",
            "What does that really mean?",
            "We're giving this rule some weight.",
            "Right, if you think about it, at the end, each rule is going to have a different weight.",
            "And when I'm going to stop, we're going to stop when this advantage is smaller than this gamma that we said, OK, but this advantage, is going to go to 0.",
            "So essentially we're going to stop exactly when it has no advantage.",
            "Which is exactly what Adaboost does, right?",
            "Remember it basically if you know how to boost it, it updates the weights so that the last rule has no advantage.",
            "So this would give us an adaptive boosting and weighted majority rule.",
            "However, it's hard to take this limit and now I'll give you a little bit about why this limit is hard to take because random walks in continuous time are.",
            "Weird thing you know, we kind of say, oh, just take the limit of continuous time.",
            "Actually it requires going to a completely different type of map.",
            "OK, so why is that?"
        ],
        [
            "So we remember the game lattice.",
            "We had it before.",
            "Now we're just going to re read."
        ],
        [
            "Index things a little bit, right?",
            "So we're going to go from instead of 123.",
            "We're going to go 1 third 2/3 one OK, and so on.",
            "OK, so that's all good.",
            "So now the question is how should we go up and down in terms of the location, right?",
            "How should we do that in order to get a meaningful limit?",
            "So the intuitive thing that I tried initially is to let this Delta of how much we go up and down B according to 1 / T and that seems to make sense, right?",
            "So we go like this.",
            "OK this."
        ],
        [
            "When we have a step of length third.",
            "Then we go like this."
        ],
        [
            "And this is what we have when we have a step in length and nine, and that all seems very good.",
            "However, if you think about it, what was underlying our underlying process of the optimal adversary?",
            "It was random walk.",
            "OK, so what's the variance?",
            "What's the distribution that we're getting here at the end?",
            "Well, the variance is T * 1 / T ^2.",
            "Right, because if each step is 1 / T then the variance of the step is 1 / T squared and then if we take T independent steps of variance 1 / T squared we get the total variance is 1 / T. And So what we get is that the variance at the end the limit is 0, so.",
            "That just doesn't work right.",
            "Basically we get the limit when we think about what is the adversary doing.",
            "It's basically keeping all of the examples exactly at 0.",
            "It doesn't.",
            "That's just not the right limit, just a meaningless limit.",
            "OK, so we have to."
        ],
        [
            "So something very weird.",
            "We actually have to say that the step size goes like 1 / sqrt T. We have to do that.",
            "There's no other choice, and here is how it looks after you."
        ],
        [
            "Take so here is when you take one step.",
            "So in one step you go plus one or minus one.",
            "If you take three steps, each step is of size 1 / sqrt 3 three steps, so that's gives you a total range of sqrt 3.",
            "OK then, if you go to now."
        ],
        [
            "Line that gives you a total range of three.",
            "And if you go T going to Infinity, what you get for so first of all, in every step you get, the variance remains one, so that's good.",
            "What is annoying is that the range goes to Infinity.",
            "And that's really annoying.",
            "But that's really what is called Brownian motion.",
            "So strange as it might sound, this is how particles in air move.",
            "They move with some small probability at arbitrarily high speed.",
            "Of course not.",
            "The speed of light, but but it goes very very high speed.",
            "This is the truth.",
            "This is really the limit that you have to use.",
            "OK, so."
        ],
        [
            "Now we take all of the analysis that we did before and we do it in continuous time.",
            "OK, so in the discrete time, what I basically showed you equations relating time T to time T + 1 based on random walks.",
            "OK, so that's all good and well when we go to continuous time we have to use.",
            "As I said a completely different type of math based on this strange limit where the size of the step is 1 / sqrt T. And that gives you what's called differential equations for the evolution of the distribution.",
            "For people who know that that's the Kolmogorov forward and backward equations for diffusion.",
            "And if you want to look at how a particular particle moves, that's even weirder.",
            "It's what's called stochastic differential equation that's called ito calculus.",
            "OK, so.",
            "Just just out of curiosity, who here knows Ito calculus?",
            "Wow kredible number so yeah, so I'm not going to go into this because most people don't know, but I'm going to just give you bye."
        ],
        [
            "Example, so how do you go from the potential in boost by majority to the potential in what's called Brown boost, which is the first version of adaptive boost by majority?",
            "OK so here is what we had as the potential update rule going backwards in time for boost by majority.",
            "And we had this condition at the end time and we had this condition at the beginning time.",
            "What we get for Brown boost when we go to the continuous time limit is that we get the equation that describes the potential function.",
            "Is this differential equation, which is really for those who know this is simply the Kolmogorov backward equation for.",
            "Brownian motion with drift gamma.",
            "Or with drift beta.",
            "And then the boundary conditions are actually the same.",
            "OK, so this boundary condition is the same and this boundary condition is essentially the same, it's just equal to Phi 00, which is no longer the binomial.",
            "Now it's the error function.",
            "OK, so this is how it looks.",
            "Really, really understand it.",
            "You'll have to go to the paper but.",
            "But that's what it is.",
            "OK, now we have actually an adaptive.",
            "Version of Boost by majority."
        ],
        [
            "OK, so this is not the end of the story unfortunately, because as I said before, we actually want to work with the margins, not going to even get into the whole theory of margins.",
            "But there is something about margins, at least.",
            "We believe there is that that explains why the error that you get on training data is much closer to what you get on the test data, even even better than just looking at the training error you're looking at the margin error.",
            "OK, So what is that?",
            "So we want to instead of minimize it training."
        ],
        [
            "Here we want to minimize the number of training examples with margins smaller than Theta.",
            "It's very similar to support vector machines.",
            "In a way, it's just in a different kind of space, but but we don't want just to get all of the get the examples correct.",
            "We want to get them correct plus correct plus a margin.",
            "OK, when we do that we need to control the magnitude.",
            "Right, so if you know the boost by the boosting the margin paper, we normalize things there.",
            "According to the L1 loss here, we actually use a different way of normalizing, which I won't get into, but we need to do that and we want to allow confidence rated weak learners.",
            "So basically weak learners that are allowed not just to say plus one or minus one, but any number in between.",
            "So all of that requires going more through this whole mechanism of stochastic calculus.",
            "But we go through it and what we get is this horrible."
        ],
        [
            "Alright, so I'll just let you stare at it for awhile.",
            "I'm not going to even try to explain it with the amount of time that I have, but this is closed form from relation for what weights you need to use.",
            "If you want to use basically adaptive boost by majority while taking into account, normalizing the margins or maximizing the minimizing the error respect to the margin and normalize and keeping the norm of the weights controlled.",
            "Not bounded, but control."
        ],
        [
            "OK, so so I'm not going to say more about that.",
            "There is a paper on my website you can read, but I'm going to show you experimental results and then I'm going to actually encourage you to go and try your own experimental results."
        ],
        [
            "OK, so first let's go and see what happens in the long answer video problem.",
            "Remember the long answer video problem where we said no convex loss function would be able to do that.",
            "And robust boost is able to do that.",
            "OK so here is."
        ],
        [
            "A little animation of what happens when you run other boost on the long answer video problem and you see up here are the iterations, so there's still going on, but Adaboost is kind of stuck.",
            "Nothing is going on.",
            "What are these examples?",
            "These are the large margins.",
            "Once these are the pullers and these down, here are the penal izers, these are the ones that it can't decide which side to put them simply because this is the potential function is trying to minimize, and this potential function.",
            "These examples that are the mistaken here is pulling are pulling too hard and they're not letting these examples have their say.",
            "OK, what happens with logic Boo?"
        ],
        [
            "It's a little bit better because the potential increases here, essentially linearly.",
            "But again, we essentially get the same problem.",
            "We get the problem that when you minimize convex loss function, you can't get to the true good classifier.",
            "And here is what happens with rogue."
        ],
        [
            "Possible something much more interesting is happening.",
            "OK, So what you see is that basically we're giving up on examples out here and out here.",
            "And we're concentrating our efforts on the examples here and finally out here.",
            "Now we actually got the correct rule.",
            "OK, so that probably went too fast, but but let me maybe go and try to show what happens in the interesting case.",
            "OK, so initially out around here or maybe around here.",
            "Here what you have here instead of steps you have this time.",
            "Sorry I. OK, what you have up here is the time so time in our case needs to go to one, right.",
            "It kind of continually increases until it gets to one or almost to 1.",
            "When we are in an initial part where time is still pretty low, then basically let's go even a little bit earlier at this stage.",
            "A robust boost looks very, very much like Adaboost or logic boosts OK, because you're still basically in the convex part of this potential function.",
            "OK, but as you go on.",
            "As you go on, what you see is that that you get out of the convex part, and these examples out here.",
            "Basically these are the large margin examples.",
            "Those mistakes on them actually get no wait at all.",
            "We're giving up on them literally giving up on them.",
            "OK, so that is kind of nice, but.",
            "Why would you believe me?",
            "This is some weird constructed example and so on, but as I showed you in the beginning, there was a natural example that we always also tried and better than that."
        ],
        [
            "There is the software that now I have on SourceForge where frantically working to make it all the bugs and everything, but it's pretty much working well now and so you can go just go to J Boorstin there's version 2.0 that supports robust boost.",
            "OK, and also gives you a nice score visualizer that lets you see these things.",
            "OK, so let's see what the score Visualizer gives us on this data that I showed you in the beginning.",
            "20% noise added to a problem which is almost linearly separable."
        ],
        [
            "OK, so these are the results I showed you."
        ],
        [
            "Before, let's first see what logic Boost does when there's zero noise.",
            "OK in this."
        ],
        [
            "Case this is the visualization tool what we get."
        ],
        [
            "It is that."
        ],
        [
            "Using logic."
        ],
        [
            "First, you get very, very nice separation.",
            "OK, so you run 2000 iterations and what you see is that the negative example, the positive examples are very very nicely separated, not completely at zero.",
            "There is still some mislabels, but almost completely OK, and this is the case where you don't need to worry too much about outliers, right?",
            "And everything works."
        ],
        [
            "But here is what happens with logic boost when you have 20."
        ],
        [
            "Added noise."
        ],
        [
            "Basically you go on iteration."
        ],
        [
            "100"
        ],
        [
            "Iteration 500, iteration 2000, and nothing really is going on.",
            "It's not separating it any better.",
            "Why is that?",
            "Because you have a lot of these negative examples that are up here.",
            "Well, they have to be up there because there are the mislabeled one.",
            "You have a lot of blue examples that are down here and those are getting a lot of weight and they're basically making the process not not get to the good minimum."
        ],
        [
            "OK."
        ],
        [
            "Here is what you get with robust boosts, so with robust."
        ],
        [
            "Initially it looks very much the same, but what you see up here is that it starts to give up on it."
        ],
        [
            "Samples."
        ],
        [
            "And now it's giving up on even more examples and even more examples, and in fact, at this stage it gives weight 0 to almost all the examples it gives non 0 weight only to a tiny fraction.",
            "That is very very close to the boundary."
        ],
        [
            "And this is how it continues.",
            "And it actually not only does that it actually pushes examples to have large margin while giving up on other examples.",
            "OK, so."
        ],
        [
            "So basically if."
        ],
        [
            "Go to the end so it ends actually much quicker."
        ],
        [
            "It ends."
        ],
        [
            "Essentially, at iteration 431 S. Beyond that you don't get any improvement on a cross validation.",
            "So we don't need to run it to 2000 iterations, but this step you got this very, very nice thing that we were hoping for right?",
            "Basically you have all of the positive examples here and all of the mislabeled positives.",
            "Because it gave up on them.",
            "It let them go where they wanted.",
            "OK.",
            "So."
        ],
        [
            "This is basically how what I told you today and that's it."
        ],
        [
            "Yes.",
            "Yes, support vector machines perform really poorly when you have inherent label noise when the Bayes optimal rule is far from zero error.",
            "Whenever that happens, I think this will perform better.",
            "Support vector machines really don't perform very well when the underlying, So what you do with support vector machines you go to a higher dimensional space in the hope that things will become very close to linearly separable.",
            "What I'm saying here is you might be in many situations where you have to give up right so you can't get below error of 20%, but can you get close to 20%?",
            "So in that case, I think that it's going to be much, much better than soft margin support vector machine.",
            "In fact, you can combine the two.",
            "So you can use this kind of approach to learn support vectors.",
            "More questions.",
            "So, So what?",
            "What I'd really like you to do is just Google J Boost and the software is ready for you there to download, and I'd be very interested for people to try it and tell me what they find out.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us start now the next talk will be by your front on a linear separation drifting.",
                    "label": 1
                },
                {
                    "sent": "Games and posting OK. Alright thank you.",
                    "label": 0
                },
                {
                    "sent": "And this is a little bit of a different title and different talk than what I was planning to talk in the title.",
                    "label": 0
                },
                {
                    "sent": "There was online learning and just occurred to me after working on this that it's just too much for one talk, so.",
                    "label": 0
                },
                {
                    "sent": "The online learning stuff will have to wait for a different opportunity.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is this?",
                    "label": 0
                },
                {
                    "sent": "OK, let me let me try to use this.",
                    "label": 0
                },
                {
                    "sent": "Can you hear me now?",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, I never performed with a microphone, so alright, let me see.",
                    "label": 0
                },
                {
                    "sent": "OK, so to start motivating this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Program this talk.",
                    "label": 0
                },
                {
                    "sent": "I want to tell you about the problem that I've been working on for a very long time, which is problem with Adaboost.",
                    "label": 0
                },
                {
                    "sent": "So probably you heard about Adaboost and many people use it, but many people that actually use it know that there is a very serious problem with it, and that is when you have you add label noise to your data.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is an example of a real data set.",
                    "label": 0
                },
                {
                    "sent": "The Irvine database we took the later.",
                    "label": 0
                },
                {
                    "sent": "The letter database from the Irvine data set and we made it into a binary problem because we want to concentrate on one issue.",
                    "label": 0
                },
                {
                    "sent": "So we look at the letters FIJ versus all the other letters.",
                    "label": 0
                },
                {
                    "sent": "OK. And if you use Adaboost or logic boost on that problem, you get very very good performance.",
                    "label": 0
                },
                {
                    "sent": "OK however, if you add label noise, so you basically take the labels and flip 20% of them then you get horrible performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so you added 20% noise, but the noise the error on unseen data now increased not just to 20% but 233 or 31%.",
                    "label": 0
                },
                {
                    "sent": "OK so logic boots tend to be a little bit better, but both of them are not so good.",
                    "label": 0
                },
                {
                    "sent": "So I've been working on this problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe for the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "And finally, I think that they have a good solution.",
                    "label": 0
                },
                {
                    "sent": "And what is the issue really?",
                    "label": 0
                },
                {
                    "sent": "Is the boosting puts too much weight on outliers?",
                    "label": 1
                },
                {
                    "sent": "So if you have problems if you have examples that are inherently noisy, so they're on the incorrect side, then boosting will tend to put more and more weight on them and either will just get stuck there or eventually will overfit.",
                    "label": 1
                },
                {
                    "sent": "So you need to give up on outliers.",
                    "label": 0
                },
                {
                    "sent": "Now that's a very hard issue, yes.",
                    "label": 0
                },
                {
                    "sent": "So with bagging, the problem is much much less and and indeed what this is a lot of this work is motivated by is by the cases in which bagging is not so good is much better than boosting.",
                    "label": 0
                },
                {
                    "sent": "OK, but he.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the new algorithm that I'm going to tell you about robust boost where you with when you add this label noise, you really don't increase the amount of error by that much OK, and that is very compareable to.",
                    "label": 0
                },
                {
                    "sent": "Bagging C 4.5 OK so we get the same result yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah I can.",
                    "label": 0
                },
                {
                    "sent": "I can work with that.",
                    "label": 0
                },
                {
                    "sent": "Do you have another one, OK. OK. OK, so so it's more even impressive if you look at the performance of this generated classifier relative to the original labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so with the original labels, if you look at the at the Adaboost or logic boost you have a huge amount of error, but with respect when you use this robust boost algorithm, you get only three point, 7% error.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get actually that the algorithm literally corrects most of the mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, so bagging does it too, but but I'm going to tell you how algorithm does it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Plan of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you about the relationship between label noise and the problems with convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'll switch.",
                    "label": 0
                },
                {
                    "sent": "Hope there won't be feedback from this, I guess disconnect.",
                    "label": 0
                },
                {
                    "sent": "If I know how to disconnect.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can hear it.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, I guess you can.",
                    "label": 0
                },
                {
                    "sent": "OK sorry, OK.",
                    "label": 0
                },
                {
                    "sent": "So let me put it closer.",
                    "label": 0
                },
                {
                    "sent": "OK you can hear me now yes good OK, very good.",
                    "label": 1
                },
                {
                    "sent": "So we're going to start with this then I'm going to tell you about a very old algorithm boost by majority.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to tell you about how I need to go through continuous time analysis to go to this new algorithm, robust boost and I'll show you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some experimental results that explain these new things that you've seen.",
                    "label": 0
                },
                {
                    "sent": "Just the summary.",
                    "label": 0
                },
                {
                    "sent": "OK, so late.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Noise and convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "So many algorithms use when they want to do classification.",
                    "label": 1
                },
                {
                    "sent": "They use some convex loss function.",
                    "label": 0
                },
                {
                    "sent": "That's what they really minimize.",
                    "label": 0
                },
                {
                    "sent": "OK, so that includes the Perceptron algorithm, Adaboost logic boost an soft margin, SVM.",
                    "label": 1
                },
                {
                    "sent": "OK, they all use some kind of convex loss function when they need to deal with those examples that are inherently incorrect.",
                    "label": 0
                },
                {
                    "sent": "OK, so this all works great when the data is linearly separable and that's what we like to concentrate on.",
                    "label": 0
                },
                {
                    "sent": "But when you don't have linear separability then you have some real problem, right?",
                    "label": 0
                },
                {
                    "sent": "Because you're paying for examples that are incorrect and Moreover you're paying for them more the further they are from the decision boundary, and there's really no reason for that other than you want to work with convex loss functions.",
                    "label": 1
                },
                {
                    "sent": "OK, so the problem is that convex functions are very poor approximation for the classification error.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, we have no efficient algorithms for minimizing non convex functions.",
                    "label": 1
                },
                {
                    "sent": "So really inherently this is a computational issue, right?",
                    "label": 0
                },
                {
                    "sent": "We don't know how to really deal with non convex functions and what I will and basically giving up is a way of being not convex.",
                    "label": 0
                },
                {
                    "sent": "So somehow I need to address this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just to illustrate my point, here is the classification error if we talk about margins, so hopefully everybody knows what I mean by margin.",
                    "label": 0
                },
                {
                    "sent": "It's simply the sum of the output of our some classifier before we take a threshold.",
                    "label": 0
                },
                {
                    "sent": "OK, so classification rule is just taking the just the step function, which we would like to really minimize.",
                    "label": 0
                },
                {
                    "sent": "Then what we really use is if we use perceptron, we use this hedge.",
                    "label": 0
                },
                {
                    "sent": "This it's called the.",
                    "label": 0
                },
                {
                    "sent": "What is called?",
                    "label": 0
                },
                {
                    "sent": "I forget the this this this kind of function if we use support, we soft margin SVM.",
                    "label": 0
                },
                {
                    "sent": "We use this function that actually starts to pay, not just when it's zero when it crosses some margin one.",
                    "label": 0
                },
                {
                    "sent": "And this is Adaboost, which goes up very, very fast exponentially.",
                    "label": 0
                },
                {
                    "sent": "And this is logic boost that goes less fast.",
                    "label": 0
                },
                {
                    "sent": "Essentially asymptotically it goes linearly, so it's better in terms of not over emphasizing these examples, but they all.",
                    "label": 0
                },
                {
                    "sent": "Overemphasize these examples.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in fact in 2008 Phil Long in Rochester Video in ICML 2008, they showed that this is inherently a problem, that this is an unresolvable problem.",
                    "label": 0
                },
                {
                    "sent": "You cannot use convex loss functions, and this is basically their example.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a linearly classifiable data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have the blue points and you have the red points and you have this separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "In between them.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's perfectly separable.",
                    "label": 0
                },
                {
                    "sent": "No algorithm will have any problem.",
                    "label": 0
                },
                {
                    "sent": "OK, now suppose you add to this some 10% label noise, so each one of these things has now noise associated with it.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have some typical examples, 10% of them.",
                    "label": 0
                },
                {
                    "sent": "What will happen with the convex loss function?",
                    "label": 0
                },
                {
                    "sent": "This is where these different we name these examples different ways.",
                    "label": 0
                },
                {
                    "sent": "These are called the large margin ones because they are furthest from this original boundary.",
                    "label": 1
                },
                {
                    "sent": "And these are called the pullers, and these are the penalize are now just starting when we add this label noise.",
                    "label": 0
                },
                {
                    "sent": "The best classifier we can use is still the same right.",
                    "label": 0
                },
                {
                    "sent": "This is the best classifier.",
                    "label": 0
                },
                {
                    "sent": "Right, there's no nothing.",
                    "label": 0
                },
                {
                    "sent": "Nothing better.",
                    "label": 0
                },
                {
                    "sent": "Well, you can play a little bit with it if you don't change any example, but other than that you just want to stay with the same thing.",
                    "label": 0
                },
                {
                    "sent": "However, if you use a convex loss function, what you want, you have to get is something like this.",
                    "label": 0
                },
                {
                    "sent": "And why is that?",
                    "label": 1
                },
                {
                    "sent": "That is because those large margin examples that are very, very far from the origin on them on the.",
                    "label": 0
                },
                {
                    "sent": "I hope you can see my laser point on them that you pay a lot for the typical examples because they're very far from the boundary.",
                    "label": 0
                },
                {
                    "sent": "So had it not been for this puller out here?",
                    "label": 0
                },
                {
                    "sent": "Basically it would make the separator very, very close to here, right?",
                    "label": 0
                },
                {
                    "sent": "Because because those examples are pulling very hard, but how will they pull?",
                    "label": 0
                },
                {
                    "sent": "Well, there are many many directions to pull, so that's where these pullers break the symmetry.",
                    "label": 0
                },
                {
                    "sent": "They cause you to go in a particular direction.",
                    "label": 0
                },
                {
                    "sent": "And what is a lot of examples that are right in here very close to the origin.",
                    "label": 0
                },
                {
                    "sent": "They're going to get actually.",
                    "label": 0
                },
                {
                    "sent": "Classified incorrectly simply because they have very little influence.",
                    "label": 0
                },
                {
                    "sent": "There's too close to everything to any boundary, so the amount you pay for them is not that big, and in fact you can prove.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That for any convex loss function, there exists a linearly separable distribution such that when you add independent label noise to it, the linear classifier that minimize the loss function has very poor classification error.",
                    "label": 1
                },
                {
                    "sent": "OK, so there's nothing you can do, as long as you were using convex loss functions.",
                    "label": 0
                },
                {
                    "sent": "We're going to have this problem, so everything suffers from this other boosts soft margin support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Everything OK, alright, so how?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Am I going to overcome it?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to tell you about a very old algorithm actually older than other boosts.",
                    "label": 0
                },
                {
                    "sent": "Believe it or not, it's called boost by majority, and through that I'll introduce this notion of drifting games, which is central to the analysis.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is boost by majority?",
                    "label": 1
                },
                {
                    "sent": "So we think about boosting now as a game that is going on between a booster and a learner.",
                    "label": 1
                },
                {
                    "sent": "OK. And the boosting is restricted to generate extremely simple rules.",
                    "label": 0
                },
                {
                    "sent": "Just I mean weighted sum is a simple thing, but this is even simpler.",
                    "label": 0
                },
                {
                    "sent": "It's unweighted sum, so each one of the week rules is going to get weight 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and Moreover, we're going to decide in advance how many rules we're going to combine.",
                    "label": 0
                },
                {
                    "sent": "OK, so that all seems extremely restrictive, but this last restriction is actually what is going to give us the ability to do non convex functions.",
                    "label": 0
                },
                {
                    "sent": "OK, I will see why in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK so here is how the game proceeds.",
                    "label": 0
                },
                {
                    "sent": "It's literally a game in the game theoretical sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so the booster assigns weight to the training examples OK?",
                    "label": 1
                },
                {
                    "sent": "Then the learner has to choose a rule whose error with respect to the chosen weights is smaller than half minus gamma.",
                    "label": 1
                },
                {
                    "sent": "OK, so it has to be better than random guessing by a given amount gamma.",
                    "label": 1
                },
                {
                    "sent": "This gamma is set up ahead of time, just like T. OK, then this rule is added to the majority to the majority rule.",
                    "label": 1
                },
                {
                    "sent": "OK, and the goal of the booster is to minimize the number of errors at the end when you get all of these T rules, you want to have the minimal number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very well defined game.",
                    "label": 0
                },
                {
                    "sent": "Any questions about it?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what we.",
                    "label": 0
                },
                {
                    "sent": "It's essentially a special case of what we call a drifting game, so a drifting game is this game that in this case happens on the integer line.",
                    "label": 0
                },
                {
                    "sent": "Each location is simply this margin, which is the label Y + 1 + 1 or minus one times the sum of the rules that we have up to that point.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically the majority.",
                    "label": 0
                },
                {
                    "sent": "The correct or incorrect nusoor incorrectness of the majority rule at that point.",
                    "label": 0
                },
                {
                    "sent": "And then we think about the examples as chips.",
                    "label": 0
                },
                {
                    "sent": "OK, that are all initially sitting on the origin.",
                    "label": 0
                },
                {
                    "sent": "OK, so chips are examples.",
                    "label": 0
                },
                {
                    "sent": "And then I contains the examples for which the difference between the number of correct and incorrect rules is.",
                    "label": 1
                },
                {
                    "sent": "I actually I'll use S most of the time for this for this, so there's a typo here.",
                    "label": 0
                },
                {
                    "sent": "But basically this bin here initially contains all the bins because initially we don't have a rule, so all of the examples have vote exactly 0.",
                    "label": 0
                },
                {
                    "sent": "OK, now I want to do a slight change here, just because it would make everything that follows much easier so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to think about the continuous chip limit.",
                    "label": 1
                },
                {
                    "sent": "What does this mean?",
                    "label": 1
                },
                {
                    "sent": "Well, you can think about it very simply as the number of training examples going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK so, but that's not really what the way that I would like you to think about it.",
                    "label": 1
                },
                {
                    "sent": "Instead, I would like to think about it as the examples are continuous probability mass OK, and there is a measure of probability measure mu defined on them.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's now a continuous mass instead of, so the advantage of that is that you can divide it in any old way.",
                    "label": 0
                },
                {
                    "sent": "Unlike discrete chips that you might have problems dividing them in the ways that you want and that gives a lot of that basically makes the game tight.",
                    "label": 0
                },
                {
                    "sent": "In a way that you'll see OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can think about this game is going on in lattice, right?",
                    "label": 0
                },
                {
                    "sent": "So basically all the chips start at zero, then at the first step some of them go to one.",
                    "label": 0
                },
                {
                    "sent": "Some of them go to minus one at the next step some of them go to two, some of them go to 0 back to zero and so on.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that the total number of iterations we're going to do is odd, OK, and why they want to do that?",
                    "label": 0
                },
                {
                    "sent": "Because otherwise at the end we might end up with something that you have exactly the same number of votes, plus in the number of number of votes minus, and then you kind of have to break the tie.",
                    "label": 0
                },
                {
                    "sent": "So when you have it odd.",
                    "label": 0
                },
                {
                    "sent": "It's much cleaner.",
                    "label": 0
                },
                {
                    "sent": "OK, just keep it that that way.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's how the game goes on.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have this initial configuration.",
                    "label": 1
                },
                {
                    "sent": "Everything is in the origin.",
                    "label": 0
                },
                {
                    "sent": "Now the boot.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other assigns weights to these examples.",
                    "label": 1
                },
                {
                    "sent": "Actually it assigns density to the bins.",
                    "label": 0
                },
                {
                    "sent": "That's the right way to think about it, but you can think about it this way too.",
                    "label": 0
                },
                {
                    "sent": "And initially there's only one place where all the examples are, so it just gives them a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "I mean, it doesn't really matter what weight it gives them.",
                    "label": 0
                },
                {
                    "sent": "And what the learner now has to do.",
                    "label": 0
                },
                {
                    "sent": "It has to choose.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subset on this, on which it will predict correctly according to the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So we choose.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "And then we have this move.",
                    "label": 0
                },
                {
                    "sent": "OK, so these examples go up in these.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples go down.",
                    "label": 0
                },
                {
                    "sent": "The incorrect examples go down.",
                    "label": 0
                },
                {
                    "sent": "That's clear enough.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have some more information.",
                    "label": 0
                },
                {
                    "sent": "Now we have some examples in which we made one correct thing and some examples on which we made some incorrect thing.",
                    "label": 0
                },
                {
                    "sent": "And now the question of which weights to put on them becomes more interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say that we put on them the weights zero point.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "6 and 0.4.",
                    "label": 0
                },
                {
                    "sent": "OK, that's actually turns out to be the optimal weights to put on them when this gamma is 0.1.",
                    "label": 1
                },
                {
                    "sent": "OK, that's that's the way we're going to put on them.",
                    "label": 0
                },
                {
                    "sent": "And now the weak learner has.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To choose subsets of each one, but it actually has to pay more attention to this set, right?",
                    "label": 0
                },
                {
                    "sent": "Because I put more weight on it and here it actually can split things so that you have more incorrect and correct because I gave it this freedom.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is the move.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and now again we're going to assign them weights now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is we're just talking about 3 iterations, so at this point there's it's actually very obvious what to do, and that is those examples that are here.",
                    "label": 0
                },
                {
                    "sent": "We don't need to put on them any weight because they are already correct and even however we change, we add the last hypothesis.",
                    "label": 0
                },
                {
                    "sent": "They're going to stay correct.",
                    "label": 0
                },
                {
                    "sent": "More Interestingly, these examples that are here are given to be incorrect.",
                    "label": 0
                },
                {
                    "sent": "Right, they're going to be incorrect no matter what we do, because they can't cross the zero, so these are the examples in which we got things twice correct and will give going to give them 0 weight.",
                    "label": 0
                },
                {
                    "sent": "But also these are going to be the ones that we got twice incorrect, and we're going to give them zero wait.",
                    "label": 0
                },
                {
                    "sent": "So this is our first example of giving up on examples that are too hard.",
                    "label": 0
                },
                {
                    "sent": "Right, something nonconvex is happening here.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we put all of the weights on these examples that are in the middle and those now the weak learner has to split into 50.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, because we didn't put any weight on these, it can do the worst thing on them.",
                    "label": 0
                },
                {
                    "sent": "It can basically predict incorrectly on all of them.",
                    "label": 0
                },
                {
                    "sent": "OK, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK, now we're going to move this way.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this is basically our final configuration.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here the majority vote is correct.",
                    "label": 0
                },
                {
                    "sent": "All the time and on all of these and on these ones it's incorrect all the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically the end of the game and and the goal of the booster is to is to somehow try to guarantee that this is as big as possible.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, so to make a Long story short.",
                    "label": 0
                },
                {
                    "sent": "It's not very easy to think about what should the booster do, but it's very, very easy to think about what should the weak learner do.",
                    "label": 0
                },
                {
                    "sent": "OK, so the weak learner has a very simple optimal strategy.",
                    "label": 0
                },
                {
                    "sent": "It chooses half plus gamma of the weight from each bin, always independent of what happened in the past.",
                    "label": 0
                },
                {
                    "sent": "OK, now it's easy to see that that is completely insensitive to what the booster does, right?",
                    "label": 0
                },
                {
                    "sent": "Because if I give you 1/2 plus gram of each and every bin, then of course I give you half plus gamma of the total weight.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's clear.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that that the that the booster can actually force the weak learner to not do any better than that.",
                    "label": 0
                },
                {
                    "sent": "That's really the new stuff.",
                    "label": 0
                },
                {
                    "sent": "Well, new as in 1995.",
                    "label": 0
                },
                {
                    "sent": "OK, so equivalent to producing too.",
                    "label": 1
                },
                {
                    "sent": "This is you can think about it differently.",
                    "label": 0
                },
                {
                    "sent": "You can think now because we actually define the measure even though we defined it just formally.",
                    "label": 0
                },
                {
                    "sent": "We can think about this as equivalent to a weak learner that produces the correct answer in dependently at random with probability half plus gamma.",
                    "label": 1
                },
                {
                    "sent": "That's interesting, right?",
                    "label": 0
                },
                {
                    "sent": "Because if I give you T rules, each one of them gives you the correct answer with probability half plus gamma.",
                    "label": 0
                },
                {
                    "sent": "Then obviously after I do some small number of iterations, I'm going to get the majority is correct most of the time.",
                    "label": 0
                },
                {
                    "sent": "So it's amazing in a sense that this kind of very very stupid weak learner is essentially our worst adversary.",
                    "label": 0
                },
                {
                    "sent": "That's something in here.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK. How do we analyze this thing formally?",
                    "label": 0
                },
                {
                    "sent": "And how do we actually come up with a proof that we have a strategy to force the weak learner not to do any better?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We define this thing that I called potential.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's the same thing as what we called before loss, but I need to give it a new name, otherwise it will get confused.",
                    "label": 0
                },
                {
                    "sent": "OK, so the potential of a configuration is the probability of examples.",
                    "label": 0
                },
                {
                    "sent": "This new probability that I define.",
                    "label": 0
                },
                {
                    "sent": "On which the final majority vote is incorrect.",
                    "label": 1
                },
                {
                    "sent": "Given that the configuration after iteration T is this configuration, so we're giving you a configuration and that the remaining steps the learner plays optimally OK, so basically I'm looking at it like this.",
                    "label": 0
                },
                {
                    "sent": "If the learner always gives me like things that look independent, well, it doesn't really matter what I do, then it's all fine.",
                    "label": 0
                },
                {
                    "sent": "But now suppose the learner did something stupid right.",
                    "label": 0
                },
                {
                    "sent": "It actually gave me some bins, it made more correct ones and some bins it did let correct ones.",
                    "label": 0
                },
                {
                    "sent": "What should I do I mean?",
                    "label": 0
                },
                {
                    "sent": "How should I weigh the example so that it can't do things worse well?",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to say OK. At this point it did something really stupid, but from now on it's going to do the smart thing.",
                    "label": 0
                },
                {
                    "sent": "OK, because it's not stupid.",
                    "label": 0
                },
                {
                    "sent": "So so that's really what configuration measures.",
                    "label": 0
                },
                {
                    "sent": "What will I gain at the end if from the following point after this configuration is reached, you'll always the adversary will always play optimally?",
                    "label": 0
                },
                {
                    "sent": "So we can think about two particularly interesting ones in terms of this configuration.",
                    "label": 0
                },
                {
                    "sent": "First, the original configuration OK, the original configuration, everything is at 0, where at times zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the initial potential.",
                    "label": 0
                },
                {
                    "sent": "Then we have the training error of the final majority vote is the potential of the final configuration simply because, well, if we say in the remaining steps, well, there are no remaining steps, right?",
                    "label": 0
                },
                {
                    "sent": "So the potential at the final step is simply the training error.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to have is that the boosting algorithm chooses the weight so that the total potential does not increase.",
                    "label": 1
                },
                {
                    "sent": "Ever cannot increase.",
                    "label": 0
                },
                {
                    "sent": "The weak learner cannot do anything to increase it.",
                    "label": 0
                },
                {
                    "sent": "And what do we have at our hand?",
                    "label": 0
                },
                {
                    "sent": "We have the weight, right?",
                    "label": 0
                },
                {
                    "sent": "We can put the weight where we want.",
                    "label": 0
                },
                {
                    "sent": "So that would basically tell us that the initial potential is bigger, larger, equal to the final training error.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we can set the initial potential to something, then it will be the final training error.",
                    "label": 0
                },
                {
                    "sent": "Now if you think about it potentially is really related to each bin, right?",
                    "label": 0
                },
                {
                    "sent": "So each bin kind of has its history coming forward and that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we're going to define now?",
                    "label": 0
                },
                {
                    "sent": "It's the potential for a particular time and that particular bin is.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the fraction of the examples in Benesse after iteration T on which the final majority rule will be incorrect, assuming that the adversary plays optimally.",
                    "label": 1
                },
                {
                    "sent": "OK, that's kind of like a long thing to try to parse, but in fact it is something very simple.",
                    "label": 0
                },
                {
                    "sent": "It is the probability of at most this number of heads when you flip this number, you do this number of coin flips where the probability of the head is half plus gamma.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Thing if you just go back to the definitions and calculate it for the optimal strategy, then it is simply this equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can just calculate it in closed form.",
                    "label": 0
                },
                {
                    "sent": "OK so this is.",
                    "label": 0
                },
                {
                    "sent": "This is what it is.",
                    "label": 0
                },
                {
                    "sent": "It's the binomial distribution or binomial distribution is the standard definition like this with these parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, and it has an interesting recursion, right?",
                    "label": 0
                },
                {
                    "sent": "So this potential potential at been at time T -- 1 it been S is basically the the weighted average of the potentials at the next time step according to the probabilities that you reach them.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what it is and you have these boundary conditions where at the end the potential is simply whether you're correct or incorrect.",
                    "label": 0
                },
                {
                    "sent": "And at the beginning it is this binomial right?",
                    "label": 0
                },
                {
                    "sent": "If we just plug in 00, all of the bins, all of the chips initially are at zero at time 0.",
                    "label": 0
                },
                {
                    "sent": "So that's exactly the potential.",
                    "label": 0
                },
                {
                    "sent": "Now what is this?",
                    "label": 0
                },
                {
                    "sent": "This is the minimal probability under the binomial distribution of T. / 2 heads.",
                    "label": 0
                },
                {
                    "sent": "Inti coin flips where the probability of head is half plus gamma actually should be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, less probability of having less than T, then T / 2 heads.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's again this vanishingly small probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is how the.",
                    "label": 0
                },
                {
                    "sent": "Proof basically goes now that we have all of these components in place, we basically say OK, FTS is the probability of the examples in bin S after iteration T. So that's like how they are distributed and the total potential.",
                    "label": 1
                },
                {
                    "sent": "You can just think about it as the weight as the average over these bin potentials according to the distribution of the bins there.",
                    "label": 0
                },
                {
                    "sent": "And now you define this DTS.",
                    "label": 0
                },
                {
                    "sent": "That's basically the advantage that the weak learner puts on the examples on that particular bin.",
                    "label": 0
                },
                {
                    "sent": "So the measure of examples on which you got things correct minus the measure on what you got things incorrect for a particular bin.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you just do a very simple kind of algebra and where what goes where you find that the total potential at iteration T plus one is the total potential iteration T plus this expression.",
                    "label": 0
                },
                {
                    "sent": "OK. And this expression turns out to be an OK.",
                    "label": 0
                },
                {
                    "sent": "So in this wait here is WTS is the difference between two neighboring potentials at time T + 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's essentially if you think about the potential.",
                    "label": 0
                },
                {
                    "sent": "It's the gradient of the potential.",
                    "label": 0
                },
                {
                    "sent": "OK. And what you can immediately see is that if these advantages that you have over the bins weighted according to these weights that I formally defined here.",
                    "label": 0
                },
                {
                    "sent": "Are larger than half plus gamma, which is exactly what the weak learner is supposed to give us.",
                    "label": 0
                },
                {
                    "sent": "Then the potential AT plus one is smaller equal to the potential AT.",
                    "label": 0
                },
                {
                    "sent": "OK, so that basically finishes it because basically it says if we set our weights to be this way and these are explicit formula for the potential, then we're guaranteed that the potential that we have at the beginning is exactly the potential that we have at the end.",
                    "label": 0
                },
                {
                    "sent": "So if we start with the potential 0.01 at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Then the training error at the end will be 0.01 at most.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is how this is what boost by majority basically tells you.",
                    "label": 0
                },
                {
                    "sent": "It says that if you use these weights, it's kind of a long question, but it just comes out of what I told you before.",
                    "label": 0
                },
                {
                    "sent": "It's particular binomial term.",
                    "label": 0
                },
                {
                    "sent": "Then it guarantees that the potential never increases, and that guarantees that the initial potential, which is epsilon, which is just this binomial set according to gamma and T, is a bound on the final training error.",
                    "label": 0
                },
                {
                    "sent": "OK, and in fact this is the optimal boosting method.",
                    "label": 0
                },
                {
                    "sent": "40 iterations when you don't.",
                    "label": 0
                },
                {
                    "sent": "When you give equal weights to the to the rules and more intra.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Singly is to look at how do these potential in weight evolve?",
                    "label": 0
                },
                {
                    "sent": "So as I said, the potential is this loss function and the weight is the gradient of the loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the weight and this is the potential.",
                    "label": 0
                },
                {
                    "sent": "So here's the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Menschel",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is how it is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evolves and what you see?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that this?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point the potential starts to not be convex.",
                    "label": 0
                },
                {
                    "sent": "OK, and accordingly the weight function has a peak.",
                    "label": 0
                },
                {
                    "sent": "It actually has a maximum here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's how.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It evolves and what will be familiar is this last step right?",
                    "label": 0
                },
                {
                    "sent": "Remember that in the last step we said the only examples we care about are all those that are exactly in the middle.",
                    "label": 0
                },
                {
                    "sent": "OK, all of these we gave up in and all of these we gave up on.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's basically how.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This new",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm works.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's actually not new.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's 1995.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, and this is how it relates to the other boosting algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have, this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the weight and potential for other boosting algorithms, so exponential Adaboost is always confusing because the derivative of an exponent is an exponent, so the potential and the wait look exactly the same, but for logic boots that was suggested by Trevor Hastie Friedman and Tibshirani that that is the equation for the potential.",
                    "label": 0
                },
                {
                    "sent": "And this is the equation for the wait.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's much better than boosting because what the weight does it basically at some point the weight just get bounded.",
                    "label": 0
                },
                {
                    "sent": "You don't get too much weight.",
                    "label": 0
                },
                {
                    "sent": "OK, but this this boost by majority does something much more extreme.",
                    "label": 0
                },
                {
                    "sent": "It basically gives more and more weight on examples that are hard.",
                    "label": 0
                },
                {
                    "sent": "And then it basically gives up just says oh, these examples are too hard.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to not predict on them.",
                    "label": 0
                },
                {
                    "sent": "Remember that was the problem with the with the Adaboost, it wouldn't give up on noisy examples.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have some hope here.",
                    "label": 0
                },
                {
                    "sent": "Let me just do a high level summary.",
                    "label": 0
                },
                {
                    "sent": "The worst case adversary splits each bin into half minus gamma, incorrect and half plus gamma correct?",
                    "label": 0
                },
                {
                    "sent": "That's the worst case adversary, so that is kind of like when you think about nature being the worst case.",
                    "label": 0
                },
                {
                    "sent": "It's actually this IID case.",
                    "label": 0
                },
                {
                    "sent": "That is very, very strange, but true.",
                    "label": 0
                },
                {
                    "sent": "An alternative interpretation is that this is a random walk with IID steps.",
                    "label": 0
                },
                {
                    "sent": "OK, you just having a random walk with a slight bias towards going up.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is derived essentially as the optimal response to this simple worst case adversary.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's kind of high level summary.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we're going to go to boosting and continuous time.",
                    "label": 0
                },
                {
                    "sent": "So really the question is OK.",
                    "label": 0
                },
                {
                    "sent": "This is from 1995.",
                    "label": 0
                },
                {
                    "sent": "Adaboost was published in 1997.",
                    "label": 0
                },
                {
                    "sent": "How come?",
                    "label": 0
                },
                {
                    "sent": "Why didn't we use this one from the start?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, boost by majority is not practical.",
                    "label": 0
                },
                {
                    "sent": "Why is it not practical?",
                    "label": 0
                },
                {
                    "sent": "Because you need to know all of these parameters ahead of time.",
                    "label": 0
                },
                {
                    "sent": "You need to know what error you're shooting for.",
                    "label": 0
                },
                {
                    "sent": "You need to know how much advantage you need OK, and that's just not realistic.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if you could set these, then you can easily calculate the number of boosting iterations that you need behaves like one over the advantage squared times log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So if your advantage is you assume that you're weak learner advantages 1%, you're committed immediately to doing 10,000 iterations of boosting.",
                    "label": 0
                },
                {
                    "sent": "Nobody in practice wants to do that.",
                    "label": 0
                },
                {
                    "sent": "Right, and you don't know what the gamma is going to be in advance.",
                    "label": 0
                },
                {
                    "sent": "So here is basically why Adaboost was so successful, because Adaboost actually for those that don't know, stands for adaptive boosting.",
                    "label": 0
                },
                {
                    "sent": "Not for boosting written order so well.",
                    "label": 0
                },
                {
                    "sent": "Some people think that, but no, it's adaptive boosting, and the reason that it is adaptive, it is that it adapts to this sequence of gammas that you get.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't need to know anything about those gammas ahead of time, you just get them as you go, and then you use them so you don't need any parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't need to know anything in advance, it's a parameter free algorithm and practical algorithms need to be parameter free.",
                    "label": 0
                },
                {
                    "sent": "That's what, yes.",
                    "label": 0
                },
                {
                    "sent": "We'll get to the margin in a minute.",
                    "label": 0
                },
                {
                    "sent": "Haven't talked about margins yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's no need to set parameters in advance.",
                    "label": 0
                },
                {
                    "sent": "It generates a weighted majority rule which is nicer and you can decide when to stop using cross validation.",
                    "label": 0
                },
                {
                    "sent": "You don't have to decide ahead of time.",
                    "label": 0
                },
                {
                    "sent": "I'm going to run for this number of iterations and remember everything that boost by majority was is that you give up when you're close to the end.",
                    "label": 0
                },
                {
                    "sent": "In other boost you don't know when you're close to the end, so of course you never give up.",
                    "label": 0
                },
                {
                    "sent": "OK, so the real question was how can we make boost by majority adaptive?",
                    "label": 0
                },
                {
                    "sent": "OK, that's really now the next.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to make other boost boost by majority adaptive like Adaboost, we are going to do an interesting thing which is letting the timestep go to 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the number of iterations that we need for getting boost by majority to work.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose that we have it setting for some gamma.",
                    "label": 0
                },
                {
                    "sent": "Well if we have a weak learner that would work for one gamma.",
                    "label": 0
                },
                {
                    "sent": "It of course works also for gamma smaller right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we keep epsilon fixed and we let gamma go to 0.",
                    "label": 0
                },
                {
                    "sent": "And then we let the number of iterations go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just a complete thought experiment, not something that we're actually going to do, but as a thought experiment, we can do it.",
                    "label": 0
                },
                {
                    "sent": "Right, and what will come out of it?",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, just as a technical thing, instead of the iterations being 123 and so on, we're going to quantify the iterations as one over big T2 over big T and so on up to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to put it on the segment 01, and here is the most important thing.",
                    "label": 0
                },
                {
                    "sent": "The same week rule is going to be added many times, right?",
                    "label": 0
                },
                {
                    "sent": "Because suppose we have gamma that we're asking is 1% and we get a rule.",
                    "label": 0
                },
                {
                    "sent": "That is, it's error is 40% OK, so we add it.",
                    "label": 0
                },
                {
                    "sent": "We go through the mechanism.",
                    "label": 0
                },
                {
                    "sent": "We get the new distribution.",
                    "label": 0
                },
                {
                    "sent": "We look at the same rule.",
                    "label": 0
                },
                {
                    "sent": "It still has an advantage of nine percent.",
                    "label": 0
                },
                {
                    "sent": "We add it again.",
                    "label": 0
                },
                {
                    "sent": "Go through this mechanism.",
                    "label": 0
                },
                {
                    "sent": "We add it again and so on.",
                    "label": 0
                },
                {
                    "sent": "We had the same rule many, many times, one after the other.",
                    "label": 0
                },
                {
                    "sent": "What does that really mean?",
                    "label": 0
                },
                {
                    "sent": "We're giving this rule some weight.",
                    "label": 0
                },
                {
                    "sent": "Right, if you think about it, at the end, each rule is going to have a different weight.",
                    "label": 0
                },
                {
                    "sent": "And when I'm going to stop, we're going to stop when this advantage is smaller than this gamma that we said, OK, but this advantage, is going to go to 0.",
                    "label": 0
                },
                {
                    "sent": "So essentially we're going to stop exactly when it has no advantage.",
                    "label": 0
                },
                {
                    "sent": "Which is exactly what Adaboost does, right?",
                    "label": 0
                },
                {
                    "sent": "Remember it basically if you know how to boost it, it updates the weights so that the last rule has no advantage.",
                    "label": 0
                },
                {
                    "sent": "So this would give us an adaptive boosting and weighted majority rule.",
                    "label": 0
                },
                {
                    "sent": "However, it's hard to take this limit and now I'll give you a little bit about why this limit is hard to take because random walks in continuous time are.",
                    "label": 0
                },
                {
                    "sent": "Weird thing you know, we kind of say, oh, just take the limit of continuous time.",
                    "label": 0
                },
                {
                    "sent": "Actually it requires going to a completely different type of map.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is that?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we remember the game lattice.",
                    "label": 0
                },
                {
                    "sent": "We had it before.",
                    "label": 0
                },
                {
                    "sent": "Now we're just going to re read.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Index things a little bit, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to go from instead of 123.",
                    "label": 0
                },
                {
                    "sent": "We're going to go 1 third 2/3 one OK, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's all good.",
                    "label": 0
                },
                {
                    "sent": "So now the question is how should we go up and down in terms of the location, right?",
                    "label": 0
                },
                {
                    "sent": "How should we do that in order to get a meaningful limit?",
                    "label": 0
                },
                {
                    "sent": "So the intuitive thing that I tried initially is to let this Delta of how much we go up and down B according to 1 / T and that seems to make sense, right?",
                    "label": 0
                },
                {
                    "sent": "So we go like this.",
                    "label": 0
                },
                {
                    "sent": "OK this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we have a step of length third.",
                    "label": 0
                },
                {
                    "sent": "Then we go like this.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what we have when we have a step in length and nine, and that all seems very good.",
                    "label": 0
                },
                {
                    "sent": "However, if you think about it, what was underlying our underlying process of the optimal adversary?",
                    "label": 0
                },
                {
                    "sent": "It was random walk.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the variance?",
                    "label": 0
                },
                {
                    "sent": "What's the distribution that we're getting here at the end?",
                    "label": 0
                },
                {
                    "sent": "Well, the variance is T * 1 / T ^2.",
                    "label": 0
                },
                {
                    "sent": "Right, because if each step is 1 / T then the variance of the step is 1 / T squared and then if we take T independent steps of variance 1 / T squared we get the total variance is 1 / T. And So what we get is that the variance at the end the limit is 0, so.",
                    "label": 0
                },
                {
                    "sent": "That just doesn't work right.",
                    "label": 0
                },
                {
                    "sent": "Basically we get the limit when we think about what is the adversary doing.",
                    "label": 0
                },
                {
                    "sent": "It's basically keeping all of the examples exactly at 0.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "That's just not the right limit, just a meaningless limit.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So something very weird.",
                    "label": 0
                },
                {
                    "sent": "We actually have to say that the step size goes like 1 / sqrt T. We have to do that.",
                    "label": 0
                },
                {
                    "sent": "There's no other choice, and here is how it looks after you.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take so here is when you take one step.",
                    "label": 0
                },
                {
                    "sent": "So in one step you go plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "If you take three steps, each step is of size 1 / sqrt 3 three steps, so that's gives you a total range of sqrt 3.",
                    "label": 0
                },
                {
                    "sent": "OK then, if you go to now.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Line that gives you a total range of three.",
                    "label": 0
                },
                {
                    "sent": "And if you go T going to Infinity, what you get for so first of all, in every step you get, the variance remains one, so that's good.",
                    "label": 0
                },
                {
                    "sent": "What is annoying is that the range goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And that's really annoying.",
                    "label": 0
                },
                {
                    "sent": "But that's really what is called Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So strange as it might sound, this is how particles in air move.",
                    "label": 0
                },
                {
                    "sent": "They move with some small probability at arbitrarily high speed.",
                    "label": 0
                },
                {
                    "sent": "Of course not.",
                    "label": 0
                },
                {
                    "sent": "The speed of light, but but it goes very very high speed.",
                    "label": 0
                },
                {
                    "sent": "This is the truth.",
                    "label": 0
                },
                {
                    "sent": "This is really the limit that you have to use.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we take all of the analysis that we did before and we do it in continuous time.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the discrete time, what I basically showed you equations relating time T to time T + 1 based on random walks.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's all good and well when we go to continuous time we have to use.",
                    "label": 0
                },
                {
                    "sent": "As I said a completely different type of math based on this strange limit where the size of the step is 1 / sqrt T. And that gives you what's called differential equations for the evolution of the distribution.",
                    "label": 0
                },
                {
                    "sent": "For people who know that that's the Kolmogorov forward and backward equations for diffusion.",
                    "label": 0
                },
                {
                    "sent": "And if you want to look at how a particular particle moves, that's even weirder.",
                    "label": 0
                },
                {
                    "sent": "It's what's called stochastic differential equation that's called ito calculus.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Just just out of curiosity, who here knows Ito calculus?",
                    "label": 0
                },
                {
                    "sent": "Wow kredible number so yeah, so I'm not going to go into this because most people don't know, but I'm going to just give you bye.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, so how do you go from the potential in boost by majority to the potential in what's called Brown boost, which is the first version of adaptive boost by majority?",
                    "label": 0
                },
                {
                    "sent": "OK so here is what we had as the potential update rule going backwards in time for boost by majority.",
                    "label": 0
                },
                {
                    "sent": "And we had this condition at the end time and we had this condition at the beginning time.",
                    "label": 0
                },
                {
                    "sent": "What we get for Brown boost when we go to the continuous time limit is that we get the equation that describes the potential function.",
                    "label": 0
                },
                {
                    "sent": "Is this differential equation, which is really for those who know this is simply the Kolmogorov backward equation for.",
                    "label": 0
                },
                {
                    "sent": "Brownian motion with drift gamma.",
                    "label": 0
                },
                {
                    "sent": "Or with drift beta.",
                    "label": 0
                },
                {
                    "sent": "And then the boundary conditions are actually the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so this boundary condition is the same and this boundary condition is essentially the same, it's just equal to Phi 00, which is no longer the binomial.",
                    "label": 0
                },
                {
                    "sent": "Now it's the error function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how it looks.",
                    "label": 0
                },
                {
                    "sent": "Really, really understand it.",
                    "label": 0
                },
                {
                    "sent": "You'll have to go to the paper but.",
                    "label": 0
                },
                {
                    "sent": "But that's what it is.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have actually an adaptive.",
                    "label": 0
                },
                {
                    "sent": "Version of Boost by majority.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is not the end of the story unfortunately, because as I said before, we actually want to work with the margins, not going to even get into the whole theory of margins.",
                    "label": 0
                },
                {
                    "sent": "But there is something about margins, at least.",
                    "label": 0
                },
                {
                    "sent": "We believe there is that that explains why the error that you get on training data is much closer to what you get on the test data, even even better than just looking at the training error you're looking at the margin error.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is that?",
                    "label": 0
                },
                {
                    "sent": "So we want to instead of minimize it training.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we want to minimize the number of training examples with margins smaller than Theta.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to support vector machines.",
                    "label": 0
                },
                {
                    "sent": "In a way, it's just in a different kind of space, but but we don't want just to get all of the get the examples correct.",
                    "label": 0
                },
                {
                    "sent": "We want to get them correct plus correct plus a margin.",
                    "label": 0
                },
                {
                    "sent": "OK, when we do that we need to control the magnitude.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you know the boost by the boosting the margin paper, we normalize things there.",
                    "label": 0
                },
                {
                    "sent": "According to the L1 loss here, we actually use a different way of normalizing, which I won't get into, but we need to do that and we want to allow confidence rated weak learners.",
                    "label": 0
                },
                {
                    "sent": "So basically weak learners that are allowed not just to say plus one or minus one, but any number in between.",
                    "label": 0
                },
                {
                    "sent": "So all of that requires going more through this whole mechanism of stochastic calculus.",
                    "label": 0
                },
                {
                    "sent": "But we go through it and what we get is this horrible.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'll just let you stare at it for awhile.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to even try to explain it with the amount of time that I have, but this is closed form from relation for what weights you need to use.",
                    "label": 0
                },
                {
                    "sent": "If you want to use basically adaptive boost by majority while taking into account, normalizing the margins or maximizing the minimizing the error respect to the margin and normalize and keeping the norm of the weights controlled.",
                    "label": 0
                },
                {
                    "sent": "Not bounded, but control.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so I'm not going to say more about that.",
                    "label": 0
                },
                {
                    "sent": "There is a paper on my website you can read, but I'm going to show you experimental results and then I'm going to actually encourage you to go and try your own experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first let's go and see what happens in the long answer video problem.",
                    "label": 0
                },
                {
                    "sent": "Remember the long answer video problem where we said no convex loss function would be able to do that.",
                    "label": 0
                },
                {
                    "sent": "And robust boost is able to do that.",
                    "label": 0
                },
                {
                    "sent": "OK so here is.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little animation of what happens when you run other boost on the long answer video problem and you see up here are the iterations, so there's still going on, but Adaboost is kind of stuck.",
                    "label": 0
                },
                {
                    "sent": "Nothing is going on.",
                    "label": 0
                },
                {
                    "sent": "What are these examples?",
                    "label": 0
                },
                {
                    "sent": "These are the large margins.",
                    "label": 0
                },
                {
                    "sent": "Once these are the pullers and these down, here are the penal izers, these are the ones that it can't decide which side to put them simply because this is the potential function is trying to minimize, and this potential function.",
                    "label": 0
                },
                {
                    "sent": "These examples that are the mistaken here is pulling are pulling too hard and they're not letting these examples have their say.",
                    "label": 0
                },
                {
                    "sent": "OK, what happens with logic Boo?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a little bit better because the potential increases here, essentially linearly.",
                    "label": 0
                },
                {
                    "sent": "But again, we essentially get the same problem.",
                    "label": 0
                },
                {
                    "sent": "We get the problem that when you minimize convex loss function, you can't get to the true good classifier.",
                    "label": 0
                },
                {
                    "sent": "And here is what happens with rogue.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible something much more interesting is happening.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you see is that basically we're giving up on examples out here and out here.",
                    "label": 0
                },
                {
                    "sent": "And we're concentrating our efforts on the examples here and finally out here.",
                    "label": 0
                },
                {
                    "sent": "Now we actually got the correct rule.",
                    "label": 0
                },
                {
                    "sent": "OK, so that probably went too fast, but but let me maybe go and try to show what happens in the interesting case.",
                    "label": 0
                },
                {
                    "sent": "OK, so initially out around here or maybe around here.",
                    "label": 0
                },
                {
                    "sent": "Here what you have here instead of steps you have this time.",
                    "label": 0
                },
                {
                    "sent": "Sorry I. OK, what you have up here is the time so time in our case needs to go to one, right.",
                    "label": 0
                },
                {
                    "sent": "It kind of continually increases until it gets to one or almost to 1.",
                    "label": 0
                },
                {
                    "sent": "When we are in an initial part where time is still pretty low, then basically let's go even a little bit earlier at this stage.",
                    "label": 0
                },
                {
                    "sent": "A robust boost looks very, very much like Adaboost or logic boosts OK, because you're still basically in the convex part of this potential function.",
                    "label": 0
                },
                {
                    "sent": "OK, but as you go on.",
                    "label": 0
                },
                {
                    "sent": "As you go on, what you see is that that you get out of the convex part, and these examples out here.",
                    "label": 0
                },
                {
                    "sent": "Basically these are the large margin examples.",
                    "label": 0
                },
                {
                    "sent": "Those mistakes on them actually get no wait at all.",
                    "label": 0
                },
                {
                    "sent": "We're giving up on them literally giving up on them.",
                    "label": 0
                },
                {
                    "sent": "OK, so that is kind of nice, but.",
                    "label": 0
                },
                {
                    "sent": "Why would you believe me?",
                    "label": 0
                },
                {
                    "sent": "This is some weird constructed example and so on, but as I showed you in the beginning, there was a natural example that we always also tried and better than that.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is the software that now I have on SourceForge where frantically working to make it all the bugs and everything, but it's pretty much working well now and so you can go just go to J Boorstin there's version 2.0 that supports robust boost.",
                    "label": 0
                },
                {
                    "sent": "OK, and also gives you a nice score visualizer that lets you see these things.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what the score Visualizer gives us on this data that I showed you in the beginning.",
                    "label": 0
                },
                {
                    "sent": "20% noise added to a problem which is almost linearly separable.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are the results I showed you.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before, let's first see what logic Boost does when there's zero noise.",
                    "label": 0
                },
                {
                    "sent": "OK in this.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case this is the visualization tool what we get.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is that.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using logic.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, you get very, very nice separation.",
                    "label": 0
                },
                {
                    "sent": "OK, so you run 2000 iterations and what you see is that the negative example, the positive examples are very very nicely separated, not completely at zero.",
                    "label": 0
                },
                {
                    "sent": "There is still some mislabels, but almost completely OK, and this is the case where you don't need to worry too much about outliers, right?",
                    "label": 0
                },
                {
                    "sent": "And everything works.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here is what happens with logic boost when you have 20.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Added noise.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically you go on iteration.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "100",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iteration 500, iteration 2000, and nothing really is going on.",
                    "label": 0
                },
                {
                    "sent": "It's not separating it any better.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because you have a lot of these negative examples that are up here.",
                    "label": 0
                },
                {
                    "sent": "Well, they have to be up there because there are the mislabeled one.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of blue examples that are down here and those are getting a lot of weight and they're basically making the process not not get to the good minimum.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is what you get with robust boosts, so with robust.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Initially it looks very much the same, but what you see up here is that it starts to give up on it.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now it's giving up on even more examples and even more examples, and in fact, at this stage it gives weight 0 to almost all the examples it gives non 0 weight only to a tiny fraction.",
                    "label": 0
                },
                {
                    "sent": "That is very very close to the boundary.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is how it continues.",
                    "label": 0
                },
                {
                    "sent": "And it actually not only does that it actually pushes examples to have large margin while giving up on other examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically if.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to the end so it ends actually much quicker.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It ends.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, at iteration 431 S. Beyond that you don't get any improvement on a cross validation.",
                    "label": 0
                },
                {
                    "sent": "So we don't need to run it to 2000 iterations, but this step you got this very, very nice thing that we were hoping for right?",
                    "label": 0
                },
                {
                    "sent": "Basically you have all of the positive examples here and all of the mislabeled positives.",
                    "label": 0
                },
                {
                    "sent": "Because it gave up on them.",
                    "label": 0
                },
                {
                    "sent": "It let them go where they wanted.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is basically how what I told you today and that's it.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, support vector machines perform really poorly when you have inherent label noise when the Bayes optimal rule is far from zero error.",
                    "label": 0
                },
                {
                    "sent": "Whenever that happens, I think this will perform better.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines really don't perform very well when the underlying, So what you do with support vector machines you go to a higher dimensional space in the hope that things will become very close to linearly separable.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying here is you might be in many situations where you have to give up right so you can't get below error of 20%, but can you get close to 20%?",
                    "label": 0
                },
                {
                    "sent": "So in that case, I think that it's going to be much, much better than soft margin support vector machine.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can combine the two.",
                    "label": 0
                },
                {
                    "sent": "So you can use this kind of approach to learn support vectors.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "So, So what?",
                    "label": 0
                },
                {
                    "sent": "What I'd really like you to do is just Google J Boost and the software is ready for you there to download, and I'd be very interested for people to try it and tell me what they find out.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}