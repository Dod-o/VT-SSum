{
    "id": "jso4neytvbgh34vumfphwh5hmmx4bb2t",
    "title": "Simple and Deterministic Matrix Sketching",
    "info": {
        "author": [
            "Edo Liberty, Yahoo! Research"
        ],
        "published": "Sept. 27, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2013_liberty_matrix_sketching/",
    "segmentation": [
        [
            "So I want to thank the committee again for choosing my.",
            "Paper this is a great honor.",
            "Thanks for everybody for coming here.",
            "So without further ado, let me just jump right in."
        ],
        [
            "I mean, we all use huge amounts of data.",
            "This is too long, right?",
            "Is this better?",
            "OK, can you hear me?",
            "OK we will, I mean use a lot of data and this data is often represented by a large matrix.",
            "What's happening?",
            "What?",
            "Is this better?",
            "Yeah, OK, so we all use pretty large amounts of data and this data is often representable by a large matrix.",
            "This is true for images, audio, text, you name it.",
            "I don't need to sell you the idea of data being a matrix.",
            "Andy."
        ],
        [
            "Is this matrix are often quite large?",
            "I just plotted here some numbers of sizes of matrices that I come across in Yahoo.",
            "And these are I would think about them is D dimensional vectors of objects.",
            "This could be documents or images or what have you and we think of.",
            "Usually this matrix is being did having the Rose and many many columns in every column is an instance of a documents and you can have a document and you can imagine, for example, that the dimensionality is fixed, but the number of documents grows.",
            "Very fast and we cannot usually work on them on one machine and this."
        ],
        [
            "Tuation is even worse when sometimes the matrix itself is comes as a stream.",
            "So you see the element and you never see it again.",
            "It's just too large to store at all.",
            "This is true in a lot of the image analysis spam detection.",
            "Web crawling and so on."
        ],
        [
            "So let's try to think.",
            "For example, what can we do with the streaming matrix?",
            "So what can you do when you get the columns of a matrix one over after the other for a very long time?",
            "And let's just try to compute the covariance of the matrix.",
            "So I just want to compute a time times A transpose.",
            "OK, so this."
        ],
        [
            "A very simple idea, maybe I don't do anything fancy.",
            "But this of course is possible from the stream.",
            "So I can just take the color of the first column of the matrix, multiply it by itself, transpose, I get the covariance matrix of rank one, and I keep adding those up.",
            "And in the end I get the covariance matrix.",
            "So we can do that, but that already takes N times this square operation.",
            "So for every column that I see I need to create a matrix of size D square.",
            "So if you just if the dimension is a million which is not outrageously high, you already the covariance matrix itself is huge.",
            "It's about a TB of space, and you need to do 10 to the 12 operations floating point operations for each update.",
            "So this is out of the question.",
            "So what I'm trying to show in this slide, even the most basic idea of the basic construct of just computing the covariance matrix, is already impossible in the streaming.",
            "In the streaming setting in large data."
        ],
        [
            "So the idea of sketching or approximating a matrix is not invented by me.",
            "The idea is to efficiently compute a concise representable matrix B, such as either be is roughly like A or BB.",
            "Transpose is roughly like a transpose.",
            "OK, and then if you if you do this efficiently enough and if the approximation is good enough, then you can work with the approximation of the original matrix.",
            "And it turns out with I mean there are many 10s of papers on these topics that given the right sketch, you can actually do dimension reduction, signal denoising, classification, regression, clustering, matrix multiplication, reconstruction recommendation, basically everything that we're used to doing on the matrix we can do on the approximation, given that we can obtain it, and then it's good enough."
        ],
        [
            "So I'm just giving here a list of results I'm not going to go over them, of course.",
            "But these are just the matrix approximation results.",
            "How do you sketch matrices?",
            "The first table?",
            "This was the aggregated by gas money and Shamian Phillips and minimize here monogamy I think is in the audience somewhere.",
            "Who actually have a follow up work to my work?",
            "Aggregated these results and then so these are column subset selection algorithms.",
            "They go over the matrix and just pick some of the columns and throw away the rest.",
            "And somehow, if you pick the columns correctly, you can show that the matrix that you get in the end is a good approximation of the original 1.",
            "There are specification results that just just nullify some of the entries and keep a sparse version of the Matrix.",
            "Again, there is a list of results.",
            "I'm going to go over."
        ],
        [
            "If there are ideas that come from subspace embedding techniques, usually they are linear embeddings.",
            "Again, a long list of papers.",
            "Actually the best paper in stock this year was given for exactly a matrix approximation result by Clarkson Woodruff.",
            "In the last.",
            "The last section is deterministic sketching algorithms.",
            "There are only three this far.",
            "The result by Feldman ET al.",
            "My result and then the result of the chamois and Philips that follow up on my work.",
            "So."
        ],
        [
            "What's the goal?",
            "Our goal is to do the following.",
            "We want to efficiently keep a matrix be with only a few columns, let's say twice, two over epsilon such that the covariance matrix of A&B are essentially the same and what they say the same up to some epsilon times of for business.",
            "Normal, Ferrero, and I can always tweak epsilon to be whatever it is that I want.",
            "Notice that I can.",
            "I can decide in advance what I want the precision to be in just running, so in some sense we don't need to run PCA or SVD and then truncate the values we can.",
            "In advance, decide what's the cut off value is and just compute that OK and the idea is to extend the Frick.",
            "The frequent items algorithm."
        ],
        [
            "Is a very famous algorithm.",
            "I half jokingly say here there was, and it was so good that it was invented four times, you know, but I'm half joking.",
            "It is.",
            "It really is fantastic and it really was invented four times so.",
            "Maybe more, it was published four times.",
            "So here is the simple algorithm.",
            "How am I on time, by the way?",
            "What?",
            "OK, so the basic idea is very simple, so let's assume I get a stream of blocks in different colors.",
            "Let's say I have six possible different colors.",
            "This would be D and by the end of the stream I'm going to ask you how many times have you seen the purple block.",
            "This is the frequency of purple, and the answer here is, say, five.",
            "OK, this is of course."
        ],
        [
            "Easily done in all of this space.",
            "I just keep one counter for every element, but of course this is sometimes impossible.",
            "Think about the router sitting somewhere and seeing different IPS all day long and then one day I come to me, come to it and ask.",
            "You know how many times have you seen 19610721 or a front end device that keeps a search engine front end machine they wants to keep to keep tabs on how many times it's in every query.",
            "Of course it cannot keep the entire history."
        ],
        [
            "So the mission agrees algorithm.",
            "Over the frequent items algorithm.",
            "Keeps only a fixed number of encounters, one of them, at least one of them is always set to 0.",
            "OK, and I just keep at most L different elements."
        ],
        [
            "When I get an element that I have accounted for, increase the counter by one when."
        ],
        [
            "I get an element that I don't have a counter for.",
            "I create a new counter for it and put it set it to one.",
            "But then I I ran out of space OK."
        ],
        [
            "So I need to make some more space right?",
            "I somehow need to always have at least one counter at 0, so I look."
        ],
        [
            "At the median counter.",
            "Let's say let's call it Delta and I just decrease all the count."
        ],
        [
            "Service by Delta.",
            "OK, if the counter was less than Delta, I set it to zero higher than Delta.",
            "I decrease it by Delta and that's it.",
            "I continue."
        ],
        [
            "OK, this is the algorithm and by the."
        ],
        [
            "End of the algorithm, they all frequencies.",
            "Your approximate frequencies are.",
            "Are given by F prime here and these approximate frequencies have."
        ],
        [
            "The following property should look at the last line for a second.",
            "Have this property that.",
            "The real frequency and approximate frequencies are at most of epsilon N apart.",
            "OK. Now I want to go over the proof for a second, not to torment you, but just to.",
            "First, show you that this is the entire proof and also the the proof of the metric.",
            "Sketching result is basically identical, so this will allow me to skip to handwave.",
            "Much more aggressively later.",
            "OK.",
            "So first of all, we know that the approximate frequency F prime is lower than F in the real frequency because we count everything once and somewhat sometimes we delete things so it cannot be higher.",
            "Also, the approximate frequency F prime is larger than the real frequency minus the sum of all Delta T, so Delta T is the how much I delete it in time step T. OK, at the worst case my item participated in each one of those deletions and it was decreased in every time step.",
            "So you have some of Delta T, so the error is at most some of Delta T. But I need to somehow bound how much some of Delta T can be so that I can do also quite easily.",
            "I know that zero is smaller than the sum of my approximate frequencies.",
            "Of course there are positive integers and they cannot be.",
            "Some of them cannot be smaller than 10.",
            "But I can also say that.",
            "There.",
            "Of this sum is smaller than the sum over all time steps.",
            "This is N / 1.",
            "This is the index that I increase every time I see an item.",
            "I increase the index and then I delete something.",
            "What do I delete?",
            "I delete at least L / 2 Delta T counts.",
            "These are the items greater than the media.",
            "OK, so if I just push the sum inside I get this is N -- L / 2 sum of Delta type the Delta T. So if I just rearrange the 1st and the last term in this role.",
            "I get the sum of Delta T is smaller than twice an over L. OK, and if I just set L to be two over epsilon, I get the last row.",
            "OK, so this is the entire proof.",
            "Nothing swept under the rug.",
            "And we've now.",
            "Completely understood, the measure agrees, or the frequent items algorithm.",
            "So let me just tell you what happens when you have a matrix now."
        ],
        [
            "So this is I doubt this algorithm frequent directions, just you know.",
            "So it sounds like frequent items, so we're going to keep a matrix of of the Rose.",
            "And the L columns and L is going to be always fixed and one of the at least one of the roses.",
            "At least one of the columns is always going to be 0, so that when."
        ],
        [
            "I get when I get."
        ],
        [
            "New column I can just."
        ],
        [
            "Store it in one of the zero columns.",
            "Of course this is.",
            "I don't lose anything by that.",
            "That's a pretty.",
            "All."
        ],
        [
            "Yes, but then my sketch becomes full and now I have to do this eviction process that we did before with the items.",
            "Now it's a little bit more complex, little bit more delicate than the frequent item setting, but not by much.",
            "So."
        ],
        [
            "I do the following thing.",
            "I take the matrix BI, compute its SVD and I write it as U times S10 V to V transpose and I just set the new sketch to be U * S. In some sense I discovered V transpose.",
            "OK, why am I allowed to do that becausw?",
            "The big."
        ],
        [
            "We transpose is actually exactly equal to be newbie new transpose.",
            "I mean, I just I just removed the rotation from the right, so VV transpose we transpose this?",
            "Or is the identity so I don't lose anything by that?",
            "OK, so now I have a mate."
        ],
        [
            "Fix that, I have a sketch.",
            "But now the columns of BR orthogonal and are in decreasing magnitude.",
            "So in some sense they kind of look like the counts."
        ],
        [
            "Before.",
            "So I take the median.",
            "I."
        ],
        [
            "The norm of the median column.",
            "And they just decrease the.",
            "I just decrease the column norms by at much by as much by Delta.",
            "OK, so everything smaller than the median I throw away everything larger than the median.",
            "I just multiply by a constant smaller than one such that the norms kind of match OK. And then I just keep going."
        ],
        [
            "OK.",
            "So if."
        ],
        [
            "So if this is not how you are used to seeing algorithms that you know, you can read this, this is the algorithm.",
            "But you know of course we're not going to go for it right now.",
            "Um?"
        ],
        [
            "Let me just show you the proof again.",
            "I'm not going to go over the math here, but it was important for me to put it here just to show you that this is the entire proof.",
            "OK, so let's first think about the cover.",
            "The distance in the difference in covariance so that.",
            "L2 distance of the covariance matrices.",
            "OK, this is a transpose minus B transpose.",
            "OK, this is exactly equivalent or analogous.",
            "Two, the difference encounters.",
            "So every two counters is how far can they be away from each other?",
            "And if we go over the math, we see that this is exactly some of Delta T. Like before.",
            "OK, think about the algorithm is taking before we took L different counters and deleted them.",
            "What this algorithm does is take L orthogonal directions in space and kind of shrinks them all in the same amount.",
            "OK, so every single direction can only suffer once from this thing because.",
            "The you know the pain is distributively distributed isomorphically.",
            "So this difference is at most some of Delta T. Also analogously to."
        ],
        [
            "Bonding some of Delta T the same way we did before.",
            "I'm just I can just say the Frobenius norm.",
            "Of the matrix.",
            "In the end of my sketches, larger than zero, this is obvious, and then when I just go through the derivation, I get that this is smaller than the Frobenius norm of a -- L / 2 sum of Delta T again completely analogous OK. And.",
            "And what we get is a bound on some of Delta T. If you remember before, it was twice times N / L here is twice times the Frobenius normal very square over L. And again, the idea is almost completely identical, so if we just combine the two things we had that the distance, the covariance, the distance in the covariance matrices at most some of Delta T."
        ],
        [
            "And some of that is smaller than twice.",
            "For business Norm of a square over L. So if we just set L to be.",
            "Two over epsilon.",
            "We get we get the theorem.",
            "OK, and again I wrote here that the proofs are, maybe unsurprisingly very similar.",
            "I want to say that it's not.",
            "Maybe they are unsurprisingly, very similar.",
            "It's in a sense.",
            "It's the same algorithm extended."
        ],
        [
            "So let me just say a couple of words on the behavior of this algorithm in practice.",
            "Um?",
            "So on the Y axis here we see.",
            "So this is.",
            "This is a synthetic matrix that I created with linearly decreasing singular values and then some Gaussian additive noise.",
            "And on the Y axis we see the the error, which is a transpose minus BB transpose.",
            "The two norm of that.",
            "And on the X axis is the number of columns that I pick in my in my.",
            "In my sketch now I'm comparing it here to a few different algorithms.",
            "First, the naive algorithm is the flat blue line.",
            "What it does is always return 0.",
            "Does it?",
            "Does it doesn't do any sketching, it just does nothing OK.",
            "The that the top three ones are sampling when you just sample columns according to the L2 norm, which is apparently the right way to do it.",
            "It's hashing and random projections, which are kind of so hashing is what is known as feature hashing usually.",
            "In the theater community, it's just pass shingles passed under projections, and this is an random projections is a dense random projection matrix.",
            "And they're all on this matrix, and maybe this is not indicative of every kind of data, But for this this data they somehow perform roughly the same.",
            "And surprisingly worse than doing nothing at all sometimes.",
            "OK, this is a side effect of undersampling and everybody that is done that before knows.",
            "What happens there?",
            "So the blue line the the light blue line below that?",
            "This is the up the theoretical worst case upper bound of frequent directions.",
            "And the yellow line is the behavior of frequent directions.",
            "And really light blue one all the way at the bottom is SVD.",
            "This is brute force.",
            "I mean we can head.",
            "We've been able to do this.",
            "I mean there we would choose that, but we of course cannot.",
            "OK so this is.",
            "Theoretically we can.",
            "We cannot do any better than that, and this returns this kind of picture returns in any setting of the number of columns, dimensions and so on.",
            "OK, so."
        ],
        [
            "Just say something about the running time.",
            "So.",
            "You might have noticed that this algorithm does this like repeat that SVD is in every step, and it's, um, sounds like it's going to be pretty cumbersome.",
            "It is indeed significantly slower than just sampling, right?",
            "If you just sample columns.",
            "Of course this is way easier.",
            "We have efficient reservoir samples.",
            "You can do all sorts of nifty tricks, and things become significantly.",
            "Faster, but I just want to make the case that even with my naive single threaded Python implementation on my laptop, things are not so bad.",
            "So this just shows that the on the right on the.",
            "Sorry, so here I fixed L the number of columns to be.",
            "100 and I vary the.",
            "In a very the dimensions and the so the number of rows and columns and if you see all the way I mean this point here.",
            "OK, this is 100,000 by 10,000 dense matrix.",
            "And to create a very efficient sketch of that matrix required 180 seconds is 3 minutes.",
            "It's not so bad.",
            "This is a four GB matrix, OK?",
            "Again, this is a very naive implementation.",
            "I'm sure that any one of you could do a much better job implementing that idea.",
            "That's it, I just want to.",
            "I want to thank you but I just want to give 2.",
            "Two comments.",
            "First of all, the sketches that we saw are completely combinable.",
            "So then you can sketch different parts of the matrix in different places and just take them and sketch the sketches and you get exactly the same guarantees, which is kind of nice when your matrix is distributed.",
            "Also there is.",
            "There is a result by gosh Amion Phillips that showed that this this sketching algorithm is surprisingly space optimal, which means that if you take instead of two over epsilon columns you take.",
            "K over epsilon columns you get a one over epsilon approximation to the best rank K approximation apparently.",
            "It matches the lower bound, the information lower bound that you have to.",
            "I mean, any streaming algorithm that will achieve this kind of bound will require at least this amount of space, so at least for space versus accuracy.",
            "We cannot improve anymore, but running time is still an open game and if you have ideas on how to improve this I will be more than happy to hear them or to read about them.",
            "That's it."
        ],
        [
            "You're very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to thank the committee again for choosing my.",
                    "label": 0
                },
                {
                    "sent": "Paper this is a great honor.",
                    "label": 0
                },
                {
                    "sent": "Thanks for everybody for coming here.",
                    "label": 0
                },
                {
                    "sent": "So without further ado, let me just jump right in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean, we all use huge amounts of data.",
                    "label": 0
                },
                {
                    "sent": "This is too long, right?",
                    "label": 0
                },
                {
                    "sent": "Is this better?",
                    "label": 0
                },
                {
                    "sent": "OK, can you hear me?",
                    "label": 0
                },
                {
                    "sent": "OK we will, I mean use a lot of data and this data is often represented by a large matrix.",
                    "label": 1
                },
                {
                    "sent": "What's happening?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Is this better?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so we all use pretty large amounts of data and this data is often representable by a large matrix.",
                    "label": 0
                },
                {
                    "sent": "This is true for images, audio, text, you name it.",
                    "label": 0
                },
                {
                    "sent": "I don't need to sell you the idea of data being a matrix.",
                    "label": 0
                },
                {
                    "sent": "Andy.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this matrix are often quite large?",
                    "label": 0
                },
                {
                    "sent": "I just plotted here some numbers of sizes of matrices that I come across in Yahoo.",
                    "label": 0
                },
                {
                    "sent": "And these are I would think about them is D dimensional vectors of objects.",
                    "label": 0
                },
                {
                    "sent": "This could be documents or images or what have you and we think of.",
                    "label": 1
                },
                {
                    "sent": "Usually this matrix is being did having the Rose and many many columns in every column is an instance of a documents and you can have a document and you can imagine, for example, that the dimensionality is fixed, but the number of documents grows.",
                    "label": 0
                },
                {
                    "sent": "Very fast and we cannot usually work on them on one machine and this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tuation is even worse when sometimes the matrix itself is comes as a stream.",
                    "label": 0
                },
                {
                    "sent": "So you see the element and you never see it again.",
                    "label": 0
                },
                {
                    "sent": "It's just too large to store at all.",
                    "label": 1
                },
                {
                    "sent": "This is true in a lot of the image analysis spam detection.",
                    "label": 0
                },
                {
                    "sent": "Web crawling and so on.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's try to think.",
                    "label": 0
                },
                {
                    "sent": "For example, what can we do with the streaming matrix?",
                    "label": 1
                },
                {
                    "sent": "So what can you do when you get the columns of a matrix one over after the other for a very long time?",
                    "label": 1
                },
                {
                    "sent": "And let's just try to compute the covariance of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So I just want to compute a time times A transpose.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A very simple idea, maybe I don't do anything fancy.",
                    "label": 0
                },
                {
                    "sent": "But this of course is possible from the stream.",
                    "label": 1
                },
                {
                    "sent": "So I can just take the color of the first column of the matrix, multiply it by itself, transpose, I get the covariance matrix of rank one, and I keep adding those up.",
                    "label": 0
                },
                {
                    "sent": "And in the end I get the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "So we can do that, but that already takes N times this square operation.",
                    "label": 0
                },
                {
                    "sent": "So for every column that I see I need to create a matrix of size D square.",
                    "label": 0
                },
                {
                    "sent": "So if you just if the dimension is a million which is not outrageously high, you already the covariance matrix itself is huge.",
                    "label": 0
                },
                {
                    "sent": "It's about a TB of space, and you need to do 10 to the 12 operations floating point operations for each update.",
                    "label": 0
                },
                {
                    "sent": "So this is out of the question.",
                    "label": 0
                },
                {
                    "sent": "So what I'm trying to show in this slide, even the most basic idea of the basic construct of just computing the covariance matrix, is already impossible in the streaming.",
                    "label": 0
                },
                {
                    "sent": "In the streaming setting in large data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea of sketching or approximating a matrix is not invented by me.",
                    "label": 0
                },
                {
                    "sent": "The idea is to efficiently compute a concise representable matrix B, such as either be is roughly like A or BB.",
                    "label": 1
                },
                {
                    "sent": "Transpose is roughly like a transpose.",
                    "label": 0
                },
                {
                    "sent": "OK, and then if you if you do this efficiently enough and if the approximation is good enough, then you can work with the approximation of the original matrix.",
                    "label": 0
                },
                {
                    "sent": "And it turns out with I mean there are many 10s of papers on these topics that given the right sketch, you can actually do dimension reduction, signal denoising, classification, regression, clustering, matrix multiplication, reconstruction recommendation, basically everything that we're used to doing on the matrix we can do on the approximation, given that we can obtain it, and then it's good enough.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just giving here a list of results I'm not going to go over them, of course.",
                    "label": 0
                },
                {
                    "sent": "But these are just the matrix approximation results.",
                    "label": 0
                },
                {
                    "sent": "How do you sketch matrices?",
                    "label": 0
                },
                {
                    "sent": "The first table?",
                    "label": 0
                },
                {
                    "sent": "This was the aggregated by gas money and Shamian Phillips and minimize here monogamy I think is in the audience somewhere.",
                    "label": 0
                },
                {
                    "sent": "Who actually have a follow up work to my work?",
                    "label": 0
                },
                {
                    "sent": "Aggregated these results and then so these are column subset selection algorithms.",
                    "label": 0
                },
                {
                    "sent": "They go over the matrix and just pick some of the columns and throw away the rest.",
                    "label": 0
                },
                {
                    "sent": "And somehow, if you pick the columns correctly, you can show that the matrix that you get in the end is a good approximation of the original 1.",
                    "label": 0
                },
                {
                    "sent": "There are specification results that just just nullify some of the entries and keep a sparse version of the Matrix.",
                    "label": 0
                },
                {
                    "sent": "Again, there is a list of results.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go over.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If there are ideas that come from subspace embedding techniques, usually they are linear embeddings.",
                    "label": 0
                },
                {
                    "sent": "Again, a long list of papers.",
                    "label": 0
                },
                {
                    "sent": "Actually the best paper in stock this year was given for exactly a matrix approximation result by Clarkson Woodruff.",
                    "label": 0
                },
                {
                    "sent": "In the last.",
                    "label": 0
                },
                {
                    "sent": "The last section is deterministic sketching algorithms.",
                    "label": 0
                },
                {
                    "sent": "There are only three this far.",
                    "label": 0
                },
                {
                    "sent": "The result by Feldman ET al.",
                    "label": 0
                },
                {
                    "sent": "My result and then the result of the chamois and Philips that follow up on my work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's the goal?",
                    "label": 0
                },
                {
                    "sent": "Our goal is to do the following.",
                    "label": 0
                },
                {
                    "sent": "We want to efficiently keep a matrix be with only a few columns, let's say twice, two over epsilon such that the covariance matrix of A&B are essentially the same and what they say the same up to some epsilon times of for business.",
                    "label": 0
                },
                {
                    "sent": "Normal, Ferrero, and I can always tweak epsilon to be whatever it is that I want.",
                    "label": 0
                },
                {
                    "sent": "Notice that I can.",
                    "label": 0
                },
                {
                    "sent": "I can decide in advance what I want the precision to be in just running, so in some sense we don't need to run PCA or SVD and then truncate the values we can.",
                    "label": 0
                },
                {
                    "sent": "In advance, decide what's the cut off value is and just compute that OK and the idea is to extend the Frick.",
                    "label": 0
                },
                {
                    "sent": "The frequent items algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a very famous algorithm.",
                    "label": 0
                },
                {
                    "sent": "I half jokingly say here there was, and it was so good that it was invented four times, you know, but I'm half joking.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "It really is fantastic and it really was invented four times so.",
                    "label": 0
                },
                {
                    "sent": "Maybe more, it was published four times.",
                    "label": 0
                },
                {
                    "sent": "So here is the simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "How am I on time, by the way?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "OK, so the basic idea is very simple, so let's assume I get a stream of blocks in different colors.",
                    "label": 1
                },
                {
                    "sent": "Let's say I have six possible different colors.",
                    "label": 1
                },
                {
                    "sent": "This would be D and by the end of the stream I'm going to ask you how many times have you seen the purple block.",
                    "label": 0
                },
                {
                    "sent": "This is the frequency of purple, and the answer here is, say, five.",
                    "label": 1
                },
                {
                    "sent": "OK, this is of course.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Easily done in all of this space.",
                    "label": 0
                },
                {
                    "sent": "I just keep one counter for every element, but of course this is sometimes impossible.",
                    "label": 0
                },
                {
                    "sent": "Think about the router sitting somewhere and seeing different IPS all day long and then one day I come to me, come to it and ask.",
                    "label": 0
                },
                {
                    "sent": "You know how many times have you seen 19610721 or a front end device that keeps a search engine front end machine they wants to keep to keep tabs on how many times it's in every query.",
                    "label": 0
                },
                {
                    "sent": "Of course it cannot keep the entire history.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the mission agrees algorithm.",
                    "label": 0
                },
                {
                    "sent": "Over the frequent items algorithm.",
                    "label": 1
                },
                {
                    "sent": "Keeps only a fixed number of encounters, one of them, at least one of them is always set to 0.",
                    "label": 1
                },
                {
                    "sent": "OK, and I just keep at most L different elements.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I get an element that I have accounted for, increase the counter by one when.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I get an element that I don't have a counter for.",
                    "label": 0
                },
                {
                    "sent": "I create a new counter for it and put it set it to one.",
                    "label": 1
                },
                {
                    "sent": "But then I I ran out of space OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I need to make some more space right?",
                    "label": 0
                },
                {
                    "sent": "I somehow need to always have at least one counter at 0, so I look.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the median counter.",
                    "label": 0
                },
                {
                    "sent": "Let's say let's call it Delta and I just decrease all the count.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Service by Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, if the counter was less than Delta, I set it to zero higher than Delta.",
                    "label": 1
                },
                {
                    "sent": "I decrease it by Delta and that's it.",
                    "label": 0
                },
                {
                    "sent": "I continue.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is the algorithm and by the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "End of the algorithm, they all frequencies.",
                    "label": 0
                },
                {
                    "sent": "Your approximate frequencies are.",
                    "label": 0
                },
                {
                    "sent": "Are given by F prime here and these approximate frequencies have.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following property should look at the last line for a second.",
                    "label": 0
                },
                {
                    "sent": "Have this property that.",
                    "label": 0
                },
                {
                    "sent": "The real frequency and approximate frequencies are at most of epsilon N apart.",
                    "label": 1
                },
                {
                    "sent": "OK. Now I want to go over the proof for a second, not to torment you, but just to.",
                    "label": 0
                },
                {
                    "sent": "First, show you that this is the entire proof and also the the proof of the metric.",
                    "label": 0
                },
                {
                    "sent": "Sketching result is basically identical, so this will allow me to skip to handwave.",
                    "label": 0
                },
                {
                    "sent": "Much more aggressively later.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we know that the approximate frequency F prime is lower than F in the real frequency because we count everything once and somewhat sometimes we delete things so it cannot be higher.",
                    "label": 0
                },
                {
                    "sent": "Also, the approximate frequency F prime is larger than the real frequency minus the sum of all Delta T, so Delta T is the how much I delete it in time step T. OK, at the worst case my item participated in each one of those deletions and it was decreased in every time step.",
                    "label": 0
                },
                {
                    "sent": "So you have some of Delta T, so the error is at most some of Delta T. But I need to somehow bound how much some of Delta T can be so that I can do also quite easily.",
                    "label": 0
                },
                {
                    "sent": "I know that zero is smaller than the sum of my approximate frequencies.",
                    "label": 0
                },
                {
                    "sent": "Of course there are positive integers and they cannot be.",
                    "label": 0
                },
                {
                    "sent": "Some of them cannot be smaller than 10.",
                    "label": 0
                },
                {
                    "sent": "But I can also say that.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Of this sum is smaller than the sum over all time steps.",
                    "label": 0
                },
                {
                    "sent": "This is N / 1.",
                    "label": 0
                },
                {
                    "sent": "This is the index that I increase every time I see an item.",
                    "label": 0
                },
                {
                    "sent": "I increase the index and then I delete something.",
                    "label": 1
                },
                {
                    "sent": "What do I delete?",
                    "label": 0
                },
                {
                    "sent": "I delete at least L / 2 Delta T counts.",
                    "label": 0
                },
                {
                    "sent": "These are the items greater than the media.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I just push the sum inside I get this is N -- L / 2 sum of Delta type the Delta T. So if I just rearrange the 1st and the last term in this role.",
                    "label": 0
                },
                {
                    "sent": "I get the sum of Delta T is smaller than twice an over L. OK, and if I just set L to be two over epsilon, I get the last row.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the entire proof.",
                    "label": 0
                },
                {
                    "sent": "Nothing swept under the rug.",
                    "label": 0
                },
                {
                    "sent": "And we've now.",
                    "label": 1
                },
                {
                    "sent": "Completely understood, the measure agrees, or the frequent items algorithm.",
                    "label": 0
                },
                {
                    "sent": "So let me just tell you what happens when you have a matrix now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is I doubt this algorithm frequent directions, just you know.",
                    "label": 1
                },
                {
                    "sent": "So it sounds like frequent items, so we're going to keep a matrix of of the Rose.",
                    "label": 0
                },
                {
                    "sent": "And the L columns and L is going to be always fixed and one of the at least one of the roses.",
                    "label": 0
                },
                {
                    "sent": "At least one of the columns is always going to be 0, so that when.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I get when I get.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New column I can just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Store it in one of the zero columns.",
                    "label": 0
                },
                {
                    "sent": "Of course this is.",
                    "label": 0
                },
                {
                    "sent": "I don't lose anything by that.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, but then my sketch becomes full and now I have to do this eviction process that we did before with the items.",
                    "label": 0
                },
                {
                    "sent": "Now it's a little bit more complex, little bit more delicate than the frequent item setting, but not by much.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I do the following thing.",
                    "label": 0
                },
                {
                    "sent": "I take the matrix BI, compute its SVD and I write it as U times S10 V to V transpose and I just set the new sketch to be U * S. In some sense I discovered V transpose.",
                    "label": 0
                },
                {
                    "sent": "OK, why am I allowed to do that becausw?",
                    "label": 0
                },
                {
                    "sent": "The big.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We transpose is actually exactly equal to be newbie new transpose.",
                    "label": 0
                },
                {
                    "sent": "I mean, I just I just removed the rotation from the right, so VV transpose we transpose this?",
                    "label": 0
                },
                {
                    "sent": "Or is the identity so I don't lose anything by that?",
                    "label": 1
                },
                {
                    "sent": "OK, so now I have a mate.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fix that, I have a sketch.",
                    "label": 0
                },
                {
                    "sent": "But now the columns of BR orthogonal and are in decreasing magnitude.",
                    "label": 1
                },
                {
                    "sent": "So in some sense they kind of look like the counts.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before.",
                    "label": 0
                },
                {
                    "sent": "So I take the median.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The norm of the median column.",
                    "label": 0
                },
                {
                    "sent": "And they just decrease the.",
                    "label": 0
                },
                {
                    "sent": "I just decrease the column norms by at much by as much by Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything smaller than the median I throw away everything larger than the median.",
                    "label": 0
                },
                {
                    "sent": "I just multiply by a constant smaller than one such that the norms kind of match OK. And then I just keep going.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if this is not how you are used to seeing algorithms that you know, you can read this, this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "But you know of course we're not going to go for it right now.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just show you the proof again.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go over the math here, but it was important for me to put it here just to show you that this is the entire proof.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's first think about the cover.",
                    "label": 0
                },
                {
                    "sent": "The distance in the difference in covariance so that.",
                    "label": 0
                },
                {
                    "sent": "L2 distance of the covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a transpose minus B transpose.",
                    "label": 0
                },
                {
                    "sent": "OK, this is exactly equivalent or analogous.",
                    "label": 0
                },
                {
                    "sent": "Two, the difference encounters.",
                    "label": 0
                },
                {
                    "sent": "So every two counters is how far can they be away from each other?",
                    "label": 0
                },
                {
                    "sent": "And if we go over the math, we see that this is exactly some of Delta T. Like before.",
                    "label": 0
                },
                {
                    "sent": "OK, think about the algorithm is taking before we took L different counters and deleted them.",
                    "label": 0
                },
                {
                    "sent": "What this algorithm does is take L orthogonal directions in space and kind of shrinks them all in the same amount.",
                    "label": 0
                },
                {
                    "sent": "OK, so every single direction can only suffer once from this thing because.",
                    "label": 0
                },
                {
                    "sent": "The you know the pain is distributively distributed isomorphically.",
                    "label": 0
                },
                {
                    "sent": "So this difference is at most some of Delta T. Also analogously to.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bonding some of Delta T the same way we did before.",
                    "label": 0
                },
                {
                    "sent": "I'm just I can just say the Frobenius norm.",
                    "label": 1
                },
                {
                    "sent": "Of the matrix.",
                    "label": 0
                },
                {
                    "sent": "In the end of my sketches, larger than zero, this is obvious, and then when I just go through the derivation, I get that this is smaller than the Frobenius norm of a -- L / 2 sum of Delta T again completely analogous OK. And.",
                    "label": 0
                },
                {
                    "sent": "And what we get is a bound on some of Delta T. If you remember before, it was twice times N / L here is twice times the Frobenius normal very square over L. And again, the idea is almost completely identical, so if we just combine the two things we had that the distance, the covariance, the distance in the covariance matrices at most some of Delta T.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And some of that is smaller than twice.",
                    "label": 0
                },
                {
                    "sent": "For business Norm of a square over L. So if we just set L to be.",
                    "label": 0
                },
                {
                    "sent": "Two over epsilon.",
                    "label": 0
                },
                {
                    "sent": "We get we get the theorem.",
                    "label": 0
                },
                {
                    "sent": "OK, and again I wrote here that the proofs are, maybe unsurprisingly very similar.",
                    "label": 1
                },
                {
                    "sent": "I want to say that it's not.",
                    "label": 0
                },
                {
                    "sent": "Maybe they are unsurprisingly, very similar.",
                    "label": 0
                },
                {
                    "sent": "It's in a sense.",
                    "label": 0
                },
                {
                    "sent": "It's the same algorithm extended.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just say a couple of words on the behavior of this algorithm in practice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So on the Y axis here we see.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is a synthetic matrix that I created with linearly decreasing singular values and then some Gaussian additive noise.",
                    "label": 1
                },
                {
                    "sent": "And on the Y axis we see the the error, which is a transpose minus BB transpose.",
                    "label": 0
                },
                {
                    "sent": "The two norm of that.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis is the number of columns that I pick in my in my.",
                    "label": 0
                },
                {
                    "sent": "In my sketch now I'm comparing it here to a few different algorithms.",
                    "label": 0
                },
                {
                    "sent": "First, the naive algorithm is the flat blue line.",
                    "label": 0
                },
                {
                    "sent": "What it does is always return 0.",
                    "label": 0
                },
                {
                    "sent": "Does it?",
                    "label": 0
                },
                {
                    "sent": "Does it doesn't do any sketching, it just does nothing OK.",
                    "label": 0
                },
                {
                    "sent": "The that the top three ones are sampling when you just sample columns according to the L2 norm, which is apparently the right way to do it.",
                    "label": 0
                },
                {
                    "sent": "It's hashing and random projections, which are kind of so hashing is what is known as feature hashing usually.",
                    "label": 0
                },
                {
                    "sent": "In the theater community, it's just pass shingles passed under projections, and this is an random projections is a dense random projection matrix.",
                    "label": 0
                },
                {
                    "sent": "And they're all on this matrix, and maybe this is not indicative of every kind of data, But for this this data they somehow perform roughly the same.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly worse than doing nothing at all sometimes.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a side effect of undersampling and everybody that is done that before knows.",
                    "label": 0
                },
                {
                    "sent": "What happens there?",
                    "label": 0
                },
                {
                    "sent": "So the blue line the the light blue line below that?",
                    "label": 0
                },
                {
                    "sent": "This is the up the theoretical worst case upper bound of frequent directions.",
                    "label": 0
                },
                {
                    "sent": "And the yellow line is the behavior of frequent directions.",
                    "label": 0
                },
                {
                    "sent": "And really light blue one all the way at the bottom is SVD.",
                    "label": 0
                },
                {
                    "sent": "This is brute force.",
                    "label": 0
                },
                {
                    "sent": "I mean we can head.",
                    "label": 0
                },
                {
                    "sent": "We've been able to do this.",
                    "label": 0
                },
                {
                    "sent": "I mean there we would choose that, but we of course cannot.",
                    "label": 0
                },
                {
                    "sent": "OK so this is.",
                    "label": 0
                },
                {
                    "sent": "Theoretically we can.",
                    "label": 1
                },
                {
                    "sent": "We cannot do any better than that, and this returns this kind of picture returns in any setting of the number of columns, dimensions and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just say something about the running time.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You might have noticed that this algorithm does this like repeat that SVD is in every step, and it's, um, sounds like it's going to be pretty cumbersome.",
                    "label": 0
                },
                {
                    "sent": "It is indeed significantly slower than just sampling, right?",
                    "label": 0
                },
                {
                    "sent": "If you just sample columns.",
                    "label": 0
                },
                {
                    "sent": "Of course this is way easier.",
                    "label": 0
                },
                {
                    "sent": "We have efficient reservoir samples.",
                    "label": 0
                },
                {
                    "sent": "You can do all sorts of nifty tricks, and things become significantly.",
                    "label": 0
                },
                {
                    "sent": "Faster, but I just want to make the case that even with my naive single threaded Python implementation on my laptop, things are not so bad.",
                    "label": 0
                },
                {
                    "sent": "So this just shows that the on the right on the.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so here I fixed L the number of columns to be.",
                    "label": 0
                },
                {
                    "sent": "100 and I vary the.",
                    "label": 1
                },
                {
                    "sent": "In a very the dimensions and the so the number of rows and columns and if you see all the way I mean this point here.",
                    "label": 0
                },
                {
                    "sent": "OK, this is 100,000 by 10,000 dense matrix.",
                    "label": 0
                },
                {
                    "sent": "And to create a very efficient sketch of that matrix required 180 seconds is 3 minutes.",
                    "label": 0
                },
                {
                    "sent": "It's not so bad.",
                    "label": 0
                },
                {
                    "sent": "This is a four GB matrix, OK?",
                    "label": 0
                },
                {
                    "sent": "Again, this is a very naive implementation.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that any one of you could do a much better job implementing that idea.",
                    "label": 0
                },
                {
                    "sent": "That's it, I just want to.",
                    "label": 0
                },
                {
                    "sent": "I want to thank you but I just want to give 2.",
                    "label": 0
                },
                {
                    "sent": "Two comments.",
                    "label": 0
                },
                {
                    "sent": "First of all, the sketches that we saw are completely combinable.",
                    "label": 0
                },
                {
                    "sent": "So then you can sketch different parts of the matrix in different places and just take them and sketch the sketches and you get exactly the same guarantees, which is kind of nice when your matrix is distributed.",
                    "label": 0
                },
                {
                    "sent": "Also there is.",
                    "label": 0
                },
                {
                    "sent": "There is a result by gosh Amion Phillips that showed that this this sketching algorithm is surprisingly space optimal, which means that if you take instead of two over epsilon columns you take.",
                    "label": 0
                },
                {
                    "sent": "K over epsilon columns you get a one over epsilon approximation to the best rank K approximation apparently.",
                    "label": 0
                },
                {
                    "sent": "It matches the lower bound, the information lower bound that you have to.",
                    "label": 0
                },
                {
                    "sent": "I mean, any streaming algorithm that will achieve this kind of bound will require at least this amount of space, so at least for space versus accuracy.",
                    "label": 0
                },
                {
                    "sent": "We cannot improve anymore, but running time is still an open game and if you have ideas on how to improve this I will be more than happy to hear them or to read about them.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're very much.",
                    "label": 0
                }
            ]
        }
    }
}