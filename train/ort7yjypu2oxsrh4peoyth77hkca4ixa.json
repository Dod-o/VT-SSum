{
    "id": "ort7yjypu2oxsrh4peoyth77hkca4ixa",
    "title": "Learning to Rank Query Graphs for Complex Question Answering over Knowledge Graphs",
    "info": {
        "author": [
            "Denis Lukovnikov, University of Bonn"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_lukovnikov_rank_query_graphs/",
    "segmentation": [
        [
            "Shortforms here the same entities and relations can be expressed in many different ways and for relations.",
            "They can even use multiple words and synonyms.",
            "So we need to make systems that can handle all these ways of expressing questions in order to understand.",
            "In order to make a.",
            "Working question answering system."
        ],
        [
            "So.",
            "Question answering knowledge cross can be solved by translating converting natural language question into a formal query.",
            "In our case, sparkle queries and it's essentially a translation problem.",
            "To form a language.",
            "And in our approach we four will follow the."
        ],
        [
            "General approach would be create a set of query candidates.",
            "And I'm going to ring them.",
            "With respect to the question.",
            "So."
        ],
        [
            "With Nick, for example, if you have a question, give me."
        ],
        [
            "List of everything we've ever done.",
            "Junior has a kitten.",
            "We"
        ],
        [
            "First have to.",
            "Find the entities and our work we rely on.",
            "That is being available being given already, so we don't focus on entity linking.",
            "Then we."
        ],
        [
            "Look at the two hope subgraph in the knowledge graph surrounding the linked entities.",
            "And we generate."
        ],
        [
            "All the possible.",
            "Basic sparkle queries.",
            "Around we're connecting the linked entities."
        ],
        [
            "Finally, we rank the candidates queries and take the high scoring one as the predicted query.",
            "So."
        ],
        [
            "One question is how to handle Sparkle queries for in this framework of ranking."
        ],
        [
            "So.",
            "We would like to Transformers pocket queries into representation that can be easily encoded with a more simple neural network and also to so we want a more compact representation of the query that retains most of the essence and reduces the boiler boilerplates tokens.",
            "So if you start from this sparkle query with two conditions, we first remove the like we take the triple patterns from the query.",
            "The distributors still contains the variable nodes for the answer answer node variable."
        ],
        [
            "But it describes the basic.",
            "Describe then you can hold it and it should be connected for the answer."
        ],
        [
            "And then we further simplify this by creating one single sequence that removes the intermediate variables."
        ],
        [
            "So we have this, and given that our entities already linked, it will be the same for all the possible queries you."
        ],
        [
            "Drop the entities so in the end we end up with this relational chain that only specifies.",
            "It only contains information about which relations should be followed from the entities.",
            "So and we will be ranking structures like this and we will be."
        ],
        [
            "Make them in a very simple ranking frame."
        ],
        [
            "Where we encode the questions into a vector and we encode the relational chains into a vector.",
            "And then we will be training our model to have smaller distance for.",
            "Smaller distance between vectors for questions and.",
            "Their correct relational chains.",
            "So we will need some kind of question encoder that takes a question."
        ],
        [
            "Like a word sequence and produces a encoded question.",
            "And we will need some."
        ],
        [
            "The query encoder or relational change encoder that takes relation to chain and produces a vector for the relational chain and then we can compute computer score.",
            "And we can use the score for prediction later in the ranking.",
            "During training we use negative sampling."
        ],
        [
            "So we randomly select some.",
            "In Greggs relational chain and also encoded using the same query encoders we used for the correct one and produce another vector, and we compute another score with question.",
            "So now we have two scores."
        ],
        [
            "In training we.",
            "Try to maximize the difference, so we try to maximize the score of the correct pair and minimize scores, incorrect pair and we do this by maximizing the margin.",
            "The difference between the two scores up to some margin, which is usually one."
        ],
        [
            "Here that's for training.",
            "During prediction we can simply use the two encoders, get their vectors of the question in the relational chain, and get the scores, and do this for all the possible candidates.",
            "For the relational chain and we rank them based on the scores and get the high scoring one as the prediction of the.",
            "Relational chain"
        ],
        [
            "So.",
            "This is the approach we follow just to find the only triple patterns and we also add some additional.",
            "Predictors for detecting the question type.",
            "So if you want to count things instead of just returning a set of things.",
            "We have an additional classifier that distinguishes between select between factorial questions, speculation questions, and counting questions.",
            "And we also do additional constraints using a separate model.",
            "Because sometimes you want to filter things based on type like.",
            "If you want to have a series where some actor acted in, if you also acted in movies.",
            "So this can."
        ],
        [
            "Trains are handled by an additional classifier ranking model.",
            "So."
        ],
        [
            "In the ranking framework we make use of encoders and we can implement encoders using some simple networks like biology and CNN or some more complicated ones like the decomposable attention model or Transformers.",
            "So in our experiments we worked."
        ],
        [
            "Buy list items so we can have a very simple.",
            "Model like this with single by lithium encoder.",
            "For the.",
            "Traditional chain and single, not another ballast encoder for the question."
        ],
        [
            "However, using this.",
            "Single encoders, we are forcing the most.",
            "Encoders produce a single representation for the whole question.",
            "Using the applications for this team.",
            "And also we encode the whole.",
            "Relational chain into a single vector.",
            "Which might introduce like it may be a small bottleneck, so we."
        ],
        [
            "Experiment, we propose a small extension to the basic Colosseum.",
            "Encoding by creating basically two representations of questions.",
            "Of a question.",
            "That internally tried to focus try to find the relation of words and focus more on them.",
            "Instead of trying to remember the.",
            "Relation worth information.",
            "Through the list of updates.",
            "So we.",
            "Introduce."
        ],
        [
            "So."
        ],
        [
            "What we want to create is to encodings of the question for the two, for the one or two relations that we will have.",
            "So and also another side will have we want two encodings of the relational chain."
        ],
        [
            "Each of which focuses on one of the relations.",
            "So we accomplished this by stacking two attentions on top of the buylist human coding and on the question sites and on the relationship."
        ],
        [
            "We don't really need to do much, we can just encode the relations separately.",
            "So the layers."
        ],
        [
            "Mention that we stack on top of the by Liceum looks like this.",
            "So we first have the buy list encoding of the question which is a sequence of vectors and then we add two parameter vectors to the model which are used to compute.",
            "Like an attention over the encodings and then we can use this attention weights to create a summary summary of the encoding.",
            "That hopefully focuses on the relation of words in the question.",
            "Yeah."
        ],
        [
            "So in our experiments we were."
        ],
        [
            "SLC quote and QD 7 so they're both on the data sets were there both data sets on the same DVD release and also quoted."
        ],
        [
            "Much more examples than Q. D7.",
            "So in our experiments, we first."
        ],
        [
            "Cats.",
            "How the models?",
            "How different models, including our proposed extension perform on?",
            "Just data sets separately and we compare with some baselines like like a simple naive by listing baseline decomposable attention model.",
            "Multi channel convolutional neural network, which assassin based model and the hierarchical residual model which was also used for ranking relations."
        ],
        [
            "And this these are results for.",
            "Just.",
            "Single data set performance.",
            "So we train or no sequence and report results in a sequence.",
            "Training quality and reports for quality.",
            "And to report the Cortina curacy, which is accuracy, which is how many times and how many cases, the correct question was ranked highest by the ranking model and we can see that, like the biosimilar works pretty well, but we get some improvement from using the slot mechanism."
        ],
        [
            "If you look at the attentions learned during the model, we can actually see that given the question.",
            "They focus on two different sets of words, which are more characterizing of one of the relations here.",
            "So mission for the mission relation."
        ],
        [
            "We also conducted another set of experiments for transfer learning."
        ],
        [
            "Which was thought was interesting.",
            "So one experiment was.",
            "Data transfer experiment where we first pre train Aussie quotes and then see how well fine tuning on QLD works.",
            "And the second set is going from a pre trained language model just straight to the QD&C code.",
            "So for that."
        ],
        [
            "The transfer we pretend to see quotes and finding on Cody and doing fine tuning.",
            "We experiment with different learning rates, learning rate schedules.",
            "And we find that."
        ],
        [
            "Schedule has a significant effect on the final performance and you also find that compared to."
        ],
        [
            "Pre training the results on Cody or much higher.",
            "If you first create a Nazi quote.",
            "And."
        ],
        [
            "We also do some experiments with pre trained language model.",
            "In our case bird.",
            "Where we?",
            "Which we put in architecture just simply by replacing the two different by listing encoders with."
        ],
        [
            "A single feature in transformer here and we still apply or slots.",
            "Like our attentions on top of it."
        ],
        [
            "And we see that no secret it's giving you even better performance between Cody.",
            "It's kind of failing, which could be explained by the fact that he is very small and versus a lot of parameters.",
            "Yeah so."
        ],
        [
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shortforms here the same entities and relations can be expressed in many different ways and for relations.",
                    "label": 0
                },
                {
                    "sent": "They can even use multiple words and synonyms.",
                    "label": 0
                },
                {
                    "sent": "So we need to make systems that can handle all these ways of expressing questions in order to understand.",
                    "label": 0
                },
                {
                    "sent": "In order to make a.",
                    "label": 0
                },
                {
                    "sent": "Working question answering system.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Question answering knowledge cross can be solved by translating converting natural language question into a formal query.",
                    "label": 0
                },
                {
                    "sent": "In our case, sparkle queries and it's essentially a translation problem.",
                    "label": 0
                },
                {
                    "sent": "To form a language.",
                    "label": 0
                },
                {
                    "sent": "And in our approach we four will follow the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "General approach would be create a set of query candidates.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to ring them.",
                    "label": 0
                },
                {
                    "sent": "With respect to the question.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With Nick, for example, if you have a question, give me.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "List of everything we've ever done.",
                    "label": 0
                },
                {
                    "sent": "Junior has a kitten.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First have to.",
                    "label": 0
                },
                {
                    "sent": "Find the entities and our work we rely on.",
                    "label": 0
                },
                {
                    "sent": "That is being available being given already, so we don't focus on entity linking.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the two hope subgraph in the knowledge graph surrounding the linked entities.",
                    "label": 0
                },
                {
                    "sent": "And we generate.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the possible.",
                    "label": 0
                },
                {
                    "sent": "Basic sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "Around we're connecting the linked entities.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we rank the candidates queries and take the high scoring one as the predicted query.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One question is how to handle Sparkle queries for in this framework of ranking.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We would like to Transformers pocket queries into representation that can be easily encoded with a more simple neural network and also to so we want a more compact representation of the query that retains most of the essence and reduces the boiler boilerplates tokens.",
                    "label": 0
                },
                {
                    "sent": "So if you start from this sparkle query with two conditions, we first remove the like we take the triple patterns from the query.",
                    "label": 0
                },
                {
                    "sent": "The distributors still contains the variable nodes for the answer answer node variable.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it describes the basic.",
                    "label": 0
                },
                {
                    "sent": "Describe then you can hold it and it should be connected for the answer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we further simplify this by creating one single sequence that removes the intermediate variables.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have this, and given that our entities already linked, it will be the same for all the possible queries you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drop the entities so in the end we end up with this relational chain that only specifies.",
                    "label": 0
                },
                {
                    "sent": "It only contains information about which relations should be followed from the entities.",
                    "label": 0
                },
                {
                    "sent": "So and we will be ranking structures like this and we will be.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make them in a very simple ranking frame.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we encode the questions into a vector and we encode the relational chains into a vector.",
                    "label": 0
                },
                {
                    "sent": "And then we will be training our model to have smaller distance for.",
                    "label": 0
                },
                {
                    "sent": "Smaller distance between vectors for questions and.",
                    "label": 0
                },
                {
                    "sent": "Their correct relational chains.",
                    "label": 0
                },
                {
                    "sent": "So we will need some kind of question encoder that takes a question.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like a word sequence and produces a encoded question.",
                    "label": 0
                },
                {
                    "sent": "And we will need some.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The query encoder or relational change encoder that takes relation to chain and produces a vector for the relational chain and then we can compute computer score.",
                    "label": 0
                },
                {
                    "sent": "And we can use the score for prediction later in the ranking.",
                    "label": 0
                },
                {
                    "sent": "During training we use negative sampling.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we randomly select some.",
                    "label": 0
                },
                {
                    "sent": "In Greggs relational chain and also encoded using the same query encoders we used for the correct one and produce another vector, and we compute another score with question.",
                    "label": 0
                },
                {
                    "sent": "So now we have two scores.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In training we.",
                    "label": 0
                },
                {
                    "sent": "Try to maximize the difference, so we try to maximize the score of the correct pair and minimize scores, incorrect pair and we do this by maximizing the margin.",
                    "label": 0
                },
                {
                    "sent": "The difference between the two scores up to some margin, which is usually one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here that's for training.",
                    "label": 0
                },
                {
                    "sent": "During prediction we can simply use the two encoders, get their vectors of the question in the relational chain, and get the scores, and do this for all the possible candidates.",
                    "label": 0
                },
                {
                    "sent": "For the relational chain and we rank them based on the scores and get the high scoring one as the prediction of the.",
                    "label": 0
                },
                {
                    "sent": "Relational chain",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the approach we follow just to find the only triple patterns and we also add some additional.",
                    "label": 0
                },
                {
                    "sent": "Predictors for detecting the question type.",
                    "label": 0
                },
                {
                    "sent": "So if you want to count things instead of just returning a set of things.",
                    "label": 0
                },
                {
                    "sent": "We have an additional classifier that distinguishes between select between factorial questions, speculation questions, and counting questions.",
                    "label": 0
                },
                {
                    "sent": "And we also do additional constraints using a separate model.",
                    "label": 0
                },
                {
                    "sent": "Because sometimes you want to filter things based on type like.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a series where some actor acted in, if you also acted in movies.",
                    "label": 0
                },
                {
                    "sent": "So this can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trains are handled by an additional classifier ranking model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the ranking framework we make use of encoders and we can implement encoders using some simple networks like biology and CNN or some more complicated ones like the decomposable attention model or Transformers.",
                    "label": 0
                },
                {
                    "sent": "So in our experiments we worked.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buy list items so we can have a very simple.",
                    "label": 0
                },
                {
                    "sent": "Model like this with single by lithium encoder.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                },
                {
                    "sent": "Traditional chain and single, not another ballast encoder for the question.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, using this.",
                    "label": 0
                },
                {
                    "sent": "Single encoders, we are forcing the most.",
                    "label": 0
                },
                {
                    "sent": "Encoders produce a single representation for the whole question.",
                    "label": 0
                },
                {
                    "sent": "Using the applications for this team.",
                    "label": 0
                },
                {
                    "sent": "And also we encode the whole.",
                    "label": 0
                },
                {
                    "sent": "Relational chain into a single vector.",
                    "label": 0
                },
                {
                    "sent": "Which might introduce like it may be a small bottleneck, so we.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiment, we propose a small extension to the basic Colosseum.",
                    "label": 0
                },
                {
                    "sent": "Encoding by creating basically two representations of questions.",
                    "label": 0
                },
                {
                    "sent": "Of a question.",
                    "label": 0
                },
                {
                    "sent": "That internally tried to focus try to find the relation of words and focus more on them.",
                    "label": 0
                },
                {
                    "sent": "Instead of trying to remember the.",
                    "label": 0
                },
                {
                    "sent": "Relation worth information.",
                    "label": 0
                },
                {
                    "sent": "Through the list of updates.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Introduce.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want to create is to encodings of the question for the two, for the one or two relations that we will have.",
                    "label": 0
                },
                {
                    "sent": "So and also another side will have we want two encodings of the relational chain.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each of which focuses on one of the relations.",
                    "label": 0
                },
                {
                    "sent": "So we accomplished this by stacking two attentions on top of the buylist human coding and on the question sites and on the relationship.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't really need to do much, we can just encode the relations separately.",
                    "label": 0
                },
                {
                    "sent": "So the layers.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mention that we stack on top of the by Liceum looks like this.",
                    "label": 0
                },
                {
                    "sent": "So we first have the buy list encoding of the question which is a sequence of vectors and then we add two parameter vectors to the model which are used to compute.",
                    "label": 0
                },
                {
                    "sent": "Like an attention over the encodings and then we can use this attention weights to create a summary summary of the encoding.",
                    "label": 0
                },
                {
                    "sent": "That hopefully focuses on the relation of words in the question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our experiments we were.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SLC quote and QD 7 so they're both on the data sets were there both data sets on the same DVD release and also quoted.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much more examples than Q. D7.",
                    "label": 0
                },
                {
                    "sent": "So in our experiments, we first.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cats.",
                    "label": 0
                },
                {
                    "sent": "How the models?",
                    "label": 0
                },
                {
                    "sent": "How different models, including our proposed extension perform on?",
                    "label": 0
                },
                {
                    "sent": "Just data sets separately and we compare with some baselines like like a simple naive by listing baseline decomposable attention model.",
                    "label": 0
                },
                {
                    "sent": "Multi channel convolutional neural network, which assassin based model and the hierarchical residual model which was also used for ranking relations.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this these are results for.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Single data set performance.",
                    "label": 0
                },
                {
                    "sent": "So we train or no sequence and report results in a sequence.",
                    "label": 0
                },
                {
                    "sent": "Training quality and reports for quality.",
                    "label": 0
                },
                {
                    "sent": "And to report the Cortina curacy, which is accuracy, which is how many times and how many cases, the correct question was ranked highest by the ranking model and we can see that, like the biosimilar works pretty well, but we get some improvement from using the slot mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the attentions learned during the model, we can actually see that given the question.",
                    "label": 0
                },
                {
                    "sent": "They focus on two different sets of words, which are more characterizing of one of the relations here.",
                    "label": 0
                },
                {
                    "sent": "So mission for the mission relation.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also conducted another set of experiments for transfer learning.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which was thought was interesting.",
                    "label": 0
                },
                {
                    "sent": "So one experiment was.",
                    "label": 0
                },
                {
                    "sent": "Data transfer experiment where we first pre train Aussie quotes and then see how well fine tuning on QLD works.",
                    "label": 0
                },
                {
                    "sent": "And the second set is going from a pre trained language model just straight to the QD&C code.",
                    "label": 0
                },
                {
                    "sent": "So for that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The transfer we pretend to see quotes and finding on Cody and doing fine tuning.",
                    "label": 0
                },
                {
                    "sent": "We experiment with different learning rates, learning rate schedules.",
                    "label": 0
                },
                {
                    "sent": "And we find that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Schedule has a significant effect on the final performance and you also find that compared to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pre training the results on Cody or much higher.",
                    "label": 0
                },
                {
                    "sent": "If you first create a Nazi quote.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also do some experiments with pre trained language model.",
                    "label": 1
                },
                {
                    "sent": "In our case bird.",
                    "label": 0
                },
                {
                    "sent": "Where we?",
                    "label": 0
                },
                {
                    "sent": "Which we put in architecture just simply by replacing the two different by listing encoders with.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A single feature in transformer here and we still apply or slots.",
                    "label": 0
                },
                {
                    "sent": "Like our attentions on top of it.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see that no secret it's giving you even better performance between Cody.",
                    "label": 0
                },
                {
                    "sent": "It's kind of failing, which could be explained by the fact that he is very small and versus a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}