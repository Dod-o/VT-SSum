{
    "id": "lz4yteycmsvrk62i7ebu7xww2bsjctsk",
    "title": "Speech-to-Text Evaluation System",
    "info": {
        "author": [
            "Andreas Stolcke, International Computer Science Institute, UC Berkeley"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2005",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/mlmi04uk_stolcke_stes/",
    "segmentation": [
        [
            "The market.",
            "Oh OK, so I'll give you a very, very brief overview because that's already been spoken about the data and the tasks.",
            "Then the audio processing."
        ],
        [
            "And then to motivate the rest, I need to talk a little bit about the decoding architecture of the Sri system, which is the basis for the for the access right meeting with this system that talk about the acoustic modeling language modeling summarize some of the lecture recognition specific things at the end and overall results and the conclusions and further work.",
            "So this is all known to you."
        ],
        [
            "Already the noteworthy things that maybe that we made sort of a mark development set out of the four development set.",
            "Adding in two of the AMI meetings.",
            "Actually 12 minute excerpts of those to have the right balance of meetings that we could tune on without screwing things up.",
            "As far as the relative amounts of data from different sources are concerned, we didn't actually tune on eval a four week app developed for as an unbiased test set, and occasionally check that things were working properly.",
            "And I will.",
            "Throughout the talk I will give results for both ivela foreign Evil 5.",
            "And the training data from eating training data is the same as everybody else been using.",
            "We didn't actually train on the CMU data because we thought it was too different from the actual eval condition data, so we left it out and for the background training data, we used everything we could.",
            "That was, you know, publicly available.",
            "That means all the switchboard and Fisher.",
            "City or state around the hub for entity two and for broadcast news data."
        ],
        [
            "These are the conditions I should say that we actually did play around with the mark three data a little bit on the development test set and then we were disappointed that it wasn't also in the eval set, but we hope to because we got interesting results on that.",
            "And again, I'll focus on the conference room results and then summarize the lecture results at the end.",
            "So this is what we started out with.",
            "This is our CTS baseline system."
        ],
        [
            "All of that purple box there and the associated acoustic and language models.",
            "And then we added the preprocessing at runtime."
        ],
        [
            "Which I'll talk about shortly, and the models were updated with with meeting date of course, and also some external data like the new web data that we collected, similar to what the ME folks did, and also the proceedings data that we put in the lecture language model."
        ],
        [
            "The acoustic acoustic preprocessing for the distant microphones is very similar, and basically it's the same preprocessing that was already described for the diarization system.",
            "The only difference is that we do a separate clustering step that is distinct from the diarization clustering.",
            "That's something we inherited from the Sri Broadcast news system for close talking microphones.",
            "We didn't do noise reduction, we tried it.",
            "It doesn't help, although it also doesn't hurt much, so you might as well do it if you believe that your data might contain noise.",
            "The waveforms are segmented and the then we have a crosstalk detection and filtering step.",
            "The I should say the segment the speech nonspeech models were the same as last year, except that at the last minute we retrained on the specific speech nonspeech models which helped a little bit.",
            "On the direct training for the distant microphone models, we actually did not apply the delay, some processing.",
            "Rather, we pulled all the different channels and obtained the larger training set that way, which will give us better results.",
            "We believe that's because you get you know, good sample of all the different noise and reverb and so forth.",
            "Conditions that obtain at the different speaker locations."
        ],
        [
            "Um?",
            "Here is the effect of our improved delay.",
            "Some processing.",
            "Last year we had a different approach that was developed by formal who participated.",
            "This year's evaluation in a different capacity.",
            "The speaker localization but, but we've got started last year with something that he implemented.",
            "We did the segmentation 1st and then estimate estimated a single time delay arrival for each segment.",
            "This year we did a continuous estimation.",
            "Well, discrete, still about.",
            "At a step every 50 milliseconds, using the method that Charlie describes, I won't go into that.",
            "The results show that the new method gives us about a 6.5% relative gain on the value for data, and in fact most of the game comes from the improved signal, not from the improved segmentation.",
            "As you can see here.",
            "The Hyatt shame."
        ],
        [
            "Crosstalk filtering is something new this year.",
            "Last year we didn't do anything of that sort.",
            "We had a little bit of post processing which turned out not to work at all, so we discarded that.",
            "This is the work of Kofi Bugatti and I should.",
            "Say this really made a big difference.",
            "It's far from perfect, which is why I made my earlier remark, but it does reduce about 1/3 of the error gap between perfect segmentation.",
            "And you know, if you didn't do any kind of crosstalk elimination, so you see here that on the both on the eval or four and evil 05 data, the number of insertions is dramatically reduced, and again, it's about closing the gap by about a third OK. And of course there's lots of room for improvement.",
            "There was one particularly nasty evil meeting that was from the nest set.",
            "It had two really bad properties.",
            "One is that there was a speaker phone with the speaker only on the speaker phone and so there was no associated individual microphone for that speaker and he or she popped up all over the place on the other channels.",
            "And also there were three channels without any speech.",
            "So basically that was a really bad meeting for us and.",
            "Just prior to this meeting I got the Thomases segmentations and his segment or did much better on that particular meeting.",
            "So if I plug that in just on those two meetings on the list, meetings are error rate drops dramatically, so maybe next time we should pull all our segmentations and figure out which segmenter works best and what source and use that.",
            "So now."
        ],
        [
            "How quickly a summary of our decoding architecture we have two systems that, one being sort of a fast version of the other one being the full version, and sometimes we use the fast version to get something done quickly.",
            "It starts with a bigram decoding using MFC based models.",
            "Non crossword models, we generate lattices which are then rescored with four gram, but the lattices themselves are from bigram decoder and they are then the PLP decoding kicks in with crossword models generating invest lists, which are again we spoke with foreground as well as two kinds of duration models.",
            "Prosodic duration models that gives us a fast recognition output about three times real time.",
            "And of course we do adaptation between these passes, so these these dotted arrows signify that the hypothesis I'm sorry.",
            "The solid arrows indicate that hypothesis are passed on to the next stage for the purpose of adaptation and the dotted line means that the lattices either generated or used by the subsequent decoding pairs.",
            "The full system."
        ],
        [
            "As an extension, all the boxes in red are similar.",
            "Decoding and re scoring steps.",
            "The game that we're playing here is that we have basically two parallel tiers, one based on average.",
            "To see acoustic models, the underworld based on PLP acoustic models and we cross adapt them so the hypothesis from some MFC pass get get passed on to the.",
            "This work?",
            "So we can see here that these lines cross each other, meaning meaning that the output of the embassy pass gets used.",
            "An addict adapting the PLP models and vice versa, and the other new feature is that twice we we combined the systems the system outputs with confusion network combination and also we regenerate the lattices with adapted models and with other parameters to make them thicker and therefore higher quality.",
            "This gives us them and another improvement in the subsequent final decoding classes.",
            "So there's a total of.",
            "You know 1234567 decoding passes and the whole system in the original form on CTS.",
            "When we generate Gaussian shortlists because we're interested in speed, it runs in about 12 times real time.",
            "We didn't bother with the Gaussian shortlist because time was not an issue in this evaluation, and it actually ran about 25 times real time.",
            "OK, so."
        ],
        [
            "So the acoustic features that we're using are, as I said, twice, a twofold.",
            "We have emergency based models and we have PLP based models.",
            "the FCC based models also argumented with voicing features and with this is important with a 25 dimensional.",
            "Vector that is derived from phone posterior features that are estimated by multilayer perceptron.",
            "This is work coming out of.",
            "EXE has been developed for the for our CTS system and gives us about a 10% relative error reduction on the system as a whole.",
            "In both of these kinds of models we use all the standard tricks we do capture normalization, VTL NH, LDA.",
            "We use constraint MLR to do feature space.",
            "Adaptation.",
            "In both training and test, some people call that speaker adaptive training or SAT and that we do MLR in all our decoding passes, we start out with a phone book model.",
            "In the first pass and then later on we do the cross adaptation with the previous hypothesis and the baseline models, meaning the CTS models were trained on 1400 hours of CTS Fisher data and Switchboard data, and the PLP models that we also use.",
            "Later on I'll talk about that.",
            "So we had a set of PLP models trained on the same CTS data, and then we had another set of PLP models.",
            "That were trained on about 900 hours of broadcast news data.",
            "OK."
        ],
        [
            "So here is just a comparison of how far, how much leverage do we get out of improving our baseline models over those period of roughly one year?",
            "So last year's meeting system was based on the fall 2003 CTS models and this year's system was based on the fall 2004 CTS models.",
            "So basically we just use those models without any meeting data thrown in and tested them to see how much that gave us, and we see about between 14 and 7% relative reduction, so it's quite substantial.",
            "But it's far from the 28% relative reduction that we get on CTS data, so this is just the effect of mismatch in training and test.",
            "OK, and now."
        ],
        [
            "We we started doing, you know, adaptation to the meeting data and we have two forms of adaptation.",
            "We have the standard map adaptation of the Gaussians using the meeting data, and we have the feature adaptation, and this is because we have these MLP features which are estimated by by multilayer perceptron and that perception can be adapted to the feature to the meeting data by doing additional training iterations on the meeting data and due to lack of time.",
            "We only did this once and we did it.",
            "We didn't have the immediate already at the time, so we.",
            "So we just used the existing in this data and only for the close talking data.",
            "So you see it gives us very little gain for the distant Mike data, but but some so we could use the adaptive features in all our systems.",
            "The.",
            "And now you have the choice.",
            "Do you?",
            "Do you adapt the features?",
            "Do you adapt the Gaussians or both?",
            "And you see all the four combinations here and you summarizing this.",
            "The feature adaptation and the Gaussian adaptation each separately give about the same game.",
            "And when you put them both together, you get an additional gain.",
            "So this is, this is quite nice that you see that extra game and the overall again is between 2:00 and 8:00 or 8 1/2%.",
            "So relative.",
            "Depending on what test said in what condition."
        ],
        [
            "Another question you might ask is so these MLP features, which were developed originally for CTS.",
            "Do they?",
            "Do they generalize you?",
            "Do they give us similar gains?",
            "Well, on CTS they gave us 2% absolute on the last evaluation into 10% relative.",
            "And here if we compare system adapted systems with and without the extra features, we gotta an 8.4% relative gain on the last eval set.",
            "So this is this is compatible and quite quite positive good news.",
            "Finally."
        ],
        [
            "We tried combining CTS based acoustic models with broadcast news based models.",
            "The rationale is that the CTS based models are better match in terms of speaking style since both meetings and and telephone conversations have spontaneous conversational speech.",
            "But the broadcast news metals are better matched on acoustic ground so the signal bandwidth is better matched.",
            "The broadcast news has some noise, some noisy data and it has some distant microphones so.",
            "If we the rationale again, is to combine these two types of models and get the best of both worlds, and this turned out to work very well for the for the distant microphone, so you see that by replacing the PLP component in the overall system with broadcast news models we see between 8 and 6% relative gain, so that's pretty nice.",
            "We see no consistent gain for the Isma condition.",
            "We also didn't see it on our development assets, so we didn't.",
            "We didn't put it in the evaluation system.",
            "There was a slight bug in the MDM system.",
            "We actually only affectively used only the Mail adaptation data on the broadcast news models.",
            "I thought this was pretty dumb and we re ran the system, but it only resulted in point 1%.",
            "Different, so not much was lost.",
            "We also used a discriminative map."
        ],
        [
            "Adaptation scheme this is something that Cambridge Dan Povey developed.",
            "There's a eurospeech paper couple years ago and this gave us a small but still significant gain about 1% relative, and we had only time to do this on the HM System, so the MDM system use standard maximum likelihood based map adaptation.",
            "Then after the evaluation, there's regards."
        ],
        [
            "Nothing from University of Washington are in demand.",
            "All doctoral student of Mari Ostendorf's is working on speaker dependent amelar class prediction and he implemented a more standard regression tree generation algorithm that space that's data driven rather than handcrafted.",
            "And this gave us a.",
            "About a 1% relative reduction there was.",
            "It doesn't seem to be working as consistently as we would like, but still on the eval set it would have given us another 1% relative.",
            "But this was not in the evaluation system.",
            "OK, now turning to."
        ],
        [
            "At the language models here, there is of course a big difference between conference meetings and lecture meetings, so I'm going to talk about those separately.",
            "The As for the ME system, we had a linear interpolation of of different sources, different source language models.",
            "I should point out that in our system we actually use three types of language models.",
            "We use bigram and trigram lattice model language models for decoding, and then restoring.",
            "We use a four gram language model and these are also pruned at different levels, so the.",
            "The bigrams are is kept small for speed reasons.",
            "The trigram somewhere bigger in the program is approved very likely, but still somewhat, and we use the entropy based pruning technique that's implemented in the Sri Allen Toolkit.",
            "The components of this mixture language model, where the CTS transcripts from Switchboard CTS transcripts from Fisher the broadcast and broadcast news transcripts.",
            "Of course, the meeting transcripts and web data that was collected in different batches.",
            "We had from our city, our system.",
            "We had about 5:30 million words of Fisher type Fisher matched Rep transcripts and we had an additional 270 million words that were collected specifically for for this evaluation, and this, by the way, is the work of Oscar Chat and Who's Who's a postdoc at Dixie?",
            "So here we did.",
            "The standard thing we optimize the perplexity of this mixture on a held out set, which turned out it was best not to include a mediator, and that probably because I'm mediator is.",
            "Is quite distinct in its character and would otherwise sort of skew the mixture, but the army data was part of one of the components, so that's that still included the.",
            "Vocabulary was about 54,000 words.",
            "We we basically started with our CTS vocabulary and then added meeting words.",
            "All the all the words that we found in the old meeting, transcripts and all the non singletons found in the army transcripts and this resulted in only rates that was below half a percent and we were quite happy with that for the lecture meeting."
        ],
        [
            "This was basically our only effort to adapt the system to two lectures, and we did that by essentially adding the little bit of you know lecture type data that we had from the Ted Corpus, the some conference proceedings data about.",
            "28 million words of it's about 10 years of unicast and ICS, LP, eurospeech and so forth.",
            "We removed official web data and we added different web data that was tailored to the to the to the proceedings.",
            "Basically about 100 finding words and we acknowledge the hints and advice that we got from Lindsay on this because they had used similar techniques previously to build their lecture language model.",
            "The OV rates was further reduced by adding.",
            "Not even 4000 words that we cleaned from the conference proceedings at this result.",
            "In there quite a low OV rate, so we're happy with that.",
            "The Flexity was that optimized, understanding way we didn't have a separate tuning set, so we did a jackknifing for development purposes.",
            "We split it into several pieces and tuned up one piece, tested on the others, and so forth, and then for the full for the eval language model we tuned it returned everything on the full chill dev test, and I should note that no chill data are.",
            "Transcripts were actually used.",
            "In in extracting N grams for the language model.",
            "OK, here there."
        ],
        [
            "Is also there's a lot of numbers that will focus on the ones involved.",
            "The on the evil off for meetings.",
            "Well, OK, this this shows you four different language model results.",
            "The original art for FIE the CTS language model, then the meeting tailored language model and the from last year I should say, and the two versions of this use language model the.",
            "One without any web data and one with web data, which is what we used in the evaluation.",
            "So the to summarize the results the.",
            "The the error rate due to meeting data last year versus meeting data this year was lowered by about 1.2% OK and this came from adding the amidee to having the new web data and additional Fisher data.",
            "Last year we had a small amount of Fisher data because not as much was available for the lecture domain.",
            "Of course the reduction was much more dramatic because the old language model was simply mismatched to the meeting domain.",
            "The the web data.",
            "If you compare all of this with and without web data, you can still see about a 1% absolute difference, which is substantial in our world.",
            "So it's something you should definitely do.",
            "Of course, for lectures, the reduction is less because presumably a lot of the same stuff is covered in the proceedings, as in the data you get from the web.",
            "Of course the the result that Sheffield got with their technique of collecting web data is quite exciting, which we should definitely try that.",
            "See if we get more out of the web data the.",
            "We tried last year to use or specific language models and we discarded that because there wasn't enough tuning data to really make it work.",
            "OK."
        ],
        [
            "So now we these are overall results for the conference meeting domain the great, so the conference meetings here.",
            "The question one question to ask is, compared to last year, you know how much better are we doing and we can do that by testing.",
            "Again, we didn't tune on last year's eval set, so we have that as a fair test set and we compare last year system and this year system on the same data and you see that the reduction is about 7 a little over 17% relative.",
            "For MDM these two numbers compared little over 17% relative reduction and little under 17% for the ihn.",
            "It's we can't directly come resolve because last year we didn't even run a full system on SDM.",
            "We only run the fast system, so take that with a grain of salt.",
            "One person we can also ask is is the gap between eye exam and MDM narrowing, because after all you know MDM is the primary eval condition.",
            "We should all be working harder on that.",
            "We should.",
            "We should learn how to bridge the mismatch and so forth between our background training data and the target data so and it seems initially that we're making progress because notice there's only 17% relative difference on eval.",
            "05 So here, between 30 point 225.9 it's a pretty small difference.",
            "If you think about it, but it turns out this is more property of the of the test set, because if you go back to the old eval set the you know the difference is still the same as it was.",
            "It's about 10% absolute.",
            "So we should work harder.",
            "He so another question is we had two new sources of data, I mean and Virginia Tech data.",
            "And does the system generalize properly to this new type of data?",
            "I mean, of course we did have matched training data.",
            "Virginia Tech we did not.",
            "And if you look at the breakdown by source for this year's system at this year's test set we see that it's pretty much the same all over.",
            "We get exactly the same error rate on three out of the five sources.",
            "OK, for IHMS that is.",
            "Interestingly, Island Virginia Tech was actually no harder than the rest.",
            "If anything, it was easier.",
            "The immediate actually stood out and that although we had match training data, it's the MDM.",
            "Performance was the worst.",
            "And I can only guess, but it might have something to do with the fact that the only distant microphones for the Army data were actually the circular array microphone, so they didn't give you much of a spread in the.",
            "You know, they were all roughly the same distance from the speaker, so they didn't really give you much of a benefit over having just a single microphone in that location.",
            "And again we did well on the blind test set, so that's that's always nice.",
            "And then we have this outlier result really here with the missed Dennis TIHM data, because for the for the reasons I already mentioned, so we should learn how to do better on these degenerate forms of meetings where you have non my participants and even participants who don't speak and just generate across time.",
            "OK for the lecture."
        ],
        [
            "System.",
            "As I found out later that the Miss Def set was basically the chill January 05 eval set plus their adaptation data so.",
            "Again, we didn't use any chill data for acoustic training or adaptation.",
            "We just reuse the meeting acoustic models.",
            "We did update the language model as I described and also we didn't return the model weights.",
            "This is typically quite important.",
            "Our system to estimate the proper weighting of language model acoustic model, insertion penalty and so forth.",
            "We didn't do that, we just ran it as we had determined to be further meetings.",
            "The crosstalk filtering was run without actually.",
            "I still don't know what at work, because the death data had only one channel on and so we couldn't run it, and then we just ran it and hoped it worked on the on the eval data because they are you actually did have multiple HM Channels and potential for cross talk and so forth.",
            "The two interesting findings were that speaker clustering didn't help.",
            "It was best, and this is sort of echoes.",
            "Defining of the diarization folks, it was best to just lump all the speech into one speaker cluster and do adaptation and normalization on that, presumably because the lecture was dominated by the lecturing speaker.",
            "The other finding was that the delay start processing on the table top Max was worse than using just a single best Mike with the lowest with the highest SNR, and Charlie found that the SNR is very widely.",
            "That was probably the reason why and that's why he started working on waiting and so forth, and hopefully will get to get that to work in the future.",
            "For the results."
        ],
        [
            "On lectures we we have two sets of results for the for the death set up here and then.",
            "For the eval set a couple of remarks so the as miss already observed the I HM error rates are quite sort of in the same ballpark As for the meetings, so that's good, but the distant Mike error rates are around 50%, so that's more than 10% worse absolute than the meeting meeting results that the conference meeting results, so there's a lot of work to be done here.",
            "Understandable because the room acoustics and the lecture setting are much more difficult inherently.",
            "The the error is also in the same ballpark as what Lindsey reported in their own test, so so we're sort of not too far off the mark, which is good because we again didn't really develop for this for this task, then we have some normal abnormal results here.",
            "I mean, the MDM is worse on the eval slightly than the SDM.",
            "That's because of this business of picking the right microphone, which you can of course make the wrong choices in and.",
            "But then on the other hand, we found very good results.",
            "In general, the best results.",
            "In the right processing came from the source localization microphones.",
            "Especially for the eval data and we're still waiting to fill in this box here.",
            "Notice how on the DEF data that was the best result using just one of these mark three arrays as the computed by folks at classroom.",
            "So John, where's the email data?",
            "Anyway, that was that was let me come to the conclusion.",
            "So pretty bright picture actually."
        ],
        [
            "We made about 17% relative progress in getting the word error rate.",
            "That word error rates down on the conference meetings.",
            "We made good use of the extra CTS in broadcast news data and the special twist there is.",
            "If you combine these two as your background models for the MDM condition, you really got a nice game.",
            "The preprocessing both for MDM and for eye exam with crosstalk suppression really improved alot, although far from perfect, especially for the I, HM, the all of the modeling components.",
            "Acoustic modeling components are adapted so features are adapted by virtue of having these MLP features.",
            "Acoustic models.",
            "The Gaussians are adapted and of course the language models are adapted, especially with Rep data being important there and the system generalized well.",
            "As I said, generalized to data, we had training data for, but was New Army we had.",
            "Generalized to data that was new but we didn't have training data for and generalized to whole new task, which is the lecture domain for future."
        ],
        [
            "Work, there's lots of stuff.",
            "I mean, we should fix the things we didn't have time to do properly.",
            "So, for instance, we should really use MMI map for their distant microphones models as well.",
            "We should use the.",
            "You know we should adapt the MLP's for the distant microphones.",
            "We didn't have time to do that for I HM, you know, forget about doing you know, acoustic modeling or language modeling.",
            "That's let's solve this problem with the speakers speech activity detection.",
            "I mean, that's the single worst offender here and for the lecture recognition there's many things we can improve.",
            "We can actually, we should actually try using the tab acoustic training data.",
            "We should try getting the MDM to work better than the SDM.",
            "And we should properly estimate the system parameters for this domain user.",
            "Almost no brainer things to do and we we should also explore what I needed, namely use MLR type techniques to bridge the gap between the mismatch between the bandwidth on the background data and the meeting domain data.",
            "The we haven't done anything to model non native speakers of American English, so we have Germans, Brits, Scots, etc.",
            "They they're mismatched because all the CTS data or for the most part is.",
            "Is American English so there is some data available we should try to do something like what you saw on our lawyers poster yesterday.",
            "You know, do accent detection and properly handle that.",
            "And then generally I think we should be more adaptive to the meeting type and the content.",
            "Have a system that really just gets it right with whatever data is presented through it."
        ],
        [
            "Remind Mike.",
            "Just go back to your results line.",
            "Will you show the the results for the?",
            "Am I supposed to the other ones?",
            "Yeah damn.",
            "I was abnormally hot.",
            "Yeah, I would just again I would like to see this result.",
            "If you really did true beamforming using the topology there a speaker legislation were, well that like everybody else, we didn't have the energy and smarts to do it right.",
            "But.",
            "Experiments on your data just because, well, I would suggest that you process the data and give us the enhanced signal and will run the recognizer.",
            "Prosoft the two rooms were very different.",
            "What is?",
            "We got lots of questions but one question.",
            "Do you have any indication on language model perplexity as you get on chill data?",
            "I mean, I'm assuming that yeah, I left those out.",
            "There's a remark here that I'm sorry.",
            "Anyway that's so perplexities are in the paper.",
            "If you look at the table already had too many numbers, so I decided not to put them in.",
            "But our perplexities on the chill data are on the order of 140, so they're harder than yours.",
            "Which is why I believe that that your technique works quite a bit better for the web to make use of the web data.",
            "Another question on animal off he said you're talking about regression plants trees.",
            "So how many transforms do you normally use it?",
            "How do you in the first pass when we do the phone number notation, we use three transforms, one for non speech and two for speech and then in the in the later passes we use typically 9 transforms.",
            "But but you know, we do back off through fuel transforms if there isn't enough adaptation data in any given class.",
            "But the maximum that you can get is 9 classes.",
            "It is to have more transforms.",
            "Yeah, I know that.",
            "Actually I'm sorry we use about we use only.",
            "I think 6 transforms for the MDM because typically the lower error rate, the more you can afford to use detailed transforms.",
            "But for the item we get at least a percent from like moving from.",
            "I don't know three transforms to mine transforms.",
            "Absolute.",
            "General.",
            "The STT general task."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The market.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so I'll give you a very, very brief overview because that's already been spoken about the data and the tasks.",
                    "label": 0
                },
                {
                    "sent": "Then the audio processing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then to motivate the rest, I need to talk a little bit about the decoding architecture of the Sri system, which is the basis for the for the access right meeting with this system that talk about the acoustic modeling language modeling summarize some of the lecture recognition specific things at the end and overall results and the conclusions and further work.",
                    "label": 0
                },
                {
                    "sent": "So this is all known to you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already the noteworthy things that maybe that we made sort of a mark development set out of the four development set.",
                    "label": 0
                },
                {
                    "sent": "Adding in two of the AMI meetings.",
                    "label": 0
                },
                {
                    "sent": "Actually 12 minute excerpts of those to have the right balance of meetings that we could tune on without screwing things up.",
                    "label": 0
                },
                {
                    "sent": "As far as the relative amounts of data from different sources are concerned, we didn't actually tune on eval a four week app developed for as an unbiased test set, and occasionally check that things were working properly.",
                    "label": 1
                },
                {
                    "sent": "And I will.",
                    "label": 0
                },
                {
                    "sent": "Throughout the talk I will give results for both ivela foreign Evil 5.",
                    "label": 1
                },
                {
                    "sent": "And the training data from eating training data is the same as everybody else been using.",
                    "label": 1
                },
                {
                    "sent": "We didn't actually train on the CMU data because we thought it was too different from the actual eval condition data, so we left it out and for the background training data, we used everything we could.",
                    "label": 0
                },
                {
                    "sent": "That was, you know, publicly available.",
                    "label": 0
                },
                {
                    "sent": "That means all the switchboard and Fisher.",
                    "label": 0
                },
                {
                    "sent": "City or state around the hub for entity two and for broadcast news data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the conditions I should say that we actually did play around with the mark three data a little bit on the development test set and then we were disappointed that it wasn't also in the eval set, but we hope to because we got interesting results on that.",
                    "label": 0
                },
                {
                    "sent": "And again, I'll focus on the conference room results and then summarize the lecture results at the end.",
                    "label": 1
                },
                {
                    "sent": "So this is what we started out with.",
                    "label": 0
                },
                {
                    "sent": "This is our CTS baseline system.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of that purple box there and the associated acoustic and language models.",
                    "label": 0
                },
                {
                    "sent": "And then we added the preprocessing at runtime.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I'll talk about shortly, and the models were updated with with meeting date of course, and also some external data like the new web data that we collected, similar to what the ME folks did, and also the proceedings data that we put in the lecture language model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The acoustic acoustic preprocessing for the distant microphones is very similar, and basically it's the same preprocessing that was already described for the diarization system.",
                    "label": 1
                },
                {
                    "sent": "The only difference is that we do a separate clustering step that is distinct from the diarization clustering.",
                    "label": 0
                },
                {
                    "sent": "That's something we inherited from the Sri Broadcast news system for close talking microphones.",
                    "label": 0
                },
                {
                    "sent": "We didn't do noise reduction, we tried it.",
                    "label": 1
                },
                {
                    "sent": "It doesn't help, although it also doesn't hurt much, so you might as well do it if you believe that your data might contain noise.",
                    "label": 0
                },
                {
                    "sent": "The waveforms are segmented and the then we have a crosstalk detection and filtering step.",
                    "label": 1
                },
                {
                    "sent": "The I should say the segment the speech nonspeech models were the same as last year, except that at the last minute we retrained on the specific speech nonspeech models which helped a little bit.",
                    "label": 0
                },
                {
                    "sent": "On the direct training for the distant microphone models, we actually did not apply the delay, some processing.",
                    "label": 0
                },
                {
                    "sent": "Rather, we pulled all the different channels and obtained the larger training set that way, which will give us better results.",
                    "label": 0
                },
                {
                    "sent": "We believe that's because you get you know, good sample of all the different noise and reverb and so forth.",
                    "label": 0
                },
                {
                    "sent": "Conditions that obtain at the different speaker locations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here is the effect of our improved delay.",
                    "label": 0
                },
                {
                    "sent": "Some processing.",
                    "label": 0
                },
                {
                    "sent": "Last year we had a different approach that was developed by formal who participated.",
                    "label": 0
                },
                {
                    "sent": "This year's evaluation in a different capacity.",
                    "label": 0
                },
                {
                    "sent": "The speaker localization but, but we've got started last year with something that he implemented.",
                    "label": 0
                },
                {
                    "sent": "We did the segmentation 1st and then estimate estimated a single time delay arrival for each segment.",
                    "label": 1
                },
                {
                    "sent": "This year we did a continuous estimation.",
                    "label": 0
                },
                {
                    "sent": "Well, discrete, still about.",
                    "label": 0
                },
                {
                    "sent": "At a step every 50 milliseconds, using the method that Charlie describes, I won't go into that.",
                    "label": 0
                },
                {
                    "sent": "The results show that the new method gives us about a 6.5% relative gain on the value for data, and in fact most of the game comes from the improved signal, not from the improved segmentation.",
                    "label": 1
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "The Hyatt shame.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crosstalk filtering is something new this year.",
                    "label": 0
                },
                {
                    "sent": "Last year we didn't do anything of that sort.",
                    "label": 0
                },
                {
                    "sent": "We had a little bit of post processing which turned out not to work at all, so we discarded that.",
                    "label": 0
                },
                {
                    "sent": "This is the work of Kofi Bugatti and I should.",
                    "label": 0
                },
                {
                    "sent": "Say this really made a big difference.",
                    "label": 0
                },
                {
                    "sent": "It's far from perfect, which is why I made my earlier remark, but it does reduce about 1/3 of the error gap between perfect segmentation.",
                    "label": 0
                },
                {
                    "sent": "And you know, if you didn't do any kind of crosstalk elimination, so you see here that on the both on the eval or four and evil 05 data, the number of insertions is dramatically reduced, and again, it's about closing the gap by about a third OK. And of course there's lots of room for improvement.",
                    "label": 0
                },
                {
                    "sent": "There was one particularly nasty evil meeting that was from the nest set.",
                    "label": 0
                },
                {
                    "sent": "It had two really bad properties.",
                    "label": 0
                },
                {
                    "sent": "One is that there was a speaker phone with the speaker only on the speaker phone and so there was no associated individual microphone for that speaker and he or she popped up all over the place on the other channels.",
                    "label": 0
                },
                {
                    "sent": "And also there were three channels without any speech.",
                    "label": 0
                },
                {
                    "sent": "So basically that was a really bad meeting for us and.",
                    "label": 0
                },
                {
                    "sent": "Just prior to this meeting I got the Thomases segmentations and his segment or did much better on that particular meeting.",
                    "label": 0
                },
                {
                    "sent": "So if I plug that in just on those two meetings on the list, meetings are error rate drops dramatically, so maybe next time we should pull all our segmentations and figure out which segmenter works best and what source and use that.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How quickly a summary of our decoding architecture we have two systems that, one being sort of a fast version of the other one being the full version, and sometimes we use the fast version to get something done quickly.",
                    "label": 0
                },
                {
                    "sent": "It starts with a bigram decoding using MFC based models.",
                    "label": 0
                },
                {
                    "sent": "Non crossword models, we generate lattices which are then rescored with four gram, but the lattices themselves are from bigram decoder and they are then the PLP decoding kicks in with crossword models generating invest lists, which are again we spoke with foreground as well as two kinds of duration models.",
                    "label": 0
                },
                {
                    "sent": "Prosodic duration models that gives us a fast recognition output about three times real time.",
                    "label": 0
                },
                {
                    "sent": "And of course we do adaptation between these passes, so these these dotted arrows signify that the hypothesis I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "The solid arrows indicate that hypothesis are passed on to the next stage for the purpose of adaptation and the dotted line means that the lattices either generated or used by the subsequent decoding pairs.",
                    "label": 0
                },
                {
                    "sent": "The full system.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As an extension, all the boxes in red are similar.",
                    "label": 0
                },
                {
                    "sent": "Decoding and re scoring steps.",
                    "label": 0
                },
                {
                    "sent": "The game that we're playing here is that we have basically two parallel tiers, one based on average.",
                    "label": 0
                },
                {
                    "sent": "To see acoustic models, the underworld based on PLP acoustic models and we cross adapt them so the hypothesis from some MFC pass get get passed on to the.",
                    "label": 0
                },
                {
                    "sent": "This work?",
                    "label": 0
                },
                {
                    "sent": "So we can see here that these lines cross each other, meaning meaning that the output of the embassy pass gets used.",
                    "label": 0
                },
                {
                    "sent": "An addict adapting the PLP models and vice versa, and the other new feature is that twice we we combined the systems the system outputs with confusion network combination and also we regenerate the lattices with adapted models and with other parameters to make them thicker and therefore higher quality.",
                    "label": 0
                },
                {
                    "sent": "This gives us them and another improvement in the subsequent final decoding classes.",
                    "label": 0
                },
                {
                    "sent": "So there's a total of.",
                    "label": 0
                },
                {
                    "sent": "You know 1234567 decoding passes and the whole system in the original form on CTS.",
                    "label": 0
                },
                {
                    "sent": "When we generate Gaussian shortlists because we're interested in speed, it runs in about 12 times real time.",
                    "label": 0
                },
                {
                    "sent": "We didn't bother with the Gaussian shortlist because time was not an issue in this evaluation, and it actually ran about 25 times real time.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the acoustic features that we're using are, as I said, twice, a twofold.",
                    "label": 0
                },
                {
                    "sent": "We have emergency based models and we have PLP based models.",
                    "label": 0
                },
                {
                    "sent": "the FCC based models also argumented with voicing features and with this is important with a 25 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Vector that is derived from phone posterior features that are estimated by multilayer perceptron.",
                    "label": 1
                },
                {
                    "sent": "This is work coming out of.",
                    "label": 0
                },
                {
                    "sent": "EXE has been developed for the for our CTS system and gives us about a 10% relative error reduction on the system as a whole.",
                    "label": 0
                },
                {
                    "sent": "In both of these kinds of models we use all the standard tricks we do capture normalization, VTL NH, LDA.",
                    "label": 0
                },
                {
                    "sent": "We use constraint MLR to do feature space.",
                    "label": 0
                },
                {
                    "sent": "Adaptation.",
                    "label": 0
                },
                {
                    "sent": "In both training and test, some people call that speaker adaptive training or SAT and that we do MLR in all our decoding passes, we start out with a phone book model.",
                    "label": 0
                },
                {
                    "sent": "In the first pass and then later on we do the cross adaptation with the previous hypothesis and the baseline models, meaning the CTS models were trained on 1400 hours of CTS Fisher data and Switchboard data, and the PLP models that we also use.",
                    "label": 0
                },
                {
                    "sent": "Later on I'll talk about that.",
                    "label": 1
                },
                {
                    "sent": "So we had a set of PLP models trained on the same CTS data, and then we had another set of PLP models.",
                    "label": 0
                },
                {
                    "sent": "That were trained on about 900 hours of broadcast news data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is just a comparison of how far, how much leverage do we get out of improving our baseline models over those period of roughly one year?",
                    "label": 0
                },
                {
                    "sent": "So last year's meeting system was based on the fall 2003 CTS models and this year's system was based on the fall 2004 CTS models.",
                    "label": 0
                },
                {
                    "sent": "So basically we just use those models without any meeting data thrown in and tested them to see how much that gave us, and we see about between 14 and 7% relative reduction, so it's quite substantial.",
                    "label": 0
                },
                {
                    "sent": "But it's far from the 28% relative reduction that we get on CTS data, so this is just the effect of mismatch in training and test.",
                    "label": 0
                },
                {
                    "sent": "OK, and now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We we started doing, you know, adaptation to the meeting data and we have two forms of adaptation.",
                    "label": 0
                },
                {
                    "sent": "We have the standard map adaptation of the Gaussians using the meeting data, and we have the feature adaptation, and this is because we have these MLP features which are estimated by by multilayer perceptron and that perception can be adapted to the feature to the meeting data by doing additional training iterations on the meeting data and due to lack of time.",
                    "label": 1
                },
                {
                    "sent": "We only did this once and we did it.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the immediate already at the time, so we.",
                    "label": 0
                },
                {
                    "sent": "So we just used the existing in this data and only for the close talking data.",
                    "label": 0
                },
                {
                    "sent": "So you see it gives us very little gain for the distant Mike data, but but some so we could use the adaptive features in all our systems.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "And now you have the choice.",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Do you adapt the features?",
                    "label": 0
                },
                {
                    "sent": "Do you adapt the Gaussians or both?",
                    "label": 0
                },
                {
                    "sent": "And you see all the four combinations here and you summarizing this.",
                    "label": 0
                },
                {
                    "sent": "The feature adaptation and the Gaussian adaptation each separately give about the same game.",
                    "label": 0
                },
                {
                    "sent": "And when you put them both together, you get an additional gain.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is quite nice that you see that extra game and the overall again is between 2:00 and 8:00 or 8 1/2%.",
                    "label": 0
                },
                {
                    "sent": "So relative.",
                    "label": 0
                },
                {
                    "sent": "Depending on what test said in what condition.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another question you might ask is so these MLP features, which were developed originally for CTS.",
                    "label": 0
                },
                {
                    "sent": "Do they?",
                    "label": 0
                },
                {
                    "sent": "Do they generalize you?",
                    "label": 0
                },
                {
                    "sent": "Do they give us similar gains?",
                    "label": 0
                },
                {
                    "sent": "Well, on CTS they gave us 2% absolute on the last evaluation into 10% relative.",
                    "label": 1
                },
                {
                    "sent": "And here if we compare system adapted systems with and without the extra features, we gotta an 8.4% relative gain on the last eval set.",
                    "label": 0
                },
                {
                    "sent": "So this is this is compatible and quite quite positive good news.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We tried combining CTS based acoustic models with broadcast news based models.",
                    "label": 1
                },
                {
                    "sent": "The rationale is that the CTS based models are better match in terms of speaking style since both meetings and and telephone conversations have spontaneous conversational speech.",
                    "label": 0
                },
                {
                    "sent": "But the broadcast news metals are better matched on acoustic ground so the signal bandwidth is better matched.",
                    "label": 1
                },
                {
                    "sent": "The broadcast news has some noise, some noisy data and it has some distant microphones so.",
                    "label": 1
                },
                {
                    "sent": "If we the rationale again, is to combine these two types of models and get the best of both worlds, and this turned out to work very well for the for the distant microphone, so you see that by replacing the PLP component in the overall system with broadcast news models we see between 8 and 6% relative gain, so that's pretty nice.",
                    "label": 0
                },
                {
                    "sent": "We see no consistent gain for the Isma condition.",
                    "label": 0
                },
                {
                    "sent": "We also didn't see it on our development assets, so we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't put it in the evaluation system.",
                    "label": 0
                },
                {
                    "sent": "There was a slight bug in the MDM system.",
                    "label": 0
                },
                {
                    "sent": "We actually only affectively used only the Mail adaptation data on the broadcast news models.",
                    "label": 0
                },
                {
                    "sent": "I thought this was pretty dumb and we re ran the system, but it only resulted in point 1%.",
                    "label": 0
                },
                {
                    "sent": "Different, so not much was lost.",
                    "label": 0
                },
                {
                    "sent": "We also used a discriminative map.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adaptation scheme this is something that Cambridge Dan Povey developed.",
                    "label": 0
                },
                {
                    "sent": "There's a eurospeech paper couple years ago and this gave us a small but still significant gain about 1% relative, and we had only time to do this on the HM System, so the MDM system use standard maximum likelihood based map adaptation.",
                    "label": 0
                },
                {
                    "sent": "Then after the evaluation, there's regards.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nothing from University of Washington are in demand.",
                    "label": 0
                },
                {
                    "sent": "All doctoral student of Mari Ostendorf's is working on speaker dependent amelar class prediction and he implemented a more standard regression tree generation algorithm that space that's data driven rather than handcrafted.",
                    "label": 1
                },
                {
                    "sent": "And this gave us a.",
                    "label": 1
                },
                {
                    "sent": "About a 1% relative reduction there was.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem to be working as consistently as we would like, but still on the eval set it would have given us another 1% relative.",
                    "label": 0
                },
                {
                    "sent": "But this was not in the evaluation system.",
                    "label": 0
                },
                {
                    "sent": "OK, now turning to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the language models here, there is of course a big difference between conference meetings and lecture meetings, so I'm going to talk about those separately.",
                    "label": 0
                },
                {
                    "sent": "The As for the ME system, we had a linear interpolation of of different sources, different source language models.",
                    "label": 0
                },
                {
                    "sent": "I should point out that in our system we actually use three types of language models.",
                    "label": 0
                },
                {
                    "sent": "We use bigram and trigram lattice model language models for decoding, and then restoring.",
                    "label": 0
                },
                {
                    "sent": "We use a four gram language model and these are also pruned at different levels, so the.",
                    "label": 0
                },
                {
                    "sent": "The bigrams are is kept small for speed reasons.",
                    "label": 0
                },
                {
                    "sent": "The trigram somewhere bigger in the program is approved very likely, but still somewhat, and we use the entropy based pruning technique that's implemented in the Sri Allen Toolkit.",
                    "label": 0
                },
                {
                    "sent": "The components of this mixture language model, where the CTS transcripts from Switchboard CTS transcripts from Fisher the broadcast and broadcast news transcripts.",
                    "label": 1
                },
                {
                    "sent": "Of course, the meeting transcripts and web data that was collected in different batches.",
                    "label": 1
                },
                {
                    "sent": "We had from our city, our system.",
                    "label": 0
                },
                {
                    "sent": "We had about 5:30 million words of Fisher type Fisher matched Rep transcripts and we had an additional 270 million words that were collected specifically for for this evaluation, and this, by the way, is the work of Oscar Chat and Who's Who's a postdoc at Dixie?",
                    "label": 0
                },
                {
                    "sent": "So here we did.",
                    "label": 0
                },
                {
                    "sent": "The standard thing we optimize the perplexity of this mixture on a held out set, which turned out it was best not to include a mediator, and that probably because I'm mediator is.",
                    "label": 0
                },
                {
                    "sent": "Is quite distinct in its character and would otherwise sort of skew the mixture, but the army data was part of one of the components, so that's that still included the.",
                    "label": 0
                },
                {
                    "sent": "Vocabulary was about 54,000 words.",
                    "label": 0
                },
                {
                    "sent": "We we basically started with our CTS vocabulary and then added meeting words.",
                    "label": 0
                },
                {
                    "sent": "All the all the words that we found in the old meeting, transcripts and all the non singletons found in the army transcripts and this resulted in only rates that was below half a percent and we were quite happy with that for the lecture meeting.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was basically our only effort to adapt the system to two lectures, and we did that by essentially adding the little bit of you know lecture type data that we had from the Ted Corpus, the some conference proceedings data about.",
                    "label": 0
                },
                {
                    "sent": "28 million words of it's about 10 years of unicast and ICS, LP, eurospeech and so forth.",
                    "label": 0
                },
                {
                    "sent": "We removed official web data and we added different web data that was tailored to the to the to the proceedings.",
                    "label": 1
                },
                {
                    "sent": "Basically about 100 finding words and we acknowledge the hints and advice that we got from Lindsay on this because they had used similar techniques previously to build their lecture language model.",
                    "label": 0
                },
                {
                    "sent": "The OV rates was further reduced by adding.",
                    "label": 1
                },
                {
                    "sent": "Not even 4000 words that we cleaned from the conference proceedings at this result.",
                    "label": 1
                },
                {
                    "sent": "In there quite a low OV rate, so we're happy with that.",
                    "label": 0
                },
                {
                    "sent": "The Flexity was that optimized, understanding way we didn't have a separate tuning set, so we did a jackknifing for development purposes.",
                    "label": 0
                },
                {
                    "sent": "We split it into several pieces and tuned up one piece, tested on the others, and so forth, and then for the full for the eval language model we tuned it returned everything on the full chill dev test, and I should note that no chill data are.",
                    "label": 0
                },
                {
                    "sent": "Transcripts were actually used.",
                    "label": 0
                },
                {
                    "sent": "In in extracting N grams for the language model.",
                    "label": 0
                },
                {
                    "sent": "OK, here there.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is also there's a lot of numbers that will focus on the ones involved.",
                    "label": 0
                },
                {
                    "sent": "The on the evil off for meetings.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, this this shows you four different language model results.",
                    "label": 0
                },
                {
                    "sent": "The original art for FIE the CTS language model, then the meeting tailored language model and the from last year I should say, and the two versions of this use language model the.",
                    "label": 0
                },
                {
                    "sent": "One without any web data and one with web data, which is what we used in the evaluation.",
                    "label": 0
                },
                {
                    "sent": "So the to summarize the results the.",
                    "label": 0
                },
                {
                    "sent": "The the error rate due to meeting data last year versus meeting data this year was lowered by about 1.2% OK and this came from adding the amidee to having the new web data and additional Fisher data.",
                    "label": 1
                },
                {
                    "sent": "Last year we had a small amount of Fisher data because not as much was available for the lecture domain.",
                    "label": 0
                },
                {
                    "sent": "Of course the reduction was much more dramatic because the old language model was simply mismatched to the meeting domain.",
                    "label": 0
                },
                {
                    "sent": "The the web data.",
                    "label": 0
                },
                {
                    "sent": "If you compare all of this with and without web data, you can still see about a 1% absolute difference, which is substantial in our world.",
                    "label": 0
                },
                {
                    "sent": "So it's something you should definitely do.",
                    "label": 0
                },
                {
                    "sent": "Of course, for lectures, the reduction is less because presumably a lot of the same stuff is covered in the proceedings, as in the data you get from the web.",
                    "label": 0
                },
                {
                    "sent": "Of course the the result that Sheffield got with their technique of collecting web data is quite exciting, which we should definitely try that.",
                    "label": 0
                },
                {
                    "sent": "See if we get more out of the web data the.",
                    "label": 0
                },
                {
                    "sent": "We tried last year to use or specific language models and we discarded that because there wasn't enough tuning data to really make it work.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we these are overall results for the conference meeting domain the great, so the conference meetings here.",
                    "label": 1
                },
                {
                    "sent": "The question one question to ask is, compared to last year, you know how much better are we doing and we can do that by testing.",
                    "label": 0
                },
                {
                    "sent": "Again, we didn't tune on last year's eval set, so we have that as a fair test set and we compare last year system and this year system on the same data and you see that the reduction is about 7 a little over 17% relative.",
                    "label": 0
                },
                {
                    "sent": "For MDM these two numbers compared little over 17% relative reduction and little under 17% for the ihn.",
                    "label": 0
                },
                {
                    "sent": "It's we can't directly come resolve because last year we didn't even run a full system on SDM.",
                    "label": 0
                },
                {
                    "sent": "We only run the fast system, so take that with a grain of salt.",
                    "label": 0
                },
                {
                    "sent": "One person we can also ask is is the gap between eye exam and MDM narrowing, because after all you know MDM is the primary eval condition.",
                    "label": 0
                },
                {
                    "sent": "We should all be working harder on that.",
                    "label": 0
                },
                {
                    "sent": "We should.",
                    "label": 1
                },
                {
                    "sent": "We should learn how to bridge the mismatch and so forth between our background training data and the target data so and it seems initially that we're making progress because notice there's only 17% relative difference on eval.",
                    "label": 0
                },
                {
                    "sent": "05 So here, between 30 point 225.9 it's a pretty small difference.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, but it turns out this is more property of the of the test set, because if you go back to the old eval set the you know the difference is still the same as it was.",
                    "label": 0
                },
                {
                    "sent": "It's about 10% absolute.",
                    "label": 0
                },
                {
                    "sent": "So we should work harder.",
                    "label": 0
                },
                {
                    "sent": "He so another question is we had two new sources of data, I mean and Virginia Tech data.",
                    "label": 0
                },
                {
                    "sent": "And does the system generalize properly to this new type of data?",
                    "label": 1
                },
                {
                    "sent": "I mean, of course we did have matched training data.",
                    "label": 0
                },
                {
                    "sent": "Virginia Tech we did not.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the breakdown by source for this year's system at this year's test set we see that it's pretty much the same all over.",
                    "label": 0
                },
                {
                    "sent": "We get exactly the same error rate on three out of the five sources.",
                    "label": 0
                },
                {
                    "sent": "OK, for IHMS that is.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, Island Virginia Tech was actually no harder than the rest.",
                    "label": 0
                },
                {
                    "sent": "If anything, it was easier.",
                    "label": 0
                },
                {
                    "sent": "The immediate actually stood out and that although we had match training data, it's the MDM.",
                    "label": 0
                },
                {
                    "sent": "Performance was the worst.",
                    "label": 0
                },
                {
                    "sent": "And I can only guess, but it might have something to do with the fact that the only distant microphones for the Army data were actually the circular array microphone, so they didn't give you much of a spread in the.",
                    "label": 0
                },
                {
                    "sent": "You know, they were all roughly the same distance from the speaker, so they didn't really give you much of a benefit over having just a single microphone in that location.",
                    "label": 0
                },
                {
                    "sent": "And again we did well on the blind test set, so that's that's always nice.",
                    "label": 1
                },
                {
                    "sent": "And then we have this outlier result really here with the missed Dennis TIHM data, because for the for the reasons I already mentioned, so we should learn how to do better on these degenerate forms of meetings where you have non my participants and even participants who don't speak and just generate across time.",
                    "label": 0
                },
                {
                    "sent": "OK for the lecture.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "As I found out later that the Miss Def set was basically the chill January 05 eval set plus their adaptation data so.",
                    "label": 0
                },
                {
                    "sent": "Again, we didn't use any chill data for acoustic training or adaptation.",
                    "label": 1
                },
                {
                    "sent": "We just reuse the meeting acoustic models.",
                    "label": 0
                },
                {
                    "sent": "We did update the language model as I described and also we didn't return the model weights.",
                    "label": 0
                },
                {
                    "sent": "This is typically quite important.",
                    "label": 0
                },
                {
                    "sent": "Our system to estimate the proper weighting of language model acoustic model, insertion penalty and so forth.",
                    "label": 1
                },
                {
                    "sent": "We didn't do that, we just ran it as we had determined to be further meetings.",
                    "label": 0
                },
                {
                    "sent": "The crosstalk filtering was run without actually.",
                    "label": 0
                },
                {
                    "sent": "I still don't know what at work, because the death data had only one channel on and so we couldn't run it, and then we just ran it and hoped it worked on the on the eval data because they are you actually did have multiple HM Channels and potential for cross talk and so forth.",
                    "label": 0
                },
                {
                    "sent": "The two interesting findings were that speaker clustering didn't help.",
                    "label": 0
                },
                {
                    "sent": "It was best, and this is sort of echoes.",
                    "label": 1
                },
                {
                    "sent": "Defining of the diarization folks, it was best to just lump all the speech into one speaker cluster and do adaptation and normalization on that, presumably because the lecture was dominated by the lecturing speaker.",
                    "label": 0
                },
                {
                    "sent": "The other finding was that the delay start processing on the table top Max was worse than using just a single best Mike with the lowest with the highest SNR, and Charlie found that the SNR is very widely.",
                    "label": 0
                },
                {
                    "sent": "That was probably the reason why and that's why he started working on waiting and so forth, and hopefully will get to get that to work in the future.",
                    "label": 0
                },
                {
                    "sent": "For the results.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On lectures we we have two sets of results for the for the death set up here and then.",
                    "label": 0
                },
                {
                    "sent": "For the eval set a couple of remarks so the as miss already observed the I HM error rates are quite sort of in the same ballpark As for the meetings, so that's good, but the distant Mike error rates are around 50%, so that's more than 10% worse absolute than the meeting meeting results that the conference meeting results, so there's a lot of work to be done here.",
                    "label": 0
                },
                {
                    "sent": "Understandable because the room acoustics and the lecture setting are much more difficult inherently.",
                    "label": 0
                },
                {
                    "sent": "The the error is also in the same ballpark as what Lindsey reported in their own test, so so we're sort of not too far off the mark, which is good because we again didn't really develop for this for this task, then we have some normal abnormal results here.",
                    "label": 0
                },
                {
                    "sent": "I mean, the MDM is worse on the eval slightly than the SDM.",
                    "label": 0
                },
                {
                    "sent": "That's because of this business of picking the right microphone, which you can of course make the wrong choices in and.",
                    "label": 0
                },
                {
                    "sent": "But then on the other hand, we found very good results.",
                    "label": 0
                },
                {
                    "sent": "In general, the best results.",
                    "label": 0
                },
                {
                    "sent": "In the right processing came from the source localization microphones.",
                    "label": 0
                },
                {
                    "sent": "Especially for the eval data and we're still waiting to fill in this box here.",
                    "label": 0
                },
                {
                    "sent": "Notice how on the DEF data that was the best result using just one of these mark three arrays as the computed by folks at classroom.",
                    "label": 0
                },
                {
                    "sent": "So John, where's the email data?",
                    "label": 0
                },
                {
                    "sent": "Anyway, that was that was let me come to the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So pretty bright picture actually.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We made about 17% relative progress in getting the word error rate.",
                    "label": 1
                },
                {
                    "sent": "That word error rates down on the conference meetings.",
                    "label": 0
                },
                {
                    "sent": "We made good use of the extra CTS in broadcast news data and the special twist there is.",
                    "label": 0
                },
                {
                    "sent": "If you combine these two as your background models for the MDM condition, you really got a nice game.",
                    "label": 0
                },
                {
                    "sent": "The preprocessing both for MDM and for eye exam with crosstalk suppression really improved alot, although far from perfect, especially for the I, HM, the all of the modeling components.",
                    "label": 1
                },
                {
                    "sent": "Acoustic modeling components are adapted so features are adapted by virtue of having these MLP features.",
                    "label": 0
                },
                {
                    "sent": "Acoustic models.",
                    "label": 0
                },
                {
                    "sent": "The Gaussians are adapted and of course the language models are adapted, especially with Rep data being important there and the system generalized well.",
                    "label": 1
                },
                {
                    "sent": "As I said, generalized to data, we had training data for, but was New Army we had.",
                    "label": 0
                },
                {
                    "sent": "Generalized to data that was new but we didn't have training data for and generalized to whole new task, which is the lecture domain for future.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work, there's lots of stuff.",
                    "label": 0
                },
                {
                    "sent": "I mean, we should fix the things we didn't have time to do properly.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, we should really use MMI map for their distant microphones models as well.",
                    "label": 0
                },
                {
                    "sent": "We should use the.",
                    "label": 0
                },
                {
                    "sent": "You know we should adapt the MLP's for the distant microphones.",
                    "label": 0
                },
                {
                    "sent": "We didn't have time to do that for I HM, you know, forget about doing you know, acoustic modeling or language modeling.",
                    "label": 0
                },
                {
                    "sent": "That's let's solve this problem with the speakers speech activity detection.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the single worst offender here and for the lecture recognition there's many things we can improve.",
                    "label": 0
                },
                {
                    "sent": "We can actually, we should actually try using the tab acoustic training data.",
                    "label": 1
                },
                {
                    "sent": "We should try getting the MDM to work better than the SDM.",
                    "label": 0
                },
                {
                    "sent": "And we should properly estimate the system parameters for this domain user.",
                    "label": 1
                },
                {
                    "sent": "Almost no brainer things to do and we we should also explore what I needed, namely use MLR type techniques to bridge the gap between the mismatch between the bandwidth on the background data and the meeting domain data.",
                    "label": 0
                },
                {
                    "sent": "The we haven't done anything to model non native speakers of American English, so we have Germans, Brits, Scots, etc.",
                    "label": 0
                },
                {
                    "sent": "They they're mismatched because all the CTS data or for the most part is.",
                    "label": 0
                },
                {
                    "sent": "Is American English so there is some data available we should try to do something like what you saw on our lawyers poster yesterday.",
                    "label": 0
                },
                {
                    "sent": "You know, do accent detection and properly handle that.",
                    "label": 0
                },
                {
                    "sent": "And then generally I think we should be more adaptive to the meeting type and the content.",
                    "label": 0
                },
                {
                    "sent": "Have a system that really just gets it right with whatever data is presented through it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remind Mike.",
                    "label": 0
                },
                {
                    "sent": "Just go back to your results line.",
                    "label": 0
                },
                {
                    "sent": "Will you show the the results for the?",
                    "label": 0
                },
                {
                    "sent": "Am I supposed to the other ones?",
                    "label": 0
                },
                {
                    "sent": "Yeah damn.",
                    "label": 0
                },
                {
                    "sent": "I was abnormally hot.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I would just again I would like to see this result.",
                    "label": 0
                },
                {
                    "sent": "If you really did true beamforming using the topology there a speaker legislation were, well that like everybody else, we didn't have the energy and smarts to do it right.",
                    "label": 0
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "Experiments on your data just because, well, I would suggest that you process the data and give us the enhanced signal and will run the recognizer.",
                    "label": 0
                },
                {
                    "sent": "Prosoft the two rooms were very different.",
                    "label": 0
                },
                {
                    "sent": "What is?",
                    "label": 0
                },
                {
                    "sent": "We got lots of questions but one question.",
                    "label": 0
                },
                {
                    "sent": "Do you have any indication on language model perplexity as you get on chill data?",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm assuming that yeah, I left those out.",
                    "label": 0
                },
                {
                    "sent": "There's a remark here that I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Anyway that's so perplexities are in the paper.",
                    "label": 0
                },
                {
                    "sent": "If you look at the table already had too many numbers, so I decided not to put them in.",
                    "label": 0
                },
                {
                    "sent": "But our perplexities on the chill data are on the order of 140, so they're harder than yours.",
                    "label": 0
                },
                {
                    "sent": "Which is why I believe that that your technique works quite a bit better for the web to make use of the web data.",
                    "label": 0
                },
                {
                    "sent": "Another question on animal off he said you're talking about regression plants trees.",
                    "label": 0
                },
                {
                    "sent": "So how many transforms do you normally use it?",
                    "label": 0
                },
                {
                    "sent": "How do you in the first pass when we do the phone number notation, we use three transforms, one for non speech and two for speech and then in the in the later passes we use typically 9 transforms.",
                    "label": 0
                },
                {
                    "sent": "But but you know, we do back off through fuel transforms if there isn't enough adaptation data in any given class.",
                    "label": 0
                },
                {
                    "sent": "But the maximum that you can get is 9 classes.",
                    "label": 0
                },
                {
                    "sent": "It is to have more transforms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know that.",
                    "label": 0
                },
                {
                    "sent": "Actually I'm sorry we use about we use only.",
                    "label": 0
                },
                {
                    "sent": "I think 6 transforms for the MDM because typically the lower error rate, the more you can afford to use detailed transforms.",
                    "label": 0
                },
                {
                    "sent": "But for the item we get at least a percent from like moving from.",
                    "label": 0
                },
                {
                    "sent": "I don't know three transforms to mine transforms.",
                    "label": 0
                },
                {
                    "sent": "Absolute.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "The STT general task.",
                    "label": 0
                }
            ]
        }
    }
}