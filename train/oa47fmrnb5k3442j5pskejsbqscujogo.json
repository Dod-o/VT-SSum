{
    "id": "oa47fmrnb5k3442j5pskejsbqscujogo",
    "title": "Large Scale Model-Based Machine Learning",
    "info": {
        "author": [
            "Tom Diethe, Amazon"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Decision Support"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_diethe_machine_learning/",
    "segmentation": [
        [
            "I'm going to talk about.",
            "This kind of high level.",
            "There's nothing, no no breaking late new research here, but I'm going to talk a bit about model based machine learning and how it can be applied to large scale problems."
        ],
        [
            "So what do we mean?",
            "Model based machine learning is quite a generic concept I think, but basically the idea is that in sort of traditional paradigm you have some kind of problem which involves data and you have a set of tools.",
            "Statistical models and such like, and you attempt to map your problem onto the standard tool set.",
            "But in model based approach you sort of say what's the model that represents my problem, and that way.",
            "In that way you can move away from black box methods to custom build statistical models, and this this is increasingly needed because we have problems that don't map into the standard tools.",
            "So some examples are, you know, we've heard a lot about this about trying to infer the quality of web pages from Clip data happens in the medical domain.",
            "If you try and infer the risk factors of diseases from medical records or something like that, or you're trying to find security risks from source code, these are things that are not obviously things where you have input vectors and output labels, and that kind of thing.",
            "But also, you know you might have some rich domain knowledge that you want to incorporate into the into the.",
            "Into the learning, and as it stands.",
            "You will have to devote some research time and that can be, you know, could be many months of someone to if the tools not available to come up with the learning well, a statistical model and then learning algorithm to solve it.",
            "And this approach obviously doesn't scale, so the goal here is to have an overriding as a single development framework which supports the creation of a wide range of bespoke models."
        ],
        [
            "And note here that I haven't actually mentioned the word Bayesian yet, and that's because it's not necessarily a Bayesian approach here, but the Bayesian framework falls very naturally into this, so they're kind of not orthogonal, but there.",
            "Sort of.",
            "On the on the same trajectory.",
            "So if we're if we're looking at this from from a Bayesian point of view, the first step obviously is to build your model.",
            "An which in the in the Bayesian world is a joint probability distribution over over all of the relevant variables.",
            "This can be represented as a graph, but doesn't have to be.",
            "The next stage is to incorporate observed data.",
            "And then the next stage is to run inference, which again in the Bayesian approach means computing distributions over desired variables and then in real time applications you iterate over the second and third of those and extend the model as required."
        ],
        [
            "So there's some obvious potential benefits to this.",
            "One is that you get a model that's optimized for your application.",
            "And.",
            "The problem is probably one of the most desirable effects is that you end up with this.",
            "You sort of open up the black, black, black box so you can.",
            "You can see directly all of the aspects of the model and this sort of links nicely with what Leonard was just saying about needing to be able to display the transparancy of the functionality of the model.",
            "The another nice desirable.",
            "Feature is that you can segregate the model from the training and inference code, so you can in that way you can.",
            "You can have newcomers who come in and they all they need to do is learn the modeling environment and they don't know.",
            "Don't need to learn about optimization and and things like that and in that way you can build up a community of model builders and bootstrap.",
            "And you can really see that model building here is a different skill from algorithm design."
        ],
        [
            "Of course.",
            "If you are in the Bayesian world, there are going to be costs involved.",
            "Classical algorithms tend to be slow.",
            "And conservative and just really intractable in these big data environments there are.",
            "There's a modern set of algorithms that are more optimistic and fast, such as loopy belief propagation, mean field approximation, and these can be competitive with fast online methods, and they really are practical for real world machine learning, but sometimes you'll have to fall back to the classical methods if.",
            "If you really care about your full, posterior and or something like that."
        ],
        [
            "So.",
            "Here I'm just going to focus on deterministic approximations, so there's two main algorithms here, expectation propagation, which is a generalized form of loopy belief propagation, an variable message passing, which is a generalized form of the mean field approximation and the basic idea is that you choose an approximating density for each of your variables, and then.",
            "You you fit the parameters of your simpler model by tuning using fixed point iterations using the KL divergent.",
            "And.",
            "That one of the nice things about this is that these deterministic methods improve with the morbet data you give them, because the posteriors start to look more Gaussian and then they fit the assumptions of the deterministic approximations better.",
            "So sampling methods don't have this feature.",
            "And so that's something that quite quite nice.",
            "So Microsoft Research have developed a tool which leverages these.",
            "Principally these two algorithms, although it does also support sampling methods.",
            "For inference in graphical models and it's called infer.net.",
            "Anne."
        ],
        [
            "So how does it work?",
            "Well, it's sort of multi stage process.",
            "You start with a by designing your model that is written down as a small dot net program so it can be written in any of the dot net languages C, sharp or or iron.",
            "Python or.",
            "And then.",
            "Usetheinfer.net compiler which which automatically compiles inference code which will solve that model.",
            "So this compiler itself is actually a chain of transforms internally, each step in the transform chain produces itself.",
            "A valid C sharp program and what it does is it automatically creates the right schedule to pass messages around your graph too.",
            "To do the.",
            "To produce the inference code so this the result and inference code can then be embedded into another application, because it just simply is a C sharp program and it can be debugged using standard tools as well.",
            "And I should say here that the data is is your observations, but it's also parameter settings and inference queries, because once you have a model you can ask multiple questions of.",
            "It's not just.",
            "You know one set of.",
            "Yes, please specify your probabilistic model is that one?",
            "Is it a propositional or is it first order?",
            "Do I have to specify something like a factor graph or is it something more general?",
            "It's it's where you can.",
            "It is basically a factor graph, but you're you're writing it down in code.",
            "I mean so in a factor graph you have, you have variables and factors, and in the in the code you define variables and factors and then plates that I'll show you an example maybe later on and that will help."
        ],
        [
            "So a couple of things to note about inference.",
            "Well, because of the nature of the algorithms, the inference process is iterative.",
            "The number of iterations is controlled manually.",
            "I mean, there's sensible defaults, but.",
            "And because these are deterministic approximations, the inference results will be deterministic, and they can only be changed through different initialization or by changing your model.",
            "It's also worth noting that it's not always perfect.",
            "The inference may not converge.",
            "Marginals might oscillate, or you can end up with improper messages, and that's just the nature of these algorithms.",
            "There's, so there's some research needed here, but there's also a little bit of skill in debugging the models."
        ],
        [
            "Anne.",
            "So another thing to note is though, although I've been focusing on custom models, if you do have a problem that Maps into one of the existing models, those can be done in infer.net as well and everything on this list here has been implemented in and a number of those will be included in the next release as well, so you can just try them out of the box.",
            "And of course, the nice thing about the model based approach is if you want to change one of the models in some way, maybe add a temporal dimension or something like that.",
            "That's easy to do, which would be quite difficult ordinarily."
        ],
        [
            "So how about scaling things up?",
            "How does this work in the large scale?",
            "Domain well, one of the main things that has been implemented in infer.net which is.",
            "Which allows.",
            "A really big speedup is something called variable sharing.",
            "So this.",
            "This basically allows you to split your inference up so that it runs on different sub models of the larger model, or it could be copies of the same model.",
            "But you share variables from the graph across those models.",
            "So typical instances might be if you have too much data to fit in memory, or maybe you have large numbers of very large messages.",
            "So an example of a large message would be a very long discrete vector, and you're switching on that vector, which happens in LDA type type models.",
            "And and that would mean that the messages may not even fit in memory, so you might want to split the model up there as well.",
            "Anne.",
            "Another obvious one is online inference, so the data might not all be available at once.",
            "And the other thing is that the system automatically creates the schedule around the graph for you, but you may want to exert some influence over that.",
            "So another way of doing that is to split the model up and then you can.",
            "You can tie things up yourself and.",
            "Alter the schedule.",
            "The other thing is that although currently parallel inference inference isn't supported.",
            "This is one way of of achieving parallel parallel inference, but because if you have a model that can be split up like that, then you can run these on in parallel basically.",
            "So the general principle is that if, say, you have models ABC... you run to convergence on Model A and then you extract the shared variable messages as outputs and then use the inputs and Model B is is the output from all of the models except for Model B and then.",
            "And so on.",
            "The other thing is that.",
            "Refering back to the large discrete vectors that you might have, and this also happens in Dirichlet models that you might have a very very long message that needs to be passed, but actually only a few of the.",
            "Elements are non zero.",
            "Anytime, well sparse.",
            "Message passing is supported so you don't need to pass the full messages around and actually it also supports approximately sparse messages.",
            "So if the the zero values aren't quite exactly 0 they can still be treated as such.",
            "Anne.",
            "And the other thing is, you can, if you have a particular factor, that you you're developing yourself, which which is can be the case that if one of the standard factors in the toolbox doesn't exist already, then you can.",
            "You can write your own customized message operators and and implement them, and those can be as fast as you like as parallel as you like.",
            "Ann, but stepping back a little bit from infer.net.",
            "So something that was brought up in the discussion yesterday was this idea of community and personalization.",
            "I think this sort of really fits well into the model based framework as well and."
        ],
        [
            "And so.",
            "I think this is quite a general recipe for efficient Bayesian inference.",
            "If you start with the Community model where you have a kind of a large data set that you can leave training as long as you like, you know it might take five hours.",
            "It might take overnight.",
            "Whatever.",
            "It doesn't really matter and then what you do is you have your individual models and you just see the priors of those individual models with posterior from the Community model and then in the in the real world for personalization all you need to do is run ADF.",
            "Payson on online learning on each of those individual models and that allows you to get fast, approximate answers.",
            "And this is something which is.",
            "Which is a repeating theme.",
            "And it should be noted that the community and individuals here doesn't necessarily mean people.",
            "These models can be.",
            "You know, this may be referring to a portfolio of experts or or something like that."
        ],
        [
            "So an example that many of you might have seen before, but you know this is a typical application, so we want to recommend some items to some users.",
            "Some users have rated some movies.",
            "How do you recommend a specific movie to a specific user?",
            "Well, one problem with this, you could just say, well, we'll do some matrix factorization and we're done.",
            "But what happens when you have a new user who hasn't given any ratings or he had new items coming in and you don't know anything about them yet?",
            "So you want to use some of the features about them all.",
            "There may be some methods to help you there as well, but also maybe you've got some some.",
            "You've got five star ratings and things like that, so maybe you your data is bit bit more complex than the standard methods will let you deal with.",
            "So the idea here is that we will learn some traits about users and traits about films, and will say that we'll just assume that they are in a common space.",
            "So what will do is will project the features that we have about the users and the features about the films into this common space, and will say that we recommend film to a user if there.",
            "Close in this space.",
            "And this was implemented as the Matchbox model by David Stern, Ralph Herrick and our good friend Tori.",
            "Anne.",
            "So how would we go about building this model?",
            "So if we start?"
        ],
        [
            "With a single user.",
            "And a single.",
            "Trait here, so we have the variable S being the trait which we're going to assume is, so I'm assuming that this has already been projected into trait space, so this is now we're just assuming that in that space it's got a Gaussian prior, so that that's the factor, and that's the variable.",
            "And we'll have a bias term as well, and then one more bit of graph notation is just this is a plate, which just means this is repeated over the number of users.",
            "So this is now one for each user.",
            "Anne."
        ],
        [
            "And then we have the same for the traits, so we can see that the bias is there is only one of those for the user.",
            "But then there's one one per trait of the.",
            "Of the trait variables."
        ],
        [
            "And then we do the same thing for items.",
            "And now we say that their affinity is is also distributed as a Gaussian with some noise by some variable.",
            "And what is that variable?",
            "Well, it's just the."
        ],
        [
            "Inner product between the items and the users which is in this factor graph represented as a product followed by some.",
            "And."
        ],
        [
            "Then the additional thing is that we had ratings on a 5 point scale.",
            "So what we do is we learn a threshold for each user.",
            "Each jump in rating because different users tend to rate things in different ways, so some people would be much more likely to give a lot of five star ratings and a lot of one star ratings and others will be much more in the middle.",
            "So this is an important part of that model as well.",
            "So a nice thing about this approach is that you can think of possible extensions.",
            "For example, you can think of recommending to groups of users.",
            "Or you could think of a dynamics model which allows the item popularity the user taste, or even the rating scale to shift in time.",
            "You could think of.",
            "Where you have positive only data.",
            "So maybe you just got likes and you don't really have negative.",
            "Ratings.",
            "And all of these can be incorporated into the model.",
            "So how would we actually run through this?",
            "Well, in the community training, the ratings are observed and the item traits T or in Ferd.",
            "And then in the personalization phase, the ratings are T&T are observed and then the user traits S are in Ferd and then in recommendation S&T are both observed and you propagate downwards and predict the ratings.",
            "So this is what it looks like.",
            "This is sort of a nice compact form of what it would look like in C sharp program.",
            "Really go into detail 'cause it's a bit hard to read, but basically you know you've got a line representing each variable in factor.",
            "The sharp lets you do the loops nicely, So what you'd actually see if you expanded all the loops out is 1 four loop for every one of the plates and the model an.",
            "These models are a fairly easy to write and fairly clear once you once you have your model defined in this way."
        ],
        [
            "So just to see if this works, just a quick demo.",
            "OK, so.",
            "Yeah, the colors are bit strange there, so this is.",
            "Kind of this specially red on the left and green green on the right.",
            "So basically I'm going to tell the system that I like some movies and I don't like others, so maybe I'll start off by saying I like Pretty Woman and we can see it's just surround estimates and we say, OK, I didn't really like Chicago.",
            "Maybe I did like Ocean's 12 and.",
            "Study.",
            "Random choices, But I didn't like Elf who would and you can see that even though after sorry for ratings there's already quite a good separation here.",
            "Another thing you could see is maybe if I choose a film that it already has quite thinks that I like quite a lot and then I choose that you'll see there's not very much change, but if I choose one that it's pretty confident that I do like and then I say I don't like it, there will be a.",
            "Big moving around and obviously the more ratings you have.",
            "The more stable it becomes.",
            "OK, so."
        ],
        [
            "So you probably read it already.",
            "This is real.",
            "I mean it's it's running on Xbox Live.",
            "These numbers are probably out of date as well.",
            "Over 50,000,000 users.",
            "I don't know how many now, but serving more than 100 million requests per day and not just movies, it's also TV programs and games.",
            "So this is, you know this is showing that this stuff is real and.",
            "Can work in the really big data world."
        ],
        [
            "So in summary, I described a general approach called model based machine learning and argued that it's well suited to large scale and online applications.",
            "I've argued that the community and personalization model is a general paradigm for efficient Bayesian inference, and I've also described the toolinfer.net which has been developed at Microsoft Research which allows you to perform Bayesian inference inference in graphical models.",
            "And you can fetch that at research.microsoft.com/infinite.",
            "OK, that's all, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "This kind of high level.",
                    "label": 0
                },
                {
                    "sent": "There's nothing, no no breaking late new research here, but I'm going to talk a bit about model based machine learning and how it can be applied to large scale problems.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we mean?",
                    "label": 0
                },
                {
                    "sent": "Model based machine learning is quite a generic concept I think, but basically the idea is that in sort of traditional paradigm you have some kind of problem which involves data and you have a set of tools.",
                    "label": 0
                },
                {
                    "sent": "Statistical models and such like, and you attempt to map your problem onto the standard tool set.",
                    "label": 0
                },
                {
                    "sent": "But in model based approach you sort of say what's the model that represents my problem, and that way.",
                    "label": 1
                },
                {
                    "sent": "In that way you can move away from black box methods to custom build statistical models, and this this is increasingly needed because we have problems that don't map into the standard tools.",
                    "label": 0
                },
                {
                    "sent": "So some examples are, you know, we've heard a lot about this about trying to infer the quality of web pages from Clip data happens in the medical domain.",
                    "label": 0
                },
                {
                    "sent": "If you try and infer the risk factors of diseases from medical records or something like that, or you're trying to find security risks from source code, these are things that are not obviously things where you have input vectors and output labels, and that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "But also, you know you might have some rich domain knowledge that you want to incorporate into the into the.",
                    "label": 0
                },
                {
                    "sent": "Into the learning, and as it stands.",
                    "label": 0
                },
                {
                    "sent": "You will have to devote some research time and that can be, you know, could be many months of someone to if the tools not available to come up with the learning well, a statistical model and then learning algorithm to solve it.",
                    "label": 0
                },
                {
                    "sent": "And this approach obviously doesn't scale, so the goal here is to have an overriding as a single development framework which supports the creation of a wide range of bespoke models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And note here that I haven't actually mentioned the word Bayesian yet, and that's because it's not necessarily a Bayesian approach here, but the Bayesian framework falls very naturally into this, so they're kind of not orthogonal, but there.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "On the on the same trajectory.",
                    "label": 0
                },
                {
                    "sent": "So if we're if we're looking at this from from a Bayesian point of view, the first step obviously is to build your model.",
                    "label": 0
                },
                {
                    "sent": "An which in the in the Bayesian world is a joint probability distribution over over all of the relevant variables.",
                    "label": 1
                },
                {
                    "sent": "This can be represented as a graph, but doesn't have to be.",
                    "label": 1
                },
                {
                    "sent": "The next stage is to incorporate observed data.",
                    "label": 0
                },
                {
                    "sent": "And then the next stage is to run inference, which again in the Bayesian approach means computing distributions over desired variables and then in real time applications you iterate over the second and third of those and extend the model as required.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's some obvious potential benefits to this.",
                    "label": 1
                },
                {
                    "sent": "One is that you get a model that's optimized for your application.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The problem is probably one of the most desirable effects is that you end up with this.",
                    "label": 0
                },
                {
                    "sent": "You sort of open up the black, black, black box so you can.",
                    "label": 0
                },
                {
                    "sent": "You can see directly all of the aspects of the model and this sort of links nicely with what Leonard was just saying about needing to be able to display the transparancy of the functionality of the model.",
                    "label": 0
                },
                {
                    "sent": "The another nice desirable.",
                    "label": 1
                },
                {
                    "sent": "Feature is that you can segregate the model from the training and inference code, so you can in that way you can.",
                    "label": 0
                },
                {
                    "sent": "You can have newcomers who come in and they all they need to do is learn the modeling environment and they don't know.",
                    "label": 0
                },
                {
                    "sent": "Don't need to learn about optimization and and things like that and in that way you can build up a community of model builders and bootstrap.",
                    "label": 0
                },
                {
                    "sent": "And you can really see that model building here is a different skill from algorithm design.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "If you are in the Bayesian world, there are going to be costs involved.",
                    "label": 0
                },
                {
                    "sent": "Classical algorithms tend to be slow.",
                    "label": 1
                },
                {
                    "sent": "And conservative and just really intractable in these big data environments there are.",
                    "label": 0
                },
                {
                    "sent": "There's a modern set of algorithms that are more optimistic and fast, such as loopy belief propagation, mean field approximation, and these can be competitive with fast online methods, and they really are practical for real world machine learning, but sometimes you'll have to fall back to the classical methods if.",
                    "label": 1
                },
                {
                    "sent": "If you really care about your full, posterior and or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here I'm just going to focus on deterministic approximations, so there's two main algorithms here, expectation propagation, which is a generalized form of loopy belief propagation, an variable message passing, which is a generalized form of the mean field approximation and the basic idea is that you choose an approximating density for each of your variables, and then.",
                    "label": 0
                },
                {
                    "sent": "You you fit the parameters of your simpler model by tuning using fixed point iterations using the KL divergent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That one of the nice things about this is that these deterministic methods improve with the morbet data you give them, because the posteriors start to look more Gaussian and then they fit the assumptions of the deterministic approximations better.",
                    "label": 0
                },
                {
                    "sent": "So sampling methods don't have this feature.",
                    "label": 0
                },
                {
                    "sent": "And so that's something that quite quite nice.",
                    "label": 0
                },
                {
                    "sent": "So Microsoft Research have developed a tool which leverages these.",
                    "label": 1
                },
                {
                    "sent": "Principally these two algorithms, although it does also support sampling methods.",
                    "label": 1
                },
                {
                    "sent": "For inference in graphical models and it's called infer.net.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "Well, it's sort of multi stage process.",
                    "label": 0
                },
                {
                    "sent": "You start with a by designing your model that is written down as a small dot net program so it can be written in any of the dot net languages C, sharp or or iron.",
                    "label": 1
                },
                {
                    "sent": "Python or.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Usetheinfer.net compiler which which automatically compiles inference code which will solve that model.",
                    "label": 0
                },
                {
                    "sent": "So this compiler itself is actually a chain of transforms internally, each step in the transform chain produces itself.",
                    "label": 0
                },
                {
                    "sent": "A valid C sharp program and what it does is it automatically creates the right schedule to pass messages around your graph too.",
                    "label": 0
                },
                {
                    "sent": "To do the.",
                    "label": 0
                },
                {
                    "sent": "To produce the inference code so this the result and inference code can then be embedded into another application, because it just simply is a C sharp program and it can be debugged using standard tools as well.",
                    "label": 0
                },
                {
                    "sent": "And I should say here that the data is is your observations, but it's also parameter settings and inference queries, because once you have a model you can ask multiple questions of.",
                    "label": 0
                },
                {
                    "sent": "It's not just.",
                    "label": 0
                },
                {
                    "sent": "You know one set of.",
                    "label": 0
                },
                {
                    "sent": "Yes, please specify your probabilistic model is that one?",
                    "label": 1
                },
                {
                    "sent": "Is it a propositional or is it first order?",
                    "label": 0
                },
                {
                    "sent": "Do I have to specify something like a factor graph or is it something more general?",
                    "label": 0
                },
                {
                    "sent": "It's it's where you can.",
                    "label": 0
                },
                {
                    "sent": "It is basically a factor graph, but you're you're writing it down in code.",
                    "label": 0
                },
                {
                    "sent": "I mean so in a factor graph you have, you have variables and factors, and in the in the code you define variables and factors and then plates that I'll show you an example maybe later on and that will help.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a couple of things to note about inference.",
                    "label": 0
                },
                {
                    "sent": "Well, because of the nature of the algorithms, the inference process is iterative.",
                    "label": 1
                },
                {
                    "sent": "The number of iterations is controlled manually.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's sensible defaults, but.",
                    "label": 0
                },
                {
                    "sent": "And because these are deterministic approximations, the inference results will be deterministic, and they can only be changed through different initialization or by changing your model.",
                    "label": 0
                },
                {
                    "sent": "It's also worth noting that it's not always perfect.",
                    "label": 0
                },
                {
                    "sent": "The inference may not converge.",
                    "label": 1
                },
                {
                    "sent": "Marginals might oscillate, or you can end up with improper messages, and that's just the nature of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's, so there's some research needed here, but there's also a little bit of skill in debugging the models.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So another thing to note is though, although I've been focusing on custom models, if you do have a problem that Maps into one of the existing models, those can be done in infer.net as well and everything on this list here has been implemented in and a number of those will be included in the next release as well, so you can just try them out of the box.",
                    "label": 0
                },
                {
                    "sent": "And of course, the nice thing about the model based approach is if you want to change one of the models in some way, maybe add a temporal dimension or something like that.",
                    "label": 0
                },
                {
                    "sent": "That's easy to do, which would be quite difficult ordinarily.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how about scaling things up?",
                    "label": 1
                },
                {
                    "sent": "How does this work in the large scale?",
                    "label": 0
                },
                {
                    "sent": "Domain well, one of the main things that has been implemented in infer.net which is.",
                    "label": 0
                },
                {
                    "sent": "Which allows.",
                    "label": 1
                },
                {
                    "sent": "A really big speedup is something called variable sharing.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This basically allows you to split your inference up so that it runs on different sub models of the larger model, or it could be copies of the same model.",
                    "label": 0
                },
                {
                    "sent": "But you share variables from the graph across those models.",
                    "label": 0
                },
                {
                    "sent": "So typical instances might be if you have too much data to fit in memory, or maybe you have large numbers of very large messages.",
                    "label": 0
                },
                {
                    "sent": "So an example of a large message would be a very long discrete vector, and you're switching on that vector, which happens in LDA type type models.",
                    "label": 0
                },
                {
                    "sent": "And and that would mean that the messages may not even fit in memory, so you might want to split the model up there as well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Another obvious one is online inference, so the data might not all be available at once.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that the system automatically creates the schedule around the graph for you, but you may want to exert some influence over that.",
                    "label": 0
                },
                {
                    "sent": "So another way of doing that is to split the model up and then you can.",
                    "label": 0
                },
                {
                    "sent": "You can tie things up yourself and.",
                    "label": 1
                },
                {
                    "sent": "Alter the schedule.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that although currently parallel inference inference isn't supported.",
                    "label": 0
                },
                {
                    "sent": "This is one way of of achieving parallel parallel inference, but because if you have a model that can be split up like that, then you can run these on in parallel basically.",
                    "label": 0
                },
                {
                    "sent": "So the general principle is that if, say, you have models ABC... you run to convergence on Model A and then you extract the shared variable messages as outputs and then use the inputs and Model B is is the output from all of the models except for Model B and then.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that.",
                    "label": 0
                },
                {
                    "sent": "Refering back to the large discrete vectors that you might have, and this also happens in Dirichlet models that you might have a very very long message that needs to be passed, but actually only a few of the.",
                    "label": 0
                },
                {
                    "sent": "Elements are non zero.",
                    "label": 0
                },
                {
                    "sent": "Anytime, well sparse.",
                    "label": 0
                },
                {
                    "sent": "Message passing is supported so you don't need to pass the full messages around and actually it also supports approximately sparse messages.",
                    "label": 0
                },
                {
                    "sent": "So if the the zero values aren't quite exactly 0 they can still be treated as such.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is, you can, if you have a particular factor, that you you're developing yourself, which which is can be the case that if one of the standard factors in the toolbox doesn't exist already, then you can.",
                    "label": 0
                },
                {
                    "sent": "You can write your own customized message operators and and implement them, and those can be as fast as you like as parallel as you like.",
                    "label": 0
                },
                {
                    "sent": "Ann, but stepping back a little bit from infer.net.",
                    "label": 0
                },
                {
                    "sent": "So something that was brought up in the discussion yesterday was this idea of community and personalization.",
                    "label": 0
                },
                {
                    "sent": "I think this sort of really fits well into the model based framework as well and.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I think this is quite a general recipe for efficient Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "If you start with the Community model where you have a kind of a large data set that you can leave training as long as you like, you know it might take five hours.",
                    "label": 1
                },
                {
                    "sent": "It might take overnight.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter and then what you do is you have your individual models and you just see the priors of those individual models with posterior from the Community model and then in the in the real world for personalization all you need to do is run ADF.",
                    "label": 0
                },
                {
                    "sent": "Payson on online learning on each of those individual models and that allows you to get fast, approximate answers.",
                    "label": 1
                },
                {
                    "sent": "And this is something which is.",
                    "label": 0
                },
                {
                    "sent": "Which is a repeating theme.",
                    "label": 0
                },
                {
                    "sent": "And it should be noted that the community and individuals here doesn't necessarily mean people.",
                    "label": 0
                },
                {
                    "sent": "These models can be.",
                    "label": 0
                },
                {
                    "sent": "You know, this may be referring to a portfolio of experts or or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an example that many of you might have seen before, but you know this is a typical application, so we want to recommend some items to some users.",
                    "label": 0
                },
                {
                    "sent": "Some users have rated some movies.",
                    "label": 0
                },
                {
                    "sent": "How do you recommend a specific movie to a specific user?",
                    "label": 0
                },
                {
                    "sent": "Well, one problem with this, you could just say, well, we'll do some matrix factorization and we're done.",
                    "label": 0
                },
                {
                    "sent": "But what happens when you have a new user who hasn't given any ratings or he had new items coming in and you don't know anything about them yet?",
                    "label": 0
                },
                {
                    "sent": "So you want to use some of the features about them all.",
                    "label": 0
                },
                {
                    "sent": "There may be some methods to help you there as well, but also maybe you've got some some.",
                    "label": 0
                },
                {
                    "sent": "You've got five star ratings and things like that, so maybe you your data is bit bit more complex than the standard methods will let you deal with.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we will learn some traits about users and traits about films, and will say that we'll just assume that they are in a common space.",
                    "label": 0
                },
                {
                    "sent": "So what will do is will project the features that we have about the users and the features about the films into this common space, and will say that we recommend film to a user if there.",
                    "label": 0
                },
                {
                    "sent": "Close in this space.",
                    "label": 0
                },
                {
                    "sent": "And this was implemented as the Matchbox model by David Stern, Ralph Herrick and our good friend Tori.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So how would we go about building this model?",
                    "label": 0
                },
                {
                    "sent": "So if we start?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a single user.",
                    "label": 0
                },
                {
                    "sent": "And a single.",
                    "label": 0
                },
                {
                    "sent": "Trait here, so we have the variable S being the trait which we're going to assume is, so I'm assuming that this has already been projected into trait space, so this is now we're just assuming that in that space it's got a Gaussian prior, so that that's the factor, and that's the variable.",
                    "label": 0
                },
                {
                    "sent": "And we'll have a bias term as well, and then one more bit of graph notation is just this is a plate, which just means this is repeated over the number of users.",
                    "label": 0
                },
                {
                    "sent": "So this is now one for each user.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have the same for the traits, so we can see that the bias is there is only one of those for the user.",
                    "label": 0
                },
                {
                    "sent": "But then there's one one per trait of the.",
                    "label": 0
                },
                {
                    "sent": "Of the trait variables.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we do the same thing for items.",
                    "label": 0
                },
                {
                    "sent": "And now we say that their affinity is is also distributed as a Gaussian with some noise by some variable.",
                    "label": 0
                },
                {
                    "sent": "And what is that variable?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inner product between the items and the users which is in this factor graph represented as a product followed by some.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the additional thing is that we had ratings on a 5 point scale.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we learn a threshold for each user.",
                    "label": 0
                },
                {
                    "sent": "Each jump in rating because different users tend to rate things in different ways, so some people would be much more likely to give a lot of five star ratings and a lot of one star ratings and others will be much more in the middle.",
                    "label": 0
                },
                {
                    "sent": "So this is an important part of that model as well.",
                    "label": 0
                },
                {
                    "sent": "So a nice thing about this approach is that you can think of possible extensions.",
                    "label": 0
                },
                {
                    "sent": "For example, you can think of recommending to groups of users.",
                    "label": 0
                },
                {
                    "sent": "Or you could think of a dynamics model which allows the item popularity the user taste, or even the rating scale to shift in time.",
                    "label": 0
                },
                {
                    "sent": "You could think of.",
                    "label": 0
                },
                {
                    "sent": "Where you have positive only data.",
                    "label": 0
                },
                {
                    "sent": "So maybe you just got likes and you don't really have negative.",
                    "label": 0
                },
                {
                    "sent": "Ratings.",
                    "label": 0
                },
                {
                    "sent": "And all of these can be incorporated into the model.",
                    "label": 1
                },
                {
                    "sent": "So how would we actually run through this?",
                    "label": 0
                },
                {
                    "sent": "Well, in the community training, the ratings are observed and the item traits T or in Ferd.",
                    "label": 1
                },
                {
                    "sent": "And then in the personalization phase, the ratings are T&T are observed and then the user traits S are in Ferd and then in recommendation S&T are both observed and you propagate downwards and predict the ratings.",
                    "label": 0
                },
                {
                    "sent": "So this is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a nice compact form of what it would look like in C sharp program.",
                    "label": 0
                },
                {
                    "sent": "Really go into detail 'cause it's a bit hard to read, but basically you know you've got a line representing each variable in factor.",
                    "label": 0
                },
                {
                    "sent": "The sharp lets you do the loops nicely, So what you'd actually see if you expanded all the loops out is 1 four loop for every one of the plates and the model an.",
                    "label": 0
                },
                {
                    "sent": "These models are a fairly easy to write and fairly clear once you once you have your model defined in this way.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to see if this works, just a quick demo.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the colors are bit strange there, so this is.",
                    "label": 0
                },
                {
                    "sent": "Kind of this specially red on the left and green green on the right.",
                    "label": 0
                },
                {
                    "sent": "So basically I'm going to tell the system that I like some movies and I don't like others, so maybe I'll start off by saying I like Pretty Woman and we can see it's just surround estimates and we say, OK, I didn't really like Chicago.",
                    "label": 0
                },
                {
                    "sent": "Maybe I did like Ocean's 12 and.",
                    "label": 0
                },
                {
                    "sent": "Study.",
                    "label": 0
                },
                {
                    "sent": "Random choices, But I didn't like Elf who would and you can see that even though after sorry for ratings there's already quite a good separation here.",
                    "label": 0
                },
                {
                    "sent": "Another thing you could see is maybe if I choose a film that it already has quite thinks that I like quite a lot and then I choose that you'll see there's not very much change, but if I choose one that it's pretty confident that I do like and then I say I don't like it, there will be a.",
                    "label": 0
                },
                {
                    "sent": "Big moving around and obviously the more ratings you have.",
                    "label": 0
                },
                {
                    "sent": "The more stable it becomes.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you probably read it already.",
                    "label": 0
                },
                {
                    "sent": "This is real.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's running on Xbox Live.",
                    "label": 0
                },
                {
                    "sent": "These numbers are probably out of date as well.",
                    "label": 0
                },
                {
                    "sent": "Over 50,000,000 users.",
                    "label": 0
                },
                {
                    "sent": "I don't know how many now, but serving more than 100 million requests per day and not just movies, it's also TV programs and games.",
                    "label": 1
                },
                {
                    "sent": "So this is, you know this is showing that this stuff is real and.",
                    "label": 0
                },
                {
                    "sent": "Can work in the really big data world.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, I described a general approach called model based machine learning and argued that it's well suited to large scale and online applications.",
                    "label": 1
                },
                {
                    "sent": "I've argued that the community and personalization model is a general paradigm for efficient Bayesian inference, and I've also described the toolinfer.net which has been developed at Microsoft Research which allows you to perform Bayesian inference inference in graphical models.",
                    "label": 0
                },
                {
                    "sent": "And you can fetch that at research.microsoft.com/infinite.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all, thanks.",
                    "label": 0
                }
            ]
        }
    }
}