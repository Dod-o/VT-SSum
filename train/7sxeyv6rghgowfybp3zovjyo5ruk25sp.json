{
    "id": "7sxeyv6rghgowfybp3zovjyo5ruk25sp",
    "title": "On oblique random forests",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Bjoern H. Menze, CALVIN, Computer Vision laboratory, ETH Zurich"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_menze_forests/",
    "segmentation": [
        [
            "Thank you very much, good afternoon.",
            "Our paper has two objectives, so it's on a bleak random forest.",
            "First, first objective is to revisit this idea.",
            "So if you look into Brandon's original paper from 10 years ago, then you find two different random forests.",
            "He proposes.",
            "The one is the one we are using with standard univariate decision trees.",
            "Just testing single features and the other one he proposed was a random forest which you stop leak splits and these obsolete object splits had random hyperplanes coefficients.",
            "And then using the East oblique projections in space for the node splits.",
            "So revisiting this idea is the one thing, and the other thing is what we actually propose here now is not to use these random splits in this random forest framework, not these random coefficients, but to use learn, not model node models in in the random splits, the kind of discriminatively near North models instead of this random split proposed by by men, and this is.",
            "This is kind of the contribution of this paper."
        ],
        [
            "Now, background, but well, I'm interested in like application wise is the among others the classification of spectral data.",
            "And here on the left hand side you see a few examples.",
            "So infrared datasets and NMR data sets on the left hand side.",
            "The actual datasets with spectral channels from left to right and different residents, lines and absorption lines and then the same data set normalized to unit variance for every channel and then two lines for two different classes.",
            "So what you?",
            "Get out of this is that the spectral data is very high dimensional, so we're talking here about easily a few 1000 feature entries for every spectrum.",
            "We have relatively few samples, typically because there is a biomedical background often and the very property of these data is that features are correlated.",
            "So if you look at this here, then neighboring channels often have a very similar value and as a result of this this correlation.",
            "We also have noise which is.",
            "Strongly correlated between channels, so just neighboring channels, but also correlation over over features which are far apart just from the kind of acquisition process which is taking part here.",
            "So this is spectral data and spectrometry does a lot of this analysis and analyzing these data, but the same kind of data with the same properties in the same problems, the same noise."
        ],
        [
            "In particular, is something which shows up very often.",
            "Of course, other kind of diagnostic tests where you have biomedical features and these microarray data, for example, which correlate and then all kinds of image data.",
            "What you see here is multi spectral data, lower left hand side.",
            "Find the archaeological site in this data set.",
            "This is something where you also classify Spectra in the 2D context recognition and detection task.",
            "So the talk yesterday with Kinect the recognition problem there.",
            "This is this is something which is approached with decision trees and and.",
            "Um data which is very similar to to the kind of spectral data I've shown you, and also medical task where you really have 3D datasets and where you want to find.",
            "In this case we worked on Ms lesion segmentation and use random forest.",
            "In this image computer vision setting to actually classify.",
            "Some binary classification problems.",
            "So whenever I speak about spectral data, this is."
        ],
        [
            "The background.",
            "What's wrong with this?",
            "Well?",
            "The problem with spectral data is what you see on the left hand side.",
            "So top left is just a two mixtures and the tree learned on or to split these two classes and what you see here in the lower left side is the kind of marginal distribution rare uses seek for the best split at every node.",
            "The problem now is if your features are correlated and and this is something we have in all these data then your data look looks like like the example on the right hand side.",
            "So this is a little bit artificially correlated, but what you see is that the split is very difficult.",
            "To actually find, because the marginal distributions are totally overlapping in the in the lower lower box you might see that an and we get really complex trees.",
            "No, I already mentioned it.",
            "The solution here and and you see it from from from this kind of little drawing is that you just use an oblique split right?",
            "So if you split model it has this degree of freedom to spit as as you wish in this space and then it can easily adapt and trees from public splits have a long tradition.",
            "So in the computer science community it's it's the early 90s that they were proposed, at least in statistics it's even even longer, late 1970s and 1980s.",
            "They were quite popular problems is that?",
            "You have to optimize them.",
            "There are many, many more parameters to optimize for every split model.",
            "Oftentimes people use global optimization approaches here, and regularization is as a consequence, relatively difficult too.",
            "Now using trees with oblique splits as baseline."
        ],
        [
            "For rent in forest is something which has been done, has been used and as I mentioned, the original paper about by man was was already mentioning these random multivariate splits and the observed extremely good results on a couple of datasets.",
            "There is a more recent work on up on rotation forests, So what they do is they just use the first item component of the data in which rests in a note, learn the the direction first direction and use this for splitting and again good qualification results.",
            "And also, um, decision Forest, where they actually use these these original these all.",
            "Big random forests, but kind of optimize every every tree individually, but again kind of difficult to optimize, but again good results in that case too.",
            "No, what we propose to use here is now."
        ],
        [
            "2."
        ],
        [
            "Learn the forest.",
            "As I mentioned, Rich is using standard, simple, standard, discriminative model in the forest.",
            "So not not this complex global optimization, it's just a recursive linear model split.",
            "We use as an oblique tree as a base learner.",
            "In this case, recursive binary splits at node M in the upper right hand side you see the split.",
            "What you have to define are the coefficients.",
            "Peter in this model and threshold, the coefficient are learned in our case from rich regression and we use rich this constraint.",
            "For two reasons, more less.",
            "First, if there is correlation than between features, then this this rich penalty is is is necessary and the other thing is that otherwise you have these.",
            "These noisy high high entries for your regression and the other reason is that if you split your data and then after a few splits you just have very few data samples in your note and maybe you're still in the high dimensional space so you can just roll deeper trees.",
            "If you have some regularization.",
            "Rich regression for finding the split direction you see the objective and kind of the the normalization and the objective.",
            "So what Rich is actually choosing as a direction is something between the task optimal direction, which is the LDA direction.",
            "So kind of in highest correlation with the labels red and green here, and something which is more in line with data variance.",
            "And this is the PCA and I'm afraid you might not see the directions.",
            "For the rich, so it's it's summer here.",
            "This aligns here, which depend on your rich coefficient Lambda.",
            "You might choose.",
            "So what you do is you can test quite a few of different directions using this rich model.",
            "Project your data into this into the subspace and then just find the best threshold as you would do with any other split model as well.",
            "So this is this is at the heart of the optic model, treason, public random force.",
            "We propose to use this kind of model."
        ],
        [
            "And the rest is essentially the same, so this is not exactly pseudocode, but just to to make very short outline.",
            "There are the two parameters of the random forest, the number of trees and the subspace dimensionality, the kind of regular seek for the best split.",
            "Then there is bootstrapping element for every single tree.",
            "Regrow this tree we have this subspace dimensionality.",
            "In every subspace we learn the best direction we evaluate different parameters of Lambda.",
            "On exponential scale and evaluate the best fit on the out of box sample out of back samples which are available at that node and grow as far as the best possible don't and we don't prove.",
            "In prediction reviews, all the samples based on the average.",
            "So and This is why we just get around of this difficulty, not the trees of pruning and adapting the complexity, the complexity."
        ],
        [
            "Tree.",
            "Now the scaling behavior for this is surprisingly good.",
            "Of course it scales with the size of the forest.",
            "The overall scaling is the same, so that kind of number of nodes is the same for both trees.",
            "This done at random forest and the oblique random forest, so it's a number of levels which is log N 2 N and probably the random forest is more like N and lock and equally balanced option is more something with the more complex split model.",
            "The complexity of the note evaluations, the splitting and every at every node.",
            "Is the feature ranking cost for the standard random forest with the univariate trees and for the oblique random forest, it's the cost of a regression and this is P squared with P dimensionality of your subspace times N. And this is for small keys or high ends.",
            "Even more efficient than the standard random forest.",
            "So there there is a you get off with a fairly good random forest and fairly efficient.",
            "Surprisingly of course, prediction is a bit more expensive because you have.",
            "This is.",
            "Many features you have to evaluate, not just the single."
        ],
        [
            "Parametrisation, the two assemble parameter assembled size.",
            "So what you see here Upper Rose classification error for a few datasets as a function of trees in the forest for standard random forests with uni, very trees and few different random forests and you see that it decreases quite well and we were not really able to overtrain here.",
            "So we chose a couple of 100 trees for most of our later experiments the subspace dimensionality is a bit more critical, but we found that like that.",
            "He fold as used for standard random forest like square root of P Das sufficiently well.",
            "So probably I would recommend to optimize that if necessary.",
            "But in our experiment we found it.",
            "It's kind of well behaving and we just went with that."
        ],
        [
            "No X."
        ],
        [
            "Garments.",
            "We compared.",
            "These classifiers on a number of datasets and we chose 40 binary classification tasks, 10 of them raw datasets.",
            "Which features were factors 10 had just standard numerical features.",
            "So just what you download from the UCI repository for example, and then twenty data set where the spectral data set I was mentioning at the beginning.",
            "So kind of the data which has this.",
            "This property with correlation between features and so on.",
            "We compare different classifiers and this was this oblique random forest with random splits.",
            "So the thing Bryant.",
            "Post and then the one we propose here with and without optimization of this rich penalty.",
            "So it would be this LDA version without penalty and rich with the optimization.",
            "Choose the default parameters I mentioned.",
            "We compared it against a few other classifiers like support vector machine, Kenyeres neighbors cut random forests.",
            "Random forest with random splits, Adaboost and optimized parameters for those an compared accuracy and error."
        ],
        [
            "Under the curse.",
            "Now these are the results and I wouldn't expect that even in the 3rd row you you get what's here.",
            "So what you have is the different datasets from top to bottom, from left to right.",
            "The different classifiers for the two different measures an what you may or may not see.",
            "There are a number of the kind of the best.",
            "The best results are underlined, so this is kind of the classifier, the combination, the classifier on the given data set which perform best on average and then in bold type are the kind of classifiers.",
            "Rich on this data set did not differ significantly from the result of this one best classifier and significantly means here.",
            "We just tested the resampled distribution on our from our cross validation.",
            "So kind of everything which is here underlined or bold is either the best method or incomparable to the best, and this is something you can count."
        ],
        [
            "And and I hope you can read this now.",
            "So this is.",
            "This is the ranking for the different methods on the three different data types.",
            "So this factorial data the first 10 datasets, the numerical data and then the spectral data for the two different measures and what you see here is that for factorial data standard random forest with univariate trees, another boost with univariate trees do very well lead the ranking while on all the other data sets the random forest with oblique splits.",
            "Are definitely preferable, so and this is kind of the main result that the differences even bigger on the spectral data then on this on the standard numerical data.",
            "But even there and also this learn split performs better than the random split prime and proposed the random objects, but Roman put post.",
            "In his original paper, so you just see it once on this list at all."
        ],
        [
            "No, with this results we were interested in.",
            "Getting some learning about some of the properties of this random forest so and."
        ],
        [
            "What we did in the first Test was just to look at the hyperplanes.",
            "What you see here is the different hyperplanes and you might be getting that one decision tree is kind of overtraining easily at the lower left example.",
            "So what this is is this Dennis mixture mixture data set where you have a couple of Gaussian mixtures for two classes and sub samples.",
            "You train on those samples which are green and red dot.",
            "Please excuse if you just see shades of Gray here and then you classify the whole space and compare this with the base optimal.",
            "Specification, which is the dashed line in there and you see single trees don't do well, and the random forest do much better.",
            "But the main interesting feature here is that this blocky structure of your standard decision tree is nothing you're resolved by pulling many and many, many trees.",
            "So kind of both random forest and Adaboost do not resolve this blocky structure of your base learner.",
            "And here is something where the assamble.",
            "Of the more complex, more adaptive optic random forest and random trees just just benefit much from from from this degree of freedom and you get something which is very close to the base optimal decision boundary, much more like the RBF kernel support vector machine, then then then the random forest or the decision trees from the Senate Random Force."
        ],
        [
            "Now this is the topology.",
            "Two more tests on the datasets from this comparison and we can be kind of group the data into data into into three classes, factorial data, numerical data and spectral data.",
            "We compared where does this difference come from?",
            "What is it we find?",
            "And I don't go into the detail, but we find that the variance is more or less the same for all random forests, and we see that the difference in accuracy we had observed that factorial data standard random forests are good and a big random forest on all the other is mostly a difference in bias."
        ],
        [
            "And the same picture if you compare this, this random decision tree or bleak random forest with the random the random forest with random decisions and the one with the learn decisions against each other.",
            "We have kind of similar variants, but the biases.",
            "Again what makes the difference in accuracy by the end of the day."
        ],
        [
            "No right at the beginning I gave you this little picture of this correlated spectral data.",
            "Without further details.",
            "What you see here is actual spectral data and the correlation between features and the first row just plotting the for two classes.",
            "Different feature or bivariate histograms in those spaces.",
            "And this is the actual correlation we observe.",
            "The kind of model this artificially, and this is what you see in the second race.",
            "2nd, 2nd row and then compare the standard random forest with.",
            "Orthogonal splits and the other of the Grand Forest with our splits and increase this degree of correlation.",
            "Student two to nearly maximum correlation will just see here in the lower right side is that the only grand Forest completely ignores it, or nearly completely ignores it.",
            "This is this green line performance stays the same while the standard random forest with universities just just just goes towards random classification at some point."
        ],
        [
            "Now this is kind of this is was just running back to the.",
            "The first lights ran for us to have a few tools which make them so attractive."
        ],
        [
            "And this is for one the importance measures.",
            "So you have the feature importance, which is this gene importance.",
            "This algorithmically defined gene importance that just a statistic of what features have been chosen and how much was the genie decrease.",
            "And there's also this permutation importance, both proposed by prime and we just had a look and try to find something something similar for our model based on public random forests with model splits and what we chose was to use an over on analysis of variance in every split and record those.",
            "Features which which which are deemed important at a certain split and just to to record the statistic of that and what you see on the left hand side is exactly this example, spectral data in the top row.",
            "If you just do a univariate test, this is kind of the importance in the 2nd row for the different channels, the higher the more important than the Gini importance and the permutation importances typically empirically shown that it correlates quite well with this gene importance and then we have this.",
            "Random forest and problems we propose here and what you see is that there are a few additional features and the general the kind of discrimination between of different features in this spectrum.",
            "You see these different peaks is much, much harder for the oblique random forest we propose here."
        ],
        [
            "Now featuring points, the other thing is sample proximity ready.",
            "Just push samples down the trees and counter.",
            "Often two samples in the test end up in the same terminal note and to keep your statistic of that.",
            "And this gives you a proximity matrix and this is the proximity matrix for one of our spectral datasets."
        ],
        [
            "What we get here if we compare this for our public random forest and the standard random forest is that the and visualize this proximity matrix in eigen space like multi dimensional scaling very much.",
            "Then you see that there is there is lots of structure in the lower hand side.",
            "Here of this plot.",
            "So kind of lower triangular and this is the standard random forest.",
            "This is something which comes from the topology which is imposes arbitrary splits on the data which are not.",
            "Given by the data, but just by the ability of how the decision tree can actually split and we don't have this problem for our public random forest, so kind of this is just just a very nice feature.",
            "If sample proximity is something you're interested in visualizing this then then the random forest sample proximity gives you something which is much closer to the data.",
            "And 2nd."
        ],
        [
            "Very last slide on.",
            "That is if you.",
            "If you look into the variance explained for the different eigenspaces then you would see that in the first eigen space the oblique random forest experience much more of the variance of this sample proximity.",
            "So less less structure in higher dimensions, which is which is a good thing to have here on many different datasets including non spectral datasets."
        ],
        [
            "So in conclusion, we propose an aplique random forest with recursive linear model splits.",
            "We find that it's sufficiently.",
            "It is a sufficiently good scaling behavior, actually, and that we observe that it outperforms on any data set except on data rich as factors as features.",
            "The standard random forest from univariate trees, the model split performs even better than the standard one.",
            "The random split we which has been out there for awhile an under some conditions this constraint is important to advantage this is this.",
            "Topology, so remember the this this example on the right hand side, mostly due to unbiased splits in the note and we also able to extract feature importance and proximity measures as in the original random forest which have a number of interesting properties here as well.",
            "Overall we.",
            "Use more complex not models, but get similar trees and by the end of the day better random forest.",
            "This is what we observe, and if you want to test this as well, we just upload it in a package which implements or which has the random forest we used in all these comparison and it should be available on the standard repository.",
            "Thank you.",
            "So far we haven't really worked on that yet.",
            "You know.",
            "Well, probably it works, but we haven't really done experiments or something like that.",
            "So how would you need to change the models in the splits?",
            "So what you would have to do is you would have to use regression trees by the end of the day, which means you just.",
            "You don't evaluate the discrimination between classes, but kind of the variance standard approach of of getting univariate split models here as well, and what you can do in the terminal node is either regret, do the regression keeping the terminal regression node or keeping the local local linear approximation or local constant approximation in the terminal node.",
            "So the challenging part is displayed.",
            "Because here the suite is a linear discriminants.",
            "Quite straightforward so, but everything where you how you?",
            "Well the the generalization of a of a classification tree to a regression tree is by exchanging the.",
            "Evaluation function, and this is something you can do 1 by 1 here as well.",
            "We haven't done it yet, but there is no principle problem to do that.",
            "So in this statistical learning poke, you also see these graphs where you.",
            "When you take many axis parallel splits, you can easily.",
            "Approach aplique split and the more splits you take.",
            "The closer you get.",
            "So I would have expected if you take more random forests at higher number of random forests, you would approach the performance that you get with your flick splits.",
            "So the first question is this something that you observe so and I have a related question.",
            "This is.",
            "Uh.",
            "OK, I forgot the related question, so you're right.",
            "So if you make it the really deep deep set of univariate split, you should be able to split your distribution into two parts.",
            "That's true, and this is what you see in this little pictogram or this figure from the first lights in the upper right high half, right?",
            "So by the end of the day, you have a decision tree univariate decision tree.",
            "There, even if it's kind of complex to split both.",
            "Anne, what we see here is that it's just not optimal to to use those, because by the end of the day, this topology is still present, present and what you see is that the strength of the ability to split both classes is much lower for the univariate splits.",
            "So we all the trees are like growing into two to the you know Terminal 2 to pure notes and increasing the size of the forest doesn't change much by the end of the day anymore.",
            "So kind of the two parameters we have here, we are both at the limit of them.",
            "OK so I like very much this picture where you show the Queen incense.",
            "The comparison between the surface separation surface of this random forest against the base of the base of the mole surface, which is also there.",
            "But"
        ],
        [
            "Yep.",
            "But I think there are a couple of things here, right?",
            "One is whether given a large enough sample, you exactly recover the base surface, right?",
            "OK, which has consistency and the other one is how fast.",
            "Are you Richard?",
            "How fast it converge that so both require direct equipment theoretical results in a sense, so I wonder if you have something already in that direction.",
            "So unfortunately there are not too many theoretical results out there for random forests, bryman's original random forests, and we haven't really dared to look into the oblique random forests here yet, so we don't have theoretical results on that, but it would be nice to have.",
            "Definitely yes."
        ],
        [
            "Maybe it is a stupid question.",
            "Just quit.",
            "Another nice bad.",
            "Is it so that in his last 10 years we didn't have paper with a similar ID?",
            "Because it seems I like this idea.",
            "What you have presented, but for me it's so natural, and I suppose there were times I use similar tricks, maybe not exactly for the random for us, but.",
            "Just to have the model for forgiven, split, and even if the spread is more or less random, it's quite obvious.",
            "And another question is like it's the former question.",
            "The question is whether we are not losing something because in the random in the primer paper, everything was based on randomness, and in that way he was proving at least the consistency.",
            "And here we are limited, though this is random.",
            "So in in some way you already gave the answer with your second questions to your first.",
            "So there are quite a few papers.",
            "There are a few other 3, two or three I mentioned here, but most of the other works which looked into the different options and the variations of all this try to make it even more random and even more extremely random, randomizing even more at note splits and in any kind of.",
            "Rae Ann and try to to get a better random forest with the classification result out of it, and this is a bit while we observe here is that you can do too much, right?",
            "So if you have a totally random decision and you have many totally random decision by the end of the day, you have still a maybe not random decision but not optimal decision.",
            "And we see here that kind of balancing off this general randomization of the random forest bagging subspace search.",
            "Or subspace subspace is is is a good randomization but but kind of inside it works better if you have strong note models which is also something you have in probabilistic boosting trees right?",
            "So this is also kind of an idea where you use very strong good note models and by the end of the day get a better overall classifier.",
            "So in some way.",
            "Bottom line is also bit kind of to swing into the other direction, not make it extremely extremely random forest next time, but but to see where the balance is between between both extremes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much, good afternoon.",
                    "label": 0
                },
                {
                    "sent": "Our paper has two objectives, so it's on a bleak random forest.",
                    "label": 0
                },
                {
                    "sent": "First, first objective is to revisit this idea.",
                    "label": 0
                },
                {
                    "sent": "So if you look into Brandon's original paper from 10 years ago, then you find two different random forests.",
                    "label": 0
                },
                {
                    "sent": "He proposes.",
                    "label": 0
                },
                {
                    "sent": "The one is the one we are using with standard univariate decision trees.",
                    "label": 0
                },
                {
                    "sent": "Just testing single features and the other one he proposed was a random forest which you stop leak splits and these obsolete object splits had random hyperplanes coefficients.",
                    "label": 0
                },
                {
                    "sent": "And then using the East oblique projections in space for the node splits.",
                    "label": 0
                },
                {
                    "sent": "So revisiting this idea is the one thing, and the other thing is what we actually propose here now is not to use these random splits in this random forest framework, not these random coefficients, but to use learn, not model node models in in the random splits, the kind of discriminatively near North models instead of this random split proposed by by men, and this is.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the contribution of this paper.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, background, but well, I'm interested in like application wise is the among others the classification of spectral data.",
                    "label": 0
                },
                {
                    "sent": "And here on the left hand side you see a few examples.",
                    "label": 0
                },
                {
                    "sent": "So infrared datasets and NMR data sets on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "The actual datasets with spectral channels from left to right and different residents, lines and absorption lines and then the same data set normalized to unit variance for every channel and then two lines for two different classes.",
                    "label": 0
                },
                {
                    "sent": "So what you?",
                    "label": 0
                },
                {
                    "sent": "Get out of this is that the spectral data is very high dimensional, so we're talking here about easily a few 1000 feature entries for every spectrum.",
                    "label": 0
                },
                {
                    "sent": "We have relatively few samples, typically because there is a biomedical background often and the very property of these data is that features are correlated.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this here, then neighboring channels often have a very similar value and as a result of this this correlation.",
                    "label": 0
                },
                {
                    "sent": "We also have noise which is.",
                    "label": 0
                },
                {
                    "sent": "Strongly correlated between channels, so just neighboring channels, but also correlation over over features which are far apart just from the kind of acquisition process which is taking part here.",
                    "label": 0
                },
                {
                    "sent": "So this is spectral data and spectrometry does a lot of this analysis and analyzing these data, but the same kind of data with the same properties in the same problems, the same noise.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, is something which shows up very often.",
                    "label": 0
                },
                {
                    "sent": "Of course, other kind of diagnostic tests where you have biomedical features and these microarray data, for example, which correlate and then all kinds of image data.",
                    "label": 0
                },
                {
                    "sent": "What you see here is multi spectral data, lower left hand side.",
                    "label": 0
                },
                {
                    "sent": "Find the archaeological site in this data set.",
                    "label": 0
                },
                {
                    "sent": "This is something where you also classify Spectra in the 2D context recognition and detection task.",
                    "label": 0
                },
                {
                    "sent": "So the talk yesterday with Kinect the recognition problem there.",
                    "label": 0
                },
                {
                    "sent": "This is this is something which is approached with decision trees and and.",
                    "label": 0
                },
                {
                    "sent": "Um data which is very similar to to the kind of spectral data I've shown you, and also medical task where you really have 3D datasets and where you want to find.",
                    "label": 0
                },
                {
                    "sent": "In this case we worked on Ms lesion segmentation and use random forest.",
                    "label": 0
                },
                {
                    "sent": "In this image computer vision setting to actually classify.",
                    "label": 0
                },
                {
                    "sent": "Some binary classification problems.",
                    "label": 0
                },
                {
                    "sent": "So whenever I speak about spectral data, this is.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The background.",
                    "label": 0
                },
                {
                    "sent": "What's wrong with this?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "The problem with spectral data is what you see on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "So top left is just a two mixtures and the tree learned on or to split these two classes and what you see here in the lower left side is the kind of marginal distribution rare uses seek for the best split at every node.",
                    "label": 0
                },
                {
                    "sent": "The problem now is if your features are correlated and and this is something we have in all these data then your data look looks like like the example on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit artificially correlated, but what you see is that the split is very difficult.",
                    "label": 0
                },
                {
                    "sent": "To actually find, because the marginal distributions are totally overlapping in the in the lower lower box you might see that an and we get really complex trees.",
                    "label": 0
                },
                {
                    "sent": "No, I already mentioned it.",
                    "label": 0
                },
                {
                    "sent": "The solution here and and you see it from from from this kind of little drawing is that you just use an oblique split right?",
                    "label": 0
                },
                {
                    "sent": "So if you split model it has this degree of freedom to spit as as you wish in this space and then it can easily adapt and trees from public splits have a long tradition.",
                    "label": 0
                },
                {
                    "sent": "So in the computer science community it's it's the early 90s that they were proposed, at least in statistics it's even even longer, late 1970s and 1980s.",
                    "label": 0
                },
                {
                    "sent": "They were quite popular problems is that?",
                    "label": 0
                },
                {
                    "sent": "You have to optimize them.",
                    "label": 0
                },
                {
                    "sent": "There are many, many more parameters to optimize for every split model.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes people use global optimization approaches here, and regularization is as a consequence, relatively difficult too.",
                    "label": 0
                },
                {
                    "sent": "Now using trees with oblique splits as baseline.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For rent in forest is something which has been done, has been used and as I mentioned, the original paper about by man was was already mentioning these random multivariate splits and the observed extremely good results on a couple of datasets.",
                    "label": 1
                },
                {
                    "sent": "There is a more recent work on up on rotation forests, So what they do is they just use the first item component of the data in which rests in a note, learn the the direction first direction and use this for splitting and again good qualification results.",
                    "label": 1
                },
                {
                    "sent": "And also, um, decision Forest, where they actually use these these original these all.",
                    "label": 0
                },
                {
                    "sent": "Big random forests, but kind of optimize every every tree individually, but again kind of difficult to optimize, but again good results in that case too.",
                    "label": 0
                },
                {
                    "sent": "No, what we propose to use here is now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learn the forest.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, Rich is using standard, simple, standard, discriminative model in the forest.",
                    "label": 0
                },
                {
                    "sent": "So not not this complex global optimization, it's just a recursive linear model split.",
                    "label": 0
                },
                {
                    "sent": "We use as an oblique tree as a base learner.",
                    "label": 0
                },
                {
                    "sent": "In this case, recursive binary splits at node M in the upper right hand side you see the split.",
                    "label": 1
                },
                {
                    "sent": "What you have to define are the coefficients.",
                    "label": 1
                },
                {
                    "sent": "Peter in this model and threshold, the coefficient are learned in our case from rich regression and we use rich this constraint.",
                    "label": 0
                },
                {
                    "sent": "For two reasons, more less.",
                    "label": 0
                },
                {
                    "sent": "First, if there is correlation than between features, then this this rich penalty is is is necessary and the other thing is that otherwise you have these.",
                    "label": 0
                },
                {
                    "sent": "These noisy high high entries for your regression and the other reason is that if you split your data and then after a few splits you just have very few data samples in your note and maybe you're still in the high dimensional space so you can just roll deeper trees.",
                    "label": 0
                },
                {
                    "sent": "If you have some regularization.",
                    "label": 0
                },
                {
                    "sent": "Rich regression for finding the split direction you see the objective and kind of the the normalization and the objective.",
                    "label": 0
                },
                {
                    "sent": "So what Rich is actually choosing as a direction is something between the task optimal direction, which is the LDA direction.",
                    "label": 0
                },
                {
                    "sent": "So kind of in highest correlation with the labels red and green here, and something which is more in line with data variance.",
                    "label": 0
                },
                {
                    "sent": "And this is the PCA and I'm afraid you might not see the directions.",
                    "label": 0
                },
                {
                    "sent": "For the rich, so it's it's summer here.",
                    "label": 0
                },
                {
                    "sent": "This aligns here, which depend on your rich coefficient Lambda.",
                    "label": 0
                },
                {
                    "sent": "You might choose.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you can test quite a few of different directions using this rich model.",
                    "label": 0
                },
                {
                    "sent": "Project your data into this into the subspace and then just find the best threshold as you would do with any other split model as well.",
                    "label": 0
                },
                {
                    "sent": "So this is this is at the heart of the optic model, treason, public random force.",
                    "label": 0
                },
                {
                    "sent": "We propose to use this kind of model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the rest is essentially the same, so this is not exactly pseudocode, but just to to make very short outline.",
                    "label": 0
                },
                {
                    "sent": "There are the two parameters of the random forest, the number of trees and the subspace dimensionality, the kind of regular seek for the best split.",
                    "label": 1
                },
                {
                    "sent": "Then there is bootstrapping element for every single tree.",
                    "label": 0
                },
                {
                    "sent": "Regrow this tree we have this subspace dimensionality.",
                    "label": 0
                },
                {
                    "sent": "In every subspace we learn the best direction we evaluate different parameters of Lambda.",
                    "label": 0
                },
                {
                    "sent": "On exponential scale and evaluate the best fit on the out of box sample out of back samples which are available at that node and grow as far as the best possible don't and we don't prove.",
                    "label": 0
                },
                {
                    "sent": "In prediction reviews, all the samples based on the average.",
                    "label": 0
                },
                {
                    "sent": "So and This is why we just get around of this difficulty, not the trees of pruning and adapting the complexity, the complexity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tree.",
                    "label": 0
                },
                {
                    "sent": "Now the scaling behavior for this is surprisingly good.",
                    "label": 1
                },
                {
                    "sent": "Of course it scales with the size of the forest.",
                    "label": 0
                },
                {
                    "sent": "The overall scaling is the same, so that kind of number of nodes is the same for both trees.",
                    "label": 0
                },
                {
                    "sent": "This done at random forest and the oblique random forest, so it's a number of levels which is log N 2 N and probably the random forest is more like N and lock and equally balanced option is more something with the more complex split model.",
                    "label": 1
                },
                {
                    "sent": "The complexity of the note evaluations, the splitting and every at every node.",
                    "label": 1
                },
                {
                    "sent": "Is the feature ranking cost for the standard random forest with the univariate trees and for the oblique random forest, it's the cost of a regression and this is P squared with P dimensionality of your subspace times N. And this is for small keys or high ends.",
                    "label": 0
                },
                {
                    "sent": "Even more efficient than the standard random forest.",
                    "label": 0
                },
                {
                    "sent": "So there there is a you get off with a fairly good random forest and fairly efficient.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly of course, prediction is a bit more expensive because you have.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Many features you have to evaluate, not just the single.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parametrisation, the two assemble parameter assembled size.",
                    "label": 0
                },
                {
                    "sent": "So what you see here Upper Rose classification error for a few datasets as a function of trees in the forest for standard random forests with uni, very trees and few different random forests and you see that it decreases quite well and we were not really able to overtrain here.",
                    "label": 0
                },
                {
                    "sent": "So we chose a couple of 100 trees for most of our later experiments the subspace dimensionality is a bit more critical, but we found that like that.",
                    "label": 0
                },
                {
                    "sent": "He fold as used for standard random forest like square root of P Das sufficiently well.",
                    "label": 0
                },
                {
                    "sent": "So probably I would recommend to optimize that if necessary.",
                    "label": 0
                },
                {
                    "sent": "But in our experiment we found it.",
                    "label": 0
                },
                {
                    "sent": "It's kind of well behaving and we just went with that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No X.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Garments.",
                    "label": 0
                },
                {
                    "sent": "We compared.",
                    "label": 0
                },
                {
                    "sent": "These classifiers on a number of datasets and we chose 40 binary classification tasks, 10 of them raw datasets.",
                    "label": 1
                },
                {
                    "sent": "Which features were factors 10 had just standard numerical features.",
                    "label": 0
                },
                {
                    "sent": "So just what you download from the UCI repository for example, and then twenty data set where the spectral data set I was mentioning at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So kind of the data which has this.",
                    "label": 0
                },
                {
                    "sent": "This property with correlation between features and so on.",
                    "label": 0
                },
                {
                    "sent": "We compare different classifiers and this was this oblique random forest with random splits.",
                    "label": 0
                },
                {
                    "sent": "So the thing Bryant.",
                    "label": 0
                },
                {
                    "sent": "Post and then the one we propose here with and without optimization of this rich penalty.",
                    "label": 0
                },
                {
                    "sent": "So it would be this LDA version without penalty and rich with the optimization.",
                    "label": 1
                },
                {
                    "sent": "Choose the default parameters I mentioned.",
                    "label": 0
                },
                {
                    "sent": "We compared it against a few other classifiers like support vector machine, Kenyeres neighbors cut random forests.",
                    "label": 0
                },
                {
                    "sent": "Random forest with random splits, Adaboost and optimized parameters for those an compared accuracy and error.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Under the curse.",
                    "label": 0
                },
                {
                    "sent": "Now these are the results and I wouldn't expect that even in the 3rd row you you get what's here.",
                    "label": 0
                },
                {
                    "sent": "So what you have is the different datasets from top to bottom, from left to right.",
                    "label": 0
                },
                {
                    "sent": "The different classifiers for the two different measures an what you may or may not see.",
                    "label": 0
                },
                {
                    "sent": "There are a number of the kind of the best.",
                    "label": 0
                },
                {
                    "sent": "The best results are underlined, so this is kind of the classifier, the combination, the classifier on the given data set which perform best on average and then in bold type are the kind of classifiers.",
                    "label": 1
                },
                {
                    "sent": "Rich on this data set did not differ significantly from the result of this one best classifier and significantly means here.",
                    "label": 0
                },
                {
                    "sent": "We just tested the resampled distribution on our from our cross validation.",
                    "label": 1
                },
                {
                    "sent": "So kind of everything which is here underlined or bold is either the best method or incomparable to the best, and this is something you can count.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and I hope you can read this now.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is the ranking for the different methods on the three different data types.",
                    "label": 0
                },
                {
                    "sent": "So this factorial data the first 10 datasets, the numerical data and then the spectral data for the two different measures and what you see here is that for factorial data standard random forest with univariate trees, another boost with univariate trees do very well lead the ranking while on all the other data sets the random forest with oblique splits.",
                    "label": 1
                },
                {
                    "sent": "Are definitely preferable, so and this is kind of the main result that the differences even bigger on the spectral data then on this on the standard numerical data.",
                    "label": 0
                },
                {
                    "sent": "But even there and also this learn split performs better than the random split prime and proposed the random objects, but Roman put post.",
                    "label": 0
                },
                {
                    "sent": "In his original paper, so you just see it once on this list at all.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, with this results we were interested in.",
                    "label": 0
                },
                {
                    "sent": "Getting some learning about some of the properties of this random forest so and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we did in the first Test was just to look at the hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "What you see here is the different hyperplanes and you might be getting that one decision tree is kind of overtraining easily at the lower left example.",
                    "label": 0
                },
                {
                    "sent": "So what this is is this Dennis mixture mixture data set where you have a couple of Gaussian mixtures for two classes and sub samples.",
                    "label": 0
                },
                {
                    "sent": "You train on those samples which are green and red dot.",
                    "label": 0
                },
                {
                    "sent": "Please excuse if you just see shades of Gray here and then you classify the whole space and compare this with the base optimal.",
                    "label": 0
                },
                {
                    "sent": "Specification, which is the dashed line in there and you see single trees don't do well, and the random forest do much better.",
                    "label": 0
                },
                {
                    "sent": "But the main interesting feature here is that this blocky structure of your standard decision tree is nothing you're resolved by pulling many and many, many trees.",
                    "label": 0
                },
                {
                    "sent": "So kind of both random forest and Adaboost do not resolve this blocky structure of your base learner.",
                    "label": 0
                },
                {
                    "sent": "And here is something where the assamble.",
                    "label": 0
                },
                {
                    "sent": "Of the more complex, more adaptive optic random forest and random trees just just benefit much from from from this degree of freedom and you get something which is very close to the base optimal decision boundary, much more like the RBF kernel support vector machine, then then then the random forest or the decision trees from the Senate Random Force.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is the topology.",
                    "label": 0
                },
                {
                    "sent": "Two more tests on the datasets from this comparison and we can be kind of group the data into data into into three classes, factorial data, numerical data and spectral data.",
                    "label": 0
                },
                {
                    "sent": "We compared where does this difference come from?",
                    "label": 0
                },
                {
                    "sent": "What is it we find?",
                    "label": 0
                },
                {
                    "sent": "And I don't go into the detail, but we find that the variance is more or less the same for all random forests, and we see that the difference in accuracy we had observed that factorial data standard random forests are good and a big random forest on all the other is mostly a difference in bias.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the same picture if you compare this, this random decision tree or bleak random forest with the random the random forest with random decisions and the one with the learn decisions against each other.",
                    "label": 0
                },
                {
                    "sent": "We have kind of similar variants, but the biases.",
                    "label": 0
                },
                {
                    "sent": "Again what makes the difference in accuracy by the end of the day.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No right at the beginning I gave you this little picture of this correlated spectral data.",
                    "label": 0
                },
                {
                    "sent": "Without further details.",
                    "label": 0
                },
                {
                    "sent": "What you see here is actual spectral data and the correlation between features and the first row just plotting the for two classes.",
                    "label": 1
                },
                {
                    "sent": "Different feature or bivariate histograms in those spaces.",
                    "label": 0
                },
                {
                    "sent": "And this is the actual correlation we observe.",
                    "label": 0
                },
                {
                    "sent": "The kind of model this artificially, and this is what you see in the second race.",
                    "label": 0
                },
                {
                    "sent": "2nd, 2nd row and then compare the standard random forest with.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal splits and the other of the Grand Forest with our splits and increase this degree of correlation.",
                    "label": 0
                },
                {
                    "sent": "Student two to nearly maximum correlation will just see here in the lower right side is that the only grand Forest completely ignores it, or nearly completely ignores it.",
                    "label": 0
                },
                {
                    "sent": "This is this green line performance stays the same while the standard random forest with universities just just just goes towards random classification at some point.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is kind of this is was just running back to the.",
                    "label": 0
                },
                {
                    "sent": "The first lights ran for us to have a few tools which make them so attractive.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is for one the importance measures.",
                    "label": 1
                },
                {
                    "sent": "So you have the feature importance, which is this gene importance.",
                    "label": 0
                },
                {
                    "sent": "This algorithmically defined gene importance that just a statistic of what features have been chosen and how much was the genie decrease.",
                    "label": 0
                },
                {
                    "sent": "And there's also this permutation importance, both proposed by prime and we just had a look and try to find something something similar for our model based on public random forests with model splits and what we chose was to use an over on analysis of variance in every split and record those.",
                    "label": 0
                },
                {
                    "sent": "Features which which which are deemed important at a certain split and just to to record the statistic of that and what you see on the left hand side is exactly this example, spectral data in the top row.",
                    "label": 1
                },
                {
                    "sent": "If you just do a univariate test, this is kind of the importance in the 2nd row for the different channels, the higher the more important than the Gini importance and the permutation importances typically empirically shown that it correlates quite well with this gene importance and then we have this.",
                    "label": 0
                },
                {
                    "sent": "Random forest and problems we propose here and what you see is that there are a few additional features and the general the kind of discrimination between of different features in this spectrum.",
                    "label": 0
                },
                {
                    "sent": "You see these different peaks is much, much harder for the oblique random forest we propose here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now featuring points, the other thing is sample proximity ready.",
                    "label": 0
                },
                {
                    "sent": "Just push samples down the trees and counter.",
                    "label": 0
                },
                {
                    "sent": "Often two samples in the test end up in the same terminal note and to keep your statistic of that.",
                    "label": 1
                },
                {
                    "sent": "And this gives you a proximity matrix and this is the proximity matrix for one of our spectral datasets.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we get here if we compare this for our public random forest and the standard random forest is that the and visualize this proximity matrix in eigen space like multi dimensional scaling very much.",
                    "label": 0
                },
                {
                    "sent": "Then you see that there is there is lots of structure in the lower hand side.",
                    "label": 1
                },
                {
                    "sent": "Here of this plot.",
                    "label": 0
                },
                {
                    "sent": "So kind of lower triangular and this is the standard random forest.",
                    "label": 0
                },
                {
                    "sent": "This is something which comes from the topology which is imposes arbitrary splits on the data which are not.",
                    "label": 0
                },
                {
                    "sent": "Given by the data, but just by the ability of how the decision tree can actually split and we don't have this problem for our public random forest, so kind of this is just just a very nice feature.",
                    "label": 0
                },
                {
                    "sent": "If sample proximity is something you're interested in visualizing this then then the random forest sample proximity gives you something which is much closer to the data.",
                    "label": 1
                },
                {
                    "sent": "And 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very last slide on.",
                    "label": 0
                },
                {
                    "sent": "That is if you.",
                    "label": 0
                },
                {
                    "sent": "If you look into the variance explained for the different eigenspaces then you would see that in the first eigen space the oblique random forest experience much more of the variance of this sample proximity.",
                    "label": 1
                },
                {
                    "sent": "So less less structure in higher dimensions, which is which is a good thing to have here on many different datasets including non spectral datasets.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we propose an aplique random forest with recursive linear model splits.",
                    "label": 1
                },
                {
                    "sent": "We find that it's sufficiently.",
                    "label": 1
                },
                {
                    "sent": "It is a sufficiently good scaling behavior, actually, and that we observe that it outperforms on any data set except on data rich as factors as features.",
                    "label": 0
                },
                {
                    "sent": "The standard random forest from univariate trees, the model split performs even better than the standard one.",
                    "label": 0
                },
                {
                    "sent": "The random split we which has been out there for awhile an under some conditions this constraint is important to advantage this is this.",
                    "label": 0
                },
                {
                    "sent": "Topology, so remember the this this example on the right hand side, mostly due to unbiased splits in the note and we also able to extract feature importance and proximity measures as in the original random forest which have a number of interesting properties here as well.",
                    "label": 0
                },
                {
                    "sent": "Overall we.",
                    "label": 1
                },
                {
                    "sent": "Use more complex not models, but get similar trees and by the end of the day better random forest.",
                    "label": 0
                },
                {
                    "sent": "This is what we observe, and if you want to test this as well, we just upload it in a package which implements or which has the random forest we used in all these comparison and it should be available on the standard repository.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So far we haven't really worked on that yet.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Well, probably it works, but we haven't really done experiments or something like that.",
                    "label": 0
                },
                {
                    "sent": "So how would you need to change the models in the splits?",
                    "label": 0
                },
                {
                    "sent": "So what you would have to do is you would have to use regression trees by the end of the day, which means you just.",
                    "label": 0
                },
                {
                    "sent": "You don't evaluate the discrimination between classes, but kind of the variance standard approach of of getting univariate split models here as well, and what you can do in the terminal node is either regret, do the regression keeping the terminal regression node or keeping the local local linear approximation or local constant approximation in the terminal node.",
                    "label": 0
                },
                {
                    "sent": "So the challenging part is displayed.",
                    "label": 0
                },
                {
                    "sent": "Because here the suite is a linear discriminants.",
                    "label": 0
                },
                {
                    "sent": "Quite straightforward so, but everything where you how you?",
                    "label": 0
                },
                {
                    "sent": "Well the the generalization of a of a classification tree to a regression tree is by exchanging the.",
                    "label": 0
                },
                {
                    "sent": "Evaluation function, and this is something you can do 1 by 1 here as well.",
                    "label": 0
                },
                {
                    "sent": "We haven't done it yet, but there is no principle problem to do that.",
                    "label": 0
                },
                {
                    "sent": "So in this statistical learning poke, you also see these graphs where you.",
                    "label": 0
                },
                {
                    "sent": "When you take many axis parallel splits, you can easily.",
                    "label": 0
                },
                {
                    "sent": "Approach aplique split and the more splits you take.",
                    "label": 0
                },
                {
                    "sent": "The closer you get.",
                    "label": 0
                },
                {
                    "sent": "So I would have expected if you take more random forests at higher number of random forests, you would approach the performance that you get with your flick splits.",
                    "label": 0
                },
                {
                    "sent": "So the first question is this something that you observe so and I have a related question.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "OK, I forgot the related question, so you're right.",
                    "label": 0
                },
                {
                    "sent": "So if you make it the really deep deep set of univariate split, you should be able to split your distribution into two parts.",
                    "label": 0
                },
                {
                    "sent": "That's true, and this is what you see in this little pictogram or this figure from the first lights in the upper right high half, right?",
                    "label": 0
                },
                {
                    "sent": "So by the end of the day, you have a decision tree univariate decision tree.",
                    "label": 0
                },
                {
                    "sent": "There, even if it's kind of complex to split both.",
                    "label": 0
                },
                {
                    "sent": "Anne, what we see here is that it's just not optimal to to use those, because by the end of the day, this topology is still present, present and what you see is that the strength of the ability to split both classes is much lower for the univariate splits.",
                    "label": 0
                },
                {
                    "sent": "So we all the trees are like growing into two to the you know Terminal 2 to pure notes and increasing the size of the forest doesn't change much by the end of the day anymore.",
                    "label": 0
                },
                {
                    "sent": "So kind of the two parameters we have here, we are both at the limit of them.",
                    "label": 0
                },
                {
                    "sent": "OK so I like very much this picture where you show the Queen incense.",
                    "label": 0
                },
                {
                    "sent": "The comparison between the surface separation surface of this random forest against the base of the base of the mole surface, which is also there.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "But I think there are a couple of things here, right?",
                    "label": 0
                },
                {
                    "sent": "One is whether given a large enough sample, you exactly recover the base surface, right?",
                    "label": 0
                },
                {
                    "sent": "OK, which has consistency and the other one is how fast.",
                    "label": 0
                },
                {
                    "sent": "Are you Richard?",
                    "label": 0
                },
                {
                    "sent": "How fast it converge that so both require direct equipment theoretical results in a sense, so I wonder if you have something already in that direction.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately there are not too many theoretical results out there for random forests, bryman's original random forests, and we haven't really dared to look into the oblique random forests here yet, so we don't have theoretical results on that, but it would be nice to have.",
                    "label": 0
                },
                {
                    "sent": "Definitely yes.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe it is a stupid question.",
                    "label": 0
                },
                {
                    "sent": "Just quit.",
                    "label": 0
                },
                {
                    "sent": "Another nice bad.",
                    "label": 0
                },
                {
                    "sent": "Is it so that in his last 10 years we didn't have paper with a similar ID?",
                    "label": 0
                },
                {
                    "sent": "Because it seems I like this idea.",
                    "label": 0
                },
                {
                    "sent": "What you have presented, but for me it's so natural, and I suppose there were times I use similar tricks, maybe not exactly for the random for us, but.",
                    "label": 0
                },
                {
                    "sent": "Just to have the model for forgiven, split, and even if the spread is more or less random, it's quite obvious.",
                    "label": 0
                },
                {
                    "sent": "And another question is like it's the former question.",
                    "label": 0
                },
                {
                    "sent": "The question is whether we are not losing something because in the random in the primer paper, everything was based on randomness, and in that way he was proving at least the consistency.",
                    "label": 0
                },
                {
                    "sent": "And here we are limited, though this is random.",
                    "label": 0
                },
                {
                    "sent": "So in in some way you already gave the answer with your second questions to your first.",
                    "label": 0
                },
                {
                    "sent": "So there are quite a few papers.",
                    "label": 0
                },
                {
                    "sent": "There are a few other 3, two or three I mentioned here, but most of the other works which looked into the different options and the variations of all this try to make it even more random and even more extremely random, randomizing even more at note splits and in any kind of.",
                    "label": 0
                },
                {
                    "sent": "Rae Ann and try to to get a better random forest with the classification result out of it, and this is a bit while we observe here is that you can do too much, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a totally random decision and you have many totally random decision by the end of the day, you have still a maybe not random decision but not optimal decision.",
                    "label": 0
                },
                {
                    "sent": "And we see here that kind of balancing off this general randomization of the random forest bagging subspace search.",
                    "label": 0
                },
                {
                    "sent": "Or subspace subspace is is is a good randomization but but kind of inside it works better if you have strong note models which is also something you have in probabilistic boosting trees right?",
                    "label": 0
                },
                {
                    "sent": "So this is also kind of an idea where you use very strong good note models and by the end of the day get a better overall classifier.",
                    "label": 0
                },
                {
                    "sent": "So in some way.",
                    "label": 0
                },
                {
                    "sent": "Bottom line is also bit kind of to swing into the other direction, not make it extremely extremely random forest next time, but but to see where the balance is between between both extremes.",
                    "label": 0
                }
            ]
        }
    }
}