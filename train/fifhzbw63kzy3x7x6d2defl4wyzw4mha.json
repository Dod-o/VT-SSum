{
    "id": "fifhzbw63kzy3x7x6d2defl4wyzw4mha",
    "title": "Safe RL",
    "info": {
        "author": [
            "Philip S. Thomas, College of Information and Computer Sciences, University of Massachusetts Amherst"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Deep Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_thomas_safe_rl/",
    "segmentation": [
        [
            "And thanks again for organizing the summer school and inviting me to come give a talk.",
            "So today I'm going to talk about safe reinforcement, learning and safe reinforcement learning algorithms.",
            "But I guess I'll start with just an."
        ],
        [
            "Overview of what I'm going to talk about in this talk so you know what to expect.",
            "I'm going to start off by just giving some background.",
            "Make sure we're on the same page about notation and I'll give some motivation for why it is that we need safe reinforcement learning algorithms.",
            "I'll then talk about what I mean when I say a safe reinforcement learning algorithm and also what some other people mean.",
            "So I'll give you a very quick overview of some other definitions of safety within reinforcement learning.",
            "I'll then talk about three different steps towards developing a safe our algorithm.",
            "There three subcomponents through three subproblems that we can actually study pretty much independently, and that's off policy evaluation.",
            "High confidence off policy evaluation, and then actually constructing this algorithm.",
            "And then finally, I'll talk about some experimental results to show that this isn't just theoretical talking.",
            "This actually works, and then I'll conclude and talk about some possible future directions and also give references all kind of there at the end."
        ],
        [
            "OK so first background.",
            "I mean, we've seen this repeatedly over and over, but we're in the IRL setting so we have an agent interacting with an environment that agent is observing the current state of the environment, choosing an action that action changes the state of the environment in the agent gets that reward signal just the standard MDP setting, and our goal is to find an optimal policy that's the optimal mapping from states to actions.",
            "And in this talk we're going to generally focus on episodic MDP's, so this this cycle repeats for some number of steps and then it stops, which you can view as we enter some.",
            "Terminal absorbing state.",
            "We reach a state from which we never leave in.",
            "The rewards are all zero, so it's an episodic setting.",
            "Also, we're going to deal with the case where this mapping this policy can be deterministic or stochastic, but there's going to be some cases where we require stochastic policies.",
            "So when I say stochastic policy, I just mean that the policy produces a distribution over actions it doesn't deterministically select match."
        ],
        [
            "OK, So what notation am I going to use for this talk?",
            "I'm going to use Peiffer policy soap.",
            "I VE given S is the probability of action in state S. I'm going to use H for history and I'm going to say history instead of trajectory, and that's just to simplify some notations that we don't use T for time and trajectory.",
            "So H history is just an observed sequence of states actions and rewards.",
            "And I said we're going to focus on the episodic setting where we terminate after some amount of time, and so L is going to be the horizon of this problem.",
            "The number of decisions that we have to make.",
            "Before this process terminates.",
            "OK so H is a history or a trajectory or an episode.",
            "The historical data D. So I'm going to talk a bunch about this historical data later.",
            "It's just going to be a set of observed trajectories or histories.",
            "It's data that we've collected from running some existing policy.",
            "And this historical data was generated by running a policy that will call the behavior policy Pi B.",
            "And our objective, as discussed, discussed before, is expected return.",
            "So we want to find the policy that maximizes the total amount of reward that we get.",
            "And I'm going to use the symbol J to note the performance of a policy pie, which is just that expected.",
            "Sum of rewards.",
            "Given that we're using that policy so pretty standard notation, it's pretty similar to what Rich was using in his talk."
        ],
        [
            "OK, so I've set this up using MVP notation an for the rest of the talk.",
            "I'm pretty much going to stick to MDP notation, but it's worth noting that when we talk about safety issues, it's important that were robust to things like partial observe ability.",
            "The real world isn't some very simple MVP where we observe the full state of the environment.",
            "We have sensors that make make observations, and those sensors are noisy and perhaps unreliable, and so really we should model the world.",
            "Is a palm DP partially observable MDP, so our agent isn't directly observing the full state of the world.",
            "It's making some observation about the world via some sensors, and these observations can be observations can be noisy, they can be incorrect.",
            "And this is the palm DP setting everything that I'm going to talk about carries over to this palm DP setting.",
            "Even though I'm going to be talking about the MDP setting mainly so it's just an important thing to keep in mind that this will be robust."
        ],
        [
            "Partial observe ability.",
            "OK, so let's talk about some potential applications where we might care about ensuring different forms of safety.",
            "So the first one is digital marketing.",
            "And so here I visited Adobe's website and they showed me an ad for Creative Cloud and actually a second one for photo shop.",
            "And these ads aren't deterministically chosen.",
            "They're not the same for everyone.",
            "They're not completely random.",
            "Their targeted specifically at U, the visitor to the web page.",
            "And so this process of targeting advertisements can be viewed as a sequential decision problem.",
            "So here the ignore the state.",
            "For second, the observation is the vector that captures everything that we know about you, the visitor to the web page when you come to our web page, the action is then a decision about which ad or type of AD are we going to show you, and the reward is plus one.",
            "If you click on the advertisement and zero if you don't click on the advertisement.",
            "And this is actually a sequential problem.",
            "Often this is treated as a bandit problem, but it's really sequential.",
            "What's the sequential nature?",
            "Well, our observation at one step.",
            "Depends on the action that we chose at the previous step.",
            "We showed you an advertisement which you either clicked on or didn't click on and that changes our beliefs about what you might be interested in or might not be interested in, and so the observation vector one step is closely dependent on the action that we chose at the previous step.",
            "So this is digital.",
            "Marketing is a sequential decision problem.",
            "Why do we care about safety here?",
            "Well, if we deploy a reinforcement learning algorithm to this problem and it does very poorly, it starts deploying advertisements that actually there's a much lower chance that you click on it.",
            "It can result in.",
            "Significantly decreased revenues, and so for companies like, for example, Adobe has a product called Test and Target which serves up advertisements for other companies.",
            "If they use reinforcement learning in a product and it ends up producing policies that are worse, they could end up losing customers, and so it could be very expensive to accompany.",
            "If in our algorithm for this task doesn't perform properly."
        ],
        [
            "OK, what's another example?",
            "Another example is intelligent tutoring systems, so I'm currently a postdoc at CMU, working with Emma Brunskill, and so I'm biased towards these applications.",
            "I like them a lot because Emma is doing these intelligent tutoring system applications.",
            "The idea here is that we have a student that's learning about a topic.",
            "This is a game, so it's not a course.",
            "In this case, it's a game, but it's the same concept, and here the students learn about fractions, so the students controlling the frog and they have to click 26.",
            "I think it says 2 six of the way up the chain, and that's where the weak point on the chain is.",
            "And if they click it correctly, then the frog.",
            "Breakthrough and they can continue and they solve a sequence of these problems.",
            "Where is the RL task here?",
            "The task is that if we give a student a problem that's too hard, then they can get frustrated and stop playing.",
            "If we only give them problems that are too easy, then they're not going to learn anything.",
            "So the task here is to choose what type of problem to give next, how to order topics, how difficult of a problem to give next to maximize the students understanding at the end of a course or after playing one of these games.",
            "So what would our sequence?",
            "What would our setting here would are setting?",
            "Beware this is a sequential decision problem, so our observation here is everything that we know about the student.",
            "What we think they understand right now.",
            "Our action is then a decision about what type of problem to give them next or how difficult of a problem to give them next.",
            "And in this case for this game called Tree Frog.",
            "The rewards were actually 0 all the way through and then at the end the student took a test on the topic on fractions and the student score on the exam was the only reward for this entire history.",
            "This entire episode, and So what we're trying to do is maximize the students score on this post test.",
            "So find the way of serving up problems to a student to maximize their understanding of this topic after the course.",
            "Why does safety matter here?",
            "Well, if we deploy a policy that's worse, then we can discourage a person from learning more about this topic.",
            "Make them think I'm not good at math or I don't like math, and so it's important that when we deploy learning algorithms here that they work properly another one."
        ],
        [
            "Is functional electrical stimulation, so this is work that I did at case Western Reserve University during my Masters thesis.",
            "And the idea is that when someone is paralyzed, their muscles and nervous system are still intact below the break often, and so you can directly stimulate the muscles in a person's arm.",
            "A paralyzed person's arm in order to move their arm for them.",
            "And so the question was, can we control a paralyzed person's arm to move it from some initial position to some target position?",
            "And researchers at Case Western Reserve University working with researchers at the Cleveland Clinic did this and they used a proportional derivative controller, standard control technique and it worked, but only reasonably well.",
            "The problem was that the individual persons arm.",
            "Didn't exactly match their model of an ideal arm, and so they asked, can we create a controller that adapts to this individual persons arm to make the control movements more smooth?",
            "Or in this case so that they would land at the correct target position and this can be modeled as an MVP?",
            "I'm guessing you can all see exactly why, and again, safety is an issue if you're trying to drink.",
            "You don't want this person pouring things over themselves or even worse in this exact setup.",
            "If you give the wrong stimulations in the wrong states, you can dislocate the person's shoulder.",
            "And this is something that we really want to make sure we don't do.",
            "We don't want to play learning algorithm that does this unsafe behavior."
        ],
        [
            "And the last one, which is really, I think, the best motivation for why we need these safe algorithms is type one diabetes treatment.",
            "So first, what is diabetes?",
            "And what is the IRL problem here so?",
            "When you eat a meal, the carbohydrates in the meal cause your blood sugar to increase.",
            "Your body then releases insulin which promotes the absorption of the sugar from your blood into various cells of your body.",
            "So throughout the day your blood sugar makes a pot that looks like this.",
            "This is minutes since midnight, so this is one day, and this vertical axis is your current blood sugar, and this is many simulated days using a simulator.",
            "This is your blood sugar spiking, 'cause of breakfast, lunch and dinner, and for this particular person, green is ideal blood sugar levels and black is the limit of healthy levels.",
            "OK, So what is the problem in type one diabetes so?"
        ],
        [
            "Type one diabetes.",
            "Their body doesn't release enough insulin.",
            "And this means that their blood sugar levels tend to be too high, which is a condition called hyperglycemia.",
            "Hyper for high.",
            "So hyperglycemia is when their blood sugar tends to be too high 'cause there's not enough insulin promoting absorption and pulling blood sugar down.",
            "Extended instances of hyperglycemia can lead to ketoacidosis, and I think it's the leading cause of nontraumatic adult amputations.",
            "So extended hyperglycemia is a problem.",
            "So to fix this."
        ],
        [
            "Someone can do is inject additional insulin into their blood, either directly or through an insulin pump.",
            "And if they don't inject enough, they'll still suffer from hyperglycemia, high blood sugar, and if they inject too much, if they put too much insulin into the blood and promote too much, the absorption of sugar from the blood into the cells, it results in hypoglycemia, and it's really easy to mix these up, it's just one letter, different, different, but hyper is too high, hypo is too low, so hypoglycemia low blood sugar is significantly worse than hyperglycemia.",
            "Severe instances of hypoglycemia can triple the five year mortality rate for personal type one diabetes.",
            "And it can lose to loss of consciousness, which is devastating.",
            "If you're driving, for example.",
            "So this is a control task.",
            "It's a task where we'd like to decide how much insulin should someone inject prior to a meal in order to keep their blood sugar levels near optimal.",
            "Whoops, near optimal without driving it too low and to try to keep it from being too high.",
            "OK, so it's a control problem so."
        ],
        [
            "We can try to address it as a control problem, and actually what is done now is in an insulin pump.",
            "There's an equation that decides roughly how much insulin someone should inject.",
            "And this is kind of a simple form of one of those equations, and it's if you're familiar with the controllers, the proportional term.",
            "This is the derivative term.",
            "So what is this what?",
            "Let's go through it.",
            "The size of the injection is the person's current blood glucose.",
            "The current amount of sugar in their blood, measured from a blood sample minus their target blood glucose, which is a value.",
            "An ideal blood glucose level specified by their diabetologists divided by a parameter CF that we get to tune.",
            "So if you think back to Peter Abeles talk earlier, we had these parameterized policy policies.",
            "This is a parameterized policy.",
            "And that's one of the parameters that we can tune, CF.",
            "The next term is the size of the meal that the person is about to eat, which we measure in terms of grams of carbohydrates, and we divide that by the 2nd parameter that we get to choose which is CR.",
            "So this is just a parameterized policy for sequential decision problem.",
            "We have three decisions.",
            "How much how much inject for breakfast, lunch and dinner, and these are correlated because the amount that you inject at breakfast changes the state of the person at lunch.",
            "OK, so the way this works now is that someone goes to their diabetologist who says these are values of CR&CF that will likely work for you and the person goes off and roughly three to six months later they come back to their diabetologist.",
            "Who says OK, given how well this is work, looking at the results from the insulin pump and the report from the person after three to six months, they say here's how I think you should change these parameters CR in CF, and so this raises the question, can this process be automated to some extent?",
            "If not to replace doctors?",
            "To handle cases where someone doesn't have the opportunity to go see a doctor every three to six months so."
        ],
        [
            "I've been some really great work coming out of the University of Alberta where the intelligent Diabetes Management Project, which is a collaboration between the University of Alberta and Alberta Diabetes Institute.",
            "They show that this problem can be treated as a reinforcement learning problem and there's a Masters thesis by Mason Bastani, working with Russ Greiner.",
            "So how is this modeled?",
            "Well?",
            "The state is the state of the person.",
            "The observation at one step is the current blood sugar levels and the size of the meal there bout to eat.",
            "The action is how much insulin the person should inject.",
            "And this repeats three times for breakfast, lunch and dinner.",
            "And we can treat that as an episode.",
            "OK, so this is a reinforcement learning problem."
        ],
        [
            "So I've listed out several problems where safety is kind of a concern and to really hammer home the problem right now with non safe reinforcement learning methods.",
            "I want to ask a question which is this.",
            "If you deploy an existing reinforcement learning algorithm, pick your favorite one, fitted Q iteration to one of these problems, do you have confidence that the policy that it produces will be better than the current policy?",
            "Do you trust that the new parameters that you pick for this diabetes controller?",
            "Are actually going to be better than the current ones.",
            "Do you trust that your method for serving up ads that's proposed by your favorite our algorithm will be better than the current existing method?",
            "So put differently, can get hands for if you've ever implemented in our algorithm on any problem where you didn't just run it, you actually kind of chose the step sizes you chose the representation, and it can even be mountain car hence.",
            "So quite a few of you, most of implementing keep up.",
            "Keep up, keep up OK, so leave them up now if the first time that you hit run it worked and put them down.",
            "If your first choice of step sizes in representations wasn't right.",
            "So this is the problem right?",
            "The first time that you run in our algorithm, it typically doesn't work right.",
            "You need to do a lot of adjusting and tuning before it works and this is the difference between a lot of these proposed applications and a lot of the successes.",
            "That reinforcement learning is had so far.",
            "This success is or for applications where it's OK to tune our representation to tune our step sizes to tune our hyperparameters.",
            "Take Atari for example.",
            "Google did a grid search over the step size parameter for this, so it's completely OK to do this.",
            "It's OK somewhere on a Google server to have Atari agents that are just losing millions of games of Atari over and over and over again while they do this search and then at some point they find step sizes that work and they can report that result.",
            "And that's a cool result that is not OK for a medical application.",
            "We can't tell someone.",
            "Sorry this treatment didn't work for you.",
            "Maybe for the next person will get our step size right.",
            "We can't do this.",
            "We need methods that on the first shot you hit the button and you know that it's going to work, or at least you know that it's not going to make matters worse.",
            "OK, but that's not to say that, oh, these methods are bad and we should only use these methods, not at all.",
            "These are different sides of reinforcement learning.",
            "There can be applications where risk isn't the main concern.",
            "And also there's another point, which is a lot of us got a lot of us got into this because we're interested in intelligence.",
            "We want to understand what is intelligence, where does it come from?",
            "Maybe how do we work?",
            "Can we create a machine that's generally intelligent and that work tends to fall over in this category?",
            "So in the rest of this talk, we're going to talk about things that personally I don't think really describe much about.",
            "What is intelligence or get at that problem?",
            "It's more about taking what we've learned in reinforcement learning and saying, can we apply this responsibly to some real problems now?",
            "OK."
        ],
        [
            "One other point that I want to make is that learning curves are really very deceptive, and so I picked plots from one of my own papers like in Bash.",
            "It these are from 2013 nips paper with will Dabney that we called natural temporal difference learning.",
            "So here's our method.",
            "Our method is this green one and many of you have probably seen mountain car in Carpool Mountain car.",
            "You're trying to get to the top of that Valley carpool.",
            "You're trying to keep the pole balanced so our method on mountain car gets a near optimal policy after a single episode.",
            "So Dilly dally around this mountain a little bit.",
            "Learning it gets to the goal and after that it's almost optimally getting to the solution every single time.",
            "Same for cart pole.",
            "The pole falls over a single time and after that it never happens again.",
            "It always keeps the full balanced, so this seems like an outstanding algorithm, right?",
            "But this is deceptive, so The thing is this is really after billions of episodes of tuning.",
            "So first we did millions of episodes of setting all of the different hyperparameters and this is standard when you read NRL paper says we manually tune the hyperparameters.",
            "We did a grid search.",
            "So millions of episodes that went into the parameters optimization.",
            "There's also probably another millions that were human intuition from working with these domains in the past.",
            "So we worked with mountain car.",
            "We know that the Fourier basis with linear function approximation works really well as representation here, and that came from many past trials.",
            "And also there's billions of episodes that went into experimental design people, making sure that this task is possible.",
            "We're not asking something that's completely impossible, and our reward function isn't poorly designed, so going into these plots is a lot of learning ahead of time and adjusting of the system, and so we should be careful when we look at these to say, well, if I apply this to diabetes treatment, I really need to worry about these.",
            "I can't just look at this horizontal axis there, OK?",
            "Yeah, so here are you saying that basically?",
            "Yes, essentially we've found the perfect step size is the perfect exploration rate so that for this task it works.",
            "Yes, I don't believe that our algorithm is so amazing that it never dropping the pole here.",
            "This is a fluke of the order of the Fourier basis, which sets the exact parameters of the representation with that step size.",
            "With that, exploration rate just happens to workout perfectly.",
            "To solve this problem.",
            "So yes, we've overfit the problem.",
            "No.",
            "Even the notion of overprinting in RL.",
            "No.",
            "That's one of the challenges, yeah?",
            "Anyway, so I think I've finished the definition of what.",
            "Sorry, I haven't finished definition.",
            "I've talked about background of notation and why we should care about safety and what it is that we want.",
            "So let's define safety."
        ],
        [
            "So what do we want?",
            "We want an algorithm that, as I said when we hit the go button, it works on the first try, so it has to be guaranteed to work, or more specifically for the rest of the talk, other than when I talk about other definitions of safety in a moment when I say a safe reinforcement learning algorithm, I mean one that can give this guarantee, one that can say with probability at least one minus Delta.",
            "I'm not going to change your policy to one that is worse than the current policy.",
            "I'm not going to make things worse, so Delta here is the probability that I'm going to do something bad.",
            "And a key property of this is you.",
            "The user should get to choose this parameter Delta.",
            "You can choose the probability that I'm allowed to fail or do something bad.",
            "So my algorithm should do is sit there and collect data.",
            "If you choose a very low probability of failure, I might have to sit there for a long time collecting data before I can say OK.",
            "Here is a policy that's better, and this guarantee should not be contingent in any way on any hyperparameter settings.",
            "So this guarantee just has to actually hope.",
            "So put in pictorial form, we want to create this black box.",
            "Which takes as input some historical data D. This is the data from running our current policy for serving up advertisements or current policy for treating diabetes and a probability 1 minus Delta and produces a new policy pie that satisfies this.",
            "The performance of this new policy is at least the performance of the behavior policy with probability 1 minus Delta.",
            "So with high probability our new policy is at least as good as the current one.",
            "Are black boxes also allowed to say no solution found if you say here's data from 3 users, give me a better policy for serving up advertisements with 99% probability.",
            "We can't do it.",
            "We have to say given 3 users I can't satisfy this guarantee that you've asked for.",
            "So we have to be able to say I can't give the guarantee that you want, which is no solution found.",
            "And that really means just return the behavior policy.",
            "Keep running the policy that you've been doing 'cause I can't improve it with high probability yet."
        ],
        [
            "OK, So what are some limitations of this setting just to make them explicit, we've assumed that an initial policy is available.",
            "We're not saying we're going to learn from scratch to solve some new challenging problem.",
            "We're assuming that we have some policy for treating diabetes, some policy for serving up ads, and we're just going to try to improve it.",
            "We're also typically going to assume that this policy is known, so there's been some recent works toward works in progress toward replacing and getting rid of this assumption.",
            "But for now, we're going to assume that we know the current policy for serving up advertisements, and we're also going to assume that the currently deployed policy is stochastic, so there is some randomness in it.",
            "And we're also for now going to look at the batch setting so this isn't online.",
            "Learning where we see one state, we change our policy.",
            "We see another state.",
            "We change our policy.",
            "This is, I gather, a bunch of data and I make a change to my policy."
        ],
        [
            "OK, so just to again drive this home a little bit more, what are we looking for?",
            "If this is episodes and this is the performance of the policy, so standard learning curve the first time that you run a typical our algorithm on this, it will do something like this.",
            "It will completely diverge.",
            "You'll tweak your learning parameters and it will do something like this.",
            "It will start to work, you'll get the step size in the right range, the value functions converging and eventually you get the right parameters and it does really well.",
            "What do we want this process to look like when you're using a safe algorithm?",
            "Well, actually, first just worth mentioning that there's a trivial safe algorithm.",
            "Has anyone seen it and thought?",
            "Wait a minute.",
            "There's something wrong here.",
            "What is an obviously safe algorithm according to this definition of safety that ensures this?",
            "I think you all just set it at once, which is do nothing right.",
            "Always return no solution found.",
            "Return the behavior policy and that is safe.",
            "So that does this.",
            "It says just always return the current policy.",
            "We're not doing something bad and it satisfies our definition of safe.",
            "It's not a particularly interesting algorithm though, so hopefully our algorithm will sometimes actually change the policy and improve performance.",
            "But we want to guarantee that we're not going to make it worse.",
            "We're not going to blow that initial policy's performance with high probability.",
            "And eventually, what we'd really like to see, and what we'll see at the end.",
            "Or some plots that look like this.",
            "We're getting improved performance.",
            "And typically it's going to lag behind.",
            "We're not going to be as fast as that unsafe method that has been overfit to the problem.",
            "That has been perfectly tuned for this problem, because we're guaranteeing that on the first run it's never going to produce this behavior, OK?"
        ],
        [
            "So I pitched my definition of safe and what I think safety should mean, but I should mention there are many other definitions of safety and safety is a broad topic in reinforcement learning.",
            "So for example, there's a survey on reinforcement learning that has nothing to do with what I've talked about for safety so far."
        ],
        [
            "In that paper they have this is table one.",
            "This isn't actually there, table just recreated it where they break safe RL into a few different categories and I want to spend a little bit of time talking about this one because it's worth knowing about so they break safe RL into these categories.",
            "The first one is changing your optimization criterion, so this argument is that for some problems expected return mean sum of rewards doesn't capture what it is that we want to optimize, and so we should choose some other objective that better captures some notion of risk.",
            "And the other one, and I'm going to talk about a couple of these on the next slide.",
            "The other one is making changes to the exploration process and most of the methods that they list here or things like inverse reinforcement learning methods and learning from demonstration.",
            "So can I bias my initial learning by learning from a human in order to not have that initial learning.",
            "Where I'm basically flailing randomly and I would argue that what I'm going to describe in this talk is 1/3 category here, which is changing the learning process?",
            "Well, let's talk a little bit about changing the objective function."
        ],
        [
            "Because again, it's a common topic in reinforcement learning.",
            "So what we optimize right now is expected return so expected sum of rewards.",
            "So let's say that this horizontal axis is the performance axis, so this is higher, returns, lower returns, and this is the distribution of returns produced by some policy \u03c0, or we'll call it the blue policy, and that's it's mean right there.",
            "So this is, uh, some policy.",
            "Let's say that I give you a different policy, the red policy that produces this distribution over outcomes.",
            "So it has much higher variance.",
            "And it's expected value is a little bit higher.",
            "Which of these two policies is better?",
            "So first, if I'm a casino?",
            "Well, I'm not like the building if I'm running a casino, then which of these is better?",
            "So these are the returns from some some slot machine of some sort.",
            "Which one is better for me?",
            "Which one?",
            "I heard someone say the red and I thought I heard someone say the green.",
            "Gotta get a hand.",
            "Anyone which one is better and why?",
            "Yes, the red one because the the expected value is higher.",
            "I'll make more money in expectation and if I lose money, sometimes that's OK, right?",
            "So expected return is a decent notion of optimality here.",
            "Maximizing expected return.",
            "But if I'm a doctor, what if these correspond to patient outcomes?",
            "And somewhere over here is something like the patient dies in this case, which of these two policies is better?",
            "The blue right?",
            "Because yes, it has slightly lower expected performance.",
            "But we don't have this really high risk down here, so maybe I need a different objective function other than expected sum of rewards to capture what it is that I want to optimize."
        ],
        [
            "So.",
            "Let's do this.",
            "Let's come up with some other objectives that capture this.",
            "One idea is to penalize variance directly, which is what we saw in the previous slide.",
            "So we can say that our objective function now is the expected return just as before, minus this term where Lambda here is just a scaling parameter that says how much we care bout variance times the variance of the observed returns and if we make Lambda large enough in this then the blue policy or the red policy will be better than the blue.",
            "It will say try to get a policy that performs as well as possible and has low variance in the results that it produces.",
            "And there's some good work from Scotland Eresma on this and doing policy gradient on this objective.",
            "There's another popular one which is.",
            "Value at risk and conditional value at Risk C var is also called.",
            "I think expected shortfall.",
            "So what do these do?",
            "Let's say that this is the distribution of returns that we see instead of returning the mean as our objective value at risk says take the percentile take some Alpha percentile.",
            "Were here Alpha would be 5%.",
            "So what is the return such that 95% of the returns that we get will be bigger than that value and that is the VAR.",
            "So it's the the sum of rewards where we know 95% of the time.",
            "I'll get at least this much.",
            "That value is what you would optimize in that case, and then see Var takes the mean of this probability mass, that is, that is lower.",
            "So these are all notions of trying to optimize the worst case in some sense."
        ],
        [
            "OK, so for some applications like we said for medical applications, maybe this is the right objective.",
            "It's the one that we should care about, but this just changing our objective function doesn't address the motivation that we had before.",
            "We still have to produce an algorithm that optimizes these objective functions and the resulting algorithms are often very similar and we need them to work on the first shot.",
            "So I view this is kind of orthogonal to what I'll be talking about in the rest of the talk.",
            "There's a choice of objective, and then there's ensuring that we actually increase that objective when we run it was there question, but I'm happy to talk about this.",
            "More afterwards, OK, so there are those on Amazon, right?",
            "So it's kind of an orthogonal quest."
        ],
        [
            "There are other definitions of safety.",
            "For example, there's a paper by Remy Moonos and others."
        ],
        [
            "Where they say that an algorithm is not safe if it doesn't handle arbitrary off policy Ness where arbitrary off pole handle means give asymptotic convergence results.",
            "So, like Sarsa, Lambda has asymptotically convergence results in the tabular on policy setting.",
            "So this is a completely different notion of safety."
        ],
        [
            "There are others so AKA Matale working in Claire Tomlin's lab at Berkeley.",
            "They've done some work that's kind of related to adaptive control, and this is something that people often think about first.",
            "When you think about Safe RL and that is state avoidance, so maybe there's some region of state space that I want to ensure that I never reach that my agent never enters these dangerous states.",
            "It never they're working with quadcopters.",
            "It never crashes into the wall.",
            "And these methods typically rely on having some strong prior knowledge about the environment distribution over possible worlds that you can be in, and this is again very closely related to adaptive control.",
            "And then there's Packer L which for the sake of time I'm going to skip over this.",
            "But it is also related."
        ],
        [
            "OK, so we've now defined our notion of safety for the rest of the talk, which is an algorithm that when I hit the go button, it's guaranteed to work, or at least not make things worse with high probability.",
            "How do we make this?",
            "Well, they're going to be 3 components that we need to build in order to do this, and the first one is off policy policy evaluation, which is really awkward 'cause that middle policy.",
            "I'll talk about why it's there and then high confidence and then actually creating the algorithm.",
            "So let's go through what these are and then we'll delve into each one in detail.",
            "And we're actually going to spend the most time in this talk.",
            "On this first one, which is a paper by doing a pre Cup from the year 2000, an important sampling."
        ],
        [
            "OK, So what is off policy policy evaluation?",
            "We're going to try to make this black box in the middle of the screen and it takes two things as input.",
            "The first thing is historical data D, so we've run some current policy and it's produced outcomes.",
            "Remember D historical data is a set of histories, a set of episodes.",
            "So for digital marketing this is the data from all of our users, and the episode is the data from the user.",
            "OK, so we have our historical data produced by that behavior policy or currently run policy and we also are given some newly proposed policy which we're going to call PII and ease for evaluation 'cause we're going to evaluate this policy.",
            "OK, so given the data and this policy, so imagine that I just have collected data and someone else ran in our algorithm and said this policy is great.",
            "You should run it and what we'd like to produce as output is a prediction of the performance of this policy that the other person proposed would like to predict.",
            "How good would this policy be?",
            "And the catch is, this black box is not allowed to run the evaluation policy because you can imagine where we're going to use this, we're going to use this to ask is it safe to run this new policy and it could potentially be a dangerous policy, so we cannot.",
            "Actually run it so we need to use the data from our current policy to evaluate this new policy not running the new policy.",
            "This is off policy policy evaluation.",
            "Why the extra policy?",
            "So there's also off policy evaluation, which typically refers to trying to estimate the value function using data that came from running a different policy.",
            "And here we're not estimating value.",
            "Functions were predicting the performance of a policy stating expected returns of an entire policy, so that's the second policy."
        ],
        [
            "OK, what is the second problem that we're going to solve?",
            "We're going to make a second block black box that takes the same historical data as input.",
            "It takes that same policy that someone else is proposed, but we're not satisfied with just an estimate of how good that policy is.",
            "We're going to try to bound it, so we're also going to take a probability 1 minus Delta as input.",
            "And we're going to produce output A1 minus Delta confidence lower bound on the performance of this new policy.",
            "So we're going to say that policy that you gave me with probability at least one minus Delta, it's going to be this good.",
            "So we're lower bounding its performance.",
            "You can see where this is going to be useful.",
            "To be safe, we can't just predict.",
            "Yes, I think this policy will be better.",
            "We need to say, I think this policy will be better, and I guarantee it's going to be at least this good with high probability.",
            "And then we'll compare that to our current policy to see if we can return the new policy.",
            "And again, this is before we have to do this without deploying the evaluation policy."
        ],
        [
            "OK, so then the last step is to take this and put it together.",
            "So we're going to make a third black box that takes the historical data as input.",
            "No policy, it just takes the data and a probability 1 minus Delta and it produces that new policy that comes with our safety guarantees.",
            "That's kind of putting the pieces together."
        ],
        [
            "OK, so let's delve into these three problems.",
            "The first one being off policy policy evaluation and hear the talk.",
            "I'm going to get very specific so that hopefully coming out of this you will be able to implement that first one methods for the first one or the second one.",
            "There's a lot of handwaving that goes into the third one, so we're going to kind of delve into the math of it.",
            "OK."
        ],
        [
            "So let's talk about important sampling.",
            "Important sampling is what we're going to use to solve this off.",
            "Policy policy evaluation problem.",
            "Just so you have an idea, can I get hands for if you've seen important sampling for reinforcement learning?",
            "OK, great so.",
            "A little less than half.",
            "So let's delve into what what is important sampling and how does it work.",
            "So this is just a review of our notation.",
            "H is a history or a trajectory, and J of Pi is our objective, so we're going to try to predict is the performance of that evaluation policy.",
            "OK, so.",
            "This picture depicts all of the possible histories that could ever occur.",
            "Every possible outcome that could occur when we run our controller for treating diabetes.",
            "Will view policies then as a distribution over these outcomes.",
            "'cause that's all the policy is.",
            "When you run a policy we get an outcome and it's not deterministic, so there's some distribution over the outcomes that occurs when I run a policy.",
            "If I were to run the evaluation policy, the policy that I'd like to predict, and this is what we're not allowed to do, because this evaluation policy could be dangerous, right?",
            "If I were to run this policy, I'd get samples from this red distribution and it would be really easy to estimate the performance of the policy.",
            "I'd use the Monte Carlo estimator.",
            "I would just average.",
            "Over my N samples in the data, the observed discounted sums of rewards, the observed returns and this would be my estimate of how good the valuation policy is, right?",
            "How do I?",
            "How do we do this though when my data doesn't come from the red distribution?",
            "What do I do when my data comes from the blue distribution?",
            "The distribution over outcomes that occurs from running the behavior policy, the current policy, and I wish it came from the red distribution.",
            "What do I do?",
            "Well, I'm going to take a weighted sum.",
            "This is 1 important sampling does.",
            "How does this work?",
            "Well, what this weight does is, it says take the third trajectory down.",
            "This is a trajectory that we're not going to see very often, but if we could run the evaluation policy, we would see that outcome much more often.",
            "It has a higher probability under the evaluation policy, so this is an outcome that we would see more.",
            "So let's give it a large wait to pretend that we would have seen this more often.",
            "What do we do in the opposite case so this third one from the bottom?",
            "This is an outcome that in our blue bag the bag we actually have we see this outcome a lot, but if we were to run the policy that we want the evaluation policy, we wouldn't have seen it as much, so we'll give it a small wait, a wait that's less than one to say pretend that I saw this outcome less frequently.",
            "And that's all that important sampling does.",
            "It says take a weighted average of the observed rewards.",
            "And what I'm going to go into next is how do we compute what is a reasonable weighting scheme that implements this intuition?",
            "OK, also at some points I might might say importance weighted return that just refers to the term that's in the red box because it's importance weighted return the sum of rewards.",
            "OK, are there any questions?",
            "So far I've been kind of flying through this stuff.",
            "Is that a hand or a head?",
            "Scratch that scratch OK?",
            "OK, cool."
        ],
        [
            "So let's delve into the derivation of where those weights come from, and I think the easiest way to do this is to set it up not in the context of RL, but to do it just using kind of general notation for general estimators.",
            "And then we'll apply it to reinforcement learning.",
            "So let's do that.",
            "Let's let X be a random variable that has some probability mass function P, and in our case X is going to be a history, an outcome, an.",
            "It was generated by our evaluation policy.",
            "So P in this case would be the distribution over outcomes.",
            "That results from running the evaluation policy, but for the general case P is just a probability mass function for this random variable X.",
            "Let's let Y be another random variable that has a different probability mass function which is Q and for just in case anyone's blanking on it, probability mass function, that just means that PP of X would be the probability of that outcome X. OK, so let's let Y be a different woman getting ahead of ourselves.",
            "Let's let Y be a different random variable that has a probability mass function Q, but which has the same range, is X.",
            "So really you can think of this as P being one distribution and cubing a different distribution.",
            "Over the same things, and in our case, the things are episodes and P is the evaluation policies distribution and Q is the behavior policy's distribution, the distribution that results from running our current policy.",
            "OK. Next, we're going to define a function F. And what we'd like to do is eventually we're going to put the expected value of this function of F of X, but F is just some function, and in our case F of X.",
            "Remember X is just a history F of X, is the return of that history in our case.",
            "And what we'd like to estimate is the expected value of F of X, but we only have samples of the random variable Y, so we'd like to estimate the expected return when trajectory is come from the evaluation policy.",
            "But we only have trajectories that came from the behavior policy, so that's how this is going to map back.",
            "But because we're going to use a slot, I'll write it on the board just so you don't forget we're trying to estimate the expected value of F of X, where X comes from P, and we have.",
            "Samples.",
            "Why that comes from some distribution Q OK.",
            "So how do we do this?",
            "Oh, there's one more thing we're going to define these terms because I'm going to be very precise in our derivation.",
            "We're going to find capital P to be the support of P Capital Q to be the supportive Q and capital F to be the supportive at what is support.",
            "So the support of any of these, let's use P, for example.",
            "The supportive P is the set of X such that P of X is not zero.",
            "Right, so it's just all of the outcomes that have nonzero probability under this distribution P, yeah.",
            "Will come to that will come to that for now, we're not assuming that yet, but we will assume something related to that.",
            "OK, any questions?",
            "Who?"
        ],
        [
            "OK so I need to go faster.",
            "OK, So what is the important sampling estimator?",
            "This is the important sampling estimator.",
            "I'll just state what it is and then we'll derive why it's a reasonable scheme.",
            "So given our one sample of this random variable Y, the important sampling estimate for the expected value of F of X.",
            "So given one history from the behavior policy are estimated, the performance of the valuation policy is this.",
            "So this is our weight and that would be the return.",
            "So this is the wait.",
            "What is this weight?",
            "It's the probability of the outcome under the evaluation distribution or the target distribution divided by its probability under the distribution that we're actually sampling from.",
            "So if this would be more likely under the distribution that we wish we had P, then this would be larger than one, and if it's less likely this will be smaller than one.",
            "So this is exactly the intuition that we talked about.",
            "But why is this ratio really what we want?",
            "So let's workout what is the expected value of this important sampling estimator.",
            "OK, so the expected value of this important sampling estimator is, well, let's write it out.",
            "So we sum over all the possible events Y and we have Q of Y.",
            "'cause why comes from the distribution Q times the the estimator is just the definition of expected value?",
            "Why am I doing Y in Q here so?",
            "We can definitely we have to sum over the support of this function Q But why don't we sum over anything else?",
            "We don't sum over anything else because for wise that aren't in Q, we'd be dividing by zero, and so this expected value would be undefined.",
            "So really, for this expected value we can only some over the elements that are in the support of Q.",
            "That distribution.",
            "OK, So what can we do next?",
            "Will just do a change of variable.",
            "So why is just a variable name here?",
            "Let's just call it X instead.",
            "That's all we've done here.",
            "OK. What did I do next?",
            "What I did next is I cancelled this Q with that Q 'cause it's on the top and on the bottom those go away and so we get P of XF of X.",
            "Or the only terms left.",
            "But we've split our sum into different components so I guess a Venn diagram could help here so we have.",
            "P. And Q and what we wanted to do is some over Q which is that region right there.",
            "So what did we do?",
            "We split that into three sums.",
            "We summed first over P. So we included all of this.",
            "Then we summed over not P but not P&Q.",
            "So that's all of this, not including the parts that are in P&Q.",
            "And then we subtract it off the bits that are in P and not Q.",
            "So what is in P and not Q?",
            "That's all of this.",
            "So we subtracted this back off and that means we're just summing over Q, so we haven't changed anything.",
            "We've just rewritten.",
            "We've split this sum into three different components, right?",
            "OK.",
            "So here I'm keeping the first term and keeping the second term, but we remove the middle term.",
            "Why did we do this?",
            "So for any X that is not in peace.",
            "Remember, capital P is the supportive P. It's the values for which P of X is not zero, so if you're not in PP of X is 0.",
            "So these terms are all zero for every term in this sum, so that middle some is just zero.",
            "So we can get rid of it.",
            "OK, so how do we go from here to getting the expected values what we want?"
        ],
        [
            "We need to make some assumptions, so on this slide I'll make one set of assumptions and on the next slide I'll make a different set of assumptions.",
            "So the first set of assumptions that we can make is that P is a subset of Q.",
            "So what does this mean?",
            "The distribution that we wish we could sample from is a subset of the distribution.",
            "Sorry this is a subset of support of the distribution that we actually have samples from.",
            "So in other words, if we'd like to know the expected value under one policy, we need to be able to see everything that policy could do in our the policy that we ran the policy that we're running never takes some action in some state.",
            "We don't know what that will do, and we can never know what that will do, and so our evaluation policy can't take that other action in that state.",
            "We need to be able to see everything under the sampling distribution.",
            "That could ever happen under the evaluation distribution, so that's what this assumption means.",
            "You can actually relax this a little bit exercise for you to get home later, so let's complete the proof with this assumption, and I'll remind me later if I forget what this assumption exactly means.",
            "In our case, it's something that we can check very easily.",
            "OK, so let's complete the proof.",
            "This is just copying the last line from before.",
            "I've removed the second term.",
            "Why did I do that?",
            "So P is a subset of Q.",
            "So that means we have P&Q, so this term is summing over the elements that are in P and not Q.",
            "So there NP and in not Q.",
            "So it has to be both here and out there.",
            "That's nothing.",
            "So this is something over the empty set.",
            "So this term is now 0 and we get this.",
            "That's just the definition of the expected value of F of X.",
            "So we're done.",
            "So we have that in this case.",
            "Given this assumption, the important sampling estimator gives an unbiased estimate of this expected value.",
            "So thinking back to our case, the ratio of the probability of the outcomes under the evaluation policy and the currently run policy times the returns is an unbiased estimate of the true performance of this evaluation policy, and I will flush that out in detail in a moment.",
            "First, I want to show that we can complete this with a different set of."
        ],
        [
            "Functions, so let's instead assume that the function we're trying to estimate F of X, which in our case is returns is non negative, so it only takes positive values.",
            "What can we do then?",
            "Well, this is just copying what we had from the end of that previous previous slide.",
            "What happens to the second term here?",
            "P is positive.",
            "Well I just gave away the answer.",
            "P is positive, F is non negative.",
            "So we're subtracting a positive term.",
            "So our expected expected value is going to be less than this value, which is just the expected value.",
            "So we don't have an unbiased estimator.",
            "We have an estimator that has negative bias.",
            "So the expected value of this important sampling estimator will be less than the true performance of our evaluation policy, which if you think to what are we about to do with this is going to be completely OK.",
            "So what are we going to do with this?",
            "We're going to predict the performance of a policy and then ask, is this performance at least this good?",
            "If we're underestimating the performance of this policy, we're just going to be conservative.",
            "So Java.",
            "Drop.",
            "So there could be that.",
            "What is the problem you're gonna stand for that license to buy here?",
            "Sorry I'm having trouble hearing there a lot of squeaking chairs.",
            "Positive probability sample advice if you buy is here.",
            "No, we're in the discrete setting for here at least, so we would never sample something with Q of why being zero.",
            "That's saying I'd sample an event with zero probability.",
            "And in the continuous case, this would be a density function and the density wouldn't be 0.",
            "For something like sample.",
            "And in the yeah, I don't know in the full measure theoretic case if this holds up so important sampling works in the measure theoretic case where this ratio is the radon nikodym derivative and you need to absolute continuity and I don't know if you need absolute continuity.",
            "Anyway that's a different question.",
            "Yep.",
            "Sorry, there's some weird noises also.",
            "Learning problems.",
            "Then mass functions you mean, yeah, so this definitely works for density functions, and I suspect it works for the full measure theoretic probability case.",
            "Certainly if you have this assumption, which really means that I always mix this up, I think it's P is absolutely continuous with respect to Q.",
            "If you have that, then this works in the measure theoretic setting.",
            "Once you have this support.",
            "Do we need to be?",
            "Ah.",
            "We so in arc.",
            "In the case that I've done and seen, these are always probability distribution, so they're normalized to integrate to one or sum to one.",
            "If these were distributions that weren't probability distributions, I'm not sure.",
            "Yes, I will get to the ratios.",
            "I think I see where this might be going, so I think I will answer that later.",
            "Yes, I will answer that OK.",
            "So we said.",
            "This."
        ],
        [
            "OK.",
            "So let's apply this to reinforcement learning.",
            "Let's get out of this X&Y and go back to RLOX was the thing we wish we had, so these are histories produced by the evaluation policy.",
            "We have histories produced by the behavior policy.",
            "The distribution P then is the distribution over histories under the valuation policy.",
            "Q is the distribution over histories under the behavior policy.",
            "And the function that we're evaluating F of H or F of X is the return the observed sums of rewards.",
            "OK, So what is then the target value we're trying to predict the expected value of F of X, so that's the expected return given that our trajectory's X come from the evaluation policy, it's the performance of the evaluation policy, and we're assuming that either the support of the evaluation policy is a subset of the support of the behavior policy, or that the returns are non negative.",
            "I I'll talk about this more later.",
            "I think I don't ask at the end.",
            "OK, So what does this mean?",
            "Let's plug all of this in and get our important sampling estimator.",
            "It is this the important sampling estimator from one history coming from the behavior policy is the probability of that history under the evaluation policy divided by the probability of that history under the behavior policy times the return.",
            "So think back to our intuition.",
            "We want this weight to be large in the case where this is more likely under the evaluation policy.",
            "That's what this does.",
            "If this is more likely under the evaluation policy, this is greater than one.",
            "It's less likely this is less than one, so this is our important sampling estimator for RL.",
            "This is the estimate from a single history, but we have many histories.",
            "Our data consists of many different episodes, so if we have many, many histories, that's our data set D. We just average the important sampling estimates from each history, or put differently, it's the average over end histories of the important sampling estimates where this Artie is the reward at time T in that ice history.",
            "OK. That question in the back was a very good one.",
            "It was, but how do we compute these things?",
            "How do we compute this importance?",
            "Wait, we don't know the probability of a history under a policy so."
        ],
        [
            "Let's look at some more.",
            "So what is the probability of a history under the evaluation policy that we put on the top?",
            "Well, let's just run through all the terms of that history in sequential order, so it's the probability of the initial state times the probability that we take the first action that we took in that state.",
            "The probability that we transition to state S2 and get reward are one.",
            "Given that we were in that state and took that action.",
            "And this is making the Markov assumption times the probability that we take the second action that we took times the probability of the second transition etc etc, right?",
            "This is just writing out the probability of a trajectory.",
            "The problem is we don't know these transition probabilities.",
            "We know our policy, but we don't know these.",
            "But when we divide by the probability under a different policy, these terms are going to cancel.",
            "We get all these same exact terms except for the action selection parts.",
            "The policy terms.",
            "So when we cancel all the probability of S1, the probability of the first transitions were left with just the Pi terms and we know pie.",
            "That's the policy we're thinking of deploying in our current policy.",
            "So put differently.",
            "That importance ratio is the product overtime from initial time until the total horizon of our problem over the ratio of the action probabilities at each time step.",
            "OK.",
            "So."
        ],
        [
            "Let's just plug that in.",
            "This is what we had before we plug in that ratio, and we get this term in the middle.",
            "And this is our important sampling estimator, so hopefully you could implement this at this point.",
            "If I have a data set, this is average.",
            "My weighted returns where there should be a little I hear in the reward.",
            "So this is the action at time T in the ice history.",
            "So this is all in the history and this is the return.",
            "So if I gave you a data set and a behavior policy and evaluation policy, you can just implement this summon.",
            "That's an unbiased estimate of the performance of this policy, and I got a little bit greedy in making this and thought I would go much, much faster.",
            "So at some point we'll skip something and I think we'll skip this, so we're going to skip.",
            "Will go through it really quickly.",
            "It'll be fine, so.",
            "Let's do something different over there question.",
            "So yes, so if Pi is deterministic, that's completely OK. That means that this could be one, and this is anything that's fine.",
            "Pi B cannot be deterministic if Pi B is deterministic, then this will actually always be one.",
            "It will never be 0.",
            "But if this is always one, what ends up happening?",
            "We have that support problem and in the deterministic case that support problem means again, we're going to underestimate and if it's deterministic it's going to be 0.",
            "So what will happen is our estimate will just always be 0 if we run a deterministic policy and the evaluation policy is different.",
            "Yeah, so keep the behavior policy stochastic if possible and if it's a if there's some cases where it's deterministic, try to match that deterministic part in your evaluation policy.",
            "OK."
        ],
        [
            "So here's an idea.",
            "Instead of using importance sampling to estimate the expected returns, let's use important sampling to estimate just one.",
            "Reward their reward at time T. So this is something else that we can do instead of estimating the whole thing, just do one piece.",
            "What do we get?",
            "Well, the important sampling estimate for the reward at time T is average oversamples.",
            "The weighted reward at time T were here, this this.",
            "This wait is the probability not of the full history, but of the history up until time step T. Why is that?",
            "It's because.",
            "The history after time T has no bearing on the reward that we see at time T there.",
            "It doesn't, so knowing the history afterwards doesn't impact our estimate of this intuitively, and you can show that this remains unbiased.",
            "OK, so this is how we would use important sampling to estimate one reward.",
            "So what we could do is sum over these.",
            "We can, we can estimate the sum overtime.",
            "What are we doing here?",
            "This is the other equation, so this is actually computing this ratio, so we put the 1 / N some reward and we're asking what is this ratio?",
            "Well, it's just the product of action probabilities to time T instead of to L, the end of the episode.",
            "So if we have this estimate of RT under the evaluation policy, we can do per decision important sampling, which says we're going to just take the weighted sum of these estimates from each time step.",
            "So estimate the first reward.",
            "Add to that our estimate of what the second reward will be.",
            "Add to that our estimate of what the third reward will be, etc.",
            "And we get an estimate of the total return, and that looks like this.",
            "So this is just plugging in the sum from T = 1 to L gamma T of our estimate, using importance sampling for the reward at time T. OK."
        ],
        [
            "I'm going to skip this slide.",
            "Actually, I want to point something out so we're going to skip this slide, but just look at this and what happens in important sampling.",
            "If the evaluation policy and behavior policy are very, very different.",
            "So what tends to happen here we get this is really the probability of histories, so we get histories where the probability of the behavior policy is larger.",
            "The probability under the evaluation policy is smaller, so we tend to take actions that are likely under our sampling distribution.",
            "So this tends to be larger.",
            "Tends to be smaller.",
            "And so if we have a long sequence of these, this tends towards 0.",
            "So as we get longer and longer episodes, or as these two policies become more and more different, the importance weights tend toward zero, and so this means that when these policies are sufficiently different or when horizons are sufficiently long.",
            "It's not here.",
            "Important sampling has high variance and it tends to just always say the estimate is 0.",
            "The estimate is 0, the estimate is 0, which is a terrible estimate."
        ],
        [
            "There's the question of right?",
            "So here's the case where important sampling tends to be zero when these are different.",
            "So what is important sampling doing?",
            "Initially?",
            "It's estimates tend toward zero, and as we get more and more data, they tend towards the correct value and this isn't necessarily the behavior that we want."
        ],
        [
            "So here's an idea.",
            "Ignore that.",
            "Ignore this.",
            "This came for something I didn't talk about.",
            "Can we make a new estimator that is going to have much lower variance than important sampling, but has some bias so it's no longer going to be unbiased, but it's at least going to be consistent, so it will converge to the right."
        ],
        [
            "Value and this is what weighted importance sampling does.",
            "So what is weighted important sampling?",
            "Well, WI is our importance weight.",
            "The important sampling estimate was just this.",
            "The average of our weighted returns, and you can push the end in there onto the weight terms.",
            "So I just rewritten important sampling.",
            "What weighted important sampling does it says instead of dividing by N, let's divide by the total sum of our importance weights.",
            "And here we're back to ordinary important sampling, not her decision for now.",
            "So what does this do?",
            "So you can view this whole term as a new weight, and what we've done is we've normalized these weights so that all of them sum to one.",
            "So you can view this as a weighted average of the observed returns, and this has some real."
        ],
        [
            "Nice properties, so first what is weight?",
            "Important sampling equal to if we have only a single history I will take a break from talking to let you think about this.",
            "Does anyone have a?",
            "An answer.",
            "What was that?",
            "Error.",
            "It equals the original policy, so it equals.",
            "I mean, it's not a policy, it's a number.",
            "It's the you're getting at it.",
            "Yeah, it's the return from the the behavior policy so.",
            "So what happens is we only have one wait.",
            "We divide by that wait.",
            "So these are just one.",
            "And So what we get out is it's the observed return under our behavior policy.",
            "This is the Monte Carlo estimator of the performance of the behavior policy.",
            "We'd like to evaluate the evaluation policy.",
            "This is estimating the performance of the behavior policy.",
            "What happens, though?",
            "As N increases, so there's this nice property which I won't run through, but it's straightforward to show that the expected value of these importance weights is always one.",
            "So that means that this sum in the limit is converging to N, and so in the limit is we get infinite data that some of importance weights converges to end almost surely.",
            "And when this is N, that's important sampling.",
            "That's ordinary important sampling, so we have as an estimator that begins as a Monte Carlo estimator of the performance of the behavior policy, and then as we get more and more data, it acts more and more like important sampling.",
            "So it's correct asymptotically, but it ends up having much much lower variance."
        ],
        [
            "And that is all I will say for weighted important sampling.",
            "So we've gone through three forms of important sampling, important sampling, weighted important sampling, an per decision, important sampling.",
            "There are many other types.",
            "You can also do weighted per decision, which is sometimes called consistent, weighted per decision.",
            "Also you can do special forms of important sampling when those supports aren't equal.",
            "You can ignore important sampling and just build a model of your MVP and compute the performance of your valuation policy on that model of an MVP.",
            "You can try to combine these model based estimators with important sampling and that's what these two.",
            "Doubly robust and weighted doubly robust do.",
            "You can also recognize that the environment might not be stationary, and so there's some time series prediction in this and combine important sampling time series prediction and there's one last one that does more combination of the model with important sampling and they'll be references for all of these at the end, so there's just to point out there's many more techniques for up."
        ],
        [
            "Policy evaluation OK, So what do these results typically look like?",
            "This is a typical plot for this is actually a toy domain, but it's representative that shows the amount of historical data that we have and the mean squared error of our different estimators.",
            "And this really is what we tend to see.",
            "These are log axes, so these are order of magnitude differences in error of these estimators.",
            "So up here is important sampling per decision.",
            "Important sampling tends to do a little bit better, it's lower variance, and if we jump to weighted important sampling, that's the.",
            "Yellow line their weighted important sampling does roughly an order of magnitude, in this case better than ordinary important sampling, but that's a trade off.",
            "It's no longer unbiased, so we're getting some bias, but that bias estimator is much more accurate and then waited per decision does a little bit better.",
            "Still.",
            "Next up is the doubly robust estimator that's combining with the model that's blue, and I think weighted WS is kind of if you want just the most accurate estimate, it's one of the state of the arts at the moment, and a key thing of this plot is to show yes, I've described these more sophisticated methods and it's tempting to say.",
            "Oh well, I'll just implement important sampling first.",
            "This is.",
            "Three orders of magnitude approximately difference in performance, so it is important to use these more sophisticated techniques if you're actually trying to do policy evaluation."
        ],
        [
            "OK, so let's go on to the next step, which is how do we do high confidence off policy evaluation?",
            "So we're creating this new black box that doesn't just predict the performance of the policy at lower bounds.",
            "The performance, so it says that new policy will be at least this good.",
            "That was the wrong button, OK?"
        ],
        [
            "So the way that we're going to do this is we're going to use our important sampling estimates, and we're going to put them through some method that takes our end important sampling estimates and spits out a confidence bound on their mean."
        ],
        [
            "And one way of doing this is using half things inequality, so let's review Hopkins inequality quickly.",
            "Let X one to XN BN independent and identically distributed random variables that are between zero and B.",
            "So the example I like to use for this is, let's say that we're going to measure let's pick N is 30.",
            "I'm going to pick 30 people from the surface of the earth with replacement so that it's identically distributed, and I'm going to measure their Heights.",
            "They measured the Heights of 30 different people.",
            "We need these to be bounded between zero and B, so you can't have a negative height, so we're bounded by zero.",
            "We need an upper bound of B, so the easy way to do this is you just define if someone is taller than 3 meters were just not defining them as human, so it's zero to three meters.",
            "OK, that should not have shown up yet.",
            "Pretend this isn't here yet.",
            "So what huffing says is with probability at least one minus Delta.",
            "This inequality holds, so the true expected value of our random variable, the true mean height of humans is at least the sample mean minus this term.",
            "So it's the sample mean that we observed minus B.",
            "That's the range of the random variable times root log one over Delta that comes from the probability that we want over 2 N, where N is the number of histories, the number of trajectories that we have.",
            "So.",
            "We know all of these terms.",
            "We could implement this for example, and also notice that this matches intuition.",
            "So the true mean height of humans is at least our sample mean minus some term in this term says how far down you have to go given the number of samples that you have a nice property of this is we're not making any other assumptions.",
            "Nowhere in here are we saying these XI or normally distributed or anything of that sort, regardless of their distribution.",
            "This will hold.",
            "So how do we use this for important sampling while it's given away?",
            "Or for reinforcement learning is given away by the part that came too soon.",
            "We're just going to plug our important sampling estimators in here.",
            "The important sampling estimator is an unbiased estimate of the true performance of this evaluation policy, so the expected return of the evaluation policy is at least our important sampling estimator minus this, so this gives us a lower bound."
        ],
        [
            "Unfortunately, this doesn't work very well, so if we actually plug this in for mountain car where we normalize returns so that they are always within 01 and we pick an evaluation policy whose performance is .19, so it's not a very good policy.",
            "We can then pick 100,000 trajectories of data of historical data.",
            "Remember on that other slide we solve this in something like 2 trajectories, so we're going to take a ton of data and we're going to then lower bound.",
            "The performance of this evaluation policy using these 100,000 trajectories from, I think the uniform random policy.",
            "So what are we doing?",
            "We're taking those 100,000 trajectories, computing the importance weighted returns.",
            "Then we're taking those an subtracting off from the importance weighted returns this term B root log one over Delta over 2 N, and we get negative 5.8 million, which is useless.",
            "'cause we normalized returns to be between zero and one.",
            "So this is just not at all useful lower bounds.",
            "So what went wrong?"
        ],
        [
            "What went wrong?",
            "Ignore this, they skipped over it.",
            "What went wrong?",
            "Is this range term B?",
            "We use the range of our random variable and the important sampling estimator can have a huge range.",
            "The range scales with exponentially with the horizon of the problem, so how?"
        ],
        [
            "We fix this.",
            "Well, we can just not.",
            "Yusuf things inequality there.",
            "More sophisticated sophisticated concentration inequality is that we can use.",
            "This is one of them.",
            "And this is actually the one that I would recommend.",
            "It comes from the triple AI paper which looks really scary, but it's all terms that we can know.",
            "It's just a matter of plugging in values and letting it chug along.",
            "And if you use remember the actual performance of this policy is 0.19.",
            "Hufton was negative, 5.8 million.",
            "You might say using Empirical Bernstein bound if you're familiar with those.",
            "That also doesn't work.",
            "There's an obscure one called Anderson in mass starts inequality, which actually works reasonably well.",
            "We can get much better using this cut inequality, so just use a different inequality, but the intuition is the same.",
            "There's another thing that we could do, which is forget using these exact concentration bounds and make some false assumptions.",
            "So which I say too lightly, so this is students T test.",
            "It says if the sum of RN random variables is normally distributed then this inequality holds the true sample mean.",
            "Sorry, the true mean is at least our sample mean minus this other term where this is the standard deviation and that's the T statistic and this tends to be much, much tighter than nothing's inequality, but it's relying on the assumption.",
            "That the mean of our random variable variables is normally distributed.",
            "Thankfully, the.",
            "Central limit Theorem Yes tells us that the sum of any number of IID random variables tends to be normally distributed as N increases, so this isn't a terrible assumption to make.",
            "It's often largely true, and there's another nice result, which is that if the rewards are not negative, then the T tests tends to be conservative, and this has to do with how this has to do with how the test works.",
            "When I have different skews on my random variable, so I'm going to skip over Bootstrap, it's just another competing way.",
            "This tends to mean that if I ask for some probability of failure.",
            "And I look at the number of samples that I have and I ask how often is this concentration bound failing?",
            "If I use half things or any of the other concentration bounds, they pretty much never have error rates that match Delta, and that's 'cause they're overly conservative.",
            "They allow for any distribution of data, whereas when I use students T test for these important sampling estimates, it tends to still be conservative, whereas bootstrap methods give tighter concentration tighter bounds, but sometimes have higher error rates than we want.",
            "And these are just results saying that yes, for mountain car we can actually do this.",
            "We can lower bound the performance of this policy, and the big takeaway is these are most of the methods that we talked about.",
            "This is using weighted and poor decision with the bootstrap confidence bounds.",
            "Key.",
            "Thing is this is zero to one and this is zero to negative, 5 to the 45th.",
            "This is huffing with important sampling so it just reiterates.",
            "You can't just use half things inequality important sampling and get this to work.",
            "You have to use some of these more sophisticated methods.",
            "We also did this on actual digital marketing data and argued that some new policy for digital marketing would be better than existing policies, and I'm going to skip over this quickly.",
            "And essentially just says what I've said here.",
            "If you want it later is pseudocode, which I have not verified beyond mouse.",
            "So if you find a mistake, email me and I will correct it, but I think it's right.",
            "This is pseudocode for doing weighted per decision.",
            "Important sampling with students T test so you can go through this and implement it and it will give you a lower bound that holds with probability 1 minus Delta on the performance of this evaluation policy approximately OK.",
            "So let's talk about the last thing which is safe policy improvement.",
            "We wanted to make that box that took the historical data the probability and produces a new policy that gives our safety guarantee.",
            "How do we do this using the components that we've put together so far?",
            "Well, we do it like this.",
            "So we take our historical data and we split it into two sets.",
            "The first set is we're going to call the training set and the 2nd is the testing set and this is completely arbitrary.",
            "But for now we tend to split it into 20% data in the training set and 80% in the testing set.",
            "We then take the testing set and we pick one single policy Pi C that we think is going to be a really good policy.",
            "So you could in your head imagine that we're running fitted Q iteration on this batch of data and saying, here's the policy that we predict is going to be best.",
            "So we pick a policy that we think is good from this training set.",
            "Just the batch RL problem.",
            "We then take that policy and use the other 80% of our data to run a safety test.",
            "What is the safety test?",
            "Well, it's our black box from the previous section.",
            "We just compute a lower bound on the oops we compute a lower bound on this policy's performance.",
            "Using are 80% of the data.",
            "So we take that 80%.",
            "We use important sampling to predict this policy's performance, and if we were using huffing, we then subtract off that term.",
            "From huffing, you're using T tests.",
            "We subtract off the standard deviation of the important sampling estimates divided by root N times the pizza statistic.",
            "And that's all that we do, so we use one of these high confidence off policy policy evaluation methods.",
            "So once we have a lower bound on this policy's performance, we check is that lower bound, at least the performance of our current policy.",
            "If it is, then we deploy the new policy.",
            "If it's not, then we don't deploy the new policy.",
            "We say no solution found.",
            "We say my lower bound was too was too low.",
            "I'm not getting the guarantee I want.",
            "So say stick with the current policy.",
            "OK, there's a catch, and the catch is shown by this figure, so let this blue box be the space of all possible policies and let's talk about how we choose the candidate policy.",
            "So this is our current policy and this is the policy that maybe we predict is the best policy from our training data.",
            "Then maybe actually this policy is better.",
            "So maybe this gradient shows that policies out in this direction perform better than policies out in this direction, so performance gets better as we go this way.",
            "However, the tightness of our lower bounds goes the other way, so remember important sampling.",
            "We get higher and higher variance as the policies become more and more different because those ratios get bigger and bigger.",
            "Important sampling tends to give that output of 00 our bounds get looser and looser as we get farther and farther away from this current policy, not just in this direction.",
            "Anytime we get farther away from our current policy, our bounds tend to get looser.",
            "So this policy might be one that's much, much better, but the lower bound that we got on its performance can be really, really low, and so we could say we think this is great, but we can't guarantee anything, and our algorithm would return no solution found, so the optimal policy, the optimal candidate policy to return is somewhere in between.",
            "It's one that balances this tradeoff.",
            "It's one that we predict back here.",
            "The optimal candid policy is one that we predict is going to pass this safety test and subject to that, it's the one that we think is going to perform the best.",
            "So it's the one where we don't go so far that we're unable to get a tight bound on its performance, and so we say it's unsafe.",
            "But we go as far as we can to get as much improvement in performance, and that just high level intuition.",
            "What this ends up meaning is we need some sort of regularization to keep us close to our current policy.",
            "OK, so in the last five minutes, let's just go through a couple of results to say yes, this really works.",
            "It's not just.",
            "Math theory to say hey, apply these bounds.",
            "OK so these are results from mountain car modified shorter horizon, so whenever you take an action agent keeps taking that action for 20 time steps.",
            "We're running the random policy.",
            "Oh first we we normalized returns so that returns fall between zero and one.",
            "So the optimal policy has an expected return of 1.",
            "This where this black bar is actually the performance of the random policy.",
            "In this case.",
            "What we're doing is asking our safe policy improvement.",
            "For them to produce a new policy that with 95% probability is better and actually in all of the results will show we're going to use 95% probability and we're using different variants of the algorithms and different amounts of historical data.",
            "So using just 50 trajectories, these black bars correspond to using that cut inequality and exact concentration bound, whereas these dashed bars correspond to using an approximate one, so they're actually using a bootstrap method called BCA.",
            "But intuitively you can think of this as being very similar to using students T test.",
            "To do that, bounding so using that approximate concentration bound were actually able to get policies returned sometimes with just 50 trajectories, using just 50 trajectories where sometimes getting here's a policy that is better and this will not be wrong more than 5% of the time, approximately when we're doing the dashed bars, 'cause of students T test as we go up, the approximate methods are solving mountain car in 200 to 900 episodes, whereas the exact methods take more but.",
            "With 5000 trajectories, we're just instantly jumping to an optimal policy, so this isn't the same as a perfectly tuned natural actor critic, for example, but we're not taking centuries worth of data in order to do this.",
            "We can also do this incrementally so we can run this repeatedly.",
            "We can gather 50 trajectories, run our algorithm, got there another 50 trajectory's, run our algorithm, and then we get these learning curves, and so this is mountain car using the student's T tests like method, the Bootstrap method and this is using our exact methods, so we're solving it in just hundreds of episodes which when we look at it now, we think, Oh well, we can solve this really quickly with an unsafe method.",
            "This seems like, oh, that's only OK, but to put some context on this, when we started trying to do this back in 2014, if we tried to do this.",
            "This would take us millions and millions of episodes, so by all of these advances in policy evaluation, we're able to get this to kind of practical, tractable amount of data.",
            "We can also do some other cool things, so instead of saying I want your performance to be at least the performance of the current policy, there's nothing stopping us from saying you need to be significantly better, or I'm not going to change anything, so this is mountain car where the red correspond to different runs of our algorithm, and we're saying guarantee that you're at least so this is the performance of the current policy.",
            "Guarantee that you're at least the performance of this blue bar before you change the policy.",
            "So I need some significant improvement before you change anything, and our algorithm waits until it has enough data to change the policy so that it's better than the blue bar and it's still being conservative when it does that.",
            "This to give some context is the performance of just a well tuned natural actor critic, so we're not lagging too far behind it.",
            "OK, we also applied this to some simulated digital marketing experiments.",
            "So just to summarize these results, they say that using a few hundred 100,000 users data we are often able to say here's a policy.",
            "It is better than what you're currently doing for serving up advertisements, so we're right in the ballpark of where we want to be.",
            "This again results from a different internal simulator at Adobe Research.",
            "This was a simple toy problem that we mocked up just to illustrate.",
            "Again, the safety property and this is a very simple mockup of a digital marketing problem which the main difference between why this takes hundreds of thousands of users data is that almost no one clicks on advertisements, so our reward signal is very sparse and in this mock up we just pretended that people actually like dads and they click on them so we get higher click rates.",
            "So this curve here is a natural actor critic with optimized hyperparameters, and if we just take the step size and I think it was .01 or something and we change it to .02, it learns faster and then it completely diverges and this is the problem.",
            "We have these hyperparameters that when we set them wrong it doesn't work.",
            "This is the very first time we ever ran our algorithm on this problem.",
            "It's guaranteed that it's not going to produce worse performance.",
            "It's not going to diverge down to worst policies, so we are slower, but we're giving that safety guarantee, and I'd like to wrap up with that illustrative example that I started with, which is diabetes treatment.",
            "So this was the problem of can we decide how much insulin to inject in order to keep someone's blood sugar in your optimal levels.",
            "The study by Mason Bastoni at the University of Alberta, showing that this is an RL problem, used a simulator called T1DMS.",
            "And this is a simulator of this entire metabolic process, and in the US, if you want to do studies on one of these controllers on humans, you have to 1st do studies on rats.",
            "Or you can use this simulator.",
            "So it's not just an arbitrary simulator, we act together, it is a high quality simulator.",
            "So we tried to reproduce their results, but using one of these safe algorithms and this shows the amount of data we're using for a simulated subject.",
            "The amount of days of data that we took.",
            "And the probability that we're able to change the persons treatment policy to be something different.",
            "So how often were we able to say here's something that's different from the current policy while ensuring that with 95% probability we don't make matters worse, and there is another catch we did something else a little bit different.",
            "Here we really care mainly about hypoglycemia, low blood sugar.",
            "So our probabilistic guarantees with respect to low blood sugar.",
            "So he said try to keep blood sugar near optimal levels while ensuring that we don't increase the amount of low blood sugar that we get.",
            "So where is this where we wanted three to six months?",
            "That's 92180 days, right?",
            "Within that window is where we're able to return policies that are better.",
            "So we're using the amount of data that we want.",
            "What is the spot this plot shows?",
            "What is the probability that we actually did something bad?",
            "So we asked for a 5% probability that we increase the prevalence of hypoglycemia.",
            "So given different amounts of data, how likely where we to produce a policy that's worse?",
            "Where the blue line here, not once across any of our trials, did this increase the prevalence of hypoglycemia, and that's because we're still using concentration bounds that tend to be overly conservative.",
            "What is the red line?",
            "This is what you get if you use a well tuned standard RL method and it kind of explains what's going on 1st.",
            "It always returns policy and what happens is early on we're getting some data set and it's random.",
            "There's a lot of randomness in when meals are eaten, errors and how big the meal actually was.",
            "And because of this randomness in the data set, we can draw false conclusions.",
            "Randomness in the data set can make us think that something is better when it's really just a fluke of the data, and so that's what's happening here we're choosing policy is to return that we think are better, but really was a fluke of the data that made us think this and what our algorithms are doing is they're looking both at the policy that we're proposing and the data we used to create it, and asking, can we conclude that this isn't a fluke of the data that this policy really is better?",
            "And if it is, then we start returning it so you'll notice right when these policies tend to typically be?",
            "Actually, better policies we start frequently returning policies.",
            "Yep.",
            "We don't, so that was one of the limitations of the setting at the start is we assume that we have an initial policy that we're willing to deploy if we have to do cold starts, and we're trying to ensure this sort of safety, you need something else.",
            "You need something like a prior distribution over models, and that's when you get to kind of the adaptive control style approaches.",
            "But this does not solve cold start.",
            "So to wrap up, in my last negative one minute.",
            "Just to conclude, there are many different definitions of safety.",
            "We talked about one ensuring that policies are better with high probability.",
            "We went through the three steps to creating a safe algorithm predicting the performance of a policy without deploying it, bounding the performance of a policy without deploying it, and then putting those pieces together to actually do policy improvement.",
            "And then we talked about some empirical results.",
            "To say this is actually possible.",
            "We can do this using realistic amounts of data and some future directions which you can look at later offline.",
            "There are many open questions still to make this better.",
            "And that's kind of shown by the list of references.",
            "A lot of these are 2017 2016 references.",
            "This is something that's kind of happening now ish, so there are a ton of open questions left to be answered.",
            "And if you'd like to know more about this, the 1st place to start really is doing a pre cups paper from 2000 which goes into important sampling.",
            "Though the weighted precision estimator is a typo.",
            "Something to be wary of and that's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And thanks again for organizing the summer school and inviting me to come give a talk.",
                    "label": 0
                },
                {
                    "sent": "So today I'm going to talk about safe reinforcement, learning and safe reinforcement learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "But I guess I'll start with just an.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overview of what I'm going to talk about in this talk so you know what to expect.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start off by just giving some background.",
                    "label": 0
                },
                {
                    "sent": "Make sure we're on the same page about notation and I'll give some motivation for why it is that we need safe reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "I'll then talk about what I mean when I say a safe reinforcement learning algorithm and also what some other people mean.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you a very quick overview of some other definitions of safety within reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I'll then talk about three different steps towards developing a safe our algorithm.",
                    "label": 1
                },
                {
                    "sent": "There three subcomponents through three subproblems that we can actually study pretty much independently, and that's off policy evaluation.",
                    "label": 1
                },
                {
                    "sent": "High confidence off policy evaluation, and then actually constructing this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then finally, I'll talk about some experimental results to show that this isn't just theoretical talking.",
                    "label": 0
                },
                {
                    "sent": "This actually works, and then I'll conclude and talk about some possible future directions and also give references all kind of there at the end.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so first background.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've seen this repeatedly over and over, but we're in the IRL setting so we have an agent interacting with an environment that agent is observing the current state of the environment, choosing an action that action changes the state of the environment in the agent gets that reward signal just the standard MDP setting, and our goal is to find an optimal policy that's the optimal mapping from states to actions.",
                    "label": 0
                },
                {
                    "sent": "And in this talk we're going to generally focus on episodic MDP's, so this this cycle repeats for some number of steps and then it stops, which you can view as we enter some.",
                    "label": 0
                },
                {
                    "sent": "Terminal absorbing state.",
                    "label": 0
                },
                {
                    "sent": "We reach a state from which we never leave in.",
                    "label": 0
                },
                {
                    "sent": "The rewards are all zero, so it's an episodic setting.",
                    "label": 0
                },
                {
                    "sent": "Also, we're going to deal with the case where this mapping this policy can be deterministic or stochastic, but there's going to be some cases where we require stochastic policies.",
                    "label": 0
                },
                {
                    "sent": "So when I say stochastic policy, I just mean that the policy produces a distribution over actions it doesn't deterministically select match.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what notation am I going to use for this talk?",
                    "label": 0
                },
                {
                    "sent": "I'm going to use Peiffer policy soap.",
                    "label": 0
                },
                {
                    "sent": "I VE given S is the probability of action in state S. I'm going to use H for history and I'm going to say history instead of trajectory, and that's just to simplify some notations that we don't use T for time and trajectory.",
                    "label": 0
                },
                {
                    "sent": "So H history is just an observed sequence of states actions and rewards.",
                    "label": 0
                },
                {
                    "sent": "And I said we're going to focus on the episodic setting where we terminate after some amount of time, and so L is going to be the horizon of this problem.",
                    "label": 0
                },
                {
                    "sent": "The number of decisions that we have to make.",
                    "label": 0
                },
                {
                    "sent": "Before this process terminates.",
                    "label": 0
                },
                {
                    "sent": "OK so H is a history or a trajectory or an episode.",
                    "label": 0
                },
                {
                    "sent": "The historical data D. So I'm going to talk a bunch about this historical data later.",
                    "label": 1
                },
                {
                    "sent": "It's just going to be a set of observed trajectories or histories.",
                    "label": 0
                },
                {
                    "sent": "It's data that we've collected from running some existing policy.",
                    "label": 1
                },
                {
                    "sent": "And this historical data was generated by running a policy that will call the behavior policy Pi B.",
                    "label": 0
                },
                {
                    "sent": "And our objective, as discussed, discussed before, is expected return.",
                    "label": 0
                },
                {
                    "sent": "So we want to find the policy that maximizes the total amount of reward that we get.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to use the symbol J to note the performance of a policy pie, which is just that expected.",
                    "label": 0
                },
                {
                    "sent": "Sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "Given that we're using that policy so pretty standard notation, it's pretty similar to what Rich was using in his talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I've set this up using MVP notation an for the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty much going to stick to MDP notation, but it's worth noting that when we talk about safety issues, it's important that were robust to things like partial observe ability.",
                    "label": 0
                },
                {
                    "sent": "The real world isn't some very simple MVP where we observe the full state of the environment.",
                    "label": 0
                },
                {
                    "sent": "We have sensors that make make observations, and those sensors are noisy and perhaps unreliable, and so really we should model the world.",
                    "label": 0
                },
                {
                    "sent": "Is a palm DP partially observable MDP, so our agent isn't directly observing the full state of the world.",
                    "label": 0
                },
                {
                    "sent": "It's making some observation about the world via some sensors, and these observations can be observations can be noisy, they can be incorrect.",
                    "label": 0
                },
                {
                    "sent": "And this is the palm DP setting everything that I'm going to talk about carries over to this palm DP setting.",
                    "label": 0
                },
                {
                    "sent": "Even though I'm going to be talking about the MDP setting mainly so it's just an important thing to keep in mind that this will be robust.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partial observe ability.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about some potential applications where we might care about ensuring different forms of safety.",
                    "label": 0
                },
                {
                    "sent": "So the first one is digital marketing.",
                    "label": 0
                },
                {
                    "sent": "And so here I visited Adobe's website and they showed me an ad for Creative Cloud and actually a second one for photo shop.",
                    "label": 0
                },
                {
                    "sent": "And these ads aren't deterministically chosen.",
                    "label": 0
                },
                {
                    "sent": "They're not the same for everyone.",
                    "label": 0
                },
                {
                    "sent": "They're not completely random.",
                    "label": 0
                },
                {
                    "sent": "Their targeted specifically at U, the visitor to the web page.",
                    "label": 0
                },
                {
                    "sent": "And so this process of targeting advertisements can be viewed as a sequential decision problem.",
                    "label": 0
                },
                {
                    "sent": "So here the ignore the state.",
                    "label": 0
                },
                {
                    "sent": "For second, the observation is the vector that captures everything that we know about you, the visitor to the web page when you come to our web page, the action is then a decision about which ad or type of AD are we going to show you, and the reward is plus one.",
                    "label": 0
                },
                {
                    "sent": "If you click on the advertisement and zero if you don't click on the advertisement.",
                    "label": 0
                },
                {
                    "sent": "And this is actually a sequential problem.",
                    "label": 0
                },
                {
                    "sent": "Often this is treated as a bandit problem, but it's really sequential.",
                    "label": 0
                },
                {
                    "sent": "What's the sequential nature?",
                    "label": 0
                },
                {
                    "sent": "Well, our observation at one step.",
                    "label": 0
                },
                {
                    "sent": "Depends on the action that we chose at the previous step.",
                    "label": 0
                },
                {
                    "sent": "We showed you an advertisement which you either clicked on or didn't click on and that changes our beliefs about what you might be interested in or might not be interested in, and so the observation vector one step is closely dependent on the action that we chose at the previous step.",
                    "label": 0
                },
                {
                    "sent": "So this is digital.",
                    "label": 0
                },
                {
                    "sent": "Marketing is a sequential decision problem.",
                    "label": 0
                },
                {
                    "sent": "Why do we care about safety here?",
                    "label": 0
                },
                {
                    "sent": "Well, if we deploy a reinforcement learning algorithm to this problem and it does very poorly, it starts deploying advertisements that actually there's a much lower chance that you click on it.",
                    "label": 0
                },
                {
                    "sent": "It can result in.",
                    "label": 0
                },
                {
                    "sent": "Significantly decreased revenues, and so for companies like, for example, Adobe has a product called Test and Target which serves up advertisements for other companies.",
                    "label": 0
                },
                {
                    "sent": "If they use reinforcement learning in a product and it ends up producing policies that are worse, they could end up losing customers, and so it could be very expensive to accompany.",
                    "label": 0
                },
                {
                    "sent": "If in our algorithm for this task doesn't perform properly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what's another example?",
                    "label": 0
                },
                {
                    "sent": "Another example is intelligent tutoring systems, so I'm currently a postdoc at CMU, working with Emma Brunskill, and so I'm biased towards these applications.",
                    "label": 1
                },
                {
                    "sent": "I like them a lot because Emma is doing these intelligent tutoring system applications.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that we have a student that's learning about a topic.",
                    "label": 0
                },
                {
                    "sent": "This is a game, so it's not a course.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a game, but it's the same concept, and here the students learn about fractions, so the students controlling the frog and they have to click 26.",
                    "label": 0
                },
                {
                    "sent": "I think it says 2 six of the way up the chain, and that's where the weak point on the chain is.",
                    "label": 0
                },
                {
                    "sent": "And if they click it correctly, then the frog.",
                    "label": 0
                },
                {
                    "sent": "Breakthrough and they can continue and they solve a sequence of these problems.",
                    "label": 0
                },
                {
                    "sent": "Where is the RL task here?",
                    "label": 0
                },
                {
                    "sent": "The task is that if we give a student a problem that's too hard, then they can get frustrated and stop playing.",
                    "label": 0
                },
                {
                    "sent": "If we only give them problems that are too easy, then they're not going to learn anything.",
                    "label": 0
                },
                {
                    "sent": "So the task here is to choose what type of problem to give next, how to order topics, how difficult of a problem to give next to maximize the students understanding at the end of a course or after playing one of these games.",
                    "label": 0
                },
                {
                    "sent": "So what would our sequence?",
                    "label": 0
                },
                {
                    "sent": "What would our setting here would are setting?",
                    "label": 0
                },
                {
                    "sent": "Beware this is a sequential decision problem, so our observation here is everything that we know about the student.",
                    "label": 0
                },
                {
                    "sent": "What we think they understand right now.",
                    "label": 0
                },
                {
                    "sent": "Our action is then a decision about what type of problem to give them next or how difficult of a problem to give them next.",
                    "label": 0
                },
                {
                    "sent": "And in this case for this game called Tree Frog.",
                    "label": 0
                },
                {
                    "sent": "The rewards were actually 0 all the way through and then at the end the student took a test on the topic on fractions and the student score on the exam was the only reward for this entire history.",
                    "label": 0
                },
                {
                    "sent": "This entire episode, and So what we're trying to do is maximize the students score on this post test.",
                    "label": 0
                },
                {
                    "sent": "So find the way of serving up problems to a student to maximize their understanding of this topic after the course.",
                    "label": 0
                },
                {
                    "sent": "Why does safety matter here?",
                    "label": 0
                },
                {
                    "sent": "Well, if we deploy a policy that's worse, then we can discourage a person from learning more about this topic.",
                    "label": 0
                },
                {
                    "sent": "Make them think I'm not good at math or I don't like math, and so it's important that when we deploy learning algorithms here that they work properly another one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is functional electrical stimulation, so this is work that I did at case Western Reserve University during my Masters thesis.",
                    "label": 1
                },
                {
                    "sent": "And the idea is that when someone is paralyzed, their muscles and nervous system are still intact below the break often, and so you can directly stimulate the muscles in a person's arm.",
                    "label": 0
                },
                {
                    "sent": "A paralyzed person's arm in order to move their arm for them.",
                    "label": 0
                },
                {
                    "sent": "And so the question was, can we control a paralyzed person's arm to move it from some initial position to some target position?",
                    "label": 0
                },
                {
                    "sent": "And researchers at Case Western Reserve University working with researchers at the Cleveland Clinic did this and they used a proportional derivative controller, standard control technique and it worked, but only reasonably well.",
                    "label": 0
                },
                {
                    "sent": "The problem was that the individual persons arm.",
                    "label": 0
                },
                {
                    "sent": "Didn't exactly match their model of an ideal arm, and so they asked, can we create a controller that adapts to this individual persons arm to make the control movements more smooth?",
                    "label": 0
                },
                {
                    "sent": "Or in this case so that they would land at the correct target position and this can be modeled as an MVP?",
                    "label": 0
                },
                {
                    "sent": "I'm guessing you can all see exactly why, and again, safety is an issue if you're trying to drink.",
                    "label": 0
                },
                {
                    "sent": "You don't want this person pouring things over themselves or even worse in this exact setup.",
                    "label": 0
                },
                {
                    "sent": "If you give the wrong stimulations in the wrong states, you can dislocate the person's shoulder.",
                    "label": 0
                },
                {
                    "sent": "And this is something that we really want to make sure we don't do.",
                    "label": 0
                },
                {
                    "sent": "We don't want to play learning algorithm that does this unsafe behavior.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last one, which is really, I think, the best motivation for why we need these safe algorithms is type one diabetes treatment.",
                    "label": 1
                },
                {
                    "sent": "So first, what is diabetes?",
                    "label": 0
                },
                {
                    "sent": "And what is the IRL problem here so?",
                    "label": 0
                },
                {
                    "sent": "When you eat a meal, the carbohydrates in the meal cause your blood sugar to increase.",
                    "label": 0
                },
                {
                    "sent": "Your body then releases insulin which promotes the absorption of the sugar from your blood into various cells of your body.",
                    "label": 0
                },
                {
                    "sent": "So throughout the day your blood sugar makes a pot that looks like this.",
                    "label": 0
                },
                {
                    "sent": "This is minutes since midnight, so this is one day, and this vertical axis is your current blood sugar, and this is many simulated days using a simulator.",
                    "label": 0
                },
                {
                    "sent": "This is your blood sugar spiking, 'cause of breakfast, lunch and dinner, and for this particular person, green is ideal blood sugar levels and black is the limit of healthy levels.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the problem in type one diabetes so?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type one diabetes.",
                    "label": 0
                },
                {
                    "sent": "Their body doesn't release enough insulin.",
                    "label": 0
                },
                {
                    "sent": "And this means that their blood sugar levels tend to be too high, which is a condition called hyperglycemia.",
                    "label": 0
                },
                {
                    "sent": "Hyper for high.",
                    "label": 0
                },
                {
                    "sent": "So hyperglycemia is when their blood sugar tends to be too high 'cause there's not enough insulin promoting absorption and pulling blood sugar down.",
                    "label": 0
                },
                {
                    "sent": "Extended instances of hyperglycemia can lead to ketoacidosis, and I think it's the leading cause of nontraumatic adult amputations.",
                    "label": 0
                },
                {
                    "sent": "So extended hyperglycemia is a problem.",
                    "label": 0
                },
                {
                    "sent": "So to fix this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone can do is inject additional insulin into their blood, either directly or through an insulin pump.",
                    "label": 0
                },
                {
                    "sent": "And if they don't inject enough, they'll still suffer from hyperglycemia, high blood sugar, and if they inject too much, if they put too much insulin into the blood and promote too much, the absorption of sugar from the blood into the cells, it results in hypoglycemia, and it's really easy to mix these up, it's just one letter, different, different, but hyper is too high, hypo is too low, so hypoglycemia low blood sugar is significantly worse than hyperglycemia.",
                    "label": 0
                },
                {
                    "sent": "Severe instances of hypoglycemia can triple the five year mortality rate for personal type one diabetes.",
                    "label": 0
                },
                {
                    "sent": "And it can lose to loss of consciousness, which is devastating.",
                    "label": 0
                },
                {
                    "sent": "If you're driving, for example.",
                    "label": 0
                },
                {
                    "sent": "So this is a control task.",
                    "label": 0
                },
                {
                    "sent": "It's a task where we'd like to decide how much insulin should someone inject prior to a meal in order to keep their blood sugar levels near optimal.",
                    "label": 0
                },
                {
                    "sent": "Whoops, near optimal without driving it too low and to try to keep it from being too high.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a control problem so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can try to address it as a control problem, and actually what is done now is in an insulin pump.",
                    "label": 0
                },
                {
                    "sent": "There's an equation that decides roughly how much insulin someone should inject.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of a simple form of one of those equations, and it's if you're familiar with the controllers, the proportional term.",
                    "label": 0
                },
                {
                    "sent": "This is the derivative term.",
                    "label": 0
                },
                {
                    "sent": "So what is this what?",
                    "label": 0
                },
                {
                    "sent": "Let's go through it.",
                    "label": 0
                },
                {
                    "sent": "The size of the injection is the person's current blood glucose.",
                    "label": 1
                },
                {
                    "sent": "The current amount of sugar in their blood, measured from a blood sample minus their target blood glucose, which is a value.",
                    "label": 1
                },
                {
                    "sent": "An ideal blood glucose level specified by their diabetologists divided by a parameter CF that we get to tune.",
                    "label": 0
                },
                {
                    "sent": "So if you think back to Peter Abeles talk earlier, we had these parameterized policy policies.",
                    "label": 0
                },
                {
                    "sent": "This is a parameterized policy.",
                    "label": 0
                },
                {
                    "sent": "And that's one of the parameters that we can tune, CF.",
                    "label": 0
                },
                {
                    "sent": "The next term is the size of the meal that the person is about to eat, which we measure in terms of grams of carbohydrates, and we divide that by the 2nd parameter that we get to choose which is CR.",
                    "label": 0
                },
                {
                    "sent": "So this is just a parameterized policy for sequential decision problem.",
                    "label": 0
                },
                {
                    "sent": "We have three decisions.",
                    "label": 0
                },
                {
                    "sent": "How much how much inject for breakfast, lunch and dinner, and these are correlated because the amount that you inject at breakfast changes the state of the person at lunch.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way this works now is that someone goes to their diabetologist who says these are values of CR&CF that will likely work for you and the person goes off and roughly three to six months later they come back to their diabetologist.",
                    "label": 0
                },
                {
                    "sent": "Who says OK, given how well this is work, looking at the results from the insulin pump and the report from the person after three to six months, they say here's how I think you should change these parameters CR in CF, and so this raises the question, can this process be automated to some extent?",
                    "label": 0
                },
                {
                    "sent": "If not to replace doctors?",
                    "label": 0
                },
                {
                    "sent": "To handle cases where someone doesn't have the opportunity to go see a doctor every three to six months so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've been some really great work coming out of the University of Alberta where the intelligent Diabetes Management Project, which is a collaboration between the University of Alberta and Alberta Diabetes Institute.",
                    "label": 1
                },
                {
                    "sent": "They show that this problem can be treated as a reinforcement learning problem and there's a Masters thesis by Mason Bastani, working with Russ Greiner.",
                    "label": 0
                },
                {
                    "sent": "So how is this modeled?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "The state is the state of the person.",
                    "label": 0
                },
                {
                    "sent": "The observation at one step is the current blood sugar levels and the size of the meal there bout to eat.",
                    "label": 0
                },
                {
                    "sent": "The action is how much insulin the person should inject.",
                    "label": 0
                },
                {
                    "sent": "And this repeats three times for breakfast, lunch and dinner.",
                    "label": 0
                },
                {
                    "sent": "And we can treat that as an episode.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a reinforcement learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I've listed out several problems where safety is kind of a concern and to really hammer home the problem right now with non safe reinforcement learning methods.",
                    "label": 0
                },
                {
                    "sent": "I want to ask a question which is this.",
                    "label": 0
                },
                {
                    "sent": "If you deploy an existing reinforcement learning algorithm, pick your favorite one, fitted Q iteration to one of these problems, do you have confidence that the policy that it produces will be better than the current policy?",
                    "label": 1
                },
                {
                    "sent": "Do you trust that the new parameters that you pick for this diabetes controller?",
                    "label": 0
                },
                {
                    "sent": "Are actually going to be better than the current ones.",
                    "label": 0
                },
                {
                    "sent": "Do you trust that your method for serving up ads that's proposed by your favorite our algorithm will be better than the current existing method?",
                    "label": 0
                },
                {
                    "sent": "So put differently, can get hands for if you've ever implemented in our algorithm on any problem where you didn't just run it, you actually kind of chose the step sizes you chose the representation, and it can even be mountain car hence.",
                    "label": 0
                },
                {
                    "sent": "So quite a few of you, most of implementing keep up.",
                    "label": 0
                },
                {
                    "sent": "Keep up, keep up OK, so leave them up now if the first time that you hit run it worked and put them down.",
                    "label": 0
                },
                {
                    "sent": "If your first choice of step sizes in representations wasn't right.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem right?",
                    "label": 0
                },
                {
                    "sent": "The first time that you run in our algorithm, it typically doesn't work right.",
                    "label": 0
                },
                {
                    "sent": "You need to do a lot of adjusting and tuning before it works and this is the difference between a lot of these proposed applications and a lot of the successes.",
                    "label": 0
                },
                {
                    "sent": "That reinforcement learning is had so far.",
                    "label": 0
                },
                {
                    "sent": "This success is or for applications where it's OK to tune our representation to tune our step sizes to tune our hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Take Atari for example.",
                    "label": 0
                },
                {
                    "sent": "Google did a grid search over the step size parameter for this, so it's completely OK to do this.",
                    "label": 0
                },
                {
                    "sent": "It's OK somewhere on a Google server to have Atari agents that are just losing millions of games of Atari over and over and over again while they do this search and then at some point they find step sizes that work and they can report that result.",
                    "label": 0
                },
                {
                    "sent": "And that's a cool result that is not OK for a medical application.",
                    "label": 0
                },
                {
                    "sent": "We can't tell someone.",
                    "label": 0
                },
                {
                    "sent": "Sorry this treatment didn't work for you.",
                    "label": 0
                },
                {
                    "sent": "Maybe for the next person will get our step size right.",
                    "label": 0
                },
                {
                    "sent": "We can't do this.",
                    "label": 0
                },
                {
                    "sent": "We need methods that on the first shot you hit the button and you know that it's going to work, or at least you know that it's not going to make matters worse.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's not to say that, oh, these methods are bad and we should only use these methods, not at all.",
                    "label": 0
                },
                {
                    "sent": "These are different sides of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "There can be applications where risk isn't the main concern.",
                    "label": 0
                },
                {
                    "sent": "And also there's another point, which is a lot of us got a lot of us got into this because we're interested in intelligence.",
                    "label": 0
                },
                {
                    "sent": "We want to understand what is intelligence, where does it come from?",
                    "label": 0
                },
                {
                    "sent": "Maybe how do we work?",
                    "label": 0
                },
                {
                    "sent": "Can we create a machine that's generally intelligent and that work tends to fall over in this category?",
                    "label": 0
                },
                {
                    "sent": "So in the rest of this talk, we're going to talk about things that personally I don't think really describe much about.",
                    "label": 0
                },
                {
                    "sent": "What is intelligence or get at that problem?",
                    "label": 0
                },
                {
                    "sent": "It's more about taking what we've learned in reinforcement learning and saying, can we apply this responsibly to some real problems now?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One other point that I want to make is that learning curves are really very deceptive, and so I picked plots from one of my own papers like in Bash.",
                    "label": 0
                },
                {
                    "sent": "It these are from 2013 nips paper with will Dabney that we called natural temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "So here's our method.",
                    "label": 0
                },
                {
                    "sent": "Our method is this green one and many of you have probably seen mountain car in Carpool Mountain car.",
                    "label": 0
                },
                {
                    "sent": "You're trying to get to the top of that Valley carpool.",
                    "label": 0
                },
                {
                    "sent": "You're trying to keep the pole balanced so our method on mountain car gets a near optimal policy after a single episode.",
                    "label": 0
                },
                {
                    "sent": "So Dilly dally around this mountain a little bit.",
                    "label": 0
                },
                {
                    "sent": "Learning it gets to the goal and after that it's almost optimally getting to the solution every single time.",
                    "label": 0
                },
                {
                    "sent": "Same for cart pole.",
                    "label": 0
                },
                {
                    "sent": "The pole falls over a single time and after that it never happens again.",
                    "label": 0
                },
                {
                    "sent": "It always keeps the full balanced, so this seems like an outstanding algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "But this is deceptive, so The thing is this is really after billions of episodes of tuning.",
                    "label": 1
                },
                {
                    "sent": "So first we did millions of episodes of setting all of the different hyperparameters and this is standard when you read NRL paper says we manually tune the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "We did a grid search.",
                    "label": 0
                },
                {
                    "sent": "So millions of episodes that went into the parameters optimization.",
                    "label": 1
                },
                {
                    "sent": "There's also probably another millions that were human intuition from working with these domains in the past.",
                    "label": 0
                },
                {
                    "sent": "So we worked with mountain car.",
                    "label": 1
                },
                {
                    "sent": "We know that the Fourier basis with linear function approximation works really well as representation here, and that came from many past trials.",
                    "label": 0
                },
                {
                    "sent": "And also there's billions of episodes that went into experimental design people, making sure that this task is possible.",
                    "label": 0
                },
                {
                    "sent": "We're not asking something that's completely impossible, and our reward function isn't poorly designed, so going into these plots is a lot of learning ahead of time and adjusting of the system, and so we should be careful when we look at these to say, well, if I apply this to diabetes treatment, I really need to worry about these.",
                    "label": 0
                },
                {
                    "sent": "I can't just look at this horizontal axis there, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here are you saying that basically?",
                    "label": 0
                },
                {
                    "sent": "Yes, essentially we've found the perfect step size is the perfect exploration rate so that for this task it works.",
                    "label": 0
                },
                {
                    "sent": "Yes, I don't believe that our algorithm is so amazing that it never dropping the pole here.",
                    "label": 0
                },
                {
                    "sent": "This is a fluke of the order of the Fourier basis, which sets the exact parameters of the representation with that step size.",
                    "label": 0
                },
                {
                    "sent": "With that, exploration rate just happens to workout perfectly.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem.",
                    "label": 0
                },
                {
                    "sent": "So yes, we've overfit the problem.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Even the notion of overprinting in RL.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "That's one of the challenges, yeah?",
                    "label": 0
                },
                {
                    "sent": "Anyway, so I think I've finished the definition of what.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I haven't finished definition.",
                    "label": 0
                },
                {
                    "sent": "I've talked about background of notation and why we should care about safety and what it is that we want.",
                    "label": 0
                },
                {
                    "sent": "So let's define safety.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do we want?",
                    "label": 0
                },
                {
                    "sent": "We want an algorithm that, as I said when we hit the go button, it works on the first try, so it has to be guaranteed to work, or more specifically for the rest of the talk, other than when I talk about other definitions of safety in a moment when I say a safe reinforcement learning algorithm, I mean one that can give this guarantee, one that can say with probability at least one minus Delta.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to change your policy to one that is worse than the current policy.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to make things worse, so Delta here is the probability that I'm going to do something bad.",
                    "label": 0
                },
                {
                    "sent": "And a key property of this is you.",
                    "label": 1
                },
                {
                    "sent": "The user should get to choose this parameter Delta.",
                    "label": 0
                },
                {
                    "sent": "You can choose the probability that I'm allowed to fail or do something bad.",
                    "label": 0
                },
                {
                    "sent": "So my algorithm should do is sit there and collect data.",
                    "label": 0
                },
                {
                    "sent": "If you choose a very low probability of failure, I might have to sit there for a long time collecting data before I can say OK.",
                    "label": 0
                },
                {
                    "sent": "Here is a policy that's better, and this guarantee should not be contingent in any way on any hyperparameter settings.",
                    "label": 0
                },
                {
                    "sent": "So this guarantee just has to actually hope.",
                    "label": 0
                },
                {
                    "sent": "So put in pictorial form, we want to create this black box.",
                    "label": 0
                },
                {
                    "sent": "Which takes as input some historical data D. This is the data from running our current policy for serving up advertisements or current policy for treating diabetes and a probability 1 minus Delta and produces a new policy pie that satisfies this.",
                    "label": 1
                },
                {
                    "sent": "The performance of this new policy is at least the performance of the behavior policy with probability 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "So with high probability our new policy is at least as good as the current one.",
                    "label": 0
                },
                {
                    "sent": "Are black boxes also allowed to say no solution found if you say here's data from 3 users, give me a better policy for serving up advertisements with 99% probability.",
                    "label": 0
                },
                {
                    "sent": "We can't do it.",
                    "label": 0
                },
                {
                    "sent": "We have to say given 3 users I can't satisfy this guarantee that you've asked for.",
                    "label": 0
                },
                {
                    "sent": "So we have to be able to say I can't give the guarantee that you want, which is no solution found.",
                    "label": 0
                },
                {
                    "sent": "And that really means just return the behavior policy.",
                    "label": 0
                },
                {
                    "sent": "Keep running the policy that you've been doing 'cause I can't improve it with high probability yet.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are some limitations of this setting just to make them explicit, we've assumed that an initial policy is available.",
                    "label": 1
                },
                {
                    "sent": "We're not saying we're going to learn from scratch to solve some new challenging problem.",
                    "label": 0
                },
                {
                    "sent": "We're assuming that we have some policy for treating diabetes, some policy for serving up ads, and we're just going to try to improve it.",
                    "label": 0
                },
                {
                    "sent": "We're also typically going to assume that this policy is known, so there's been some recent works toward works in progress toward replacing and getting rid of this assumption.",
                    "label": 0
                },
                {
                    "sent": "But for now, we're going to assume that we know the current policy for serving up advertisements, and we're also going to assume that the currently deployed policy is stochastic, so there is some randomness in it.",
                    "label": 0
                },
                {
                    "sent": "And we're also for now going to look at the batch setting so this isn't online.",
                    "label": 0
                },
                {
                    "sent": "Learning where we see one state, we change our policy.",
                    "label": 0
                },
                {
                    "sent": "We see another state.",
                    "label": 0
                },
                {
                    "sent": "We change our policy.",
                    "label": 0
                },
                {
                    "sent": "This is, I gather, a bunch of data and I make a change to my policy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to again drive this home a little bit more, what are we looking for?",
                    "label": 0
                },
                {
                    "sent": "If this is episodes and this is the performance of the policy, so standard learning curve the first time that you run a typical our algorithm on this, it will do something like this.",
                    "label": 0
                },
                {
                    "sent": "It will completely diverge.",
                    "label": 0
                },
                {
                    "sent": "You'll tweak your learning parameters and it will do something like this.",
                    "label": 0
                },
                {
                    "sent": "It will start to work, you'll get the step size in the right range, the value functions converging and eventually you get the right parameters and it does really well.",
                    "label": 0
                },
                {
                    "sent": "What do we want this process to look like when you're using a safe algorithm?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, first just worth mentioning that there's a trivial safe algorithm.",
                    "label": 0
                },
                {
                    "sent": "Has anyone seen it and thought?",
                    "label": 0
                },
                {
                    "sent": "Wait a minute.",
                    "label": 0
                },
                {
                    "sent": "There's something wrong here.",
                    "label": 0
                },
                {
                    "sent": "What is an obviously safe algorithm according to this definition of safety that ensures this?",
                    "label": 0
                },
                {
                    "sent": "I think you all just set it at once, which is do nothing right.",
                    "label": 0
                },
                {
                    "sent": "Always return no solution found.",
                    "label": 0
                },
                {
                    "sent": "Return the behavior policy and that is safe.",
                    "label": 0
                },
                {
                    "sent": "So that does this.",
                    "label": 0
                },
                {
                    "sent": "It says just always return the current policy.",
                    "label": 0
                },
                {
                    "sent": "We're not doing something bad and it satisfies our definition of safe.",
                    "label": 0
                },
                {
                    "sent": "It's not a particularly interesting algorithm though, so hopefully our algorithm will sometimes actually change the policy and improve performance.",
                    "label": 0
                },
                {
                    "sent": "But we want to guarantee that we're not going to make it worse.",
                    "label": 0
                },
                {
                    "sent": "We're not going to blow that initial policy's performance with high probability.",
                    "label": 0
                },
                {
                    "sent": "And eventually, what we'd really like to see, and what we'll see at the end.",
                    "label": 0
                },
                {
                    "sent": "Or some plots that look like this.",
                    "label": 0
                },
                {
                    "sent": "We're getting improved performance.",
                    "label": 0
                },
                {
                    "sent": "And typically it's going to lag behind.",
                    "label": 0
                },
                {
                    "sent": "We're not going to be as fast as that unsafe method that has been overfit to the problem.",
                    "label": 0
                },
                {
                    "sent": "That has been perfectly tuned for this problem, because we're guaranteeing that on the first run it's never going to produce this behavior, OK?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I pitched my definition of safe and what I think safety should mean, but I should mention there are many other definitions of safety and safety is a broad topic in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So for example, there's a survey on reinforcement learning that has nothing to do with what I've talked about for safety so far.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that paper they have this is table one.",
                    "label": 0
                },
                {
                    "sent": "This isn't actually there, table just recreated it where they break safe RL into a few different categories and I want to spend a little bit of time talking about this one because it's worth knowing about so they break safe RL into these categories.",
                    "label": 0
                },
                {
                    "sent": "The first one is changing your optimization criterion, so this argument is that for some problems expected return mean sum of rewards doesn't capture what it is that we want to optimize, and so we should choose some other objective that better captures some notion of risk.",
                    "label": 0
                },
                {
                    "sent": "And the other one, and I'm going to talk about a couple of these on the next slide.",
                    "label": 0
                },
                {
                    "sent": "The other one is making changes to the exploration process and most of the methods that they list here or things like inverse reinforcement learning methods and learning from demonstration.",
                    "label": 0
                },
                {
                    "sent": "So can I bias my initial learning by learning from a human in order to not have that initial learning.",
                    "label": 0
                },
                {
                    "sent": "Where I'm basically flailing randomly and I would argue that what I'm going to describe in this talk is 1/3 category here, which is changing the learning process?",
                    "label": 0
                },
                {
                    "sent": "Well, let's talk a little bit about changing the objective function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because again, it's a common topic in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So what we optimize right now is expected return so expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "So let's say that this horizontal axis is the performance axis, so this is higher, returns, lower returns, and this is the distribution of returns produced by some policy \u03c0, or we'll call it the blue policy, and that's it's mean right there.",
                    "label": 0
                },
                {
                    "sent": "So this is, uh, some policy.",
                    "label": 0
                },
                {
                    "sent": "Let's say that I give you a different policy, the red policy that produces this distribution over outcomes.",
                    "label": 0
                },
                {
                    "sent": "So it has much higher variance.",
                    "label": 0
                },
                {
                    "sent": "And it's expected value is a little bit higher.",
                    "label": 0
                },
                {
                    "sent": "Which of these two policies is better?",
                    "label": 0
                },
                {
                    "sent": "So first, if I'm a casino?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not like the building if I'm running a casino, then which of these is better?",
                    "label": 0
                },
                {
                    "sent": "So these are the returns from some some slot machine of some sort.",
                    "label": 0
                },
                {
                    "sent": "Which one is better for me?",
                    "label": 0
                },
                {
                    "sent": "Which one?",
                    "label": 0
                },
                {
                    "sent": "I heard someone say the red and I thought I heard someone say the green.",
                    "label": 0
                },
                {
                    "sent": "Gotta get a hand.",
                    "label": 0
                },
                {
                    "sent": "Anyone which one is better and why?",
                    "label": 0
                },
                {
                    "sent": "Yes, the red one because the the expected value is higher.",
                    "label": 0
                },
                {
                    "sent": "I'll make more money in expectation and if I lose money, sometimes that's OK, right?",
                    "label": 0
                },
                {
                    "sent": "So expected return is a decent notion of optimality here.",
                    "label": 0
                },
                {
                    "sent": "Maximizing expected return.",
                    "label": 0
                },
                {
                    "sent": "But if I'm a doctor, what if these correspond to patient outcomes?",
                    "label": 0
                },
                {
                    "sent": "And somewhere over here is something like the patient dies in this case, which of these two policies is better?",
                    "label": 0
                },
                {
                    "sent": "The blue right?",
                    "label": 0
                },
                {
                    "sent": "Because yes, it has slightly lower expected performance.",
                    "label": 0
                },
                {
                    "sent": "But we don't have this really high risk down here, so maybe I need a different objective function other than expected sum of rewards to capture what it is that I want to optimize.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's do this.",
                    "label": 0
                },
                {
                    "sent": "Let's come up with some other objectives that capture this.",
                    "label": 0
                },
                {
                    "sent": "One idea is to penalize variance directly, which is what we saw in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So we can say that our objective function now is the expected return just as before, minus this term where Lambda here is just a scaling parameter that says how much we care bout variance times the variance of the observed returns and if we make Lambda large enough in this then the blue policy or the red policy will be better than the blue.",
                    "label": 0
                },
                {
                    "sent": "It will say try to get a policy that performs as well as possible and has low variance in the results that it produces.",
                    "label": 0
                },
                {
                    "sent": "And there's some good work from Scotland Eresma on this and doing policy gradient on this objective.",
                    "label": 0
                },
                {
                    "sent": "There's another popular one which is.",
                    "label": 0
                },
                {
                    "sent": "Value at risk and conditional value at Risk C var is also called.",
                    "label": 1
                },
                {
                    "sent": "I think expected shortfall.",
                    "label": 0
                },
                {
                    "sent": "So what do these do?",
                    "label": 0
                },
                {
                    "sent": "Let's say that this is the distribution of returns that we see instead of returning the mean as our objective value at risk says take the percentile take some Alpha percentile.",
                    "label": 0
                },
                {
                    "sent": "Were here Alpha would be 5%.",
                    "label": 0
                },
                {
                    "sent": "So what is the return such that 95% of the returns that we get will be bigger than that value and that is the VAR.",
                    "label": 0
                },
                {
                    "sent": "So it's the the sum of rewards where we know 95% of the time.",
                    "label": 0
                },
                {
                    "sent": "I'll get at least this much.",
                    "label": 0
                },
                {
                    "sent": "That value is what you would optimize in that case, and then see Var takes the mean of this probability mass, that is, that is lower.",
                    "label": 0
                },
                {
                    "sent": "So these are all notions of trying to optimize the worst case in some sense.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for some applications like we said for medical applications, maybe this is the right objective.",
                    "label": 1
                },
                {
                    "sent": "It's the one that we should care about, but this just changing our objective function doesn't address the motivation that we had before.",
                    "label": 0
                },
                {
                    "sent": "We still have to produce an algorithm that optimizes these objective functions and the resulting algorithms are often very similar and we need them to work on the first shot.",
                    "label": 0
                },
                {
                    "sent": "So I view this is kind of orthogonal to what I'll be talking about in the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "There's a choice of objective, and then there's ensuring that we actually increase that objective when we run it was there question, but I'm happy to talk about this.",
                    "label": 0
                },
                {
                    "sent": "More afterwards, OK, so there are those on Amazon, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of an orthogonal quest.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are other definitions of safety.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a paper by Remy Moonos and others.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where they say that an algorithm is not safe if it doesn't handle arbitrary off policy Ness where arbitrary off pole handle means give asymptotic convergence results.",
                    "label": 0
                },
                {
                    "sent": "So, like Sarsa, Lambda has asymptotically convergence results in the tabular on policy setting.",
                    "label": 0
                },
                {
                    "sent": "So this is a completely different notion of safety.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are others so AKA Matale working in Claire Tomlin's lab at Berkeley.",
                    "label": 0
                },
                {
                    "sent": "They've done some work that's kind of related to adaptive control, and this is something that people often think about first.",
                    "label": 0
                },
                {
                    "sent": "When you think about Safe RL and that is state avoidance, so maybe there's some region of state space that I want to ensure that I never reach that my agent never enters these dangerous states.",
                    "label": 0
                },
                {
                    "sent": "It never they're working with quadcopters.",
                    "label": 0
                },
                {
                    "sent": "It never crashes into the wall.",
                    "label": 0
                },
                {
                    "sent": "And these methods typically rely on having some strong prior knowledge about the environment distribution over possible worlds that you can be in, and this is again very closely related to adaptive control.",
                    "label": 0
                },
                {
                    "sent": "And then there's Packer L which for the sake of time I'm going to skip over this.",
                    "label": 0
                },
                {
                    "sent": "But it is also related.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've now defined our notion of safety for the rest of the talk, which is an algorithm that when I hit the go button, it's guaranteed to work, or at least not make things worse with high probability.",
                    "label": 0
                },
                {
                    "sent": "How do we make this?",
                    "label": 0
                },
                {
                    "sent": "Well, they're going to be 3 components that we need to build in order to do this, and the first one is off policy policy evaluation, which is really awkward 'cause that middle policy.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about why it's there and then high confidence and then actually creating the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So let's go through what these are and then we'll delve into each one in detail.",
                    "label": 0
                },
                {
                    "sent": "And we're actually going to spend the most time in this talk.",
                    "label": 0
                },
                {
                    "sent": "On this first one, which is a paper by doing a pre Cup from the year 2000, an important sampling.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is off policy policy evaluation?",
                    "label": 0
                },
                {
                    "sent": "We're going to try to make this black box in the middle of the screen and it takes two things as input.",
                    "label": 0
                },
                {
                    "sent": "The first thing is historical data D, so we've run some current policy and it's produced outcomes.",
                    "label": 0
                },
                {
                    "sent": "Remember D historical data is a set of histories, a set of episodes.",
                    "label": 1
                },
                {
                    "sent": "So for digital marketing this is the data from all of our users, and the episode is the data from the user.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have our historical data produced by that behavior policy or currently run policy and we also are given some newly proposed policy which we're going to call PII and ease for evaluation 'cause we're going to evaluate this policy.",
                    "label": 1
                },
                {
                    "sent": "OK, so given the data and this policy, so imagine that I just have collected data and someone else ran in our algorithm and said this policy is great.",
                    "label": 0
                },
                {
                    "sent": "You should run it and what we'd like to produce as output is a prediction of the performance of this policy that the other person proposed would like to predict.",
                    "label": 0
                },
                {
                    "sent": "How good would this policy be?",
                    "label": 0
                },
                {
                    "sent": "And the catch is, this black box is not allowed to run the evaluation policy because you can imagine where we're going to use this, we're going to use this to ask is it safe to run this new policy and it could potentially be a dangerous policy, so we cannot.",
                    "label": 0
                },
                {
                    "sent": "Actually run it so we need to use the data from our current policy to evaluate this new policy not running the new policy.",
                    "label": 1
                },
                {
                    "sent": "This is off policy policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "Why the extra policy?",
                    "label": 0
                },
                {
                    "sent": "So there's also off policy evaluation, which typically refers to trying to estimate the value function using data that came from running a different policy.",
                    "label": 0
                },
                {
                    "sent": "And here we're not estimating value.",
                    "label": 0
                },
                {
                    "sent": "Functions were predicting the performance of a policy stating expected returns of an entire policy, so that's the second policy.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what is the second problem that we're going to solve?",
                    "label": 0
                },
                {
                    "sent": "We're going to make a second block black box that takes the same historical data as input.",
                    "label": 0
                },
                {
                    "sent": "It takes that same policy that someone else is proposed, but we're not satisfied with just an estimate of how good that policy is.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to bound it, so we're also going to take a probability 1 minus Delta as input.",
                    "label": 0
                },
                {
                    "sent": "And we're going to produce output A1 minus Delta confidence lower bound on the performance of this new policy.",
                    "label": 1
                },
                {
                    "sent": "So we're going to say that policy that you gave me with probability at least one minus Delta, it's going to be this good.",
                    "label": 0
                },
                {
                    "sent": "So we're lower bounding its performance.",
                    "label": 0
                },
                {
                    "sent": "You can see where this is going to be useful.",
                    "label": 0
                },
                {
                    "sent": "To be safe, we can't just predict.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think this policy will be better.",
                    "label": 0
                },
                {
                    "sent": "We need to say, I think this policy will be better, and I guarantee it's going to be at least this good with high probability.",
                    "label": 0
                },
                {
                    "sent": "And then we'll compare that to our current policy to see if we can return the new policy.",
                    "label": 0
                },
                {
                    "sent": "And again, this is before we have to do this without deploying the evaluation policy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so then the last step is to take this and put it together.",
                    "label": 0
                },
                {
                    "sent": "So we're going to make a third black box that takes the historical data as input.",
                    "label": 1
                },
                {
                    "sent": "No policy, it just takes the data and a probability 1 minus Delta and it produces that new policy that comes with our safety guarantees.",
                    "label": 1
                },
                {
                    "sent": "That's kind of putting the pieces together.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's delve into these three problems.",
                    "label": 0
                },
                {
                    "sent": "The first one being off policy policy evaluation and hear the talk.",
                    "label": 1
                },
                {
                    "sent": "I'm going to get very specific so that hopefully coming out of this you will be able to implement that first one methods for the first one or the second one.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of handwaving that goes into the third one, so we're going to kind of delve into the math of it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about important sampling.",
                    "label": 0
                },
                {
                    "sent": "Important sampling is what we're going to use to solve this off.",
                    "label": 0
                },
                {
                    "sent": "Policy policy evaluation problem.",
                    "label": 0
                },
                {
                    "sent": "Just so you have an idea, can I get hands for if you've seen important sampling for reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "OK, great so.",
                    "label": 0
                },
                {
                    "sent": "A little less than half.",
                    "label": 0
                },
                {
                    "sent": "So let's delve into what what is important sampling and how does it work.",
                    "label": 0
                },
                {
                    "sent": "So this is just a review of our notation.",
                    "label": 0
                },
                {
                    "sent": "H is a history or a trajectory, and J of Pi is our objective, so we're going to try to predict is the performance of that evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This picture depicts all of the possible histories that could ever occur.",
                    "label": 0
                },
                {
                    "sent": "Every possible outcome that could occur when we run our controller for treating diabetes.",
                    "label": 0
                },
                {
                    "sent": "Will view policies then as a distribution over these outcomes.",
                    "label": 0
                },
                {
                    "sent": "'cause that's all the policy is.",
                    "label": 0
                },
                {
                    "sent": "When you run a policy we get an outcome and it's not deterministic, so there's some distribution over the outcomes that occurs when I run a policy.",
                    "label": 0
                },
                {
                    "sent": "If I were to run the evaluation policy, the policy that I'd like to predict, and this is what we're not allowed to do, because this evaluation policy could be dangerous, right?",
                    "label": 0
                },
                {
                    "sent": "If I were to run this policy, I'd get samples from this red distribution and it would be really easy to estimate the performance of the policy.",
                    "label": 0
                },
                {
                    "sent": "I'd use the Monte Carlo estimator.",
                    "label": 0
                },
                {
                    "sent": "I would just average.",
                    "label": 0
                },
                {
                    "sent": "Over my N samples in the data, the observed discounted sums of rewards, the observed returns and this would be my estimate of how good the valuation policy is, right?",
                    "label": 0
                },
                {
                    "sent": "How do I?",
                    "label": 0
                },
                {
                    "sent": "How do we do this though when my data doesn't come from the red distribution?",
                    "label": 0
                },
                {
                    "sent": "What do I do when my data comes from the blue distribution?",
                    "label": 0
                },
                {
                    "sent": "The distribution over outcomes that occurs from running the behavior policy, the current policy, and I wish it came from the red distribution.",
                    "label": 1
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to take a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "This is 1 important sampling does.",
                    "label": 0
                },
                {
                    "sent": "How does this work?",
                    "label": 0
                },
                {
                    "sent": "Well, what this weight does is, it says take the third trajectory down.",
                    "label": 0
                },
                {
                    "sent": "This is a trajectory that we're not going to see very often, but if we could run the evaluation policy, we would see that outcome much more often.",
                    "label": 0
                },
                {
                    "sent": "It has a higher probability under the evaluation policy, so this is an outcome that we would see more.",
                    "label": 1
                },
                {
                    "sent": "So let's give it a large wait to pretend that we would have seen this more often.",
                    "label": 0
                },
                {
                    "sent": "What do we do in the opposite case so this third one from the bottom?",
                    "label": 0
                },
                {
                    "sent": "This is an outcome that in our blue bag the bag we actually have we see this outcome a lot, but if we were to run the policy that we want the evaluation policy, we wouldn't have seen it as much, so we'll give it a small wait, a wait that's less than one to say pretend that I saw this outcome less frequently.",
                    "label": 0
                },
                {
                    "sent": "And that's all that important sampling does.",
                    "label": 0
                },
                {
                    "sent": "It says take a weighted average of the observed rewards.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to go into next is how do we compute what is a reasonable weighting scheme that implements this intuition?",
                    "label": 0
                },
                {
                    "sent": "OK, also at some points I might might say importance weighted return that just refers to the term that's in the red box because it's importance weighted return the sum of rewards.",
                    "label": 1
                },
                {
                    "sent": "OK, are there any questions?",
                    "label": 0
                },
                {
                    "sent": "So far I've been kind of flying through this stuff.",
                    "label": 0
                },
                {
                    "sent": "Is that a hand or a head?",
                    "label": 0
                },
                {
                    "sent": "Scratch that scratch OK?",
                    "label": 0
                },
                {
                    "sent": "OK, cool.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's delve into the derivation of where those weights come from, and I think the easiest way to do this is to set it up not in the context of RL, but to do it just using kind of general notation for general estimators.",
                    "label": 0
                },
                {
                    "sent": "And then we'll apply it to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "Let's let X be a random variable that has some probability mass function P, and in our case X is going to be a history, an outcome, an.",
                    "label": 1
                },
                {
                    "sent": "It was generated by our evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "So P in this case would be the distribution over outcomes.",
                    "label": 0
                },
                {
                    "sent": "That results from running the evaluation policy, but for the general case P is just a probability mass function for this random variable X.",
                    "label": 1
                },
                {
                    "sent": "Let's let Y be another random variable that has a different probability mass function which is Q and for just in case anyone's blanking on it, probability mass function, that just means that PP of X would be the probability of that outcome X. OK, so let's let Y be a different woman getting ahead of ourselves.",
                    "label": 0
                },
                {
                    "sent": "Let's let Y be a different random variable that has a probability mass function Q, but which has the same range, is X.",
                    "label": 0
                },
                {
                    "sent": "So really you can think of this as P being one distribution and cubing a different distribution.",
                    "label": 0
                },
                {
                    "sent": "Over the same things, and in our case, the things are episodes and P is the evaluation policies distribution and Q is the behavior policy's distribution, the distribution that results from running our current policy.",
                    "label": 1
                },
                {
                    "sent": "OK. Next, we're going to define a function F. And what we'd like to do is eventually we're going to put the expected value of this function of F of X, but F is just some function, and in our case F of X.",
                    "label": 0
                },
                {
                    "sent": "Remember X is just a history F of X, is the return of that history in our case.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to estimate is the expected value of F of X, but we only have samples of the random variable Y, so we'd like to estimate the expected return when trajectory is come from the evaluation policy.",
                    "label": 1
                },
                {
                    "sent": "But we only have trajectories that came from the behavior policy, so that's how this is going to map back.",
                    "label": 0
                },
                {
                    "sent": "But because we're going to use a slot, I'll write it on the board just so you don't forget we're trying to estimate the expected value of F of X, where X comes from P, and we have.",
                    "label": 0
                },
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "Why that comes from some distribution Q OK.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Oh, there's one more thing we're going to define these terms because I'm going to be very precise in our derivation.",
                    "label": 0
                },
                {
                    "sent": "We're going to find capital P to be the support of P Capital Q to be the supportive Q and capital F to be the supportive at what is support.",
                    "label": 0
                },
                {
                    "sent": "So the support of any of these, let's use P, for example.",
                    "label": 0
                },
                {
                    "sent": "The supportive P is the set of X such that P of X is not zero.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just all of the outcomes that have nonzero probability under this distribution P, yeah.",
                    "label": 0
                },
                {
                    "sent": "Will come to that will come to that for now, we're not assuming that yet, but we will assume something related to that.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions?",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I need to go faster.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the important sampling estimator?",
                    "label": 0
                },
                {
                    "sent": "This is the important sampling estimator.",
                    "label": 0
                },
                {
                    "sent": "I'll just state what it is and then we'll derive why it's a reasonable scheme.",
                    "label": 0
                },
                {
                    "sent": "So given our one sample of this random variable Y, the important sampling estimate for the expected value of F of X.",
                    "label": 1
                },
                {
                    "sent": "So given one history from the behavior policy are estimated, the performance of the valuation policy is this.",
                    "label": 0
                },
                {
                    "sent": "So this is our weight and that would be the return.",
                    "label": 0
                },
                {
                    "sent": "So this is the wait.",
                    "label": 0
                },
                {
                    "sent": "What is this weight?",
                    "label": 0
                },
                {
                    "sent": "It's the probability of the outcome under the evaluation distribution or the target distribution divided by its probability under the distribution that we're actually sampling from.",
                    "label": 0
                },
                {
                    "sent": "So if this would be more likely under the distribution that we wish we had P, then this would be larger than one, and if it's less likely this will be smaller than one.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly the intuition that we talked about.",
                    "label": 0
                },
                {
                    "sent": "But why is this ratio really what we want?",
                    "label": 0
                },
                {
                    "sent": "So let's workout what is the expected value of this important sampling estimator.",
                    "label": 0
                },
                {
                    "sent": "OK, so the expected value of this important sampling estimator is, well, let's write it out.",
                    "label": 0
                },
                {
                    "sent": "So we sum over all the possible events Y and we have Q of Y.",
                    "label": 0
                },
                {
                    "sent": "'cause why comes from the distribution Q times the the estimator is just the definition of expected value?",
                    "label": 0
                },
                {
                    "sent": "Why am I doing Y in Q here so?",
                    "label": 0
                },
                {
                    "sent": "We can definitely we have to sum over the support of this function Q But why don't we sum over anything else?",
                    "label": 0
                },
                {
                    "sent": "We don't sum over anything else because for wise that aren't in Q, we'd be dividing by zero, and so this expected value would be undefined.",
                    "label": 0
                },
                {
                    "sent": "So really, for this expected value we can only some over the elements that are in the support of Q.",
                    "label": 0
                },
                {
                    "sent": "That distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can we do next?",
                    "label": 0
                },
                {
                    "sent": "Will just do a change of variable.",
                    "label": 0
                },
                {
                    "sent": "So why is just a variable name here?",
                    "label": 0
                },
                {
                    "sent": "Let's just call it X instead.",
                    "label": 0
                },
                {
                    "sent": "That's all we've done here.",
                    "label": 0
                },
                {
                    "sent": "OK. What did I do next?",
                    "label": 0
                },
                {
                    "sent": "What I did next is I cancelled this Q with that Q 'cause it's on the top and on the bottom those go away and so we get P of XF of X.",
                    "label": 0
                },
                {
                    "sent": "Or the only terms left.",
                    "label": 0
                },
                {
                    "sent": "But we've split our sum into different components so I guess a Venn diagram could help here so we have.",
                    "label": 0
                },
                {
                    "sent": "P. And Q and what we wanted to do is some over Q which is that region right there.",
                    "label": 0
                },
                {
                    "sent": "So what did we do?",
                    "label": 0
                },
                {
                    "sent": "We split that into three sums.",
                    "label": 0
                },
                {
                    "sent": "We summed first over P. So we included all of this.",
                    "label": 0
                },
                {
                    "sent": "Then we summed over not P but not P&Q.",
                    "label": 0
                },
                {
                    "sent": "So that's all of this, not including the parts that are in P&Q.",
                    "label": 0
                },
                {
                    "sent": "And then we subtract it off the bits that are in P and not Q.",
                    "label": 0
                },
                {
                    "sent": "So what is in P and not Q?",
                    "label": 0
                },
                {
                    "sent": "That's all of this.",
                    "label": 0
                },
                {
                    "sent": "So we subtracted this back off and that means we're just summing over Q, so we haven't changed anything.",
                    "label": 0
                },
                {
                    "sent": "We've just rewritten.",
                    "label": 0
                },
                {
                    "sent": "We've split this sum into three different components, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here I'm keeping the first term and keeping the second term, but we remove the middle term.",
                    "label": 0
                },
                {
                    "sent": "Why did we do this?",
                    "label": 0
                },
                {
                    "sent": "So for any X that is not in peace.",
                    "label": 0
                },
                {
                    "sent": "Remember, capital P is the supportive P. It's the values for which P of X is not zero, so if you're not in PP of X is 0.",
                    "label": 0
                },
                {
                    "sent": "So these terms are all zero for every term in this sum, so that middle some is just zero.",
                    "label": 0
                },
                {
                    "sent": "So we can get rid of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we go from here to getting the expected values what we want?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We need to make some assumptions, so on this slide I'll make one set of assumptions and on the next slide I'll make a different set of assumptions.",
                    "label": 0
                },
                {
                    "sent": "So the first set of assumptions that we can make is that P is a subset of Q.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "The distribution that we wish we could sample from is a subset of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is a subset of support of the distribution that we actually have samples from.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if we'd like to know the expected value under one policy, we need to be able to see everything that policy could do in our the policy that we ran the policy that we're running never takes some action in some state.",
                    "label": 0
                },
                {
                    "sent": "We don't know what that will do, and we can never know what that will do, and so our evaluation policy can't take that other action in that state.",
                    "label": 0
                },
                {
                    "sent": "We need to be able to see everything under the sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "That could ever happen under the evaluation distribution, so that's what this assumption means.",
                    "label": 0
                },
                {
                    "sent": "You can actually relax this a little bit exercise for you to get home later, so let's complete the proof with this assumption, and I'll remind me later if I forget what this assumption exactly means.",
                    "label": 0
                },
                {
                    "sent": "In our case, it's something that we can check very easily.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's complete the proof.",
                    "label": 0
                },
                {
                    "sent": "This is just copying the last line from before.",
                    "label": 0
                },
                {
                    "sent": "I've removed the second term.",
                    "label": 0
                },
                {
                    "sent": "Why did I do that?",
                    "label": 0
                },
                {
                    "sent": "So P is a subset of Q.",
                    "label": 0
                },
                {
                    "sent": "So that means we have P&Q, so this term is summing over the elements that are in P and not Q.",
                    "label": 0
                },
                {
                    "sent": "So there NP and in not Q.",
                    "label": 0
                },
                {
                    "sent": "So it has to be both here and out there.",
                    "label": 0
                },
                {
                    "sent": "That's nothing.",
                    "label": 0
                },
                {
                    "sent": "So this is something over the empty set.",
                    "label": 0
                },
                {
                    "sent": "So this term is now 0 and we get this.",
                    "label": 0
                },
                {
                    "sent": "That's just the definition of the expected value of F of X.",
                    "label": 0
                },
                {
                    "sent": "So we're done.",
                    "label": 0
                },
                {
                    "sent": "So we have that in this case.",
                    "label": 0
                },
                {
                    "sent": "Given this assumption, the important sampling estimator gives an unbiased estimate of this expected value.",
                    "label": 1
                },
                {
                    "sent": "So thinking back to our case, the ratio of the probability of the outcomes under the evaluation policy and the currently run policy times the returns is an unbiased estimate of the true performance of this evaluation policy, and I will flush that out in detail in a moment.",
                    "label": 0
                },
                {
                    "sent": "First, I want to show that we can complete this with a different set of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions, so let's instead assume that the function we're trying to estimate F of X, which in our case is returns is non negative, so it only takes positive values.",
                    "label": 0
                },
                {
                    "sent": "What can we do then?",
                    "label": 0
                },
                {
                    "sent": "Well, this is just copying what we had from the end of that previous previous slide.",
                    "label": 0
                },
                {
                    "sent": "What happens to the second term here?",
                    "label": 0
                },
                {
                    "sent": "P is positive.",
                    "label": 0
                },
                {
                    "sent": "Well I just gave away the answer.",
                    "label": 0
                },
                {
                    "sent": "P is positive, F is non negative.",
                    "label": 0
                },
                {
                    "sent": "So we're subtracting a positive term.",
                    "label": 0
                },
                {
                    "sent": "So our expected expected value is going to be less than this value, which is just the expected value.",
                    "label": 0
                },
                {
                    "sent": "So we don't have an unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "We have an estimator that has negative bias.",
                    "label": 0
                },
                {
                    "sent": "So the expected value of this important sampling estimator will be less than the true performance of our evaluation policy, which if you think to what are we about to do with this is going to be completely OK.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to do with this?",
                    "label": 0
                },
                {
                    "sent": "We're going to predict the performance of a policy and then ask, is this performance at least this good?",
                    "label": 0
                },
                {
                    "sent": "If we're underestimating the performance of this policy, we're just going to be conservative.",
                    "label": 0
                },
                {
                    "sent": "So Java.",
                    "label": 0
                },
                {
                    "sent": "Drop.",
                    "label": 0
                },
                {
                    "sent": "So there could be that.",
                    "label": 0
                },
                {
                    "sent": "What is the problem you're gonna stand for that license to buy here?",
                    "label": 0
                },
                {
                    "sent": "Sorry I'm having trouble hearing there a lot of squeaking chairs.",
                    "label": 0
                },
                {
                    "sent": "Positive probability sample advice if you buy is here.",
                    "label": 0
                },
                {
                    "sent": "No, we're in the discrete setting for here at least, so we would never sample something with Q of why being zero.",
                    "label": 0
                },
                {
                    "sent": "That's saying I'd sample an event with zero probability.",
                    "label": 0
                },
                {
                    "sent": "And in the continuous case, this would be a density function and the density wouldn't be 0.",
                    "label": 0
                },
                {
                    "sent": "For something like sample.",
                    "label": 0
                },
                {
                    "sent": "And in the yeah, I don't know in the full measure theoretic case if this holds up so important sampling works in the measure theoretic case where this ratio is the radon nikodym derivative and you need to absolute continuity and I don't know if you need absolute continuity.",
                    "label": 0
                },
                {
                    "sent": "Anyway that's a different question.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sorry, there's some weird noises also.",
                    "label": 0
                },
                {
                    "sent": "Learning problems.",
                    "label": 0
                },
                {
                    "sent": "Then mass functions you mean, yeah, so this definitely works for density functions, and I suspect it works for the full measure theoretic probability case.",
                    "label": 0
                },
                {
                    "sent": "Certainly if you have this assumption, which really means that I always mix this up, I think it's P is absolutely continuous with respect to Q.",
                    "label": 0
                },
                {
                    "sent": "If you have that, then this works in the measure theoretic setting.",
                    "label": 0
                },
                {
                    "sent": "Once you have this support.",
                    "label": 0
                },
                {
                    "sent": "Do we need to be?",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "We so in arc.",
                    "label": 0
                },
                {
                    "sent": "In the case that I've done and seen, these are always probability distribution, so they're normalized to integrate to one or sum to one.",
                    "label": 0
                },
                {
                    "sent": "If these were distributions that weren't probability distributions, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Yes, I will get to the ratios.",
                    "label": 0
                },
                {
                    "sent": "I think I see where this might be going, so I think I will answer that later.",
                    "label": 0
                },
                {
                    "sent": "Yes, I will answer that OK.",
                    "label": 0
                },
                {
                    "sent": "So we said.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's apply this to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Let's get out of this X&Y and go back to RLOX was the thing we wish we had, so these are histories produced by the evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "We have histories produced by the behavior policy.",
                    "label": 0
                },
                {
                    "sent": "The distribution P then is the distribution over histories under the valuation policy.",
                    "label": 0
                },
                {
                    "sent": "Q is the distribution over histories under the behavior policy.",
                    "label": 0
                },
                {
                    "sent": "And the function that we're evaluating F of H or F of X is the return the observed sums of rewards.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is then the target value we're trying to predict the expected value of F of X, so that's the expected return given that our trajectory's X come from the evaluation policy, it's the performance of the evaluation policy, and we're assuming that either the support of the evaluation policy is a subset of the support of the behavior policy, or that the returns are non negative.",
                    "label": 0
                },
                {
                    "sent": "I I'll talk about this more later.",
                    "label": 0
                },
                {
                    "sent": "I think I don't ask at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Let's plug all of this in and get our important sampling estimator.",
                    "label": 0
                },
                {
                    "sent": "It is this the important sampling estimator from one history coming from the behavior policy is the probability of that history under the evaluation policy divided by the probability of that history under the behavior policy times the return.",
                    "label": 0
                },
                {
                    "sent": "So think back to our intuition.",
                    "label": 0
                },
                {
                    "sent": "We want this weight to be large in the case where this is more likely under the evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "That's what this does.",
                    "label": 0
                },
                {
                    "sent": "If this is more likely under the evaluation policy, this is greater than one.",
                    "label": 0
                },
                {
                    "sent": "It's less likely this is less than one, so this is our important sampling estimator for RL.",
                    "label": 0
                },
                {
                    "sent": "This is the estimate from a single history, but we have many histories.",
                    "label": 0
                },
                {
                    "sent": "Our data consists of many different episodes, so if we have many, many histories, that's our data set D. We just average the important sampling estimates from each history, or put differently, it's the average over end histories of the important sampling estimates where this Artie is the reward at time T in that ice history.",
                    "label": 0
                },
                {
                    "sent": "OK. That question in the back was a very good one.",
                    "label": 0
                },
                {
                    "sent": "It was, but how do we compute these things?",
                    "label": 0
                },
                {
                    "sent": "How do we compute this importance?",
                    "label": 0
                },
                {
                    "sent": "Wait, we don't know the probability of a history under a policy so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at some more.",
                    "label": 0
                },
                {
                    "sent": "So what is the probability of a history under the evaluation policy that we put on the top?",
                    "label": 0
                },
                {
                    "sent": "Well, let's just run through all the terms of that history in sequential order, so it's the probability of the initial state times the probability that we take the first action that we took in that state.",
                    "label": 0
                },
                {
                    "sent": "The probability that we transition to state S2 and get reward are one.",
                    "label": 0
                },
                {
                    "sent": "Given that we were in that state and took that action.",
                    "label": 0
                },
                {
                    "sent": "And this is making the Markov assumption times the probability that we take the second action that we took times the probability of the second transition etc etc, right?",
                    "label": 0
                },
                {
                    "sent": "This is just writing out the probability of a trajectory.",
                    "label": 0
                },
                {
                    "sent": "The problem is we don't know these transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "We know our policy, but we don't know these.",
                    "label": 0
                },
                {
                    "sent": "But when we divide by the probability under a different policy, these terms are going to cancel.",
                    "label": 0
                },
                {
                    "sent": "We get all these same exact terms except for the action selection parts.",
                    "label": 0
                },
                {
                    "sent": "The policy terms.",
                    "label": 0
                },
                {
                    "sent": "So when we cancel all the probability of S1, the probability of the first transitions were left with just the Pi terms and we know pie.",
                    "label": 0
                },
                {
                    "sent": "That's the policy we're thinking of deploying in our current policy.",
                    "label": 0
                },
                {
                    "sent": "So put differently.",
                    "label": 0
                },
                {
                    "sent": "That importance ratio is the product overtime from initial time until the total horizon of our problem over the ratio of the action probabilities at each time step.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just plug that in.",
                    "label": 0
                },
                {
                    "sent": "This is what we had before we plug in that ratio, and we get this term in the middle.",
                    "label": 0
                },
                {
                    "sent": "And this is our important sampling estimator, so hopefully you could implement this at this point.",
                    "label": 0
                },
                {
                    "sent": "If I have a data set, this is average.",
                    "label": 0
                },
                {
                    "sent": "My weighted returns where there should be a little I hear in the reward.",
                    "label": 0
                },
                {
                    "sent": "So this is the action at time T in the ice history.",
                    "label": 0
                },
                {
                    "sent": "So this is all in the history and this is the return.",
                    "label": 0
                },
                {
                    "sent": "So if I gave you a data set and a behavior policy and evaluation policy, you can just implement this summon.",
                    "label": 0
                },
                {
                    "sent": "That's an unbiased estimate of the performance of this policy, and I got a little bit greedy in making this and thought I would go much, much faster.",
                    "label": 0
                },
                {
                    "sent": "So at some point we'll skip something and I think we'll skip this, so we're going to skip.",
                    "label": 0
                },
                {
                    "sent": "Will go through it really quickly.",
                    "label": 0
                },
                {
                    "sent": "It'll be fine, so.",
                    "label": 0
                },
                {
                    "sent": "Let's do something different over there question.",
                    "label": 0
                },
                {
                    "sent": "So yes, so if Pi is deterministic, that's completely OK. That means that this could be one, and this is anything that's fine.",
                    "label": 0
                },
                {
                    "sent": "Pi B cannot be deterministic if Pi B is deterministic, then this will actually always be one.",
                    "label": 0
                },
                {
                    "sent": "It will never be 0.",
                    "label": 0
                },
                {
                    "sent": "But if this is always one, what ends up happening?",
                    "label": 0
                },
                {
                    "sent": "We have that support problem and in the deterministic case that support problem means again, we're going to underestimate and if it's deterministic it's going to be 0.",
                    "label": 0
                },
                {
                    "sent": "So what will happen is our estimate will just always be 0 if we run a deterministic policy and the evaluation policy is different.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so keep the behavior policy stochastic if possible and if it's a if there's some cases where it's deterministic, try to match that deterministic part in your evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an idea.",
                    "label": 0
                },
                {
                    "sent": "Instead of using importance sampling to estimate the expected returns, let's use important sampling to estimate just one.",
                    "label": 1
                },
                {
                    "sent": "Reward their reward at time T. So this is something else that we can do instead of estimating the whole thing, just do one piece.",
                    "label": 0
                },
                {
                    "sent": "What do we get?",
                    "label": 0
                },
                {
                    "sent": "Well, the important sampling estimate for the reward at time T is average oversamples.",
                    "label": 0
                },
                {
                    "sent": "The weighted reward at time T were here, this this.",
                    "label": 0
                },
                {
                    "sent": "This wait is the probability not of the full history, but of the history up until time step T. Why is that?",
                    "label": 0
                },
                {
                    "sent": "It's because.",
                    "label": 0
                },
                {
                    "sent": "The history after time T has no bearing on the reward that we see at time T there.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, so knowing the history afterwards doesn't impact our estimate of this intuitively, and you can show that this remains unbiased.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how we would use important sampling to estimate one reward.",
                    "label": 0
                },
                {
                    "sent": "So what we could do is sum over these.",
                    "label": 0
                },
                {
                    "sent": "We can, we can estimate the sum overtime.",
                    "label": 0
                },
                {
                    "sent": "What are we doing here?",
                    "label": 0
                },
                {
                    "sent": "This is the other equation, so this is actually computing this ratio, so we put the 1 / N some reward and we're asking what is this ratio?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just the product of action probabilities to time T instead of to L, the end of the episode.",
                    "label": 0
                },
                {
                    "sent": "So if we have this estimate of RT under the evaluation policy, we can do per decision important sampling, which says we're going to just take the weighted sum of these estimates from each time step.",
                    "label": 0
                },
                {
                    "sent": "So estimate the first reward.",
                    "label": 0
                },
                {
                    "sent": "Add to that our estimate of what the second reward will be.",
                    "label": 0
                },
                {
                    "sent": "Add to that our estimate of what the third reward will be, etc.",
                    "label": 0
                },
                {
                    "sent": "And we get an estimate of the total return, and that looks like this.",
                    "label": 1
                },
                {
                    "sent": "So this is just plugging in the sum from T = 1 to L gamma T of our estimate, using importance sampling for the reward at time T. OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to skip this slide.",
                    "label": 0
                },
                {
                    "sent": "Actually, I want to point something out so we're going to skip this slide, but just look at this and what happens in important sampling.",
                    "label": 0
                },
                {
                    "sent": "If the evaluation policy and behavior policy are very, very different.",
                    "label": 0
                },
                {
                    "sent": "So what tends to happen here we get this is really the probability of histories, so we get histories where the probability of the behavior policy is larger.",
                    "label": 1
                },
                {
                    "sent": "The probability under the evaluation policy is smaller, so we tend to take actions that are likely under our sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "So this tends to be larger.",
                    "label": 0
                },
                {
                    "sent": "Tends to be smaller.",
                    "label": 0
                },
                {
                    "sent": "And so if we have a long sequence of these, this tends towards 0.",
                    "label": 0
                },
                {
                    "sent": "So as we get longer and longer episodes, or as these two policies become more and more different, the importance weights tend toward zero, and so this means that when these policies are sufficiently different or when horizons are sufficiently long.",
                    "label": 0
                },
                {
                    "sent": "It's not here.",
                    "label": 0
                },
                {
                    "sent": "Important sampling has high variance and it tends to just always say the estimate is 0.",
                    "label": 1
                },
                {
                    "sent": "The estimate is 0, the estimate is 0, which is a terrible estimate.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's the question of right?",
                    "label": 0
                },
                {
                    "sent": "So here's the case where important sampling tends to be zero when these are different.",
                    "label": 0
                },
                {
                    "sent": "So what is important sampling doing?",
                    "label": 0
                },
                {
                    "sent": "Initially?",
                    "label": 0
                },
                {
                    "sent": "It's estimates tend toward zero, and as we get more and more data, they tend towards the correct value and this isn't necessarily the behavior that we want.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an idea.",
                    "label": 0
                },
                {
                    "sent": "Ignore that.",
                    "label": 0
                },
                {
                    "sent": "Ignore this.",
                    "label": 0
                },
                {
                    "sent": "This came for something I didn't talk about.",
                    "label": 0
                },
                {
                    "sent": "Can we make a new estimator that is going to have much lower variance than important sampling, but has some bias so it's no longer going to be unbiased, but it's at least going to be consistent, so it will converge to the right.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value and this is what weighted importance sampling does.",
                    "label": 1
                },
                {
                    "sent": "So what is weighted important sampling?",
                    "label": 0
                },
                {
                    "sent": "Well, WI is our importance weight.",
                    "label": 0
                },
                {
                    "sent": "The important sampling estimate was just this.",
                    "label": 0
                },
                {
                    "sent": "The average of our weighted returns, and you can push the end in there onto the weight terms.",
                    "label": 0
                },
                {
                    "sent": "So I just rewritten important sampling.",
                    "label": 0
                },
                {
                    "sent": "What weighted important sampling does it says instead of dividing by N, let's divide by the total sum of our importance weights.",
                    "label": 0
                },
                {
                    "sent": "And here we're back to ordinary important sampling, not her decision for now.",
                    "label": 0
                },
                {
                    "sent": "So what does this do?",
                    "label": 0
                },
                {
                    "sent": "So you can view this whole term as a new weight, and what we've done is we've normalized these weights so that all of them sum to one.",
                    "label": 0
                },
                {
                    "sent": "So you can view this as a weighted average of the observed returns, and this has some real.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice properties, so first what is weight?",
                    "label": 0
                },
                {
                    "sent": "Important sampling equal to if we have only a single history I will take a break from talking to let you think about this.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have a?",
                    "label": 0
                },
                {
                    "sent": "An answer.",
                    "label": 0
                },
                {
                    "sent": "What was that?",
                    "label": 0
                },
                {
                    "sent": "Error.",
                    "label": 0
                },
                {
                    "sent": "It equals the original policy, so it equals.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not a policy, it's a number.",
                    "label": 0
                },
                {
                    "sent": "It's the you're getting at it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's the return from the the behavior policy so.",
                    "label": 0
                },
                {
                    "sent": "So what happens is we only have one wait.",
                    "label": 0
                },
                {
                    "sent": "We divide by that wait.",
                    "label": 0
                },
                {
                    "sent": "So these are just one.",
                    "label": 0
                },
                {
                    "sent": "And So what we get out is it's the observed return under our behavior policy.",
                    "label": 0
                },
                {
                    "sent": "This is the Monte Carlo estimator of the performance of the behavior policy.",
                    "label": 1
                },
                {
                    "sent": "We'd like to evaluate the evaluation policy.",
                    "label": 0
                },
                {
                    "sent": "This is estimating the performance of the behavior policy.",
                    "label": 0
                },
                {
                    "sent": "What happens, though?",
                    "label": 0
                },
                {
                    "sent": "As N increases, so there's this nice property which I won't run through, but it's straightforward to show that the expected value of these importance weights is always one.",
                    "label": 0
                },
                {
                    "sent": "So that means that this sum in the limit is converging to N, and so in the limit is we get infinite data that some of importance weights converges to end almost surely.",
                    "label": 0
                },
                {
                    "sent": "And when this is N, that's important sampling.",
                    "label": 0
                },
                {
                    "sent": "That's ordinary important sampling, so we have as an estimator that begins as a Monte Carlo estimator of the performance of the behavior policy, and then as we get more and more data, it acts more and more like important sampling.",
                    "label": 0
                },
                {
                    "sent": "So it's correct asymptotically, but it ends up having much much lower variance.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that is all I will say for weighted important sampling.",
                    "label": 0
                },
                {
                    "sent": "So we've gone through three forms of important sampling, important sampling, weighted important sampling, an per decision, important sampling.",
                    "label": 0
                },
                {
                    "sent": "There are many other types.",
                    "label": 0
                },
                {
                    "sent": "You can also do weighted per decision, which is sometimes called consistent, weighted per decision.",
                    "label": 0
                },
                {
                    "sent": "Also you can do special forms of important sampling when those supports aren't equal.",
                    "label": 0
                },
                {
                    "sent": "You can ignore important sampling and just build a model of your MVP and compute the performance of your valuation policy on that model of an MVP.",
                    "label": 0
                },
                {
                    "sent": "You can try to combine these model based estimators with important sampling and that's what these two.",
                    "label": 0
                },
                {
                    "sent": "Doubly robust and weighted doubly robust do.",
                    "label": 1
                },
                {
                    "sent": "You can also recognize that the environment might not be stationary, and so there's some time series prediction in this and combine important sampling time series prediction and there's one last one that does more combination of the model with important sampling and they'll be references for all of these at the end, so there's just to point out there's many more techniques for up.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Policy evaluation OK, So what do these results typically look like?",
                    "label": 1
                },
                {
                    "sent": "This is a typical plot for this is actually a toy domain, but it's representative that shows the amount of historical data that we have and the mean squared error of our different estimators.",
                    "label": 0
                },
                {
                    "sent": "And this really is what we tend to see.",
                    "label": 0
                },
                {
                    "sent": "These are log axes, so these are order of magnitude differences in error of these estimators.",
                    "label": 0
                },
                {
                    "sent": "So up here is important sampling per decision.",
                    "label": 0
                },
                {
                    "sent": "Important sampling tends to do a little bit better, it's lower variance, and if we jump to weighted important sampling, that's the.",
                    "label": 0
                },
                {
                    "sent": "Yellow line their weighted important sampling does roughly an order of magnitude, in this case better than ordinary important sampling, but that's a trade off.",
                    "label": 0
                },
                {
                    "sent": "It's no longer unbiased, so we're getting some bias, but that bias estimator is much more accurate and then waited per decision does a little bit better.",
                    "label": 0
                },
                {
                    "sent": "Still.",
                    "label": 0
                },
                {
                    "sent": "Next up is the doubly robust estimator that's combining with the model that's blue, and I think weighted WS is kind of if you want just the most accurate estimate, it's one of the state of the arts at the moment, and a key thing of this plot is to show yes, I've described these more sophisticated methods and it's tempting to say.",
                    "label": 0
                },
                {
                    "sent": "Oh well, I'll just implement important sampling first.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Three orders of magnitude approximately difference in performance, so it is important to use these more sophisticated techniques if you're actually trying to do policy evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's go on to the next step, which is how do we do high confidence off policy evaluation?",
                    "label": 1
                },
                {
                    "sent": "So we're creating this new black box that doesn't just predict the performance of the policy at lower bounds.",
                    "label": 0
                },
                {
                    "sent": "The performance, so it says that new policy will be at least this good.",
                    "label": 0
                },
                {
                    "sent": "That was the wrong button, OK?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way that we're going to do this is we're going to use our important sampling estimates, and we're going to put them through some method that takes our end important sampling estimates and spits out a confidence bound on their mean.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one way of doing this is using half things inequality, so let's review Hopkins inequality quickly.",
                    "label": 0
                },
                {
                    "sent": "Let X one to XN BN independent and identically distributed random variables that are between zero and B.",
                    "label": 1
                },
                {
                    "sent": "So the example I like to use for this is, let's say that we're going to measure let's pick N is 30.",
                    "label": 0
                },
                {
                    "sent": "I'm going to pick 30 people from the surface of the earth with replacement so that it's identically distributed, and I'm going to measure their Heights.",
                    "label": 0
                },
                {
                    "sent": "They measured the Heights of 30 different people.",
                    "label": 0
                },
                {
                    "sent": "We need these to be bounded between zero and B, so you can't have a negative height, so we're bounded by zero.",
                    "label": 0
                },
                {
                    "sent": "We need an upper bound of B, so the easy way to do this is you just define if someone is taller than 3 meters were just not defining them as human, so it's zero to three meters.",
                    "label": 0
                },
                {
                    "sent": "OK, that should not have shown up yet.",
                    "label": 0
                },
                {
                    "sent": "Pretend this isn't here yet.",
                    "label": 0
                },
                {
                    "sent": "So what huffing says is with probability at least one minus Delta.",
                    "label": 1
                },
                {
                    "sent": "This inequality holds, so the true expected value of our random variable, the true mean height of humans is at least the sample mean minus this term.",
                    "label": 0
                },
                {
                    "sent": "So it's the sample mean that we observed minus B.",
                    "label": 0
                },
                {
                    "sent": "That's the range of the random variable times root log one over Delta that comes from the probability that we want over 2 N, where N is the number of histories, the number of trajectories that we have.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know all of these terms.",
                    "label": 0
                },
                {
                    "sent": "We could implement this for example, and also notice that this matches intuition.",
                    "label": 0
                },
                {
                    "sent": "So the true mean height of humans is at least our sample mean minus some term in this term says how far down you have to go given the number of samples that you have a nice property of this is we're not making any other assumptions.",
                    "label": 0
                },
                {
                    "sent": "Nowhere in here are we saying these XI or normally distributed or anything of that sort, regardless of their distribution.",
                    "label": 0
                },
                {
                    "sent": "This will hold.",
                    "label": 0
                },
                {
                    "sent": "So how do we use this for important sampling while it's given away?",
                    "label": 0
                },
                {
                    "sent": "Or for reinforcement learning is given away by the part that came too soon.",
                    "label": 0
                },
                {
                    "sent": "We're just going to plug our important sampling estimators in here.",
                    "label": 0
                },
                {
                    "sent": "The important sampling estimator is an unbiased estimate of the true performance of this evaluation policy, so the expected return of the evaluation policy is at least our important sampling estimator minus this, so this gives us a lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unfortunately, this doesn't work very well, so if we actually plug this in for mountain car where we normalize returns so that they are always within 01 and we pick an evaluation policy whose performance is .19, so it's not a very good policy.",
                    "label": 0
                },
                {
                    "sent": "We can then pick 100,000 trajectories of data of historical data.",
                    "label": 0
                },
                {
                    "sent": "Remember on that other slide we solve this in something like 2 trajectories, so we're going to take a ton of data and we're going to then lower bound.",
                    "label": 0
                },
                {
                    "sent": "The performance of this evaluation policy using these 100,000 trajectories from, I think the uniform random policy.",
                    "label": 0
                },
                {
                    "sent": "So what are we doing?",
                    "label": 0
                },
                {
                    "sent": "We're taking those 100,000 trajectories, computing the importance weighted returns.",
                    "label": 0
                },
                {
                    "sent": "Then we're taking those an subtracting off from the importance weighted returns this term B root log one over Delta over 2 N, and we get negative 5.8 million, which is useless.",
                    "label": 0
                },
                {
                    "sent": "'cause we normalized returns to be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So this is just not at all useful lower bounds.",
                    "label": 0
                },
                {
                    "sent": "So what went wrong?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What went wrong?",
                    "label": 0
                },
                {
                    "sent": "Ignore this, they skipped over it.",
                    "label": 0
                },
                {
                    "sent": "What went wrong?",
                    "label": 0
                },
                {
                    "sent": "Is this range term B?",
                    "label": 0
                },
                {
                    "sent": "We use the range of our random variable and the important sampling estimator can have a huge range.",
                    "label": 0
                },
                {
                    "sent": "The range scales with exponentially with the horizon of the problem, so how?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We fix this.",
                    "label": 0
                },
                {
                    "sent": "Well, we can just not.",
                    "label": 0
                },
                {
                    "sent": "Yusuf things inequality there.",
                    "label": 0
                },
                {
                    "sent": "More sophisticated sophisticated concentration inequality is that we can use.",
                    "label": 0
                },
                {
                    "sent": "This is one of them.",
                    "label": 0
                },
                {
                    "sent": "And this is actually the one that I would recommend.",
                    "label": 0
                },
                {
                    "sent": "It comes from the triple AI paper which looks really scary, but it's all terms that we can know.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of plugging in values and letting it chug along.",
                    "label": 0
                },
                {
                    "sent": "And if you use remember the actual performance of this policy is 0.19.",
                    "label": 0
                },
                {
                    "sent": "Hufton was negative, 5.8 million.",
                    "label": 0
                },
                {
                    "sent": "You might say using Empirical Bernstein bound if you're familiar with those.",
                    "label": 0
                },
                {
                    "sent": "That also doesn't work.",
                    "label": 0
                },
                {
                    "sent": "There's an obscure one called Anderson in mass starts inequality, which actually works reasonably well.",
                    "label": 0
                },
                {
                    "sent": "We can get much better using this cut inequality, so just use a different inequality, but the intuition is the same.",
                    "label": 1
                },
                {
                    "sent": "There's another thing that we could do, which is forget using these exact concentration bounds and make some false assumptions.",
                    "label": 0
                },
                {
                    "sent": "So which I say too lightly, so this is students T test.",
                    "label": 0
                },
                {
                    "sent": "It says if the sum of RN random variables is normally distributed then this inequality holds the true sample mean.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the true mean is at least our sample mean minus this other term where this is the standard deviation and that's the T statistic and this tends to be much, much tighter than nothing's inequality, but it's relying on the assumption.",
                    "label": 0
                },
                {
                    "sent": "That the mean of our random variable variables is normally distributed.",
                    "label": 0
                },
                {
                    "sent": "Thankfully, the.",
                    "label": 0
                },
                {
                    "sent": "Central limit Theorem Yes tells us that the sum of any number of IID random variables tends to be normally distributed as N increases, so this isn't a terrible assumption to make.",
                    "label": 0
                },
                {
                    "sent": "It's often largely true, and there's another nice result, which is that if the rewards are not negative, then the T tests tends to be conservative, and this has to do with how this has to do with how the test works.",
                    "label": 0
                },
                {
                    "sent": "When I have different skews on my random variable, so I'm going to skip over Bootstrap, it's just another competing way.",
                    "label": 0
                },
                {
                    "sent": "This tends to mean that if I ask for some probability of failure.",
                    "label": 0
                },
                {
                    "sent": "And I look at the number of samples that I have and I ask how often is this concentration bound failing?",
                    "label": 0
                },
                {
                    "sent": "If I use half things or any of the other concentration bounds, they pretty much never have error rates that match Delta, and that's 'cause they're overly conservative.",
                    "label": 0
                },
                {
                    "sent": "They allow for any distribution of data, whereas when I use students T test for these important sampling estimates, it tends to still be conservative, whereas bootstrap methods give tighter concentration tighter bounds, but sometimes have higher error rates than we want.",
                    "label": 0
                },
                {
                    "sent": "And these are just results saying that yes, for mountain car we can actually do this.",
                    "label": 0
                },
                {
                    "sent": "We can lower bound the performance of this policy, and the big takeaway is these are most of the methods that we talked about.",
                    "label": 0
                },
                {
                    "sent": "This is using weighted and poor decision with the bootstrap confidence bounds.",
                    "label": 0
                },
                {
                    "sent": "Key.",
                    "label": 0
                },
                {
                    "sent": "Thing is this is zero to one and this is zero to negative, 5 to the 45th.",
                    "label": 0
                },
                {
                    "sent": "This is huffing with important sampling so it just reiterates.",
                    "label": 0
                },
                {
                    "sent": "You can't just use half things inequality important sampling and get this to work.",
                    "label": 0
                },
                {
                    "sent": "You have to use some of these more sophisticated methods.",
                    "label": 0
                },
                {
                    "sent": "We also did this on actual digital marketing data and argued that some new policy for digital marketing would be better than existing policies, and I'm going to skip over this quickly.",
                    "label": 0
                },
                {
                    "sent": "And essentially just says what I've said here.",
                    "label": 0
                },
                {
                    "sent": "If you want it later is pseudocode, which I have not verified beyond mouse.",
                    "label": 0
                },
                {
                    "sent": "So if you find a mistake, email me and I will correct it, but I think it's right.",
                    "label": 0
                },
                {
                    "sent": "This is pseudocode for doing weighted per decision.",
                    "label": 0
                },
                {
                    "sent": "Important sampling with students T test so you can go through this and implement it and it will give you a lower bound that holds with probability 1 minus Delta on the performance of this evaluation policy approximately OK.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about the last thing which is safe policy improvement.",
                    "label": 0
                },
                {
                    "sent": "We wanted to make that box that took the historical data the probability and produces a new policy that gives our safety guarantee.",
                    "label": 0
                },
                {
                    "sent": "How do we do this using the components that we've put together so far?",
                    "label": 0
                },
                {
                    "sent": "Well, we do it like this.",
                    "label": 0
                },
                {
                    "sent": "So we take our historical data and we split it into two sets.",
                    "label": 0
                },
                {
                    "sent": "The first set is we're going to call the training set and the 2nd is the testing set and this is completely arbitrary.",
                    "label": 0
                },
                {
                    "sent": "But for now we tend to split it into 20% data in the training set and 80% in the testing set.",
                    "label": 0
                },
                {
                    "sent": "We then take the testing set and we pick one single policy Pi C that we think is going to be a really good policy.",
                    "label": 0
                },
                {
                    "sent": "So you could in your head imagine that we're running fitted Q iteration on this batch of data and saying, here's the policy that we predict is going to be best.",
                    "label": 0
                },
                {
                    "sent": "So we pick a policy that we think is good from this training set.",
                    "label": 0
                },
                {
                    "sent": "Just the batch RL problem.",
                    "label": 0
                },
                {
                    "sent": "We then take that policy and use the other 80% of our data to run a safety test.",
                    "label": 0
                },
                {
                    "sent": "What is the safety test?",
                    "label": 0
                },
                {
                    "sent": "Well, it's our black box from the previous section.",
                    "label": 0
                },
                {
                    "sent": "We just compute a lower bound on the oops we compute a lower bound on this policy's performance.",
                    "label": 0
                },
                {
                    "sent": "Using are 80% of the data.",
                    "label": 0
                },
                {
                    "sent": "So we take that 80%.",
                    "label": 0
                },
                {
                    "sent": "We use important sampling to predict this policy's performance, and if we were using huffing, we then subtract off that term.",
                    "label": 0
                },
                {
                    "sent": "From huffing, you're using T tests.",
                    "label": 0
                },
                {
                    "sent": "We subtract off the standard deviation of the important sampling estimates divided by root N times the pizza statistic.",
                    "label": 0
                },
                {
                    "sent": "And that's all that we do, so we use one of these high confidence off policy policy evaluation methods.",
                    "label": 1
                },
                {
                    "sent": "So once we have a lower bound on this policy's performance, we check is that lower bound, at least the performance of our current policy.",
                    "label": 0
                },
                {
                    "sent": "If it is, then we deploy the new policy.",
                    "label": 0
                },
                {
                    "sent": "If it's not, then we don't deploy the new policy.",
                    "label": 0
                },
                {
                    "sent": "We say no solution found.",
                    "label": 0
                },
                {
                    "sent": "We say my lower bound was too was too low.",
                    "label": 0
                },
                {
                    "sent": "I'm not getting the guarantee I want.",
                    "label": 0
                },
                {
                    "sent": "So say stick with the current policy.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a catch, and the catch is shown by this figure, so let this blue box be the space of all possible policies and let's talk about how we choose the candidate policy.",
                    "label": 0
                },
                {
                    "sent": "So this is our current policy and this is the policy that maybe we predict is the best policy from our training data.",
                    "label": 0
                },
                {
                    "sent": "Then maybe actually this policy is better.",
                    "label": 0
                },
                {
                    "sent": "So maybe this gradient shows that policies out in this direction perform better than policies out in this direction, so performance gets better as we go this way.",
                    "label": 0
                },
                {
                    "sent": "However, the tightness of our lower bounds goes the other way, so remember important sampling.",
                    "label": 0
                },
                {
                    "sent": "We get higher and higher variance as the policies become more and more different because those ratios get bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "Important sampling tends to give that output of 00 our bounds get looser and looser as we get farther and farther away from this current policy, not just in this direction.",
                    "label": 0
                },
                {
                    "sent": "Anytime we get farther away from our current policy, our bounds tend to get looser.",
                    "label": 0
                },
                {
                    "sent": "So this policy might be one that's much, much better, but the lower bound that we got on its performance can be really, really low, and so we could say we think this is great, but we can't guarantee anything, and our algorithm would return no solution found, so the optimal policy, the optimal candidate policy to return is somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "It's one that balances this tradeoff.",
                    "label": 0
                },
                {
                    "sent": "It's one that we predict back here.",
                    "label": 0
                },
                {
                    "sent": "The optimal candid policy is one that we predict is going to pass this safety test and subject to that, it's the one that we think is going to perform the best.",
                    "label": 0
                },
                {
                    "sent": "So it's the one where we don't go so far that we're unable to get a tight bound on its performance, and so we say it's unsafe.",
                    "label": 0
                },
                {
                    "sent": "But we go as far as we can to get as much improvement in performance, and that just high level intuition.",
                    "label": 0
                },
                {
                    "sent": "What this ends up meaning is we need some sort of regularization to keep us close to our current policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the last five minutes, let's just go through a couple of results to say yes, this really works.",
                    "label": 0
                },
                {
                    "sent": "It's not just.",
                    "label": 0
                },
                {
                    "sent": "Math theory to say hey, apply these bounds.",
                    "label": 0
                },
                {
                    "sent": "OK so these are results from mountain car modified shorter horizon, so whenever you take an action agent keeps taking that action for 20 time steps.",
                    "label": 0
                },
                {
                    "sent": "We're running the random policy.",
                    "label": 0
                },
                {
                    "sent": "Oh first we we normalized returns so that returns fall between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So the optimal policy has an expected return of 1.",
                    "label": 0
                },
                {
                    "sent": "This where this black bar is actually the performance of the random policy.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is asking our safe policy improvement.",
                    "label": 0
                },
                {
                    "sent": "For them to produce a new policy that with 95% probability is better and actually in all of the results will show we're going to use 95% probability and we're using different variants of the algorithms and different amounts of historical data.",
                    "label": 0
                },
                {
                    "sent": "So using just 50 trajectories, these black bars correspond to using that cut inequality and exact concentration bound, whereas these dashed bars correspond to using an approximate one, so they're actually using a bootstrap method called BCA.",
                    "label": 0
                },
                {
                    "sent": "But intuitively you can think of this as being very similar to using students T test.",
                    "label": 0
                },
                {
                    "sent": "To do that, bounding so using that approximate concentration bound were actually able to get policies returned sometimes with just 50 trajectories, using just 50 trajectories where sometimes getting here's a policy that is better and this will not be wrong more than 5% of the time, approximately when we're doing the dashed bars, 'cause of students T test as we go up, the approximate methods are solving mountain car in 200 to 900 episodes, whereas the exact methods take more but.",
                    "label": 0
                },
                {
                    "sent": "With 5000 trajectories, we're just instantly jumping to an optimal policy, so this isn't the same as a perfectly tuned natural actor critic, for example, but we're not taking centuries worth of data in order to do this.",
                    "label": 0
                },
                {
                    "sent": "We can also do this incrementally so we can run this repeatedly.",
                    "label": 0
                },
                {
                    "sent": "We can gather 50 trajectories, run our algorithm, got there another 50 trajectory's, run our algorithm, and then we get these learning curves, and so this is mountain car using the student's T tests like method, the Bootstrap method and this is using our exact methods, so we're solving it in just hundreds of episodes which when we look at it now, we think, Oh well, we can solve this really quickly with an unsafe method.",
                    "label": 0
                },
                {
                    "sent": "This seems like, oh, that's only OK, but to put some context on this, when we started trying to do this back in 2014, if we tried to do this.",
                    "label": 0
                },
                {
                    "sent": "This would take us millions and millions of episodes, so by all of these advances in policy evaluation, we're able to get this to kind of practical, tractable amount of data.",
                    "label": 0
                },
                {
                    "sent": "We can also do some other cool things, so instead of saying I want your performance to be at least the performance of the current policy, there's nothing stopping us from saying you need to be significantly better, or I'm not going to change anything, so this is mountain car where the red correspond to different runs of our algorithm, and we're saying guarantee that you're at least so this is the performance of the current policy.",
                    "label": 0
                },
                {
                    "sent": "Guarantee that you're at least the performance of this blue bar before you change the policy.",
                    "label": 0
                },
                {
                    "sent": "So I need some significant improvement before you change anything, and our algorithm waits until it has enough data to change the policy so that it's better than the blue bar and it's still being conservative when it does that.",
                    "label": 0
                },
                {
                    "sent": "This to give some context is the performance of just a well tuned natural actor critic, so we're not lagging too far behind it.",
                    "label": 0
                },
                {
                    "sent": "OK, we also applied this to some simulated digital marketing experiments.",
                    "label": 0
                },
                {
                    "sent": "So just to summarize these results, they say that using a few hundred 100,000 users data we are often able to say here's a policy.",
                    "label": 0
                },
                {
                    "sent": "It is better than what you're currently doing for serving up advertisements, so we're right in the ballpark of where we want to be.",
                    "label": 0
                },
                {
                    "sent": "This again results from a different internal simulator at Adobe Research.",
                    "label": 0
                },
                {
                    "sent": "This was a simple toy problem that we mocked up just to illustrate.",
                    "label": 0
                },
                {
                    "sent": "Again, the safety property and this is a very simple mockup of a digital marketing problem which the main difference between why this takes hundreds of thousands of users data is that almost no one clicks on advertisements, so our reward signal is very sparse and in this mock up we just pretended that people actually like dads and they click on them so we get higher click rates.",
                    "label": 0
                },
                {
                    "sent": "So this curve here is a natural actor critic with optimized hyperparameters, and if we just take the step size and I think it was .01 or something and we change it to .02, it learns faster and then it completely diverges and this is the problem.",
                    "label": 0
                },
                {
                    "sent": "We have these hyperparameters that when we set them wrong it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "This is the very first time we ever ran our algorithm on this problem.",
                    "label": 0
                },
                {
                    "sent": "It's guaranteed that it's not going to produce worse performance.",
                    "label": 0
                },
                {
                    "sent": "It's not going to diverge down to worst policies, so we are slower, but we're giving that safety guarantee, and I'd like to wrap up with that illustrative example that I started with, which is diabetes treatment.",
                    "label": 0
                },
                {
                    "sent": "So this was the problem of can we decide how much insulin to inject in order to keep someone's blood sugar in your optimal levels.",
                    "label": 0
                },
                {
                    "sent": "The study by Mason Bastoni at the University of Alberta, showing that this is an RL problem, used a simulator called T1DMS.",
                    "label": 0
                },
                {
                    "sent": "And this is a simulator of this entire metabolic process, and in the US, if you want to do studies on one of these controllers on humans, you have to 1st do studies on rats.",
                    "label": 0
                },
                {
                    "sent": "Or you can use this simulator.",
                    "label": 0
                },
                {
                    "sent": "So it's not just an arbitrary simulator, we act together, it is a high quality simulator.",
                    "label": 0
                },
                {
                    "sent": "So we tried to reproduce their results, but using one of these safe algorithms and this shows the amount of data we're using for a simulated subject.",
                    "label": 0
                },
                {
                    "sent": "The amount of days of data that we took.",
                    "label": 0
                },
                {
                    "sent": "And the probability that we're able to change the persons treatment policy to be something different.",
                    "label": 0
                },
                {
                    "sent": "So how often were we able to say here's something that's different from the current policy while ensuring that with 95% probability we don't make matters worse, and there is another catch we did something else a little bit different.",
                    "label": 0
                },
                {
                    "sent": "Here we really care mainly about hypoglycemia, low blood sugar.",
                    "label": 0
                },
                {
                    "sent": "So our probabilistic guarantees with respect to low blood sugar.",
                    "label": 0
                },
                {
                    "sent": "So he said try to keep blood sugar near optimal levels while ensuring that we don't increase the amount of low blood sugar that we get.",
                    "label": 0
                },
                {
                    "sent": "So where is this where we wanted three to six months?",
                    "label": 0
                },
                {
                    "sent": "That's 92180 days, right?",
                    "label": 0
                },
                {
                    "sent": "Within that window is where we're able to return policies that are better.",
                    "label": 0
                },
                {
                    "sent": "So we're using the amount of data that we want.",
                    "label": 0
                },
                {
                    "sent": "What is the spot this plot shows?",
                    "label": 0
                },
                {
                    "sent": "What is the probability that we actually did something bad?",
                    "label": 0
                },
                {
                    "sent": "So we asked for a 5% probability that we increase the prevalence of hypoglycemia.",
                    "label": 0
                },
                {
                    "sent": "So given different amounts of data, how likely where we to produce a policy that's worse?",
                    "label": 0
                },
                {
                    "sent": "Where the blue line here, not once across any of our trials, did this increase the prevalence of hypoglycemia, and that's because we're still using concentration bounds that tend to be overly conservative.",
                    "label": 0
                },
                {
                    "sent": "What is the red line?",
                    "label": 0
                },
                {
                    "sent": "This is what you get if you use a well tuned standard RL method and it kind of explains what's going on 1st.",
                    "label": 0
                },
                {
                    "sent": "It always returns policy and what happens is early on we're getting some data set and it's random.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of randomness in when meals are eaten, errors and how big the meal actually was.",
                    "label": 0
                },
                {
                    "sent": "And because of this randomness in the data set, we can draw false conclusions.",
                    "label": 0
                },
                {
                    "sent": "Randomness in the data set can make us think that something is better when it's really just a fluke of the data, and so that's what's happening here we're choosing policy is to return that we think are better, but really was a fluke of the data that made us think this and what our algorithms are doing is they're looking both at the policy that we're proposing and the data we used to create it, and asking, can we conclude that this isn't a fluke of the data that this policy really is better?",
                    "label": 0
                },
                {
                    "sent": "And if it is, then we start returning it so you'll notice right when these policies tend to typically be?",
                    "label": 0
                },
                {
                    "sent": "Actually, better policies we start frequently returning policies.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "We don't, so that was one of the limitations of the setting at the start is we assume that we have an initial policy that we're willing to deploy if we have to do cold starts, and we're trying to ensure this sort of safety, you need something else.",
                    "label": 0
                },
                {
                    "sent": "You need something like a prior distribution over models, and that's when you get to kind of the adaptive control style approaches.",
                    "label": 0
                },
                {
                    "sent": "But this does not solve cold start.",
                    "label": 0
                },
                {
                    "sent": "So to wrap up, in my last negative one minute.",
                    "label": 0
                },
                {
                    "sent": "Just to conclude, there are many different definitions of safety.",
                    "label": 0
                },
                {
                    "sent": "We talked about one ensuring that policies are better with high probability.",
                    "label": 0
                },
                {
                    "sent": "We went through the three steps to creating a safe algorithm predicting the performance of a policy without deploying it, bounding the performance of a policy without deploying it, and then putting those pieces together to actually do policy improvement.",
                    "label": 0
                },
                {
                    "sent": "And then we talked about some empirical results.",
                    "label": 0
                },
                {
                    "sent": "To say this is actually possible.",
                    "label": 0
                },
                {
                    "sent": "We can do this using realistic amounts of data and some future directions which you can look at later offline.",
                    "label": 0
                },
                {
                    "sent": "There are many open questions still to make this better.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of shown by the list of references.",
                    "label": 0
                },
                {
                    "sent": "A lot of these are 2017 2016 references.",
                    "label": 0
                },
                {
                    "sent": "This is something that's kind of happening now ish, so there are a ton of open questions left to be answered.",
                    "label": 0
                },
                {
                    "sent": "And if you'd like to know more about this, the 1st place to start really is doing a pre cups paper from 2000 which goes into important sampling.",
                    "label": 0
                },
                {
                    "sent": "Though the weighted precision estimator is a typo.",
                    "label": 0
                },
                {
                    "sent": "Something to be wary of and that's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}