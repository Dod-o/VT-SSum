{
    "id": "6ewrce2cfkyhvsfl766ngiygldt7qqdp",
    "title": "Multidataset Independent Subspace Analysis",
    "info": {
        "author": [
            "Rogers F. Silva, Department of Electrical and Computer Engineering, University of New Mexico"
        ],
        "published": "July 27, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_silva_subspace_analysis/",
    "segmentation": [
        [
            "Alright, so I'm Roger Silva.",
            "I'll be talking about the work that I did at the Mind Research Network on multi data set independent subspace analysis, an extension of independent subspace analysis for multiple datasets.",
            "Specifically for multimodal brain imaging data, which is the focus of our research, it was done in collaboration with circuit.",
            "Please, Talea Dolly Mario's Boutique is an Vince Calhoun."
        ],
        [
            "So OK, before we begin, let's consider the case of optimizing for independence, which is what I'll be showing here.",
            "There are two things to take into account.",
            "First is the order of the factors will not make a difference.",
            "On the computation of on the value that you will get your estimate of independence.",
            "The second thing is if you scale these independent variables arbitrarily, the sizes over the estimates of the independence are also not going are going to be invariant to that.",
            "So they're not going to change.",
            "And here I'm going to give an example of just to set up the problem that I'm trying to solve.",
            "Consider the case of independent images.",
            "And specifically, color images where the mic the data that these images get mixed in different ways on each of its channels.",
            "So there is a mixture process that could be linear or nonlinear which transforms or which mixes the each channel in separately so the mixture process is different for each one of these channels.",
            "And then this mixed data is what we use too.",
            "Recover.",
            "The original image is an one way we could do that is by separately processing each one of these images with something like independent component analysis.",
            "So well studied, the well developed method, and then after the fact just trying to combine the results.",
            "The problem with that is that.",
            "As you increase the number of components, this matching problem becomes combinatorial, so it's better to have a model that understands that these different components from different datasets are related and not entirely independent.",
            "So there is independence within the datasets, but there is cross communication across datasets which indicates which components are related and the entire decomposition on all three channels.",
            "Can therefore be learned.",
            "Simultaneously, which basically solves not only the separation but also the.",
            "The combination part of the problem and then yet another extension of this problem is when we have images that are very similar.",
            "They are dependent rather than independent, so we cannot separate it with independence.",
            "But we might want to do instead is.",
            "Separate.",
            "Those similar images into a separate group which is independent of the other of other groups of images, so it brings the idea or the notion of.",
            "Groups of images.",
            "Groups of independent images."
        ],
        [
            "So.",
            "This brings us to the idea that."
        ],
        [
            "But there is a hierarchy of models or of problems actually.",
            "That tried to explore independence on single datasets.",
            "Here's an example of what ICA would look like.",
            "Basically you have a transformation from.",
            "Into a smaller subspace from the data.",
            "Basically minimizing KL divergences.",
            "And here is an example of what the output nonlinearities will look like in the case of assuming.",
            "Laplace prior?",
            "Distributions of these.",
            "Latent variables and here the latent variables are the images that we're trying to recover."
        ],
        [
            "Now we can extend that with the example that I showed to a model that is more complex, where here I color coded.",
            "I don't know if you can see it.",
            "Yeah, I color coded components that should have some built independence and should be put together and the way to retain that dependence within a data store within those.",
            "Within that, what I call a subspace is to model the into these nonlinear transformation joint distribution of these.",
            "Components.",
            "Yeah.",
            "So."
        ],
        [
            "So we see that there is a nice transition from single datasets to multiple datasets."
        ],
        [
            "And but we want to consider now the case of multiple.",
            "Groups of sources.",
            "In a single data set."
        ],
        [
            "And that's the case of independent subspace analysis.",
            "That's an example.",
            "For on a single data set, or we again I'm trying to make these subspaces independent.",
            "But the subspaces are composed of sets of.",
            "Not entirely independent or not, maybe not at all independent components.",
            "So that highlight."
        ],
        [
            "That there is another hierarchy in the direction of going from unit dimensional components on a single data set versus multidimensional components.",
            "Again, in one data set."
        ],
        [
            "And putting these ideas altogether, but we can have a more complicated model or more general model.",
            "Better way, better set.",
            "Where?",
            "This components from multiple data sets can be arbitrarily put together and their joint dependence can be modeled.",
            "It's just a matter of picking a distribution that doesn't factor at least not easily, so that joint distribution could be something like a multivariate Laplace again.",
            "We just use this assignment matrix to indicate it's basically a set of businesses to indicate which components from each modality should.",
            "Make each of the subspaces, yeah.",
            "Supplied by you or there you can do either way and I'm going to show a way that you can learn them, but they are at least in this model.",
            "There is strictly sparse, so every component just goes only into one subspace.",
            "So if you were to think of this second layer as.",
            "Fully connected layer.",
            "Lots of those layers with most of those layers would be 0 so."
        ],
        [
            "OK, so just to complete the idea, you can extend these notions of generalized models into the cases of what kind of statistics are you modeling from from the data.",
            "So there are methods that look at 2nd order statistics only.",
            "This could be Canonical correlation analysis for example, and you could some methods like inflamax just look at.",
            "Higher order statistics.",
            "So the data."
        ],
        [
            "I'm going to try to sell here is that we can do both, and that's going to get better results."
        ],
        [
            "So a bit of review.",
            "One cool idea is to instead of.",
            "So historically these methods depend on data reduction.",
            "Because these.",
            "There is a you have to compute the determinant of the these weight matrices.",
            "If you want to.",
            "To go the traditional route and when you have these weight matrices that are actually reducing the space, you can compute the determinant.",
            "So one interesting idea in the literature was to use another encoder to learn the latent representation at the same time you learn these independent representations.",
            "And in this case the.",
            "The example is from.",
            "Lee and colleagues.",
            "Who basically proposed a simple linear encoder to do the task and use the auto encoder as a regularizer for the independence cost.",
            "So that's an interesting idea an what I decided to do was to borrow from there an except instead of using the transpose like it's indicated here, I just substituted it by the pseudoinverse with the motivation to make it.",
            "More general for the case of non white data, which wasn't an assumption in the original.",
            "Publication.",
            "So with this formulation I applied it then to every data set individually and that way we can learn these subspaces as we optimize for these independence of places.",
            "Yeah, so to clarify, the auto encoders are run first.",
            "2.",
            "Reduce the data in a way to find this reduced space.",
            "And then the.",
            "Optimization for independence starts.",
            "But it continues using the encoder as a constraint to basically prevent it from from losing that needs representation that it already or basically allows it to search within that smaller subspace.",
            "That reduced representation search for directions that have independence."
        ],
        [
            "So the idea here I just wanted to show that these general models simplifies into more traditional models and.",
            "I at this point I wanted to mention that.",
            "So there is a combinatorial problem to solve.",
            "To find this matrix, be here.",
            "Ann, the way I'm solving it here is initially fix this weights, fix the matrix P, optimize the cost function.",
            "Once you learn this latent representations, they might be incorrect and then what you do is what I did at least is.",
            "Simple greedy combinatorial optimization, just.",
            "Intuitively, just pairing these sources together, whether or not they grouping them together causes a reduction in a class function.",
            "So it's a simple strategy and."
        ],
        [
            "Did actually give some really nice results.",
            "Slightly quickly on this one, just showing an example on basic independent component analysis.",
            "The method compared to inflamax.",
            "Now is in general performing considerably better.",
            "We don't need to do data reduction here, we're just using the auto encoder to do this.",
            "To learn these representations directly from the data."
        ],
        [
            "In the case of independent subspace analysis, the.",
            "No, no, the the combinatorial optimization is critical to actually get any any good results because otherwise.",
            "We tend very often to find a local minima.",
            "An doing that sweep over the components and trying to rearrange them together into groups actually moves the the current solution out of the local minimum has that effect and then restarting the optimization for the original independence cost.",
            "Then can actually find.",
            "Can lower the costs even further.",
            "So basically with an effect of."
        ],
        [
            "Moving out of local minima.",
            "So an example on if you are going to skip this one, but it's doing much better than state of the art."
        ],
        [
            "And finalize with the hybrid example.",
            "This is basically took brain imaging data from different modalities.",
            "From three different publications and artificially made the like these triplets here, there are three of them.",
            "So the first top panel here.",
            "There are three images.",
            "These three images are dependent.",
            "And but they are independent from the others that I'm showing the bottom.",
            "So basically showing that the method can learn these representations directly from the data datasets are quite big and.",
            "The results were obtained with just 600 examples.",
            "Training on just 600 examples, and they can learn pretty well showing the ground truth versus the estimate so you can see that for each one of these pairs, do the Maps that are recovered pretty."
        ],
        [
            "Close to the ground truth.",
            "So the next thing we want to try now is obviously try and substitute those linear transformations by deeper nonlinear transformations and explore these non linear subspaces independent subspaces.",
            "Perhaps modified the optimization strategy.",
            "The community optimization for something that directly.",
            "They can simultaneously estimate those connections as we're learning the subspaces, independence of spaces and finally.",
            "Try and combine these with other more.",
            "Complicated architectures like or just different architectures like recurrent neural Nets.",
            "Yeah, are there any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm Roger Silva.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about the work that I did at the Mind Research Network on multi data set independent subspace analysis, an extension of independent subspace analysis for multiple datasets.",
                    "label": 1
                },
                {
                    "sent": "Specifically for multimodal brain imaging data, which is the focus of our research, it was done in collaboration with circuit.",
                    "label": 0
                },
                {
                    "sent": "Please, Talea Dolly Mario's Boutique is an Vince Calhoun.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, before we begin, let's consider the case of optimizing for independence, which is what I'll be showing here.",
                    "label": 0
                },
                {
                    "sent": "There are two things to take into account.",
                    "label": 0
                },
                {
                    "sent": "First is the order of the factors will not make a difference.",
                    "label": 0
                },
                {
                    "sent": "On the computation of on the value that you will get your estimate of independence.",
                    "label": 0
                },
                {
                    "sent": "The second thing is if you scale these independent variables arbitrarily, the sizes over the estimates of the independence are also not going are going to be invariant to that.",
                    "label": 0
                },
                {
                    "sent": "So they're not going to change.",
                    "label": 0
                },
                {
                    "sent": "And here I'm going to give an example of just to set up the problem that I'm trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Consider the case of independent images.",
                    "label": 0
                },
                {
                    "sent": "And specifically, color images where the mic the data that these images get mixed in different ways on each of its channels.",
                    "label": 0
                },
                {
                    "sent": "So there is a mixture process that could be linear or nonlinear which transforms or which mixes the each channel in separately so the mixture process is different for each one of these channels.",
                    "label": 0
                },
                {
                    "sent": "And then this mixed data is what we use too.",
                    "label": 0
                },
                {
                    "sent": "Recover.",
                    "label": 0
                },
                {
                    "sent": "The original image is an one way we could do that is by separately processing each one of these images with something like independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "So well studied, the well developed method, and then after the fact just trying to combine the results.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is that.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of components, this matching problem becomes combinatorial, so it's better to have a model that understands that these different components from different datasets are related and not entirely independent.",
                    "label": 0
                },
                {
                    "sent": "So there is independence within the datasets, but there is cross communication across datasets which indicates which components are related and the entire decomposition on all three channels.",
                    "label": 0
                },
                {
                    "sent": "Can therefore be learned.",
                    "label": 0
                },
                {
                    "sent": "Simultaneously, which basically solves not only the separation but also the.",
                    "label": 0
                },
                {
                    "sent": "The combination part of the problem and then yet another extension of this problem is when we have images that are very similar.",
                    "label": 0
                },
                {
                    "sent": "They are dependent rather than independent, so we cannot separate it with independence.",
                    "label": 0
                },
                {
                    "sent": "But we might want to do instead is.",
                    "label": 0
                },
                {
                    "sent": "Separate.",
                    "label": 0
                },
                {
                    "sent": "Those similar images into a separate group which is independent of the other of other groups of images, so it brings the idea or the notion of.",
                    "label": 0
                },
                {
                    "sent": "Groups of images.",
                    "label": 0
                },
                {
                    "sent": "Groups of independent images.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This brings us to the idea that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there is a hierarchy of models or of problems actually.",
                    "label": 1
                },
                {
                    "sent": "That tried to explore independence on single datasets.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of what ICA would look like.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a transformation from.",
                    "label": 0
                },
                {
                    "sent": "Into a smaller subspace from the data.",
                    "label": 0
                },
                {
                    "sent": "Basically minimizing KL divergences.",
                    "label": 0
                },
                {
                    "sent": "And here is an example of what the output nonlinearities will look like in the case of assuming.",
                    "label": 0
                },
                {
                    "sent": "Laplace prior?",
                    "label": 0
                },
                {
                    "sent": "Distributions of these.",
                    "label": 0
                },
                {
                    "sent": "Latent variables and here the latent variables are the images that we're trying to recover.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can extend that with the example that I showed to a model that is more complex, where here I color coded.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I color coded components that should have some built independence and should be put together and the way to retain that dependence within a data store within those.",
                    "label": 0
                },
                {
                    "sent": "Within that, what I call a subspace is to model the into these nonlinear transformation joint distribution of these.",
                    "label": 0
                },
                {
                    "sent": "Components.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see that there is a nice transition from single datasets to multiple datasets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And but we want to consider now the case of multiple.",
                    "label": 0
                },
                {
                    "sent": "Groups of sources.",
                    "label": 0
                },
                {
                    "sent": "In a single data set.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's the case of independent subspace analysis.",
                    "label": 0
                },
                {
                    "sent": "That's an example.",
                    "label": 0
                },
                {
                    "sent": "For on a single data set, or we again I'm trying to make these subspaces independent.",
                    "label": 0
                },
                {
                    "sent": "But the subspaces are composed of sets of.",
                    "label": 0
                },
                {
                    "sent": "Not entirely independent or not, maybe not at all independent components.",
                    "label": 0
                },
                {
                    "sent": "So that highlight.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That there is another hierarchy in the direction of going from unit dimensional components on a single data set versus multidimensional components.",
                    "label": 0
                },
                {
                    "sent": "Again, in one data set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And putting these ideas altogether, but we can have a more complicated model or more general model.",
                    "label": 0
                },
                {
                    "sent": "Better way, better set.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "This components from multiple data sets can be arbitrarily put together and their joint dependence can be modeled.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of picking a distribution that doesn't factor at least not easily, so that joint distribution could be something like a multivariate Laplace again.",
                    "label": 0
                },
                {
                    "sent": "We just use this assignment matrix to indicate it's basically a set of businesses to indicate which components from each modality should.",
                    "label": 0
                },
                {
                    "sent": "Make each of the subspaces, yeah.",
                    "label": 0
                },
                {
                    "sent": "Supplied by you or there you can do either way and I'm going to show a way that you can learn them, but they are at least in this model.",
                    "label": 0
                },
                {
                    "sent": "There is strictly sparse, so every component just goes only into one subspace.",
                    "label": 0
                },
                {
                    "sent": "So if you were to think of this second layer as.",
                    "label": 0
                },
                {
                    "sent": "Fully connected layer.",
                    "label": 0
                },
                {
                    "sent": "Lots of those layers with most of those layers would be 0 so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to complete the idea, you can extend these notions of generalized models into the cases of what kind of statistics are you modeling from from the data.",
                    "label": 0
                },
                {
                    "sent": "So there are methods that look at 2nd order statistics only.",
                    "label": 0
                },
                {
                    "sent": "This could be Canonical correlation analysis for example, and you could some methods like inflamax just look at.",
                    "label": 0
                },
                {
                    "sent": "Higher order statistics.",
                    "label": 0
                },
                {
                    "sent": "So the data.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to try to sell here is that we can do both, and that's going to get better results.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a bit of review.",
                    "label": 0
                },
                {
                    "sent": "One cool idea is to instead of.",
                    "label": 0
                },
                {
                    "sent": "So historically these methods depend on data reduction.",
                    "label": 0
                },
                {
                    "sent": "Because these.",
                    "label": 0
                },
                {
                    "sent": "There is a you have to compute the determinant of the these weight matrices.",
                    "label": 0
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "To go the traditional route and when you have these weight matrices that are actually reducing the space, you can compute the determinant.",
                    "label": 0
                },
                {
                    "sent": "So one interesting idea in the literature was to use another encoder to learn the latent representation at the same time you learn these independent representations.",
                    "label": 0
                },
                {
                    "sent": "And in this case the.",
                    "label": 0
                },
                {
                    "sent": "The example is from.",
                    "label": 0
                },
                {
                    "sent": "Lee and colleagues.",
                    "label": 0
                },
                {
                    "sent": "Who basically proposed a simple linear encoder to do the task and use the auto encoder as a regularizer for the independence cost.",
                    "label": 1
                },
                {
                    "sent": "So that's an interesting idea an what I decided to do was to borrow from there an except instead of using the transpose like it's indicated here, I just substituted it by the pseudoinverse with the motivation to make it.",
                    "label": 1
                },
                {
                    "sent": "More general for the case of non white data, which wasn't an assumption in the original.",
                    "label": 0
                },
                {
                    "sent": "Publication.",
                    "label": 0
                },
                {
                    "sent": "So with this formulation I applied it then to every data set individually and that way we can learn these subspaces as we optimize for these independence of places.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so to clarify, the auto encoders are run first.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 1
                },
                {
                    "sent": "Reduce the data in a way to find this reduced space.",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                },
                {
                    "sent": "Optimization for independence starts.",
                    "label": 0
                },
                {
                    "sent": "But it continues using the encoder as a constraint to basically prevent it from from losing that needs representation that it already or basically allows it to search within that smaller subspace.",
                    "label": 0
                },
                {
                    "sent": "That reduced representation search for directions that have independence.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea here I just wanted to show that these general models simplifies into more traditional models and.",
                    "label": 0
                },
                {
                    "sent": "I at this point I wanted to mention that.",
                    "label": 0
                },
                {
                    "sent": "So there is a combinatorial problem to solve.",
                    "label": 0
                },
                {
                    "sent": "To find this matrix, be here.",
                    "label": 0
                },
                {
                    "sent": "Ann, the way I'm solving it here is initially fix this weights, fix the matrix P, optimize the cost function.",
                    "label": 0
                },
                {
                    "sent": "Once you learn this latent representations, they might be incorrect and then what you do is what I did at least is.",
                    "label": 0
                },
                {
                    "sent": "Simple greedy combinatorial optimization, just.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, just pairing these sources together, whether or not they grouping them together causes a reduction in a class function.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple strategy and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did actually give some really nice results.",
                    "label": 0
                },
                {
                    "sent": "Slightly quickly on this one, just showing an example on basic independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "The method compared to inflamax.",
                    "label": 0
                },
                {
                    "sent": "Now is in general performing considerably better.",
                    "label": 0
                },
                {
                    "sent": "We don't need to do data reduction here, we're just using the auto encoder to do this.",
                    "label": 0
                },
                {
                    "sent": "To learn these representations directly from the data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case of independent subspace analysis, the.",
                    "label": 0
                },
                {
                    "sent": "No, no, the the combinatorial optimization is critical to actually get any any good results because otherwise.",
                    "label": 0
                },
                {
                    "sent": "We tend very often to find a local minima.",
                    "label": 0
                },
                {
                    "sent": "An doing that sweep over the components and trying to rearrange them together into groups actually moves the the current solution out of the local minimum has that effect and then restarting the optimization for the original independence cost.",
                    "label": 0
                },
                {
                    "sent": "Then can actually find.",
                    "label": 0
                },
                {
                    "sent": "Can lower the costs even further.",
                    "label": 0
                },
                {
                    "sent": "So basically with an effect of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moving out of local minima.",
                    "label": 0
                },
                {
                    "sent": "So an example on if you are going to skip this one, but it's doing much better than state of the art.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finalize with the hybrid example.",
                    "label": 0
                },
                {
                    "sent": "This is basically took brain imaging data from different modalities.",
                    "label": 0
                },
                {
                    "sent": "From three different publications and artificially made the like these triplets here, there are three of them.",
                    "label": 0
                },
                {
                    "sent": "So the first top panel here.",
                    "label": 0
                },
                {
                    "sent": "There are three images.",
                    "label": 0
                },
                {
                    "sent": "These three images are dependent.",
                    "label": 0
                },
                {
                    "sent": "And but they are independent from the others that I'm showing the bottom.",
                    "label": 0
                },
                {
                    "sent": "So basically showing that the method can learn these representations directly from the data datasets are quite big and.",
                    "label": 0
                },
                {
                    "sent": "The results were obtained with just 600 examples.",
                    "label": 0
                },
                {
                    "sent": "Training on just 600 examples, and they can learn pretty well showing the ground truth versus the estimate so you can see that for each one of these pairs, do the Maps that are recovered pretty.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Close to the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So the next thing we want to try now is obviously try and substitute those linear transformations by deeper nonlinear transformations and explore these non linear subspaces independent subspaces.",
                    "label": 0
                },
                {
                    "sent": "Perhaps modified the optimization strategy.",
                    "label": 0
                },
                {
                    "sent": "The community optimization for something that directly.",
                    "label": 0
                },
                {
                    "sent": "They can simultaneously estimate those connections as we're learning the subspaces, independence of spaces and finally.",
                    "label": 0
                },
                {
                    "sent": "Try and combine these with other more.",
                    "label": 0
                },
                {
                    "sent": "Complicated architectures like or just different architectures like recurrent neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Yeah, are there any questions?",
                    "label": 0
                }
            ]
        }
    }
}