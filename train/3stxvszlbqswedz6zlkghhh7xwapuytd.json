{
    "id": "3stxvszlbqswedz6zlkghhh7xwapuytd",
    "title": "Extracting and Composing Robust Features with Denoising Autoencoders",
    "info": {
        "author": [
            "Pascal Vincent, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_vincent_ecrf/",
    "segmentation": [
        [
            "So hello everybody, I'm Pascal Vincent and I'm going to tell you about extracting and composing robust features with denoising autoencoders.",
            "This is work done with my collaborators at the University Of Montreal.",
            "You go level shell, Yoshua Bengio and piano and months ago."
        ],
        [
            "OK.",
            "So.",
            "In machine learning, we are typically faced with the problem of building good predictors on very complex domains.",
            "So this typically means that what we want to learn are actually quite complicated functions.",
            "So, um.",
            "Much of the work in our in our lab in recent recent time has been based on the premise that such complicated functions are probably best represented by multiple levels of nonlinear operations.",
            "That means deep architectures.",
            "So multiple composition of multiple levels of functions.",
            "Now, deep architectures are certainly not a new idea.",
            "If you think back at the the the years of multilayer perceptrons, multilayer perceptrons are the typical example of deep architectures.",
            "For those of you who don't know, multilayer perceptrons is kind of neural networks.",
            "You can think of them as if you know generalized linear models.",
            "Multilayer perceptrons are generalized nonlinear models.",
            "Now, so that's all fine, but it happens that the learning the parameters of these architectures proved quite quite shy."
        ],
        [
            "So if we look at how this has been approached, the traditional approach which was.",
            "Proposed by Rumelhart, Hinton and Williams in 86 already is to start with a neural network, so a nonlinear parameterized function initializes parameters at random and then perform gradient descent on the objective function which you want to minimize.",
            "Um?",
            "Now when using multi layer multiple layers this led to rather disappointing performance.",
            "Actually it seems that with one layer this works fine.",
            "1 hidden layer.",
            "This works fine, but adding layers doesn't really seem to allow us to to improve contrary to what was expected.",
            "Ann and this is pretty much what we were it since 1986.",
            "With these neural networks we're kind of stuck in this situation."
        ],
        [
            "Until quite recently.",
            "Where are Geoffrey Hinton and someone send Aaron anywhere take in 2006?",
            "Proposed a different technique?",
            "And they actually used to train deep belief networks which were initialized by stacking restricted Boltzmann machines.",
            "So it's a layer wise procedure where you stack them one by one, an then fine tune them with another program.",
            "So this led to quite an impressive performance.",
            "The key to this success seems to be a good unsupervised layer by layer initialization.",
            "So in other words, what was really a bad idea in solution one was just to start initializing at random and then follow the gradient for the supervised strictly supervised cost.",
            "So following this idea."
        ],
        [
            "Here.",
            "Another solution has been attempted by my coworkers Yoshua Bengio and Google Shell, which was to initialize these deep networks by stacking autoencoders.",
            "So and then find to the movie gradient descent.",
            "This is much closer to the initial neural network feedforward neural network kind, and it's a very simple generic procedure.",
            "Know something required, all fine and its performance was almost as good as solution to.",
            "OK, almost there."
        ],
        [
            "Good, but not quite.",
            "I have to say in our experiments now, can we do better?"
        ],
        [
            "Can we do better?",
            "So the way.",
            "I tried to approach this question is really what we're doing when we learning this layer by layer is will learning we're trying to find good intermediate representation, so mapping from the input to some good intermediate representations.",
            "Now the question is what makes up a good representation?",
            "What is a good criterion will be a good criterion for finding good initial representation?",
            "Anne Anne here.",
            "Let me give you my inspiration, which came from what I call our ability, our human ability to fill in the blanks so.",
            "As an example, probably when you were a child at some point you very young Ann walking down the street with your with your mother Ann.",
            "You saw this thing coming at coming at you and barking, right?",
            "So you saw the picture of the dog and you heard the sound of dog barking and then you heard your mother say oh this is a dog or bad doc.",
            "And then you made the Association.",
            "You had this like three kinds of stimuli.",
            "The image of the dog.",
            "The barking sound and the word.",
            "The sound of the word dog.",
            "So now these three were kind of associated.",
            "Now if you close your eyes OK an if you hear barking then immediately you're going to see the image of a dog and remember the word dog in your mind or vice versa.",
            "If I say the word dog, you can picture a dog in here.",
            "Imagine the barking right.",
            "So this is a kind of filling in the blanks, so there's some.",
            "This is very high level perspective now from lower level.",
            "There we have this ability to fill in like small occlusions and images or missing pixels etc.",
            "So.",
            "This amazing ability tells us something and the main idea here is that if we're able to, sorry.",
            "If we're able to.",
            "Fill in the blank.",
            "You have a system that is able to fill in blanks with high performers.",
            "Then it means it has captured our essentials of the distribution for the underlying distribution.",
            "This is linked to the old notion of.",
            "Associative memory alright all of a sudden.",
            "This is the old notion of associative memory that's better due back to Hopfield and.",
            "So what we propose is to initialize a deep network by training it to explicitly fill in the blanks.",
            "In other words, we want it to be robust.",
            "2.",
            "Input patterns that we have participated by introducing blanks and we wanted to be able to build an intermittent representation that is going to be able to regenerate to fill in those blanks."
        ],
        [
            "So completely here's what we propose.",
            "They did."
        ],
        [
            "Using autoencoder, so we start with some input X.",
            "Clean input X in 01 to the D. It gets partially destroyed, so we're going to introduce some blanks.",
            "Here, which will give us a corrupted input, textile down and the corruption process.",
            "We call it QD like destruction.",
            "This."
        ],
        [
            "X~ is then mapped.",
            "To some hidden representation Y.",
            "Through some mapping function F Theta parameterized by feeder and these parameters are what we're actually going to want to learn."
        ],
        [
            "And from this Y we're going to reconstruct Z with a mapping function G parameterized by theater prime.",
            "OK, and we want this Z to be a good reconstruction of X."
        ],
        [
            "We actually want these two to be close, and this closeness measure is is measured by a reconstruction error, particular construction error which actually cross entropy reconstruction error, which is the cross entropy between Bernoulli with parameter X and Prometric."
        ],
        [
            "OK.",
            "So regarding the distraction correction process, actually we're simply we're using very simple corruption process.",
            "We're taking our input X and picking at random.",
            "Given fixed proportion knew of the components of X an, resetting them to 0.",
            "So this can be viewed as replacing the components by missing values or default.",
            "So obviously other corruption process is impossible."
        ],
        [
            "As for the mappings, we use Standard Sigma neural network layers, so the corrupted input X~ is actually mapped through some affine transformation parameterized by your matrix W, and the bias so weight matrix W and bias B and pass through a sigmoid.",
            "Just like.",
            "Logistic regression.",
            "Angie Theater Prime is the same form."
        ],
        [
            "So actually denoising after doing this work, we found out that denoising using slightly different kind of autoencoders, more classical autoencoders had actually been introduced much earlier by Yahoo account in 87 as an alternative to helpful networks at the time.",
            "To perform denoising.",
            "Now what's?"
        ],
        [
            "The new is.",
            "Counselor motivation to learn intermediate representations and actually we're going to use these denoising auto encoders to build deep networks.",
            "So we're going to stack them.",
            "OK, so this is just initialization procedure.",
            "So what we do is we take our input X.",
            "And we train a denoising autoencoder to learn our mapping F, Theta with some corruption noise here.",
            "Once we've done this, we take the noise down to zero an, which actually corresponds to applying F Theta directly to X.",
            "So there's no, there's no work."
        ],
        [
            "Corruption anymore, so we get rid of."
        ],
        [
            "The scaffolding, so we keep only X and it's mapping too.",
            "To the hidden representation, why?"
        ],
        [
            "From this we."
        ],
        [
            "We build another denoising autoencoder on the new representation.",
            "OK, so we start over again.",
            "We take this new presentation to learn SC2 for the second level."
        ],
        [
            "Remove the side scaffolding and etc."
        ],
        [
            "OK, we iterate like that to initialize subsequent layers."
        ],
        [
            "So that's initialization.",
            "So once we've done this initialization, which was done in a totally unsupervised way, meaning we're not using any of the labels."
        ],
        [
            "What we do is we use this to as initialization for supervised task for the supplies task that we are interested in.",
            "So for the two first task we are provided target."
        ],
        [
            "So we add a an output layer.",
            "And a supervised cost.",
            "That's going to tell us how good we do in our input compared to the target, how good our input is a prediction of the target, and we fine tune the whole thing via gradient descent on the supervised criterion.",
            "So grand is going to kind of fine tune.",
            "Also, all the intermediate weights which were just initialized with."
        ],
        [
            "All 10 coders.",
            "OK.",
            "I'm.",
            "How much time do I have?",
            "8 minutes OK. 10 minutes right?",
            "So I'm going to go quickly on this.",
            "This is a I'm going to show you quickly three different point of views to kind of get some feeling of what's going on here and maybe justify the approach a little bit.",
            "So 1 First Perspective is that of viewing denoising auto encoders from manifold learning perspectives.",
            "Now if you think of your data points is lying on a lower dimensional manifold in the very high dimensional input space which is represented here by this black line here and the little axis, then when we apply a corruption or corruption process to get our X~ where going to typically textile, that's outside that manifold.",
            "It's further away from that manifold.",
            "And then our function, our composition of the two function F, Theta of X. Till there then pass through GC to prime is going to reproduce X, right?",
            "So we're going to kind of re project on that manifold, so it's going to take points that are.",
            "It's going to take probability mass that's outside the manifold and put it back end on the manifold.",
            "So from this point of view.",
            "From this point of view, the.",
            "Hidden representation Y can be interpreted as a coordinate system for points on the manifold.",
            "OK, because.",
            "Several points that are kind of outside the manifold will get mapped to a Y that will correspond to a coordinate system on the manifold.",
            "So that's."
        ],
        [
            "Perspective.",
            "A second perspective is looking at this For more information theoretic point of view and actually we can show that maximizing the minimizing the expected reconstruction error as it just showed amounts to maximizing a lower bound on the mutual information between clean input X and.",
            "And the hidden representation why?",
            "And this?",
            "Even though, why is a deterministic function of a?",
            "Corrupted input, so essentially what we're doing is saying make sure that this hidden representation captures all information as much as necessary.",
            "Information about the clean input X, even though you're just a deterministic function of the corrupted X."
        ],
        [
            "Another point of view and finally 1/3 point of view is generative model perspective.",
            "So we can show that the denoising autoencoders training is equivalent to maximizing a variational bound on the likelihood of a generative model of the corrupted data.",
            "So I won't enter into details of this."
        ],
        [
            "Alright.",
            "Now we perform some experiments with this.",
            "Here going to present the data sets that we used.",
            "These datasets were were chosen because they are quite challenging.",
            "They have a number of.",
            "Factors of variations that quite varying so.",
            "And they're quite challenging for current learning algorithms, so one is M missed.",
            "The first basic is just as.",
            "Is a subset of emnace with 10,000 training samples.",
            "All other datasets are also have 10,000 training samples, so route is a MNIST weather added rotation, so you see the characters are rotated BG round is background random noise.",
            "As you can see it's been added.",
            "BG Image is background images that have been added and relative rotation background images is a combination of the rotation of background images."
        ],
        [
            "Fun datasets.",
            "An rectangle there are two versions.",
            "There's the simple rectangle on top which is to discriminate between tall and wide rectangle on the back black background.",
            "And rectangle image is to discriminate between tall and wide rectangle on image background.",
            "And with the rectangular themselves being images and finally convex is also binary classification, where the goal is to say is it a convex shape or a non convex shape."
        ],
        [
            "So weak."
        ],
        [
            "Repaired several."
        ],
        [
            "Homes."
        ],
        [
            "On all these datasets.",
            "For all algorithms, the hyperparameters were tuned on a validation set, and here we are reporting the error on a separate test."
        ],
        [
            "So our baseline with support vector machines with RBF kernels.",
            "So this is how well they're doing on all these datasets.",
            "We also tried them with.",
            "Polynomial kernels.",
            "An results were similar to little worse with polynomial kernels, so I'm not reporting them."
        ],
        [
            "Then we compare this with Hinton and Co workers.",
            "A deep belief networks.",
            "So which train was stacked?",
            "Restricted Boltzmann machines, so already the results were quite good."
        ],
        [
            "Now these were the results.",
            "Will simply stacked auto associators without the noise without the noise.",
            "So this corresponds to what we've just shown that with absolute absolutely no corruption to the data.",
            "So we can see that was all.",
            "It was almost as good as it was.",
            "Often better than support vector machines.",
            "Some quite as good as deep belief networks, but not quite."
        ],
        [
            "And this is a new algorithm.",
            "So this as the A3 for stack denoising auto associators.",
            "Autoencoders with three hidden layers.",
            "Anne knew is our percentage of corruption that was optimal on the validation set and the errors on the test set.",
            "As can be seen in red, we always put in read either the best or those that are for which the constant confidence interval overlaps with the best so we can see where best or equivalent to best.",
            "On about all datasets except this one, except background random.",
            "And we can see also that it's much better than without without the noise where we caught up with the deep belief networks and even improved on them on the number of tasks.",
            "Now, just for reference, we also tried support vector machines with corrupted data.",
            "Similarly corrupted data as input only."
        ],
        [
            "And it doesn't do it doesn't do any better so."
        ],
        [
            "Alright, and now what I found the most interesting actually of all these experiments are these pictures.",
            "Here what I'm showing are the filters they learned filters by the algorithm.",
            "That is, the rows of the matrix.",
            "So each little Patch here corresponds to a row of the matrix which has same dimensionality as the input.",
            "So we can display it as a little image image.",
            "And this show the filters that were learned simply with the with the denoising autoencoders.",
            "So without a fine tuning just for the first layer with when we don't perform any destruction, so zero person destructive noise.",
            "An we can see many of these neurons are have filters.",
            "There are kind of grayish and not not very interesting, but there's a few interesting ones.",
            "Now.",
            "What's very interesting is if we train now a denoising autoencoder initialized with the same random weights but with a in additional."
        ],
        [
            "10% noise?",
            "OK, so we see some more of the filters are doing something interesting.",
            "They're kind of learning to look at biggest structures.",
            "To be sensitive to bigger structures, you can see edge detectors appearing where they weren't before."
        ],
        [
            "So I'm gonna move."
        ],
        [
            "From zero percent 10."
        ],
        [
            "Sent."
        ],
        [
            "25%.",
            "And 50% so it's quite striking."
        ],
        [
            "Striking that."
        ],
        [
            "Without the denoising.",
            "Seems that many of these units are not doing anything interesting, so really the network is starting from a position that it's a very difficult place to to find out."
        ],
        [
            "Interesting."
        ],
        [
            "And as we increase the noise.",
            "Whoops sorry.",
            "We we get to these.",
            "Another point that's worth noticing is remarking is that our good results that we obtained on the datasets were often obtained for overcomplete representations.",
            "So hidden layer with more units than the input, which is not something that you would normally do with a normal autoencoders cousin.",
            "Autoencoder, 'cause just learn to copy the input and it has meant to reproduce it perfectly, but with the as we're training them to do the denoising task.",
            "They have to capture interesting structure even though there are more hidden units than than input dimensions."
        ],
        [
            "OK, so to conclude, we've shown that an unsupervised initialization of layers with an explicit denoising criterion can capture interesting structure in the input.",
            "And this leads to the learning of intermediate representations, which are much better suited for the learning of a supervised task afterwards.",
            "This improves on benchmarks.",
            "Anne.",
            "Also, one point here we focused on the supervised setting, but this algorithm can actually use and assuming supervised setting.",
            "Since all denoising autoencoders don't use label labels in any matter, so all these initialization can be done using the available Unley, all available unlabeled data.",
            "We need to make some experience regarding this and future work.",
            "We will also investigate the effect of different kinds of noise and."
        ],
        [
            "Option process is.",
            "Thank you very much.",
            "Pyramids on the rotated digits in your noise takes into account random rotations that know Now the noise.",
            "The noise is just the noise that I described early on is totally it doesn't use any prior knowledge, it's just putting 20 some other pixels at random.",
            "That's all.",
            "Yes.",
            "How much should I have?",
            "Can I have too much noise?",
            "You know we generate more samples because it can draw muffled like we have one example but can create multiple corrupted copies, yes?",
            "OK, so.",
            "So the amount of noise is obviously the hyperparameter that's that's been tuned.",
            "As for actually we are using stochastic gradient descent, so we generally using a corrupted version of the example at each time.",
            "So it's not as if we were generating the big set of corrupted examples 1st and then using them all.",
            "We're just corrupting them each time we will look at an example.",
            "Now this is for denoising autoencoders.",
            "Why did the experiments to compare with support vector machines?",
            "Then I had to generate the data set that was like 10 times bigger because it had 1010 corrupted versions of well non corrupted versions of the input which made the SVM training even much slower and it didn't improve their results in anyway.",
            "So for.",
            "For this neural networks, it's really.",
            "It's really like mimicking the some, maybe some noise that's that's there.",
            "In the neural processes or in the input or.",
            "Yep.",
            "So noise sometimes acts as a regularization, yes.",
            "And different supervised learning methods.",
            "So.",
            "OK, several answers to this.",
            "First, as you see where they add noise and try a different supervised method like support vector machines with added noise and it didn't improve the results.",
            "I mean not not by the margin that we see with these methods.",
            "Second, it's been shown that small in the limit of small additional additive noise, small Gaussian additive noise.",
            "This is equivalent to.",
            "Like Ridge regularization away decay for for neural networks, but this is not the kind of noise we have we have.",
            "We don't have small additive noise, right?",
            "Regularization means that you kind of insensitive to little perturbations in your input right?",
            "Which is equivalent to being kind and sweet.",
            "Little perturbations in your weights and etc.",
            "But here it's not a little perturbations where doing big perturbations were resetting to 0.",
            "Sum of the of the inputs.",
            "And we're not just we're not training supervisor right here on that, but we're training a layer to fill in those blanks, right?",
            "So it's very different objective that's being being being modeled here.",
            "And also the equivalence with regularization.",
            "I think there's not hole for this kind of noise, which is not small additive noise."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hello everybody, I'm Pascal Vincent and I'm going to tell you about extracting and composing robust features with denoising autoencoders.",
                    "label": 1
                },
                {
                    "sent": "This is work done with my collaborators at the University Of Montreal.",
                    "label": 0
                },
                {
                    "sent": "You go level shell, Yoshua Bengio and piano and months ago.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In machine learning, we are typically faced with the problem of building good predictors on very complex domains.",
                    "label": 1
                },
                {
                    "sent": "So this typically means that what we want to learn are actually quite complicated functions.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Much of the work in our in our lab in recent recent time has been based on the premise that such complicated functions are probably best represented by multiple levels of nonlinear operations.",
                    "label": 1
                },
                {
                    "sent": "That means deep architectures.",
                    "label": 1
                },
                {
                    "sent": "So multiple composition of multiple levels of functions.",
                    "label": 0
                },
                {
                    "sent": "Now, deep architectures are certainly not a new idea.",
                    "label": 0
                },
                {
                    "sent": "If you think back at the the the years of multilayer perceptrons, multilayer perceptrons are the typical example of deep architectures.",
                    "label": 1
                },
                {
                    "sent": "For those of you who don't know, multilayer perceptrons is kind of neural networks.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as if you know generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "Multilayer perceptrons are generalized nonlinear models.",
                    "label": 0
                },
                {
                    "sent": "Now, so that's all fine, but it happens that the learning the parameters of these architectures proved quite quite shy.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at how this has been approached, the traditional approach which was.",
                    "label": 0
                },
                {
                    "sent": "Proposed by Rumelhart, Hinton and Williams in 86 already is to start with a neural network, so a nonlinear parameterized function initializes parameters at random and then perform gradient descent on the objective function which you want to minimize.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now when using multi layer multiple layers this led to rather disappointing performance.",
                    "label": 0
                },
                {
                    "sent": "Actually it seems that with one layer this works fine.",
                    "label": 0
                },
                {
                    "sent": "1 hidden layer.",
                    "label": 0
                },
                {
                    "sent": "This works fine, but adding layers doesn't really seem to allow us to to improve contrary to what was expected.",
                    "label": 0
                },
                {
                    "sent": "Ann and this is pretty much what we were it since 1986.",
                    "label": 0
                },
                {
                    "sent": "With these neural networks we're kind of stuck in this situation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Until quite recently.",
                    "label": 0
                },
                {
                    "sent": "Where are Geoffrey Hinton and someone send Aaron anywhere take in 2006?",
                    "label": 1
                },
                {
                    "sent": "Proposed a different technique?",
                    "label": 0
                },
                {
                    "sent": "And they actually used to train deep belief networks which were initialized by stacking restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "So it's a layer wise procedure where you stack them one by one, an then fine tune them with another program.",
                    "label": 1
                },
                {
                    "sent": "So this led to quite an impressive performance.",
                    "label": 1
                },
                {
                    "sent": "The key to this success seems to be a good unsupervised layer by layer initialization.",
                    "label": 0
                },
                {
                    "sent": "So in other words, what was really a bad idea in solution one was just to start initializing at random and then follow the gradient for the supervised strictly supervised cost.",
                    "label": 0
                },
                {
                    "sent": "So following this idea.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Another solution has been attempted by my coworkers Yoshua Bengio and Google Shell, which was to initialize these deep networks by stacking autoencoders.",
                    "label": 1
                },
                {
                    "sent": "So and then find to the movie gradient descent.",
                    "label": 1
                },
                {
                    "sent": "This is much closer to the initial neural network feedforward neural network kind, and it's a very simple generic procedure.",
                    "label": 0
                },
                {
                    "sent": "Know something required, all fine and its performance was almost as good as solution to.",
                    "label": 1
                },
                {
                    "sent": "OK, almost there.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, but not quite.",
                    "label": 0
                },
                {
                    "sent": "I have to say in our experiments now, can we do better?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we do better?",
                    "label": 0
                },
                {
                    "sent": "So the way.",
                    "label": 0
                },
                {
                    "sent": "I tried to approach this question is really what we're doing when we learning this layer by layer is will learning we're trying to find good intermediate representation, so mapping from the input to some good intermediate representations.",
                    "label": 0
                },
                {
                    "sent": "Now the question is what makes up a good representation?",
                    "label": 0
                },
                {
                    "sent": "What is a good criterion will be a good criterion for finding good initial representation?",
                    "label": 1
                },
                {
                    "sent": "Anne Anne here.",
                    "label": 0
                },
                {
                    "sent": "Let me give you my inspiration, which came from what I call our ability, our human ability to fill in the blanks so.",
                    "label": 0
                },
                {
                    "sent": "As an example, probably when you were a child at some point you very young Ann walking down the street with your with your mother Ann.",
                    "label": 0
                },
                {
                    "sent": "You saw this thing coming at coming at you and barking, right?",
                    "label": 0
                },
                {
                    "sent": "So you saw the picture of the dog and you heard the sound of dog barking and then you heard your mother say oh this is a dog or bad doc.",
                    "label": 0
                },
                {
                    "sent": "And then you made the Association.",
                    "label": 0
                },
                {
                    "sent": "You had this like three kinds of stimuli.",
                    "label": 0
                },
                {
                    "sent": "The image of the dog.",
                    "label": 0
                },
                {
                    "sent": "The barking sound and the word.",
                    "label": 0
                },
                {
                    "sent": "The sound of the word dog.",
                    "label": 0
                },
                {
                    "sent": "So now these three were kind of associated.",
                    "label": 0
                },
                {
                    "sent": "Now if you close your eyes OK an if you hear barking then immediately you're going to see the image of a dog and remember the word dog in your mind or vice versa.",
                    "label": 0
                },
                {
                    "sent": "If I say the word dog, you can picture a dog in here.",
                    "label": 0
                },
                {
                    "sent": "Imagine the barking right.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of filling in the blanks, so there's some.",
                    "label": 0
                },
                {
                    "sent": "This is very high level perspective now from lower level.",
                    "label": 1
                },
                {
                    "sent": "There we have this ability to fill in like small occlusions and images or missing pixels etc.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This amazing ability tells us something and the main idea here is that if we're able to, sorry.",
                    "label": 0
                },
                {
                    "sent": "If we're able to.",
                    "label": 0
                },
                {
                    "sent": "Fill in the blank.",
                    "label": 0
                },
                {
                    "sent": "You have a system that is able to fill in blanks with high performers.",
                    "label": 0
                },
                {
                    "sent": "Then it means it has captured our essentials of the distribution for the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "This is linked to the old notion of.",
                    "label": 1
                },
                {
                    "sent": "Associative memory alright all of a sudden.",
                    "label": 1
                },
                {
                    "sent": "This is the old notion of associative memory that's better due back to Hopfield and.",
                    "label": 0
                },
                {
                    "sent": "So what we propose is to initialize a deep network by training it to explicitly fill in the blanks.",
                    "label": 0
                },
                {
                    "sent": "In other words, we want it to be robust.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Input patterns that we have participated by introducing blanks and we wanted to be able to build an intermittent representation that is going to be able to regenerate to fill in those blanks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So completely here's what we propose.",
                    "label": 0
                },
                {
                    "sent": "They did.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using autoencoder, so we start with some input X.",
                    "label": 0
                },
                {
                    "sent": "Clean input X in 01 to the D. It gets partially destroyed, so we're going to introduce some blanks.",
                    "label": 1
                },
                {
                    "sent": "Here, which will give us a corrupted input, textile down and the corruption process.",
                    "label": 0
                },
                {
                    "sent": "We call it QD like destruction.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "X~ is then mapped.",
                    "label": 0
                },
                {
                    "sent": "To some hidden representation Y.",
                    "label": 1
                },
                {
                    "sent": "Through some mapping function F Theta parameterized by feeder and these parameters are what we're actually going to want to learn.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from this Y we're going to reconstruct Z with a mapping function G parameterized by theater prime.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want this Z to be a good reconstruction of X.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually want these two to be close, and this closeness measure is is measured by a reconstruction error, particular construction error which actually cross entropy reconstruction error, which is the cross entropy between Bernoulli with parameter X and Prometric.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So regarding the distraction correction process, actually we're simply we're using very simple corruption process.",
                    "label": 0
                },
                {
                    "sent": "We're taking our input X and picking at random.",
                    "label": 1
                },
                {
                    "sent": "Given fixed proportion knew of the components of X an, resetting them to 0.",
                    "label": 1
                },
                {
                    "sent": "So this can be viewed as replacing the components by missing values or default.",
                    "label": 0
                },
                {
                    "sent": "So obviously other corruption process is impossible.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As for the mappings, we use Standard Sigma neural network layers, so the corrupted input X~ is actually mapped through some affine transformation parameterized by your matrix W, and the bias so weight matrix W and bias B and pass through a sigmoid.",
                    "label": 1
                },
                {
                    "sent": "Just like.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Angie Theater Prime is the same form.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually denoising after doing this work, we found out that denoising using slightly different kind of autoencoders, more classical autoencoders had actually been introduced much earlier by Yahoo account in 87 as an alternative to helpful networks at the time.",
                    "label": 1
                },
                {
                    "sent": "To perform denoising.",
                    "label": 0
                },
                {
                    "sent": "Now what's?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The new is.",
                    "label": 0
                },
                {
                    "sent": "Counselor motivation to learn intermediate representations and actually we're going to use these denoising auto encoders to build deep networks.",
                    "label": 0
                },
                {
                    "sent": "So we're going to stack them.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just initialization procedure.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take our input X.",
                    "label": 0
                },
                {
                    "sent": "And we train a denoising autoencoder to learn our mapping F, Theta with some corruption noise here.",
                    "label": 1
                },
                {
                    "sent": "Once we've done this, we take the noise down to zero an, which actually corresponds to applying F Theta directly to X.",
                    "label": 0
                },
                {
                    "sent": "So there's no, there's no work.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Corruption anymore, so we get rid of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The scaffolding, so we keep only X and it's mapping too.",
                    "label": 0
                },
                {
                    "sent": "To the hidden representation, why?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From this we.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We build another denoising autoencoder on the new representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start over again.",
                    "label": 0
                },
                {
                    "sent": "We take this new presentation to learn SC2 for the second level.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remove the side scaffolding and etc.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we iterate like that to initialize subsequent layers.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's initialization.",
                    "label": 0
                },
                {
                    "sent": "So once we've done this initialization, which was done in a totally unsupervised way, meaning we're not using any of the labels.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is we use this to as initialization for supervised task for the supplies task that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So for the two first task we are provided target.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we add a an output layer.",
                    "label": 1
                },
                {
                    "sent": "And a supervised cost.",
                    "label": 0
                },
                {
                    "sent": "That's going to tell us how good we do in our input compared to the target, how good our input is a prediction of the target, and we fine tune the whole thing via gradient descent on the supervised criterion.",
                    "label": 1
                },
                {
                    "sent": "So grand is going to kind of fine tune.",
                    "label": 0
                },
                {
                    "sent": "Also, all the intermediate weights which were just initialized with.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All 10 coders.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "8 minutes OK. 10 minutes right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go quickly on this.",
                    "label": 0
                },
                {
                    "sent": "This is a I'm going to show you quickly three different point of views to kind of get some feeling of what's going on here and maybe justify the approach a little bit.",
                    "label": 0
                },
                {
                    "sent": "So 1 First Perspective is that of viewing denoising auto encoders from manifold learning perspectives.",
                    "label": 0
                },
                {
                    "sent": "Now if you think of your data points is lying on a lower dimensional manifold in the very high dimensional input space which is represented here by this black line here and the little axis, then when we apply a corruption or corruption process to get our X~ where going to typically textile, that's outside that manifold.",
                    "label": 0
                },
                {
                    "sent": "It's further away from that manifold.",
                    "label": 0
                },
                {
                    "sent": "And then our function, our composition of the two function F, Theta of X. Till there then pass through GC to prime is going to reproduce X, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to kind of re project on that manifold, so it's going to take points that are.",
                    "label": 0
                },
                {
                    "sent": "It's going to take probability mass that's outside the manifold and put it back end on the manifold.",
                    "label": 0
                },
                {
                    "sent": "So from this point of view.",
                    "label": 0
                },
                {
                    "sent": "From this point of view, the.",
                    "label": 0
                },
                {
                    "sent": "Hidden representation Y can be interpreted as a coordinate system for points on the manifold.",
                    "label": 1
                },
                {
                    "sent": "OK, because.",
                    "label": 0
                },
                {
                    "sent": "Several points that are kind of outside the manifold will get mapped to a Y that will correspond to a coordinate system on the manifold.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perspective.",
                    "label": 0
                },
                {
                    "sent": "A second perspective is looking at this For more information theoretic point of view and actually we can show that maximizing the minimizing the expected reconstruction error as it just showed amounts to maximizing a lower bound on the mutual information between clean input X and.",
                    "label": 1
                },
                {
                    "sent": "And the hidden representation why?",
                    "label": 0
                },
                {
                    "sent": "And this?",
                    "label": 1
                },
                {
                    "sent": "Even though, why is a deterministic function of a?",
                    "label": 1
                },
                {
                    "sent": "Corrupted input, so essentially what we're doing is saying make sure that this hidden representation captures all information as much as necessary.",
                    "label": 0
                },
                {
                    "sent": "Information about the clean input X, even though you're just a deterministic function of the corrupted X.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another point of view and finally 1/3 point of view is generative model perspective.",
                    "label": 0
                },
                {
                    "sent": "So we can show that the denoising autoencoders training is equivalent to maximizing a variational bound on the likelihood of a generative model of the corrupted data.",
                    "label": 1
                },
                {
                    "sent": "So I won't enter into details of this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now we perform some experiments with this.",
                    "label": 0
                },
                {
                    "sent": "Here going to present the data sets that we used.",
                    "label": 0
                },
                {
                    "sent": "These datasets were were chosen because they are quite challenging.",
                    "label": 0
                },
                {
                    "sent": "They have a number of.",
                    "label": 0
                },
                {
                    "sent": "Factors of variations that quite varying so.",
                    "label": 0
                },
                {
                    "sent": "And they're quite challenging for current learning algorithms, so one is M missed.",
                    "label": 0
                },
                {
                    "sent": "The first basic is just as.",
                    "label": 0
                },
                {
                    "sent": "Is a subset of emnace with 10,000 training samples.",
                    "label": 1
                },
                {
                    "sent": "All other datasets are also have 10,000 training samples, so route is a MNIST weather added rotation, so you see the characters are rotated BG round is background random noise.",
                    "label": 0
                },
                {
                    "sent": "As you can see it's been added.",
                    "label": 1
                },
                {
                    "sent": "BG Image is background images that have been added and relative rotation background images is a combination of the rotation of background images.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fun datasets.",
                    "label": 0
                },
                {
                    "sent": "An rectangle there are two versions.",
                    "label": 0
                },
                {
                    "sent": "There's the simple rectangle on top which is to discriminate between tall and wide rectangle on the back black background.",
                    "label": 1
                },
                {
                    "sent": "And rectangle image is to discriminate between tall and wide rectangle on image background.",
                    "label": 0
                },
                {
                    "sent": "And with the rectangular themselves being images and finally convex is also binary classification, where the goal is to say is it a convex shape or a non convex shape.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So weak.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repaired several.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Homes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On all these datasets.",
                    "label": 0
                },
                {
                    "sent": "For all algorithms, the hyperparameters were tuned on a validation set, and here we are reporting the error on a separate test.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our baseline with support vector machines with RBF kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is how well they're doing on all these datasets.",
                    "label": 0
                },
                {
                    "sent": "We also tried them with.",
                    "label": 0
                },
                {
                    "sent": "Polynomial kernels.",
                    "label": 0
                },
                {
                    "sent": "An results were similar to little worse with polynomial kernels, so I'm not reporting them.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we compare this with Hinton and Co workers.",
                    "label": 0
                },
                {
                    "sent": "A deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "So which train was stacked?",
                    "label": 0
                },
                {
                    "sent": "Restricted Boltzmann machines, so already the results were quite good.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now these were the results.",
                    "label": 0
                },
                {
                    "sent": "Will simply stacked auto associators without the noise without the noise.",
                    "label": 0
                },
                {
                    "sent": "So this corresponds to what we've just shown that with absolute absolutely no corruption to the data.",
                    "label": 0
                },
                {
                    "sent": "So we can see that was all.",
                    "label": 0
                },
                {
                    "sent": "It was almost as good as it was.",
                    "label": 0
                },
                {
                    "sent": "Often better than support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Some quite as good as deep belief networks, but not quite.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a new algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this as the A3 for stack denoising auto associators.",
                    "label": 0
                },
                {
                    "sent": "Autoencoders with three hidden layers.",
                    "label": 0
                },
                {
                    "sent": "Anne knew is our percentage of corruption that was optimal on the validation set and the errors on the test set.",
                    "label": 0
                },
                {
                    "sent": "As can be seen in red, we always put in read either the best or those that are for which the constant confidence interval overlaps with the best so we can see where best or equivalent to best.",
                    "label": 0
                },
                {
                    "sent": "On about all datasets except this one, except background random.",
                    "label": 0
                },
                {
                    "sent": "And we can see also that it's much better than without without the noise where we caught up with the deep belief networks and even improved on them on the number of tasks.",
                    "label": 0
                },
                {
                    "sent": "Now, just for reference, we also tried support vector machines with corrupted data.",
                    "label": 0
                },
                {
                    "sent": "Similarly corrupted data as input only.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it doesn't do it doesn't do any better so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and now what I found the most interesting actually of all these experiments are these pictures.",
                    "label": 0
                },
                {
                    "sent": "Here what I'm showing are the filters they learned filters by the algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is, the rows of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So each little Patch here corresponds to a row of the matrix which has same dimensionality as the input.",
                    "label": 0
                },
                {
                    "sent": "So we can display it as a little image image.",
                    "label": 0
                },
                {
                    "sent": "And this show the filters that were learned simply with the with the denoising autoencoders.",
                    "label": 1
                },
                {
                    "sent": "So without a fine tuning just for the first layer with when we don't perform any destruction, so zero person destructive noise.",
                    "label": 0
                },
                {
                    "sent": "An we can see many of these neurons are have filters.",
                    "label": 0
                },
                {
                    "sent": "There are kind of grayish and not not very interesting, but there's a few interesting ones.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What's very interesting is if we train now a denoising autoencoder initialized with the same random weights but with a in additional.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10% noise?",
                    "label": 0
                },
                {
                    "sent": "OK, so we see some more of the filters are doing something interesting.",
                    "label": 0
                },
                {
                    "sent": "They're kind of learning to look at biggest structures.",
                    "label": 0
                },
                {
                    "sent": "To be sensitive to bigger structures, you can see edge detectors appearing where they weren't before.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm gonna move.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From zero percent 10.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "25%.",
                    "label": 0
                },
                {
                    "sent": "And 50% so it's quite striking.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Striking that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Without the denoising.",
                    "label": 0
                },
                {
                    "sent": "Seems that many of these units are not doing anything interesting, so really the network is starting from a position that it's a very difficult place to to find out.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as we increase the noise.",
                    "label": 0
                },
                {
                    "sent": "Whoops sorry.",
                    "label": 0
                },
                {
                    "sent": "We we get to these.",
                    "label": 0
                },
                {
                    "sent": "Another point that's worth noticing is remarking is that our good results that we obtained on the datasets were often obtained for overcomplete representations.",
                    "label": 0
                },
                {
                    "sent": "So hidden layer with more units than the input, which is not something that you would normally do with a normal autoencoders cousin.",
                    "label": 0
                },
                {
                    "sent": "Autoencoder, 'cause just learn to copy the input and it has meant to reproduce it perfectly, but with the as we're training them to do the denoising task.",
                    "label": 0
                },
                {
                    "sent": "They have to capture interesting structure even though there are more hidden units than than input dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude, we've shown that an unsupervised initialization of layers with an explicit denoising criterion can capture interesting structure in the input.",
                    "label": 0
                },
                {
                    "sent": "And this leads to the learning of intermediate representations, which are much better suited for the learning of a supervised task afterwards.",
                    "label": 0
                },
                {
                    "sent": "This improves on benchmarks.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Also, one point here we focused on the supervised setting, but this algorithm can actually use and assuming supervised setting.",
                    "label": 0
                },
                {
                    "sent": "Since all denoising autoencoders don't use label labels in any matter, so all these initialization can be done using the available Unley, all available unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "We need to make some experience regarding this and future work.",
                    "label": 0
                },
                {
                    "sent": "We will also investigate the effect of different kinds of noise and.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Option process is.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Pyramids on the rotated digits in your noise takes into account random rotations that know Now the noise.",
                    "label": 0
                },
                {
                    "sent": "The noise is just the noise that I described early on is totally it doesn't use any prior knowledge, it's just putting 20 some other pixels at random.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "How much should I have?",
                    "label": 0
                },
                {
                    "sent": "Can I have too much noise?",
                    "label": 0
                },
                {
                    "sent": "You know we generate more samples because it can draw muffled like we have one example but can create multiple corrupted copies, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the amount of noise is obviously the hyperparameter that's that's been tuned.",
                    "label": 0
                },
                {
                    "sent": "As for actually we are using stochastic gradient descent, so we generally using a corrupted version of the example at each time.",
                    "label": 0
                },
                {
                    "sent": "So it's not as if we were generating the big set of corrupted examples 1st and then using them all.",
                    "label": 0
                },
                {
                    "sent": "We're just corrupting them each time we will look at an example.",
                    "label": 0
                },
                {
                    "sent": "Now this is for denoising autoencoders.",
                    "label": 1
                },
                {
                    "sent": "Why did the experiments to compare with support vector machines?",
                    "label": 0
                },
                {
                    "sent": "Then I had to generate the data set that was like 10 times bigger because it had 1010 corrupted versions of well non corrupted versions of the input which made the SVM training even much slower and it didn't improve their results in anyway.",
                    "label": 0
                },
                {
                    "sent": "So for.",
                    "label": 0
                },
                {
                    "sent": "For this neural networks, it's really.",
                    "label": 0
                },
                {
                    "sent": "It's really like mimicking the some, maybe some noise that's that's there.",
                    "label": 0
                },
                {
                    "sent": "In the neural processes or in the input or.",
                    "label": 1
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So noise sometimes acts as a regularization, yes.",
                    "label": 0
                },
                {
                    "sent": "And different supervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, several answers to this.",
                    "label": 0
                },
                {
                    "sent": "First, as you see where they add noise and try a different supervised method like support vector machines with added noise and it didn't improve the results.",
                    "label": 0
                },
                {
                    "sent": "I mean not not by the margin that we see with these methods.",
                    "label": 0
                },
                {
                    "sent": "Second, it's been shown that small in the limit of small additional additive noise, small Gaussian additive noise.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Like Ridge regularization away decay for for neural networks, but this is not the kind of noise we have we have.",
                    "label": 0
                },
                {
                    "sent": "We don't have small additive noise, right?",
                    "label": 0
                },
                {
                    "sent": "Regularization means that you kind of insensitive to little perturbations in your input right?",
                    "label": 0
                },
                {
                    "sent": "Which is equivalent to being kind and sweet.",
                    "label": 0
                },
                {
                    "sent": "Little perturbations in your weights and etc.",
                    "label": 0
                },
                {
                    "sent": "But here it's not a little perturbations where doing big perturbations were resetting to 0.",
                    "label": 0
                },
                {
                    "sent": "Sum of the of the inputs.",
                    "label": 0
                },
                {
                    "sent": "And we're not just we're not training supervisor right here on that, but we're training a layer to fill in those blanks, right?",
                    "label": 0
                },
                {
                    "sent": "So it's very different objective that's being being being modeled here.",
                    "label": 0
                },
                {
                    "sent": "And also the equivalence with regularization.",
                    "label": 0
                },
                {
                    "sent": "I think there's not hole for this kind of noise, which is not small additive noise.",
                    "label": 0
                }
            ]
        }
    }
}